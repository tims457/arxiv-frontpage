{"created":"2024-03-05 18:59:35","title":"The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning","abstract":"The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop CUT, a state-of-the-art unlearning method based on controlling model representations. CUT reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai","sentences":["The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons.","To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs.","However, current evaluations are private, preventing further research into mitigating risk.","Furthermore, they focus on only a few, highly specific pathways for malicious use.","To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security.","WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release.","WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge.","To guide progress on unlearning, we develop CUT, a state-of-the-art unlearning method based on controlling model representations.","CUT reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs.","We release our benchmark and code publicly at https://wmdp.ai"],"url":"http://arxiv.org/abs/2403.03218v1","category":"cs.LG"}
{"created":"2024-03-05 18:57:06","title":"Performance of a modular ton-scale pixel-readout liquid argon time projection chamber","abstract":"The Module-0 Demonstrator is a single-phase 600 kg liquid argon time projection chamber operated as a prototype for the DUNE liquid argon near detector. Based on the ArgonCube design concept, Module-0 features a novel 80k-channel pixelated charge readout and advanced high-coverage photon detection system. In this paper, we present an analysis of an eight-day data set consisting of 25 million cosmic ray events collected in the spring of 2021. We use this sample to demonstrate the imaging performance of the charge and light readout systems as well as the signal correlations between the two. We also report argon purity and detector uniformity measurements, and provide comparisons to detector simulations.","sentences":["The Module-0 Demonstrator is a single-phase 600 kg liquid argon time projection chamber operated as a prototype for the DUNE liquid argon near detector.","Based on the ArgonCube design concept, Module-0 features a novel 80k-channel pixelated charge readout and advanced high-coverage photon detection system.","In this paper, we present an analysis of an eight-day data set consisting of 25 million cosmic ray events collected in the spring of 2021.","We use this sample to demonstrate the imaging performance of the charge and light readout systems as well as the signal correlations between the two.","We also report argon purity and detector uniformity measurements, and provide comparisons to detector simulations."],"url":"http://arxiv.org/abs/2403.03212v1","category":"physics.ins-det"}
{"created":"2024-03-05 18:49:29","title":"Anomalous continuum scattering and higher-order van Hove singularity in the strongly anisotropic S = 1/2 triangular lattice antiferromagnet","abstract":"The S = 1/2 triangular lattice antiferromagnet (TLAF) stands out as a paradigmatic example of frustrated quantum magnetism. An ongoing challenge involves understanding the influence of exchange anisotropy on the collective behavior within such systems. Using inelastic neutron scattering (INS) and advanced calculation techniques, we have studied the low and high temperature spin dynamics of Ba2La2CoTe2O12 (BLCTO): a Co2+-based Jeff = 1/2 TLAF that exhibits 120 deg order below TN = 3.26 K. The spin Hamiltonian was determined by fitting the energy-resolved paramagnetic excitations measured at T > TN, revealing an exceptionally strong easy-plane XXZ exchange anisotropy. Below TN, the excitation spectrum exhibits a high energy continuum having a larger spectral weight than the single-magnon modes. Combined with advanced theoretical calculations of magnetic excitations based on magnons and spinons, this observation suggests a scenario characterized by a spinon confinement length that markedly exceeds the lattice spacing. We conjecture that this phenomenon arises due to the proximity to a quantum melting point, which persists even in the presence of strong easy-plane XXZ anisotropy. Finally, we highlight characteristic flat features in the excitation spectrum, which are connected to higher-order van Hove singularities in the magnon dispersion and are directly induced by easy-plane XXZ anisotropy. Our results provide a rare experimental insight into the nature of highly anisotropic S = 1/2 TLAFs between the Heisenberg and XY limits.","sentences":["The S = 1/2 triangular lattice antiferromagnet (TLAF) stands out as a paradigmatic example of frustrated quantum magnetism.","An ongoing challenge involves understanding the influence of exchange anisotropy on the collective behavior within such systems.","Using inelastic neutron scattering (INS) and advanced calculation techniques, we have studied the low and high temperature spin dynamics of Ba2La2CoTe2O12 (BLCTO): a Co2+-based Jeff = 1/2 TLAF that exhibits 120 deg order below TN = 3.26 K.","The spin Hamiltonian was determined by fitting the energy-resolved paramagnetic excitations measured at T > TN, revealing an exceptionally strong easy-plane XXZ exchange anisotropy.","Below TN, the excitation spectrum exhibits a high energy continuum having a larger spectral weight than the single-magnon modes.","Combined with advanced theoretical calculations of magnetic excitations based on magnons and spinons, this observation suggests a scenario characterized by a spinon confinement length that markedly exceeds the lattice spacing.","We conjecture that this phenomenon arises due to the proximity to a quantum melting point, which persists even in the presence of strong easy-plane XXZ anisotropy.","Finally, we highlight characteristic flat features in the excitation spectrum, which are connected to higher-order van Hove singularities in the magnon dispersion and are directly induced by easy-plane XXZ anisotropy.","Our results provide a rare experimental insight into the nature of highly anisotropic S = 1/2 TLAFs between the Heisenberg and XY limits."],"url":"http://arxiv.org/abs/2403.03210v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 18:46:50","title":"Active Statistical Inference","abstract":"Inspired by the concept of active learning, we propose active inference$\\unicode{x2013}$a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model's predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values. We evaluate active inference on datasets from public opinion research, census analysis, and proteomics.","sentences":["Inspired by the concept of active learning, we propose active inference$\\unicode{x2013}$a methodology for statistical inference with machine-learning-assisted data collection.","Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget.","It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model's predictions where it is confident.","Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution.","The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data.","This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values.","We evaluate active inference on datasets from public opinion research, census analysis, and proteomics."],"url":"http://arxiv.org/abs/2403.03208v1","category":"stat.ML"}
{"created":"2024-03-05 18:41:37","title":"CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments","abstract":"The integration of learning and reasoning is high on the research agenda in AI. Nevertheless, there is only a little attention to use existing background knowledge for reasoning about partially observed scenes to answer questions about the scene. Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones). Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment-specific. We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints. In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged to generate plausible answers to questions about a hidden object in a given partial scene. For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed. Through experiments, we observe that the low performance of pre-trained vision language models like CLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POC ascertains the necessity for frameworks that can handle reasoning-intensive tasks where environment-specific background knowledge is available and crucial. Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an LLM like GPT-4 with a visual perception network and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC.","sentences":["The integration of learning and reasoning is high on the research agenda in AI.","Nevertheless, there is only a little attention to use existing background knowledge for reasoning about partially observed scenes to answer questions about the scene.","Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones).","Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment-specific.","We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints.","In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged to generate plausible answers to questions about a hidden object in a given partial scene.","For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed.","Through experiments, we observe that the low performance of pre-trained vision language models like CLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POC ascertains the necessity for frameworks that can handle reasoning-intensive tasks where environment-specific background knowledge is available and crucial.","Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an LLM like GPT-4 with a visual perception network and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC."],"url":"http://arxiv.org/abs/2403.03203v1","category":"cs.AI"}
{"created":"2024-03-05 18:29:17","title":"Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract Reasoning process","abstract":"Abstract reasoning problems pose significant challenges to artificial intelligence algorithms, demanding cognitive capabilities beyond those required for perception tasks. This study introduces the Triple-CFN approach to tackle the Bongard-Logo problem, achieving notable reasoning accuracy by implicitly reorganizing the concept space of conflicting instances. Additionally, the Triple-CFN paradigm proves effective for the RPM problem with necessary modifications, yielding competitive results. To further enhance performance on the RPM issue, we develop the Meta Triple-CFN network, which explicitly structures the problem space while maintaining interpretability on progressive patterns. The success of Meta Triple-CFN is attributed to its paradigm of modeling the conceptual space, equivalent to normalizing reasoning information. Based on this ideology, we introduce the Re-space layer, enhancing the performance of both Meta Triple-CFN and Triple-CFN. This paper aims to contribute to advancements in machine intelligence by exploring innovative network designs for addressing abstract reasoning problems, paving the way for further breakthroughs in this domain.","sentences":["Abstract reasoning problems pose significant challenges to artificial intelligence algorithms, demanding cognitive capabilities beyond those required for perception tasks.","This study introduces the Triple-CFN approach to tackle the Bongard-Logo problem, achieving notable reasoning accuracy by implicitly reorganizing the concept space of conflicting instances.","Additionally, the Triple-CFN paradigm proves effective for the RPM problem with necessary modifications, yielding competitive results.","To further enhance performance on the RPM issue, we develop the Meta Triple-CFN network, which explicitly structures the problem space while maintaining interpretability on progressive patterns.","The success of Meta Triple-CFN is attributed to its paradigm of modeling the conceptual space, equivalent to normalizing reasoning information.","Based on this ideology, we introduce the Re-space layer, enhancing the performance of both Meta Triple-CFN and Triple-CFN.","This paper aims to contribute to advancements in machine intelligence by exploring innovative network designs for addressing abstract reasoning problems, paving the way for further breakthroughs in this domain."],"url":"http://arxiv.org/abs/2403.03190v2","category":"cs.CV"}
{"created":"2024-03-05 18:29:12","title":"On Projective Planes of Order 16 Associated with 1-rotational 2-(52, 4, 1) Designs","abstract":"A maximal arc of degree k in a finite projective plane P of order q = ks is a set of (q-s+1)k points that meets every line of P in either k or 0 points. The collection of the nonempty intersections of a maximal arc with the lines of P is a resolvable Steiner 2-((q-s+1)k, k, 1) design. Necessary and sufficient conditions for a resolvable Steiner 2- design to be embeddable as a maximal arc in a projective plane were proved recently in [8]. Steiner designs associated with maximal arcs in the known projective planes of order 16 were analyzed in [6], where it was shown that some of the associated designs are embeddable in two non-isomorphic planes. Using MAGMA, we conducted an analysis to ascertain whether any of the 22 non-isomorphic 1-rotational 2-(52,4,1) designs, previously classified in [3], could be embedded in maximal arcs of degree 4 within projective planes of order 16. This paper presents a summary of our findings, revealing that precisely only one out of the the twenty-two 1-rotational designs from [3] is embeddable in a plane of order 16, being the Desarguesian plane P G(2, 16).","sentences":["A maximal arc of degree k in a finite projective plane P of order q = ks is a set of (q-s+1)k points that meets every line of P in either k or 0 points.","The collection of the nonempty intersections of a maximal arc with the lines of P is a resolvable Steiner 2-((q-s+1)k, k, 1) design.","Necessary and sufficient conditions for a resolvable Steiner 2- design to be embeddable as a maximal arc in a projective plane were proved recently in [8].","Steiner designs associated with maximal arcs in the known projective planes of order 16 were analyzed in [6], where it was shown that some of the associated designs are embeddable in two non-isomorphic planes.","Using MAGMA, we conducted an analysis to ascertain whether any of the 22 non-isomorphic 1-rotational 2-(52,4,1) designs, previously classified in [3], could be embedded in maximal arcs of degree 4 within projective planes of order 16.","This paper presents a summary of our findings, revealing that precisely only one out of the the twenty-two 1-rotational designs from [3] is embeddable in a plane of order 16, being the Desarguesian plane P G(2, 16)."],"url":"http://arxiv.org/abs/2403.03189v1","category":"math.CO"}
{"created":"2024-03-05 18:24:52","title":"Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement","abstract":"Real-time flood forecasting plays a crucial role in enabling timely and effective emergency responses. However, a significant challenge lies in bridging the gap between complex numerical flood models and practical decision-making. Decision-makers often rely on experts to interpret these models for optimizing flood mitigation strategies. And the public requires complex techniques to inquiry and understand socio-cultural and institutional factors, often hinders the public's understanding of flood risks. To overcome these challenges, our study introduces an innovative solution: a customized AI Assistant powered by the GPT-4 Large Language Model. This AI Assistant is designed to facilitate effective communication between decision-makers, the general public, and flood forecasters, without the requirement of specialized knowledge. The new framework utilizes GPT-4's advanced natural language understanding and function calling capabilities to provide immediate flood alerts and respond to various flood-related inquiries. Our developed prototype integrates real-time flood warnings with flood maps and social vulnerability data. It also effectively translates complex flood zone information into actionable risk management advice. To assess its performance, we evaluated the prototype using six criteria within three main categories: relevance, error resilience, and understanding of context. Our research marks a significant step towards a more accessible and user-friendly approach in flood risk management. This study highlights the potential of advanced AI tools like GPT-4 in democratizing information and enhancing public engagement in critical social and environmental issues.","sentences":["Real-time flood forecasting plays a crucial role in enabling timely and effective emergency responses.","However, a significant challenge lies in bridging the gap between complex numerical flood models and practical decision-making.","Decision-makers often rely on experts to interpret these models for optimizing flood mitigation strategies.","And the public requires complex techniques to inquiry and understand socio-cultural and institutional factors, often hinders the public's understanding of flood risks.","To overcome these challenges, our study introduces an innovative solution: a customized AI Assistant powered by the GPT-4 Large Language Model.","This AI Assistant is designed to facilitate effective communication between decision-makers, the general public, and flood forecasters, without the requirement of specialized knowledge.","The new framework utilizes GPT-4's advanced natural language understanding and function calling capabilities to provide immediate flood alerts and respond to various flood-related inquiries.","Our developed prototype integrates real-time flood warnings with flood maps and social vulnerability data.","It also effectively translates complex flood zone information into actionable risk management advice.","To assess its performance, we evaluated the prototype using six criteria within three main categories: relevance, error resilience, and understanding of context.","Our research marks a significant step towards a more accessible and user-friendly approach in flood risk management.","This study highlights the potential of advanced AI tools like GPT-4 in democratizing information and enhancing public engagement in critical social and environmental issues."],"url":"http://arxiv.org/abs/2403.03188v1","category":"cs.AI"}
{"created":"2024-03-05 18:22:33","title":"Reliable, Adaptable, and Attributable Language Models with Retrieval","abstract":"Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.","sentences":["Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability.","However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability.","In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs.","By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable.","Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling.","To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs.","This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference."],"url":"http://arxiv.org/abs/2403.03187v1","category":"cs.CL"}
{"created":"2024-03-05 18:22:29","title":"Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study","abstract":"Recent studies have demonstrated the success of foundation agents in specific tasks or scenarios. However, existing agents cannot generalize across different scenarios, mainly due to their diverse observation and action spaces and semantic gaps, or reliance on task-specific resources. In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction. To target GCC, we propose Cradle, an agent framework with strong reasoning abilities, including self-reflection, task inference, and skill curation, to ensure generalizability and self-improvement across various tasks. To demonstrate the capabilities of Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as a preliminary attempt towards GCC with a challenging target. Our agent can follow the main storyline and finish real missions in this complex AAA game, with minimal reliance on prior knowledge and application-specific resources. The project website is at https://baai-agents.github.io/Cradle/.","sentences":["Recent studies have demonstrated the success of foundation agents in specific tasks or scenarios.","However, existing agents cannot generalize across different scenarios, mainly due to their diverse observation and action spaces and semantic gaps, or reliance on task-specific resources.","In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction.","To target GCC, we propose Cradle, an agent framework with strong reasoning abilities, including self-reflection, task inference, and skill curation, to ensure generalizability and self-improvement across various tasks.","To demonstrate the capabilities of Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as a preliminary attempt towards GCC with a challenging target.","Our agent can follow the main storyline and finish real missions in this complex AAA game, with minimal reliance on prior knowledge and application-specific resources.","The project website is at https://baai-agents.github.io/Cradle/."],"url":"http://arxiv.org/abs/2403.03186v1","category":"cs.AI"}
{"created":"2024-03-05 18:22:15","title":"Preventing Reward Hacking with Occupancy Measure Regularization","abstract":"Reward hacking occurs when an agent performs very well with respect to a \"proxy\" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a \"safe\" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM). Thus, we propose regularizing based on the OM divergence between policies instead of AD divergence to prevent reward hacking. We theoretically establish that OM regularization can more effectively avoid large drops in true reward. Then, we empirically demonstrate in a variety of realistic environments that OM divergence is superior to AD divergence for preventing reward hacking by regularizing towards a safe policy. Furthermore, we show that occupancy measure divergence can also regularize learned policies away from reward hacking behavior. Our code and data are available at https://github.com/cassidylaidlaw/orpo","sentences":["Reward hacking occurs when an agent performs very well with respect to a \"proxy\" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward.","Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively.","Prior work has particularly focused on enforcing the learned policy to behave similarly to a \"safe\" policy by penalizing the KL divergence between their action distributions (AD).","However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity.","Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM).","Thus, we propose regularizing based on the OM divergence between policies instead of AD divergence to prevent reward hacking.","We theoretically establish that OM regularization can more effectively avoid large drops in true reward.","Then, we empirically demonstrate in a variety of realistic environments that OM divergence is superior to AD divergence for preventing reward hacking by regularizing towards a safe policy.","Furthermore, we show that occupancy measure divergence can also regularize learned policies away from reward hacking behavior.","Our code and data are available at https://github.com/cassidylaidlaw/orpo"],"url":"http://arxiv.org/abs/2403.03185v1","category":"cs.LG"}
{"created":"2024-03-05 18:20:10","title":"How Well Can Transformers Emulate In-context Newton's Method?","abstract":"Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms. Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression. In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve $\\epsilon$ error with only a logarithmic to the error more layers. As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers. These results suggest the ability of the Transformer architecture to implement complex algorithms, beyond gradient descent.","sentences":["Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms.","Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression.","In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression.","We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve $\\epsilon$ error with only a logarithmic to the error more layers.","As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers.","These results suggest the ability of the Transformer architecture to implement complex algorithms, beyond gradient descent."],"url":"http://arxiv.org/abs/2403.03183v1","category":"cs.LG"}
{"created":"2024-03-05 18:19:29","title":"Behavior Generation with Latent Actions","abstract":"Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector quantization module. Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies. Importantly, we demonstrate VQ-BeT's improved ability to capture behavior modes while accelerating inference speed 5x over Diffusion Policies. Videos and code can be found https://sjlee.cc/vq-bet","sentences":["Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making.","Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction.","A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes.","However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions.","In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations.","VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector quantization module.","Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies.","Importantly, we demonstrate VQ-BeT's improved ability to capture behavior modes while accelerating inference speed 5x over Diffusion Policies.","Videos and code can be found https://sjlee.cc/vq-bet"],"url":"http://arxiv.org/abs/2403.03181v1","category":"cs.LG"}
{"created":"2024-03-05 18:13:18","title":"Unifying and Certifying Top-Quality Planning","abstract":"The growing utilization of planning tools in practical scenarios has sparked an interest in generating multiple high-quality plans. Consequently, a range of computational problems under the general umbrella of top-quality planning were introduced over a short time period, each with its own definition. In this work, we show that the existing definitions can be unified into one, based on a dominance relation. The different computational problems, therefore, simply correspond to different dominance relations. Given the unified definition, we can now certify the top-quality of the solutions, leveraging existing certification of unsolvability and optimality. We show that task transformations found in the existing literature can be employed for the efficient certification of various top-quality planning problems and propose a novel transformation to efficiently certify loopless top-quality planning.","sentences":["The growing utilization of planning tools in practical scenarios has sparked an interest in generating multiple high-quality plans.","Consequently, a range of computational problems under the general umbrella of top-quality planning were introduced over a short time period, each with its own definition.","In this work, we show that the existing definitions can be unified into one, based on a dominance relation.","The different computational problems, therefore, simply correspond to different dominance relations.","Given the unified definition, we can now certify the top-quality of the solutions, leveraging existing certification of unsolvability and optimality.","We show that task transformations found in the existing literature can be employed for the efficient certification of various top-quality planning problems and propose a novel transformation to efficiently certify loopless top-quality planning."],"url":"http://arxiv.org/abs/2403.03176v1","category":"cs.AI"}
{"created":"2024-03-05 18:08:45","title":"MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting","abstract":"Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from broad sources. To scaffold the VLM's reasoning in zero-shot, we propose a visual prompting technique that annotates marks on the images, converting the prediction of keypoints and waypoints into a series of visual question answering problems that are feasible for the VLM to solve. Using the robot experiences collected in this way, we further investigate ways to bootstrap the performance through in-context learning and policy distillation. We evaluate and analyze MOKA's performance on a variety of manipulation tasks specified by free-form language descriptions, such as tool use, deformable body manipulation, and object rearrangement.","sentences":["Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals.","While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question.","In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions.","At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world.","By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from broad sources.","To scaffold the VLM's reasoning in zero-shot, we propose a visual prompting technique that annotates marks on the images, converting the prediction of keypoints and waypoints into a series of visual question answering problems that are feasible for the VLM to solve.","Using the robot experiences collected in this way, we further investigate ways to bootstrap the performance through in-context learning and policy distillation.","We evaluate and analyze MOKA's performance on a variety of manipulation tasks specified by free-form language descriptions, such as tool use, deformable body manipulation, and object rearrangement."],"url":"http://arxiv.org/abs/2403.03174v1","category":"cs.RO"}
{"created":"2024-03-05 18:07:34","title":"Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination","abstract":"Reaching consensus is key to multi-agent coordination. To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward. However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem. In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal. The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states. We directly model this distribution with a self-supervised generative model, thus alleviating the \"curse of dimensinality\" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods. We show that such efficient consensus mechanism can guide all agents cooperatively reaching valuable future states. Results on Multi-agent Particle-Environments and Google Research Football environment demonstrate the superiority of MAGI in both sample efficiency and performance.","sentences":["Reaching consensus is key to multi-agent coordination.","To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward.","However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem.","In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents.","The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal.","The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states.","We directly model this distribution with a self-supervised generative model, thus alleviating the \"curse of dimensinality\" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods.","We show that such efficient consensus mechanism can guide all agents cooperatively reaching valuable future states.","Results on Multi-agent Particle-Environments and Google Research Football environment demonstrate the superiority of MAGI in both sample efficiency and performance."],"url":"http://arxiv.org/abs/2403.03172v1","category":"cs.AI"}
{"created":"2024-03-05 18:04:59","title":"SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection","abstract":"Misinformation is a prevalent societal issue due to its potential high risks. Out-of-context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model's concept alignment of generic objects with news-domain entities and the second stage leverages language-only GPT-4 generated OOC-specific instruction data to fine-tune the model's discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification. Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy. SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations.","sentences":["Misinformation is a prevalent societal issue due to its potential high risks.","Out-of-context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences.","Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation.","While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences.","In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation.","SNIFFER employs two-stage instruction tuning on InstructBLIP.","The first stage refines the model's concept alignment of generic objects with news-domain entities and the second stage leverages language-only GPT-4 generated OOC-specific instruction data to fine-tune the model's discriminatory powers.","Enhanced by external tools and retrieval, SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification.","Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy.","SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations."],"url":"http://arxiv.org/abs/2403.03170v1","category":"cs.MM"}
{"created":"2024-03-05 18:03:51","title":"Learning Explicitly Conditioned Sparsifying Transforms","abstract":"Sparsifying transforms became in the last decades widely known tools for finding structured sparse representations of signals in certain transform domains. Despite the popularity of classical transforms such as DCT and Wavelet, learning optimal transforms that guarantee good representations of data into the sparse domain has been recently analyzed in a series of papers. Typically, the conditioning number and representation ability are complementary key features of learning square transforms that may not be explicitly controlled in a given optimization model. Unlike the existing approaches from the literature, in our paper, we consider a new sparsifying transform model that enforces explicit control over the data representation quality and the condition number of the learned transforms. We confirm through numerical experiments that our model presents better numerical behavior than the state-of-the-art.","sentences":["Sparsifying transforms became in the last decades widely known tools for finding structured sparse representations of signals in certain transform domains.","Despite the popularity of classical transforms such as DCT and Wavelet, learning optimal transforms that guarantee good representations of data into the sparse domain has been recently analyzed in a series of papers.","Typically, the conditioning number and representation ability are complementary key features of learning square transforms that may not be explicitly controlled in a given optimization model.","Unlike the existing approaches from the literature, in our paper, we consider a new sparsifying transform model that enforces explicit control over the data representation quality and the condition number of the learned transforms.","We confirm through numerical experiments that our model presents better numerical behavior than the state-of-the-art."],"url":"http://arxiv.org/abs/2403.03168v1","category":"math.NA"}
{"created":"2024-03-05 17:58:26","title":"Leveraging Federated Learning and Edge Computing for Recommendation Systems within Cloud Computing Networks","abstract":"To enable large-scale and efficient deployment of artificial intelligence (AI), the combination of AI and edge computing has spawned Edge Intelligence, which leverages the computing and communication capabilities of end devices and edge servers to process data closer to where it is generated. A key technology for edge intelligence is the privacy-protecting machine learning paradigm known as Federated Learning (FL), which enables data owners to train models without having to transfer raw data to third-party servers. However, FL networks are expected to involve thousands of heterogeneous distributed devices. As a result, communication efficiency remains a key bottleneck. To reduce node failures and device exits, a Hierarchical Federated Learning (HFL) framework is proposed, where a designated cluster leader supports the data owner through intermediate model aggregation. Therefore, based on the improvement of edge server resource utilization, this paper can effectively make up for the limitation of cache capacity. In order to mitigate the impact of soft clicks on the quality of user experience (QoE), the authors model the user QoE as a comprehensive system cost. To solve the formulaic problem, the authors propose a decentralized caching algorithm with federated deep reinforcement learning (DRL) and federated learning (FL), where multiple agents learn and make decisions independently","sentences":["To enable large-scale and efficient deployment of artificial intelligence (AI), the combination of AI and edge computing has spawned Edge Intelligence, which leverages the computing and communication capabilities of end devices and edge servers to process data closer to where it is generated.","A key technology for edge intelligence is the privacy-protecting machine learning paradigm known as Federated Learning (FL), which enables data owners to train models without having to transfer raw data to third-party servers.","However, FL networks are expected to involve thousands of heterogeneous distributed devices.","As a result, communication efficiency remains a key bottleneck.","To reduce node failures and device exits, a Hierarchical Federated Learning (HFL) framework is proposed, where a designated cluster leader supports the data owner through intermediate model aggregation.","Therefore, based on the improvement of edge server resource utilization, this paper can effectively make up for the limitation of cache capacity.","In order to mitigate the impact of soft clicks on the quality of user experience (QoE), the authors model the user QoE as a comprehensive system cost.","To solve the formulaic problem, the authors propose a decentralized caching algorithm with federated deep reinforcement learning (DRL) and federated learning (FL), where multiple agents learn and make decisions independently"],"url":"http://arxiv.org/abs/2403.03165v1","category":"cs.AI"}
{"created":"2024-03-05 17:54:03","title":"Soliton Frequency Combs in Elastomer Membrane-Cavity Optomechanics","abstract":"Solitons, arising from nonlinear wave-matter interactions, stand out for their intrinsic stability during wave propagation and exceptional spectral characteristics1. Their applications span diverse physical systems, including telecommunications, atomic clocks, and precise measurements. In recent years, significant strides have been made in developing cavity-optomechanics based approaches to generate optical frequency combs (FCs). In this study, we present an innovative approach, never explored before, that leverages elastomer membrane (EM)-cavity optomechanics to achieve the generation of soliton FCs, a highly sought-after phenomenon in the realm of nonlinear wave-matter interactions. Our method represents a significant breakthrough due to its streamlined simplicity, relying on a single continuous-wave (CW) laser pump and an externally applied acoustic wave exciting an EM-cavity, which gives rise to phonons, quantized vibrational energy states intrinsic to the elastomer's crystalline lattice structure. The mechanical resonator and electromagnetic cavity resonance are parametrically coupled within the microwave frequency range, collectively orchestrate the process of soliton FCs formation with remarkable efficiency. Numerical simulations and experimental observations demonstrate the emergence of multiple stable localized opto-mechanical wave packets, characterized by a narrow pulses time-domain response. Crucially, by setting the acoustic wave frequency to match the natural frequency of the EM resonator, the solitons' teeth are precisely spaced, and the EM's motion is significantly amplified, giving rise to a Kerr medium. The successful realization of optomechanical stable solitons represents a monumental advancement with transformative potential across various fields, including quantum computing and spectroscopy.","sentences":["Solitons, arising from nonlinear wave-matter interactions, stand out for their intrinsic stability during wave propagation and exceptional spectral characteristics1.","Their applications span diverse physical systems, including telecommunications, atomic clocks, and precise measurements.","In recent years, significant strides have been made in developing cavity-optomechanics based approaches to generate optical frequency combs (FCs).","In this study, we present an innovative approach, never explored before, that leverages elastomer membrane (EM)-cavity optomechanics to achieve the generation of soliton FCs, a highly sought-after phenomenon in the realm of nonlinear wave-matter interactions.","Our method represents a significant breakthrough due to its streamlined simplicity, relying on a single continuous-wave (CW) laser pump and an externally applied acoustic wave exciting an EM-cavity, which gives rise to phonons, quantized vibrational energy states intrinsic to the elastomer's crystalline lattice structure.","The mechanical resonator and electromagnetic cavity resonance are parametrically coupled within the microwave frequency range, collectively orchestrate the process of soliton FCs formation with remarkable efficiency.","Numerical simulations and experimental observations demonstrate the emergence of multiple stable localized opto-mechanical wave packets, characterized by a narrow pulses time-domain response.","Crucially, by setting the acoustic wave frequency to match the natural frequency of the EM resonator, the solitons' teeth are precisely spaced, and the EM's motion is significantly amplified, giving rise to a Kerr medium.","The successful realization of optomechanical stable solitons represents a monumental advancement with transformative potential across various fields, including quantum computing and spectroscopy."],"url":"http://arxiv.org/abs/2403.03160v1","category":"physics.optics"}
{"created":"2024-03-05 17:47:22","title":"Quantum Many-Body Physics Calculations with Large Language Models","abstract":"Large language models (LLMs) have demonstrated an unprecedented ability to perform complex tasks in multiple domains, including mathematical and scientific reasoning. We demonstrate that with carefully designed prompts, LLMs can accurately carry out key calculations in research papers in theoretical physics. We focus on a broadly used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations. To carry out the calculations using LLMs, we design multi-step prompt templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information. We evaluate GPT-4's performance in executing the calculation for 15 research papers from the past decade, demonstrating that, with correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases. Aggregating across all research papers, we find an average score of 87.5 (out of 100) on the execution of individual calculation steps. Overall, the requisite skill for doing these calculations is at the graduate level in quantum condensed matter theory. We further use LLMs to mitigate the two primary bottlenecks in this evaluation process: (i) extracting information from papers to fill in templates and (ii) automatic scoring of the calculation steps, demonstrating good results in both cases. The strong performance is the first step for developing algorithms that automatically explore theoretical hypotheses at an unprecedented scale.","sentences":["Large language models (LLMs) have demonstrated an unprecedented ability to perform complex tasks in multiple domains, including mathematical and scientific reasoning.","We demonstrate that with carefully designed prompts, LLMs can accurately carry out key calculations in research papers in theoretical physics.","We focus on a broadly used approximation method in quantum physics: the Hartree-Fock method, requiring an analytic multi-step calculation deriving approximate Hamiltonian and corresponding self-consistency equations.","To carry out the calculations using LLMs, we design multi-step prompt templates that break down the analytic calculation into standardized steps with placeholders for problem-specific information.","We evaluate GPT-4's performance in executing the calculation for 15 research papers from the past decade, demonstrating that, with correction of intermediate steps, it can correctly derive the final Hartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases.","Aggregating across all research papers, we find an average score of 87.5 (out of 100) on the execution of individual calculation steps.","Overall, the requisite skill for doing these calculations is at the graduate level in quantum condensed matter theory.","We further use LLMs to mitigate the two primary bottlenecks in this evaluation process: (i) extracting information from papers to fill in templates and (ii) automatic scoring of the calculation steps, demonstrating good results in both cases.","The strong performance is the first step for developing algorithms that automatically explore theoretical hypotheses at an unprecedented scale."],"url":"http://arxiv.org/abs/2403.03154v1","category":"physics.comp-ph"}
{"created":"2024-03-05 17:42:39","title":"Deep-Learned Compression for Radio-Frequency Signal Classification","abstract":"Next-generation cellular concepts rely on the processing of large quantities of radio-frequency (RF) samples. This includes Radio Access Networks (RAN) connecting the cellular front-end based on software defined radios (SDRs) and a framework for the AI processing of spectrum-related data. The RF data collected by the dense RAN radio units and spectrum sensors may need to be jointly processed for intelligent decision making. Moving large amounts of data to AI agents may result in significant bandwidth and latency costs. We propose a deep learned compression (DLC) model, HQARF, based on learned vector quantization (VQ), to compress the complex-valued samples of RF signals comprised of 6 modulation classes. We are assessing the effects of HQARF on the performance of an AI model trained to infer the modulation class of the RF signal. Compression of narrow-band RF samples for the training and off-the-site inference will allow for an efficient use of the bandwidth and storage for non-real-time analytics, and for a decreased delay in real-time applications. While exploring the effectiveness of the HQARF signal reconstructions in modulation classification tasks, we highlight the DLC optimization space and some open problems related to the training of the VQ embedded in HQARF.","sentences":["Next-generation cellular concepts rely on the processing of large quantities of radio-frequency (RF) samples.","This includes Radio Access Networks (RAN) connecting the cellular front-end based on software defined radios (SDRs) and a framework for the AI processing of spectrum-related data.","The RF data collected by the dense RAN radio units and spectrum sensors may need to be jointly processed for intelligent decision making.","Moving large amounts of data to AI agents may result in significant bandwidth and latency costs.","We propose a deep learned compression (DLC) model, HQARF, based on learned vector quantization (VQ), to compress the complex-valued samples of RF signals comprised of 6 modulation classes.","We are assessing the effects of HQARF on the performance of an AI model trained to infer the modulation class of the RF signal.","Compression of narrow-band RF samples for the training and off-the-site inference will allow for an efficient use of the bandwidth and storage for non-real-time analytics, and for a decreased delay in real-time applications.","While exploring the effectiveness of the HQARF signal reconstructions in modulation classification tasks, we highlight the DLC optimization space and some open problems related to the training of the VQ embedded in HQARF."],"url":"http://arxiv.org/abs/2403.03150v1","category":"cs.LG"}
{"created":"2024-03-05 17:21:31","title":"Simplicity in Complexity","abstract":"The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation. Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite \\textit{complex}. There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise. On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem. Here we propose to model complexity using segment-based representations of images. We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively. We find that complexity is well-explained by a simple linear model with these two features across six diverse image-sets of naturalistic scene and art images. This suggests that the complexity of images can be surprisingly simple.","sentences":["The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation.","Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite \\textit{complex}.","There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise.","On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem.","Here we propose to model complexity using segment-based representations of images.","We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively.","We find that complexity is well-explained by a simple linear model with these two features across six diverse image-sets of naturalistic scene and art images.","This suggests that the complexity of images can be surprisingly simple."],"url":"http://arxiv.org/abs/2403.03134v1","category":"cs.CV"}
{"created":"2024-03-05 17:16:37","title":"Jovian sodium nebula and Io plasma torus S$^+$ and brightnesses 2017 -- 2023: insights into volcanic vs. sublimation supply","abstract":"We present first results derived from the largest collection of contemporaneously recorded Jovian sodium nebula and Io plasma torus (IPT) in [S II] 673.1 nm images assembled to date. The data were recorded by the Planetary Science Institute's Io Input/Output observatory (IoIO) and provide important context to Io geologic and atmospheric studies as well as the Juno mission and supporting observations. Enhancements in the observed emission are common, typically lasting 1 -- 3 months, such that the average flux of material from Io is determined by the enhancements, not any quiescent state. The enhancements are not seen at periodicities associated with modulation in solar insolation of Io's surface, thus physical process(es) other than insolation-driven sublimation must ultimately drive the bulk of Io's atmospheric escape. We suggest that geologic activity, likely involving volcanic plumes, drives escape.","sentences":["We present first results derived from the largest collection of contemporaneously recorded Jovian sodium nebula and Io plasma torus (IPT) in [S II] 673.1 nm images assembled to date.","The data were recorded by the Planetary Science Institute's Io Input/Output observatory (IoIO) and provide important context to Io geologic and atmospheric studies as well as the Juno mission and supporting observations.","Enhancements in the observed emission are common, typically lasting 1 -- 3 months, such that the average flux of material from Io is determined by the enhancements, not any quiescent state.","The enhancements are not seen at periodicities associated with modulation in solar insolation of Io's surface, thus physical process(es) other than insolation-driven sublimation must ultimately drive the bulk of Io's atmospheric escape.","We suggest that geologic activity, likely involving volcanic plumes, drives escape."],"url":"http://arxiv.org/abs/2403.03131v1","category":"astro-ph.EP"}
{"created":"2024-03-05 17:01:17","title":"Motion-Corrected Moving Average: Including Post-Hoc Temporal Information for Improved Video Segmentation","abstract":"Real-time computational speed and a high degree of precision are requirements for computer-assisted interventions. Applying a segmentation network to a medical video processing task can introduce significant inter-frame prediction noise. Existing approaches can reduce inconsistencies by including temporal information but often impose requirements on the architecture or dataset. This paper proposes a method to include temporal information in any segmentation model and, thus, a technique to improve video segmentation performance without alterations during training or additional labeling. With Motion-Corrected Moving Average, we refine the exponential moving average between the current and previous predictions. Using optical flow to estimate the movement between consecutive frames, we can shift the prior term in the moving-average calculation to align with the geometry of the current frame. The optical flow calculation does not require the output of the model and can therefore be performed in parallel, leading to no significant runtime penalty for our approach. We evaluate our approach on two publicly available segmentation datasets and two proprietary endoscopic datasets and show improvements over a baseline approach.","sentences":["Real-time computational speed and a high degree of precision are requirements for computer-assisted interventions.","Applying a segmentation network to a medical video processing task can introduce significant inter-frame prediction noise.","Existing approaches can reduce inconsistencies by including temporal information but often impose requirements on the architecture or dataset.","This paper proposes a method to include temporal information in any segmentation model and, thus, a technique to improve video segmentation performance without alterations during training or additional labeling.","With Motion-Corrected Moving Average, we refine the exponential moving average between the current and previous predictions.","Using optical flow to estimate the movement between consecutive frames, we can shift the prior term in the moving-average calculation to align with the geometry of the current frame.","The optical flow calculation does not require the output of the model and can therefore be performed in parallel, leading to no significant runtime penalty for our approach.","We evaluate our approach on two publicly available segmentation datasets and two proprietary endoscopic datasets and show improvements over a baseline approach."],"url":"http://arxiv.org/abs/2403.03120v1","category":"cs.CV"}
{"created":"2024-03-05 16:56:09","title":"Equilibria in Two-Stage Facility Location with Atomic Clients","abstract":"We consider competitive facility location as a two-stage multi-agent system with two types of clients. For a given host graph with weighted clients on the vertices, first facility agents strategically select vertices for opening their facilities. Then, the clients strategically select which of the opened facilities in their neighborhood to patronize. Facilities want to attract as much client weight as possible, clients want to minimize congestion on the chosen facility.   All recently studied versions of this model assume that clients can split their weight strategically. We consider clients with unsplittable weights, but allow mixed strategies. So clients may randomize over which facility to patronize. Besides modeling a natural client behavior, this subtle change yields drastic changes, e.g., for a given facility placement, qualitatively different client equilibria are possible.   As our main result, we show that pure subgame perfect equilibria always exist if all client weights are identical. For this, we use a novel potential function argument, employing a hierarchical classification of the clients and sophisticated rounding in each step. In contrast, for non-identical clients, we show that deciding the existence of even approximately stable states is computationally intractable. On the positive side, we give a tight bound of 2 on the price of anarchy which implies high social welfare of equilibria, if they exist.","sentences":["We consider competitive facility location as a two-stage multi-agent system with two types of clients.","For a given host graph with weighted clients on the vertices, first facility agents strategically select vertices for opening their facilities.","Then, the clients strategically select which of the opened facilities in their neighborhood to patronize.","Facilities want to attract as much client weight as possible, clients want to minimize congestion on the chosen facility.   ","All recently studied versions of this model assume that clients can split their weight strategically.","We consider clients with unsplittable weights, but allow mixed strategies.","So clients may randomize over which facility to patronize.","Besides modeling a natural client behavior, this subtle change yields drastic changes, e.g., for a given facility placement, qualitatively different client equilibria are possible.   ","As our main result, we show that pure subgame perfect equilibria always exist if all client weights are identical.","For this, we use a novel potential function argument, employing a hierarchical classification of the clients and sophisticated rounding in each step.","In contrast, for non-identical clients, we show that deciding the existence of even approximately stable states is computationally intractable.","On the positive side, we give a tight bound of 2 on the price of anarchy which implies high social welfare of equilibria, if they exist."],"url":"http://arxiv.org/abs/2403.03114v1","category":"cs.GT"}
{"created":"2024-03-05 16:53:24","title":"Improved LiDAR Odometry and Mapping using Deep Semantic Segmentation and Novel Outliers Detection","abstract":"Perception is a key element for enabling intelligent autonomous navigation. Understanding the semantics of the surrounding environment and accurate vehicle pose estimation are essential capabilities for autonomous vehicles, including self-driving cars and mobile robots that perform complex tasks. Fast moving platforms like self-driving cars impose a hard challenge for localization and mapping algorithms. In this work, we propose a novel framework for real-time LiDAR odometry and mapping based on LOAM architecture for fast moving platforms. Our framework utilizes semantic information produced by a deep learning model to improve point-to-line and point-to-plane matching between LiDAR scans and build a semantic map of the environment, leading to more accurate motion estimation using LiDAR data. We observe that including semantic information in the matching process introduces a new type of outlier matches to the process, where matching occur between different objects of the same semantic class. To this end, we propose a novel algorithm that explicitly identifies and discards potential outliers in the matching process. In our experiments, we study the effect of improving the matching process on the robustness of LiDAR odometry against high speed motion. Our experimental evaluations on KITTI dataset demonstrate that utilizing semantic information and rejecting outliers significantly enhance the robustness of LiDAR odometry and mapping when there are large gaps between scan acquisition poses, which is typical for fast moving platforms.","sentences":["Perception is a key element for enabling intelligent autonomous navigation.","Understanding the semantics of the surrounding environment and accurate vehicle pose estimation are essential capabilities for autonomous vehicles, including self-driving cars and mobile robots that perform complex tasks.","Fast moving platforms like self-driving cars impose a hard challenge for localization and mapping algorithms.","In this work, we propose a novel framework for real-time LiDAR odometry and mapping based on LOAM architecture for fast moving platforms.","Our framework utilizes semantic information produced by a deep learning model to improve point-to-line and point-to-plane matching between LiDAR scans and build a semantic map of the environment, leading to more accurate motion estimation using LiDAR data.","We observe that including semantic information in the matching process introduces a new type of outlier matches to the process, where matching occur between different objects of the same semantic class.","To this end, we propose a novel algorithm that explicitly identifies and discards potential outliers in the matching process.","In our experiments, we study the effect of improving the matching process on the robustness of LiDAR odometry against high speed motion.","Our experimental evaluations on KITTI dataset demonstrate that utilizing semantic information and rejecting outliers significantly enhance the robustness of LiDAR odometry and mapping when there are large gaps between scan acquisition poses, which is typical for fast moving platforms."],"url":"http://arxiv.org/abs/2403.03111v1","category":"cs.CV"}
{"created":"2024-03-05 16:52:03","title":"Grid-constrained online scheduling of flexible electric vehicle charging","abstract":"We study Electric Vehicle (EV) charging from a scheduling perspective, aiming to minimize delays while respecting the grid constraints. A network of parking lots is considered, each with a given number of charging stations for electric vehicles. Some of the parking lots have a roof with solar panels. The demand that can be served at each parking lot is limited by the capacity of the cables connecting them to the grid. We assume that EVs arrive at the parking lots according to a known distribution. Upon arrival, we learn the desired departure time, the amount of electrical energy it needs to charge its battery, and the range of rates that it can be charged at. Vehicle arrival patterns, connection times, and charging volume are based on data collected in the city of Utrecht. The departure time of an EV is delayed if it has not finished charging in time for its desired departure. We aim to minimize the total delay. We present a novel approach, based on an online variant of well-known schedule generation schemes. We extend these schemes and include them in a destroy-and-repair heuristic. This resulted in several scheduling strategies. We show their effectiveness using a discrete event simulation. With this, we show that applying scheduling approaches increases the amount of EVs that can be charged at a site and reduces the average delay. Furthermore, we argue the importance of considering aspects of the grid layout in electricity networks and show the benefits of using flexible charging rates.","sentences":["We study Electric Vehicle (EV) charging from a scheduling perspective, aiming to minimize delays while respecting the grid constraints.","A network of parking lots is considered, each with a given number of charging stations for electric vehicles.","Some of the parking lots have a roof with solar panels.","The demand that can be served at each parking lot is limited by the capacity of the cables connecting them to the grid.","We assume that EVs arrive at the parking lots according to a known distribution.","Upon arrival, we learn the desired departure time, the amount of electrical energy it needs to charge its battery, and the range of rates that it can be charged at.","Vehicle arrival patterns, connection times, and charging volume are based on data collected in the city of Utrecht.","The departure time of an EV is delayed if it has not finished charging in time for its desired departure.","We aim to minimize the total delay.","We present a novel approach, based on an online variant of well-known schedule generation schemes.","We extend these schemes and include them in a destroy-and-repair heuristic.","This resulted in several scheduling strategies.","We show their effectiveness using a discrete event simulation.","With this, we show that applying scheduling approaches increases the amount of EVs that can be charged at a site and reduces the average delay.","Furthermore, we argue the importance of considering aspects of the grid layout in electricity networks and show the benefits of using flexible charging rates."],"url":"http://arxiv.org/abs/2403.03109v1","category":"math.OC"}
{"created":"2024-03-05 16:51:02","title":"On-demand Mobility Services for Urban Resilience: A Review Towards Human-Machine Collaborative Future","abstract":"Mobility-on-demand (MOD) services have the potential to significantly improve the adaptiveness and recovery of urban logistics and transportation infrastructure, in the wake of disruptive events. This paper presents a survey on the usage of MOD services for resilience improvement (MOD-R) and finds a noticeable increase within recent years on this topic across four main areas: resilient MOD services, novel usage of MOD-R services for improving supply chain resilience, empirical impact evaluation, and supporting technologies. MOD-R services have been utilized for anomaly detection, essential supply delivery, evacuation and rescue, on-site medical care, power grid stabilization, transit service substitution during downtime, and infrastructure and equipment repair. The review reveals integrating electrification, automation, and advanced communication technologies offers significant synergistic benefits. The review also suggests the importance of harnessing the collective capabilities of humans and intelligent machines to effectively implement versatile, multi-functional MOD-R services during crises.","sentences":["Mobility-on-demand (MOD) services have the potential to significantly improve the adaptiveness and recovery of urban logistics and transportation infrastructure, in the wake of disruptive events.","This paper presents a survey on the usage of MOD services for resilience improvement (MOD-R) and finds a noticeable increase within recent years on this topic across four main areas: resilient MOD services, novel usage of MOD-R services for improving supply chain resilience, empirical impact evaluation, and supporting technologies.","MOD-R services have been utilized for anomaly detection, essential supply delivery, evacuation and rescue, on-site medical care, power grid stabilization, transit service substitution during downtime, and infrastructure and equipment repair.","The review reveals integrating electrification, automation, and advanced communication technologies offers significant synergistic benefits.","The review also suggests the importance of harnessing the collective capabilities of humans and intelligent machines to effectively implement versatile, multi-functional MOD-R services during crises."],"url":"http://arxiv.org/abs/2403.03107v1","category":"cs.CY"}
{"created":"2024-03-05 16:46:18","title":"Biomechanical Comparison of Human Walking Locomotion on Solid Ground and Sand","abstract":"Current studies on human locomotion focus mainly on solid ground walking conditions. In this paper, we present a biomechanic comparison of human walking locomotion on solid ground and sand. A novel dataset containing 3-dimensional motion and biomechanical data from 20 able-bodied adults for locomotion on solid ground and sand is collected. We present the data collection methods and report the sensor data along with the kinematic and kinetic profiles of joint biomechanics. A comprehensive analysis of human gait and joint stiffness profiles is presented. The kinematic and kinetic analysis reveals that human walking locomotion on sand shows different ground reaction forces and joint torque profiles, compared with those patterns from walking on solid ground. These gait differences reflect that humans adopt motion control strategies for yielding terrain conditions such as sand. The dataset also provides a source of locomotion data for researchers to study human activity recognition and assistive devices for walking on different terrains.","sentences":["Current studies on human locomotion focus mainly on solid ground walking conditions.","In this paper, we present a biomechanic comparison of human walking locomotion on solid ground and sand.","A novel dataset containing 3-dimensional motion and biomechanical data from 20 able-bodied adults for locomotion on solid ground and sand is collected.","We present the data collection methods and report the sensor data along with the kinematic and kinetic profiles of joint biomechanics.","A comprehensive analysis of human gait and joint stiffness profiles is presented.","The kinematic and kinetic analysis reveals that human walking locomotion on sand shows different ground reaction forces and joint torque profiles, compared with those patterns from walking on solid ground.","These gait differences reflect that humans adopt motion control strategies for yielding terrain conditions such as sand.","The dataset also provides a source of locomotion data for researchers to study human activity recognition and assistive devices for walking on different terrains."],"url":"http://arxiv.org/abs/2403.03105v2","category":"cs.RO"}
{"created":"2024-03-05 16:43:25","title":"Emergent Equivariance in Deep Ensembles","abstract":"We demonstrate that deep ensembles are secretly equivariant models. More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments.","sentences":["We demonstrate that deep ensembles are secretly equivariant models.","More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation.","Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit.","The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is.","Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments."],"url":"http://arxiv.org/abs/2403.03103v1","category":"cs.LG"}
{"created":"2024-03-05 16:43:03","title":"\"In Dialogues We Learn\": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning","abstract":"Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.","sentences":["Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas.","However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility.","We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles.","Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively.","Additionally, the results of human evaluations further validate the efficacy of our proposed method."],"url":"http://arxiv.org/abs/2403.03102v1","category":"cs.CL"}
{"created":"2024-03-05 16:39:12","title":"KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents","abstract":"Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation. Code is available in https://github.com/zjunlp/KnowAgent.","sentences":["Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions.","This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination.","To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge.","Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents.","Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines.","Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation.","Code is available in https://github.com/zjunlp/KnowAgent."],"url":"http://arxiv.org/abs/2403.03101v1","category":"cs.CL"}
{"created":"2024-03-05 16:35:25","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models","abstract":"While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model the intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.","sentences":["While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody.","Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually.","Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way.","Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt.","With this factorization design, NaturalSpeech 3 can effectively and efficiently model the intricate speech with disentangled subspaces in a divide-and-conquer way.","Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility.","Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data."],"url":"http://arxiv.org/abs/2403.03100v1","category":"eess.AS"}
{"created":"2024-03-05 16:35:11","title":"Data Nuggets: A Method for Reducing Big Data While Preserving Data Structure","abstract":"Big data, with NxP dimension where N is extremely large, has created new challenges for data analysis, particularly in the realm of creating meaningful clusters of data. Clustering techniques, such as K-means or hierarchical clustering are popular methods for performing exploratory analysis on large datasets. Unfortunately, these methods are not always possible to apply to big data due to memory or time constraints generated by calculations of order PxN(N-1). To circumvent this problem, typically, the clustering technique is applied to a random sample drawn from the dataset: however, a weakness is that the structure of the dataset, particularly at the edges, is not necessarily maintained. We propose a new solution through the concept of \"data nuggets\", which reduce a large dataset into a small collection of nuggets of data, each containing a center, weight, and scale parameter. The data nuggets are then input into algorithms that compute methods such as principal components analysis and clustering in a more computationally efficient manner. We show the consistency of the data nuggets-based covariance estimator and apply the methodology of data nuggets to perform exploratory analysis of a flow cytometry dataset containing over one million observations using PCA and K-means clustering for weighted observations. Supplementary materials for this article are available online.","sentences":["Big data, with NxP dimension where N is extremely large, has created new challenges for data analysis, particularly in the realm of creating meaningful clusters of data.","Clustering techniques, such as K-means or hierarchical clustering are popular methods for performing exploratory analysis on large datasets.","Unfortunately, these methods are not always possible to apply to big data due to memory or time constraints generated by calculations of order PxN(N-1).","To circumvent this problem, typically, the clustering technique is applied to a random sample drawn from the dataset: however, a weakness is that the structure of the dataset, particularly at the edges, is not necessarily maintained.","We propose a new solution through the concept of \"data nuggets\", which reduce a large dataset into a small collection of nuggets of data, each containing a center, weight, and scale parameter.","The data nuggets are then input into algorithms that compute methods such as principal components analysis and clustering in a more computationally efficient manner.","We show the consistency of the data nuggets-based covariance estimator and apply the methodology of data nuggets to perform exploratory analysis of a flow cytometry dataset containing over one million observations using PCA and K-means clustering for weighted observations.","Supplementary materials for this article are available online."],"url":"http://arxiv.org/abs/2403.03099v1","category":"stat.ME"}
{"created":"2024-03-05 16:25:05","title":"Collective self-caging of active filaments in virtual confinement","abstract":"Motility coupled to responsive behavior is essential for many microorganisms to seek and establish appropriate habitats. One of the simplest possible responses, reversing the direction of motion, is believed to enable filamentous cyanobacteria to form stable aggregates or accumulate in suitable light conditions. Here, we demonstrate that filamentous morphology in combination with responding to light gradients by reversals has consequences far beyond simple accumulation: Entangled aggregates form at the boundaries of illuminated regions, harnessing the boundary to establish local order. We explore how the light pattern, in particular its boundary curvature, impacts aggregation. A minimal mechanistic model of active flexible filaments resembles the experimental findings, thereby revealing the emergent and generic character of these structures. This phenomenon may enable elongated microorganisms to generate adaptive colony architectures in limited habitats, or guide the assembly of biomimetic fibrous materials.","sentences":["Motility coupled to responsive behavior is essential for many microorganisms to seek and establish appropriate habitats.","One of the simplest possible responses, reversing the direction of motion, is believed to enable filamentous cyanobacteria to form stable aggregates or accumulate in suitable light conditions.","Here, we demonstrate that filamentous morphology in combination with responding to light gradients by reversals has consequences far beyond simple accumulation: Entangled aggregates form at the boundaries of illuminated regions, harnessing the boundary to establish local order.","We explore how the light pattern, in particular its boundary curvature, impacts aggregation.","A minimal mechanistic model of active flexible filaments resembles the experimental findings, thereby revealing the emergent and generic character of these structures.","This phenomenon may enable elongated microorganisms to generate adaptive colony architectures in limited habitats, or guide the assembly of biomimetic fibrous materials."],"url":"http://arxiv.org/abs/2403.03093v1","category":"physics.bio-ph"}
{"created":"2024-03-05 16:21:53","title":"VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism","abstract":"The pursuit of optimizing cancer therapies is significantly advanced by the accurate prediction of drug synergy. Traditional methods, such as clinical trials, are reliable yet encumbered by extensive time and financial demands. The emergence of high-throughput screening and computational innovations has heralded a shift towards more efficient methodologies for exploring drug interactions. In this study, we present VQSynergy, a novel framework that employs the Vector Quantization (VQ) mechanism, integrated with gated residuals and a tailored attention mechanism, to enhance the precision and generalizability of drug synergy predictions. Our findings demonstrate that VQSynergy surpasses existing models in terms of robustness, particularly under Gaussian noise conditions, highlighting its superior performance and utility in the complex and often noisy domain of drug synergy research. This study underscores the potential of VQSynergy in revolutionizing the field through its advanced predictive capabilities, thereby contributing to the optimization of cancer treatment strategies.","sentences":["The pursuit of optimizing cancer therapies is significantly advanced by the accurate prediction of drug synergy.","Traditional methods, such as clinical trials, are reliable yet encumbered by extensive time and financial demands.","The emergence of high-throughput screening and computational innovations has heralded a shift towards more efficient methodologies for exploring drug interactions.","In this study, we present VQSynergy, a novel framework that employs the Vector Quantization (VQ) mechanism, integrated with gated residuals and a tailored attention mechanism, to enhance the precision and generalizability of drug synergy predictions.","Our findings demonstrate that VQSynergy surpasses existing models in terms of robustness, particularly under Gaussian noise conditions, highlighting its superior performance and utility in the complex and often noisy domain of drug synergy research.","This study underscores the potential of VQSynergy in revolutionizing the field through its advanced predictive capabilities, thereby contributing to the optimization of cancer treatment strategies."],"url":"http://arxiv.org/abs/2403.03089v1","category":"q-bio.QM"}
{"created":"2024-03-05 16:09:55","title":"Tooling Offline Runtime Verification against Interaction Models : recognizing sliced behaviors using parameterized simulation","abstract":"Offline runtime verification involves the static analysis of executions of a system against a specification. For distributed systems, it is generally not possible to characterize executions in the form of global traces, given the absence of a global clock. To account for this, we model executions as collections of local traces called multi-traces, with one local trace per group of co-localized actors that share a common clock. Due to the difficulty of synchronizing the start and end of the recordings of local traces, events may be missing at their beginning or end. Considering such partially observed multi-traces is challenging for runtime verification. To that end, we propose an algorithm that verifies the conformity of such traces against formal specifications called Interactions (akin to Message Sequence Charts). It relies on parameterized simulation to reconstitute unobserved behaviors.","sentences":["Offline runtime verification involves the static analysis of executions of a system against a specification.","For distributed systems, it is generally not possible to characterize executions in the form of global traces, given the absence of a global clock.","To account for this, we model executions as collections of local traces called multi-traces, with one local trace per group of co-localized actors that share a common clock.","Due to the difficulty of synchronizing the start and end of the recordings of local traces, events may be missing at their beginning or end.","Considering such partially observed multi-traces is challenging for runtime verification.","To that end, we propose an algorithm that verifies the conformity of such traces against formal specifications called Interactions (akin to Message Sequence Charts).","It relies on parameterized simulation to reconstitute unobserved behaviors."],"url":"http://arxiv.org/abs/2403.03083v1","category":"cs.SE"}
{"created":"2024-03-05 16:08:59","title":"Recall-Oriented Continual Learning with Generative Adversarial Meta-Model","abstract":"The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recall-oriented continual learning framework to address this challenge. Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only effectively learns new knowledge without any disruption but also achieves high stability of previous knowledge in both task-aware and task-agnostic learning scenarios. Our code is available at: https://github.com/bigdata-inha/recall-oriented-cl-framework.","sentences":["The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks.","In this paper, we propose the recall-oriented continual learning framework to address this challenge.","Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary.","In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task.","Through our experiments, we show that our framework not only effectively learns new knowledge without any disruption but also achieves high stability of previous knowledge in both task-aware and task-agnostic learning scenarios.","Our code is available at: https://github.com/bigdata-inha/recall-oriented-cl-framework."],"url":"http://arxiv.org/abs/2403.03082v1","category":"cs.LG"}
{"created":"2024-03-05 15:57:11","title":"Enhancing single-atom loading in tightly confined dipole traps with ancillary dipole beam","abstract":"Single atoms trapped in tightly focused optical dipole traps provide an excellent experimental platform for quantum computing, precision measurement, and fundamental physics research. In this work, we propose and demonstrate a novel approach to enhancing the loading of single atoms by introducing a weak ancillary dipole beam. The loading rate of single atoms in a dipole trap can be significantly improved by only a few tens of microwatts of counter-propagating beam. It was also demonstrated that multiple atoms could be loaded with the assistance of a counter-propagating beam. By reducing the power requirements for trapping single atoms and enabling the trapping of multiple atoms, our method facilitates the extension of single-atom arrays and the investigation of collective light-atom interactions.","sentences":["Single atoms trapped in tightly focused optical dipole traps provide an excellent experimental platform for quantum computing, precision measurement, and fundamental physics research.","In this work, we propose and demonstrate a novel approach to enhancing the loading of single atoms by introducing a weak ancillary dipole beam.","The loading rate of single atoms in a dipole trap can be significantly improved by only a few tens of microwatts of counter-propagating beam.","It was also demonstrated that multiple atoms could be loaded with the assistance of a counter-propagating beam.","By reducing the power requirements for trapping single atoms and enabling the trapping of multiple atoms, our method facilitates the extension of single-atom arrays and the investigation of collective light-atom interactions."],"url":"http://arxiv.org/abs/2403.03068v1","category":"quant-ph"}
{"created":"2024-03-05 15:49:33","title":"When Industry meets Trustworthy AI: A Systematic Review of AI for Industry 5.0","abstract":"Industry is at the forefront of adopting new technologies, and the process followed by the adoption has a significant impact on the economy and society. In this work, we focus on analysing the current paradigm in which industry evolves, making it more sustainable and Trustworthy. In Industry 5.0, Artificial Intelligence (AI), among other technology enablers, is used to build services from a sustainable, human-centric and resilient perspective. It is crucial to understand those aspects that can bring AI to industry, respecting Trustworthy principles by collecting information to define how it is incorporated in the early stages, its impact, and the trends observed in the field. In addition, to understand the challenges and gaps in the transition from Industry 4.0 to Industry 5.0, a general perspective on the industry's readiness for new technologies is described. This provides practitioners with novel opportunities to be explored in pursuit of the adoption of Trustworthy AI in the sector.","sentences":["Industry is at the forefront of adopting new technologies, and the process followed by the adoption has a significant impact on the economy and society.","In this work, we focus on analysing the current paradigm in which industry evolves, making it more sustainable and Trustworthy.","In Industry 5.0, Artificial Intelligence (AI), among other technology enablers, is used to build services from a sustainable, human-centric and resilient perspective.","It is crucial to understand those aspects that can bring AI to industry, respecting Trustworthy principles by collecting information to define how it is incorporated in the early stages, its impact, and the trends observed in the field.","In addition, to understand the challenges and gaps in the transition from Industry 4.0 to Industry 5.0, a general perspective on the industry's readiness for new technologies is described.","This provides practitioners with novel opportunities to be explored in pursuit of the adoption of Trustworthy AI in the sector."],"url":"http://arxiv.org/abs/2403.03061v1","category":"cs.CY"}
{"created":"2024-03-05 15:44:41","title":"Efficient Interaction-Based Offline Runtime Verification of Distributed Systems with Lifeline Removal","abstract":"Runtime Verification (RV) refers to a family of techniques in which system executions are observed and confronted to formal specifications, with the aim of identifying faults. In Offline RV, observation is done in a first step and verification in a second, on a static artifact collected during observation. In this paper, we define an approach to offline RV of Distributed Systems (DS) against interactions. Interactions are formal models describing communications within a DS. DS are composed of subsystems deployed on different machines and interacting via message passing. Therefore, observing executions of a DS entails logging a collection of local execution traces, one for each subsystem, that we call a multi-trace. A major challenge in analyzing multi-traces is that there are no practical means to synchronize the ends of observations of all local traces. We address this via an operation, called lifeline removal, which we apply on-the-fly on the specification during verification once a local trace has been entirely analyzed. This operation removes from the interaction the specification of actions occurring on the subsystem that is no-longer observed. This may allow further execution of the specification via removing deadlocks due to the partial orders of actions. We prove the correctness of the resulting RV algorithm and introduce two optimization techniques which we also prove correct. We implement a Partial Order Reduction (POR) technique via the selection of a one-unambiguous action (as a unique first step to a linearization) which existence is determined via another use of the lifeline removal operator. Additionally, Local Analyses (LOC) i.e., the verification of local traces, can be leveraged during the global multi-trace analysis to prove failure more quickly. Experiments illustrate the application of our RV approach and the benefits of our optimizations.","sentences":["Runtime Verification (RV) refers to a family of techniques in which system executions are observed and confronted to formal specifications, with the aim of identifying faults.","In Offline RV, observation is done in a first step and verification in a second, on a static artifact collected during observation.","In this paper, we define an approach to offline RV of Distributed Systems (DS) against interactions.","Interactions are formal models describing communications within a DS.","DS are composed of subsystems deployed on different machines and interacting via message passing.","Therefore, observing executions of a DS entails logging a collection of local execution traces, one for each subsystem, that we call a multi-trace.","A major challenge in analyzing multi-traces is that there are no practical means to synchronize the ends of observations of all local traces.","We address this via an operation, called lifeline removal, which we apply on-the-fly on the specification during verification once a local trace has been entirely analyzed.","This operation removes from the interaction the specification of actions occurring on the subsystem that is no-longer observed.","This may allow further execution of the specification via removing deadlocks due to the partial orders of actions.","We prove the correctness of the resulting RV algorithm and introduce two optimization techniques which we also prove correct.","We implement a Partial Order Reduction (POR) technique via the selection of a one-unambiguous action (as a unique first step to a linearization) which existence is determined via another use of the lifeline removal operator.","Additionally, Local Analyses (LOC) i.e., the verification of local traces, can be leveraged during the global multi-trace analysis to prove failure more quickly.","Experiments illustrate the application of our RV approach and the benefits of our optimizations."],"url":"http://arxiv.org/abs/2403.03057v1","category":"cs.FL"}
{"created":"2024-03-05 15:37:06","title":"Neural Codebook Design for Network Beam Management","abstract":"Obtaining accurate and timely channel state information (CSI) is a fundamental challenge for large antenna systems. Mobile systems like 5G use a beam management framework that joins the initial access, beamforming, CSI acquisition, and data transmission. The design of codebooks for these stages, however, is challenging due to their interrelationships, varying array sizes, and site-specific channel and user distributions. Furthermore, beam management is often focused on single-sector operations while ignoring the overarching network- and system-level optimization. In this paper, we proposed an end-to-end learned codebook design algorithm, network beamspace learning (NBL), that captures and optimizes codebooks to mitigate interference while maximizing the achievable performance with extremely large hybrid arrays. The proposed algorithm requires limited shared information yet designs codebooks that outperform traditional codebooks by over 10dB in beam alignment and achieve more than 25% improvements in network spectral efficiency.","sentences":["Obtaining accurate and timely channel state information (CSI) is a fundamental challenge for large antenna systems.","Mobile systems like 5G use a beam management framework that joins the initial access, beamforming, CSI acquisition, and data transmission.","The design of codebooks for these stages, however, is challenging due to their interrelationships, varying array sizes, and site-specific channel and user distributions.","Furthermore, beam management is often focused on single-sector operations while ignoring the overarching network- and system-level optimization.","In this paper, we proposed an end-to-end learned codebook design algorithm, network beamspace learning (NBL), that captures and optimizes codebooks to mitigate interference while maximizing the achievable performance with extremely large hybrid arrays.","The proposed algorithm requires limited shared information yet designs codebooks that outperform traditional codebooks by over 10dB in beam alignment and achieve more than 25% improvements in network spectral efficiency."],"url":"http://arxiv.org/abs/2403.03053v1","category":"eess.SP"}
{"created":"2024-03-05 15:18:02","title":"A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives","abstract":"Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpack of skills that a robot can carry around and use when needed. We demonstrate the effectiveness and efficiency of our approach on four Ego4D benchmarks, outperforming current state-of-the-art methods.","sentences":["Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once.","We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills.","To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills.","We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpack of skills that a robot can carry around and use when needed.","We demonstrate the effectiveness and efficiency of our approach on four Ego4D benchmarks, outperforming current state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.03037v1","category":"cs.CV"}
{"created":"2024-03-05 15:06:16","title":"Unifying Controller Design for Stabilizing Nonlinear Systems with Norm-Bounded Control Inputs","abstract":"This paper revisits a classical challenge in the design of stabilizing controllers for nonlinear systems with a norm-bounded input constraint. By extending Lin-Sontag's universal formula and introducing a generic (state-dependent) scaling term, a unifying controller design method is proposed. The incorporation of this generic scaling term gives a unified controller and enables the derivation of alternative universal formulas with various favorable properties, which makes it suitable for tailored control designs to meet specific requirements and provides versatility across different control scenarios. Additionally, we present a constructive approach to determine the optimal scaling term, leading to an explicit solution to an optimization problem, named optimization-based universal formula. The resulting controller ensures asymptotic stability, satisfies a norm-bounded input constraint, and optimizes a predefined cost function. Finally, the essential properties of the unified controllers are analyzed, including smoothness, continuity at the origin, stability margin, and inverse optimality. Simulations validate the approach, showcasing its effectiveness in addressing a challenging stabilizing control problem of a nonlinear system.","sentences":["This paper revisits a classical challenge in the design of stabilizing controllers for nonlinear systems with a norm-bounded input constraint.","By extending Lin-Sontag's universal formula and introducing a generic (state-dependent) scaling term, a unifying controller design method is proposed.","The incorporation of this generic scaling term gives a unified controller and enables the derivation of alternative universal formulas with various favorable properties, which makes it suitable for tailored control designs to meet specific requirements and provides versatility across different control scenarios.","Additionally, we present a constructive approach to determine the optimal scaling term, leading to an explicit solution to an optimization problem, named optimization-based universal formula.","The resulting controller ensures asymptotic stability, satisfies a norm-bounded input constraint, and optimizes a predefined cost function.","Finally, the essential properties of the unified controllers are analyzed, including smoothness, continuity at the origin, stability margin, and inverse optimality.","Simulations validate the approach, showcasing its effectiveness in addressing a challenging stabilizing control problem of a nonlinear system."],"url":"http://arxiv.org/abs/2403.03030v1","category":"eess.SY"}
{"created":"2024-03-05 15:04:18","title":"Word Importance Explains How Prompts Affect Language Model Outputs","abstract":"The emergence of large language models (LLMs) has revolutionized numerous applications across industries. However, their \"black box\" nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use. This study presents a method to improve the explainability of LLMs by varying individual words in prompts to uncover their statistical impact on the model outputs. This approach, inspired by permutation importance for tabular data, masks each word in the system prompt and evaluates its effect on the outputs based on the available text scores aggregated over multiple user inputs. Unlike classical attention, word importance measures the impact of prompt words on arbitrarily-defined text scores, which enables decomposing the importance of words into the specific measures of interest--including bias, reading level, verbosity, etc. This procedure also enables measuring impact when attention weights are not available. To test the fidelity of this approach, we explore the effect of adding different suffixes to multiple different system prompts and comparing subsequent generations with different large language models. Results show that word importance scores are closely related to the expected suffix importances for multiple scoring functions.","sentences":["The emergence of large language models (LLMs) has revolutionized numerous applications across industries.","However, their \"black box\" nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use.","This study presents a method to improve the explainability of LLMs by varying individual words in prompts to uncover their statistical impact on the model outputs.","This approach, inspired by permutation importance for tabular data, masks each word in the system prompt and evaluates its effect on the outputs based on the available text scores aggregated over multiple user inputs.","Unlike classical attention, word importance measures the impact of prompt words on arbitrarily-defined text scores, which enables decomposing the importance of words into the specific measures of interest--including bias, reading level, verbosity, etc.","This procedure also enables measuring impact when attention weights are not available.","To test the fidelity of this approach, we explore the effect of adding different suffixes to multiple different system prompts and comparing subsequent generations with different large language models.","Results show that word importance scores are closely related to the expected suffix importances for multiple scoring functions."],"url":"http://arxiv.org/abs/2403.03028v1","category":"cs.AI"}
{"created":"2024-03-05 14:57:04","title":"SplAgger: Split Aggregation for Meta-Reinforcement Learning","abstract":"A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end. In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task. These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods. However, recent evidence suggests that task inference objectives are unnecessary in practice. Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models with permutation invariant aggregation, which exploit the fact that, due to the Markov property, the task posterior does not depend on the order of data. We empirically confirm the advantage of permutation invariant sequence models without the use of task inference objectives. However, we also find, surprisingly, that there are multiple conditions under which permutation variance remains useful. Therefore, we propose SplAgger, which uses both permutation variant and invariant components to achieve the best of both worlds, outperforming all baselines on continuous control and memory environments.","sentences":["A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks.","Meta-RL aims to achieve this by directly learning such agents.","One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end.","In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task.","These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods.","However, recent evidence suggests that task inference objectives are unnecessary in practice.","Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not.","In this paper, we present strong evidence that task inference sequence models are still beneficial.","In particular, we investigate sequence models with permutation invariant aggregation, which exploit the fact that, due to the Markov property, the task posterior does not depend on the order of data.","We empirically confirm the advantage of permutation invariant sequence models without the use of task inference objectives.","However, we also find, surprisingly, that there are multiple conditions under which permutation variance remains useful.","Therefore, we propose SplAgger, which uses both permutation variant and invariant components to achieve the best of both worlds, outperforming all baselines on continuous control and memory environments."],"url":"http://arxiv.org/abs/2403.03020v1","category":"cs.LG"}
{"created":"2024-03-05 14:53:53","title":"OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following","abstract":"Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components-ranging from visual perception to action execution-on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance. Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.","sentences":["Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions.","Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF.","Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components-ranging from visual perception to action execution-on task performance.","To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor.","Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance.","Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance.","Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance."],"url":"http://arxiv.org/abs/2403.03017v1","category":"cs.AI"}
{"created":"2024-03-05 14:51:46","title":"Low Complexity Channel Estimation for RIS-Assisted THz Systems with Beam Split","abstract":"To support extremely high data rates, reconfigurable intelligent surface (RIS)-assisted terahertz (THz) communication is considered to be a promising technology for future sixth-generation networks. However, due to the typical employment of hybrid beamforming architecture in THz systems, as well as the passive nature of RIS which lacks the capability to process pilot signals, obtaining channel state information (CSI) is facing significant challenges. To accurately estimate the cascaded channel, we propose a novel low-complexity channel estimation scheme, which includes three steps. Specifically, we first estimate full CSI within a small subset of subcarriers (SCs). Then, we acquire angular information at base station and RIS based on the full CSI. Finally, we derive spatial directions and recover full-CSI for the remaining SCs. Theoretical analysis and simulation results demonstrate that the proposed scheme can achieve superior performance in terms of normalized mean-square-error and exhibit a lower computational complexity compared with the existing algorithms.","sentences":["To support extremely high data rates, reconfigurable intelligent surface (RIS)-assisted terahertz (THz) communication is considered to be a promising technology for future sixth-generation networks.","However, due to the typical employment of hybrid beamforming architecture in THz systems, as well as the passive nature of RIS which lacks the capability to process pilot signals, obtaining channel state information (CSI) is facing significant challenges.","To accurately estimate the cascaded channel, we propose a novel low-complexity channel estimation scheme, which includes three steps.","Specifically, we first estimate full CSI within a small subset of subcarriers (SCs).","Then, we acquire angular information at base station and RIS based on the full CSI.","Finally, we derive spatial directions and recover full-CSI for the remaining SCs.","Theoretical analysis and simulation results demonstrate that the proposed scheme can achieve superior performance in terms of normalized mean-square-error and exhibit a lower computational complexity compared with the existing algorithms."],"url":"http://arxiv.org/abs/2403.03015v1","category":"cs.IT"}
{"created":"2024-03-05 14:41:12","title":"Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations","abstract":"In the era of personalized education, the provision of comprehensible explanations for learning recommendations is of a great value to enhance the learner's understanding and engagement with the recommended learning content. Large language models (LLMs) and generative AI in general have recently opened new doors for generating human-like explanations, for and along learning recommendations. However, their precision is still far away from acceptable in a sensitive field like education. To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context, for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context. We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations. With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM. Domain experts were integrated in the prompt engineering phase as part of a study, to ensure that explanations include information that is relevant to the learner. We evaluate our approach quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively with experts and learners. Our results show an enhanced recall and precision of the generated explanations compared to those generated solely by the GPT model, with a greatly reduced risk of generating imprecise information in the final learning explanation.","sentences":["In the era of personalized education, the provision of comprehensible explanations for learning recommendations is of a great value to enhance the learner's understanding and engagement with the recommended learning content.","Large language models (LLMs) and generative AI in general have recently opened new doors for generating human-like explanations, for and along learning recommendations.","However, their precision is still far away from acceptable in a sensitive field like education.","To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context, for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context.","We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations.","With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM.","Domain experts were integrated in the prompt engineering phase as part of a study, to ensure that explanations include information that is relevant to the learner.","We evaluate our approach quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively with experts and learners.","Our results show an enhanced recall and precision of the generated explanations compared to those generated solely by the GPT model, with a greatly reduced risk of generating imprecise information in the final learning explanation."],"url":"http://arxiv.org/abs/2403.03008v1","category":"cs.AI"}
{"created":"2024-03-05 14:35:34","title":"Scalable Bayesian inference for the generalized linear mixed model","abstract":"The generalized linear mixed model (GLMM) is a popular statistical approach for handling correlated data, and is used extensively in applications areas where big data is common, including biomedical data settings. The focus of this paper is scalable statistical inference for the GLMM, where we define statistical inference as: (i) estimation of population parameters, and (ii) evaluation of scientific hypotheses in the presence of uncertainty. Artificial intelligence (AI) learning algorithms excel at scalable statistical estimation, but rarely include uncertainty quantification. In contrast, Bayesian inference provides full statistical inference, since uncertainty quantification results automatically from the posterior distribution. Unfortunately, Bayesian inference algorithms, including Markov Chain Monte Carlo (MCMC), become computationally intractable in big data settings. In this paper, we introduce a statistical inference algorithm at the intersection of AI and Bayesian inference, that leverages the scalability of modern AI algorithms with guaranteed uncertainty quantification that accompanies Bayesian inference. Our algorithm is an extension of stochastic gradient MCMC with novel contributions that address the treatment of correlated data (i.e., intractable marginal likelihood) and proper posterior variance estimation. Through theoretical and empirical results we establish our algorithm's statistical inference properties, and apply the method in a large electronic health records database.","sentences":["The generalized linear mixed model (GLMM) is a popular statistical approach for handling correlated data, and is used extensively in applications areas where big data is common, including biomedical data settings.","The focus of this paper is scalable statistical inference for the GLMM, where we define statistical inference as: (i) estimation of population parameters, and (ii) evaluation of scientific hypotheses in the presence of uncertainty.","Artificial intelligence (AI) learning algorithms excel at scalable statistical estimation, but rarely include uncertainty quantification.","In contrast, Bayesian inference provides full statistical inference, since uncertainty quantification results automatically from the posterior distribution.","Unfortunately, Bayesian inference algorithms, including Markov Chain Monte Carlo (MCMC), become computationally intractable in big data settings.","In this paper, we introduce a statistical inference algorithm at the intersection of AI and Bayesian inference, that leverages the scalability of modern AI algorithms with guaranteed uncertainty quantification that accompanies Bayesian inference.","Our algorithm is an extension of stochastic gradient MCMC with novel contributions that address the treatment of correlated data (i.e., intractable marginal likelihood) and proper posterior variance estimation.","Through theoretical and empirical results we establish our algorithm's statistical inference properties, and apply the method in a large electronic health records database."],"url":"http://arxiv.org/abs/2403.03007v1","category":"stat.CO"}
{"created":"2024-03-05 14:28:40","title":"Mem-elements based Neuromorphic Hardware for Neural Network Application","abstract":"The thesis investigates the utilization of memristive and memcapacitive crossbar arrays in low-power machine learning accelerators, offering a comprehensive co-design framework for deep neural networks (DNN). The model, implemented through a hybrid Python and PyTorch approach, accounts for various non-idealities, achieving exceptional training accuracies of 90.02% and 91.03% for the CIFAR-10 dataset with memristive and memcapacitive crossbar arrays on an 8-layer VGG network. Additionally, the thesis introduces a novel approach to emulate meminductor devices using Operational Transconductance Amplifiers (OTA) and capacitors, showcasing adjustable behavior. Transistor-level simulations in 180 nm CMOS technology, operating at 60 MHz, demonstrate the proposed meminductor emulator's viability with a power consumption of 0.337 mW. The design is further validated in neuromorphic circuits and CNN accelerators, achieving training and testing accuracies of 91.04% and 88.82%, respectively. Notably, the exclusive use of MOS transistors ensures the feasibility of monolithic IC fabrication. This research significantly contributes to the exploration of advanced hardware solutions for efficient and high-performance machine-learning applications.","sentences":["The thesis investigates the utilization of memristive and memcapacitive crossbar arrays in low-power machine learning accelerators, offering a comprehensive co-design framework for deep neural networks (DNN).","The model, implemented through a hybrid Python and PyTorch approach, accounts for various non-idealities, achieving exceptional training accuracies of 90.02% and 91.03% for the CIFAR-10 dataset with memristive and memcapacitive crossbar arrays on an 8-layer VGG network.","Additionally, the thesis introduces a novel approach to emulate meminductor devices using Operational Transconductance Amplifiers (OTA) and capacitors, showcasing adjustable behavior.","Transistor-level simulations in 180 nm CMOS technology, operating at 60 MHz, demonstrate the proposed meminductor emulator's viability with a power consumption of 0.337 mW.","The design is further validated in neuromorphic circuits and CNN accelerators, achieving training and testing accuracies of 91.04% and 88.82%, respectively.","Notably, the exclusive use of MOS transistors ensures the feasibility of monolithic IC fabrication.","This research significantly contributes to the exploration of advanced hardware solutions for efficient and high-performance machine-learning applications."],"url":"http://arxiv.org/abs/2403.03002v1","category":"cs.NE"}
{"created":"2024-03-05 14:21:57","title":"Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees","abstract":"Malicious URLs provide adversarial opportunities across various industries, including transportation, healthcare, energy, and banking which could be detrimental to business operations. Consequently, the detection of these URLs is of crucial importance; however, current Machine Learning (ML) models are susceptible to backdoor attacks. These attacks involve manipulating a small percentage of training data labels, such as Label Flipping (LF), which changes benign labels to malicious ones and vice versa. This manipulation results in misclassification and leads to incorrect model behavior. Therefore, integrating defense mechanisms into the architecture of ML models becomes an imperative consideration to fortify against potential attacks.   The focus of this study is on backdoor attacks in the context of URL detection using ensemble trees. By illuminating the motivations behind such attacks, highlighting the roles of attackers, and emphasizing the critical importance of effective defense strategies, this paper contributes to the ongoing efforts to fortify ML models against adversarial threats within the ML domain in network security. We propose an innovative alarm system that detects the presence of poisoned labels and a defense mechanism designed to uncover the original class labels with the aim of mitigating backdoor attacks on ensemble tree classifiers. We conducted a case study using the Alexa and Phishing Site URL datasets and showed that LF attacks can be addressed using our proposed defense mechanism. Our experimental results prove that the LF attack achieved an Attack Success Rate (ASR) between 50-65% within 2-5%, and the innovative defense method successfully detected poisoned labels with an accuracy of up to 100%.","sentences":["Malicious URLs provide adversarial opportunities across various industries, including transportation, healthcare, energy, and banking which could be detrimental to business operations.","Consequently, the detection of these URLs is of crucial importance; however, current Machine Learning (ML) models are susceptible to backdoor attacks.","These attacks involve manipulating a small percentage of training data labels, such as Label Flipping (LF), which changes benign labels to malicious ones and vice versa.","This manipulation results in misclassification and leads to incorrect model behavior.","Therefore, integrating defense mechanisms into the architecture of ML models becomes an imperative consideration to fortify against potential attacks.   ","The focus of this study is on backdoor attacks in the context of URL detection using ensemble trees.","By illuminating the motivations behind such attacks, highlighting the roles of attackers, and emphasizing the critical importance of effective defense strategies, this paper contributes to the ongoing efforts to fortify ML models against adversarial threats within the ML domain in network security.","We propose an innovative alarm system that detects the presence of poisoned labels and a defense mechanism designed to uncover the original class labels with the aim of mitigating backdoor attacks on ensemble tree classifiers.","We conducted a case study using the Alexa and Phishing Site URL datasets and showed that LF attacks can be addressed using our proposed defense mechanism.","Our experimental results prove that the LF attack achieved an Attack Success Rate (ASR) between 50-65% within 2-5%, and the innovative defense method successfully detected poisoned labels with an accuracy of up to 100%."],"url":"http://arxiv.org/abs/2403.02995v1","category":"cs.CR"}
{"created":"2024-03-05 14:18:15","title":"Localized Zeroth-Order Prompt Optimization","abstract":"The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs. Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks. This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization. To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights. Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (Insight I). The choice of the input domain, covering both the generation and the representation of prompts, affects the identification of well-performing local optima (Insight II). Inspired by these insights, we propose a novel algorithm, namely localized zeroth-order prompt optimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived Gaussian process into standard zeroth-order optimization for an efficient search of well-performing local optima in prompt optimization. Remarkably, ZOPO outperforms existing baselines in terms of both the optimization performance and the query efficiency, which we demonstrate through extensive experiments.","sentences":["The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs.","Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks.","This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization.","To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights.","Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (Insight I).","The choice of the input domain, covering both the generation and the representation of prompts, affects the identification of well-performing local optima (Insight II).","Inspired by these insights, we propose a novel algorithm, namely localized zeroth-order prompt optimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived Gaussian process into standard zeroth-order optimization for an efficient search of well-performing local optima in prompt optimization.","Remarkably, ZOPO outperforms existing baselines in terms of both the optimization performance and the query efficiency, which we demonstrate through extensive experiments."],"url":"http://arxiv.org/abs/2403.02993v1","category":"cs.AI"}
{"created":"2024-03-05 14:11:54","title":"Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges","abstract":"In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a foundational guide for researchers and practitioners in this field.","sentences":["In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection.","This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond.","From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training.","Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation.","This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a foundational guide for researchers and practitioners in this field."],"url":"http://arxiv.org/abs/2403.02990v1","category":"cs.CL"}
{"created":"2024-03-05 14:04:13","title":"Evolution Transformer: In-Context Evolutionary Optimization","abstract":"Evolutionary optimization algorithms are often derived from loose biological analogies and struggle to leverage information obtained during the sequential course of optimization. An alternative promising approach is to leverage data and directly discover powerful optimization principles via meta-optimization. In this work, we follow such a paradigm and introduce Evolution Transformer, a causal Transformer architecture, which can flexibly characterize a family of Evolution Strategies. Given a trajectory of evaluations and search distribution statistics, Evolution Transformer outputs a performance-improving update to the search distribution. The architecture imposes a set of suitable inductive biases, i.e. the invariance of the distribution update to the order of population members within a generation and equivariance to the order of the search dimensions. We train the model weights using Evolutionary Algorithm Distillation, a technique for supervised optimization of sequence models using teacher algorithm trajectories. The resulting model exhibits strong in-context optimization performance and shows strong generalization capabilities to otherwise challenging neuroevolution tasks. We analyze the resulting properties of the Evolution Transformer and propose a technique to fully self-referentially train the Evolution Transformer, starting from a random initialization and bootstrapping its own learning progress. We provide an open source implementation under https://github.com/RobertTLange/evosax.","sentences":["Evolutionary optimization algorithms are often derived from loose biological analogies and struggle to leverage information obtained during the sequential course of optimization.","An alternative promising approach is to leverage data and directly discover powerful optimization principles via meta-optimization.","In this work, we follow such a paradigm and introduce Evolution Transformer, a causal Transformer architecture, which can flexibly characterize a family of Evolution Strategies.","Given a trajectory of evaluations and search distribution statistics, Evolution Transformer outputs a performance-improving update to the search distribution.","The architecture imposes a set of suitable inductive biases, i.e. the invariance of the distribution update to the order of population members within a generation and equivariance to the order of the search dimensions.","We train the model weights using Evolutionary Algorithm Distillation, a technique for supervised optimization of sequence models using teacher algorithm trajectories.","The resulting model exhibits strong in-context optimization performance and shows strong generalization capabilities to otherwise challenging neuroevolution tasks.","We analyze the resulting properties of the Evolution Transformer and propose a technique to fully self-referentially train the Evolution Transformer, starting from a random initialization and bootstrapping its own learning progress.","We provide an open source implementation under https://github.com/RobertTLange/evosax."],"url":"http://arxiv.org/abs/2403.02985v1","category":"cs.AI"}
{"created":"2024-03-05 14:03:15","title":"Federated Learning Under Attack: Exposing Vulnerabilities through Data Poisoning Attacks in Computer Networks","abstract":"Federated Learning (FL) is a machine learning (ML) approach that enables multiple decentralized devices or edge servers to collaboratively train a shared model without exchanging raw data. During the training and sharing of model updates between clients and servers, data and models are susceptible to different data-poisoning attacks.   In this study, our motivation is to explore the severity of data poisoning attacks in the computer network domain because they are easy to implement but difficult to detect. We considered two types of data-poisoning attacks, label flipping (LF) and feature poisoning (FP), and applied them with a novel approach. In LF, we randomly flipped the labels of benign data and trained the model on the manipulated data. For FP, we randomly manipulated the highly contributing features determined using the Random Forest algorithm. The datasets used in this experiment were CIC and UNSW related to computer networks. We generated adversarial samples using the two attacks mentioned above, which were applied to a small percentage of datasets. Subsequently, we trained and tested the accuracy of the model on adversarial datasets. We recorded the results for both benign and manipulated datasets and observed significant differences between the accuracy of the models on different datasets. From the experimental results, it is evident that the LF attack failed, whereas the FP attack showed effective results, which proved its significance in fooling a server. With a 1% LF attack on the CIC, the accuracy was approximately 0.0428 and the ASR was 0.9564; hence, the attack is easily detectable, while with a 1% FP attack, the accuracy and ASR were both approximately 0.9600, hence, FP attacks are difficult to detect. We repeated the experiment with different poisoning percentages.","sentences":["Federated Learning (FL) is a machine learning (ML) approach that enables multiple decentralized devices or edge servers to collaboratively train a shared model without exchanging raw data.","During the training and sharing of model updates between clients and servers, data and models are susceptible to different data-poisoning attacks.   ","In this study, our motivation is to explore the severity of data poisoning attacks in the computer network domain because they are easy to implement but difficult to detect.","We considered two types of data-poisoning attacks, label flipping (LF) and feature poisoning (FP), and applied them with a novel approach.","In LF, we randomly flipped the labels of benign data and trained the model on the manipulated data.","For FP, we randomly manipulated the highly contributing features determined using the Random Forest algorithm.","The datasets used in this experiment were CIC and UNSW related to computer networks.","We generated adversarial samples using the two attacks mentioned above, which were applied to a small percentage of datasets.","Subsequently, we trained and tested the accuracy of the model on adversarial datasets.","We recorded the results for both benign and manipulated datasets and observed significant differences between the accuracy of the models on different datasets.","From the experimental results, it is evident that the LF attack failed, whereas the FP attack showed effective results, which proved its significance in fooling a server.","With a 1% LF attack on the CIC, the accuracy was approximately 0.0428 and the ASR was 0.9564; hence, the attack is easily detectable, while with a 1% FP attack, the accuracy and ASR were both approximately 0.9600, hence, FP attacks are difficult to detect.","We repeated the experiment with different poisoning percentages."],"url":"http://arxiv.org/abs/2403.02983v1","category":"cs.CR"}
{"created":"2024-03-05 13:55:16","title":"A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching","abstract":"Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community question answering, searching, chatbot, and recommendation. Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \\textit{keywords} and \\textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance. Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external NER techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory NER tools are usually hard to obtain. In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the model from the reliance on NER models. To this end, we devise a \\underline{M}ulti-\\underline{C}oncept \\underline{P}arsed \\underline{S}emantic \\underline{M}atching framework based on the pre-trained language models, abbreviated as \\textbf{MCP-SM}, to extract various concepts and infuse them into the classification tokens. We conduct comprehensive experiments on English datasets QQP and MRPC, and Chinese dataset Medical-SM. Besides, we experiment on Arabic datasets MQ2Q and XNLI, the outstanding performance further prove MCP-SM's applicability in low-resource languages.","sentences":["Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community question answering, searching, chatbot, and recommendation.","Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \\textit{keywords} and \\textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance.","Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external NER techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory NER tools are usually hard to obtain.","In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the model from the reliance on NER models.","To this end, we devise a \\underline{M}ulti-\\underline{C}oncept \\underline{P}arsed \\underline{S}emantic \\underline{M}atching framework based on the pre-trained language models, abbreviated as \\textbf{MCP-SM}, to extract various concepts and infuse them into the classification tokens.","We conduct comprehensive experiments on English datasets QQP and MRPC, and Chinese dataset Medical-SM.","Besides, we experiment on Arabic datasets MQ2Q and XNLI, the outstanding performance further prove MCP-SM's applicability in low-resource languages."],"url":"http://arxiv.org/abs/2403.02975v1","category":"cs.CL"}
{"created":"2024-03-05 13:43:58","title":"Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering","abstract":"Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.","sentences":["Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin.","Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues.","These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence.","To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs.","We optimize an open-source LLM as a fact summarizer through distillation and preference alignment.","Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary."],"url":"http://arxiv.org/abs/2403.02966v1","category":"cs.CL"}
{"created":"2024-03-05 13:41:25","title":"ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities","abstract":"This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks. We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation. Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks. Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy. Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks. Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics.","sentences":["This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks.","We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation.","Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks.","Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy.","Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks.","Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics."],"url":"http://arxiv.org/abs/2403.02965v1","category":"cs.CV"}
{"created":"2024-03-05 13:34:49","title":"Opportunistic User Scheduling for Secure RIS-aided Wireless Communications","abstract":"In this paper, we provide expressions for the secrecy outage probability (SOP) for suboptimal and optimal opportunistic scheduling schemes in a reconfigurable intelligent surface (RIS) aided system with multiple eavesdroppers in approximate closed form. A suboptimal scheduling (SS) scheme is analyzed, which is used when the channel state information (CSI) of the eavesdropping links is unavailable, and the optimal scheduling (OS) scheme is also analyzed, which is used when the global CSI is available. For each scheme, we provide a simplified expression for the SOP in the high signal-to-noise ratio (SNR) regime to demonstrate its behavior as a function of the key system parameters. At high SNR, the SOP saturates to a constant level which decreases exponentially with the number of RIS elements in the SS scheme and with the product of the number of RIS elements and the number of users in the OS scheme. We compare the performance of the opportunistic user scheduling schemes with that of a non-orthogonal multiple access (NOMA) based scheduling scheme which chooses a pair of users in each time slot for scheduling and we show that the opportunistic schemes outperform the NOMA-based scheme. We also derive a closed-form expression for the SOP of a decode-and-forward (DF) relay-aided scheduling scheme in order to compare it with that of the RIS-aided system. It is found that the RIS-aided system outperforms the relay-aided systems when the number of RIS elements is sufficiently large. An increased number of RIS elements is required to outperform the relay-aided system at higher operating frequencies.","sentences":["In this paper, we provide expressions for the secrecy outage probability (SOP) for suboptimal and optimal opportunistic scheduling schemes in a reconfigurable intelligent surface (RIS) aided system with multiple eavesdroppers in approximate closed form.","A suboptimal scheduling (SS) scheme is analyzed, which is used when the channel state information (CSI) of the eavesdropping links is unavailable, and the optimal scheduling (OS) scheme is also analyzed, which is used when the global CSI is available.","For each scheme, we provide a simplified expression for the SOP in the high signal-to-noise ratio (SNR) regime to demonstrate its behavior as a function of the key system parameters.","At high SNR, the SOP saturates to a constant level which decreases exponentially with the number of RIS elements in the SS scheme and with the product of the number of RIS elements and the number of users in the OS scheme.","We compare the performance of the opportunistic user scheduling schemes with that of a non-orthogonal multiple access (NOMA) based scheduling scheme which chooses a pair of users in each time slot for scheduling and we show that the opportunistic schemes outperform the NOMA-based scheme.","We also derive a closed-form expression for the SOP of a decode-and-forward (DF) relay-aided scheduling scheme in order to compare it with that of the RIS-aided system.","It is found that the RIS-aided system outperforms the relay-aided systems when the number of RIS elements is sufficiently large.","An increased number of RIS elements is required to outperform the relay-aided system at higher operating frequencies."],"url":"http://arxiv.org/abs/2403.02963v1","category":"cs.IT"}
{"created":"2024-03-05 13:33:12","title":"WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction","abstract":"Tabular data, as a crucial form of data representation, exists in diverse formats on the Web. When confronted with complex and irregular tables, manual modification becomes a laborious task. This paper investigates the performance of Large Language Models (LLMs) in the context of table editing tasks. Existing research mainly focuses on regular-shaped tables, wherein instructions are used to generate code in SQL, Python, or Excel Office-script for manipulating the tables. Nevertheless, editing tables with irregular structures, particularly those containing merged cells spanning multiple rows, poses a challenge when using code. To address this, we introduce the WikiTableEdit dataset. Leveraging 26,531 tables from the WikiSQL dataset, we automatically generate natural language instructions for six distinct basic operations and the corresponding outcomes, resulting in over 200,000 instances. Subsequently, we evaluate several representative large language models on the WikiTableEdit dataset to demonstrate the challenge of this task. The dataset will be released to the community to promote related researches.","sentences":["Tabular data, as a crucial form of data representation, exists in diverse formats on the Web.","When confronted with complex and irregular tables, manual modification becomes a laborious task.","This paper investigates the performance of Large Language Models (LLMs) in the context of table editing tasks.","Existing research mainly focuses on regular-shaped tables, wherein instructions are used to generate code in SQL, Python, or Excel Office-script for manipulating the tables.","Nevertheless, editing tables with irregular structures, particularly those containing merged cells spanning multiple rows, poses a challenge when using code.","To address this, we introduce the WikiTableEdit dataset.","Leveraging 26,531 tables from the WikiSQL dataset, we automatically generate natural language instructions for six distinct basic operations and the corresponding outcomes, resulting in over 200,000 instances.","Subsequently, we evaluate several representative large language models on the WikiTableEdit dataset to demonstrate the challenge of this task.","The dataset will be released to the community to promote related researches."],"url":"http://arxiv.org/abs/2403.02962v1","category":"cs.AI"}
{"created":"2024-03-05 13:30:02","title":"SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents","abstract":"With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt. Our framework follows the real-world classic court trial process, consisting of court debate simulation, legal information retrieval and judgement refinement to simulate the decision-making of judge. (3) we perform extensive experiments, the results demonstrate that, our framework outperforms the existing advanced methods in various aspects, especially in generating legal grounds, where our model achieves significant improvements of 8.6% and 9.1% F1 score in the first and second instance settings, respectively.","sentences":["With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry.","However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration.","As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence.","In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents.","To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge.","(2) we propose a novel multi-agent framework, AgentsCourt.","Our framework follows the real-world classic court trial process, consisting of court debate simulation, legal information retrieval and judgement refinement to simulate the decision-making of judge.","(3) we perform extensive experiments, the results demonstrate that, our framework outperforms the existing advanced methods in various aspects, especially in generating legal grounds, where our model achieves significant improvements of 8.6% and 9.1% F1 score in the first and second instance settings, respectively."],"url":"http://arxiv.org/abs/2403.02959v1","category":"cs.CL"}
{"created":"2024-03-05 13:25:30","title":"XAI-Based Detection of Adversarial Attacks on Deepfake Detectors","abstract":"We introduce a novel methodology for identifying adversarial attacks on deepfake detectors using eXplainable Artificial Intelligence (XAI). In an era characterized by digital advancement, deepfakes have emerged as a potent tool, creating a demand for efficient detection systems. However, these systems are frequently targeted by adversarial attacks that inhibit their performance. We address this gap, developing a defensible deepfake detector by leveraging the power of XAI. The proposed methodology uses XAI to generate interpretability maps for a given method, providing explicit visualizations of decision-making factors within the AI models. We subsequently employ a pretrained feature extractor that processes both the input image and its corresponding XAI image. The feature embeddings extracted from this process are then used for training a simple yet effective classifier. Our approach contributes not only to the detection of deepfakes but also enhances the understanding of possible adversarial attacks, pinpointing potential vulnerabilities. Furthermore, this approach does not change the performance of the deepfake detector. The paper demonstrates promising results suggesting a potential pathway for future deepfake detection mechanisms. We believe this study will serve as a valuable contribution to the community, sparking much-needed discourse on safeguarding deepfake detectors.","sentences":["We introduce a novel methodology for identifying adversarial attacks on deepfake detectors using eXplainable Artificial Intelligence (XAI).","In an era characterized by digital advancement, deepfakes have emerged as a potent tool, creating a demand for efficient detection systems.","However, these systems are frequently targeted by adversarial attacks that inhibit their performance.","We address this gap, developing a defensible deepfake detector by leveraging the power of XAI.","The proposed methodology uses XAI to generate interpretability maps for a given method, providing explicit visualizations of decision-making factors within the AI models.","We subsequently employ a pretrained feature extractor that processes both the input image and its corresponding XAI image.","The feature embeddings extracted from this process are then used for training a simple yet effective classifier.","Our approach contributes not only to the detection of deepfakes but also enhances the understanding of possible adversarial attacks, pinpointing potential vulnerabilities.","Furthermore, this approach does not change the performance of the deepfake detector.","The paper demonstrates promising results suggesting a potential pathway for future deepfake detection mechanisms.","We believe this study will serve as a valuable contribution to the community, sparking much-needed discourse on safeguarding deepfake detectors."],"url":"http://arxiv.org/abs/2403.02955v1","category":"cs.CR"}
{"created":"2024-03-05 13:23:48","title":"Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation","abstract":"Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions. To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer valuable insights for enhancing the development of LLM-based Text-to-SQL systems.","sentences":["Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods.","Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks.","Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions.","To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs.","Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.","Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task.","These findings offer valuable insights for enhancing the development of LLM-based Text-to-SQL systems."],"url":"http://arxiv.org/abs/2403.02951v2","category":"cs.CL"}
{"created":"2024-03-05 13:21:20","title":"A general approach to enhance the survivability of backdoor attacks by decision path coupling","abstract":"Backdoor attacks have been one of the emerging security threats to deep neural networks (DNNs), leading to serious consequences. One of the mainstream backdoor defenses is model reconstruction-based. Such defenses adopt model unlearning or pruning to eliminate backdoors. However, little attention has been paid to survive from such defenses. To bridge the gap, we propose Venom, the first generic backdoor attack enhancer to improve the survivability of existing backdoor attacks against model reconstruction-based defenses. We formalize Venom as a binary-task optimization problem. The first is the original backdoor attack task to preserve the original attack capability, while the second is the attack enhancement task to improve the attack survivability. To realize the second task, we propose attention imitation loss to force the decision path of poisoned samples in backdoored models to couple with the crucial decision path of benign samples, which makes backdoors difficult to eliminate. Our extensive evaluation on two DNNs and three datasets has demonstrated that Venom significantly improves the survivability of eight state-of-the-art attacks against eight state-of-the-art defenses without impacting the capability of the original attacks.","sentences":["Backdoor attacks have been one of the emerging security threats to deep neural networks (DNNs), leading to serious consequences.","One of the mainstream backdoor defenses is model reconstruction-based.","Such defenses adopt model unlearning or pruning to eliminate backdoors.","However, little attention has been paid to survive from such defenses.","To bridge the gap, we propose Venom, the first generic backdoor attack enhancer to improve the survivability of existing backdoor attacks against model reconstruction-based defenses.","We formalize Venom as a binary-task optimization problem.","The first is the original backdoor attack task to preserve the original attack capability, while the second is the attack enhancement task to improve the attack survivability.","To realize the second task, we propose attention imitation loss to force the decision path of poisoned samples in backdoored models to couple with the crucial decision path of benign samples, which makes backdoors difficult to eliminate.","Our extensive evaluation on two DNNs and three datasets has demonstrated that Venom significantly improves the survivability of eight state-of-the-art attacks against eight state-of-the-art defenses without impacting the capability of the original attacks."],"url":"http://arxiv.org/abs/2403.02950v1","category":"cs.AI"}
{"created":"2024-03-05 13:17:09","title":"SAFFIRA: a Framework for Assessing the Reliability of Systolic-Array-Based DNN Accelerators","abstract":"Systolic array has emerged as a prominent architecture for Deep Neural Network (DNN) hardware accelerators, providing high-throughput and low-latency performance essential for deploying DNNs across diverse applications. However, when used in safety-critical applications, reliability assessment is mandatory to guarantee the correct behavior of DNN accelerators. While fault injection stands out as a well-established practical and robust method for reliability assessment, it is still a very time-consuming process. This paper addresses the time efficiency issue by introducing a novel hierarchical software-based hardware-aware fault injection strategy tailored for systolic array-based DNN accelerators.","sentences":["Systolic array has emerged as a prominent architecture for Deep Neural Network (DNN) hardware accelerators, providing high-throughput and low-latency performance essential for deploying DNNs across diverse applications.","However, when used in safety-critical applications, reliability assessment is mandatory to guarantee the correct behavior of DNN accelerators.","While fault injection stands out as a well-established practical and robust method for reliability assessment, it is still a very time-consuming process.","This paper addresses the time efficiency issue by introducing a novel hierarchical software-based hardware-aware fault injection strategy tailored for systolic array-based DNN accelerators."],"url":"http://arxiv.org/abs/2403.02946v1","category":"cs.AI"}
{"created":"2024-03-05 13:16:37","title":"Unsupervised Learning Approaches for Identifying ICU Patient Subgroups: Do Results Generalise?","abstract":"The use of unsupervised learning to identify patient subgroups has emerged as a potentially promising direction to improve the efficiency of Intensive Care Units (ICUs). By identifying subgroups of patients with similar levels of medical resource need, ICUs could be restructured into a collection of smaller subunits, each catering to a specific group. However, it is unclear whether common patient subgroups exist across different ICUs, which would determine whether ICU restructuring could be operationalised in a standardised manner. In this paper, we tested the hypothesis that common ICU patient subgroups exist by examining whether the results from one existing study generalise to a different dataset. We extracted 16 features representing medical resource need and used consensus clustering to derive patient subgroups, replicating the previous study. We found limited similarities between our results and those of the previous study, providing evidence against the hypothesis. Our findings imply that there is significant variation between ICUs; thus, a standardised restructuring approach is unlikely to be appropriate. Instead, potential efficiency gains might be greater when the number and nature of the subunits are tailored to each ICU individually.","sentences":["The use of unsupervised learning to identify patient subgroups has emerged as a potentially promising direction to improve the efficiency of Intensive Care Units (ICUs).","By identifying subgroups of patients with similar levels of medical resource need, ICUs could be restructured into a collection of smaller subunits, each catering to a specific group.","However, it is unclear whether common patient subgroups exist across different ICUs, which would determine whether ICU restructuring could be operationalised in a standardised manner.","In this paper, we tested the hypothesis that common ICU patient subgroups exist by examining whether the results from one existing study generalise to a different dataset.","We extracted 16 features representing medical resource need and used consensus clustering to derive patient subgroups, replicating the previous study.","We found limited similarities between our results and those of the previous study, providing evidence against the hypothesis.","Our findings imply that there is significant variation between ICUs; thus, a standardised restructuring approach is unlikely to be appropriate.","Instead, potential efficiency gains might be greater when the number and nature of the subunits are tailored to each ICU individually."],"url":"http://arxiv.org/abs/2403.02945v1","category":"cs.LG"}
{"created":"2024-03-05 13:10:06","title":"PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers","abstract":"With the rapid growth of scholarly archives, researchers subscribe to \"paper alert\" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.","sentences":["With the rapid growth of scholarly archives, researchers subscribe to \"paper alert\" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers.","However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts.","To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers.","PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects.","Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers."],"url":"http://arxiv.org/abs/2403.02939v1","category":"cs.DL"}
{"created":"2024-03-05 13:08:52","title":"AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models","abstract":"Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension. To further utilize this capability, systems that automatically adjust the playback speed according to the user's condition and the type of content to assist in more efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility. The system uses the speech recognizer score as a proxy for how well a human can hear a certain unit of speech and maximizes the speech playback speed to the extent that a human can hear. This method can be used to produce fast but intelligible speech. In the evaluation experiment, we compared the speech played back at a constant fast speed and the flexibly speed-up speech generated by the proposed method in a blind test and confirmed that the proposed method produced speech that was easier to listen to.","sentences":["Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension.","To further utilize this capability, systems that automatically adjust the playback speed according to the user's condition and the type of content to assist in more efficient comprehension of time-series content have been developed.","However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans.","In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility.","The system uses the speech recognizer score as a proxy for how well a human can hear a certain unit of speech and maximizes the speech playback speed to the extent that a human can hear.","This method can be used to produce fast but intelligible speech.","In the evaluation experiment, we compared the speech played back at a constant fast speed and the flexibly speed-up speech generated by the proposed method in a blind test and confirmed that the proposed method produced speech that was easier to listen to."],"url":"http://arxiv.org/abs/2403.02938v1","category":"cs.CL"}
{"created":"2024-03-05 13:03:31","title":"AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN Accelerators","abstract":"In this paper, we propose an architecture of a novel adaptive fault-tolerant approximate multiplier tailored for ASIC-based DNN accelerators.","sentences":["In this paper, we propose an architecture of a novel adaptive fault-tolerant approximate multiplier tailored for ASIC-based DNN accelerators."],"url":"http://arxiv.org/abs/2403.02936v1","category":"cs.AI"}
{"created":"2024-03-05 18:58:39","title":"A Safety-Critical Framework for UGVs in Complex Environments: A Data-Driven Discrepancy-Aware Approach","abstract":"This work presents a novel data-driven multi-layered planning and control framework for the safe navigation of a class of unmanned ground vehicles (UGVs) in the presence of unknown stationary obstacles and additive modeling uncertainties. The foundation of this framework is a novel robust model predictive planner, designed to generate optimal collision-free trajectories given an occupancy grid map, and a paired ancillary controller, augmented to provide robustness against model uncertainties extracted from learning data.   To tackle modeling discrepancies, we identify both matched (input discrepancies) and unmatched model residuals between the true and the nominal reduced-order models using closed-loop tracking errors as training data. Utilizing conformal prediction, we extract probabilistic upper bounds for the unknown model residuals, which serve to construct a robustifying ancillary controller. Further, we also determine maximum tracking discrepancies, also known as the robust control invariance tube, under the augmented policy, formulating them as collision buffers. Employing a LiDAR-based occupancy map to characterize the environment, we construct a discrepancy-aware cost map that incorporates these collision buffers. This map is then integrated into a sampling-based model predictive path planner that generates optimal and safe trajectories that can be robustly tracked by the augmented ancillary controller in the presence of model mismatches.   The effectiveness of the framework is experimentally validated for autonomous high-speed trajectory tracking in a cluttered environment with four different vehicle-terrain configurations. We also showcase the framework's versatility by reformulating it as a driver-assist program, providing collision avoidance corrections based on user joystick commands.","sentences":["This work presents a novel data-driven multi-layered planning and control framework for the safe navigation of a class of unmanned ground vehicles (UGVs) in the presence of unknown stationary obstacles and additive modeling uncertainties.","The foundation of this framework is a novel robust model predictive planner, designed to generate optimal collision-free trajectories given an occupancy grid map, and a paired ancillary controller, augmented to provide robustness against model uncertainties extracted from learning data.   ","To tackle modeling discrepancies, we identify both matched (input discrepancies) and unmatched model residuals between the true and the nominal reduced-order models using closed-loop tracking errors as training data.","Utilizing conformal prediction, we extract probabilistic upper bounds for the unknown model residuals, which serve to construct a robustifying ancillary controller.","Further, we also determine maximum tracking discrepancies, also known as the robust control invariance tube, under the augmented policy, formulating them as collision buffers.","Employing a LiDAR-based occupancy map to characterize the environment, we construct a discrepancy-aware cost map that incorporates these collision buffers.","This map is then integrated into a sampling-based model predictive path planner that generates optimal and safe trajectories that can be robustly tracked by the augmented ancillary controller in the presence of model mismatches.   ","The effectiveness of the framework is experimentally validated for autonomous high-speed trajectory tracking in a cluttered environment with four different vehicle-terrain configurations.","We also showcase the framework's versatility by reformulating it as a driver-assist program, providing collision avoidance corrections based on user joystick commands."],"url":"http://arxiv.org/abs/2403.03215v1","category":"cs.RO"}
{"created":"2024-03-05 18:45:39","title":"Scaling Rectified Flow Transformers for High-Resolution Image Synthesis","abstract":"Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.","sentences":["Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos.","Rectified flow is a recent generative model formulation that connects data and noise in a straight line.","Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice.","In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales.","Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis.","Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings.","We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations.","Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available."],"url":"http://arxiv.org/abs/2403.03206v1","category":"cs.CV"}
{"created":"2024-03-05 18:38:10","title":"Site Symmetry and Multiorbital Flat Bands on Kagome and Pyrochlore Lattices","abstract":"Flat bands in electronic band structures are intriguing platforms for strong correlation and topological physics, primarily due to the suppressed kinetic energy of electrons. Various methods have been developed to create flat bands, utilizing lattice geometry or finely tuned parameters. Despite this, the investigation of orbital symmetry in multiorbital materials is a relatively new area of focus. In this work, we propose a site symmetry based systematic approach to emerging multiorbital flat bands in lattices made of corner-connecting motifs such as the kagome and pyrochlore lattices. As a conceptual advance, the one-orbital flat bands are shown to originate as mutual eigenstates of isolated molecular motifs. Further developing the mutual eigenstate method for multiorbitals transforming differently under the site symmetries such as mirror and inversion, we derive multiorbital flat bands from the skew-symmetric interorbital Hamiltonian and introduce an isolated molecule enabled group-theoretic description of the flat band wavefunctions. Realizations of the multiorbital flatbands in relevant materials are shown to be possible under the Slater-Koster formalism. Our findings provide new directions for exploring flatband electronic structures for novel correlated and topological quantum states.","sentences":["Flat bands in electronic band structures are intriguing platforms for strong correlation and topological physics, primarily due to the suppressed kinetic energy of electrons.","Various methods have been developed to create flat bands, utilizing lattice geometry or finely tuned parameters.","Despite this, the investigation of orbital symmetry in multiorbital materials is a relatively new area of focus.","In this work, we propose a site symmetry based systematic approach to emerging multiorbital flat bands in lattices made of corner-connecting motifs such as the kagome and pyrochlore lattices.","As a conceptual advance, the one-orbital flat bands are shown to originate as mutual eigenstates of isolated molecular motifs.","Further developing the mutual eigenstate method for multiorbitals transforming differently under the site symmetries such as mirror and inversion, we derive multiorbital flat bands from the skew-symmetric interorbital Hamiltonian and introduce an isolated molecule enabled group-theoretic description of the flat band wavefunctions.","Realizations of the multiorbital flatbands in relevant materials are shown to be possible under the Slater-Koster formalism.","Our findings provide new directions for exploring flatband electronic structures for novel correlated and topological quantum states."],"url":"http://arxiv.org/abs/2403.03201v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 18:37:53","title":"Concavity Properties of Solutions of Elliptic Equations under Conformal Deformations","abstract":"We study the Dirichlet problem for the weighted Schr\\\"odinger operator \\[-\\Delta u +Vu = \\lambda \\rho u,\\] where $\\rho$ is a positive weighting function and $V$ is a potential. Such equations appear naturally in conformal geometry and in the composite membrane problem. Our primary goal is to establish concavity estimates for the principle eigenfunction with respect to conformal connections. Doing so, we obtain new bounds on the fundamental gap problem, which is the difference between the first and second eigenvalues. In particular, we partially resolve a conjecture of Nguyen, Stancu and Wei [IMRN 2022] on the fundamental gap of horoconvex domains. In addition, we obtain a power convexity estimate for solutions to the torsion problem in spherical geometry on convex domains which are not too large.","sentences":["We study the Dirichlet problem for the weighted Schr\\\"odinger operator \\[-\\Delta u +Vu = \\lambda \\rho u,\\] where $\\rho$ is a positive weighting function and $V$ is a potential.","Such equations appear naturally in conformal geometry and in the composite membrane problem.","Our primary goal is to establish concavity estimates for the principle eigenfunction with respect to conformal connections.","Doing so, we obtain new bounds on the fundamental gap problem, which is the difference between the first and second eigenvalues.","In particular, we partially resolve a conjecture of Nguyen, Stancu and Wei [IMRN 2022] on the fundamental gap of horoconvex domains.","In addition, we obtain a power convexity estimate for solutions to the torsion problem in spherical geometry on convex domains which are not too large."],"url":"http://arxiv.org/abs/2403.03200v1","category":"math.DG"}
{"created":"2024-03-05 18:33:32","title":"Metallic mean Wang tiles II: the dynamics of an aperiodic computer chip","abstract":"We consider a new family $(\\mathcal{T}_n)_{n\\geq1}$ of aperiodic sets of Wang tiles and we describe the dynamical properties of the set $\\Omega_n$ of valid configurations $\\mathbb{Z}^2\\to\\mathcal{T}_n$. The tiles can be defined as the different instances of a square shape computer chip whose inputs and outputs are 3-dimensional integer vectors. The family include the Ammann aperiodic set of 16 Wang tiles and gathers the hallmarks of other small aperiodic sets of Wang tiles. Notably, the tiles satisfy additive versions of equations verified by the Kari--Culik aperiodic sets of 14 and 13 Wang tiles. Also configurations in $\\Omega_n$ are the codings of a $\\mathbb{Z}^2$-action on a 2-dimensional torus like the Jeandel--Rao aperiodic set of 11 Wang tiles. The family broadens the relation between quadratic integers and aperiodic tilings beyond the omnipresent golden ratio as the dynamics of $\\Omega_n$ involves the positive root $\\beta$ of the polynomial $x^2-nx-1$, also known as the $n$-th metallic mean. We show the existence of an almost one-to-one factor map $\\Omega_n\\to\\mathbb{T}^2$ which commutes the shift action on $\\Omega_n$ with horizontal and vertical translations by $\\beta$ on $\\mathbb{T}^2$. The factor map can be explicitely defined by the average of the top labels from the same row of tiles as in Kari and Culik examples. The proofs are based on the minimality of $\\Omega_n$ (proved in a previous article) and a polygonal partition of $\\mathbb{T}^2$ which we show is a Markov partition for the toral $\\mathbb{Z}^2$-action. The partition and the sets of Wang tiles are symmetric which makes them, like Penrose tilings, worthy of investigation.","sentences":["We consider a new family $(\\mathcal{T}_n)_{n\\geq1}$ of aperiodic sets of Wang tiles and we describe the dynamical properties of the set $\\Omega_n$ of valid configurations $\\mathbb{Z}^2\\to\\mathcal{T}_n$. The tiles can be defined as the different instances of a square shape computer chip whose inputs and outputs are 3-dimensional integer vectors.","The family include the Ammann aperiodic set of 16 Wang tiles and gathers the hallmarks of other small aperiodic sets of Wang tiles.","Notably, the tiles satisfy additive versions of equations verified by the Kari--Culik aperiodic sets of 14 and 13 Wang tiles.","Also configurations in $\\Omega_n$ are the codings of a $\\mathbb{Z}^2$-action on a 2-dimensional torus like the Jeandel--Rao aperiodic set of 11 Wang tiles.","The family broadens the relation between quadratic integers and aperiodic tilings beyond the omnipresent golden ratio as the dynamics of $\\Omega_n$ involves the positive root $\\beta$ of the polynomial $x^2-nx-1$, also known as the $n$-th metallic mean.","We show the existence of an almost one-to-one factor map $\\Omega_n\\to\\mathbb{T}^2$ which commutes the shift action on $\\Omega_n$ with horizontal and vertical translations by $\\beta$ on $\\mathbb{T}^2$. The factor map can be explicitely defined by the average of the top labels from the same row of tiles as in Kari and Culik examples.","The proofs are based on the minimality of $\\Omega_n$ (proved in a previous article) and a polygonal partition of $\\mathbb{T}^2$ which we show is a Markov partition for the toral $\\mathbb{Z}^2$-action.","The partition and the sets of Wang tiles are symmetric which makes them, like Penrose tilings, worthy of investigation."],"url":"http://arxiv.org/abs/2403.03197v1","category":"math.DS"}
{"created":"2024-03-05 18:32:00","title":"SmartSantander: IoT Experimentation over a Smart City Testbed","abstract":"This paper describes the deployment and experimentation architecture of the Internet of Things experimentation facility being deployed at Santander city. The facility is implemented within the SmartSantander project, one of the projects of the Future Internet Research and Experimentation initiative of the European Commission and represents a unique in the world city-scale experimental research facility. Additionally, this facility supports typical applications and services of a smart city. Tangible results are expected to influence the definition and specification of Future Internet architecture design from viewpoints of Internet of Things and Internet of Services. The facility comprises a large number of Internet of Things devices deployed in several urban scenarios which will be federated into a single testbed. In this paper the deployment being carried out at the main location, namely Santander city, is described. Besides presenting the current deployment, in this article the main insights in terms of the architectural design of a large-scale IoT testbed are presented as well. Furthermore, solutions adopted for implementation of the different components addressing the required testbed functionalities are also sketched out. The IoT experimentation facility described in this paper is conceived to provide a suitable platform for large scale experimentation and evaluation of IoT concepts under real-life conditions.","sentences":["This paper describes the deployment and experimentation architecture of the Internet of Things experimentation facility being deployed at Santander city.","The facility is implemented within the SmartSantander project, one of the projects of the Future Internet Research and Experimentation initiative of the European Commission and represents a unique in the world city-scale experimental research facility.","Additionally, this facility supports typical applications and services of a smart city.","Tangible results are expected to influence the definition and specification of Future Internet architecture design from viewpoints of Internet of Things and Internet of Services.","The facility comprises a large number of Internet of Things devices deployed in several urban scenarios which will be federated into a single testbed.","In this paper the deployment being carried out at the main location, namely Santander city, is described.","Besides presenting the current deployment, in this article the main insights in terms of the architectural design of a large-scale IoT testbed are presented as well.","Furthermore, solutions adopted for implementation of the different components addressing the required testbed functionalities are also sketched out.","The IoT experimentation facility described in this paper is conceived to provide a suitable platform for large scale experimentation and evaluation of IoT concepts under real-life conditions."],"url":"http://arxiv.org/abs/2403.03196v1","category":"cs.NI"}
{"created":"2024-03-05 18:31:32","title":"Concentration-compactness via profile decomposition for systems of coupled Schr\u00f6dinger equations of Hamiltonian type","abstract":"We analyse Hamiltonian-type systems of second-order elliptic PDE invariant under a non-compact group and, consequently, involve a lack of compactness of the Sobolev embedding. We show that the loss of compactness can be compensated by using a concentration-compactness principle via weak profile decomposition for bounded Palais-Smale sequences in Banach spaces. Our analysis to prove the existence of ground states involves a reduction by the inversion method of the system to a fourth-order equation combined with a variational principle of a minimax nature. Among other results, including regularity and a Pohozaev-type identity, we also prove the non-existence of weak solutions for a class of Lane-Emden systems.","sentences":["We analyse Hamiltonian-type systems of second-order elliptic PDE invariant under a non-compact group and, consequently, involve a lack of compactness of the Sobolev embedding.","We show that the loss of compactness can be compensated by using a concentration-compactness principle via weak profile decomposition for bounded Palais-Smale sequences in Banach spaces.","Our analysis to prove the existence of ground states involves a reduction by the inversion method of the system to a fourth-order equation combined with a variational principle of a minimax nature.","Among other results, including regularity and a Pohozaev-type identity, we also prove the non-existence of weak solutions for a class of Lane-Emden systems."],"url":"http://arxiv.org/abs/2403.03195v1","category":"math.AP"}
{"created":"2024-03-05 18:19:55","title":"On the computation of stable coupled state-space models for dynamic substructuring applications","abstract":"This paper aims at introducing a methodology to compute stable coupled state-space models for dynamic substructuring applications by introducing two novel approaches targeted to accomplish this task: a) a procedure to impose Newtons's second law without relying on the use of undamped RCMs (residual compensation modes) and b) a novel approach to impose stability on unstable coupled state-space models. The enforcement of stability is performed by dividing the unstable model into two different models, one composed by the stable poles (stable model) and the other composed by the unstable ones (unstable model). Then, the poles of the unstable state-space model are forced to be stable, leading to the computation of a stabilized state-space model. Afterwards, to make sure that the Frequency Response Functions (FRFs) of the stabilized model well match the FRFs of the unstable model, the Least-Squares Frequency Domain (LSFD) method is exploited to update the modal parameters of the stabilized model composed by the pairs of complex conjugate poles. The validity of the proposed methodologies is presented and discussed by exploiting experimental data. Indeed, by exploiting the FRFs of a real system, accurate state-space models respecting Newton's second law are computed. Then, decoupling and coupling operations are performed with the identified state-space models, no matter the models resultant from the decoupling/coupling operations are unstable. Stability is then imposed on the computed unstable coupled model by following the approach proposed in this paper. The methodology proved to work well on these data. Moreover, the paper also shows that the coupled state-space models obtained using this methodology are suitable to be exploited in time-domain analyses and simulations.","sentences":["This paper aims at introducing a methodology to compute stable coupled state-space models for dynamic substructuring applications by introducing two novel approaches targeted to accomplish this task: a) a procedure to impose Newtons's second law without relying on the use of undamped RCMs (residual compensation modes) and b) a novel approach to impose stability on unstable coupled state-space models.","The enforcement of stability is performed by dividing the unstable model into two different models, one composed by the stable poles (stable model) and the other composed by the unstable ones (unstable model).","Then, the poles of the unstable state-space model are forced to be stable, leading to the computation of a stabilized state-space model.","Afterwards, to make sure that the Frequency Response Functions (FRFs) of the stabilized model well match the FRFs of the unstable model, the Least-Squares Frequency Domain (LSFD) method is exploited to update the modal parameters of the stabilized model composed by the pairs of complex conjugate poles.","The validity of the proposed methodologies is presented and discussed by exploiting experimental data.","Indeed, by exploiting the FRFs of a real system, accurate state-space models respecting Newton's second law are computed.","Then, decoupling and coupling operations are performed with the identified state-space models, no matter the models resultant from the decoupling/coupling operations are unstable.","Stability is then imposed on the computed unstable coupled model by following the approach proposed in this paper.","The methodology proved to work well on these data.","Moreover, the paper also shows that the coupled state-space models obtained using this methodology are suitable to be exploited in time-domain analyses and simulations."],"url":"http://arxiv.org/abs/2403.03182v1","category":"eess.SY"}
{"created":"2024-03-05 18:16:49","title":"Quantum 2D Liouville Path-Integral Is a Sum over Geometries in AdS$_3$ Einstein Gravity","abstract":"There is a renowned solution of the modular bootstrap that defines the UV complete quantum Liouville theory. We triangulate the path-integral of this Liouville CFT on any 2D surface $\\mathcal{M}$, by proposing a shrinkable boundary condition for this special CFT that allows small holes to close, analogous to the proposal in rational CFTs [1-3]. This is essentially a tensor network that admits an interpretation of a state-sum of a 3D topological theory constructed with quantum 6j symbols of $\\mathcal{U}_q(SL(2,\\mathbb{R}))$ with non-trivial boundary conditions, and it reduces to a sum over 3D geometries weighted by the Einstein-Hilbert action to leading order in large $c$. The boundary conditions of quantum Liouville theory specifies a very special sum over bulk geometries to faithfully reproduce the CFT path-integral. The triangulation coincides with producing a network of geodesics in the AdS bulk, which can be changed making use of the pentagon identity and orthogonality condition satisfied by the 6j symbols, and arranged into a precise holographic tensor network.","sentences":["There is a renowned solution of the modular bootstrap that defines the UV complete quantum Liouville theory.","We triangulate the path-integral of this Liouville CFT on any 2D surface $\\mathcal{M}$, by proposing a shrinkable boundary condition for this special CFT that allows small holes to close, analogous to the proposal in rational CFTs [1-3].","This is essentially a tensor network that admits an interpretation of a state-sum of a 3D topological theory constructed with quantum 6j symbols of $\\mathcal{U}_q(SL(2,\\mathbb{R}))$ with non-trivial boundary conditions, and it reduces to a sum over 3D geometries weighted by the Einstein-Hilbert action to leading order in large $c$. The boundary conditions of quantum Liouville theory specifies a very special sum over bulk geometries to faithfully reproduce the CFT path-integral.","The triangulation coincides with producing a network of geodesics in the AdS bulk, which can be changed making use of the pentagon identity and orthogonality condition satisfied by the 6j symbols, and arranged into a precise holographic tensor network."],"url":"http://arxiv.org/abs/2403.03179v1","category":"hep-th"}
{"created":"2024-03-05 17:58:32","title":"Hybrid data assimilation techniques using the adjoint method in a coupled Lorenz system","abstract":"A hybrid 4D-variational data assimilation method for numerical climate models is introduced using the Lorenz '63 model. This new approach has the potential to optimise a high complexity Earth system model (ESM) by utilising the adjoint equations of an intermediate complexity ESM. The method is conceptually demonstrated by consecutively synchronising two Lorenz '63 systems to observations before optimisation. The first represents a 'high complexity' model and the second an 'intermediate complexity' model which has adjoint equations. This method will save computational power for a full ESM and has negligible error and uncertainty change compared to the optimisation of a single model with adjoint equations. A similar setup can be applied to sparse observations. An alternative assimilation setup, with two identical models, is used to filter noisy data. This reduces optimised parametric model uncertainty by approximately one third. Such a precision gain could prove valuable for seasonal, annual, and decadal predictions.","sentences":["A hybrid 4D-variational data assimilation method for numerical climate models is introduced using the Lorenz '63 model.","This new approach has the potential to optimise a high complexity Earth system model (ESM) by utilising the adjoint equations of an intermediate complexity ESM.","The method is conceptually demonstrated by consecutively synchronising two Lorenz '63 systems to observations before optimisation.","The first represents a 'high complexity' model and the second an 'intermediate complexity' model which has adjoint equations.","This method will save computational power for a full ESM and has negligible error and uncertainty change compared to the optimisation of a single model with adjoint equations.","A similar setup can be applied to sparse observations.","An alternative assimilation setup, with two identical models, is used to filter noisy data.","This reduces optimised parametric model uncertainty by approximately one third.","Such a precision gain could prove valuable for seasonal, annual, and decadal predictions."],"url":"http://arxiv.org/abs/2403.03166v1","category":"physics.ao-ph"}
{"created":"2024-03-05 17:49:09","title":"Rethinking Clustered Federated Learning in NOMA Enhanced Wireless Networks","abstract":"This study explores the benefits of integrating the novel clustered federated learning (CFL) approach with non-orthogonal multiple access (NOMA) under non-independent and identically distributed (non-IID) datasets, where multiple devices participate in the aggregation with time limitations and a finite number of sub-channels. A detailed theoretical analysis of the generalization gap that measures the degree of non-IID in the data distribution is presented. Following that, solutions to address the challenges posed by non-IID conditions are proposed with the analysis of the properties. Specifically, users' data distributions are parameterized as concentration parameters and grouped using spectral clustering, with Dirichlet distribution serving as the prior. The investigation into the generalization gap and convergence rate guides the design of sub-channel assignments through the matching-based algorithm, and the power allocation is achieved by Karush-Kuhn-Tucker (KKT) conditions with the derived closed-form solution. The extensive simulation results show that the proposed cluster-based FL framework can outperform FL baselines in terms of both test accuracy and convergence rate. Moreover, jointly optimizing sub-channel and power allocation in NOMA-enhanced networks can lead to a significant improvement.","sentences":["This study explores the benefits of integrating the novel clustered federated learning (CFL) approach with non-orthogonal multiple access (NOMA) under non-independent and identically distributed (non-IID) datasets, where multiple devices participate in the aggregation with time limitations and a finite number of sub-channels.","A detailed theoretical analysis of the generalization gap that measures the degree of non-IID in the data distribution is presented.","Following that, solutions to address the challenges posed by non-IID conditions are proposed with the analysis of the properties.","Specifically, users' data distributions are parameterized as concentration parameters and grouped using spectral clustering, with Dirichlet distribution serving as the prior.","The investigation into the generalization gap and convergence rate guides the design of sub-channel assignments through the matching-based algorithm, and the power allocation is achieved by Karush-Kuhn-Tucker (KKT) conditions with the derived closed-form solution.","The extensive simulation results show that the proposed cluster-based FL framework can outperform FL baselines in terms of both test accuracy and convergence rate.","Moreover, jointly optimizing sub-channel and power allocation in NOMA-enhanced networks can lead to a significant improvement."],"url":"http://arxiv.org/abs/2403.03157v1","category":"cs.NI"}
{"created":"2024-03-05 17:33:47","title":"On dynamics of gasless combustion in slowly varying periodic media: periodic fronts, their stability and propagation-extinction-diffusion-reignition pattern","abstract":"In this paper we consider a classical model of gasless combustion in a one dimensional formulation under the assumption of ignition temperature kinetics. We study the propagation of flame fronts in this model when the initial distribution of the solid fuel is a spatially periodic function that varies on a large scale. It is shown that in certain parametric regimes the model supports periodic traveling fronts. An accurate asymptotic formula for the velocity of the flame front is derived and studied. The stability of periodic fronts is also explored, and a critical condition in terms of parameters of the problem is derived. It is also shown that the instability of periodic fronts, in a certain parametric regimes, results in a propagation-extinction-diffusion-reignition pattern which is studied numerically.","sentences":["In this paper we consider a classical model of gasless combustion in a one dimensional formulation under the assumption of ignition temperature kinetics.","We study the propagation of flame fronts in this model when the initial distribution of the solid fuel is a spatially periodic function that varies on a large scale.","It is shown that in certain parametric regimes the model supports periodic traveling fronts.","An accurate asymptotic formula for the velocity of the flame front is derived and studied.","The stability of periodic fronts is also explored, and a critical condition in terms of parameters of the problem is derived.","It is also shown that the instability of periodic fronts, in a certain parametric regimes, results in a propagation-extinction-diffusion-reignition pattern which is studied numerically."],"url":"http://arxiv.org/abs/2403.03144v1","category":"nlin.PS"}
{"created":"2024-03-05 17:27:05","title":"Using Smartphones to Study Vaccination Decisions in the Wild","abstract":"One of the most important tools available to limit the spread and impact of infectious diseases is vaccination. It is therefore important to understand what factors determine people's vaccination decisions. To this end, previous behavioural research made use of, (i) controlled but often abstract or hypothetical studies (e.g., vignettes) or, (ii) realistic but typically less flexible studies that make it difficult to understand individual decision processes (e.g., clinical trials). Combining the best of these approaches, we propose integrating real-world Bluetooth contacts via smartphones in several rounds of a game scenario, as a novel methodology to study vaccination decisions and disease spread. In our 12-week proof-of-concept study conducted with $N$ = 494 students, we found that participants strongly responded to some of the information provided to them during or after each decision round, particularly those related to their individual health outcomes. In contrast, information related to others' decisions and outcomes (e.g., the number of vaccinated or infected individuals) appeared to be less important. We discuss the potential of this novel method and point to fruitful areas for future research.","sentences":["One of the most important tools available to limit the spread and impact of infectious diseases is vaccination.","It is therefore important to understand what factors determine people's vaccination decisions.","To this end, previous behavioural research made use of, (i) controlled but often abstract or hypothetical studies (e.g., vignettes) or, (ii) realistic but typically less flexible studies that make it difficult to understand individual decision processes (e.g., clinical trials).","Combining the best of these approaches, we propose integrating real-world Bluetooth contacts via smartphones in several rounds of a game scenario, as a novel methodology to study vaccination decisions and disease spread.","In our 12-week proof-of-concept study conducted with $N$ = 494 students, we found that participants strongly responded to some of the information provided to them during or after each decision round, particularly those related to their individual health outcomes.","In contrast, information related to others' decisions and outcomes (e.g., the number of vaccinated or infected individuals) appeared to be less important.","We discuss the potential of this novel method and point to fruitful areas for future research."],"url":"http://arxiv.org/abs/2403.03143v1","category":"physics.soc-ph"}
{"created":"2024-03-05 18:59:51","title":"FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation","abstract":"Estimating relative camera poses between images has been a central problem in computer vision. Methods that find correspondences and solve for the fundamental matrix offer high precision in most cases. Conversely, methods predicting pose directly using neural networks are more robust to limited overlap and can infer absolute translation scale, but at the expense of reduced precision. We show how to combine the best of both methods; our approach yields results that are both precise and robust, while also accurately inferring translation scales. At the heart of our model lies a Transformer that (1) learns to balance between solved and learned pose estimations, and (2) provides a prior to guide a solver. A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free Relocalization.","sentences":["Estimating relative camera poses between images has been a central problem in computer vision.","Methods that find correspondences and solve for the fundamental matrix offer high precision in most cases.","Conversely, methods predicting pose directly using neural networks are more robust to limited overlap and can infer absolute translation scale, but at the expense of reduced precision.","We show how to combine the best of both methods; our approach yields results that are both precise and robust, while also accurately inferring translation scales.","At the heart of our model lies a Transformer that (1) learns to balance between solved and learned pose estimations, and (2) provides a prior to guide a solver.","A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free Relocalization."],"url":"http://arxiv.org/abs/2403.03221v1","category":"cs.CV"}
{"created":"2024-03-05 17:54:22","title":"PalmProbNet: A Probabilistic Approach to Understanding Palm Distributions in Ecuadorian Tropical Forest via Transfer Learning","abstract":"Palms play an outsized role in tropical forests and are important resources for humans and wildlife. A central question in tropical ecosystems is understanding palm distribution and abundance. However, accurately identifying and localizing palms in geospatial imagery presents significant challenges due to dense vegetation, overlapping canopies, and variable lighting conditions in mixed-forest landscapes. Addressing this, we introduce PalmProbNet, a probabilistic approach utilizing transfer learning to analyze high-resolution UAV-derived orthomosaic imagery, enabling the detection of palm trees within the dense canopy of the Ecuadorian Rainforest. This approach represents a substantial advancement in automated palm detection, effectively pinpointing palm presence and locality in mixed tropical rainforests. Our process begins by generating an orthomosaic image from UAV images, from which we extract and label palm and non-palm image patches in two distinct sizes. These patches are then used to train models with an identical architecture, consisting of an unaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with specifically trained parameters. Subsequently, PalmProbNet employs a sliding window technique on the landscape orthomosaic, using both small and large window sizes to generate a probability heatmap. This heatmap effectively visualizes the distribution of palms, showcasing the scalability and adaptability of our approach in various forest densities. Despite the challenging terrain, our method demonstrated remarkable performance, achieving an accuracy of 97.32% and a Cohen's kappa of 94.59% in testing.","sentences":["Palms play an outsized role in tropical forests and are important resources for humans and wildlife.","A central question in tropical ecosystems is understanding palm distribution and abundance.","However, accurately identifying and localizing palms in geospatial imagery presents significant challenges due to dense vegetation, overlapping canopies, and variable lighting conditions in mixed-forest landscapes.","Addressing this, we introduce PalmProbNet, a probabilistic approach utilizing transfer learning to analyze high-resolution UAV-derived orthomosaic imagery, enabling the detection of palm trees within the dense canopy of the Ecuadorian Rainforest.","This approach represents a substantial advancement in automated palm detection, effectively pinpointing palm presence and locality in mixed tropical rainforests.","Our process begins by generating an orthomosaic image from UAV images, from which we extract and label palm and non-palm image patches in two distinct sizes.","These patches are then used to train models with an identical architecture, consisting of an unaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with specifically trained parameters.","Subsequently, PalmProbNet employs a sliding window technique on the landscape orthomosaic, using both small and large window sizes to generate a probability heatmap.","This heatmap effectively visualizes the distribution of palms, showcasing the scalability and adaptability of our approach in various forest densities.","Despite the challenging terrain, our method demonstrated remarkable performance, achieving an accuracy of 97.32% and a Cohen's kappa of 94.59% in testing."],"url":"http://arxiv.org/abs/2403.03161v1","category":"cs.CV"}
{"created":"2024-03-05 17:41:35","title":"Robust Federated Learning Mitigates Client-side Training Data Distribution Inference Attacks","abstract":"Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data. While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack.   In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks. In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives. A client's model update is considered malicious if it significantly deviates from the computed median update. We conduct a thorough evaluation of our proposed InferGuard on five benchmark datasets and perform a comparison with ten baseline methods. The results of our experiments indicate that our defense mechanism is highly effective in protecting against client-side training data distribution inference attacks, even against strong adaptive attacks. Furthermore, our method substantially outperforms the baseline methods in various practical FL scenarios.","sentences":["Recent studies have revealed that federated learning (FL), once considered secure due to clients not sharing their private data with the server, is vulnerable to attacks such as client-side training data distribution inference, where a malicious client can recreate the victim's data.","While various countermeasures exist, they are not practical, often assuming server access to some training data or knowledge of label distribution before the attack.   ","In this work, we bridge the gap by proposing InferGuard, a novel Byzantine-robust aggregation rule aimed at defending against client-side training data distribution inference attacks.","In our proposed InferGuard, the server first calculates the coordinate-wise median of all the model updates it receives.","A client's model update is considered malicious if it significantly deviates from the computed median update.","We conduct a thorough evaluation of our proposed InferGuard on five benchmark datasets and perform a comparison with ten baseline methods.","The results of our experiments indicate that our defense mechanism is highly effective in protecting against client-side training data distribution inference attacks, even against strong adaptive attacks.","Furthermore, our method substantially outperforms the baseline methods in various practical FL scenarios."],"url":"http://arxiv.org/abs/2403.03149v1","category":"cs.CR"}
{"created":"2024-03-05 17:26:41","title":"Language Guided Exploration for RL Agents in Text Environments","abstract":"Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts. In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer.","sentences":["Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents.","Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts.","In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER).","We observe that on ScienceWorld (Wang et al.,2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer."],"url":"http://arxiv.org/abs/2403.03141v1","category":"cs.CL"}
{"created":"2024-03-05 17:20:50","title":"Isostructural Softening of Vulcanized Nanocomposites","abstract":"Following a previous work evidencing that short poly-propylene glycol (PPG) chains incorporated to crude SBR-silica nanocomposites act as filler-network softeners without changing their structure, we propose in the present letter to examine more operative cross-linked materials. We first evidence that the adsorption of PPG onto silica deactivates progressively the particle's catalytic effect on vulcanization, without perturbing however the cross-link density distribution that we investigate through multiple-quantum NMR. In addition, electron microscopy confirms that silica structure is conserved after vulcanization and that it does not depend on the PPG content either. Composites containing various amount of PPG can thus be seen as structurally identical, both from a matrix and filler point of view - which is confirmed by small and medium amplitude oscillation shear rheology showing strikingly identical viscoelastic properties. PPG signature only appears above 100% in tensile deformation where it is seen to soften dramatically the filler network. Our discovery makes it consequently possible to decorrelate the mechanical behavior of reinforced rubbers under normal conditions of use and urgent needs of energy dissipation.","sentences":["Following a previous work evidencing that short poly-propylene glycol (PPG) chains incorporated to crude SBR-silica nanocomposites act as filler-network softeners without changing their structure, we propose in the present letter to examine more operative cross-linked materials.","We first evidence that the adsorption of PPG onto silica deactivates progressively the particle's catalytic effect on vulcanization, without perturbing however the cross-link density distribution that we investigate through multiple-quantum NMR.","In addition, electron microscopy confirms that silica structure is conserved after vulcanization and that it does not depend on the PPG content either.","Composites containing various amount of PPG can thus be seen as structurally identical, both from a matrix and filler point of view - which is confirmed by small and medium amplitude oscillation shear rheology showing strikingly identical viscoelastic properties.","PPG signature only appears above 100% in tensile deformation where it is seen to soften dramatically the filler network.","Our discovery makes it consequently possible to decorrelate the mechanical behavior of reinforced rubbers under normal conditions of use and urgent needs of energy dissipation."],"url":"http://arxiv.org/abs/2403.03133v1","category":"physics.chem-ph"}
{"created":"2024-03-05 17:07:29","title":"NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors","abstract":"Faithfully modeling the space of articulations is a crucial task that allows recovery and generation of realistic poses, and remains a notorious challenge. To this end, we introduce Neural Riemannian Distance Fields (NRDFs), data-driven priors modeling the space of plausible articulations, represented as the zero-level-set of a neural field in a high-dimensional product-quaternion space. To train NRDFs only on positive examples, we introduce a new sampling algorithm, ensuring that the geodesic distances follow a desired distribution, yielding a principled distance field learning paradigm. We then devise a projection algorithm to map any random pose onto the level-set by an adaptive-step Riemannian optimizer, adhering to the product manifold of joint rotations at all times. NRDFs can compute the Riemannian gradient via backpropagation and by mathematical analogy, are related to Riemannian flow matching, a recent generative model. We conduct a comprehensive evaluation of NRDF against other pose priors in various downstream tasks, i.e., pose generation, image-based pose estimation, and solving inverse kinematics, highlighting NRDF's superior performance. Besides humans, NRDF's versatility extends to hand and animal poses, as it can effectively represent any articulation.","sentences":["Faithfully modeling the space of articulations is a crucial task that allows recovery and generation of realistic poses, and remains a notorious challenge.","To this end, we introduce Neural Riemannian Distance Fields (NRDFs), data-driven priors modeling the space of plausible articulations, represented as the zero-level-set of a neural field in a high-dimensional product-quaternion space.","To train NRDFs only on positive examples, we introduce a new sampling algorithm, ensuring that the geodesic distances follow a desired distribution, yielding a principled distance field learning paradigm.","We then devise a projection algorithm to map any random pose onto the level-set by an adaptive-step Riemannian optimizer, adhering to the product manifold of joint rotations at all times.","NRDFs can compute the Riemannian gradient via backpropagation and by mathematical analogy, are related to Riemannian flow matching, a recent generative model.","We conduct a comprehensive evaluation of NRDF against other pose priors in various downstream tasks, i.e., pose generation, image-based pose estimation, and solving inverse kinematics, highlighting NRDF's superior performance.","Besides humans, NRDF's versatility extends to hand and animal poses, as it can effectively represent any articulation."],"url":"http://arxiv.org/abs/2403.03122v1","category":"cs.CV"}
{"created":"2024-03-05 16:56:00","title":"Effect of particle stiffness and surface properties on the nonlinear viscoelasticity of dense microgel suspensions","abstract":"Particle surface chemistry and internal softness are two fundamental parameters in governing the mechanical properties of dense colloidal suspensions, dictating structure and flow, therefore of interest from materials fabrication to processing. Here, we modulate softness by tuning the crosslinker content of poly(N-isopropylacrylamide) microgels, and we adjust their surface properties by co-polymerization with polyethylene glycol (PEG) chains, controlling adhesion, friction and fuzziness. We investigate the distinct effects of these parameters on the entire mechanical response from restructuring to complete fluidization of jammed samples at varying packing fractions under large-amplitude oscillatory shear experiments, and we complement rheological data with colloidal-probe atomic force microscopy to unravel variations in the particles' surface properties. We find that surface properties play a fundamental role at smaller packings; decreasing adhesion and friction at contact causes the samples to yield and fluidify in a lower deformation range. Instead, increasing softness or fuzziness has a similar effect at ultra-high densities, making suspensions able to better adapt to the applied shear and reach complete fluidization over a larger deformation range. These findings shed new light on the single-particle parameters governing the mechanical response of dense suspensions subjected to deformation, offering synthetic approaches to design materials with tailored mechanical properties.","sentences":["Particle surface chemistry and internal softness are two fundamental parameters in governing the mechanical properties of dense colloidal suspensions, dictating structure and flow, therefore of interest from materials fabrication to processing.","Here, we modulate softness by tuning the crosslinker content of poly(N-isopropylacrylamide) microgels, and we adjust their surface properties by co-polymerization with polyethylene glycol (PEG) chains, controlling adhesion, friction and fuzziness.","We investigate the distinct effects of these parameters on the entire mechanical response from restructuring to complete fluidization of jammed samples at varying packing fractions under large-amplitude oscillatory shear experiments, and we complement rheological data with colloidal-probe atomic force microscopy to unravel variations in the particles' surface properties.","We find that surface properties play a fundamental role at smaller packings; decreasing adhesion and friction at contact causes the samples to yield and fluidify in a lower deformation range.","Instead, increasing softness or fuzziness has a similar effect at ultra-high densities, making suspensions able to better adapt to the applied shear and reach complete fluidization over a larger deformation range.","These findings shed new light on the single-particle parameters governing the mechanical response of dense suspensions subjected to deformation, offering synthetic approaches to design materials with tailored mechanical properties."],"url":"http://arxiv.org/abs/2403.03113v1","category":"cond-mat.soft"}
{"created":"2024-03-05 16:30:58","title":"Stretch-independent magnetization in incompressible magnetorheological elastomers","abstract":"In this study, we perform a critical examination of the phenomenon where the magnetization is stretch-independent in incompressible hard-magnetic magnetorheological elastomers (h-MREs), as observed in several recent experimental and numerical investigations. We demonstrate that the fully dissipative model proposed by Mukherjee et al. (2021) may be reduced, under physically consistent assumptions, to that of Yan et al. (2023), but not that of Zhao et al. (2019). In cases where the h-MRE solid undergoes non-negligible stretching, the model of Zhao et al. (2019) provides predictions that are in disagreement with experimental observations, given that, by construction, that model produces a magnetization response that is not stretch-independent. By contrast, the other two models are able to describe this important feature present in h-MREs, as well as in incompressible magnetically soft s-MRES. Note that in cases where stretching is negligible, such as for inextensible slender structures under bending deformation, the Zhao et al. (2019) model provides accurate predictions despite its underlying assumptions. Additionally, our analysis reveals two key points about the magnetization vector in the context of the more general, fully dissipative model. First, the magnetization can be related to an internal variable in that theory. However, it cannot be formally used as an internal variable except in the special case of an ideal magnet, and, as such, it is subject to constitutive assumptions. Furthermore, we clarify that the magnetization vector alone is insufficient to describe entirely the magnetic response of an MRE solid; instead, the introduction of one of the original Maxwell fields is always necessary for a complete representation.","sentences":["In this study, we perform a critical examination of the phenomenon where the magnetization is stretch-independent in incompressible hard-magnetic magnetorheological elastomers (h-MREs), as observed in several recent experimental and numerical investigations.","We demonstrate that the fully dissipative model proposed by Mukherjee et al.","(2021) may be reduced, under physically consistent assumptions, to that of Yan et al. (2023), but not that of Zhao et al. (2019).","In cases where the h-MRE solid undergoes non-negligible stretching, the model of Zhao et al. (2019) provides predictions that are in disagreement with experimental observations, given that, by construction, that model produces a magnetization response that is not stretch-independent.","By contrast, the other two models are able to describe this important feature present in h-MREs, as well as in incompressible magnetically soft s-MRES.","Note that in cases where stretching is negligible, such as for inextensible slender structures under bending deformation, the Zhao et al. (2019) model provides accurate predictions despite its underlying assumptions.","Additionally, our analysis reveals two key points about the magnetization vector in the context of the more general, fully dissipative model.","First, the magnetization can be related to an internal variable in that theory.","However, it cannot be formally used as an internal variable except in the special case of an ideal magnet, and, as such, it is subject to constitutive assumptions.","Furthermore, we clarify that the magnetization vector alone is insufficient to describe entirely the magnetic response of an MRE solid; instead, the introduction of one of the original Maxwell fields is always necessary for a complete representation."],"url":"http://arxiv.org/abs/2403.03096v1","category":"cond-mat.soft"}
{"created":"2024-03-05 16:28:48","title":"Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization","abstract":"Audio-Visual Source Localization (AVSL) is the task of identifying specific sounding objects in the scene given audio cues. In our work, we focus on semi-supervised AVSL with pseudo-labeling. To address the issues with vanilla hard pseudo-labels including bias accumulation, noise sensitivity, and instability, we propose a novel method named Cross Pseudo-Labeling (XPL), wherein two models learn from each other with the cross-refine mechanism to avoid bias accumulation. We equip XPL with two effective components. Firstly, the soft pseudo-labels with sharpening and pseudo-label exponential moving average mechanisms enable models to achieve gradual self-improvement and ensure stable training. Secondly, the curriculum data selection module adaptively selects pseudo-labels with high quality during training to mitigate potential bias. Experimental results demonstrate that XPL significantly outperforms existing methods, achieving state-of-the-art performance while effectively mitigating confirmation bias and ensuring training stability.","sentences":["Audio-Visual Source Localization (AVSL) is the task of identifying specific sounding objects in the scene given audio cues.","In our work, we focus on semi-supervised AVSL with pseudo-labeling.","To address the issues with vanilla hard pseudo-labels including bias accumulation, noise sensitivity, and instability, we propose a novel method named Cross Pseudo-Labeling (XPL), wherein two models learn from each other with the cross-refine mechanism to avoid bias accumulation.","We equip XPL with two effective components.","Firstly, the soft pseudo-labels with sharpening and pseudo-label exponential moving average mechanisms enable models to achieve gradual self-improvement and ensure stable training.","Secondly, the curriculum data selection module adaptively selects pseudo-labels with high quality during training to mitigate potential bias.","Experimental results demonstrate that XPL significantly outperforms existing methods, achieving state-of-the-art performance while effectively mitigating confirmation bias and ensuring training stability."],"url":"http://arxiv.org/abs/2403.03095v1","category":"cs.CV"}
{"created":"2024-03-05 15:59:54","title":"Geometry-dependent matching pursuit: a transition phase for convergence on linear regression and LASSO","abstract":"Greedy first-order methods, such as coordinate descent with Gauss-Southwell rule or matching pursuit, have become popular in optimization due to their natural tendency to propose sparse solutions and their refined convergence guarantees. In this work, we propose a principled approach to generating (regularized) matching pursuit algorithms adapted to the geometry of the problem at hand, as well as their convergence guarantees. Building on these results, we derive approximate convergence guarantees and describe a transition phenomenon in the convergence of (regularized) matching pursuit from underparametrized to overparametrized models.","sentences":["Greedy first-order methods, such as coordinate descent with Gauss-Southwell rule or matching pursuit, have become popular in optimization due to their natural tendency to propose sparse solutions and their refined convergence guarantees.","In this work, we propose a principled approach to generating (regularized) matching pursuit algorithms adapted to the geometry of the problem at hand, as well as their convergence guarantees.","Building on these results, we derive approximate convergence guarantees and describe a transition phenomenon in the convergence of (regularized) matching pursuit from underparametrized to overparametrized models."],"url":"http://arxiv.org/abs/2403.03072v1","category":"math.OC"}
{"created":"2024-03-05 15:34:41","title":"Prediction of turbulent channel flow using Fourier neural operator-based machine-learning strategy","abstract":"Fast and accurate predictions of turbulent flows are of great importance in the science and engineering field. In this paper, we investigate the implicit U-Net enhanced Fourier neural operator (IUFNO) in the stable prediction of long-time dynamics of three-dimensional (3D) turbulent channel flows. The trained IUFNO models are tested in the large-eddy simulations (LES) at coarse grids for three friction Reynolds numbers: $Re_{\\tau}\\approx180$, $395$ and $590$. The adopted near-wall mesh grids are tangibly coarser than the general requirements for wall-resolved LES. The numerical experiments show that the IUFNO framework outperforms the traditional dynamic Smagorinsky model (DSM) and the wall-adapted local eddy-viscosity (WALE) model in the predictions of a variety of flow statistics and structures, including the mean and fluctuating velocities, the probability density functions (PDFs) and joint PDF of velocity fluctuations, the Reynolds stress profile, the kinetic energy spectrum, and the Q-criterion (vortex structures). Meanwhile, the trained IUFNO models are computationally much faster than the traditional LES models. Thus, the IUFNO is a promising approach for the fast prediction of wall-bounded turbulent flow.","sentences":["Fast and accurate predictions of turbulent flows are of great importance in the science and engineering field.","In this paper, we investigate the implicit U-Net enhanced Fourier neural operator (IUFNO) in the stable prediction of long-time dynamics of three-dimensional (3D) turbulent channel flows.","The trained IUFNO models are tested in the large-eddy simulations (LES) at coarse grids for three friction Reynolds numbers: $Re_{\\tau}\\approx180$, $395$ and $590$. The adopted near-wall mesh grids are tangibly coarser than the general requirements for wall-resolved LES.","The numerical experiments show that the IUFNO framework outperforms the traditional dynamic Smagorinsky model (DSM) and the wall-adapted local eddy-viscosity (WALE) model in the predictions of a variety of flow statistics and structures, including the mean and fluctuating velocities, the probability density functions (PDFs) and joint PDF of velocity fluctuations, the Reynolds stress profile, the kinetic energy spectrum, and the Q-criterion (vortex structures).","Meanwhile, the trained IUFNO models are computationally much faster than the traditional LES models.","Thus, the IUFNO is a promising approach for the fast prediction of wall-bounded turbulent flow."],"url":"http://arxiv.org/abs/2403.03051v1","category":"physics.flu-dyn"}
{"created":"2024-03-05 15:33:34","title":"Microscopic origin of abrupt transition in interdependent superconducting networks","abstract":"The paradigm of interdependent networks has recently been manifested in experimentally testable lab setup of interdependent superconducting networks. This system experiences an abrupt transition due to the thermal dissipation between the networks but its underlying mechanism remains elusive. Here we study the critical behavior and the underlying mechanism of the transition, unveiling its unique microscopic nature. The microscopic characteristics of the transition result in a macroscopic long-living plateau that lasts for thousands of seconds and increases with the size of the system. We characterize the critical behavior of the transition and find that the critical exponents are consistent with those predicted theoretically for percolation of abstract interdependent networks and interdependent ferromagnetic networks, supporting a common universal origin of interdependent systems.","sentences":["The paradigm of interdependent networks has recently been manifested in experimentally testable lab setup of interdependent superconducting networks.","This system experiences an abrupt transition due to the thermal dissipation between the networks but its underlying mechanism remains elusive.","Here we study the critical behavior and the underlying mechanism of the transition, unveiling its unique microscopic nature.","The microscopic characteristics of the transition result in a macroscopic long-living plateau that lasts for thousands of seconds and increases with the size of the system.","We characterize the critical behavior of the transition and find that the critical exponents are consistent with those predicted theoretically for percolation of abstract interdependent networks and interdependent ferromagnetic networks, supporting a common universal origin of interdependent systems."],"url":"http://arxiv.org/abs/2403.03050v1","category":"physics.soc-ph"}
{"created":"2024-03-05 15:28:24","title":"Adding Multimodal Capabilities to a Text-only Translation Model","abstract":"While most current work in multimodal machine translation (MMT) uses the Multi30k dataset for training and evaluation, we find that the resulting models overfit to the Multi30k dataset to an extreme degree. Consequently, these models perform very badly when evaluated against typical text-only testing sets such as the WMT newstest datasets. In order to perform well on both Multi30k and typical text-only datasets, we use a performant text-only machine translation (MT) model as the starting point of our MMT model. We add vision-text adapter layers connected via gating mechanisms to the MT model, and incrementally transform the MT model into an MMT model by 1) pre-training using vision-based masking of the source text and 2) fine-tuning on Multi30k.","sentences":["While most current work in multimodal machine translation (MMT) uses the Multi30k dataset for training and evaluation, we find that the resulting models overfit to the Multi30k dataset to an extreme degree.","Consequently, these models perform very badly when evaluated against typical text-only testing sets such as the WMT newstest datasets.","In order to perform well on both Multi30k and typical text-only datasets, we use a performant text-only machine translation (MT) model as the starting point of our MMT model.","We add vision-text adapter layers connected via gating mechanisms to the MT model, and incrementally transform the MT model into an MMT model by 1) pre-training using vision-based masking of the source text and 2) fine-tuning on Multi30k."],"url":"http://arxiv.org/abs/2403.03045v1","category":"cs.CL"}
{"created":"2024-03-05 15:12:25","title":"Global dissipative martingale solutions to the variational wave equation with stochastic forcing","abstract":"We consider the variational wave equation in one-dimensional space with stochastic forcing by an additive noise. Blow-up of local smooth solutions is established, and global existence is proved in the class of weak martingale solutions.","sentences":["We consider the variational wave equation in one-dimensional space with stochastic forcing by an additive noise.","Blow-up of local smooth solutions is established, and global existence is proved in the class of weak martingale solutions."],"url":"http://arxiv.org/abs/2403.03034v1","category":"math.AP"}
{"created":"2024-03-05 15:08:16","title":"Learning to Use Tools via Cooperative and Interactive Agents","abstract":"Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the superiority of our ConAgents (e.g., 6 point improvement over the SOTA baseline). We further provide fine-granularity analysis for the efficiency and consistency of our framework.","sentences":["Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability.","Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction.","However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails.","To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents.","We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment.","Experiments conducted on three datasets demonstrate the superiority of our ConAgents (e.g., 6 point improvement over the SOTA baseline).","We further provide fine-granularity analysis for the efficiency and consistency of our framework."],"url":"http://arxiv.org/abs/2403.03031v1","category":"cs.CL"}
{"created":"2024-03-05 14:31:24","title":"Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models","abstract":"Despite remarkable progress, existing multimodal large language models (MLLMs) are still inferior in granular visual recognition. Contrary to previous works, we study this problem from the perspective of image resolution, and reveal that a combination of low- and high-resolution visual features can effectively mitigate this shortcoming. Based on this observation, we propose a novel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation (MRA). In particular, MRA adopts two visual pathways for images with different resolutions, where high-resolution visual information is embedded into the low-resolution pathway via the novel mixture-of-resolution adapters (MR-Adapters). This design also greatly reduces the input sequence length of MLLMs. To validate MRA, we apply it to a recent MLLM called LLaVA, and term the new model LLaVA-HR. We conduct extensive experiments on 11 vision-language (VL) tasks, which show that LLaVA-HR outperforms existing MLLMs on 8 VL tasks, e.g., +9.4% on TextVQA. More importantly, both training and inference of LLaVA-HR remain efficient with MRA, e.g., 20 training hours and 3$\\times$ inference speed than LLaVA-1.5. Source codes are released at: https://github.com/luogen1996/LLaVA-HR.","sentences":["Despite remarkable progress, existing multimodal large language models (MLLMs) are still inferior in granular visual recognition.","Contrary to previous works, we study this problem from the perspective of image resolution, and reveal that a combination of low- and high-resolution visual features can effectively mitigate this shortcoming.","Based on this observation, we propose a novel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation (MRA).","In particular, MRA adopts two visual pathways for images with different resolutions, where high-resolution visual information is embedded into the low-resolution pathway via the novel mixture-of-resolution adapters (MR-Adapters).","This design also greatly reduces the input sequence length of MLLMs.","To validate MRA, we apply it to a recent MLLM called LLaVA, and term the new model LLaVA-HR.","We conduct extensive experiments on 11 vision-language (VL) tasks, which show that LLaVA-HR outperforms existing MLLMs on 8 VL tasks, e.g., +9.4% on TextVQA.","More importantly, both training and inference of LLaVA-HR remain efficient with MRA, e.g., 20 training hours and 3$\\times$ inference speed than LLaVA-1.5.","Source codes are released at: https://github.com/luogen1996/LLaVA-HR."],"url":"http://arxiv.org/abs/2403.03003v1","category":"cs.CV"}
{"created":"2024-03-05 14:16:52","title":"Adaptive Integrate-and-Fire Time Encoding Machine","abstract":"An integrate-and-fire time-encoding machine (IF-TEM) is an effective asynchronous sampler that translates amplitude information into non-uniform time sequences. In this work, we propose a novel Adaptive IF-TEM (AIF-TEM) approach. This design dynamically adjusts the TEM's sensitivity to changes in the input signal's amplitude and frequency in real-time. We provide a comprehensive analysis of AIF-TEM's oversampling and distortion properties. By the adaptive adjustments, AIF-TEM as we show can achieve significant performance improvements in practical finite regime, in terms of sampling rate-distortion. We demonstrate empirically that in the scenarios tested AIF-TEM outperforms classical IF-TEM and traditional Nyquist (i.e., periodic) sampling methods for band-limited signals. In terms of Mean Square Error (MSE), the reduction reaches at least 12dB (fixing the oversampling rate).","sentences":["An integrate-and-fire time-encoding machine (IF-TEM) is an effective asynchronous sampler that translates amplitude information into non-uniform time sequences.","In this work, we propose a novel Adaptive IF-TEM (AIF-TEM) approach.","This design dynamically adjusts the TEM's sensitivity to changes in the input signal's amplitude and frequency in real-time.","We provide a comprehensive analysis of AIF-TEM's oversampling and distortion properties.","By the adaptive adjustments, AIF-TEM as we show can achieve significant performance improvements in practical finite regime, in terms of sampling rate-distortion.","We demonstrate empirically that in the scenarios tested AIF-TEM outperforms classical IF-TEM and traditional Nyquist (i.e., periodic) sampling methods for band-limited signals.","In terms of Mean Square Error (MSE), the reduction reaches at least 12dB (fixing the oversampling rate)."],"url":"http://arxiv.org/abs/2403.02992v1","category":"eess.SP"}
{"created":"2024-03-05 14:13:50","title":"MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer","abstract":"Vision-Language Transformers (VLTs) have shown great success recently, but are meanwhile accompanied by heavy computation costs, where a major reason can be attributed to the large number of visual and language tokens. Existing token pruning research for compressing VLTs mainly follows a single-modality-based scheme yet ignores the critical role of aligning different modalities for guiding the token pruning process, causing the important tokens for one modality to be falsely pruned in another modality branch. Meanwhile, existing VLT pruning works also lack the flexibility to dynamically compress each layer based on different input samples. To this end, we propose a novel framework named Multimodal Alignment-Guided Dynamic Token Pruning (MADTP) for accelerating various VLTs. Specifically, we first introduce a well-designed Multi-modality Alignment Guidance (MAG) module that can align features of the same semantic concept from different modalities, to ensure the pruned tokens are less important for all modalities. We further design a novel Dynamic Token Pruning (DTP) module, which can adaptively adjust the token compression ratio in each layer based on different input instances. Extensive experiments on various benchmarks demonstrate that MADTP significantly reduces the computational complexity of kinds of multimodal models while preserving competitive performance. Notably, when applied to the BLIP model in the NLVR2 dataset, MADTP can reduce the GFLOPs by 80% with less than 4% performance degradation.","sentences":["Vision-Language Transformers (VLTs) have shown great success recently, but are meanwhile accompanied by heavy computation costs, where a major reason can be attributed to the large number of visual and language tokens.","Existing token pruning research for compressing VLTs mainly follows a single-modality-based scheme yet ignores the critical role of aligning different modalities for guiding the token pruning process, causing the important tokens for one modality to be falsely pruned in another modality branch.","Meanwhile, existing VLT pruning works also lack the flexibility to dynamically compress each layer based on different input samples.","To this end, we propose a novel framework named Multimodal Alignment-Guided Dynamic Token Pruning (MADTP) for accelerating various VLTs.","Specifically, we first introduce a well-designed Multi-modality Alignment Guidance (MAG) module that can align features of the same semantic concept from different modalities, to ensure the pruned tokens are less important for all modalities.","We further design a novel Dynamic Token Pruning (DTP) module, which can adaptively adjust the token compression ratio in each layer based on different input instances.","Extensive experiments on various benchmarks demonstrate that MADTP significantly reduces the computational complexity of kinds of multimodal models while preserving competitive performance.","Notably, when applied to the BLIP model in the NLVR2 dataset, MADTP can reduce the GFLOPs by 80% with less than 4% performance degradation."],"url":"http://arxiv.org/abs/2403.02991v1","category":"cs.CV"}
{"created":"2024-03-05 13:53:48","title":"Online Learning of Human Constraints from Feedback in Shared Autonomy","abstract":"Real-time collaboration with humans poses challenges due to the different behavior patterns of humans resulting from diverse physical constraints. Existing works typically focus on learning safety constraints for collaboration, or how to divide and distribute the subtasks between the participating agents to carry out the main task. In contrast, we propose to learn a human constraints model that, in addition, considers the diverse behaviors of different human operators. We consider a type of collaboration in a shared-autonomy fashion, where both a human operator and an assistive robot act simultaneously in the same task space that affects each other's actions. The task of the assistive agent is to augment the skill of humans to perform a shared task by supporting humans as much as possible, both in terms of reducing the workload and minimizing the discomfort for the human operator. Therefore, we propose an augmentative assistant agent capable of learning and adapting to human physical constraints, aligning its actions with the ergonomic preferences and limitations of the human operator.","sentences":["Real-time collaboration with humans poses challenges due to the different behavior patterns of humans resulting from diverse physical constraints.","Existing works typically focus on learning safety constraints for collaboration, or how to divide and distribute the subtasks between the participating agents to carry out the main task.","In contrast, we propose to learn a human constraints model that, in addition, considers the diverse behaviors of different human operators.","We consider a type of collaboration in a shared-autonomy fashion, where both a human operator and an assistive robot act simultaneously in the same task space that affects each other's actions.","The task of the assistive agent is to augment the skill of humans to perform a shared task by supporting humans as much as possible, both in terms of reducing the workload and minimizing the discomfort for the human operator.","Therefore, we propose an augmentative assistant agent capable of learning and adapting to human physical constraints, aligning its actions with the ergonomic preferences and limitations of the human operator."],"url":"http://arxiv.org/abs/2403.02974v1","category":"cs.RO"}
{"created":"2024-03-05 13:15:01","title":"Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity","abstract":"Recent advances in text-guided image compression have shown great potential to enhance the perceptual quality of reconstructed images. These methods, however, tend to have significantly degraded pixel-wise fidelity, limiting their practicality. To fill this gap, we develop a new text-guided image compression algorithm that achieves both high perceptual and pixel-wise fidelity. In particular, we propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, we avoid decoding based on text-guided generative models -- known for high generative diversity -- and effectively utilize the semantic information of text at a global level. Experimental results on various datasets show that our method can achieve high pixel-level and perceptual quality, with either human- or machine-generated captions. In particular, our method outperforms all baselines in terms of LPIPS, with some room for even more improvements when we use more carefully generated captions.","sentences":["Recent advances in text-guided image compression have shown great potential to enhance the perceptual quality of reconstructed images.","These methods, however, tend to have significantly degraded pixel-wise fidelity, limiting their practicality.","To fill this gap, we develop a new text-guided image compression algorithm that achieves both high perceptual and pixel-wise fidelity.","In particular, we propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss.","By doing so, we avoid decoding based on text-guided generative models -- known for high generative diversity -- and effectively utilize the semantic information of text at a global level.","Experimental results on various datasets show that our method can achieve high pixel-level and perceptual quality, with either human- or machine-generated captions.","In particular, our method outperforms all baselines in terms of LPIPS, with some room for even more improvements when we use more carefully generated captions."],"url":"http://arxiv.org/abs/2403.02944v1","category":"cs.CV"}
{"created":"2024-03-05 12:50:36","title":"RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules","abstract":"Weakly supervised text classification (WSTC), also called zero-shot or dataless text classification, has attracted increasing attention due to its applicability in classifying a mass of texts within the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data. With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus. In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories. Then, we develop a prompting PLM-based approach named RulePrompt for the WSTC task, consisting of a rule mining module and a rule-enhanced pseudo label generation module, plus a self-supervised fine-tuning module to make the PLM align with this task. Within this framework, the inaccurate pseudo labels assigned to texts and the imprecise logical rules associated with categories mutually enhance each other in an alternative manner. That establishes a self-iterative closed loop of knowledge (rule) acquisition and utilization, with seed words serving as the starting point. Extensive experiments validate the effectiveness and robustness of our approach, which markedly outperforms state-of-the-art weakly supervised methods. What is more, our approach yields interpretable category rules, proving its advantage in disambiguating easily-confused categories.","sentences":["Weakly supervised text classification (WSTC), also called zero-shot or dataless text classification, has attracted increasing attention due to its applicability in classifying a mass of texts within the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data.","With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus.","In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories.","Then, we develop a prompting PLM-based approach named RulePrompt for the WSTC task, consisting of a rule mining module and a rule-enhanced pseudo label generation module, plus a self-supervised fine-tuning module to make the PLM align with this task.","Within this framework, the inaccurate pseudo labels assigned to texts and the imprecise logical rules associated with categories mutually enhance each other in an alternative manner.","That establishes a self-iterative closed loop of knowledge (rule) acquisition and utilization, with seed words serving as the starting point.","Extensive experiments validate the effectiveness and robustness of our approach, which markedly outperforms state-of-the-art weakly supervised methods.","What is more, our approach yields interpretable category rules, proving its advantage in disambiguating easily-confused categories."],"url":"http://arxiv.org/abs/2403.02932v1","category":"cs.CL"}
{"created":"2024-03-05 12:44:54","title":"User-Driven Adaptation: Tailoring Autonomous Driving Systems with Dynamic Preferences","abstract":"In the realm of autonomous vehicles, dynamic user preferences are critical yet challenging to accommodate. Existing methods often misrepresent these preferences, either by overlooking their dynamism or overburdening users as humans often find it challenging to express their objectives mathematically. The previously introduced framework, which interprets dynamic preferences as inherent uncertainty and includes a ``human-on-the-loop'' mechanism enabling users to give feedback when dissatisfied with system behaviors, addresses this gap. In this study, we further enhance the approach with a user study of 20 participants, focusing on aligning system behavior with user expectations through feedback-driven adaptation. The findings affirm the approach's ability to effectively merge algorithm-driven adjustments with user complaints, leading to improved participants' subjective satisfaction in autonomous systems.","sentences":["In the realm of autonomous vehicles, dynamic user preferences are critical yet challenging to accommodate.","Existing methods often misrepresent these preferences, either by overlooking their dynamism or overburdening users as humans often find it challenging to express their objectives mathematically.","The previously introduced framework, which interprets dynamic preferences as inherent uncertainty and includes a ``human-on-the-loop'' mechanism enabling users to give feedback when dissatisfied with system behaviors, addresses this gap.","In this study, we further enhance the approach with a user study of 20 participants, focusing on aligning system behavior with user expectations through feedback-driven adaptation.","The findings affirm the approach's ability to effectively merge algorithm-driven adjustments with user complaints, leading to improved participants' subjective satisfaction in autonomous systems."],"url":"http://arxiv.org/abs/2403.02928v1","category":"cs.HC"}
{"created":"2024-03-05 12:44:39","title":"Risk-Constrained Community Battery Utilisation Optimisation for Electric Vehicle Charging with Photovoltaic Resources","abstract":"High penetration of renewable generation in the electricity grid presents power system operators with challenges including voltage instability mainly due to fluctuating power generation. To cope with intermittent generation, community batteries introduce an elegant solution for storing excess generation of renewable resources and reverting to the grid in peak demand periods. The question of the right battery type and size coupled with the right investment is challenging. Furthermore, the growth in adapting EVs imposes additional demand challenges on the power system compared to traditional industrial and household demand. This paper introduces long-term planning for community batteries to capture the surplus generation of PV resources for a given area and redirect these resources to charge EVs, without direct injection to the grid. For long-term investment planning on batteries, we consider 15 years' worth of historical data associated with solar irradiance, temperature, EV demands, and household demands. A novel stochastic mathematical model is proposed for decision-making on battery specifications (the type and capacity per year) based on the four standard battery types provided by the CSIRO in Australia. Uncertainties related to the EVs and RESs are captured by a non-parametric robust technique, named information gap decision theory, from optimistic and pessimistic perspectives. The investment decision-making part is formulated as mixed-integer linear programming taking advantage of the powerful commercial solver -- GUROBI -- which leads to finding feasible global solutions with low computational burden. The outcomes of this investigation not only detect optimal battery installation strategies to improve the stability profile of the grid by capturing the excess generation of PV resources but also facilitate EV integration in the community toward reaching net-zero emissions targets.","sentences":["High penetration of renewable generation in the electricity grid presents power system operators with challenges including voltage instability mainly due to fluctuating power generation.","To cope with intermittent generation, community batteries introduce an elegant solution for storing excess generation of renewable resources and reverting to the grid in peak demand periods.","The question of the right battery type and size coupled with the right investment is challenging.","Furthermore, the growth in adapting EVs imposes additional demand challenges on the power system compared to traditional industrial and household demand.","This paper introduces long-term planning for community batteries to capture the surplus generation of PV resources for a given area and redirect these resources to charge EVs, without direct injection to the grid.","For long-term investment planning on batteries, we consider 15 years' worth of historical data associated with solar irradiance, temperature, EV demands, and household demands.","A novel stochastic mathematical model is proposed for decision-making on battery specifications (the type and capacity per year) based on the four standard battery types provided by the CSIRO in Australia.","Uncertainties related to the EVs and RESs are captured by a non-parametric robust technique, named information gap decision theory, from optimistic and pessimistic perspectives.","The investment decision-making part is formulated as mixed-integer linear programming taking advantage of the powerful commercial solver -- GUROBI -- which leads to finding feasible global solutions with low computational burden.","The outcomes of this investigation not only detect optimal battery installation strategies to improve the stability profile of the grid by capturing the excess generation of PV resources but also facilitate EV integration in the community toward reaching net-zero emissions targets."],"url":"http://arxiv.org/abs/2403.02927v1","category":"math.OC"}
{"created":"2024-03-05 12:31:24","title":"DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting","abstract":"The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment. Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment. To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors. These methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region. Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical. Worse still, the complex technical design may ultimately lead to a model with weak generalizability. In this paper, we introduce for the first time the concept of spatio-temporal data dynamic sparse training and are committed to adaptively, dynamically filtering important sensor distributions. To our knowledge, this is the first proposal (termed DynST) of an industry-level deployment optimization concept at the data level. However, due to the existence of the temporal dimension, pruning of spatio-temporal data may lead to conflicts at different timestamps. To achieve this goal, we employ dynamic merge technology, along with ingenious dimensional mapping to mitigate potential impacts caused by the temporal aspect. During the training process, DynST utilize iterative pruning and sparse training, repeatedly identifying and dynamically removing sensor perception areas that contribute the least to future predictions.","sentences":["The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment.","Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment.","To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors.","These methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region.","Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical.","Worse still, the complex technical design may ultimately lead to a model with weak generalizability.","In this paper, we introduce for the first time the concept of spatio-temporal data dynamic sparse training and are committed to adaptively, dynamically filtering important sensor distributions.","To our knowledge, this is the first proposal (termed DynST) of an industry-level deployment optimization concept at the data level.","However, due to the existence of the temporal dimension, pruning of spatio-temporal data may lead to conflicts at different timestamps.","To achieve this goal, we employ dynamic merge technology, along with ingenious dimensional mapping to mitigate potential impacts caused by the temporal aspect.","During the training process, DynST utilize iterative pruning and sparse training, repeatedly identifying and dynamically removing sensor perception areas that contribute the least to future predictions."],"url":"http://arxiv.org/abs/2403.02914v1","category":"cs.AI"}
{"created":"2024-03-05 12:06:48","title":"Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation","abstract":"Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between domains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts. A promising technique is to leverage the knowledge of large-scale pre-trained vision-language models for more guided adaptation. Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer. Moreover, prompting only the language branch lacks flexibility to adapt both modalities dynamically. To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings. Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-conditioned way. Meanwhile, visual prompts are imposed based on the domain-agnostic textual prompt to elicit domain-invariant visual embeddings. These two branches of prompts are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss. Experiments on three UDA benchmarks demonstrate the superiority of DAMP over state-of-the-art approaches.","sentences":["Conventional Unsupervised Domain Adaptation (UDA) strives to minimize distribution discrepancy between domains, which neglects to harness rich semantics from data and struggles to handle complex domain shifts.","A promising technique is to leverage the knowledge of large-scale pre-trained vision-language models for more guided adaptation.","Despite some endeavors, current methods often learn textual prompts to embed domain semantics for source and target domains separately and perform classification within each domain, limiting cross-domain knowledge transfer.","Moreover, prompting only the language branch lacks flexibility to adapt both modalities dynamically.","To bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit domain-invariant semantics by mutually aligning visual and textual embeddings.","Specifically, the image contextual information is utilized to prompt the language branch in a domain-agnostic and instance-conditioned way.","Meanwhile, visual prompts are imposed based on the domain-agnostic textual prompt to elicit domain-invariant visual embeddings.","These two branches of prompts are learned mutually with a cross-attention module and regularized with a semantic-consistency loss and an instance-discrimination contrastive loss.","Experiments on three UDA benchmarks demonstrate the superiority of DAMP over state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.02899v1","category":"cs.AI"}
{"created":"2024-03-05 10:52:13","title":"STAR-RIS Assisted Wireless-Powered and Backscattering Mobile Edge Computing Networks","abstract":"Wireless powered and backscattering mobile edge computing (WPB-MEC) network is a novel network paradigm to supply energy supplies and computing resource to wireless sensors (WSs). However, its performance is seriously affected by severe attenuations and inappropriate assumptions of infinite computing capability at the hybrid access point (HAP). To address the above issues, in this paper, we propose a simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) aided scheme for boosting the performance of WPB-MEC network under the constraint of finite computing capability. Specifically, energy-constrained WSs are able to offload tasks actively or passively from them to the HAP. In this process, the STAR-RIS is utilized to improve the quantity of harvested energy and strengthen the offloading efficiency by adapting its operating protocols. We then maximize the sum computational bits (SCBs) under the finite computing capability constraint. To handle the solving challenges, we first present interesting results in closed-form and then design a block coordinate descent (BCD) based algorithm, ensuring a near-optimal solution. Finally, simulation results are provided to confirm that our proposed scheme can improve the SCBs by 9.9 times compared to the local computing only scheme.","sentences":["Wireless powered and backscattering mobile edge computing (WPB-MEC) network is a novel network paradigm to supply energy supplies and computing resource to wireless sensors (WSs).","However, its performance is seriously affected by severe attenuations and inappropriate assumptions of infinite computing capability at the hybrid access point (HAP).","To address the above issues, in this paper, we propose a simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) aided scheme for boosting the performance of WPB-MEC network under the constraint of finite computing capability.","Specifically, energy-constrained WSs are able to offload tasks actively or passively from them to the HAP.","In this process, the STAR-RIS is utilized to improve the quantity of harvested energy and strengthen the offloading efficiency by adapting its operating protocols.","We then maximize the sum computational bits (SCBs) under the finite computing capability constraint.","To handle the solving challenges, we first present interesting results in closed-form and then design a block coordinate descent (BCD) based algorithm, ensuring a near-optimal solution.","Finally, simulation results are provided to confirm that our proposed scheme can improve the SCBs by 9.9 times compared to the local computing only scheme."],"url":"http://arxiv.org/abs/2403.02854v1","category":"eess.SP"}
{"created":"2024-03-05 10:11:19","title":"Second-order robust parallel integrators for dynamical low-rank approximation","abstract":"Due to its reduced memory and computational demands, dynamical low-rank approximation (DLRA) has sparked significant interest in multiple research communities. A central challenge in DLRA is the development of time integrators that are robust to the curvature of the manifold of low-rank matrices. Recently, a parallel robust time integrator that permits dynamic rank adaptation and enables a fully parallel update of all low-rank factors was introduced. Despite its favorable computational efficiency, the construction as a first-order approximation to the augmented basis-update & Galerkin integrator restricts the parallel integrator's accuracy to order one. In this work, an extension to higher order is proposed by a careful basis augmentation before solving the matrix differential equations of the factorized solution. A robust error bound with an improved dependence on normal components of the vector field together with a norm preservation property up to small terms is derived. These analytic results are complemented and demonstrated through a series of numerical experiments.","sentences":["Due to its reduced memory and computational demands, dynamical low-rank approximation (DLRA) has sparked significant interest in multiple research communities.","A central challenge in DLRA is the development of time integrators that are robust to the curvature of the manifold of low-rank matrices.","Recently, a parallel robust time integrator that permits dynamic rank adaptation and enables a fully parallel update of all low-rank factors was introduced.","Despite its favorable computational efficiency, the construction as a first-order approximation to the augmented basis-update & Galerkin integrator restricts the parallel integrator's accuracy to order one.","In this work, an extension to higher order is proposed by a careful basis augmentation before solving the matrix differential equations of the factorized solution.","A robust error bound with an improved dependence on normal components of the vector field together with a norm preservation property up to small terms is derived.","These analytic results are complemented and demonstrated through a series of numerical experiments."],"url":"http://arxiv.org/abs/2403.02834v1","category":"math.NA"}
{"created":"2024-03-05 10:03:21","title":"SpaceHopper: A Small-Scale Legged Robot for Exploring Low-Gravity Celestial Bodies","abstract":"We present SpaceHopper, a three-legged, small-scale robot designed for future mobile exploration of asteroids and moons. The robot weighs 5.2kg and has a body size of 245mm while using space-qualifiable components. Furthermore, SpaceHopper's design and controls make it well-adapted for investigating dynamic locomotion modes with extended flight-phases. Instead of gyroscopes or fly-wheels, the system uses its three legs to reorient the body during flight in preparation for landing. We control the leg motion for reorientation using Deep Reinforcement Learning policies. In a simulation of Ceres' gravity (0.029g), the robot can reliably jump to commanded positions up to 6m away. Our real-world experiments show that SpaceHopper can successfully reorient to a safe landing orientation within 9.7 degree inside a rotational gimbal and jump in a counterweight setup in Earth's gravity. Overall, we consider SpaceHopper an important step towards controlled jumping locomotion in low-gravity environments.","sentences":["We present SpaceHopper, a three-legged, small-scale robot designed for future mobile exploration of asteroids and moons.","The robot weighs 5.2kg and has a body size of 245mm while using space-qualifiable components.","Furthermore, SpaceHopper's design and controls make it well-adapted for investigating dynamic locomotion modes with extended flight-phases.","Instead of gyroscopes or fly-wheels, the system uses its three legs to reorient the body during flight in preparation for landing.","We control the leg motion for reorientation using Deep Reinforcement Learning policies.","In a simulation of Ceres' gravity (0.029g), the robot can reliably jump to commanded positions up to 6m away.","Our real-world experiments show that SpaceHopper can successfully reorient to a safe landing orientation within 9.7 degree inside a rotational gimbal and jump in a counterweight setup in Earth's gravity.","Overall, we consider SpaceHopper an important step towards controlled jumping locomotion in low-gravity environments."],"url":"http://arxiv.org/abs/2403.02831v1","category":"cs.RO"}
{"created":"2024-03-05 09:44:51","title":"An Adaptive Hydropower Management Approach for Downstream Ecosystem Preservation","abstract":"Hydropower plants play a pivotal role in advancing clean and sustainable energy production, contributing significantly to the global transition towards renewable energy sources. However, hydropower plants are currently perceived both positively as sources of renewable energy and negatively as disruptors of ecosystems. In this work, we highlight the overlooked potential of using hydropower plant as protectors of ecosystems by using adaptive ecological discharges. To advocate for this perspective, we propose using a neural network to predict the minimum ecological discharge value at each desired time. Additionally, we present a novel framework that seamlessly integrates it into hydropower management software, taking advantage of the well-established approach of using traditional constrained optimisation algorithms. This novel approach not only protects the ecosystems from climate change but also contributes to potentially increase the electricity production.","sentences":["Hydropower plants play a pivotal role in advancing clean and sustainable energy production, contributing significantly to the global transition towards renewable energy sources.","However, hydropower plants are currently perceived both positively as sources of renewable energy and negatively as disruptors of ecosystems.","In this work, we highlight the overlooked potential of using hydropower plant as protectors of ecosystems by using adaptive ecological discharges.","To advocate for this perspective, we propose using a neural network to predict the minimum ecological discharge value at each desired time.","Additionally, we present a novel framework that seamlessly integrates it into hydropower management software, taking advantage of the well-established approach of using traditional constrained optimisation algorithms.","This novel approach not only protects the ecosystems from climate change but also contributes to potentially increase the electricity production."],"url":"http://arxiv.org/abs/2403.02821v1","category":"cs.LG"}
{"created":"2024-03-05 08:57:28","title":"DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image Semantic Segmentation with Unsupervised Domain Adaptation","abstract":"Semantic segmentation of remote sensing images is a challenging and hot issue due to the large amount of unlabeled data. Unsupervised domain adaptation (UDA) has proven to be advantageous in incorporating unclassified information from the target domain. However, independently fine-tuning UDA models on the source and target domains has a limited effect on the outcome. This paper proposes a hybrid training strategy as well as a novel dual-domain image fusion strategy that effectively utilizes the original image, transformation image, and intermediate domain information. Moreover, to enhance the precision of pseudo-labels, we present a pseudo-label region-specific weight strategy. The efficacy of our approach is substantiated by extensive benchmark experiments and ablation studies conducted on the ISPRS Vaihingen and Potsdam datasets.","sentences":["Semantic segmentation of remote sensing images is a challenging and hot issue due to the large amount of unlabeled data.","Unsupervised domain adaptation (UDA) has proven to be advantageous in incorporating unclassified information from the target domain.","However, independently fine-tuning UDA models on the source and target domains has a limited effect on the outcome.","This paper proposes a hybrid training strategy as well as a novel dual-domain image fusion strategy that effectively utilizes the original image, transformation image, and intermediate domain information.","Moreover, to enhance the precision of pseudo-labels, we present a pseudo-label region-specific weight strategy.","The efficacy of our approach is substantiated by extensive benchmark experiments and ablation studies conducted on the ISPRS Vaihingen and Potsdam datasets."],"url":"http://arxiv.org/abs/2403.02784v1","category":"cs.CV"}
{"created":"2024-03-05 08:41:41","title":"Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models","abstract":"Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on the downscaling task. Further, our method generalizes to climate states unseen during training without explicitly formulated physical constraints.","sentences":["Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive.","Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches.","However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training.","We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner.","Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data.","We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on the downscaling task.","Further, our method generalizes to climate states unseen during training without explicitly formulated physical constraints."],"url":"http://arxiv.org/abs/2403.02774v1","category":"physics.ao-ph"}
{"created":"2024-03-05 08:22:41","title":"Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models","abstract":"The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt. The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains. Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion. This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities.","sentences":["The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience.","Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains.","In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy.","This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting.","2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training.","3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt.","The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains.","Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion.","This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities."],"url":"http://arxiv.org/abs/2403.02756v1","category":"cs.CL"}
{"created":"2024-03-05 07:58:02","title":"Self-adaptive Traffic Anomaly Detection System for IoT Smart Home Environments","abstract":"With the growth of internet of things (IoT) devices, cyberattacks, such as distributed denial of service, that exploit vulnerable devices infected with malware have increased. Therefore, vendors and users must keep their device firmware updated to eliminate vulnerabilities and quickly handle unknown cyberattacks. However, it is difficult for both vendors and users to continually keep the devices safe because vendors must provide updates quickly and the users must continuously manage the conditions of all deployed devices. Therefore, to ensure security, it is necessary for a system to adapt autonomously to changes in cyberattacks. In addition, it is important to consider network-side security that detects and filters anomalous traffic at the gateway to comprehensively protect those devices. This paper proposes a self-adaptive anomaly detection system for IoT traffic, including unknown attacks. The proposed system comprises a honeypot server and a gateway. The honeypot server continuously captures traffic and adaptively generates an anomaly detection model using real-time captured traffic. Thereafter, the gateway uses the generated model to detect anomalous traffic. Thus, the proposed system can adapt to unknown attacks to reflect pattern changes in anomalous traffic based on real-time captured traffic. Three experiments were conducted to evaluate the proposed system: a virtual experiment using pre-captured traffic from various regions across the world, a demonstration experiment using real-time captured traffic, and a virtual experiment using a public dataset containing the traffic generated by malware. The experimental results indicate that a system adaptable in real time to evolving cyberattacks is a novel approach for ensuring the comprehensive security of IoT devices against both known and unknown attacks.","sentences":["With the growth of internet of things (IoT) devices, cyberattacks, such as distributed denial of service, that exploit vulnerable devices infected with malware have increased.","Therefore, vendors and users must keep their device firmware updated to eliminate vulnerabilities and quickly handle unknown cyberattacks.","However, it is difficult for both vendors and users to continually keep the devices safe because vendors must provide updates quickly and the users must continuously manage the conditions of all deployed devices.","Therefore, to ensure security, it is necessary for a system to adapt autonomously to changes in cyberattacks.","In addition, it is important to consider network-side security that detects and filters anomalous traffic at the gateway to comprehensively protect those devices.","This paper proposes a self-adaptive anomaly detection system for IoT traffic, including unknown attacks.","The proposed system comprises a honeypot server and a gateway.","The honeypot server continuously captures traffic and adaptively generates an anomaly detection model using real-time captured traffic.","Thereafter, the gateway uses the generated model to detect anomalous traffic.","Thus, the proposed system can adapt to unknown attacks to reflect pattern changes in anomalous traffic based on real-time captured traffic.","Three experiments were conducted to evaluate the proposed system: a virtual experiment using pre-captured traffic from various regions across the world, a demonstration experiment using real-time captured traffic, and a virtual experiment using a public dataset containing the traffic generated by malware.","The experimental results indicate that a system adaptable in real time to evolving cyberattacks is a novel approach for ensuring the comprehensive security of IoT devices against both known and unknown attacks."],"url":"http://arxiv.org/abs/2403.02744v1","category":"cs.CR"}
{"created":"2024-03-05 07:43:24","title":"Distributed OpenMP Offloading of OpenMC on Intel GPU MAX Accelerators","abstract":"Monte Carlo (MC) simulations play a pivotal role in diverse scientific and engineering domains, with applications ranging from nuclear physics to materials science. Harnessing the computational power of high-performance computing (HPC) systems, especially Graphics Processing Units (GPUs), has become essential for accelerating MC simulations. This paper focuses on the adaptation and optimization of the OpenMC neutron and photon transport Monte Carlo code for Intel GPUs, specifically the Intel Data Center Max 1100 GPU (codename Ponte Vecchio, PVC), through distributed OpenMP offloading. Building upon prior work by Tramm J.R., et al. (2022), which laid the groundwork for GPU adaptation, our study meticulously extends the OpenMC code's capabilities to Intel GPUs. We present a comprehensive benchmarking and scaling analysis, comparing performance on Intel MAX GPUs to state-of-the-art CPU execution (Intel Xeon Platinum 8480+ Processor, codename 4th generation Sapphire Rapids). The results demonstrate a remarkable acceleration factor compared to CPU execution, showcasing the GPU-adapted code's superiority over its CPU counterpart as computational load increases.","sentences":["Monte Carlo (MC) simulations play a pivotal role in diverse scientific and engineering domains, with applications ranging from nuclear physics to materials science.","Harnessing the computational power of high-performance computing (HPC) systems, especially Graphics Processing Units (GPUs), has become essential for accelerating MC simulations.","This paper focuses on the adaptation and optimization of the OpenMC neutron and photon transport Monte Carlo code for Intel GPUs, specifically the Intel Data Center Max 1100 GPU (codename Ponte Vecchio, PVC), through distributed OpenMP offloading.","Building upon prior work by Tramm J.R., et al. (2022), which laid the groundwork for GPU adaptation, our study meticulously extends the OpenMC code's capabilities to Intel GPUs.","We present a comprehensive benchmarking and scaling analysis, comparing performance on Intel MAX GPUs to state-of-the-art CPU execution (Intel Xeon Platinum 8480+ Processor, codename 4th generation Sapphire Rapids).","The results demonstrate a remarkable acceleration factor compared to CPU execution, showcasing the GPU-adapted code's superiority over its CPU counterpart as computational load increases."],"url":"http://arxiv.org/abs/2403.02735v1","category":"cs.DC"}
{"created":"2024-03-05 07:29:12","title":"Minimum Topology Attacks for Graph Neural Networks","abstract":"With the great popularity of Graph Neural Networks (GNNs), their robustness to adversarial topology attacks has received significant attention. Although many attack methods have been proposed, they mainly focus on fixed-budget attacks, aiming at finding the most adversarial perturbations within a fixed budget for target node. However, considering the varied robustness of each node, there is an inevitable dilemma caused by the fixed budget, i.e., no successful perturbation is found when the budget is relatively small, while if it is too large, the yielding redundant perturbations will hurt the invisibility. To break this dilemma, we propose a new type of topology attack, named minimum-budget topology attack, aiming to adaptively find the minimum perturbation sufficient for a successful attack on each node. To this end, we propose an attack model, named MiBTack, based on a dynamic projected gradient descent algorithm, which can effectively solve the involving non-convex constraint optimization on discrete topology. Extensive results on three GNNs and four real-world datasets show that MiBTack can successfully lead all target nodes misclassified with the minimum perturbation edges. Moreover, the obtained minimum budget can be used to measure node robustness, so we can explore the relationships of robustness, topology, and uncertainty for nodes, which is beyond what the current fixed-budget topology attacks can offer.","sentences":["With the great popularity of Graph Neural Networks (GNNs), their robustness to adversarial topology attacks has received significant attention.","Although many attack methods have been proposed, they mainly focus on fixed-budget attacks, aiming at finding the most adversarial perturbations within a fixed budget for target node.","However, considering the varied robustness of each node, there is an inevitable dilemma caused by the fixed budget, i.e., no successful perturbation is found when the budget is relatively small, while if it is too large, the yielding redundant perturbations will hurt the invisibility.","To break this dilemma, we propose a new type of topology attack, named minimum-budget topology attack, aiming to adaptively find the minimum perturbation sufficient for a successful attack on each node.","To this end, we propose an attack model, named MiBTack, based on a dynamic projected gradient descent algorithm, which can effectively solve the involving non-convex constraint optimization on discrete topology.","Extensive results on three GNNs and four real-world datasets show that MiBTack can successfully lead all target nodes misclassified with the minimum perturbation edges.","Moreover, the obtained minimum budget can be used to measure node robustness, so we can explore the relationships of robustness, topology, and uncertainty for nodes, which is beyond what the current fixed-budget topology attacks can offer."],"url":"http://arxiv.org/abs/2403.02723v1","category":"cs.AI"}
{"created":"2024-03-05 07:25:38","title":"A mid-infrared dual-comb spectrometer in step-sweep mode for high-resolution molecular spectroscopy","abstract":"To meet the challenges of high-resolution molecular spectroscopy, increasingly sophisticated spectroscopic techniques were developed. For a long time FTIR and laser-based spectroscopies were used for these studies. The recent development of dual-comb spectroscopy at high-resolution makes this technique a powerful tool for gas phase studies. We report on the use and characterization of the IRis-F1, a tabletop mid-infrared dual-comb spectrometer, in the newly developed step-sweep mode. The resolution of the wavenumber axis is increased by step-wise tuning (interleaving) and accurate measurement of the laser center wavelength and repetition frequency. Doppler limited measurements of N2O and CH4 reveal a wavenumber accuracy of 1E-4 cm-1 on the covered range of > 50 cm-1. Measured half-widths of absorption lines show no systematic broadening, indicating a negligible instrument response function. Finally, measurements of nitrogen pressure broadening coefficients in the v4 band of methane show that quantum cascade laser dual-comb spectroscopy in step-sweep mode is well adapted for measurements of precision spectroscopic data, in particular line shape parameters.","sentences":["To meet the challenges of high-resolution molecular spectroscopy, increasingly sophisticated spectroscopic techniques were developed.","For a long time FTIR and laser-based spectroscopies were used for these studies.","The recent development of dual-comb spectroscopy at high-resolution makes this technique a powerful tool for gas phase studies.","We report on the use and characterization of the IRis-F1, a tabletop mid-infrared dual-comb spectrometer, in the newly developed step-sweep mode.","The resolution of the wavenumber axis is increased by step-wise tuning (interleaving) and accurate measurement of the laser center wavelength and repetition frequency.","Doppler limited measurements of N2O and CH4 reveal a wavenumber accuracy of 1E-4 cm-1 on the covered range of > 50 cm-1.","Measured half-widths of absorption lines show no systematic broadening, indicating a negligible instrument response function.","Finally, measurements of nitrogen pressure broadening coefficients in the v4 band of methane show that quantum cascade laser dual-comb spectroscopy in step-sweep mode is well adapted for measurements of precision spectroscopic data, in particular line shape parameters."],"url":"http://arxiv.org/abs/2403.02720v1","category":"physics.optics"}
{"created":"2024-03-05 07:10:25","title":"DomainVerse: A Benchmark Towards Real-World Distribution Shifts For Tuning-Free Adaptive Domain Generalization","abstract":"Traditional cross-domain tasks, including domain adaptation and domain generalization, rely heavily on training model by source domain data. With the recent advance of vision-language models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target domains equipped with prior domain knowledge, and we name this task Adaptive Domain Generalization (ADG). However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which drives us to establish a novel dataset DomainVerse for ADG. Benefiting from the introduced hierarchical definition of domain shifts, DomainVerse consists of about 0.5 million images from 390 fine-grained realistic domains. With the help of the constructed DomainVerse and VLMs, we propose two methods called Domain CLIP and Domain++ CLIP for tuning-free adaptive domain generalization. Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods.","sentences":["Traditional cross-domain tasks, including domain adaptation and domain generalization, rely heavily on training model by source domain data.","With the recent advance of vision-language models (VLMs), viewed as natural source models, the cross-domain task changes to directly adapt the pre-trained source model to arbitrary target domains equipped with prior domain knowledge, and we name this task Adaptive Domain Generalization (ADG).","However, current cross-domain datasets have many limitations, such as unrealistic domains, unclear domain definitions, and the inability to fine-grained domain decomposition, which drives us to establish a novel dataset DomainVerse for ADG.","Benefiting from the introduced hierarchical definition of domain shifts, DomainVerse consists of about 0.5 million images from 390 fine-grained realistic domains.","With the help of the constructed DomainVerse and VLMs, we propose two methods called Domain CLIP and Domain++ CLIP for tuning-free adaptive domain generalization.","Extensive and comprehensive experiments demonstrate the significance of the dataset and the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2403.02714v1","category":"cs.CV"}
{"created":"2024-03-05 06:57:37","title":"Enhancing Generalization in Medical Visual Question Answering Tasks via Gradient-Guided Model Perturbation","abstract":"Leveraging pre-trained visual language models has become a widely adopted approach for improving performance in downstream visual question answering (VQA) applications. However, in the specialized field of medical VQA, the scarcity of available data poses a significant barrier to achieving reliable model generalization. Numerous methods have been proposed to enhance model generalization, addressing the issue from data-centric and model-centric perspectives. Data augmentation techniques are commonly employed to enrich the dataset, while various regularization approaches aim to prevent model overfitting, especially when training on limited data samples. In this paper, we introduce a method that incorporates gradient-guided parameter perturbations to the visual encoder of the multimodality model during both pre-training and fine-tuning phases, to improve model generalization for downstream medical VQA tasks. The small perturbation is adaptively generated by aligning with the direction of the moving average gradient in the optimization landscape, which is opposite to the directions of the optimizer's historical updates. It is subsequently injected into the model's visual encoder. The results show that, even with a significantly smaller pre-training image caption dataset, our approach achieves competitive outcomes on both VQA-RAD and SLAKE datasets.","sentences":["Leveraging pre-trained visual language models has become a widely adopted approach for improving performance in downstream visual question answering (VQA) applications.","However, in the specialized field of medical VQA, the scarcity of available data poses a significant barrier to achieving reliable model generalization.","Numerous methods have been proposed to enhance model generalization, addressing the issue from data-centric and model-centric perspectives.","Data augmentation techniques are commonly employed to enrich the dataset, while various regularization approaches aim to prevent model overfitting, especially when training on limited data samples.","In this paper, we introduce a method that incorporates gradient-guided parameter perturbations to the visual encoder of the multimodality model during both pre-training and fine-tuning phases, to improve model generalization for downstream medical VQA tasks.","The small perturbation is adaptively generated by aligning with the direction of the moving average gradient in the optimization landscape, which is opposite to the directions of the optimizer's historical updates.","It is subsequently injected into the model's visual encoder.","The results show that, even with a significantly smaller pre-training image caption dataset, our approach achieves competitive outcomes on both VQA-RAD and SLAKE datasets."],"url":"http://arxiv.org/abs/2403.02707v1","category":"cs.CV"}
{"created":"2024-03-05 06:46:43","title":"Fighting Game Adaptive Background Music for Improved Gameplay","abstract":"This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding adaptive features. The adaptive BGM consists of three different categories of instruments playing the BGM of the winner sound design from the 2022 DareFightingICE Competition. The BGM adapts by changing the volume of each category of instruments. Each category is connected to a different element of the game. We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI agent that only uses audio as input (Blind DL AI). The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM.","sentences":["This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding adaptive features.","The adaptive BGM consists of three different categories of instruments playing the BGM of the winner sound design from the 2022 DareFightingICE Competition.","The BGM adapts by changing the volume of each category of instruments.","Each category is connected to a different element of the game.","We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI agent that only uses audio as input (Blind DL AI).","The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM."],"url":"http://arxiv.org/abs/2403.02701v1","category":"cs.SD"}
{"created":"2024-03-05 06:23:23","title":"Optimizing Mobile-Friendly Viewport Prediction for Live 360-Degree Video Streaming","abstract":"Viewport prediction is the crucial task for adaptive 360-degree video streaming, as the bitrate control algorithms usually require the knowledge of the user's viewing portions of the frames. Various methods are studied and adopted for viewport prediction from less accurate statistic tools to highly calibrated deep neural networks. Conventionally, it is difficult to implement sophisticated deep learning methods on mobile devices, which have limited computation capability. In this work, we propose an advanced learning-based viewport prediction approach and carefully design it to introduce minimal transmission and computation overhead for mobile terminals. We also propose a model-agnostic meta-learning (MAML) based saliency prediction network trainer, which provides a few-sample fast training solution to obtain the prediction model by utilizing the information from the past models. We further discuss how to integrate this mobile-friendly viewport prediction (MFVP) approach into a typical 360-degree video live streaming system by formulating and solving the bitrate adaptation problem. Extensive experiment results show that our prediction approach can work in real-time for live video streaming and can achieve higher accuracies compared to other existing prediction methods on mobile end, which, together with our bitrate adaptation algorithm, significantly improves the streaming QoE from various aspects. We observe the accuracy of MFVP is 8.1$\\%$ to 28.7$\\%$ higher than other algorithms and achieves 3.73$\\%$ to 14.96$\\%$ higher average quality level and 49.6$\\%$ to 74.97$\\%$ less quality level change than other algorithms.","sentences":["Viewport prediction is the crucial task for adaptive 360-degree video streaming, as the bitrate control algorithms usually require the knowledge of the user's viewing portions of the frames.","Various methods are studied and adopted for viewport prediction from less accurate statistic tools to highly calibrated deep neural networks.","Conventionally, it is difficult to implement sophisticated deep learning methods on mobile devices, which have limited computation capability.","In this work, we propose an advanced learning-based viewport prediction approach and carefully design it to introduce minimal transmission and computation overhead for mobile terminals.","We also propose a model-agnostic meta-learning (MAML) based saliency prediction network trainer, which provides a few-sample fast training solution to obtain the prediction model by utilizing the information from the past models.","We further discuss how to integrate this mobile-friendly viewport prediction (MFVP) approach into a typical 360-degree video live streaming system by formulating and solving the bitrate adaptation problem.","Extensive experiment results show that our prediction approach can work in real-time for live video streaming and can achieve higher accuracies compared to other existing prediction methods on mobile end, which, together with our bitrate adaptation algorithm, significantly improves the streaming QoE from various aspects.","We observe the accuracy of MFVP is 8.1$\\%$ to 28.7$\\%$ higher than other algorithms and achieves 3.73$\\%$ to 14.96$\\%$ higher average quality level and 49.6$\\%$ to 74.97$\\%$ less quality level change than other algorithms."],"url":"http://arxiv.org/abs/2403.02693v1","category":"cs.MM"}
{"created":"2024-03-05 06:17:13","title":"DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators","abstract":"Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments. However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations. While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism. To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally drifting noises. The DOCTOR framework intelligently monitors the chip status using adaptive probing and performs fast in-situ training-free calibration to restore accuracy when necessary. Recognizing nonuniform spatial variation distributions across devices and tensor cores, we also propose a variation-aware architectural remapping strategy to avoid executing critical tasks on noisy devices. Extensive experiments show that our proposed framework can guarantee sustained performance under drifting variations with 34% higher accuracy and 2-3 orders-of-magnitude lower overhead compared to state-of-the-art on-chip training methods.","sentences":["Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments.","However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations.","While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism.","To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally drifting noises.","The DOCTOR framework intelligently monitors the chip status using adaptive probing and performs fast in-situ training-free calibration to restore accuracy when necessary.","Recognizing nonuniform spatial variation distributions across devices and tensor cores, we also propose a variation-aware architectural remapping strategy to avoid executing critical tasks on noisy devices.","Extensive experiments show that our proposed framework can guarantee sustained performance under drifting variations with 34% higher accuracy and 2-3 orders-of-magnitude lower overhead compared to state-of-the-art on-chip training methods."],"url":"http://arxiv.org/abs/2403.02688v1","category":"cs.ET"}
{"created":"2024-03-05 06:10:28","title":"Learning to Defer to a Population: A Meta-Learning Approach","abstract":"The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert. All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained. In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time. We accomplish this by using meta-learning, considering both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert's abilities. In the experiments, we validate our methods on image recognition, traffic sign detection, and skin lesion diagnosis benchmarks.","sentences":["The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert.","All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained.","In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time.","We accomplish this by using meta-learning, considering both optimization- and model-based variants.","Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy.","For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert's abilities.","In the experiments, we validate our methods on image recognition, traffic sign detection, and skin lesion diagnosis benchmarks."],"url":"http://arxiv.org/abs/2403.02683v1","category":"cs.LG"}
{"created":"2024-03-05 06:10:22","title":"Time Weaver: A Conditional Time Series Generation Model","abstract":"Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (weather, location, etc.). Current approaches to time series generation often ignore this paired metadata, and its heterogeneity poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce Time Weaver, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series. Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series. We show that Time Weaver outperforms state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by up to 27% in downstream classification tasks on real-world energy, medical, air quality, and traffic data sets.","sentences":["Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze.","Such real-world time series are often enriched with paired heterogeneous contextual metadata (weather, location, etc.).","Current approaches to time series generation often ignore this paired metadata, and its heterogeneity poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain.","To address this gap, we introduce Time Weaver, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation.","Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient.","These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series.","Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series.","We show that Time Weaver outperforms state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by up to 27% in downstream classification tasks on real-world energy, medical, air quality, and traffic data sets."],"url":"http://arxiv.org/abs/2403.02682v1","category":"cs.LG"}
{"created":"2024-03-05 05:44:23","title":"Passive and active suppression of transduced noise in silicon spin qubits","abstract":"Addressing and mitigating decoherence sources plays an essential role in the development of a scalable quantum computing system, which requires low gate errors to be consistently maintained throughout the circuit execution. While nuclear spin-free materials, such as isotopically purified silicon, exhibit intrinsically promising coherence properties for electron spin qubits, the omnipresent charge noise, when converted to magnetic noise under a strong magnetic field gradient, often hinders stable qubit operation within a time frame comparable to the data acquisition time. Here, we demonstrate both open- and closed-loop suppression techniques for the transduced noise in silicon spin qubits, resulting in a more than two-fold (ten-fold) improvement of the inhomogeneous coherence time (Rabi oscillation quality) that leads to a single-qubit gate fidelity of over 99.6% even in the presence of a strong decoherence field gradient. Utilizing gate set tomography, we show that adaptive qubit control also reduces the non-Markovian noise in the system, which validates the stability of the gate fidelity. The technique can be used to learn multiple Hamiltonian parameters and is useful for the intermittent calibration of the circuit parameters with affordable experimental overhead, providing a useful subroutine during the repeated execution of general quantum circuits.","sentences":["Addressing and mitigating decoherence sources plays an essential role in the development of a scalable quantum computing system, which requires low gate errors to be consistently maintained throughout the circuit execution.","While nuclear spin-free materials, such as isotopically purified silicon, exhibit intrinsically promising coherence properties for electron spin qubits, the omnipresent charge noise, when converted to magnetic noise under a strong magnetic field gradient, often hinders stable qubit operation within a time frame comparable to the data acquisition time.","Here, we demonstrate both open- and closed-loop suppression techniques for the transduced noise in silicon spin qubits, resulting in a more than two-fold (ten-fold) improvement of the inhomogeneous coherence time (Rabi oscillation quality) that leads to a single-qubit gate fidelity of over 99.6% even in the presence of a strong decoherence field gradient.","Utilizing gate set tomography, we show that adaptive qubit control also reduces the non-Markovian noise in the system, which validates the stability of the gate fidelity.","The technique can be used to learn multiple Hamiltonian parameters and is useful for the intermittent calibration of the circuit parameters with affordable experimental overhead, providing a useful subroutine during the repeated execution of general quantum circuits."],"url":"http://arxiv.org/abs/2403.02666v1","category":"quant-ph"}
{"created":"2024-03-05 04:48:24","title":"Learning at the Speed of Wireless: Online Real-Time Learning for AI-Enabled MIMO in NextG","abstract":"Integration of artificial intelligence (AI) and machine learning (ML) into the air interface has been envisioned as a key technology for next-generation (NextG) cellular networks. At the air interface, multiple-input multiple-output (MIMO) and its variants such as multi-user MIMO (MU-MIMO) and massive/full-dimension MIMO have been key enablers across successive generations of cellular networks with evolving complexity and design challenges. Initiating active investigation into leveraging AI/ML tools to address these challenges for MIMO becomes a critical step towards an AI-enabled NextG air interface. At the NextG air interface, the underlying wireless environment will be extremely dynamic with operation adaptations performed on a sub-millisecond basis by MIMO operations such as MU-MIMO scheduling and rank/link adaptation. Given the enormously large number of operation adaptation possibilities, we contend that online real-time AI/ML-based approaches constitute a promising paradigm. To this end, we outline the inherent challenges and offer insights into the design of such online real-time AI/ML-based solutions for MIMO operations. An online real-time AI/ML-based method for MIMO-OFDM channel estimation is then presented, serving as a potential roadmap for developing similar techniques across various MIMO operations in NextG.","sentences":["Integration of artificial intelligence (AI) and machine learning (ML) into the air interface has been envisioned as a key technology for next-generation (NextG) cellular networks.","At the air interface, multiple-input multiple-output (MIMO) and its variants such as multi-user MIMO (MU-MIMO) and massive/full-dimension MIMO have been key enablers across successive generations of cellular networks with evolving complexity and design challenges.","Initiating active investigation into leveraging AI/ML tools to address these challenges for MIMO becomes a critical step towards an AI-enabled NextG air interface.","At the NextG air interface, the underlying wireless environment will be extremely dynamic with operation adaptations performed on a sub-millisecond basis by MIMO operations such as MU-MIMO scheduling and rank/link adaptation.","Given the enormously large number of operation adaptation possibilities, we contend that online real-time AI/ML-based approaches constitute a promising paradigm.","To this end, we outline the inherent challenges and offer insights into the design of such online real-time AI/ML-based solutions for MIMO operations.","An online real-time AI/ML-based method for MIMO-OFDM channel estimation is then presented, serving as a potential roadmap for developing similar techniques across various MIMO operations in NextG."],"url":"http://arxiv.org/abs/2403.02651v1","category":"eess.SP"}
{"created":"2024-03-05 04:38:13","title":"Few-shot Learner Parameterization by Diffusion Time-steps","abstract":"Even when using large multi-modal foundation models, few-shot learning is still challenging -- if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels. To this end, we find an inductive bias that the time-steps of a Diffusion Model (DM) can isolate the nuanced class attributes, i.e., as the forward diffusion adds noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent. Building on this, we propose Time-step Few-shot (TiF) learner. We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a prompt. Hence, at a small time-step, the adapter and prompt are essentially a parameterization of only the nuanced class attributes. For a test image, we can use the parameterization to only extract the nuanced class attributes for classification. TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized few-shot learning tasks. Codes are in https://github.com/yue-zhongqi/tif.","sentences":["Even when using large multi-modal foundation models, few-shot learning is still challenging -- if there is no proper inductive bias, it is nearly impossible to keep the nuanced class attributes while removing the visually prominent attributes that spuriously correlate with class labels.","To this end, we find an inductive bias that the time-steps of a Diffusion Model (DM) can isolate the nuanced class attributes, i.e., as the forward diffusion adds noise to an image at each time-step, nuanced attributes are usually lost at an earlier time-step than the spurious attributes that are visually prominent.","Building on this, we propose Time-step Few-shot (TiF) learner.","We train class-specific low-rank adapters for a text-conditioned DM to make up for the lost attributes, such that images can be accurately reconstructed from their noisy ones given a prompt.","Hence, at a small time-step, the adapter and prompt are essentially a parameterization of only the nuanced class attributes.","For a test image, we can use the parameterization to only extract the nuanced class attributes for classification.","TiF learner significantly outperforms OpenCLIP and its adapters on a variety of fine-grained and customized few-shot learning tasks.","Codes are in https://github.com/yue-zhongqi/tif."],"url":"http://arxiv.org/abs/2403.02649v1","category":"cs.CV"}
{"created":"2024-03-05 04:35:59","title":"Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad","abstract":"Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \\left(\\frac{\\log T}{\\sqrt{T}} \\right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.","sentences":["Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive.","This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm.","We prove the scale-invariance of KATE for the case of Generalized Linear Models.","Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \\left(\\frac{\\log T}{\\sqrt{T}} \\right)$ for KATE, matching the best-known ones for AdaGrad and Adam.","We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data.","The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios."],"url":"http://arxiv.org/abs/2403.02648v1","category":"cs.LG"}
{"created":"2024-03-05 03:59:01","title":"PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of Multi-Agent Reinforcement Learning","abstract":"Training for multi-agent reinforcement learning(MARL) is a time-consuming process caused by distribution shift of each agent. One drawback is that strategy of each agent in MARL is independent but actually in cooperation. Thus, a vertical issue in multi-agent reinforcement learning is how to efficiently accelerate training process. To address this problem, current research has leveraged a centralized function(CF) across multiple agents to learn contribution of the team reward for each agent. However, CF based methods introduce joint error from other agents in estimation of value network. In so doing, inspired by federated learning, we propose three simple novel approaches called Average Periodically Parameter Sharing(A-PPS), Reward-Scalability Periodically Parameter Sharing(RS-PPS) and Partial Personalized Periodically Parameter Sharing(PP-PPS) mechanism to accelerate training of MARL. Agents share Q-value network periodically during the training process. Agents which has same identity adapt collected reward as scalability and update partial neural network during period to share different parameters. We apply our approaches in classical MARL method QMIX and evaluate our approaches on various tasks in StarCraft Multi-Agent Challenge(SMAC) environment. Performance of numerical experiments yield enormous enhancement, with an average improvement of 10\\%-30\\%, and enable to win tasks that QMIX cannot. Our code can be downloaded from https://github.com/ColaZhang22/PPS-QMIX","sentences":["Training for multi-agent reinforcement learning(MARL) is a time-consuming process caused by distribution shift of each agent.","One drawback is that strategy of each agent in MARL is independent but actually in cooperation.","Thus, a vertical issue in multi-agent reinforcement learning is how to efficiently accelerate training process.","To address this problem, current research has leveraged a centralized function(CF) across multiple agents to learn contribution of the team reward for each agent.","However, CF based methods introduce joint error from other agents in estimation of value network.","In so doing, inspired by federated learning, we propose three simple novel approaches called Average Periodically Parameter Sharing(A-PPS), Reward-Scalability Periodically Parameter Sharing(RS-PPS) and Partial Personalized Periodically Parameter Sharing(PP-PPS) mechanism to accelerate training of MARL.","Agents share Q-value network periodically during the training process.","Agents which has same identity adapt collected reward as scalability and update partial neural network during period to share different parameters.","We apply our approaches in classical MARL method QMIX and evaluate our approaches on various tasks in StarCraft Multi-Agent Challenge(SMAC) environment.","Performance of numerical experiments yield enormous enhancement, with an average improvement of 10\\%-30\\%, and enable to win tasks that QMIX cannot.","Our code can be downloaded from https://github.com/ColaZhang22/PPS-QMIX"],"url":"http://arxiv.org/abs/2403.02635v1","category":"cs.AI"}
{"created":"2024-03-05 03:55:17","title":"Human Activity Recognition with Low-Resolution Infrared Array Sensor Using Semi-supervised Cross-domain Neural Networks for Indoor Environment","abstract":"Low-resolution infrared-based human activity recognition (HAR) attracted enormous interests due to its low-cost and private. In this paper, a novel semi-supervised crossdomain neural network (SCDNN) based on 8 $\\times$ 8 low-resolution infrared sensor is proposed for accurately identifying human activity despite changes in the environment at a low-cost. The SCDNN consists of feature extractor, domain discriminator and label classifier. In the feature extractor, the unlabeled and minimal labeled target domain data are trained for domain adaptation to achieve a mapping of the source domain and target domain data. The domain discriminator employs the unsupervised learning to migrate data from the source domain to the target domain. The label classifier obtained from training the source domain data improves the recognition of target domain activities due to the semi-supervised learning utilized in training the target domain data. Experimental results show that the proposed method achieves 92.12\\% accuracy for recognition of activities in the target domain by migrating the source and target domains. The proposed approach adapts superior to cross-domain scenarios compared to the existing deep learning methods, and it provides a low-cost yet highly adaptable solution for cross-domain scenarios.","sentences":["Low-resolution infrared-based human activity recognition (HAR) attracted enormous interests due to its low-cost and private.","In this paper, a novel semi-supervised crossdomain neural network (SCDNN) based on 8 $\\times$ 8 low-resolution infrared sensor is proposed for accurately identifying human activity despite changes in the environment at a low-cost.","The SCDNN consists of feature extractor, domain discriminator and label classifier.","In the feature extractor, the unlabeled and minimal labeled target domain data are trained for domain adaptation to achieve a mapping of the source domain and target domain data.","The domain discriminator employs the unsupervised learning to migrate data from the source domain to the target domain.","The label classifier obtained from training the source domain data improves the recognition of target domain activities due to the semi-supervised learning utilized in training the target domain data.","Experimental results show that the proposed method achieves 92.12\\% accuracy for recognition of activities in the target domain by migrating the source and target domains.","The proposed approach adapts superior to cross-domain scenarios compared to the existing deep learning methods, and it provides a low-cost yet highly adaptable solution for cross-domain scenarios."],"url":"http://arxiv.org/abs/2403.02632v1","category":"eess.SP"}
{"created":"2024-03-05 03:11:02","title":"Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive Anomaly Diagnosis of Industrial Cyber-physical Systems","abstract":"Accurate detection and diagnosis of abnormal behaviors such as network attacks from multivariate time series (MTS) are crucial for ensuring the stable and effective operation of industrial cyber-physical systems (CPS). However, existing researches pay little attention to the logical dependencies among system working states, and have difficulties in explaining the evolution mechanisms of abnormal signals. To reveal the spatio-temporal association relationships and evolution mechanisms of the working states of industrial CPS, this paper proposes a fine-grained adaptive anomaly diagnosis method (i.e. MAD-Transformer) to identify and diagnose anomalies in MTS. MAD-Transformer first constructs a temporal state matrix to characterize and estimate the change patterns of the system states in the temporal dimension. Then, to better locate the anomalies, a spatial state matrix is also constructed to capture the inter-sensor state correlation relationships within the system. Subsequently, based on these two types of state matrices, a three-branch structure of series-temporal-spatial attention module is designed to simultaneously capture the series, temporal, and space dependencies among MTS. Afterwards, three associated alignment loss functions and a reconstruction loss are constructed to jointly optimize the model. Finally, anomalies are determined and diagnosed by comparing the residual matrices with the original matrices. We conducted comparative experiments on five publicly datasets spanning three application domains (service monitoring, spatial and earth exploration, and water treatment), along with a petroleum refining simulation dataset collected by ourselves. The results demonstrate that MAD-Transformer can adaptively detect fine-grained anomalies with short duration, and outperforms the state-of-the-art baselines in terms of noise robustness and localization performance.","sentences":["Accurate detection and diagnosis of abnormal behaviors such as network attacks from multivariate time series (MTS) are crucial for ensuring the stable and effective operation of industrial cyber-physical systems (CPS).","However, existing researches pay little attention to the logical dependencies among system working states, and have difficulties in explaining the evolution mechanisms of abnormal signals.","To reveal the spatio-temporal association relationships and evolution mechanisms of the working states of industrial CPS, this paper proposes a fine-grained adaptive anomaly diagnosis method (i.e. MAD-Transformer) to identify and diagnose anomalies in MTS.","MAD-Transformer first constructs a temporal state matrix to characterize and estimate the change patterns of the system states in the temporal dimension.","Then, to better locate the anomalies, a spatial state matrix is also constructed to capture the inter-sensor state correlation relationships within the system.","Subsequently, based on these two types of state matrices, a three-branch structure of series-temporal-spatial attention module is designed to simultaneously capture the series, temporal, and space dependencies among MTS.","Afterwards, three associated alignment loss functions and a reconstruction loss are constructed to jointly optimize the model.","Finally, anomalies are determined and diagnosed by comparing the residual matrices with the original matrices.","We conducted comparative experiments on five publicly datasets spanning three application domains (service monitoring, spatial and earth exploration, and water treatment), along with a petroleum refining simulation dataset collected by ourselves.","The results demonstrate that MAD-Transformer can adaptively detect fine-grained anomalies with short duration, and outperforms the state-of-the-art baselines in terms of noise robustness and localization performance."],"url":"http://arxiv.org/abs/2403.02616v1","category":"cs.LG"}
{"created":"2024-03-05 03:07:10","title":"Exploring the Limitations of Large Language Models in Compositional Relation Reasoning","abstract":"We present a comprehensive evaluation of large language models(LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other. Acknowledging the significance of multilingual capabilities, we expanded our assessment to include translations of these cases into Chinese, Japanese, French, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling composition relation reasoning across diverse linguistic contexts.","sentences":["We present a comprehensive evaluation of large language models(LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other.","Acknowledging the significance of multilingual capabilities, we expanded our assessment to include translations of these cases into Chinese, Japanese, French, and Korean.","Our Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling composition relation reasoning across diverse linguistic contexts."],"url":"http://arxiv.org/abs/2403.02615v1","category":"cs.CL"}
{"created":"2024-03-05 02:29:18","title":"Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning","abstract":"For image super-resolution (SR), bridging the gap between the performance on synthetic datasets and real-world degradation scenarios remains a challenge. This work introduces a novel \"Low-Res Leads the Way\" (LWay) training framework, merging Supervised Pre-training with Self-supervised Learning to enhance the adaptability of SR models to real-world images. Our approach utilizes a low-resolution (LR) reconstruction network to extract degradation embeddings from LR images, merging them with super-resolved outputs for LR reconstruction. Leveraging unseen LR images for self-supervised learning guides the model to adapt its modeling space to the target domain, facilitating fine-tuning of SR models without requiring paired high-resolution (HR) images. The integration of Discrete Wavelet Transform (DWT) further refines the focus on high-frequency details. Extensive evaluations show that our method significantly improves the generalization and detail restoration capabilities of SR models on unseen real-world datasets, outperforming existing methods. Our training regime is universally compatible, requiring no network architecture modifications, making it a practical solution for real-world SR applications.","sentences":["For image super-resolution (SR), bridging the gap between the performance on synthetic datasets and real-world degradation scenarios remains a challenge.","This work introduces a novel \"Low-Res Leads the Way\" (LWay) training framework, merging Supervised Pre-training with Self-supervised Learning to enhance the adaptability of SR models to real-world images.","Our approach utilizes a low-resolution (LR) reconstruction network to extract degradation embeddings from LR images, merging them with super-resolved outputs for LR reconstruction.","Leveraging unseen LR images for self-supervised learning guides the model to adapt its modeling space to the target domain, facilitating fine-tuning of SR models without requiring paired high-resolution (HR) images.","The integration of Discrete Wavelet Transform (DWT) further refines the focus on high-frequency details.","Extensive evaluations show that our method significantly improves the generalization and detail restoration capabilities of SR models on unseen real-world datasets, outperforming existing methods.","Our training regime is universally compatible, requiring no network architecture modifications, making it a practical solution for real-world SR applications."],"url":"http://arxiv.org/abs/2403.02601v1","category":"eess.IV"}
{"created":"2024-03-05 02:27:52","title":"TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts","abstract":"Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events. Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for temporal characteristics and in-situ modeling. In this paper, we propose a novel deep learning model named TESTAM, which individually models recurring and non-recurring traffic patterns by a mixture-of-experts model with three experts on temporal modeling, spatio-temporal modeling with static graph, and dynamic spatio-temporal dependency modeling with dynamic graph. By introducing different experts and properly routing them, TESTAM could better model various circumstances, including spatially isolated nodes, highly related nodes, and recurring and non-recurring events. For the proper routing, we reformulate a gating problem into a classification problem with pseudo labels. Experimental results on three public traffic network datasets, METR-LA, PEMS-BAY, and EXPY-TKY, demonstrate that TESTAM achieves a better indication and modeling of recurring and non-recurring traffic. We published the official code at https://github.com/HyunWookL/TESTAM","sentences":["Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events.","Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for temporal characteristics and in-situ modeling.","In this paper, we propose a novel deep learning model named TESTAM, which individually models recurring and non-recurring traffic patterns by a mixture-of-experts model with three experts on temporal modeling, spatio-temporal modeling with static graph, and dynamic spatio-temporal dependency modeling with dynamic graph.","By introducing different experts and properly routing them, TESTAM could better model various circumstances, including spatially isolated nodes, highly related nodes, and recurring and non-recurring events.","For the proper routing, we reformulate a gating problem into a classification problem with pseudo labels.","Experimental results on three public traffic network datasets, METR-LA, PEMS-BAY, and EXPY-TKY, demonstrate that TESTAM achieves a better indication and modeling of recurring and non-recurring traffic.","We published the official code at https://github.com/HyunWookL/TESTAM"],"url":"http://arxiv.org/abs/2403.02600v1","category":"cs.LG"}
{"created":"2024-03-05 01:37:37","title":"Generative Software Engineering","abstract":"The rapid development of deep learning techniques, improved computational power, and the availability of vast training data have led to significant advancements in pre-trained models and large language models (LLMs). Pre-trained models based on architectures such as BERT and Transformer, as well as LLMs like ChatGPT, have demonstrated remarkable language capabilities and found applications in Software engineering. Software engineering tasks can be divided into many categories, among which generative tasks are the most concern by researchers, where pre-trained models and LLMs possess powerful language representation and contextual awareness capabilities, enabling them to leverage diverse training data and adapt to generative tasks through fine-tuning, transfer learning, and prompt engineering. These advantages make them effective tools in generative tasks and have demonstrated excellent performance. In this paper, we present a comprehensive literature review of generative tasks in SE using pre-trained models and LLMs. We accurately categorize SE generative tasks based on software engineering methodologies and summarize the advanced pre-trained models and LLMs involved, as well as the datasets and evaluation metrics used. Additionally, we identify key strengths, weaknesses, and gaps in existing approaches, and propose potential research directions. This review aims to provide researchers and practitioners with an in-depth analysis and guidance on the application of pre-trained models and LLMs in generative tasks within SE.","sentences":["The rapid development of deep learning techniques, improved computational power, and the availability of vast training data have led to significant advancements in pre-trained models and large language models (LLMs).","Pre-trained models based on architectures such as BERT and Transformer, as well as LLMs like ChatGPT, have demonstrated remarkable language capabilities and found applications in Software engineering.","Software engineering tasks can be divided into many categories, among which generative tasks are the most concern by researchers, where pre-trained models and LLMs possess powerful language representation and contextual awareness capabilities, enabling them to leverage diverse training data and adapt to generative tasks through fine-tuning, transfer learning, and prompt engineering.","These advantages make them effective tools in generative tasks and have demonstrated excellent performance.","In this paper, we present a comprehensive literature review of generative tasks in SE using pre-trained models and LLMs.","We accurately categorize SE generative tasks based on software engineering methodologies and summarize the advanced pre-trained models and LLMs involved, as well as the datasets and evaluation metrics used.","Additionally, we identify key strengths, weaknesses, and gaps in existing approaches, and propose potential research directions.","This review aims to provide researchers and practitioners with an in-depth analysis and guidance on the application of pre-trained models and LLMs in generative tasks within SE."],"url":"http://arxiv.org/abs/2403.02583v1","category":"cs.SE"}
{"created":"2024-03-05 01:14:08","title":"Topological protection revealed by real-time longitudinal and transverse studies","abstract":"Topology provides an essential concept for achieving unchanged (or protected) quantum properties in the presence of perturbations. A challenge facing realistic applications is that the level of protection displayed in real systems is subject to substantial variations. Some key differences stem from mechanisms influencing the reconstruction behaviors of extended dissipationless modes. Despite various insightful results on potential causes of backscattering, the edge-state-based approach is limited because the bulk states, as shown by breakdown tests, contribute indispensably. This study investigates the influence of bulk reconstruction where dissipationless modes are global objects instead of being restricted to the sample edge. An integer quantum Hall effect (IQHE) hosted in a Corbino sample geometry is adopted and brought continuously to the verge of a breakdown. A detection technique is developed to include two independent setups capable of simultaneously capturing the onset of dissipation in both longitudinal and transverse directions. The real-time correspondence between orthogonal results confirms two facts. 1. Dissipationless charge modes undergo frequent reconstruction in response to electrochemical potential changes, causing dissipationless current paths to expand transversely into the bulk while preserving chirality. A breakdown only occurs when a backscattering emerges between reconfigured dissipationless current paths bridging opposite edge contacts. 2. Impurity screening is vital in enhancing protection, and topological protection is subject to an intriguing interplay of disorder, electron-electron interaction, and topology. The proposed reconstruction mechanism qualitatively explains the robustness variations, beneficial for developing means for optimization.","sentences":["Topology provides an essential concept for achieving unchanged (or protected) quantum properties in the presence of perturbations.","A challenge facing realistic applications is that the level of protection displayed in real systems is subject to substantial variations.","Some key differences stem from mechanisms influencing the reconstruction behaviors of extended dissipationless modes.","Despite various insightful results on potential causes of backscattering, the edge-state-based approach is limited because the bulk states, as shown by breakdown tests, contribute indispensably.","This study investigates the influence of bulk reconstruction where dissipationless modes are global objects instead of being restricted to the sample edge.","An integer quantum Hall effect (IQHE) hosted in a Corbino sample geometry is adopted and brought continuously to the verge of a breakdown.","A detection technique is developed to include two independent setups capable of simultaneously capturing the onset of dissipation in both longitudinal and transverse directions.","The real-time correspondence between orthogonal results confirms two facts.","1.","Dissipationless charge modes undergo frequent reconstruction in response to electrochemical potential changes, causing dissipationless current paths to expand transversely into the bulk while preserving chirality.","A breakdown only occurs when a backscattering emerges between reconfigured dissipationless current paths bridging opposite edge contacts.","2.","Impurity screening is vital in enhancing protection, and topological protection is subject to an intriguing interplay of disorder, electron-electron interaction, and topology.","The proposed reconstruction mechanism qualitatively explains the robustness variations, beneficial for developing means for optimization."],"url":"http://arxiv.org/abs/2403.02575v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-05 00:27:43","title":"Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research","abstract":"Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data (\"zero-\" or \"few-shot\" approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the \"Minimum information about clinical artificial intelligence modeling\" (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in 2020, provided a set of six steps with guidelines on the minimum information necessary to encourage transparent, reproducible research for artificial intelligence (AI) in medicine. Here, we propose modifications to the original checklist that highlight differences in training, evaluation, interpretability, and reproducibility of generative models compared to traditional AI models for clinical research. This updated checklist also seeks to clarify cohort selection reporting and adds additional items on alignment with ethical standards.","sentences":["Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed.","While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks.","In particular, the ability of these models to produce useful outputs with little to no specialized training data (\"zero-\" or \"few-shot\" approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models.","In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we begin to formalize some of these guidelines by building on the \"Minimum information about clinical artificial intelligence modeling\" (MI-CLAIM) checklist.","The MI-CLAIM checklist, originally developed in 2020, provided a set of six steps with guidelines on the minimum information necessary to encourage transparent, reproducible research for artificial intelligence (AI) in medicine.","Here, we propose modifications to the original checklist that highlight differences in training, evaluation, interpretability, and reproducibility of generative models compared to traditional AI models for clinical research.","This updated checklist also seeks to clarify cohort selection reporting and adds additional items on alignment with ethical standards."],"url":"http://arxiv.org/abs/2403.02558v1","category":"cs.CL"}
{"created":"2024-03-04 23:40:20","title":"Wukong: Towards a Scaling Law for Large-Scale Recommendation","abstract":"Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wukong's scalability on an internal, large-scale dataset. The results show that Wukong retains its superiority in quality over state-of-the-art models, while holding the scaling law across two orders of magnitude in model complexity, extending beyond 100 Gflop or equivalently up to GPT-3/LLaMa-2 scale of total training compute, where prior arts fall short.","sentences":["Scaling laws play an instrumental role in the sustainable improvement in model quality.","Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms.","This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets.","In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation.","Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers.","We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise.","Further, we assessed Wukong's scalability on an internal, large-scale dataset.","The results show that Wukong retains its superiority in quality over state-of-the-art models, while holding the scaling law across two orders of magnitude in model complexity, extending beyond 100 Gflop or equivalently up to GPT-3/LLaMa-2 scale of total training compute, where prior arts fall short."],"url":"http://arxiv.org/abs/2403.02545v1","category":"cs.LG"}
{"created":"2024-03-04 21:41:27","title":"Pseudo-Labeling and Contextual Curriculum Learning for Online Grasp Learning in Robotic Bin Picking","abstract":"The prevailing grasp prediction methods predominantly rely on offline learning, overlooking the dynamic grasp learning that occurs during real-time adaptation to novel picking scenarios. These scenarios may involve previously unseen objects, variations in camera perspectives, and bin configurations, among other factors. In this paper, we introduce a novel approach, SSL-ConvSAC, that combines semi-supervised learning and reinforcement learning for online grasp learning. By treating pixels with reward feedback as labeled data and others as unlabeled, it efficiently exploits unlabeled data to enhance learning. In addition, we address the imbalance between labeled and unlabeled data by proposing a contextual curriculum-based method. We ablate the proposed approach on real-world evaluation data and demonstrate promise for improving online grasp learning on bin picking tasks using a physical 7-DoF Franka Emika robot arm with a suction gripper. Video: https://youtu.be/OAro5pg8I9U","sentences":["The prevailing grasp prediction methods predominantly rely on offline learning, overlooking the dynamic grasp learning that occurs during real-time adaptation to novel picking scenarios.","These scenarios may involve previously unseen objects, variations in camera perspectives, and bin configurations, among other factors.","In this paper, we introduce a novel approach, SSL-ConvSAC, that combines semi-supervised learning and reinforcement learning for online grasp learning.","By treating pixels with reward feedback as labeled data and others as unlabeled, it efficiently exploits unlabeled data to enhance learning.","In addition, we address the imbalance between labeled and unlabeled data by proposing a contextual curriculum-based method.","We ablate the proposed approach on real-world evaluation data and demonstrate promise for improving online grasp learning on bin picking tasks using a physical 7-DoF Franka Emika robot arm with a suction gripper.","Video: https://youtu.be/OAro5pg8I9U"],"url":"http://arxiv.org/abs/2403.02495v1","category":"cs.RO"}
{"created":"2024-03-04 21:04:54","title":"MORBDD: Multiobjective Restricted Binary Decision Diagrams by Learning to Sparsify","abstract":"In multicriteria decision-making, a user seeks a set of non-dominated solutions to a (constrained) multiobjective optimization problem, the so-called Pareto frontier. In this work, we seek to bring a state-of-the-art method for exact multiobjective integer linear programming into the heuristic realm. We focus on binary decision diagrams (BDDs) which first construct a graph that represents all feasible solutions to the problem and then traverse the graph to extract the Pareto frontier. Because the Pareto frontier may be exponentially large, enumerating it over the BDD can be time-consuming. We explore how restricted BDDs, which have already been shown to be effective as heuristics for single-objective problems, can be adapted to multiobjective optimization through the use of machine learning (ML). MORBDD, our ML-based BDD sparsifier, first trains a binary classifier to eliminate BDD nodes that are unlikely to contribute to Pareto solutions, then post-processes the sparse BDD to ensure its connectivity via optimization. Experimental results on multiobjective knapsack problems show that MORBDD is highly effective at producing very small restricted BDDs with excellent approximation quality, outperforming width-limited restricted BDDs and the well-known evolutionary algorithm NSGA-II.","sentences":["In multicriteria decision-making, a user seeks a set of non-dominated solutions to a (constrained) multiobjective optimization problem, the so-called Pareto frontier.","In this work, we seek to bring a state-of-the-art method for exact multiobjective integer linear programming into the heuristic realm.","We focus on binary decision diagrams (BDDs) which first construct a graph that represents all feasible solutions to the problem and then traverse the graph to extract the Pareto frontier.","Because the Pareto frontier may be exponentially large, enumerating it over the BDD can be time-consuming.","We explore how restricted BDDs, which have already been shown to be effective as heuristics for single-objective problems, can be adapted to multiobjective optimization through the use of machine learning (ML).","MORBDD, our ML-based BDD sparsifier, first trains a binary classifier to eliminate BDD nodes that are unlikely to contribute to Pareto solutions, then post-processes the sparse BDD to ensure its connectivity via optimization.","Experimental results on multiobjective knapsack problems show that MORBDD is highly effective at producing very small restricted BDDs with excellent approximation quality, outperforming width-limited restricted BDDs and the well-known evolutionary algorithm NSGA-II."],"url":"http://arxiv.org/abs/2403.02482v1","category":"cs.AI"}
{"created":"2024-03-04 20:20:14","title":"MagicClay: Sculpting Meshes With Generative Neural Fields","abstract":"The recent developments in neural fields have brought phenomenal capabilities to the field of shape generation, but they lack crucial properties, such as incremental control - a fundamental requirement for artistic work. Triangular meshes, on the other hand, are the representation of choice for most geometry related tasks, offering efficiency and intuitive control, but do not lend themselves to neural optimization. To support downstream tasks, previous art typically proposes a two-step approach, where first a shape is generated using neural fields, and then a mesh is extracted for further processing. Instead, in this paper we introduce a hybrid approach that maintains both a mesh and a Signed Distance Field (SDF) representations consistently. Using this representation, we introduce MagicClay - an artist friendly tool for sculpting regions of a mesh according to textual prompts while keeping other regions untouched. Our framework carefully and efficiently balances consistency between the representations and regularizations in every step of the shape optimization; Relying on the mesh representation, we show how to render the SDF at higher resolutions and faster. In addition, we employ recent work in differentiable mesh reconstruction to adaptively allocate triangles in the mesh where required, as indicated by the SDF. Using an implemented prototype, we demonstrate superior generated geometry compared to the state-of-the-art, and novel consistent control, allowing sequential prompt-based edits to the same mesh for the first time.","sentences":["The recent developments in neural fields have brought phenomenal capabilities to the field of shape generation, but they lack crucial properties, such as incremental control - a fundamental requirement for artistic work.","Triangular meshes, on the other hand, are the representation of choice for most geometry related tasks, offering efficiency and intuitive control, but do not lend themselves to neural optimization.","To support downstream tasks, previous art typically proposes a two-step approach, where first a shape is generated using neural fields, and then a mesh is extracted for further processing.","Instead, in this paper we introduce a hybrid approach that maintains both a mesh and a Signed Distance Field (SDF) representations consistently.","Using this representation, we introduce MagicClay - an artist friendly tool for sculpting regions of a mesh according to textual prompts while keeping other regions untouched.","Our framework carefully and efficiently balances consistency between the representations and regularizations in every step of the shape optimization; Relying on the mesh representation, we show how to render the SDF at higher resolutions and faster.","In addition, we employ recent work in differentiable mesh reconstruction to adaptively allocate triangles in the mesh where required, as indicated by the SDF.","Using an implemented prototype, we demonstrate superior generated geometry compared to the state-of-the-art, and novel consistent control, allowing sequential prompt-based edits to the same mesh for the first time."],"url":"http://arxiv.org/abs/2403.02460v1","category":"cs.GR"}
{"created":"2024-03-04 19:26:39","title":"On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation","abstract":"We study a new technique for understanding convergence of learning agents under small modifications of data. We show that such convergence can be understood via an analogue of Fatou's lemma which yields gamma-convergence. We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning.","sentences":["We study a new technique for understanding convergence of learning agents under small modifications of data.","We show that such convergence can be understood via an analogue of Fatou's lemma which yields gamma-convergence.","We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning."],"url":"http://arxiv.org/abs/2403.02432v1","category":"stat.ML"}
{"created":"2024-03-04 19:23:50","title":"Bayesian Constraint Inference from User Demonstrations Based on Margin-Respecting Preference Models","abstract":"It is crucial for robots to be aware of the presence of constraints in order to acquire safe policies. However, explicitly specifying all constraints in an environment can be a challenging task. State-of-the-art constraint inference algorithms learn constraints from demonstrations, but tend to be computationally expensive and prone to instability issues. In this paper, we propose a novel Bayesian method that infers constraints based on preferences over demonstrations. The main advantages of our proposed approach are that it 1) infers constraints without calculating a new policy at each iteration, 2) uses a simple and more realistic ranking of groups of demonstrations, without requiring pairwise comparisons over all demonstrations, and 3) adapts to cases where there are varying levels of constraint violation. Our empirical results demonstrate that our proposed Bayesian approach infers constraints of varying severity, more accurately than state-of-the-art constraint inference methods.","sentences":["It is crucial for robots to be aware of the presence of constraints in order to acquire safe policies.","However, explicitly specifying all constraints in an environment can be a challenging task.","State-of-the-art constraint inference algorithms learn constraints from demonstrations, but tend to be computationally expensive and prone to instability issues.","In this paper, we propose a novel Bayesian method that infers constraints based on preferences over demonstrations.","The main advantages of our proposed approach are that it 1) infers constraints without calculating a new policy at each iteration, 2) uses a simple and more realistic ranking of groups of demonstrations, without requiring pairwise comparisons over all demonstrations, and 3) adapts to cases where there are varying levels of constraint violation.","Our empirical results demonstrate that our proposed Bayesian approach infers constraints of varying severity, more accurately than state-of-the-art constraint inference methods."],"url":"http://arxiv.org/abs/2403.02431v1","category":"cs.RO"}
{"created":"2024-03-04 19:11:39","title":"Successive quasienergy collapse and the driven Dicke phase transition in the few-emitter limit","abstract":"The emergent behavior that arises in many-body systems of increasing size follows universal laws that become apparent in order-to-disorder transitions. While this behavior has been traditionally explored for large numbers of emitters, recent progress allows for the exploration of the few-emitter limit, where correlations can be measured and connected to microscopic models to gain further insight into order-to-disorder transitions. We explore this few-body limit in the driven and damped Tavis--Cummings model, which describes a collection of atoms interacting with a driven and damped cavity mode. Our exploration revolves around the dressed states of the atomic ensemble and field, whose energies are shown to collapse as the driving field is increased to mark the onset of a dissipative quantum phase transition. The collapse occurs in stages and is an effect of light-matter correlations that are overlooked for single atoms and neglected in mean-field models. The implications of these correlations over the macroscopic observables of the system are presented. We encounter a shift in the expected transition point and an increased number of parity-broken states to choose from once the ordered phase is reached.","sentences":["The emergent behavior that arises in many-body systems of increasing size follows universal laws that become apparent in order-to-disorder transitions.","While this behavior has been traditionally explored for large numbers of emitters, recent progress allows for the exploration of the few-emitter limit, where correlations can be measured and connected to microscopic models to gain further insight into order-to-disorder transitions.","We explore this few-body limit in the driven and damped Tavis--Cummings model, which describes a collection of atoms interacting with a driven and damped cavity mode.","Our exploration revolves around the dressed states of the atomic ensemble and field, whose energies are shown to collapse as the driving field is increased to mark the onset of a dissipative quantum phase transition.","The collapse occurs in stages and is an effect of light-matter correlations that are overlooked for single atoms and neglected in mean-field models.","The implications of these correlations over the macroscopic observables of the system are presented.","We encounter a shift in the expected transition point and an increased number of parity-broken states to choose from once the ordered phase is reached."],"url":"http://arxiv.org/abs/2403.02417v1","category":"quant-ph"}
{"created":"2024-03-04 19:06:13","title":"A Spatio-temporal Aligned SUNet Model for Low-light Video Enhancement","abstract":"Distortions caused by low-light conditions are not only visually unpleasant but also degrade the performance of computer vision tasks. The restoration and enhancement have proven to be highly beneficial. However, there are only a limited number of enhancement methods explicitly designed for videos acquired in low-light conditions. We propose a Spatio-Temporal Aligned SUNet (STA-SUNet) model using a Swin Transformer as a backbone to capture low light video features and exploit their spatio-temporal correlations. The STA-SUNet model is trained on a novel, fully registered dataset (BVI), which comprises dynamic scenes captured under varying light conditions. It is further analysed comparatively against various other models over three test datasets. The model demonstrates superior adaptivity across all datasets, obtaining the highest PSNR and SSIM values. It is particularly effective in extreme low-light conditions, yielding fairly good visualisation results.","sentences":["Distortions caused by low-light conditions are not only visually unpleasant but also degrade the performance of computer vision tasks.","The restoration and enhancement have proven to be highly beneficial.","However, there are only a limited number of enhancement methods explicitly designed for videos acquired in low-light conditions.","We propose a Spatio-Temporal Aligned SUNet (STA-SUNet) model using a Swin Transformer as a backbone to capture low light video features and exploit their spatio-temporal correlations.","The STA-SUNet model is trained on a novel, fully registered dataset (BVI), which comprises dynamic scenes captured under varying light conditions.","It is further analysed comparatively against various other models over three test datasets.","The model demonstrates superior adaptivity across all datasets, obtaining the highest PSNR and SSIM values.","It is particularly effective in extreme low-light conditions, yielding fairly good visualisation results."],"url":"http://arxiv.org/abs/2403.02408v1","category":"eess.IV"}
{"created":"2024-03-04 19:00:07","title":"Tunable quantum criticality and pseudocriticality across the fixed-point annihilation in the anisotropic spin-boson model","abstract":"Spin-boson models are simple examples of quantum dissipative systems, but also serve as effective models in quantum magnetism and exhibit nontrivial quantum criticality. Recently, they have been established as a platform to study the nontrivial renormalization-group (RG) scenario of fixed-point annihilation, in which two intermediate-coupling RG fixed points collide and generate an extremely slow RG flow near the collision. For the Bose Kondo model, a single $S=1/2$ spin where each spin component couples to an independent bosonic bath with power-law spectrum $\\propto \\omega^s$ via dissipation strengths $\\alpha_i$, $i\\in\\{x,y,z\\}$, such phenomena occur sequentially for the U(1)-symmetric model at $\\alpha_z=0$ and the SU(2)-symmetric case at $\\alpha_z = \\alpha_{xy}$, as the bath exponent $s<1$ is tuned. Here we use an exact wormhole quantum Monte Carlo method to show how fixed-point annihilations within symmetry-enhanced parameter manifolds affect the anisotropy-driven criticality across them. We find a tunable transition between two long-range-ordered localized phases that can be continuous or strongly first-order, and even becomes weakly first-order in an extended regime close to the fixed-point collision. We extract critical exponents at the continuous transition, but also find scaling behavior at the symmetry-enhanced first-order transition, for which the inverse correlation-length exponent is given by the bath exponent $s$. In particular, we provide direct numerical evidence for pseudocritical scaling on both sides of the fixed-point collision, which manifests in an extremely slow drift of the correlation-length exponent. In addition, we also study the crossover behavior away from the SU(2)-symmetric case and determine the phase boundary of an extended U(1)-symmetric critical phase for $\\alpha_z < \\alpha_{xy}$.","sentences":["Spin-boson models are simple examples of quantum dissipative systems, but also serve as effective models in quantum magnetism and exhibit nontrivial quantum criticality.","Recently, they have been established as a platform to study the nontrivial renormalization-group (RG) scenario of fixed-point annihilation, in which two intermediate-coupling RG fixed points collide and generate an extremely slow RG flow near the collision.","For the Bose Kondo model, a single $S=1/2$ spin where each spin component couples to an independent bosonic bath with power-law spectrum $\\propto \\omega^s$ via dissipation strengths $\\alpha_i$, $i\\in\\{x,y,z\\}$, such phenomena occur sequentially for the U(1)-symmetric model at $\\alpha_z=0$ and the SU(2)-symmetric case at $\\alpha_z = \\alpha_{xy}$, as the bath exponent $s<1$ is tuned.","Here we use an exact wormhole quantum Monte Carlo method to show how fixed-point annihilations within symmetry-enhanced parameter manifolds affect the anisotropy-driven criticality across them.","We find a tunable transition between two long-range-ordered localized phases that can be continuous or strongly first-order, and even becomes weakly first-order in an extended regime close to the fixed-point collision.","We extract critical exponents at the continuous transition, but also find scaling behavior at the symmetry-enhanced first-order transition, for which the inverse correlation-length exponent is given by the bath exponent $s$. In particular, we provide direct numerical evidence for pseudocritical scaling on both sides of the fixed-point collision, which manifests in an extremely slow drift of the correlation-length exponent.","In addition, we also study the crossover behavior away from the SU(2)-symmetric case and determine the phase boundary of an extended U(1)-symmetric critical phase for $\\alpha_z < \\alpha_{xy}$."],"url":"http://arxiv.org/abs/2403.02400v1","category":"cond-mat.str-el"}
{"created":"2024-03-04 19:00:04","title":"End-to-end variational quantum sensing","abstract":"Harnessing quantum correlations can enable sensing beyond the classical limits of precision, with the realization of such sensors poised for transformative impacts across science and engineering. Real devices, however, face the accumulated impacts of noise effects, architecture constraints, and finite sampling rates, making the design and success of practical quantum sensors challenging. Numerical and theoretical frameworks that support the optimization and analysis of imperfections from one end of a sensing protocol through to the other (i.e., from probe state preparation through to parameter estimation) are thus crucial for translating quantum advantage into widespread practice. Here, we present an end-to-end variational framework for quantum sensing protocols, where parameterized quantum circuits and neural networks form trainable, adaptive models for quantum sensor dynamics and estimation, respectively. The framework is general and can be adapted towards arbitrary qubit architectures, as we demonstrate with experimentally-relevant ans\\\"atze for trapped-ion and photonic systems, and enables to directly quantify the impacts that noisy state preparation/measurement and finite data sampling have on parameter estimation. End-to-end variational frameworks can thus underpin powerful design and analysis tools for realizing quantum advantage in practical, robust sensors.","sentences":["Harnessing quantum correlations can enable sensing beyond the classical limits of precision, with the realization of such sensors poised for transformative impacts across science and engineering.","Real devices, however, face the accumulated impacts of noise effects, architecture constraints, and finite sampling rates, making the design and success of practical quantum sensors challenging.","Numerical and theoretical frameworks that support the optimization and analysis of imperfections from one end of a sensing protocol through to the other (i.e., from probe state preparation through to parameter estimation) are thus crucial for translating quantum advantage into widespread practice.","Here, we present an end-to-end variational framework for quantum sensing protocols, where parameterized quantum circuits and neural networks form trainable, adaptive models for quantum sensor dynamics and estimation, respectively.","The framework is general and can be adapted towards arbitrary qubit architectures, as we demonstrate with experimentally-relevant ans\\\"atze for trapped-ion and photonic systems, and enables to directly quantify the impacts that noisy state preparation/measurement and finite data sampling have on parameter estimation.","End-to-end variational frameworks can thus underpin powerful design and analysis tools for realizing quantum advantage in practical, robust sensors."],"url":"http://arxiv.org/abs/2403.02394v1","category":"quant-ph"}
{"created":"2024-03-04 18:57:11","title":"COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks","abstract":"Multi-sensor fusion systems (MSFs) play a vital role as the perception module in modern autonomous vehicles (AVs). Therefore, ensuring their robustness against common and realistic adversarial semantic transformations, such as rotation and shifting in the physical world, is crucial for the safety of AVs. While empirical evidence suggests that MSFs exhibit improved robustness compared to single-modal models, they are still vulnerable to adversarial semantic transformations. Despite the proposal of empirical defenses, several works show that these defenses can be attacked again by new adaptive attacks. So far, there is no certified defense proposed for MSFs. In this work, we propose the first robustness certification framework COMMIT certify robustness of multi-sensor fusion systems against semantic attacks. In particular, we propose a practical anisotropic noise mechanism that leverages randomized smoothing with multi-modal data and performs a grid-based splitting method to characterize complex semantic transformations. We also propose efficient algorithms to compute the certification in terms of object detection accuracy and IoU for large-scale MSF models. Empirically, we evaluate the efficacy of COMMIT in different settings and provide a comprehensive benchmark of certified robustness for different MSF models using the CARLA simulation platform. We show that the certification for MSF models is at most 48.39% higher than that of single-modal models, which validates the advantages of MSF models. We believe our certification framework and benchmark will contribute an important step towards certifiably robust AVs in practice.","sentences":["Multi-sensor fusion systems (MSFs) play a vital role as the perception module in modern autonomous vehicles (AVs).","Therefore, ensuring their robustness against common and realistic adversarial semantic transformations, such as rotation and shifting in the physical world, is crucial for the safety of AVs.","While empirical evidence suggests that MSFs exhibit improved robustness compared to single-modal models, they are still vulnerable to adversarial semantic transformations.","Despite the proposal of empirical defenses, several works show that these defenses can be attacked again by new adaptive attacks.","So far, there is no certified defense proposed for MSFs.","In this work, we propose the first robustness certification framework COMMIT certify robustness of multi-sensor fusion systems against semantic attacks.","In particular, we propose a practical anisotropic noise mechanism that leverages randomized smoothing with multi-modal data and performs a grid-based splitting method to characterize complex semantic transformations.","We also propose efficient algorithms to compute the certification in terms of object detection accuracy and IoU for large-scale MSF models.","Empirically, we evaluate the efficacy of COMMIT in different settings and provide a comprehensive benchmark of certified robustness for different MSF models using the CARLA simulation platform.","We show that the certification for MSF models is at most 48.39% higher than that of single-modal models, which validates the advantages of MSF models.","We believe our certification framework and benchmark will contribute an important step towards certifiably robust AVs in practice."],"url":"http://arxiv.org/abs/2403.02329v1","category":"cs.LG"}
{"created":"2024-03-04 18:53:27","title":"Contract Design for Pandora's Box","abstract":"We study a natural application of contract design to search problems with probabilistic prior and exploration costs. These problems have a plethora of applications and are expressed concisely within the Pandora's Box model. Its optimal solution is the ingenious index policy proposed originally by Weitzman in 1979.   In our principal-agent setting, the search task is delegated to an agent. The agent performs a sequential exploration of $n$ boxes, suffers the exploration cost for each inspected box, and selects the content (called the prize) of one inspected box as outcome. Agent and principal obtain an individual value based on the selected prize. To influence the search, the principal a-priori designs a contract with a non-negative payment to the agent for each potential prize. The goal of the principal to maximize her expected reward, i.e., value minus payment. We show how to compute optimal contracts for the principal in several scenarios.   A popular and important subclass are linear contracts, and we show how to compute optimal linear contracts in polynomial time. For general contracts, we consider the standard assumption that the agent suffers cost but obtains value only from the transfers by the principal. Interestingly, a suitable adaptation of the index policy results in an optimal contract here. More generally, for general contracts with non-zero agent values for outcomes we show how to compute an optimal contract in two cases: (1) when each box has only one prize with non-zero value for principal and agent, (2) for i.i.d. boxes with a single prize with positive value for the principal. These results show that optimal contracts can be highly non-trivial, and their design goes significantly beyond the application or re-interpretation of the index policy.","sentences":["We study a natural application of contract design to search problems with probabilistic prior and exploration costs.","These problems have a plethora of applications and are expressed concisely within the Pandora's Box model.","Its optimal solution is the ingenious index policy proposed originally by Weitzman in 1979.   ","In our principal-agent setting, the search task is delegated to an agent.","The agent performs a sequential exploration of $n$ boxes, suffers the exploration cost for each inspected box, and selects the content (called the prize) of one inspected box as outcome.","Agent and principal obtain an individual value based on the selected prize.","To influence the search, the principal a-priori designs a contract with a non-negative payment to the agent for each potential prize.","The goal of the principal to maximize her expected reward, i.e., value minus payment.","We show how to compute optimal contracts for the principal in several scenarios.   ","A popular and important subclass are linear contracts, and we show how to compute optimal linear contracts in polynomial time.","For general contracts, we consider the standard assumption that the agent suffers cost but obtains value only from the transfers by the principal.","Interestingly, a suitable adaptation of the index policy results in an optimal contract here.","More generally, for general contracts with non-zero agent values for outcomes we show how to compute an optimal contract in two cases: (1) when each box has only one prize with non-zero value for principal and agent, (2) for i.i.d. boxes with a single prize with positive value for the principal.","These results show that optimal contracts can be highly non-trivial, and their design goes significantly beyond the application or re-interpretation of the index policy."],"url":"http://arxiv.org/abs/2403.02317v1","category":"cs.GT"}
{"created":"2024-03-04 18:46:20","title":"Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures","abstract":"Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks. Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations. Our evaluations in image classification demonstrate that VRWKV matches ViT's classification performance with significantly faster speeds and lower memory usage. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code is released at \\url{https://github.com/OpenGVLab/Vision-RWKV}.","sentences":["Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis.","This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks.","Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets.","Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations.","Our evaluations in image classification demonstrate that VRWKV matches ViT's classification performance with significantly faster speeds and lower memory usage.","In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds.","These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks.","Code is released at \\url{https://github.com/OpenGVLab/Vision-RWKV}."],"url":"http://arxiv.org/abs/2403.02308v1","category":"cs.CV"}
{"created":"2024-03-04 18:44:30","title":"Harnessing Intra-group Variations Via a Population-Level Context for Pathology Detection","abstract":"Realizing sufficient separability between the distributions of healthy and pathological samples is a critical obstacle for pathology detection convolutional models. Moreover, these models exhibit a bias for contrast-based images, with diminished performance on texture-based medical images. This study introduces the notion of a population-level context for pathology detection and employs a graph theoretic approach to model and incorporate it into the latent code of an autoencoder via a refinement module we term PopuSense. PopuSense seeks to capture additional intra-group variations inherent in biomedical data that a local or global context of the convolutional model might miss or smooth out. Experiments on contrast-based and texture-based images, with minimal adaptation, encounter the existing preference for intensity-based input. Nevertheless, PopuSense demonstrates improved separability in contrast-based images, presenting an additional avenue for refining representations learned by a model.","sentences":["Realizing sufficient separability between the distributions of healthy and pathological samples is a critical obstacle for pathology detection convolutional models.","Moreover, these models exhibit a bias for contrast-based images, with diminished performance on texture-based medical images.","This study introduces the notion of a population-level context for pathology detection and employs a graph theoretic approach to model and incorporate it into the latent code of an autoencoder via a refinement module we term PopuSense.","PopuSense seeks to capture additional intra-group variations inherent in biomedical data that a local or global context of the convolutional model might miss or smooth out.","Experiments on contrast-based and texture-based images, with minimal adaptation, encounter the existing preference for intensity-based input.","Nevertheless, PopuSense demonstrates improved separability in contrast-based images, presenting an additional avenue for refining representations learned by a model."],"url":"http://arxiv.org/abs/2403.02307v1","category":"eess.IV"}
{"created":"2024-03-04 18:18:36","title":"PixIT: Joint Training of Speaker Diarization and Speech Separation from Real-world Multi-speaker Recordings","abstract":"A major drawback of supervised speech separation (SSep) systems is their reliance on synthetic data, leading to poor real-world generalization. Mixture invariant training (MixIT) was proposed as an unsupervised alternative that uses real recordings, yet struggles with overseparation and adapting to long-form audio. We introduce PixIT, a joint approach that combines permutation invariant training (PIT) for speaker diarization (SD) and MixIT for SSep. With a small extra requirement of needing SD labels, it solves the problem of overseparation and allows stitching local separated sources leveraging existing work on clustering-based neural SD. We measure the quality of the separated sources via applying automatic speech recognition (ASR) systems to them. PixIT boosts the performance of various ASR systems across two meeting corpora both in terms of the speaker-attributed and utterance-based word error rates while not requiring any fine-tuning.","sentences":["A major drawback of supervised speech separation (SSep) systems is their reliance on synthetic data, leading to poor real-world generalization.","Mixture invariant training (MixIT) was proposed as an unsupervised alternative that uses real recordings, yet struggles with overseparation and adapting to long-form audio.","We introduce PixIT, a joint approach that combines permutation invariant training (PIT) for speaker diarization (SD) and MixIT for SSep.","With a small extra requirement of needing SD labels, it solves the problem of overseparation and allows stitching local separated sources leveraging existing work on clustering-based neural SD.","We measure the quality of the separated sources via applying automatic speech recognition (ASR) systems to them.","PixIT boosts the performance of various ASR systems across two meeting corpora both in terms of the speaker-attributed and utterance-based word error rates while not requiring any fine-tuning."],"url":"http://arxiv.org/abs/2403.02288v1","category":"eess.AS"}
{"created":"2024-03-04 18:15:14","title":"Detection of Non-recorded Word Senses in English and Swedish","abstract":"This study addresses the task of Unknown Sense Detection in English and Swedish. The primary objective of this task is to determine whether the meaning of a particular word usage is documented in a dictionary or not. For this purpose, sense entries are compared with word usages from modern and historical corpora using a pre-trained Word-in-Context embedder that allows us to model this task in a few-shot scenario. Additionally, we use human annotations to adapt and evaluate our models. Compared to a random sample from a corpus, our model is able to considerably increase the detected number of word usages with non-recorded senses.","sentences":["This study addresses the task of Unknown Sense Detection in English and Swedish.","The primary objective of this task is to determine whether the meaning of a particular word usage is documented in a dictionary or not.","For this purpose, sense entries are compared with word usages from modern and historical corpora using a pre-trained Word-in-Context embedder that allows us to model this task in a few-shot scenario.","Additionally, we use human annotations to adapt and evaluate our models.","Compared to a random sample from a corpus, our model is able to considerably increase the detected number of word usages with non-recorded senses."],"url":"http://arxiv.org/abs/2403.02285v1","category":"cs.CL"}
{"created":"2024-03-04 18:06:07","title":"Zonostrophic turbulence in the subsurface oceans of the Jovian and Saturnian moons","abstract":"In order to characterize the global circulation of the subsurface ocean of Jovian and Saturnian moons, we analyze the properties of 21 three-dimensional simulations of Boussinesq thermal convection in a rapidly rotating spherical shell. Flow is driven by an adverse temperature contrast imposed across the domain, and is subjected to no-slip boundary conditions. We cover a region of parameter space previously unexplored by global simulations, both in terms of rapid rotation and vigor of convective forcing, closer to, yet still admittedly far from, the conditions appropriate for the subsurface ocean of Ganymede, Europa, Enceladus, and Titan. Our most extreme simulations exhibit a dynamic global circulation that combines powerful east-west zonal jets, planetary waves, and vortices. A spectral analysis of the kinetic energy distribution performed in cylindrical geometry reveals a high degree of anisotropy of the simulated flows. Specifically, the axisymmetric zonal energy spectra follow a steep $-5$ slope in wavenumber space, with the energy amplitude exclusively controlled by the rotation rate. In contrast, the non-axisymmetric residual spectra display a gentle $-5/3$ slope, with the energy amplitude controlled by the thermal buoyancy input power. This spectral behavior conforms with the theory of zonostrophic turbulence and allows us to propose tentative extrapolations of these findings to the more extreme conditions of icy satellites. By assuming that kinetic energy dissipates via Ekman friction we predict an upper bound for the zonal velocity ranging from a few centimeters per second for Enceladus to about one meter per second for Ganymede, with residual velocities smaller than the zonal velocity by an order of magnitude on each moon. These predictions yield typical jets size approaching the ocean depth of Titan, Ganymede and Europa and $10$ to $40\\%$ of the ocean depth on Enceladus.","sentences":["In order to characterize the global circulation of the subsurface ocean of Jovian and Saturnian moons, we analyze the properties of 21 three-dimensional simulations of Boussinesq thermal convection in a rapidly rotating spherical shell.","Flow is driven by an adverse temperature contrast imposed across the domain, and is subjected to no-slip boundary conditions.","We cover a region of parameter space previously unexplored by global simulations, both in terms of rapid rotation and vigor of convective forcing, closer to, yet still admittedly far from, the conditions appropriate for the subsurface ocean of Ganymede, Europa, Enceladus, and Titan.","Our most extreme simulations exhibit a dynamic global circulation that combines powerful east-west zonal jets, planetary waves, and vortices.","A spectral analysis of the kinetic energy distribution performed in cylindrical geometry reveals a high degree of anisotropy of the simulated flows.","Specifically, the axisymmetric zonal energy spectra follow a steep $-5$ slope in wavenumber space, with the energy amplitude exclusively controlled by the rotation rate.","In contrast, the non-axisymmetric residual spectra display a gentle $-5/3$ slope, with the energy amplitude controlled by the thermal buoyancy input power.","This spectral behavior conforms with the theory of zonostrophic turbulence and allows us to propose tentative extrapolations of these findings to the more extreme conditions of icy satellites.","By assuming that kinetic energy dissipates via Ekman friction we predict an upper bound for the zonal velocity ranging from a few centimeters per second for Enceladus to about one meter per second for Ganymede, with residual velocities smaller than the zonal velocity by an order of magnitude on each moon.","These predictions yield typical jets size approaching the ocean depth of Titan, Ganymede and Europa and $10$ to $40\\%$ of the ocean depth on Enceladus."],"url":"http://arxiv.org/abs/2403.02277v1","category":"physics.flu-dyn"}
{"created":"2024-03-04 18:04:52","title":"Bounded Depth Frege Lower Bounds for Random 3-CNFs via Deterministic Restrictions","abstract":"A major open problem in proof complexity is to show that random 3-CNFs with linear number of clauses require super-polynomial size refutations in bounded depth Frege. We make a first step towards this question by showing a super-linear lower bound: for every $k$, there exists $\\epsilon > 0$ such that any depth-$k$ Frege refutation of a random $n$-variable 3-CNF with $\\Theta(n)$ clauses has $\\Omega(n^{1 + \\epsilon})$ steps w.h.p. Our proof involves a novel adaptation of the deterministic restriction technique of Chaudhuri and Radhakrishnan (STOC'96).","sentences":["A major open problem in proof complexity is to show that random 3-CNFs with linear number of clauses require super-polynomial size refutations in bounded depth Frege.","We make a first step towards this question by showing a super-linear lower bound: for every $k$, there exists $\\epsilon > 0$ such that any depth-$k$ Frege refutation of a random $n$-variable 3-CNF with $\\Theta(n)$ clauses has $\\Omega(n^{1 + \\epsilon})$ steps w.h.p.","Our proof involves a novel adaptation of the deterministic restriction technique of Chaudhuri and Radhakrishnan (STOC'96)."],"url":"http://arxiv.org/abs/2403.02275v1","category":"cs.CC"}
{"created":"2024-03-04 17:34:57","title":"Electrical Control of Exciton-Polariton Condensate Josephson Junctions","abstract":"We propose the electrical control of a device probing the Josephson effect in exciton-polariton (EP) condensates, which can be switched between various dynamical modes. We model the device by a four-component Gross-Pitaevskii equation assuming that ideal EP condensates are established with well-balanced pumping and dissipation. All the model parameters are calculated microscopically. In particular, we obtain the polariton tunneling strength across the junction as a second-order process of electron-hole pair tunneling. We find that the EP condensates can be manipulated through distinctive degrees of freedom not present in other coherent quantum systems, and the dynamics of EP Josephson junctions are far richer than that of the conventional superconducting junctions.","sentences":["We propose the electrical control of a device probing the Josephson effect in exciton-polariton (EP) condensates, which can be switched between various dynamical modes.","We model the device by a four-component Gross-Pitaevskii equation assuming that ideal EP condensates are established with well-balanced pumping and dissipation.","All the model parameters are calculated microscopically.","In particular, we obtain the polariton tunneling strength across the junction as a second-order process of electron-hole pair tunneling.","We find that the EP condensates can be manipulated through distinctive degrees of freedom not present in other coherent quantum systems, and the dynamics of EP Josephson junctions are far richer than that of the conventional superconducting junctions."],"url":"http://arxiv.org/abs/2403.02248v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-04 17:34:46","title":"Birbal: An efficient 7B instruct-model fine-tuned with curated datasets","abstract":"LLMOps incur significant costs due to hardware requirements, hindering their widespread accessibility. Additionally, a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible. To tackle these challenges, the LLM Efficiency Challenge was introduced at NeurIPS Workshop, aiming to adapt foundation models on a diverse set of tasks via fine-tuning on a single GPU (RTX 4090 or A100 with 40GB) within a 24-hour timeframe. In this system description paper, we introduce Birbal, our Mistral-7B based winning model, fine-tuned on a single RTX 4090 for 16 hours. Birbal's success lies in curating high-quality instructions covering diverse tasks, resulting in a 35% performance improvement over second-best Qwen-14B based submission.","sentences":["LLMOps incur significant costs due to hardware requirements, hindering their widespread accessibility.","Additionally, a lack of transparency in model training methods and data contributes to the majority of models being non-reproducible.","To tackle these challenges, the LLM Efficiency Challenge was introduced at NeurIPS Workshop, aiming to adapt foundation models on a diverse set of tasks via fine-tuning on a single GPU (RTX 4090 or A100 with 40GB) within a 24-hour timeframe.","In this system description paper, we introduce Birbal, our Mistral-7B based winning model, fine-tuned on a single RTX 4090 for 16 hours.","Birbal's success lies in curating high-quality instructions covering diverse tasks, resulting in a 35% performance improvement over second-best Qwen-14B based submission."],"url":"http://arxiv.org/abs/2403.02247v1","category":"cs.CL"}
{"created":"2024-03-04 17:22:43","title":"Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection","abstract":"This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset. The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively. Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance. Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection. The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation to address the evolving nature of malware. This research contributes to ongoing discussions in cybersecurity and provides practical insights for developing more robust malware detection systems in the digital era.","sentences":["This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset.","The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively.","Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored.","Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance.","Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection.","The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation to address the evolving nature of malware.","This research contributes to ongoing discussions in cybersecurity and provides practical insights for developing more robust malware detection systems in the digital era."],"url":"http://arxiv.org/abs/2403.02232v1","category":"cs.CR"}
{"created":"2024-03-04 17:08:57","title":"TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models","abstract":"Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management. The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data. However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention. Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem. It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learning. Recognizing the sequential nature of traffic data, similar to language, we introduce TPLLM, a novel traffic prediction framework leveraging LLMs. In this framework, we construct a sequence embedding layer based on Convolutional Neural Networks (CNNs) and a graph embedding layer based on Graph Convolutional Networks (GCNs) to extract sequence features and spatial features, respectively. These are subsequently integrated to form inputs that are suitable for LLMs. A Low-Rank Adaptation (LoRA) fine-tuning approach is applied to TPLLM, thereby facilitating efficient learning and minimizing computational demands. Experiments on two real-world datasets demonstrate that TPLLM exhibits commendable performance in both full-sample and few-shot prediction scenarios, effectively supporting the development of ITS in regions with scarce historical traffic data.","sentences":["Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management.","The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data.","However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention.","Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem.","It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learning.","Recognizing the sequential nature of traffic data, similar to language, we introduce TPLLM, a novel traffic prediction framework leveraging LLMs.","In this framework, we construct a sequence embedding layer based on Convolutional Neural Networks (CNNs) and a graph embedding layer based on Graph Convolutional Networks (GCNs) to extract sequence features and spatial features, respectively.","These are subsequently integrated to form inputs that are suitable for LLMs.","A Low-Rank Adaptation (LoRA) fine-tuning approach is applied to TPLLM, thereby facilitating efficient learning and minimizing computational demands.","Experiments on two real-world datasets demonstrate that TPLLM exhibits commendable performance in both full-sample and few-shot prediction scenarios, effectively supporting the development of ITS in regions with scarce historical traffic data."],"url":"http://arxiv.org/abs/2403.02221v1","category":"cs.LG"}
{"created":"2024-03-04 17:01:28","title":"Global weak solutions of the Serre-Green-Naghdi equations with surface tension","abstract":"We consider in this paper the Serre--Green--Naghdi equations with surface tension. Smooth solutions of this system conserve an $H^1$-equivalent energy. We prove the existence of global weak dissipative solutions for any relatively small-energy initial data. We also prove that the Riemann invariants of the solutions satisfy a one-sided Oleinik inequality.","sentences":["We consider in this paper the Serre--Green--Naghdi equations with surface tension.","Smooth solutions of this system conserve an $H^1$-equivalent energy.","We prove the existence of global weak dissipative solutions for any relatively small-energy initial data.","We also prove that the Riemann invariants of the solutions satisfy a one-sided Oleinik inequality."],"url":"http://arxiv.org/abs/2403.02214v1","category":"math.AP"}
{"created":"2024-03-04 16:49:36","title":"Parametric multi-element coupling architecture for coherent and dissipative control of superconducting qubits","abstract":"As systems for quantum computing keep growing in size and number of qubits, challenges in scaling the control capabilities are becoming increasingly relevant. Efficient schemes to simultaneously mediate coherent interactions between multiple quantum systems and to reduce decoherence errors can minimize the control overhead in next-generation quantum processors. Here, we present a superconducting qubit architecture based on tunable parametric interactions to perform two-qubit gates, reset, leakage recovery and to read out the qubits. In this architecture, parametrically driven multi-element couplers selectively couple qubits to resonators and neighbouring qubits, according to the frequency of the drive. We consider a system with two qubits and one readout resonator interacting via a single coupling circuit and experimentally demonstrate a controlled-Z gate with a fidelity of $98.30\\pm 0.23 \\%$, a reset operation that unconditionally prepares the qubit ground state with a fidelity of $99.80\\pm 0.02 \\%$ and a leakage recovery operation with a $98.5\\pm 0.3 \\%$ success probability. Furthermore, we implement a parametric readout with a single-shot assignment fidelity of $88.0\\pm 0.4 \\%$. These operations are all realized using a single tunable coupler, demonstrating the experimental feasibility of the proposed architecture and its potential for reducing the system complexity in scalable quantum processors.","sentences":["As systems for quantum computing keep growing in size and number of qubits, challenges in scaling the control capabilities are becoming increasingly relevant.","Efficient schemes to simultaneously mediate coherent interactions between multiple quantum systems and to reduce decoherence errors can minimize the control overhead in next-generation quantum processors.","Here, we present a superconducting qubit architecture based on tunable parametric interactions to perform two-qubit gates, reset, leakage recovery and to read out the qubits.","In this architecture, parametrically driven multi-element couplers selectively couple qubits to resonators and neighbouring qubits, according to the frequency of the drive.","We consider a system with two qubits and one readout resonator interacting via a single coupling circuit and experimentally demonstrate a controlled-Z gate with a fidelity of $98.30\\pm 0.23 \\%$, a reset operation that unconditionally prepares the qubit ground state with a fidelity of $99.80\\pm 0.02 \\%$ and a leakage recovery operation with a $98.5\\pm 0.3 \\%$ success probability.","Furthermore, we implement a parametric readout with a single-shot assignment fidelity of $88.0\\pm 0.4 \\%$. These operations are all realized using a single tunable coupler, demonstrating the experimental feasibility of the proposed architecture and its potential for reducing the system complexity in scalable quantum processors."],"url":"http://arxiv.org/abs/2403.02203v1","category":"quant-ph"}
{"created":"2024-03-04 16:34:10","title":"Boosting Distributional Copula Regression for Bivariate Binary, Discrete and Mixed Responses","abstract":"Motivated by challenges in the analysis of biomedical data and observational studies, we develop statistical boosting for the general class of bivariate distributional copula regression with arbitrary marginal distributions, which is suited to model binary, count, continuous or mixed outcomes. In our framework, the joint distribution of arbitrary, bivariate responses is modelled through a parametric copula. To arrive at a model for the entire conditional distribution, not only the marginal distribution parameters but also the copula parameters are related to covariates through additive predictors. We suggest efficient and scalable estimation by means of an adapted component-wise gradient boosting algorithm with statistical models as base-learners. A key benefit of boosting as opposed to classical likelihood or Bayesian estimation is the implicit data-driven variable selection mechanism as well as shrinkage without additional input or assumptions from the analyst. To the best of our knowledge, our implementation is the only one that combines a wide range of covariate effects, marginal distributions, copula functions, and implicit data-driven variable selection. We showcase the versatility of our approach on data from genetic epidemiology, healthcare utilization and childhood undernutrition. Our developments are implemented in the R package gamboostLSS, fostering transparent and reproducible research.","sentences":["Motivated by challenges in the analysis of biomedical data and observational studies, we develop statistical boosting for the general class of bivariate distributional copula regression with arbitrary marginal distributions, which is suited to model binary, count, continuous or mixed outcomes.","In our framework, the joint distribution of arbitrary, bivariate responses is modelled through a parametric copula.","To arrive at a model for the entire conditional distribution, not only the marginal distribution parameters but also the copula parameters are related to covariates through additive predictors.","We suggest efficient and scalable estimation by means of an adapted component-wise gradient boosting algorithm with statistical models as base-learners.","A key benefit of boosting as opposed to classical likelihood or Bayesian estimation is the implicit data-driven variable selection mechanism as well as shrinkage without additional input or assumptions from the analyst.","To the best of our knowledge, our implementation is the only one that combines a wide range of covariate effects, marginal distributions, copula functions, and implicit data-driven variable selection.","We showcase the versatility of our approach on data from genetic epidemiology, healthcare utilization and childhood undernutrition.","Our developments are implemented in the R package gamboostLSS, fostering transparent and reproducible research."],"url":"http://arxiv.org/abs/2403.02194v1","category":"stat.ME"}
{"created":"2024-03-04 16:31:58","title":"Domain adaptation, Explainability & Fairness in AI for Medical Image Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans","abstract":"The paper presents the DEF-AI-MIA COV19D Competition, which is organized in the framework of the 'Domain adaptation, Explainability, Fairness in AI for Medical Image Analysis (DEF-AI-MIA)' Workshop of the 2024 Computer Vision and Pattern Recognition (CVPR) Conference. The Competition is the 4th in the series, following the first three Competitions held in the framework of ICCV 2021, ECCV 2022 and ICASSP 2023 International Conferences respectively. It includes two Challenges on: i) Covid-19 Detection and ii) Covid-19 Domain Adaptation. The Competition use data from COV19-CT-DB database, which is described in the paper and includes a large number of chest CT scan series. Each chest CT scan series consists of a sequence of 2-D CT slices, the number of which is between 50 and 700. Training, validation and test datasets have been extracted from COV19-CT-DB and provided to the participants in both Challenges. The paper presents the baseline models used in the Challenges and the performance which was obtained respectively.","sentences":["The paper presents the DEF-AI-MIA COV19D Competition, which is organized in the framework of the 'Domain adaptation, Explainability, Fairness in AI for Medical Image Analysis (DEF-AI-MIA)'","Workshop of the 2024 Computer Vision and Pattern Recognition (CVPR) Conference.","The Competition is the 4th in the series, following the first three Competitions held in the framework of ICCV 2021, ECCV 2022 and ICASSP 2023 International Conferences respectively.","It includes two Challenges on: i) Covid-19 Detection and ii) Covid-19 Domain Adaptation.","The Competition use data from COV19-CT-DB database, which is described in the paper and includes a large number of chest CT scan series.","Each chest CT scan series consists of a sequence of 2-D CT slices, the number of which is between 50 and 700.","Training, validation and test datasets have been extracted from COV19-CT-DB and provided to the participants in both Challenges.","The paper presents the baseline models used in the Challenges and the performance which was obtained respectively."],"url":"http://arxiv.org/abs/2403.02192v1","category":"eess.IV"}
{"created":"2024-03-04 16:23:58","title":"Not all Layers of LLMs are Necessary during Inference","abstract":"The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, \"During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?\" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment tasks, while maintaining comparable performance. Additionally, this method is orthogonal to other model acceleration techniques, potentially boosting inference efficiency further.","sentences":["The inference phase of Large Language Models (LLMs) is very expensive.","An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability).","In this paper, we try to answer the question, \"During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?\"","To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks.","Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively.","More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks.","Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment tasks, while maintaining comparable performance.","Additionally, this method is orthogonal to other model acceleration techniques, potentially boosting inference efficiency further."],"url":"http://arxiv.org/abs/2403.02181v1","category":"cs.CL"}
{"created":"2024-03-05 18:58:22","title":"Out of Time Order Correlation of the Hubbard Model with Random Local Disorder","abstract":"The out of time order correlator (OTOC) serves as a powerful tool for investigating quantum information spreading and chaos in complex systems. We present a method employing non-equilibrium dynamical mean-field theory (DMFT) and coherent potential approximation (CPA) combined with diagrammatic perturbation on the Schwinger-Keldysh contour to calculate the OTOC for correlated fermionic systems subjected to both random disorder and electrons interaction. Our key finding is that random disorder enhances the OTOC decay in the Hubbard model for the metallic phase in the weak coupling limit. However, the current limitation of our perturbative solver restricts the applicability to weak interaction regimes.","sentences":["The out of time order correlator (OTOC) serves as a powerful tool for investigating quantum information spreading and chaos in complex systems.","We present a method employing non-equilibrium dynamical mean-field theory (DMFT) and coherent potential approximation (CPA) combined with diagrammatic perturbation on the Schwinger-Keldysh contour to calculate the OTOC for correlated fermionic systems subjected to both random disorder and electrons interaction.","Our key finding is that random disorder enhances the OTOC decay in the Hubbard model for the metallic phase in the weak coupling limit.","However, the current limitation of our perturbative solver restricts the applicability to weak interaction regimes."],"url":"http://arxiv.org/abs/2403.03214v1","category":"cond-mat.str-el"}
{"created":"2024-03-05 18:37:37","title":"Operator Learning Renormalization Group","abstract":"In this paper, we present a general framework for quantum many-body simulations called the operator learning renormalization group (OLRG). Inspired by machine learning perspectives, OLRG is a generalization of Wilson's numerical renormalization group and White's density matrix renormalization group, which recursively builds a simulatable system to approximate a target system of the same number of sites via operator maps. OLRG uses a loss function to minimize the error of a target property directly by learning the operator map in lieu of a state ansatz. This loss function is designed by a scaling consistency condition that also provides a provable bound for real-time evolution. We implement two versions of the operator maps for classical and quantum simulations. The former, which we call the Operator Matrix Map, can be implemented via neural networks on classical computers. The latter, which we call the Hamiltonian Expression Map, generates device pulse sequences to leverage the capabilities of quantum computing hardware. We illustrate the performance of both maps for calculating time-dependent quantities in the quantum Ising model Hamiltonian.","sentences":["In this paper, we present a general framework for quantum many-body simulations called the operator learning renormalization group (OLRG).","Inspired by machine learning perspectives, OLRG is a generalization of Wilson's numerical renormalization group and White's density matrix renormalization group, which recursively builds a simulatable system to approximate a target system of the same number of sites via operator maps.","OLRG uses a loss function to minimize the error of a target property directly by learning the operator map in lieu of a state ansatz.","This loss function is designed by a scaling consistency condition that also provides a provable bound for real-time evolution.","We implement two versions of the operator maps for classical and quantum simulations.","The former, which we call the Operator Matrix Map, can be implemented via neural networks on classical computers.","The latter, which we call the Hamiltonian Expression Map, generates device pulse sequences to leverage the capabilities of quantum computing hardware.","We illustrate the performance of both maps for calculating time-dependent quantities in the quantum Ising model Hamiltonian."],"url":"http://arxiv.org/abs/2403.03199v1","category":"quant-ph"}
{"created":"2024-03-05 18:29:26","title":"Generic models for genus 2 curves with real multiplication","abstract":"Explicit models of families of genus 2 curves with multiplication by $\\sqrt D$ are known for $D= 2, 3, 5$. We obtain generic models for genus 2 curves over $\\mathbb Q$ with real multiplication in 12 new cases, including all fundamental discriminants $D < 40$. A key step in our proof is to develop an algorithm for minimisation of conic bundles fibred over $\\mathbb{P}^2$. We apply this algorithm to simplify the equations for the Mestre conic associated to the generic point on the Hilbert modular surface of fundamental discriminant $D < 100$ computed by Elkies--Kumar.","sentences":["Explicit models of families of genus 2 curves with multiplication by $\\sqrt D$ are known for $D= 2, 3, 5$. We obtain generic models for genus 2 curves over $\\mathbb Q$ with real multiplication in 12 new cases, including all fundamental discriminants $D < 40$.","A key step in our proof is to develop an algorithm for minimisation of conic bundles fibred over $\\mathbb{P}^2$. We apply this algorithm to simplify the equations for the Mestre conic associated to the generic point on the Hilbert modular surface of fundamental discriminant $D < 100$ computed by Elkies--Kumar."],"url":"http://arxiv.org/abs/2403.03191v1","category":"math.NT"}
{"created":"2024-03-05 18:15:57","title":"Reduction of Cosymplectic Groupoids","abstract":"There is a well-known fact in Poisson geometry that reduction commutes with integration (of the associated integrable Lie algebroid). This is also valid for other types of geometries given by 2-dimensional closed forms. In this manuscript we extend this type of result to the particular case of cosymplectic groupoids, obtaining in particular the reduction of a central extension defined by a 2-IM form and a 1-IM form.","sentences":["There is a well-known fact in Poisson geometry that reduction commutes with integration (of the associated integrable Lie algebroid).","This is also valid for other types of geometries given by 2-dimensional closed forms.","In this manuscript we extend this type of result to the particular case of cosymplectic groupoids, obtaining in particular the reduction of a central extension defined by a 2-IM form and a 1-IM form."],"url":"http://arxiv.org/abs/2403.03178v1","category":"math.SG"}
{"created":"2024-03-05 17:56:10","title":"Statistical modeling of equilibrium phase transition in confined fluids","abstract":"The phase transition of confined fluids in mesoporous materials deviates from that of bulk fluids due to the former's interactions with the surrounding heterogeneous structure. For example, metal-organic frameworks (MOFs) create a strong heterogeneous field, so adsorbed fluids in MOFs have atypical phase characteristics such as capillary condensation and higher-order phase transitions. These characteristics are modeled by decoupling the host-guest and guest-guest interactions as a many-body problem in the presence of an external nonuniform field. To solve the three-dimensional Ising model, we use mean-field theory to approximate the guest-guest interactions and Mayer's (f)-functions to describe the host-guest interactions in a unit cell. Later, using Hill's theory of nanothermodynamics, we define differential and integral thermodynamic functions to describe confined fluids. These integral properties are then used to understand the phase transition in confined fluids. The investigation reveals a distinct behavior where fluids confined in larger pores undergo a discontinuous (first-order) phase transition, whereas those confined in smaller pores undergo a continuous (higher-order) phase transition. Furthermore, the results indicate that the free-energy barrier for phase transitions is lower in confined fluids than in bulk fluids, which helps explain the lower condensation pressure relative to the bulk saturation pressure. Finally, the integral thermodynamic functions are succinctly presented in the form of a phase diagram, marking an initial step toward a more practical approach for understanding the phase behavior of confined fluids.","sentences":["The phase transition of confined fluids in mesoporous materials deviates from that of bulk fluids due to the former's interactions with the surrounding heterogeneous structure.","For example, metal-organic frameworks (MOFs) create a strong heterogeneous field, so adsorbed fluids in MOFs have atypical phase characteristics such as capillary condensation and higher-order phase transitions.","These characteristics are modeled by decoupling the host-guest and guest-guest interactions as a many-body problem in the presence of an external nonuniform field.","To solve the three-dimensional Ising model, we use mean-field theory to approximate the guest-guest interactions and Mayer's (f)-functions to describe the host-guest interactions in a unit cell.","Later, using Hill's theory of nanothermodynamics, we define differential and integral thermodynamic functions to describe confined fluids.","These integral properties are then used to understand the phase transition in confined fluids.","The investigation reveals a distinct behavior where fluids confined in larger pores undergo a discontinuous (first-order) phase transition, whereas those confined in smaller pores undergo a continuous (higher-order) phase transition.","Furthermore, the results indicate that the free-energy barrier for phase transitions is lower in confined fluids than in bulk fluids, which helps explain the lower condensation pressure relative to the bulk saturation pressure.","Finally, the integral thermodynamic functions are succinctly presented in the form of a phase diagram, marking an initial step toward a more practical approach for understanding the phase behavior of confined fluids."],"url":"http://arxiv.org/abs/2403.03162v1","category":"cond-mat.soft"}
{"created":"2024-03-05 17:52:20","title":"Novel approach of exploring ASEP-like models through the Yang Baxter Equation","abstract":"We explore the algebraic structure of a particular ansatz of Yang Baxter Equation which is inspired from the Bethe Ansatz treatment of the ASEP spin-model. Various classes of Hamiltonian density arriving from two types of R-Matrices are found which also appear as solutions of constant YBE. We identify the idempotent and nilpotent categories of such constant R-Matrices and perform a rank-1 numerical search for the lowest dimension. A summary of finalised results reveals general non-hermitian spin-1/2 chain models.","sentences":["We explore the algebraic structure of a particular ansatz of Yang Baxter Equation which is inspired from the Bethe Ansatz treatment of the ASEP spin-model.","Various classes of Hamiltonian density arriving from two types of R-Matrices are found which also appear as solutions of constant YBE.","We identify the idempotent and nilpotent categories of such constant R-Matrices and perform a rank-1 numerical search for the lowest dimension.","A summary of finalised results reveals general non-hermitian spin-1/2 chain models."],"url":"http://arxiv.org/abs/2403.03159v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-05 17:50:24","title":"The Amplitude Equation for the Space-Fractional Swift-Hohenberg Equation","abstract":"Non-local reaction-diffusion partial differential equations (PDEs) involving the fractional Laplacian have arisen in a wide variety of applications. One common tool to analyse the dynamics of classical local PDEs near instability is to derive local amplitude/modulation approximations, which provide local normal forms classifying a wide variety of pattern-formation phenomena. In this work, we study amplitude equations for the space-fractional Swift-Hohenberg equation. The Swift-Hohenberg equation is a basic model problem motivated by pattern formation in fluid dynamics and has served as one of the main PDEs to develop general techniques to derive amplitude equations. We prove that there exists near the first bifurcation point an approximation by a (real) Ginzburg-Landau equation. Interestingly, this Ginzburg-Landau equation is a local PDE, which provides a rigorous justification of the physical conjecture that suitably localized unstable modes can out-compete superdiffusion and re-localize a PDE near instability. Our main technical contributions are to provide a suitable function space setting for the approximation problem, and to then bound the residual between the original PDE and its amplitude equation.","sentences":["Non-local reaction-diffusion partial differential equations (PDEs) involving the fractional Laplacian have arisen in a wide variety of applications.","One common tool to analyse the dynamics of classical local PDEs near instability is to derive local amplitude/modulation approximations, which provide local normal forms classifying a wide variety of pattern-formation phenomena.","In this work, we study amplitude equations for the space-fractional Swift-Hohenberg equation.","The Swift-Hohenberg equation is a basic model problem motivated by pattern formation in fluid dynamics and has served as one of the main PDEs to develop general techniques to derive amplitude equations.","We prove that there exists near the first bifurcation point an approximation by a (real) Ginzburg-Landau equation.","Interestingly, this Ginzburg-Landau equation is a local PDE, which provides a rigorous justification of the physical conjecture that suitably localized unstable modes can out-compete superdiffusion and re-localize a PDE near instability.","Our main technical contributions are to provide a suitable function space setting for the approximation problem, and to then bound the residual between the original PDE and its amplitude equation."],"url":"http://arxiv.org/abs/2403.03158v1","category":"math.AP"}
{"created":"2024-03-05 17:48:43","title":"Four-band effective square lattice model for Bernal-stacked bilayer graphene","abstract":"Bernal-stacked bilayer graphene (BLG) provides an ideal basis for gate-controlled, and free of etching, electronic devices. Theoretical modeling of realistic devices is an essential part of research, however, simulations of large-scale BLG devices continue to be extremely challenging. Micrometer-sized systems are predominantly beyond the reach of the commonly used atomistic tight-binding method, while other numerical approaches based on the two dimensional Dirac equation are not straightforward to conduct due to the fermion doubling problem. Here we present an approach based on the continuum model, unharmed by the fermion doubling. The discretization of the BLG continuum Hamiltonian leads to an effective four-band model, with both valleys built-in. We demonstrate its performance with realistic, large-scale systems, and obtain results consistent with experiments and with the tight-binding model, over a broad range of magnetic field.","sentences":["Bernal-stacked bilayer graphene (BLG) provides an ideal basis for gate-controlled, and free of etching, electronic devices.","Theoretical modeling of realistic devices is an essential part of research, however, simulations of large-scale BLG devices continue to be extremely challenging.","Micrometer-sized systems are predominantly beyond the reach of the commonly used atomistic tight-binding method, while other numerical approaches based on the two dimensional Dirac equation are not straightforward to conduct due to the fermion doubling problem.","Here we present an approach based on the continuum model, unharmed by the fermion doubling.","The discretization of the BLG continuum Hamiltonian leads to an effective four-band model, with both valleys built-in.","We demonstrate its performance with realistic, large-scale systems, and obtain results consistent with experiments and with the tight-binding model, over a broad range of magnetic field."],"url":"http://arxiv.org/abs/2403.03155v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-05 17:20:19","title":"Asymptotic expansions with subordinate variables for solutions of the Navier-Stokes equations","abstract":"We study the three-dimensional Navier-Stokes equations in a periodic domain with the force decaying in time. Although the force has a certain coherent decay, as time tends to infinity, it can be too complicated for the previous theory of asymptotic expansions to be applicable. To deal with this issue, we systematically develop a new theory of asymptotic expansions containing the so-called subordinate variables which can be defined recursively. We apply it to obtain an asymptotic expansion for any Leray-Hopf weak solutions. The expansion, in fact, is constructed explicitly and the impact of the subordinate variables can be clearly specified. The complexifications of the Gevrey-Sobolev spaces, and of the Stokes and bilinear operators of the Navier-Stokes equations are utilized to facilitate such a construction.","sentences":["We study the three-dimensional Navier-Stokes equations in a periodic domain with the force decaying in time.","Although the force has a certain coherent decay, as time tends to infinity, it can be too complicated for the previous theory of asymptotic expansions to be applicable.","To deal with this issue, we systematically develop a new theory of asymptotic expansions containing the so-called subordinate variables which can be defined recursively.","We apply it to obtain an asymptotic expansion for any Leray-Hopf weak solutions.","The expansion, in fact, is constructed explicitly and the impact of the subordinate variables can be clearly specified.","The complexifications of the Gevrey-Sobolev spaces, and of the Stokes and bilinear operators of the Navier-Stokes equations are utilized to facilitate such a construction."],"url":"http://arxiv.org/abs/2403.03132v1","category":"math.AP"}
{"created":"2024-03-05 16:59:53","title":"Higher form symmetries and orbifolds of two-dimensional Yang-Mills theory","abstract":"We undertake a detailed study of the gaugings of two-dimensional Yang-Mills theory by its intrinsic charge conjugation 0-form and centre 1-form global symmetries, elucidating their higher algebraic and geometric structures, as well as the meaning of dual lower form symmetries. Our derivations of orbifold gauge theories make use of a combination of standard continuum path integral methods, networks of topological defects, and techniques from higher gauge theory. We provide a unified description of higher and lower form gauge fields for a $p$-form symmetry in the geometric setting of $p$-gerbes, and derive reverse orbifolds by the dual $(-1)$-form symmetries. We identify those orbifolds in which charge conjugation symmetry is spontaneously broken, and relate the breaking to mixed anomalies involving $(-1)$-form symmetries. We extend these considerations to gaugings by the non-invertible 1-form symmetries of two-dimensional Yang-Mills theory by introducing a notion of generalized $\\theta$-angle.","sentences":["We undertake a detailed study of the gaugings of two-dimensional Yang-Mills theory by its intrinsic charge conjugation 0-form and centre 1-form global symmetries, elucidating their higher algebraic and geometric structures, as well as the meaning of dual lower form symmetries.","Our derivations of orbifold gauge theories make use of a combination of standard continuum path integral methods, networks of topological defects, and techniques from higher gauge theory.","We provide a unified description of higher and lower form gauge fields for a $p$-form symmetry in the geometric setting of $p$-gerbes, and derive reverse orbifolds by the dual $(-1)$-form symmetries.","We identify those orbifolds in which charge conjugation symmetry is spontaneously broken, and relate the breaking to mixed anomalies involving $(-1)$-form symmetries.","We extend these considerations to gaugings by the non-invertible 1-form symmetries of two-dimensional Yang-Mills theory by introducing a notion of generalized $\\theta$-angle."],"url":"http://arxiv.org/abs/2403.03119v1","category":"hep-th"}
{"created":"2024-03-05 16:56:39","title":"An elliptic problem in dimension N with a varying drift term bounded in $L^N$","abstract":"The present paper is devoted to study the asymptotic behavior of a sequence of linear elliptic equations with a varying drift term, whose coefficients are just bounded in $L^N(\\Omega)$, with $N$ the dimension of the space. It is known that there exists a unique solution for each of these problems in the Sobolev space $H^1_0(\\Omega)$. However, because the operators are not coercive, there is no uniform estimate of the solutions in this space. We use some estimates in \\cite{boc1}, and a regularization obtained by adding a small nonlinear first order term, to pass to the limit in these problems.","sentences":["The present paper is devoted to study the asymptotic behavior of a sequence of linear elliptic equations with a varying drift term, whose coefficients are just bounded in $L^N(\\Omega)$, with $N$ the dimension of the space.","It is known that there exists a unique solution for each of these problems in the Sobolev space $H^1_0(\\Omega)$.","However, because the operators are not coercive, there is no uniform estimate of the solutions in this space.","We use some estimates in \\cite{boc1}, and a regularization obtained by adding a small nonlinear first order term, to pass to the limit in these problems."],"url":"http://arxiv.org/abs/2403.03115v1","category":"math.AP"}
{"created":"2024-03-05 16:45:41","title":"Low-rank approximated Kalman-Bucy filters using Oja's principal component flow for linear time-invariant systems","abstract":"The Kalman-Bucy filter is widely used in various applications. However, the filter becomes computationally complex under large-scale systems. To address this problem, a low-rank approximated Kalman-Bucy filter consisting of Oja's principal component flow and a low-dimensional Riccati differential equation was proposed. However, the estimation error was established only for linear time-invariant systems with a symmetric system matrix. This study removes restrictions on the symmetricity of the system matrix and reveals the equilibrium points of the Oja flow and their stability for general real square matrices. In addition, the attraction domain for a set of stable equilibrium points is estimated. Based on these results, we demonstrate that the low-rank approximated Kalman-Bucy filter has a bounded estimation error covariance matrix when the system is controllable and observable.","sentences":["The Kalman-Bucy filter is widely used in various applications.","However, the filter becomes computationally complex under large-scale systems.","To address this problem, a low-rank approximated Kalman-Bucy filter consisting of Oja's principal component flow and a low-dimensional Riccati differential equation was proposed.","However, the estimation error was established only for linear time-invariant systems with a symmetric system matrix.","This study removes restrictions on the symmetricity of the system matrix and reveals the equilibrium points of the Oja flow and their stability for general real square matrices.","In addition, the attraction domain for a set of stable equilibrium points is estimated.","Based on these results, we demonstrate that the low-rank approximated Kalman-Bucy filter has a bounded estimation error covariance matrix when the system is controllable and observable."],"url":"http://arxiv.org/abs/2403.03104v1","category":"math.OC"}
{"created":"2024-03-05 16:03:34","title":"Topological balance of cell distributions in plane monolayers","abstract":"Most of normal proliferative epithelia of plants and metazoans are topologically invariant and characterized by similar cell distributions according to the number of cell neighbors (DCNs). Here we study peculiarities of these distributions and explain why the DCN obtained from the location of intercellular boundaries and that based on the Voronoi tessellation with nodes located on cell nuclei may differ from each other. As we demonstrate, special microdomains where four or more intercellular boundaries converge are topologically charged. Using this fact, we deduce a new equation describing the topological balance of the DCNs. The developed theory is applied for a series of microphotographs of non-tumoral epithelial cells of the human cervix (HCerEpiC) to improve the image processing near the edges of microphotographs and reveal the topological invariance of the examined monolayers. Special contact microdomains may be present in epithelia of various natures, however, considering the well-known vertex model of epithelium, we show that such contacts are absent in the usual solid-like state of the model and appear only in the liquid-like cancer state. Also, we discuss a possible biological role of special contacts in context of proliferative epithelium dynamics and tissue morphogenesis.","sentences":["Most of normal proliferative epithelia of plants and metazoans are topologically invariant and characterized by similar cell distributions according to the number of cell neighbors (DCNs).","Here we study peculiarities of these distributions and explain why the DCN obtained from the location of intercellular boundaries and that based on the Voronoi tessellation with nodes located on cell nuclei may differ from each other.","As we demonstrate, special microdomains where four or more intercellular boundaries converge are topologically charged.","Using this fact, we deduce a new equation describing the topological balance of the DCNs.","The developed theory is applied for a series of microphotographs of non-tumoral epithelial cells of the human cervix (HCerEpiC) to improve the image processing near the edges of microphotographs and reveal the topological invariance of the examined monolayers.","Special contact microdomains may be present in epithelia of various natures, however, considering the well-known vertex model of epithelium, we show that such contacts are absent in the usual solid-like state of the model and appear only in the liquid-like cancer state.","Also, we discuss a possible biological role of special contacts in context of proliferative epithelium dynamics and tissue morphogenesis."],"url":"http://arxiv.org/abs/2403.03079v1","category":"cond-mat.soft"}
{"created":"2024-03-05 16:01:28","title":"Fast and robust method for screened Poisson lattice Green's function using asymptotic expansion and Fast Fourier Transform","abstract":"We study the lattice Green's function (LGF) of the screened Poisson equation on a two-dimensional rectangular lattice. This LGF arises in numerical analysis, random walks, solid-state physics, and other fields. Its defining characteristic is the screening term, which defines different regimes. When its coefficient is large, we can accurately approximate the LGF with an exponentially converging asymptotic expansion, and its convergence rate monotonically increases with the coefficient of the screening term. To tabulate the LGF when the coefficient is not large, we derive a one-dimensional integral representation of the LGF. We show that the trapezoidal rule can approximate this integral with exponential convergence, and we propose an efficient algorithm for its evaluation via the Fast Fourier Transform. We discuss applications including computing the LGF of the three-dimensional Poisson equation with one periodic direction and the return probability of a two-dimensional random walk with killing.","sentences":["We study the lattice Green's function (LGF) of the screened Poisson equation on a two-dimensional rectangular lattice.","This LGF arises in numerical analysis, random walks, solid-state physics, and other fields.","Its defining characteristic is the screening term, which defines different regimes.","When its coefficient is large, we can accurately approximate the LGF with an exponentially converging asymptotic expansion, and its convergence rate monotonically increases with the coefficient of the screening term.","To tabulate the LGF when the coefficient is not large, we derive a one-dimensional integral representation of the LGF.","We show that the trapezoidal rule can approximate this integral with exponential convergence, and we propose an efficient algorithm for its evaluation via the Fast Fourier Transform.","We discuss applications including computing the LGF of the three-dimensional Poisson equation with one periodic direction and the return probability of a two-dimensional random walk with killing."],"url":"http://arxiv.org/abs/2403.03076v1","category":"math.NA"}
{"created":"2024-03-05 15:59:54","title":"On a Neural Implementation of Brenier's Polar Factorization","abstract":"In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^* \\circ F$, or learned as an auxiliary network. Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator. We illustrate possible applications of \\citeauthor{Brenier1991PolarFA}'s polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave.","sentences":["In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$.","The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning.","The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network.","The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^*","\\circ F$, or learned as an auxiliary network.","Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator.","We illustrate possible applications of \\citeauthor{Brenier1991PolarFA}'s polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave."],"url":"http://arxiv.org/abs/2403.03071v1","category":"stat.ML"}
{"created":"2024-03-05 15:33:30","title":"Some solutions to the eigenstate equation of free scalar quantum field theory in the Schr\u00f6dinger representation","abstract":"Using extensions of the quadratic form in the potential term we construct Gaussian eigenstates of the free scalar quantum Hamiltonian operator acting in nontrivial functional space. Admissible positive closed extensions are generated at least by two external sources, the distance between them being limited by the extension parameter. The constructed functionals, in contrast to the ground state of the free theory, correspond to different boundary conditions and can be interpreted as eigenstates of some renormalized asymptotically free quantum Hamiltonian.","sentences":["Using extensions of the quadratic form in the potential term we construct Gaussian eigenstates of the free scalar quantum Hamiltonian operator acting in nontrivial functional space.","Admissible positive closed extensions are generated at least by two external sources, the distance between them being limited by the extension parameter.","The constructed functionals, in contrast to the ground state of the free theory, correspond to different boundary conditions and can be interpreted as eigenstates of some renormalized asymptotically free quantum Hamiltonian."],"url":"http://arxiv.org/abs/2403.03049v1","category":"math-ph"}
{"created":"2024-03-05 15:31:35","title":"Design of Stochastic Quantizers for Privacy Preservation","abstract":"In this paper, we examine the role of stochastic quantizers for privacy preservation. We first employ a static stochastic quantizer and investigate its corresponding privacy-preserving properties. Specifically, we demonstrate that a sufficiently large quantization step guarantees $(0, \\delta)$ differential privacy. Additionally, the degradation of control performance caused by quantization is evaluated as the tracking error of output regulation. These two analyses characterize the trade-off between privacy and control performance, determined by the quantization step. This insight enables us to use quantization intentionally as a means to achieve the seemingly conflicting two goals of maintaining control performance and preserving privacy at the same time; towards this end, we further investigate a dynamic stochastic quantizer. Under a stability assumption, the dynamic stochastic quantizer can enhance privacy, more than the static one, while achieving the same control performance. We further handle the unstable case by additionally applying input Gaussian noise.","sentences":["In this paper, we examine the role of stochastic quantizers for privacy preservation.","We first employ a static stochastic quantizer and investigate its corresponding privacy-preserving properties.","Specifically, we demonstrate that a sufficiently large quantization step guarantees $(0, \\delta)$ differential privacy.","Additionally, the degradation of control performance caused by quantization is evaluated as the tracking error of output regulation.","These two analyses characterize the trade-off between privacy and control performance, determined by the quantization step.","This insight enables us to use quantization intentionally as a means to achieve the seemingly conflicting two goals of maintaining control performance and preserving privacy at the same time; towards this end, we further investigate a dynamic stochastic quantizer.","Under a stability assumption, the dynamic stochastic quantizer can enhance privacy, more than the static one, while achieving the same control performance.","We further handle the unstable case by additionally applying input Gaussian noise."],"url":"http://arxiv.org/abs/2403.03048v1","category":"eess.SY"}
{"created":"2024-03-05 15:22:49","title":"Proof-of-concept for a nonadditive stochastic model of supercooled liquids","abstract":"The recently proposed non-additive stochastic model (NSM) offers a coherent physical interpretation for diffusive phenomena in glass-forming systems. This model presents non-exponential relationships between viscosity, activation energy, and temperature, characterizing the non-Arrhenius behavior observed in supercooled liquids. In this work, we fit the NSM viscosity equation to experimental temperature-dependent viscosity data corresponding to twenty-five glass-forming liquids and compare the fit parameters with those obtained using the Vogel-Fulcher-Tammann (VFT), Avramov-Milchev (AM), and Mauro-Yue-Ellison-Gupta-Allan (MYEGA) models. The results demonstrate that the NSM provides an effective fitting equation for modeling viscosity experimental data in comparison with other established models (VFT, AM and MYEGA), characterizing the activation energy in fragile liquids, presenting a reliable indicator of the degree of fragility of the glass-forming liquids.","sentences":["The recently proposed non-additive stochastic model (NSM) offers a coherent physical interpretation for diffusive phenomena in glass-forming systems.","This model presents non-exponential relationships between viscosity, activation energy, and temperature, characterizing the non-Arrhenius behavior observed in supercooled liquids.","In this work, we fit the NSM viscosity equation to experimental temperature-dependent viscosity data corresponding to twenty-five glass-forming liquids and compare the fit parameters with those obtained using the Vogel-Fulcher-Tammann (VFT), Avramov-Milchev (AM), and Mauro-Yue-Ellison-Gupta-Allan (MYEGA) models.","The results demonstrate that the NSM provides an effective fitting equation for modeling viscosity experimental data in comparison with other established models (VFT, AM and MYEGA), characterizing the activation energy in fragile liquids, presenting a reliable indicator of the degree of fragility of the glass-forming liquids."],"url":"http://arxiv.org/abs/2403.03041v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-05 14:58:31","title":"Predictive power of the Berezinskii-Kosterlitz-Thouless theory based on Renormalization Group throughout the BCS-BEC crossover in 2D superconductors","abstract":"Recent experiments on 2D superconductors allow the characterization of the critical temperature and of the phase diagram across the BCS-BEC crossover as a function of density. We obtain from these experiments the microscopic parameters of the superconducting state at low temperatures by the BCS mean-field approach. For Li$_x$ZrNCl, the extracted parameters are used to evaluate the superconducting phase stiffness and the Berezinskii-Kosterlitz-Thouless (BKT) critical temperature throughout the BCS-BEC crossover, by implementing the corresponding Renormalization Group (RG) approach. In this way, we make a quantitative test of the predictive power of the BKT theory for evaluating the critical temperature. The RG flow equations turn out to give a sizable renormalization of the phase stiffness and of the critical temperature, which is crucial to obtain a satisfactory agreement between the BKT theory and the experiments, in particular in the BCS-BEC crossover regime. We predict the temperature range where phase stiffness renormalization can be measured in Li$_x$ZrNCl across the BCS-BEC crossover. Contrary to other microscopic theories of superconductivity, we find that the BKT theory can be exploited to evaluate quantitatively the critical temperature of 2D superconductors in different pairing regimes.","sentences":["Recent experiments on 2D superconductors allow the characterization of the critical temperature and of the phase diagram across the BCS-BEC crossover as a function of density.","We obtain from these experiments the microscopic parameters of the superconducting state at low temperatures by the BCS mean-field approach.","For Li$_x$ZrNCl, the extracted parameters are used to evaluate the superconducting phase stiffness and the Berezinskii-Kosterlitz-Thouless (BKT) critical temperature throughout the BCS-BEC crossover, by implementing the corresponding Renormalization Group (RG) approach.","In this way, we make a quantitative test of the predictive power of the BKT theory for evaluating the critical temperature.","The RG flow equations turn out to give a sizable renormalization of the phase stiffness and of the critical temperature, which is crucial to obtain a satisfactory agreement between the BKT theory and the experiments, in particular in the BCS-BEC crossover regime.","We predict the temperature range where phase stiffness renormalization can be measured in Li$_x$ZrNCl across the BCS-BEC crossover.","Contrary to other microscopic theories of superconductivity, we find that the BKT theory can be exploited to evaluate quantitatively the critical temperature of 2D superconductors in different pairing regimes."],"url":"http://arxiv.org/abs/2403.03025v1","category":"cond-mat.supr-con"}
{"created":"2024-03-05 14:57:23","title":"On Airy solutions of P$_\\mathrm{II}$ and the complex cubic ensemble of random matrices, II","abstract":"We describe the pole-free regions of the one-parameter family of special solutions of P$_\\mathrm{II}$, the second Painlev\\'e equation, constructed from the Airy functions. This is achieved by exploiting the connection between these solutions and the recurrence coefficients of orthogonal polynomials that appear in the analysis of the ensemble of random matrices corresponding to the cubic potential.","sentences":["We describe the pole-free regions of the one-parameter family of special solutions of P$_\\mathrm{II}$, the second Painlev\\'e equation, constructed from the Airy functions.","This is achieved by exploiting the connection between these solutions and the recurrence coefficients of orthogonal polynomials that appear in the analysis of the ensemble of random matrices corresponding to the cubic potential."],"url":"http://arxiv.org/abs/2403.03023v1","category":"math-ph"}
{"created":"2024-03-05 14:57:13","title":"Accelerating the convergence of Newton's method for nonlinear elliptic PDEs using Fourier neural operators","abstract":"It is well known that Newton's method, especially when applied to large problems such as the discretization of nonlinear partial differential equations (PDEs), can have trouble converging if the initial guess is too far from the solution. This work focuses on accelerating this convergence, in the context of the discretization of nonlinear elliptic PDEs. We first provide a quick review of existing methods, and justify our choice of learning an initial guess with a Fourier neural operator (FNO). This choice was motivated by the mesh-independence of such operators, whose training and evaluation can be performed on grids with different resolutions. The FNO is trained using a loss minimization over generated data, loss functions based on the PDE discretization. Numerical results, in one and two dimensions, show that the proposed initial guess accelerates the convergence of Newton's method by a large margin compared to a naive initial guess, especially for highly nonlinear or anisotropic problems.","sentences":["It is well known that Newton's method, especially when applied to large problems such as the discretization of nonlinear partial differential equations (PDEs), can have trouble converging if the initial guess is too far from the solution.","This work focuses on accelerating this convergence, in the context of the discretization of nonlinear elliptic PDEs.","We first provide a quick review of existing methods, and justify our choice of learning an initial guess with a Fourier neural operator (FNO).","This choice was motivated by the mesh-independence of such operators, whose training and evaluation can be performed on grids with different resolutions.","The FNO is trained using a loss minimization over generated data, loss functions based on the PDE discretization.","Numerical results, in one and two dimensions, show that the proposed initial guess accelerates the convergence of Newton's method by a large margin compared to a naive initial guess, especially for highly nonlinear or anisotropic problems."],"url":"http://arxiv.org/abs/2403.03021v1","category":"math.NA"}
{"created":"2024-03-05 14:52:09","title":"Wavelet Scattering Networks for Identifying Radio Galaxy Morphologies","abstract":"Classifying the morphologies of radio galaxies is important to understand their physical properties and evolutionary histories. A galaxy's morphology is often determined by visual inspection, but as survey size increases robust automated techniques will be needed. Deep neural networks are an attractive method for automated classification, but have many free parameters and therefore require extensive training data and are subject to overfitting and generalization issues. We explore hybrid classification methods using the scattering transform, the recursive wavelet decomposition of an input image. We analyse the performance of the scattering transform for the Fanaroff-Riley classification of radio galaxies with respect to CNNs and other machine learning algorithms. We test the robustness of the different classification methods with training data truncation and noise injection, and find that the scattering transform can offer competitive performance with the most accurate CNNs.","sentences":["Classifying the morphologies of radio galaxies is important to understand their physical properties and evolutionary histories.","A galaxy's morphology is often determined by visual inspection, but as survey size increases robust automated techniques will be needed.","Deep neural networks are an attractive method for automated classification, but have many free parameters and therefore require extensive training data and are subject to overfitting and generalization issues.","We explore hybrid classification methods using the scattering transform, the recursive wavelet decomposition of an input image.","We analyse the performance of the scattering transform for the Fanaroff-Riley classification of radio galaxies with respect to CNNs and other machine learning algorithms.","We test the robustness of the different classification methods with training data truncation and noise injection, and find that the scattering transform can offer competitive performance with the most accurate CNNs."],"url":"http://arxiv.org/abs/2403.03016v1","category":"astro-ph.IM"}
{"created":"2024-03-05 14:34:29","title":"Generation of gigahertz frequency surface acoustic waves in YIG/ZnO heterostructures","abstract":"We study surface acoustic waves (SAWs) in yttrium iron garnet (YIG)/zinc oxide (ZnO) heterostructures, comparing the results of a computationally lightweight analytical model with time-resolved micro-focused Brillouin light scattering data. Interdigital transducers (IDTs), with operational frequencies in the gigahertz regime, were fabricated on 50 and 100nm thin films of YIG prior to sputter deposition of 830nm and 890nm films of piezoelectric ZnO. We find good agreement between our analytical model and micro-focused Brillouin light scattering data of the IDT frequency response and SAW group velocity, with clear differentiation between the Rayleigh and Sezawa-like modes. This work paves the way for the study of SAW-spin wave (SW) interactions in low SW damping YIG, with the possibility of a method for future energy-efficient SW excitation.","sentences":["We study surface acoustic waves (SAWs) in yttrium iron garnet (YIG)/zinc oxide (ZnO) heterostructures, comparing the results of a computationally lightweight analytical model with time-resolved micro-focused Brillouin light scattering data.","Interdigital transducers (IDTs), with operational frequencies in the gigahertz regime, were fabricated on 50 and 100nm thin films of YIG prior to sputter deposition of 830nm and 890nm films of piezoelectric ZnO.","We find good agreement between our analytical model and micro-focused Brillouin light scattering data of the IDT frequency response and SAW group velocity, with clear differentiation between the Rayleigh and Sezawa-like modes.","This work paves the way for the study of SAW-spin wave (SW) interactions in low SW damping YIG, with the possibility of a method for future energy-efficient SW excitation."],"url":"http://arxiv.org/abs/2403.03006v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-05 14:32:26","title":"Ultralight vector dark matter search using data from the KAGRA O3GK run","abstract":"Among the various candidates for dark matter (DM), ultralight vector DM can be probed by laser interferometric gravitational wave detectors through the measurement of oscillating length changes in the arm cavities. In this context, KAGRA has a unique feature due to differing compositions of its mirrors, enhancing the signal of vector DM in the length change in the auxiliary channels. Here we present the result of a search for $U(1)_{B-L}$ gauge boson DM using the KAGRA data from auxiliary length channels during the first joint observation run together with GEO600. By applying our search pipeline, which takes into account the stochastic nature of ultralight DM, upper bounds on the coupling strength between the $U(1)_{B-L}$ gauge boson and ordinary matter are obtained for a range of DM masses. While our constraints are less stringent than those derived from previous experiments, this study demonstrates the applicability of our method to the lower-mass vector DM search, which is made difficult in this measurement by the short observation time compared to the auto-correlation time scale of DM.","sentences":["Among the various candidates for dark matter (DM), ultralight vector DM can be probed by laser interferometric gravitational wave detectors through the measurement of oscillating length changes in the arm cavities.","In this context, KAGRA has a unique feature due to differing compositions of its mirrors, enhancing the signal of vector DM in the length change in the auxiliary channels.","Here we present the result of a search for $U(1)_{B-L}$ gauge boson DM using the KAGRA data from auxiliary length channels during the first joint observation run together with GEO600.","By applying our search pipeline, which takes into account the stochastic nature of ultralight DM, upper bounds on the coupling strength between the $U(1)_{B-L}$ gauge boson and ordinary matter are obtained for a range of DM masses.","While our constraints are less stringent than those derived from previous experiments, this study demonstrates the applicability of our method to the lower-mass vector DM search, which is made difficult in this measurement by the short observation time compared to the auto-correlation time scale of DM."],"url":"http://arxiv.org/abs/2403.03004v1","category":"astro-ph.CO"}
{"created":"2024-03-05 14:21:58","title":"A Convex Optimization Framework for Computing Robustness Margins of Kalman Filters","abstract":"This paper proposes a novel convex optimization framework for designing robust Kalman filters that guarantee a user-specified steady-state error while maximizing process and sensor noise. The proposed framework simultaneously determines the Kalman gain and the robustness margin in terms of the process and sensor noise. This is the first paper to present such a joint formulation for Kalman filtering. The proposed methodology is validated through two distinct examples: the Clohessy-Wiltshire-Hill equations for a chaser spacecraft in an elliptical orbit and the longitudinal motion model of an F-16 aircraft.","sentences":["This paper proposes a novel convex optimization framework for designing robust Kalman filters that guarantee a user-specified steady-state error while maximizing process and sensor noise.","The proposed framework simultaneously determines the Kalman gain and the robustness margin in terms of the process and sensor noise.","This is the first paper to present such a joint formulation for Kalman filtering.","The proposed methodology is validated through two distinct examples: the Clohessy-Wiltshire-Hill equations for a chaser spacecraft in an elliptical orbit and the longitudinal motion model of an F-16 aircraft."],"url":"http://arxiv.org/abs/2403.02996v1","category":"eess.SY"}
{"created":"2024-03-05 14:03:56","title":"Assessing the distribution of cancer stem cells in tumorspheres","abstract":"In previous theoretical research, we inferred that cancer stem cells (CSCs), the cells that presumably drive tumor growth and resistance to conventional cancer treatments, are not uniformly distributed in the bulk of a tumorsphere. To confirm this theoretical prediction, we cultivated tumorspheres enriched in CSCs, and performed immunofluorecent detection of the stemness marker SOX2 using a confocal microscope.   In this article, we present a method developed to process the images that reconstruct the amount and location of the CSCs in the tumorspheres. Its advantage is the use of a statistical criterion to classify the cells in stem and differentiated instead of setting an arbitrary threshold. From the analysis of the results of the methods using graph theory and computational modeling, we concluded that the distribution of Cancer Stem Cells in an experimental tumorsphere is non-homogeneous. This method is independent of the tumorsphere assay being useful for analyzing images in which several different kinds of cells are stained with different markers.","sentences":["In previous theoretical research, we inferred that cancer stem cells (CSCs), the cells that presumably drive tumor growth and resistance to conventional cancer treatments, are not uniformly distributed in the bulk of a tumorsphere.","To confirm this theoretical prediction, we cultivated tumorspheres enriched in CSCs, and performed immunofluorecent detection of the stemness marker SOX2 using a confocal microscope.   ","In this article, we present a method developed to process the images that reconstruct the amount and location of the CSCs in the tumorspheres.","Its advantage is the use of a statistical criterion to classify the cells in stem and differentiated instead of setting an arbitrary threshold.","From the analysis of the results of the methods using graph theory and computational modeling, we concluded that the distribution of Cancer Stem Cells in an experimental tumorsphere is non-homogeneous.","This method is independent of the tumorsphere assay being useful for analyzing images in which several different kinds of cells are stained with different markers."],"url":"http://arxiv.org/abs/2403.02984v1","category":"q-bio.QM"}
{"created":"2024-03-05 13:58:18","title":"System performance of a TDM test-bed with long flex harness towards the new X-IFU FPA-DM","abstract":"SRON (Netherlands Institute for Space Research) is developing the Focal Plane Assembly (FPA) for Athena X-IFU, whose Demonstration Model (DM) will use for the first time a time domain multiplexing (TDM)-based readout system for the on-board transition-edge sensors (TES). We report on the characterization activities on a TDM setup provided by NASA Goddard Space Flight Center (GSFC) and National Institute for Standards and Technology (NIST) and tested in SRON cryogenic test facilities. The goal of these activities is to study the impact of the longer harness, closer to X-IFU specs, in a different EMI environment and switching from a single-ended to a differential readout scheme. In this contribution we describe the advancement in the debugging of the system in the SRON cryostat, which led to the demonstration of the nominal spectral performance of 2.8 eV at 5.9~keV with 16-row multiplexing, as well as an outlook for the future endeavours for the TDM readout integration on X-IFU's FPA-DM at SRON.","sentences":["SRON (Netherlands Institute for Space Research) is developing the Focal Plane Assembly (FPA) for Athena X-IFU, whose Demonstration Model (DM) will use for the first time a time domain multiplexing (TDM)-based readout system for the on-board transition-edge sensors (TES).","We report on the characterization activities on a TDM setup provided by NASA Goddard Space Flight Center (GSFC) and National Institute for Standards and Technology (NIST) and tested in SRON cryogenic test facilities.","The goal of these activities is to study the impact of the longer harness, closer to X-IFU specs, in a different EMI environment and switching from a single-ended to a differential readout scheme.","In this contribution we describe the advancement in the debugging of the system in the SRON cryostat, which led to the demonstration of the nominal spectral performance of 2.8 eV at 5.9~keV with 16-row multiplexing, as well as an outlook for the future endeavours for the TDM readout integration on X-IFU's FPA-DM at SRON."],"url":"http://arxiv.org/abs/2403.02978v1","category":"astro-ph.IM"}
{"created":"2024-03-05 13:48:55","title":"Periodically activated physics-informed neural networks for assimilation tasks for three-dimensional Rayleigh-Benard convection","abstract":"We apply physics-informed neural networks to three-dimensional Rayleigh-Benard convection in a cubic cell with a Rayleigh number of Ra=10^6 and a Prandtl number of Pr=0.7 to assimilate the velocity vector field from given temperature fields and vice versa. With the respective ground truth data provided by a direct numerical simulation, we are able to evaluate the performance of the different activation functions applied (sine, hyperbolic tangent and exponential linear unit) and different numbers of neuron (32, 64, 128) for each of the five hidden layers of the multi-layer perceptron. The main result is that the use of a periodic activation function (sine) typically benefits the assimilation performance in terms of the analyzed metrics, correlation with the ground truth and mean average error. The higher quality of results from sine-activated physics-informed neural networks is also manifested in the probability density function and power spectra of the inferred velocity or temperature fields. Regarding the two assimilation directions, the assimilation of temperature fields based on velocities appeared to be more challenging in the sense that it exhibited a sharper limit on the number of neurons below which viable assimilation results could not be achieved.","sentences":["We apply physics-informed neural networks to three-dimensional Rayleigh-Benard convection in a cubic cell with a Rayleigh number of Ra=10^6 and a Prandtl number of Pr=0.7 to assimilate the velocity vector field from given temperature fields and vice versa.","With the respective ground truth data provided by a direct numerical simulation, we are able to evaluate the performance of the different activation functions applied (sine, hyperbolic tangent and exponential linear unit) and different numbers of neuron (32, 64, 128) for each of the five hidden layers of the multi-layer perceptron.","The main result is that the use of a periodic activation function (sine) typically benefits the assimilation performance in terms of the analyzed metrics, correlation with the ground truth and mean average error.","The higher quality of results from sine-activated physics-informed neural networks is also manifested in the probability density function and power spectra of the inferred velocity or temperature fields.","Regarding the two assimilation directions, the assimilation of temperature fields based on velocities appeared to be more challenging in the sense that it exhibited a sharper limit on the number of neurons below which viable assimilation results could not be achieved."],"url":"http://arxiv.org/abs/2403.02970v1","category":"physics.flu-dyn"}
{"created":"2024-03-05 13:20:55","title":"Radial amplitude equations for fully localised planar patterns","abstract":"Isolated patches of spatially oscillating pattern have been found to emerge near a pattern-forming instability in a wide variety of experiments and mathematical models. However, there is currently no mathematical theory to explain this emergence or characterise the structure of these patches. We provide a method for formally deriving radial amplitude equations to planar patterns via non-autonomous multiple-scale analysis and convolutional sums of products of Bessel functions. Our novel approach introduces nonautonomous differential operators, which allow for the systematic manipulation of Bessel functions, as well as previously unseen identities involving infinite sums of Bessel functions. Solutions of the amplitude equations describe fully localised patterns with non-trivial angular dependence, where localisation occurs in a purely radial direction. Amplitude equations are derived for multiple examples of patterns with dihedral symmetry, including fully localised hexagons and quasipatterns with twelve-fold rotational symmetry. In particular, we show how to apply the asymptotic method to the Swift--Hohenberg equation and general reaction-diffusion systems.","sentences":["Isolated patches of spatially oscillating pattern have been found to emerge near a pattern-forming instability in a wide variety of experiments and mathematical models.","However, there is currently no mathematical theory to explain this emergence or characterise the structure of these patches.","We provide a method for formally deriving radial amplitude equations to planar patterns via non-autonomous multiple-scale analysis and convolutional sums of products of Bessel functions.","Our novel approach introduces nonautonomous differential operators, which allow for the systematic manipulation of Bessel functions, as well as previously unseen identities involving infinite sums of Bessel functions.","Solutions of the amplitude equations describe fully localised patterns with non-trivial angular dependence, where localisation occurs in a purely radial direction.","Amplitude equations are derived for multiple examples of patterns with dihedral symmetry, including fully localised hexagons and quasipatterns with twelve-fold rotational symmetry.","In particular, we show how to apply the asymptotic method to the Swift--Hohenberg equation and general reaction-diffusion systems."],"url":"http://arxiv.org/abs/2403.02949v1","category":"math.DS"}
{"created":"2024-03-05 13:11:28","title":"ISC: an RADI-type method for stochastic continuous-time algebraic Riccati equations","abstract":"In this paper, we propose an RADI-type method for large-scale stochastic continuous-time algebraic Riccati equations with sparse and low-rank structures. The so-called ISC method is developed by using the Incorporation idea together with different Shifts to accelerate the convergence and Compressions to reduce the storage and complexity. Numerical experiments are given to show its efficiency.","sentences":["In this paper, we propose an RADI-type method for large-scale stochastic continuous-time algebraic Riccati equations with sparse and low-rank structures.","The so-called ISC method is developed by using the Incorporation idea together with different Shifts to accelerate the convergence and Compressions to reduce the storage and complexity.","Numerical experiments are given to show its efficiency."],"url":"http://arxiv.org/abs/2403.02940v1","category":"math.NA"}
{"created":"2024-03-05 12:51:40","title":"Fuzzy Datalog$^\\exists$ over Arbitrary t-Norms","abstract":"One of the main challenges in the area of Neuro-Symbolic AI is to perform logical reasoning in the presence of both neural and symbolic data. This requires combining heterogeneous data sources such as knowledge graphs, neural model predictions, structured databases, crowd-sourced data, and many more. To allow for such reasoning, we generalise the standard rule-based language Datalog with existential rules (commonly referred to as tuple-generating dependencies) to the fuzzy setting, by allowing for arbitrary t-norms in the place of classical conjunctions in rule bodies. The resulting formalism allows us to perform reasoning about data associated with degrees of uncertainty while preserving computational complexity results and the applicability of reasoning techniques established for the standard Datalog setting. In particular, we provide fuzzy extensions of Datalog chases which produce fuzzy universal models and we exploit them to show that in important fragments of the language, reasoning has the same complexity as in the classical setting.","sentences":["One of the main challenges in the area of Neuro-Symbolic AI is to perform logical reasoning in the presence of both neural and symbolic data.","This requires combining heterogeneous data sources such as knowledge graphs, neural model predictions, structured databases, crowd-sourced data, and many more.","To allow for such reasoning, we generalise the standard rule-based language Datalog with existential rules (commonly referred to as tuple-generating dependencies) to the fuzzy setting, by allowing for arbitrary t-norms in the place of classical conjunctions in rule bodies.","The resulting formalism allows us to perform reasoning about data associated with degrees of uncertainty while preserving computational complexity results and the applicability of reasoning techniques established for the standard Datalog setting.","In particular, we provide fuzzy extensions of Datalog chases which produce fuzzy universal models and we exploit them to show that in important fragments of the language, reasoning has the same complexity as in the classical setting."],"url":"http://arxiv.org/abs/2403.02933v1","category":"cs.AI"}
{"created":"2024-03-05 12:48:02","title":"Loss Design for Single-carrier Joint Communication and Neural Network-based Sensing","abstract":"We evaluate the influence of multi-snapshot sensing and varying signal-to-noise ratio (SNR) on the overall performance of neural network (NN)-based joint communication and sensing (JCAS) systems. To enhance the training behavior, we decouple the loss functions from the respective SNR values and the number of sensing snapshots, using bounds of the sensing performance. Pre-processing is done through conventional sensing signal processing steps on the inputs to the sensing NN. The proposed method outperforms classical algorithms, such as a Neyman-Pearson-based power detector for object detection and ESPRIT for angle of arrival (AoA) estimation for quadrature amplitude modulation (QAM) at low SNRs.","sentences":["We evaluate the influence of multi-snapshot sensing and varying signal-to-noise ratio (SNR) on the overall performance of neural network (NN)-based joint communication and sensing (JCAS) systems.","To enhance the training behavior, we decouple the loss functions from the respective SNR values and the number of sensing snapshots, using bounds of the sensing performance.","Pre-processing is done through conventional sensing signal processing steps on the inputs to the sensing NN.","The proposed method outperforms classical algorithms, such as a Neyman-Pearson-based power detector for object detection and ESPRIT for angle of arrival (AoA) estimation for quadrature amplitude modulation (QAM) at low SNRs."],"url":"http://arxiv.org/abs/2403.02929v1","category":"eess.SP"}
{"created":"2024-03-05 12:41:21","title":"Weighted floating functions and weighted functional affine surface areas","abstract":"The purpose of this paper is to introduce the new concept of weighted floating functions associated with log concave or $s$-concave functions. This leads to new notions of weighted functional affine surface areas. Their relation to more traditional versions of functional affine surface areas as well as to the classical affine surface areas for convex bodies is discussed in detail.","sentences":["The purpose of this paper is to introduce the new concept of weighted floating functions associated with log concave or $s$-concave functions.","This leads to new notions of weighted functional affine surface areas.","Their relation to more traditional versions of functional affine surface areas as well as to the classical affine surface areas for convex bodies is discussed in detail."],"url":"http://arxiv.org/abs/2403.02925v1","category":"math.MG"}
{"created":"2024-03-05 12:38:54","title":"From Spectra to Biophysical Insights: End-to-End Learning with a Biased Radiative Transfer Model","abstract":"Advances in machine learning have boosted the use of Earth observation data for climate change research. Yet, the interpretability of machine-learned representations remains a challenge, particularly in understanding forests' biophysical reactions to climate change. Traditional methods in remote sensing that invert radiative transfer models (RTMs) to retrieve biophysical variables from spectral data often fail to account for biases inherent in the RTM, especially for complex forests. We propose to integrate RTMs into an auto-encoder architecture, creating an end-to-end learning approach. Our method not only corrects biases in RTMs but also outperforms traditional techniques for variable retrieval like neural network regression. Furthermore, our framework has potential generally for inverting biased physical models. The code is available on https://github.com/yihshe/ai-refined-rtm.git.","sentences":["Advances in machine learning have boosted the use of Earth observation data for climate change research.","Yet, the interpretability of machine-learned representations remains a challenge, particularly in understanding forests' biophysical reactions to climate change.","Traditional methods in remote sensing that invert radiative transfer models (RTMs) to retrieve biophysical variables from spectral data often fail to account for biases inherent in the RTM, especially for complex forests.","We propose to integrate RTMs into an auto-encoder architecture, creating an end-to-end learning approach.","Our method not only corrects biases in RTMs but also outperforms traditional techniques for variable retrieval like neural network regression.","Furthermore, our framework has potential generally for inverting biased physical models.","The code is available on https://github.com/yihshe/ai-refined-rtm.git."],"url":"http://arxiv.org/abs/2403.02922v1","category":"cs.LG"}
{"created":"2024-03-05 12:35:18","title":"Single-Channel Robot Ego-Speech Filtering during Human-Robot Interaction","abstract":"In this paper, we study how well human speech can automatically be filtered when this overlaps with the voice and fan noise of a social robot, Pepper. We ultimately aim for an HRI scenario where the microphone can remain open when the robot is speaking, enabling a more natural turn-taking scheme where the human can interrupt the robot. To respond appropriately, the robot would need to understand what the interlocutor said in the overlapping part of the speech, which can be accomplished by target speech extraction (TSE). To investigate how well TSE can be accomplished in the context of the popular social robot Pepper, we set out to manufacture a datase composed of a mixture of recorded speech of Pepper itself, its fan noise (which is close to the microphones), and human speech as recorded by the Pepper microphone, in a room with low reverberation and high reverberation. Comparing a signal processing approach, with and without post-filtering, and a convolutional recurrent neural network (CRNN) approach to a state-of-the-art speaker identification-based TSE model, we found that the signal processing approach without post-filtering yielded the best performance in terms of Word Error Rate on the overlapping speech signals with low reverberation, while the CRNN approach is more robust for reverberation. These results show that estimating the human voice in overlapping speech with a robot is possible in real-life application, provided that the room reverberation is low and the human speech has a high volume or high pitch.","sentences":["In this paper, we study how well human speech can automatically be filtered when this overlaps with the voice and fan noise of a social robot, Pepper.","We ultimately aim for an HRI scenario where the microphone can remain open when the robot is speaking, enabling a more natural turn-taking scheme where the human can interrupt the robot.","To respond appropriately, the robot would need to understand what the interlocutor said in the overlapping part of the speech, which can be accomplished by target speech extraction (TSE).","To investigate how well TSE can be accomplished in the context of the popular social robot Pepper, we set out to manufacture a datase composed of a mixture of recorded speech of Pepper itself, its fan noise (which is close to the microphones), and human speech as recorded by the Pepper microphone, in a room with low reverberation and high reverberation.","Comparing a signal processing approach, with and without post-filtering, and a convolutional recurrent neural network (CRNN) approach to a state-of-the-art speaker identification-based TSE model, we found that the signal processing approach without post-filtering yielded the best performance in terms of Word Error Rate on the overlapping speech signals with low reverberation, while the CRNN approach is more robust for reverberation.","These results show that estimating the human voice in overlapping speech with a robot is possible in real-life application, provided that the room reverberation is low and the human speech has a high volume or high pitch."],"url":"http://arxiv.org/abs/2403.02918v1","category":"cs.RO"}
{"created":"2024-03-05 12:28:04","title":"Scientific machine learning for closure models in multiscale problems: a review","abstract":"Closure problems are omnipresent when simulating multiscale systems, where some quantities and processes cannot be fully prescribed despite their effects on the simulation's accuracy. Recently, scientific machine learning approaches have been proposed as a way to tackle the closure problem, combining traditional (physics-based) modeling with data-driven (machine-learned) techniques, typically through enriching differential equations with neural networks. This paper reviews the different reduced model forms, distinguished by the degree to which they include known physics, and the different objectives of a priori and a posteriori learning. The importance of adhering to physical laws (such as symmetries and conservation laws) in choosing the reduced model form and choosing the learning method is discussed. The effect of spatial and temporal discretization and recent trends toward discretization-invariant models are reviewed. In addition, we make the connections between closure problems and several other research disciplines: inverse problems, Mori-Zwanzig theory, and multi-fidelity methods. In conclusion, much progress has been made with scientific machine learning approaches for solving closure problems, but many challenges remain. In particular, the generalizability and interpretability of learned models is a major issue that needs to be addressed further.","sentences":["Closure problems are omnipresent when simulating multiscale systems, where some quantities and processes cannot be fully prescribed despite their effects on the simulation's accuracy.","Recently, scientific machine learning approaches have been proposed as a way to tackle the closure problem, combining traditional (physics-based) modeling with data-driven (machine-learned) techniques, typically through enriching differential equations with neural networks.","This paper reviews the different reduced model forms, distinguished by the degree to which they include known physics, and the different objectives of a priori and a posteriori learning.","The importance of adhering to physical laws (such as symmetries and conservation laws) in choosing the reduced model form and choosing the learning method is discussed.","The effect of spatial and temporal discretization and recent trends toward discretization-invariant models are reviewed.","In addition, we make the connections between closure problems and several other research disciplines: inverse problems, Mori-Zwanzig theory, and multi-fidelity methods.","In conclusion, much progress has been made with scientific machine learning approaches for solving closure problems, but many challenges remain.","In particular, the generalizability and interpretability of learned models is a major issue that needs to be addressed further."],"url":"http://arxiv.org/abs/2403.02913v1","category":"math.NA"}
{"created":"2024-03-05 12:28:00","title":"Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems","abstract":"We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting. We propose $(\\varepsilon, \\delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $\\log(d)/\\sqrt{n} + \\log(d)/[n\\varepsilon]^{1/2}$ with constant success probability. This result provides evidence of the near-optimality of the approach. Finally, we show that combining our methods with acceleration techniques from online learning leads to the first algorithm for DP Stochastic Convex Optimization in the polyhedral setting that is not based on Frank-Wolfe methods. For convex and first-order-smooth stochastic objectives, our algorithms attain an excess risk of $\\sqrt{\\log(d)/n} + \\log(d)^{7/10}/[n\\varepsilon]^{2/5}$, and when additionally assuming second-order-smoothness, we improve the rate to $\\sqrt{\\log(d)/n} + \\log(d)/\\sqrt{n\\varepsilon}$. Instrumental to all of these results are various extensions of the classical Maurey Sparsification Lemma, which may be of independent interest.","sentences":["We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting.","We propose $(\\varepsilon, \\delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives.","For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size.","Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $\\sqrt{\\log(d)/n} + (\\log(d)^{3/2}/[n\\varepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $\\log(d)/\\sqrt{n} + \\log(d)/[n\\varepsilon]^{1/2}$ with constant success probability.","This result provides evidence of the near-optimality of the approach.","Finally, we show that combining our methods with acceleration techniques from online learning leads to the first algorithm for DP Stochastic Convex Optimization in the polyhedral setting that is not based on Frank-Wolfe methods.","For convex and first-order-smooth stochastic objectives, our algorithms attain an excess risk of $\\sqrt{\\log(d)/n} + \\log(d)^{7/10}/[n\\varepsilon]^{2/5}$, and when additionally assuming second-order-smoothness, we improve the rate to $\\sqrt{\\log(d)/n} + \\log(d)/\\sqrt{n\\varepsilon}$. Instrumental to all of these results are various extensions of the classical Maurey Sparsification Lemma, which may be of independent interest."],"url":"http://arxiv.org/abs/2403.02912v1","category":"math.OC"}
{"created":"2024-03-05 12:18:12","title":"Gaze-Vector Estimation in the Dark with Temporally Encoded Event-driven Neural Networks","abstract":"In this paper, we address the intricate challenge of gaze vector prediction, a pivotal task with applications ranging from human-computer interaction to driver monitoring systems. Our innovative approach is designed for the demanding setting of extremely low-light conditions, leveraging a novel temporal event encoding scheme, and a dedicated neural network architecture. The temporal encoding method seamlessly integrates Dynamic Vision Sensor (DVS) events with grayscale guide frames, generating consecutively encoded images for input into our neural network. This unique solution not only captures diverse gaze responses from participants within the active age group but also introduces a curated dataset tailored for low-light conditions. The encoded temporal frames paired with our network showcase impressive spatial localization and reliable gaze direction in their predictions. Achieving a remarkable 100-pixel accuracy of 100%, our research underscores the potency of our neural network to work with temporally consecutive encoded images for precise gaze vector predictions in challenging low-light videos, contributing to the advancement of gaze prediction technologies.","sentences":["In this paper, we address the intricate challenge of gaze vector prediction, a pivotal task with applications ranging from human-computer interaction to driver monitoring systems.","Our innovative approach is designed for the demanding setting of extremely low-light conditions, leveraging a novel temporal event encoding scheme, and a dedicated neural network architecture.","The temporal encoding method seamlessly integrates Dynamic Vision Sensor (DVS) events with grayscale guide frames, generating consecutively encoded images for input into our neural network.","This unique solution not only captures diverse gaze responses from participants within the active age group but also introduces a curated dataset tailored for low-light conditions.","The encoded temporal frames paired with our network showcase impressive spatial localization and reliable gaze direction in their predictions.","Achieving a remarkable 100-pixel accuracy of 100%, our research underscores the potency of our neural network to work with temporally consecutive encoded images for precise gaze vector predictions in challenging low-light videos, contributing to the advancement of gaze prediction technologies."],"url":"http://arxiv.org/abs/2403.02909v1","category":"cs.CV"}
{"created":"2024-03-05 12:13:27","title":"Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects","abstract":"Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts. Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps. Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted. Therefore, researchers working in this area increasingly need support to process this incoming information. One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest. Another way is to automate the process with image recognition using convolutional neural networks. During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis.","sentences":["Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts.","Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps.","Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted.","Therefore, researchers working in this area increasingly need support to process this incoming information.","One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest.","Another way is to automate the process with image recognition using convolutional neural networks.","During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis."],"url":"http://arxiv.org/abs/2403.02906v1","category":"cs.HC"}
{"created":"2024-03-05 12:10:11","title":"Two models forsandpile growth in weighted graphs","abstract":"In this paper we study $\\infty$-Laplacian type diffusion equations in weighted graphs obtained as limit as $p\\to \\infty$ to two types of $p$-Laplacian evolution equations in such graphs. We propose these diffusion equations, that are governed by the subdifferential of a convex energy functionals associated to the indicator function of the set $$K^G_{\\infty}:= \\left\\{ u \\in L^2(V, \\nu_G) \\ : \\ \\vert u(y) - u(x) \\vert \\leq 1 \\ \\ \\hbox{if} \\ \\ x \\sim y \\right\\}$$ and the set $$K^w_{\\infty}:= \\left\\{ u \\in L^2(V, \\nu_G) \\ : \\ \\vert u(y) - u(x) \\vert \\leq \\sqrt{w_{xy}} \\ \\ \\hbox{if} \\ \\ x \\sim y \\right\\}$$ as models for sandpile growth in weighted graphs. Moreover, we also analyse the collapse of the initial condition when it does not belong to the stable sets $K^G_{\\infty}$ or $K^w_{\\infty}$ by means of an abstract result given in~\\cite{BEG}. We give an interpretation of the limit problems in terms of Monge-Kantorovich mass transport theory. Finally, we give some explicit solutions of simple examples that illustrate the dynamics of the sandpile growing or collapsing.","sentences":["In this paper we study $\\infty$-Laplacian type diffusion equations in weighted graphs obtained as limit as $p\\to \\infty$ to two types of $p$-Laplacian evolution equations in such graphs.","We propose these diffusion equations, that are governed by the subdifferential of a convex energy functionals associated to the indicator function of the set $$K^G_{\\infty}:= \\left\\{ u \\in L^2(V, \\nu_G) \\ :","\\ \\vert u(y) - u(x)","\\vert \\leq 1 \\ \\ \\hbox{if} \\ \\","x \\sim y \\right\\}$$ and the set $$K^w_{\\infty}:= \\left\\{ u \\in L^2(V, \\nu_G) \\ :","\\ \\vert u(y) - u(x) \\vert \\leq \\sqrt{w_{xy}} \\ \\ \\hbox{if} \\ \\ x \\sim y \\right\\}$$ as models for sandpile growth in weighted graphs.","Moreover, we also analyse the collapse of the initial condition when it does not belong to the stable sets $K^G_{\\infty}$ or $K^w_{\\infty}$ by means of an abstract result given in~\\cite{BEG}.","We give an interpretation of the limit problems in terms of Monge-Kantorovich mass transport theory.","Finally, we give some explicit solutions of simple examples that illustrate the dynamics of the sandpile growing or collapsing."],"url":"http://arxiv.org/abs/2403.02900v1","category":"math.AP"}
{"created":"2024-03-05 11:51:00","title":"Coupling Polyatomic Molecules to Lossy Nanocavities: Lindblad versus Schr\u00f6dinger description","abstract":"The usage of cavities to impact molecular structure and dynamics has become popular. As cavities, in particular plasmonic nanocavities, are lossy and the lifetime of their modes can be very short, their lossy nature must be incorporated into the calculations. The Lindblad master equation is commonly considered as an appropriate tool to describe this lossy nature. This approach requires the dynamics of the density operator and is thus substantially more costly than approaches employing the Schr\\\"odinger equation for the quantum wave function when several or many nuclear degrees of freedom are involved. In this work we compare numerically the Lindblad and Schr\\\"odinger descriptions discussed in the literature for a molecular example where the cavity is pumped by a laser. The laser and cavity properties are varied over a range of parameters. It is found that the Schr\\\"odinger description adequately describes the dynamics of the polaritons and emission signal as long as the laser intensity is moderate and the pump time is not much longer than the lifetime of the cavity mode. Otherwise, it is demonstrated that the Schr\\\"odinger description gradually fails. The results are discussed and analyzed.","sentences":["The usage of cavities to impact molecular structure and dynamics has become popular.","As cavities, in particular plasmonic nanocavities, are lossy and the lifetime of their modes can be very short, their lossy nature must be incorporated into the calculations.","The Lindblad master equation is commonly considered as an appropriate tool to describe this lossy nature.","This approach requires the dynamics of the density operator and is thus substantially more costly than approaches employing the Schr\\\"odinger equation for the quantum wave function when several or many nuclear degrees of freedom are involved.","In this work we compare numerically the Lindblad and Schr\\\"odinger descriptions discussed in the literature for a molecular example where the cavity is pumped by a laser.","The laser and cavity properties are varied over a range of parameters.","It is found that the Schr\\\"odinger description adequately describes the dynamics of the polaritons and emission signal as long as the laser intensity is moderate and the pump time is not much longer than the lifetime of the cavity mode.","Otherwise, it is demonstrated that the Schr\\\"odinger description gradually fails.","The results are discussed and analyzed."],"url":"http://arxiv.org/abs/2403.02890v1","category":"physics.chem-ph"}
{"created":"2024-03-05 11:44:14","title":"Revisiting Confidence Estimation: Towards Reliable Failure Prediction","abstract":"Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications. However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and out-of-distribution (OOD) samples from unknown classes. In recent years, many confidence calibration and OOD detection methods have been developed. In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors. We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not. Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failure prediction performance under various settings including balanced, long-tailed, and covariate-shift classification scenarios. Our study not only provides a strong baseline for reliable confidence estimation but also acts as a bridge between understanding calibration, OOD detection, and failure prediction. The code is available at \\url{https://github.com/Impression2805/FMFP}.","sentences":["Reliable confidence estimation is a challenging yet fundamental requirement in many risk-sensitive applications.","However, modern deep neural networks are often overconfident for their incorrect predictions, i.e., misclassified samples from known classes, and out-of-distribution (OOD) samples from unknown classes.","In recent years, many confidence calibration and OOD detection methods have been developed.","In this paper, we find a general, widely existing but actually-neglected phenomenon that most confidence estimation methods are harmful for detecting misclassification errors.","We investigate this problem and reveal that popular calibration and OOD detection methods often lead to worse confidence separation between correctly classified and misclassified examples, making it difficult to decide whether to trust a prediction or not.","Finally, we propose to enlarge the confidence gap by finding flat minima, which yields state-of-the-art failure prediction performance under various settings including balanced, long-tailed, and covariate-shift classification scenarios.","Our study not only provides a strong baseline for reliable confidence estimation but also acts as a bridge between understanding calibration, OOD detection, and failure prediction.","The code is available at \\url{https://github.com/Impression2805/FMFP}."],"url":"http://arxiv.org/abs/2403.02886v1","category":"cs.CV"}
{"created":"2024-03-05 11:39:07","title":"ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving","abstract":"End-to-end differentiable learning for autonomous driving (AD) has recently become a prominent paradigm. One main bottleneck lies in its voracious appetite for high-quality labeled data e.g. 3D bounding boxes and semantic segmentation, which are notoriously expensive to manually annotate. The difficulty is further pronounced due to the prominent fact that the behaviors within samples in AD often suffer from long tailed distribution. In other words, a large part of collected data can be trivial (e.g. simply driving forward in a straight road) and only a few cases are safety-critical. In this paper, we explore a practically important yet under-explored problem about how to achieve sample and label efficiency for end-to-end AD. Specifically, we design a planning-oriented active learning method which progressively annotates part of collected raw data according to the proposed diversity and usefulness criteria for planning routes. Empirically, we show that our planning-oriented approach could outperform general active learning methods by a large margin. Notably, our method achieves comparable performance with state-of-the-art end-to-end AD methods - by using only 30% nuScenes data. We hope our work could inspire future works to explore end-to-end AD from a data-centric perspective in addition to methodology efforts.","sentences":["End-to-end differentiable learning for autonomous driving (AD) has recently become a prominent paradigm.","One main bottleneck lies in its voracious appetite for high-quality labeled data e.g. 3D bounding boxes and semantic segmentation, which are notoriously expensive to manually annotate.","The difficulty is further pronounced due to the prominent fact that the behaviors within samples in AD often suffer from long tailed distribution.","In other words, a large part of collected data can be trivial (e.g. simply driving forward in a straight road) and only a few cases are safety-critical.","In this paper, we explore a practically important yet under-explored problem about how to achieve sample and label efficiency for end-to-end AD.","Specifically, we design a planning-oriented active learning method which progressively annotates part of collected raw data according to the proposed diversity and usefulness criteria for planning routes.","Empirically, we show that our planning-oriented approach could outperform general active learning methods by a large margin.","Notably, our method achieves comparable performance with state-of-the-art end-to-end AD methods - by using only 30% nuScenes data.","We hope our work could inspire future works to explore end-to-end AD from a data-centric perspective in addition to methodology efforts."],"url":"http://arxiv.org/abs/2403.02877v1","category":"cs.CV"}
{"created":"2024-03-05 11:13:04","title":"Spintronic Implementation of UNet for Image Segmentation","abstract":"Image segmentation plays a crucial role in computer vision applications like self-driving cars, satellite imagery analysis, and medical diagnosis. Implementing these complex deep neural networks on conventional hardware is highly inefficient. In this work, we propose hardware implementation of UNet for segmentation tasks, using spintronic devices. Our approach involves designing hardware for convolution, deconvolution, ReLU, and max pooling layers of the UNet architecture. We demonstrate the synaptic behavior of the domain wall MTJ, and design convolution and deconvolution layers using the domain wall-based crossbar array. We utilize the orthogonal current injected MTJ with its continuous resistance change and showcase the ReLU and max pooling functions. We employ a hybrid simulation setup by coupling micromagnetic simulation, non-equilibrium Green's function, Landau-Lifshitz-Gilbert-Slonczewski equations, and circuit simulation with Python programming to incorporate the diverse physics of spin-transport, magnetization dynamics, and CMOS elements in our proposed designs. We evaluate our UNet design on the CamVid dataset and achieve segmentation accuracies that are comparable to software implementation. During training, our design consumes 43.59pJ of energy for synaptic weight updates.","sentences":["Image segmentation plays a crucial role in computer vision applications like self-driving cars, satellite imagery analysis, and medical diagnosis.","Implementing these complex deep neural networks on conventional hardware is highly inefficient.","In this work, we propose hardware implementation of UNet for segmentation tasks, using spintronic devices.","Our approach involves designing hardware for convolution, deconvolution, ReLU, and max pooling layers of the UNet architecture.","We demonstrate the synaptic behavior of the domain wall MTJ, and design convolution and deconvolution layers using the domain wall-based crossbar array.","We utilize the orthogonal current injected MTJ with its continuous resistance change and showcase the ReLU and max pooling functions.","We employ a hybrid simulation setup by coupling micromagnetic simulation, non-equilibrium Green's function, Landau-Lifshitz-Gilbert-Slonczewski equations, and circuit simulation with Python programming to incorporate the diverse physics of spin-transport, magnetization dynamics, and CMOS elements in our proposed designs.","We evaluate our UNet design on the CamVid dataset and achieve segmentation accuracies that are comparable to software implementation.","During training, our design consumes 43.59pJ of energy for synaptic weight updates."],"url":"http://arxiv.org/abs/2403.02863v1","category":"cs.ET"}
{"created":"2024-03-05 11:11:55","title":"Numerical investigation of stabilization in the Hybridizable Discontinuous Galerkin method for linear anisotropic elastic equation","abstract":"This work concerns the implementation of the hybridizable discontinuous Galerkin (HDG) method to solve the linear anisotropic elastic equation in the frequency domain. First-order formulation with the compliance tensor and Voigt notation are employed to provide a compact description of the discretized problem and flexibility with highly heterogeneous media. We further focus on the question of optimal choice of stabilization in the definition of HDG numerical traces. For this purpose, we construct a hybridized Godunov-upwind flux for anisotropic elasticity possessing three distinct wavespeeds. This stabilization removes the need to choose scaling factors, contrary to identity and Kelvin-Christoffel based stabilizations which are popular choices in literature. We carry out comparisons among these families for isotropic and anisotropic material, with constant background and highly heterogeneous ones, in two and three dimensions. They establish the optimality of the Godunov stabilization which can be used as a reference choice for generic material and different types of waves.","sentences":["This work concerns the implementation of the hybridizable discontinuous Galerkin (HDG) method to solve the linear anisotropic elastic equation in the frequency domain.","First-order formulation with the compliance tensor and Voigt notation are employed to provide a compact description of the discretized problem and flexibility with highly heterogeneous media.","We further focus on the question of optimal choice of stabilization in the definition of HDG numerical traces.","For this purpose, we construct a hybridized Godunov-upwind flux for anisotropic elasticity possessing three distinct wavespeeds.","This stabilization removes the need to choose scaling factors, contrary to identity and Kelvin-Christoffel based stabilizations which are popular choices in literature.","We carry out comparisons among these families for isotropic and anisotropic material, with constant background and highly heterogeneous ones, in two and three dimensions.","They establish the optimality of the Godunov stabilization which can be used as a reference choice for generic material and different types of waves."],"url":"http://arxiv.org/abs/2403.02862v1","category":"math.AP"}
{"created":"2024-03-05 18:59:50","title":"How much information can be extracted from galaxy clustering at the field level?","abstract":"We present optimal Bayesian field-level cosmological constraints from nonlinear tracers of the large-scale structure, specifically the amplitude $\\sigma_8$ of linear matter fluctuations inferred from rest-frame simulated dark matter halos in a comoving volume of $8\\,(h^{-1}\\mathrm{Gpc})^3$. Our constraint on $\\sigma_8$ is entirely due to nonlinear information, and obtained by explicitly sampling the initial conditions along with bias and noise parameters via a Lagrangian EFT-based forward model, LEFTfield. The comparison with a simulation-based inference analysis employing the power spectrum and bispectrum likewise using the LEFTfield forward model shows that, when including precisely the same modes of the same data up to $k_{\\mathrm{max}}= 0.10\\,h\\,\\mathrm{Mpc}^{-1}$ ($0.12\\,h\\,\\mathrm{Mpc}^{-1}$), the field-level approach yields a factor of 1.7 (2.6) improvement on the $\\sigma_8$ constraint, from 9.9% to 5.8% (9.4% to 3.6%). This study provides the first direct insights into cosmological information encoded in galaxy clustering beyond low-order $n$-point functions.","sentences":["We present optimal Bayesian field-level cosmological constraints from nonlinear tracers of the large-scale structure, specifically the amplitude $\\sigma_8$ of linear matter fluctuations inferred from rest-frame simulated dark matter halos in a comoving volume of $8\\,(h^{-1}\\mathrm{Gpc})^3$. Our constraint on $\\sigma_8$ is entirely due to nonlinear information, and obtained by explicitly sampling the initial conditions along with bias and noise parameters via a Lagrangian EFT-based forward model, LEFTfield.","The comparison with a simulation-based inference analysis employing the power spectrum and bispectrum likewise using the LEFTfield forward model shows that, when including precisely the same modes of the same data up to $k_{\\mathrm{max}}= 0.10\\,h\\,\\mathrm{Mpc}^{-1}$ ($0.12\\,h\\,\\mathrm{Mpc}^{-1}$), the field-level approach yields a factor of 1.7 (2.6) improvement on the $\\sigma_8$ constraint, from 9.9% to 5.8% (9.4% to 3.6%).","This study provides the first direct insights into cosmological information encoded in galaxy clustering beyond low-order $n$-point functions."],"url":"http://arxiv.org/abs/2403.03220v1","category":"astro-ph.CO"}
{"created":"2024-03-05 18:41:48","title":"Non-Gaussian two mode squeezed thermal states in continuous variable quantum teleportation","abstract":"While photon catalyzed two mode squeezed vacuum state has been considered in context of quantum teleportation, similar studies have not been yet conducted for photon catalyzed two-mode squeezed thermal (TMST) state. This can be attributed to challenges involved in the evaluation of teleportation fidelity for photon catalyzed TMST state. In this article, we consider a practical scheme for the implementation of non-Gaussian operation, viz., photon subtraction, photon addition, and photon catalysis, on TMST state. The generated states are employed as resources in continuous-variable quantum teleportation. The results show that the three non-Gaussian operations can enhance the teleportation fidelity. Considering the success probability of the non-Gaussian operations, we identify single-photon catalysis and single photon subtraction to be optimal for teleporting input coherent states, at low and intermediate squeezing levels.","sentences":["While photon catalyzed two mode squeezed vacuum state has been considered in context of quantum teleportation, similar studies have not been yet conducted for photon catalyzed two-mode squeezed thermal (TMST) state.","This can be attributed to challenges involved in the evaluation of teleportation fidelity for photon catalyzed TMST state.","In this article, we consider a practical scheme for the implementation of non-Gaussian operation, viz., photon subtraction, photon addition, and photon catalysis, on TMST state.","The generated states are employed as resources in continuous-variable quantum teleportation.","The results show that the three non-Gaussian operations can enhance the teleportation fidelity.","Considering the success probability of the non-Gaussian operations, we identify single-photon catalysis and single photon subtraction to be optimal for teleporting input coherent states, at low and intermediate squeezing levels."],"url":"http://arxiv.org/abs/2403.03204v1","category":"quant-ph"}
{"created":"2024-03-05 18:39:01","title":"Quantum superpositions of current states in Rydberg-atom networks","abstract":"Quantum simulation of many-body quantum systems using Rydberg-atom platforms has become of extreme interest in the last years. The possibility to realize spin Hamiltonians and the accurate control at the single atom level paved the way for the study of quantum phases of matter and dynamics. Here, we propose a quantum optimal control protocol based on the GRAPE algorithm to engineer quantum current states. Besides current states characterized by a single winding number, our approach allow to access superposition of quantum current states. The single current states are eigenstates of the current operator that therefore can define an observable that remains persistent at any time. In particular, the features of the excitations dynamics reflects the nature of current states, a fact that in principle can be used to characterize the nature of the flow experimentally.","sentences":["Quantum simulation of many-body quantum systems using Rydberg-atom platforms has become of extreme interest in the last years.","The possibility to realize spin Hamiltonians and the accurate control at the single atom level paved the way for the study of quantum phases of matter and dynamics.","Here, we propose a quantum optimal control protocol based on the GRAPE algorithm to engineer quantum current states.","Besides current states characterized by a single winding number, our approach allow to access superposition of quantum current states.","The single current states are eigenstates of the current operator that therefore can define an observable that remains persistent at any time.","In particular, the features of the excitations dynamics reflects the nature of current states, a fact that in principle can be used to characterize the nature of the flow experimentally."],"url":"http://arxiv.org/abs/2403.03202v1","category":"quant-ph"}
{"created":"2024-03-05 18:19:02","title":"Shuffling Momentum Gradient Algorithm for Convex Optimization","abstract":"The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number of training epochs. Our analysis is a state-of-the-art, matching the best rates of existing shuffling stochastic gradient algorithms in the literature.","sentences":["The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets.","In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants.","However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings.","In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems.","We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number of training epochs.","Our analysis is a state-of-the-art, matching the best rates of existing shuffling stochastic gradient algorithms in the literature."],"url":"http://arxiv.org/abs/2403.03180v1","category":"math.OC"}
{"created":"2024-03-05 17:46:04","title":"Solving non-native combinatorial optimization problems using hybrid quantum-classical algorithms","abstract":"Combinatorial optimization is a challenging problem applicable in a wide range of fields from logistics to finance. Recently, quantum computing has been used to attempt to solve these problems using a range of algorithms, including parameterized quantum circuits, adiabatic protocols, and quantum annealing. These solutions typically have several challenges: 1) there is little to no performance gain over classical methods, 2) not all constraints and objectives may be efficiently encoded in the quantum ansatz, and 3) the solution domain of the objective function may not be the same as the bit strings of measurement outcomes. This work presents \"non-native hybrid algorithms\" (NNHA): a framework to overcome these challenges by integrating quantum and classical resources with a hybrid approach. By designing non-native quantum variational ansatzes that inherit some but not all problem structure, measurement outcomes from the quantum computer can act as a resource to be used by classical routines to indirectly compute optimal solutions, partially overcoming the challenges of contemporary quantum optimization approaches. These methods are demonstrated using a publicly available neutral-atom quantum computer on two simple problems of Max $k$-Cut and maximum independent set. We find improvements in solution quality when comparing the hybrid algorithm to its ``no quantum\" version, a demonstration of a \"comparative advantage\".","sentences":["Combinatorial optimization is a challenging problem applicable in a wide range of fields from logistics to finance.","Recently, quantum computing has been used to attempt to solve these problems using a range of algorithms, including parameterized quantum circuits, adiabatic protocols, and quantum annealing.","These solutions typically have several challenges: 1) there is little to no performance gain over classical methods, 2) not all constraints and objectives may be efficiently encoded in the quantum ansatz, and 3) the solution domain of the objective function may not be the same as the bit strings of measurement outcomes.","This work presents \"non-native hybrid algorithms\" (NNHA): a framework to overcome these challenges by integrating quantum and classical resources with a hybrid approach.","By designing non-native quantum variational ansatzes that inherit some but not all problem structure, measurement outcomes from the quantum computer can act as a resource to be used by classical routines to indirectly compute optimal solutions, partially overcoming the challenges of contemporary quantum optimization approaches.","These methods are demonstrated using a publicly available neutral-atom quantum computer on two simple problems of Max $k$-Cut and maximum independent set.","We find improvements in solution quality when comparing the hybrid algorithm to its ``no quantum\" version, a demonstration of a \"comparative advantage\"."],"url":"http://arxiv.org/abs/2403.03153v1","category":"quant-ph"}
{"created":"2024-03-05 17:23:52","title":"A novel methodological framework for the analysis of health trajectories and survival outcomes in heart failure patients","abstract":"Heart failure (HF) contributes to circa 200,000 annual hospitalizations in France. With the increasing age of HF patients, elucidating the specific causes of inpatient mortality became a public health problematic. We introduce a novel methodological framework designed to identify prevalent health trajectories and investigate their impact on death. The initial step involves applying sequential pattern mining to characterize patients' trajectories, followed by an unsupervised clustering algorithm based on a new metric for measuring the distance between hospitalization diagnoses. Finally, a survival analysis is conducted to assess survival outcomes. The application of this framework to HF patients from a representative sample of the French population demonstrates its methodological significance in enhancing the analysis of healthcare trajectories.","sentences":["Heart failure (HF) contributes to circa 200,000 annual hospitalizations in France.","With the increasing age of HF patients, elucidating the specific causes of inpatient mortality became a public health problematic.","We introduce a novel methodological framework designed to identify prevalent health trajectories and investigate their impact on death.","The initial step involves applying sequential pattern mining to characterize patients' trajectories, followed by an unsupervised clustering algorithm based on a new metric for measuring the distance between hospitalization diagnoses.","Finally, a survival analysis is conducted to assess survival outcomes.","The application of this framework to HF patients from a representative sample of the French population demonstrates its methodological significance in enhancing the analysis of healthcare trajectories."],"url":"http://arxiv.org/abs/2403.03138v1","category":"stat.ME"}
{"created":"2024-03-05 17:15:40","title":"A Comprehensive Stochastic Programming Model for Transfer Synchronization in Transit Networks","abstract":"We investigate the stochastic transfer synchronization problem, which seeks to synchronize the timetables of different routes in a transit network to reduce transfer waiting times, delay times, and unnecessary in-vehicle times. We present a sophisticated two-stage stochastic mixed-integer programming model that takes into account variability in passenger walking times between bus stops, bus running times, dwell times, and demand uncertainty. Our model incorporates new features related to dwell time determination by considering passenger arrival patterns at bus stops which have been neglected in the literature on transfer synchronization and timetabling. We solve a sample average approximation of our model using a problem-based scenario reduction approach, and the progressive hedging algorithm. As a proof of concept, our computational experiments on two single transfer nodes in the City of Toronto, with a mixture of low- and high-frequency routes, demonstrate the potential advantages of the proposed model. Our findings highlight the necessity and value of incorporating stochasticity in transfer-based timetabling models.","sentences":["We investigate the stochastic transfer synchronization problem, which seeks to synchronize the timetables of different routes in a transit network to reduce transfer waiting times, delay times, and unnecessary in-vehicle times.","We present a sophisticated two-stage stochastic mixed-integer programming model that takes into account variability in passenger walking times between bus stops, bus running times, dwell times, and demand uncertainty.","Our model incorporates new features related to dwell time determination by considering passenger arrival patterns at bus stops which have been neglected in the literature on transfer synchronization and timetabling.","We solve a sample average approximation of our model using a problem-based scenario reduction approach, and the progressive hedging algorithm.","As a proof of concept, our computational experiments on two single transfer nodes in the City of Toronto, with a mixture of low- and high-frequency routes, demonstrate the potential advantages of the proposed model.","Our findings highlight the necessity and value of incorporating stochasticity in transfer-based timetabling models."],"url":"http://arxiv.org/abs/2403.03130v1","category":"math.OC"}
{"created":"2024-03-05 16:20:11","title":"Shear-enhanced Liquid Crystal Spinning of Conjugated Polymer Fibers","abstract":"Conjugated polymer fibers can be used to manufacture various soft fibrous optoelectronic devices, significantly advancing wearable devices and smart textiles. Recently, conjugated polymer-based fibrous electronic devices have been widely used in energy conversion, electrochemical sensing, and human-machine interaction. However, the insufficient mechanical properties of conjugated polymer fibers, the difficulty in solution processing semiconductors with rigid main chains, and the challenges in large-scale continuous production have limited their further development in the wearable field. We regulated the pi - pi stacking interactions in conjugated polymer molecules below their critical liquid crystal concentration by applying fluid shear stress. We implemented secondary orientation, leading to the continuous fabrication of anisotropic semiconductor fibers. This strategy enables conjugated polymers with rigid backbones to synergistically enhance the mechanical and semiconductor properties of fibers through liquid crystal spinning. Furthermore, conjugated polymer fibers, exhibiting excellent electrochemical performance and high mechanical strength (600 MPa) that essentially meet the requirements for industrialized preparation, maintain stability under extreme temperatures, radiation, and chemical reagents. Lastly, we have demonstrated logic circuits using semiconductor fiber organic electrochemical transistors, showcasing its application potential in the field of wearable fabric-style logic processing. These findings confirm the importance of the liquid crystalline state and solution control in optimizing the performance of conjugated polymer fibers, thus paving the way for developing a new generation of soft fiber semiconductor devices.","sentences":["Conjugated polymer fibers can be used to manufacture various soft fibrous optoelectronic devices, significantly advancing wearable devices and smart textiles.","Recently, conjugated polymer-based fibrous electronic devices have been widely used in energy conversion, electrochemical sensing, and human-machine interaction.","However, the insufficient mechanical properties of conjugated polymer fibers, the difficulty in solution processing semiconductors with rigid main chains, and the challenges in large-scale continuous production have limited their further development in the wearable field.","We regulated the pi - pi stacking interactions in conjugated polymer molecules below their critical liquid crystal concentration by applying fluid shear stress.","We implemented secondary orientation, leading to the continuous fabrication of anisotropic semiconductor fibers.","This strategy enables conjugated polymers with rigid backbones to synergistically enhance the mechanical and semiconductor properties of fibers through liquid crystal spinning.","Furthermore, conjugated polymer fibers, exhibiting excellent electrochemical performance and high mechanical strength (600 MPa) that essentially meet the requirements for industrialized preparation, maintain stability under extreme temperatures, radiation, and chemical reagents.","Lastly, we have demonstrated logic circuits using semiconductor fiber organic electrochemical transistors, showcasing its application potential in the field of wearable fabric-style logic processing.","These findings confirm the importance of the liquid crystalline state and solution control in optimizing the performance of conjugated polymer fibers, thus paving the way for developing a new generation of soft fiber semiconductor devices."],"url":"http://arxiv.org/abs/2403.03088v2","category":"cond-mat.soft"}
{"created":"2024-03-05 16:05:02","title":"Demonstrating efficient and robust bosonic state reconstruction via optimized excitation counting","abstract":"Quantum state reconstruction is an essential element in quantum information processing. However, efficient and reliable reconstruction of non-trivial quantum states in the presence of hardware imperfections can be challenging. This task is particularly demanding for high-dimensional states encoded in continuous-variable (CV) systems, as many error-prone measurements are needed to cover the relevant degrees of freedom of the system in phase space. In this work, we introduce an efficient and robust technique for optimized reconstruction based on excitation number sampling (ORENS). We use a standard bosonic circuit quantum electrodynamics (cQED) setup to experimentally demonstrate the robustness of ORENS and show that it outperforms the existing cQED reconstruction techniques such as Wigner tomography and Husimi Q-function. Our investigation highlights that ORENS is naturally free of parasitic system dynamics and resilient to decoherence effects in the hardware. Finally, ORENS relies only on the ability to accurately measure the excitation number of the state, making it a versatile and accessible tool for a wide range of CV platforms and readily scalable to multimode systems. Thus, our work provides a crucial and valuable primitive for practical quantum information processing using bosonic modes.","sentences":["Quantum state reconstruction is an essential element in quantum information processing.","However, efficient and reliable reconstruction of non-trivial quantum states in the presence of hardware imperfections can be challenging.","This task is particularly demanding for high-dimensional states encoded in continuous-variable (CV) systems, as many error-prone measurements are needed to cover the relevant degrees of freedom of the system in phase space.","In this work, we introduce an efficient and robust technique for optimized reconstruction based on excitation number sampling (ORENS).","We use a standard bosonic circuit quantum electrodynamics (cQED) setup to experimentally demonstrate the robustness of ORENS and show that it outperforms the existing cQED reconstruction techniques such as Wigner tomography and Husimi Q-function.","Our investigation highlights that ORENS is naturally free of parasitic system dynamics and resilient to decoherence effects in the hardware.","Finally, ORENS relies only on the ability to accurately measure the excitation number of the state, making it a versatile and accessible tool for a wide range of CV platforms and readily scalable to multimode systems.","Thus, our work provides a crucial and valuable primitive for practical quantum information processing using bosonic modes."],"url":"http://arxiv.org/abs/2403.03080v2","category":"quant-ph"}
{"created":"2024-03-05 15:56:32","title":"Tracking-in-range Formulations for Numerical Optimal Control","abstract":"In contrast to set-point tracking which aims to reduce the tracking error between the tracker and the reference, tracking-in-range problems only focus on whether the tracker is within a given range around the reference, making it more suitable for the mission specifications of many practical applications. In this work, we present novel optimal control formulations to solve tracking-in-range problems, for both problems requiring the tracker to be always in range, and problems allowing the tracker to go out of range to yield overall better outcomes. As the problem naturally involves discontinuous functions, we present alternative formulations and regularisation strategies to improve the performance of numerical solvers. The extension to in-range tracking with multiple trackers and in-range tracking in high dimensional space are also discussed and illustrated with numerical examples, demonstrating substantial increases in mission duration in comparison to traditional set-point tracking.","sentences":["In contrast to set-point tracking which aims to reduce the tracking error between the tracker and the reference, tracking-in-range problems only focus on whether the tracker is within a given range around the reference, making it more suitable for the mission specifications of many practical applications.","In this work, we present novel optimal control formulations to solve tracking-in-range problems, for both problems requiring the tracker to be always in range, and problems allowing the tracker to go out of range to yield overall better outcomes.","As the problem naturally involves discontinuous functions, we present alternative formulations and regularisation strategies to improve the performance of numerical solvers.","The extension to in-range tracking with multiple trackers and in-range tracking in high dimensional space are also discussed and illustrated with numerical examples, demonstrating substantial increases in mission duration in comparison to traditional set-point tracking."],"url":"http://arxiv.org/abs/2403.03066v1","category":"eess.SY"}
{"created":"2024-03-05 15:38:54","title":"Distributed Policy Gradient for Linear Quadratic Networked Control with Limited Communication Range","abstract":"This paper proposes a scalable distributed policy gradient method and proves its convergence to near-optimal solution in multi-agent linear quadratic networked systems. The agents engage within a specified network under local communication constraints, implying that each agent can only exchange information with a limited number of neighboring agents. On the underlying graph of the network, each agent implements its control input depending on its nearby neighbors' states in the linear quadratic control setting. We show that it is possible to approximate the exact gradient only using local information. Compared with the centralized optimal controller, the performance gap decreases to zero exponentially as the communication and control ranges increase. We also demonstrate how increasing the communication range enhances system stability in the gradient descent process, thereby elucidating a critical trade-off. The simulation results verify our theoretical findings.","sentences":["This paper proposes a scalable distributed policy gradient method and proves its convergence to near-optimal solution in multi-agent linear quadratic networked systems.","The agents engage within a specified network under local communication constraints, implying that each agent can only exchange information with a limited number of neighboring agents.","On the underlying graph of the network, each agent implements its control input depending on its nearby neighbors' states in the linear quadratic control setting.","We show that it is possible to approximate the exact gradient only using local information.","Compared with the centralized optimal controller, the performance gap decreases to zero exponentially as the communication and control ranges increase.","We also demonstrate how increasing the communication range enhances system stability in the gradient descent process, thereby elucidating a critical trade-off.","The simulation results verify our theoretical findings."],"url":"http://arxiv.org/abs/2403.03055v1","category":"cs.MA"}
{"created":"2024-03-05 15:30:24","title":"The Exchange Problem","abstract":"Auctions are widely used in exchanges to match buy and sell requests. Once the buyers and sellers place their requests, the exchange determines how these requests are to be matched. The two most popular objectives used while determining the matching are maximizing volume at a uniform price and maximizing volume with dynamic pricing. In this work, we study the algorithmic complexity of the problems arising from these matching tasks.   We present a linear time algorithm for uniform price matching which is an improvement over the previous algorithms that take $O(n\\log n)$ time to match $n$ requests. For dynamic price matching, we establish a lower bound of $\\Omega(n \\log n)$ on the running time, thereby proving that the currently known best algorithm is time-optimal.","sentences":["Auctions are widely used in exchanges to match buy and sell requests.","Once the buyers and sellers place their requests, the exchange determines how these requests are to be matched.","The two most popular objectives used while determining the matching are maximizing volume at a uniform price and maximizing volume with dynamic pricing.","In this work, we study the algorithmic complexity of the problems arising from these matching tasks.   ","We present a linear time algorithm for uniform price matching which is an improvement over the previous algorithms that take $O(n\\log n)$ time to match $n$ requests.","For dynamic price matching, we establish a lower bound of $\\Omega(n \\log n)$ on the running time, thereby proving that the currently known best algorithm is time-optimal."],"url":"http://arxiv.org/abs/2403.03046v1","category":"cs.DS"}
{"created":"2024-03-05 15:19:53","title":"A hybrid optimization framework for the General Continuous Energy-Constrained Scheduling Problem","abstract":"We present a hybrid optimization framework for a class of problems, formalized as a generalization of the Continuous Energy-Con\\-strained Scheduling Problem (CECSP), introduced by Nattaf et al. (2014). This class is obtained from challenges concerning demand response in energy networks. Our framework extends a previously developed approach. A set of jobs has to be processed on a continuous, shared resource. Consequently, a schedule for a job does not only contain a start and completion time, but also a resource consumption profile, where we have to respect lower and upper bounds on resource consumption during processing. In this work, we develop a hybrid approach for the case where the objective is a step-wise increasing function of completion time, using local search, linear programming and O(n) lower bounds. We exploit that the costs are known in the local search and use bounds to assess feasibility more efficiently than by LP. We compare its performance to a mixed-integer linear program. After that, we extend this to a hybrid optimization framework for the General CECSP. This uses an event-based model, and applies a decomposition in two parts: 1) determining the order of events and 2) finding the event times, and hence the start and completion times of jobs, together with the resource consumption profiles. We argue the broad applicability of this framework.","sentences":["We present a hybrid optimization framework for a class of problems, formalized as a generalization of the Continuous Energy-Con\\-strained Scheduling Problem (CECSP), introduced by Nattaf et al. (2014).","This class is obtained from challenges concerning demand response in energy networks.","Our framework extends a previously developed approach.","A set of jobs has to be processed on a continuous, shared resource.","Consequently, a schedule for a job does not only contain a start and completion time, but also a resource consumption profile, where we have to respect lower and upper bounds on resource consumption during processing.","In this work, we develop a hybrid approach for the case where the objective is a step-wise increasing function of completion time, using local search, linear programming and O(n) lower bounds.","We exploit that the costs are known in the local search and use bounds to assess feasibility more efficiently than by LP.","We compare its performance to a mixed-integer linear program.","After that, we extend this to a hybrid optimization framework for the General CECSP.","This uses an event-based model, and applies a decomposition in two parts: 1) determining the order of events and 2) finding the event times, and hence the start and completion times of jobs, together with the resource consumption profiles.","We argue the broad applicability of this framework."],"url":"http://arxiv.org/abs/2403.03039v1","category":"math.OC"}
{"created":"2024-03-05 14:20:24","title":"SLICK: Strong Lensing Identification of Candidates Kindred in gravitational wave data","abstract":"By the end of the next decade, we hope to have detected strongly lensed gravitational waves by galaxies or clusters. Although there exist optimal methods for identifying lensed signal, it is shown that machine learning (ML) algorithms can give comparable performance but are orders of magnitude faster than non-ML methods. We present the SLICK pipeline which comprises a parallel network based on deep learning. We analyse the Q-transform maps (QT maps) and the Sine-Gaussian maps (SGP-maps) generated for the binary black hole signals injected in Gaussian as well as real noise. We compare our network performance with the previous work and find that the efficiency of our model is higher by a factor of 5 at a false positive rate of 0.001. Further, we show that including SGP maps with QT maps data results in a better performance than analysing QT maps alone. When combined with sky localisation constraints, we hope to get unprecedented accuracy in the predictions than previously possible. We also evaluate our model on the real events detected by the LIGO--Virgo collaboration and find that our network correctly classifies all of them, consistent with non-detection of lensing.","sentences":["By the end of the next decade, we hope to have detected strongly lensed gravitational waves by galaxies or clusters.","Although there exist optimal methods for identifying lensed signal, it is shown that machine learning (ML) algorithms can give comparable performance but are orders of magnitude faster than non-ML methods.","We present the SLICK pipeline which comprises a parallel network based on deep learning.","We analyse the Q-transform maps (QT maps) and the Sine-Gaussian maps (SGP-maps) generated for the binary black hole signals injected in Gaussian as well as real noise.","We compare our network performance with the previous work and find that the efficiency of our model is higher by a factor of 5 at a false positive rate of 0.001.","Further, we show that including SGP maps with QT maps data results in a better performance than analysing QT maps alone.","When combined with sky localisation constraints, we hope to get unprecedented accuracy in the predictions than previously possible.","We also evaluate our model on the real events detected by the LIGO--Virgo collaboration and find that our network correctly classifies all of them, consistent with non-detection of lensing."],"url":"http://arxiv.org/abs/2403.02994v1","category":"astro-ph.HE"}
{"created":"2024-03-05 14:07:37","title":"Operational Nonclassicality in Quantum Communication Networks","abstract":"To quantify quantum advantage in communication networks, we apply an operational framework for witnessing quantum nonclassicality. Following previous approaches in the field, this framework first computes linear constraints on the input/output probabilities that arise in classical networks when the amount of communication is bounded. We then apply variational quantum algorithms to optimize these probabilities when quantum communication resources are introduced. Any violation of the classical constraints indicates that extra classical communication is needed to simulate the comparable quantum network, thereby demonstrating an explicit quantum advantage. We demonstrate nonclassicality in many basic networks such as entanglement-assisted point-to-point and multi-point channels. In all examples, we find that equipping classical or quantum channels with entanglement leads to nonclassicality, whereas networks having multiple senders do not require entanglement to achieve nonclassicality. Finally, we discuss how our approaches could be implemented on quantum networking hardware and used to automatically establish certain protocols.","sentences":["To quantify quantum advantage in communication networks, we apply an operational framework for witnessing quantum nonclassicality.","Following previous approaches in the field, this framework first computes linear constraints on the input/output probabilities that arise in classical networks when the amount of communication is bounded.","We then apply variational quantum algorithms to optimize these probabilities when quantum communication resources are introduced.","Any violation of the classical constraints indicates that extra classical communication is needed to simulate the comparable quantum network, thereby demonstrating an explicit quantum advantage.","We demonstrate nonclassicality in many basic networks such as entanglement-assisted point-to-point and multi-point channels.","In all examples, we find that equipping classical or quantum channels with entanglement leads to nonclassicality, whereas networks having multiple senders do not require entanglement to achieve nonclassicality.","Finally, we discuss how our approaches could be implemented on quantum networking hardware and used to automatically establish certain protocols."],"url":"http://arxiv.org/abs/2403.02988v1","category":"quant-ph"}
{"created":"2024-03-05 13:52:48","title":"Model Predictive Control for setpoint tracking","abstract":"The main objective of tracking control is to steer the tracking error, that is the difference between the reference and the output, to zero while the plant's operation limits are satisfied. This requires that some assumptions on the evolution of the future values of the reference must be taken into account. Typically a simple evolution of the reference is considered, such as step, ramp, or parabolic reference signals. It is important to notice that the tracking problem considers possible variations in the reference to be tracked, such as steps or slope variations of the ramps. Then the tracking control problem is inherently uncertain, since the reference may differ from what is expected. If the value of the reference is changed, then there is no guarantee that the feasibility and stability properties of the resulting control law hold. This report presents the MPC for tracking (MPCT) approach, which ensures recursive feasibility and asymptotic stability of the setpoint when the value of the reference is changed.","sentences":["The main objective of tracking control is to steer the tracking error, that is the difference between the reference and the output, to zero while the plant's operation limits are satisfied.","This requires that some assumptions on the evolution of the future values of the reference must be taken into account.","Typically a simple evolution of the reference is considered, such as step, ramp, or parabolic reference signals.","It is important to notice that the tracking problem considers possible variations in the reference to be tracked, such as steps or slope variations of the ramps.","Then the tracking control problem is inherently uncertain, since the reference may differ from what is expected.","If the value of the reference is changed, then there is no guarantee that the feasibility and stability properties of the resulting control law hold.","This report presents the MPC for tracking (MPCT) approach, which ensures recursive feasibility and asymptotic stability of the setpoint when the value of the reference is changed."],"url":"http://arxiv.org/abs/2403.02973v1","category":"math.OC"}
{"created":"2024-03-05 13:49:32","title":"Space Complexity of Euclidean Clustering","abstract":"The $(k, z)$-Clustering problem in Euclidean space $\\mathbb{R}^d$ has been extensively studied. Given the scale of data involved, compression methods for the Euclidean $(k, z)$-Clustering problem, such as data compression and dimension reduction, have received significant attention in the literature. However, the space complexity of the clustering problem, specifically, the number of bits required to compress the cost function within a multiplicative error $\\varepsilon$, remains unclear in existing literature.   This paper initiates the study of space complexity for Euclidean $(k, z)$-Clustering and offers both upper and lower bounds. Our space bounds are nearly tight when $k$ is constant, indicating that storing a coreset, a well-known data compression approach, serves as the optimal compression scheme. Furthermore, our lower bound result for $(k, z)$-Clustering establishes a tight space bound of $\\Theta( n d )$ for terminal embedding, where $n$ represents the dataset size. Our technical approach leverages new geometric insights for principal angles and discrepancy methods, which may hold independent interest.","sentences":["The $(k, z)$-Clustering problem in Euclidean space $\\mathbb{R}^d$ has been extensively studied.","Given the scale of data involved, compression methods for the Euclidean $(k, z)$-Clustering problem, such as data compression and dimension reduction, have received significant attention in the literature.","However, the space complexity of the clustering problem, specifically, the number of bits required to compress the cost function within a multiplicative error $\\varepsilon$, remains unclear in existing literature.   ","This paper initiates the study of space complexity for Euclidean $(k, z)$-Clustering and offers both upper and lower bounds.","Our space bounds are nearly tight when $k$ is constant, indicating that storing a coreset, a well-known data compression approach, serves as the optimal compression scheme.","Furthermore, our lower bound result for $(k, z)$-Clustering establishes a tight space bound of $\\Theta( n d )$ for terminal embedding, where $n$ represents the dataset size.","Our technical approach leverages new geometric insights for principal angles and discrepancy methods, which may hold independent interest."],"url":"http://arxiv.org/abs/2403.02971v2","category":"cs.CG"}
{"created":"2024-03-05 13:43:58","title":"Non-Convex Stochastic Composite Optimization with Polyak Momentum","abstract":"The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.","sentences":["The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning.","However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used).","In this paper, we focus on the stochastic proximal gradient method with Polyak momentum.","We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size.","Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly.","Finally, we provide numerical experiments to validate our theoretical results."],"url":"http://arxiv.org/abs/2403.02967v1","category":"math.OC"}
{"created":"2024-03-05 13:30:55","title":"Regret-based budgeted decision rules under severe uncertainty","abstract":"One way to make decisions under uncertainty is to select an optimal option from a possible range of options, by maximizing the expected utilities derived from a probability model. However, under severe uncertainty, identifying precise probabilities is hard. For this reason, imprecise probability models uncertainty through convex sets of probabilities, and considers decision rules that can return multiple options to reflect insufficient information. Many well-founded decision rules have been studied in the past, but none of those standard rules are able to control the number of returned alternatives. This can be a problem for large decision problems, due to the cognitive burden decision makers have to face when presented with a large number of alternatives. Our contribution proposes regret-based ideas to construct new decision rules which return a bounded number of options, where the limit on the number of options is set in advance by the decision maker as an expression of their cognitive limitation. We also study their consistency and numerical behaviour.","sentences":["One way to make decisions under uncertainty is to select an optimal option from a possible range of options, by maximizing the expected utilities derived from a probability model.","However, under severe uncertainty, identifying precise probabilities is hard.","For this reason, imprecise probability models uncertainty through convex sets of probabilities, and considers decision rules that can return multiple options to reflect insufficient information.","Many well-founded decision rules have been studied in the past, but none of those standard rules are able to control the number of returned alternatives.","This can be a problem for large decision problems, due to the cognitive burden decision makers have to face when presented with a large number of alternatives.","Our contribution proposes regret-based ideas to construct new decision rules which return a bounded number of options, where the limit on the number of options is set in advance by the decision maker as an expression of their cognitive limitation.","We also study their consistency and numerical behaviour."],"url":"http://arxiv.org/abs/2403.02960v1","category":"math.ST"}
{"created":"2024-03-05 13:25:44","title":"On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models","abstract":"Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.","sentences":["Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks.","Despite their practical utility, there is a notable gap in their theoretical understanding.","This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps.","The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training.","We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off.","The theoretical findings are validated by numerical results."],"url":"http://arxiv.org/abs/2403.02957v1","category":"cs.LG"}
{"created":"2024-03-05 13:17:54","title":"Phase Behavior and Dynamics of Active Brownian Particles in an Alignment Field","abstract":"Self-propelled particles that are subject to noise are a well-established generic model system for active matter. A homogeneous alignment field can be used to orient the direction of the self-propulsion velocity and to model systems like phoretic Janus particles with a magnetic dipole moment or magnetotactic bacteria in an external magnetic field. Computer simulations are used to predict the phase behavior and dynamics of self-propelled Brownian particles in a homogeneous alignment field in two dimensions. Phase boundaries of the gas-liquid coexistence region are calculated for various P\\'eclet numbers, particle densities, and alignment field strengths. Critical points and exponents are calculated and, in agreement with previous simulations, do not seem to belong to the universality class of the 2D Ising model. Finally, the dynamics of spinodal decomposition for quenching the system from the one-phase to the two-phase coexistence region by increasing P\\'eclet number is characterized. Our results may help to identify parameters for optimal transport of active matter in complex environments.","sentences":["Self-propelled particles that are subject to noise are a well-established generic model system for active matter.","A homogeneous alignment field can be used to orient the direction of the self-propulsion velocity and to model systems like phoretic Janus particles with a magnetic dipole moment or magnetotactic bacteria in an external magnetic field.","Computer simulations are used to predict the phase behavior and dynamics of self-propelled Brownian particles in a homogeneous alignment field in two dimensions.","Phase boundaries of the gas-liquid coexistence region are calculated for various P\\'eclet numbers, particle densities, and alignment field strengths.","Critical points and exponents are calculated and, in agreement with previous simulations, do not seem to belong to the universality class of the 2D Ising model.","Finally, the dynamics of spinodal decomposition for quenching the system from the one-phase to the two-phase coexistence region by increasing P\\'eclet number is characterized.","Our results may help to identify parameters for optimal transport of active matter in complex environments."],"url":"http://arxiv.org/abs/2403.02947v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-05 12:03:10","title":"A\u03b1-spectral radius and path-factor covered graphs","abstract":"Let $\\alpha\\in[0,1)$, and let $G$ be a connected graph of order $n$ with $n\\geq f(\\alpha)$, where $f(\\alpha)=14$ for $\\alpha\\in[0,\\frac{1}{2}]$, $f(\\alpha)=17$ for $\\alpha\\in(\\frac{1}{2},\\frac{2}{3}]$, $f(\\alpha)=20$ for $\\alpha\\in(\\frac{2}{3},\\frac{3}{4}]$ and $f(\\alpha)=\\frac{5}{1-\\alpha}+1$ for $\\alpha\\in(\\frac{3}{4},1)$. A path factor is a spanning subgraph $F$ of $G$ such that every component of $F$ is a path with at least two vertices. Let $k\\geq2$ be an integer. A $P_{\\geq k}$-factor means a path-factor with each component being a path of order at least $k$. A graph $G$ is called a $P_{\\geq k}$-factor covered graph if $G$ has a $P_{\\geq k}$-factor containing $e$ for any $e\\in E(G)$. Let $A_{\\alpha}(G)=\\alpha D(G)+(1-\\alpha)A(G)$, where $D(G)$ denotes the diagonal matrix of vertex degrees of $G$ and $A(G)$ denotes the adjacency matrix of $G$. The largest eigenvalue of $A_{\\alpha}(G)$ is called the $A_{\\alpha}$-spectral radius of $G$, which is denoted by $\\rho_{\\alpha}(G)$. In this paper, it is proved that $G$ is a $P_{\\geq2}$-factor covered graph if $\\rho_{\\alpha}(G)>\\eta(n)$, where $\\eta(n)$ is the largest root of $x^{3}-((\\alpha+1)n+\\alpha-4)x^{2}+(\\alpha n^{2}+(\\alpha^{2}-2\\alpha-1)n-2\\alpha+1)x-\\alpha^{2}n^{2}+(5\\alpha^{2}-3\\alpha+2)n-10\\alpha^{2}+15\\alpha-8=0$. Furthermore, we provide a graph to show that the bound on $A_{\\alpha}$-spectral radius is optimal.","sentences":["Let $\\alpha\\in[0,1)$, and let $G$ be a connected graph of order $n$ with $n\\geq f(\\alpha)$, where $f(\\alpha)=14$ for $\\alpha\\in[0,\\frac{1}{2}]$, $f(\\alpha)=17$ for $\\alpha\\in(\\frac{1}{2},\\frac{2}{3}]$, $f(\\alpha)=20$ for $\\alpha\\in(\\frac{2}{3},\\frac{3}{4}]$ and $f(\\alpha)=\\frac{5}{1-\\alpha}+1$ for $\\alpha\\in(\\frac{3}{4},1)$. A path factor is a spanning subgraph $F$ of $G$ such that every component of $F$ is a path with at least two vertices.","Let $k\\geq2$ be an integer.","A $P_{\\geq k}$-factor means a path-factor with each component being a path of order at least $k$. A graph $G$ is called a $P_{\\geq k}$-factor covered graph if $G$ has a $P_{\\geq k}$-factor containing $e$ for any $e\\in E(G)$. Let $A_{\\alpha}(G)=\\alpha D(G)+(1-\\alpha)A(G)$, where $D(G)$ denotes the diagonal matrix of vertex degrees of $G$ and $A(G)$ denotes the adjacency matrix of $G$. The largest eigenvalue of $A_{\\alpha}(G)$ is called the $A_{\\alpha}$-spectral radius of $G$, which is denoted by $\\rho_{\\alpha}(G)$. In this paper, it is proved that $G$ is a $P_{\\geq2}$-factor covered graph if $\\rho_{\\alpha}(G)>\\eta(n)$, where $\\eta(n)$ is the largest root of $x^{3}-((\\alpha+1)n+\\alpha-4)x^{2}+(\\alpha n^{2}+(\\alpha^{2}-2\\alpha-1)n-2\\alpha+1)x-\\alpha^{2}n^{2}+(5\\alpha^{2}-3\\alpha+2)n-10\\alpha^{2}+15\\alpha-8=0$.","Furthermore, we provide a graph to show that the bound on $A_{\\alpha}$-spectral radius is optimal."],"url":"http://arxiv.org/abs/2403.02896v1","category":"math.CO"}
{"created":"2024-03-05 11:55:10","title":"Design of full order proportional-integral observer for the state estimation of discrete-time linear time-invariant systems","abstract":"This paper is devoted to the design of full order proportional-integral observer for the state estimation of discrete-time linear time-invariant systems. In particular, explicit necessary and sufficient conditions are established for the existence of proportional-integral observer for the state estimation of discrete-time linear time-invariant systems and a simple procedure is given for the construction of the observer. Our approach is based on properties of real and polynomial matrices.","sentences":["This paper is devoted to the design of full order proportional-integral observer for the state estimation of discrete-time linear time-invariant systems.","In particular, explicit necessary and sufficient conditions are established for the existence of proportional-integral observer for the state estimation of discrete-time linear time-invariant systems and a simple procedure is given for the construction of the observer.","Our approach is based on properties of real and polynomial matrices."],"url":"http://arxiv.org/abs/2403.02891v2","category":"math.OC"}
{"created":"2024-03-05 11:39:17","title":"Zero-LED: Zero-Reference Lighting Estimation Diffusion Model for Low-Light Image Enhancement","abstract":"Diffusion model-based low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application. Meanwhile, existing unsupervised methods lack effective bridging capabilities for unknown degradation. To address these limitations, we propose a novel zero-reference lighting estimation diffusion model for low-light image enhancement called Zero-LED. It utilizes the stable convergence ability of diffusion models to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning. Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the diffusion model and the initial optimization network through multiple objective functions. Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement. In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations. Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities. We will open the source code upon acceptance of the paper.","sentences":["Diffusion model-based low-light image enhancement methods rely heavily on paired training data, leading to limited extensive application.","Meanwhile, existing unsupervised methods lack effective bridging capabilities for unknown degradation.","To address these limitations, we propose a novel zero-reference lighting estimation diffusion model for low-light image enhancement called Zero-LED.","It utilizes the stable convergence ability of diffusion models to bridge the gap between low-light domains and real normal-light domains and successfully alleviates the dependence on pairwise training data via zero-reference learning.","Specifically, we first design the initial optimization network to preprocess the input image and implement bidirectional constraints between the diffusion model and the initial optimization network through multiple objective functions.","Subsequently, the degradation factors of the real-world scene are optimized iteratively to achieve effective light enhancement.","In addition, we explore a frequency-domain based and semantically guided appearance reconstruction module that encourages feature alignment of the recovered image at a fine-grained level and satisfies subjective expectations.","Finally, extensive experiments demonstrate the superiority of our approach to other state-of-the-art methods and more significant generalization capabilities.","We will open the source code upon acceptance of the paper."],"url":"http://arxiv.org/abs/2403.02879v1","category":"cs.CV"}
{"created":"2024-03-05 11:38:20","title":"A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails","abstract":"This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization. We show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze. This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities. Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique.","sentences":["This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization.","We show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze.","This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities.","Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique."],"url":"http://arxiv.org/abs/2403.02873v1","category":"cs.LG"}
{"created":"2024-03-05 11:10:46","title":"Efficient sparse probability measures recovery via Bregman gradient","abstract":"This paper presents an algorithm tailored for the efficient recovery of sparse probability measures incorporating $\\ell_0$-sparse regularization within the probability simplex constraint. Employing the Bregman proximal gradient method, our algorithm achieves sparsity by explicitly solving underlying subproblems. We rigorously establish the convergence properties of the algorithm, showcasing its capacity to converge to a local minimum with a convergence rate of $O(1/k)$ under mild assumptions. To substantiate the efficacy of our algorithm, we conduct numerical experiments, offering a compelling demonstration of its efficiency in recovering sparse probability measures.","sentences":["This paper presents an algorithm tailored for the efficient recovery of sparse probability measures incorporating $\\ell_0$-sparse regularization within the probability simplex constraint.","Employing the Bregman proximal gradient method, our algorithm achieves sparsity by explicitly solving underlying subproblems.","We rigorously establish the convergence properties of the algorithm, showcasing its capacity to converge to a local minimum with a convergence rate of $O(1/k)$ under mild assumptions.","To substantiate the efficacy of our algorithm, we conduct numerical experiments, offering a compelling demonstration of its efficiency in recovering sparse probability measures."],"url":"http://arxiv.org/abs/2403.02861v1","category":"math.OC"}
{"created":"2024-03-05 11:09:45","title":"Novel Limited Memory Quasi-Newton Methods Based On Optimal Matrix Approximation","abstract":"Update formulas for the Hessian approximations in quasi-Newton methods such as BFGS can be derived as analytical solutions to certain nearest-matrix problems. In this article, we propose a similar idea for deriving new limited memory versions of quasi-Newton methods. Most limited memory quasi-Newton methods make use of Hessian approximations that can be written as a scaled identity matrix plus a symmetric matrix with limited rank. We derive a way of finding the nearest matrix of this type to an arbitrary symmetric matrix, in either the Frobenius norm, the induced $l^2$ norm, or a dissimilarity measure for positive definite matrices in terms of trace and determinant. In doing so, we lay down a framework for more general matrix optimization problems with unitarily invariant matrix norms and arbitrary constraints on the set of eigenvalues. We then propose a trust region method in which the Hessian approximation, after having been updated by a Broyden class formula and used to solve a trust-region problem, is replaced by one of its closest limited memory approximations. We propose to store the Hessian approximation in terms of its eigenvectors and eigenvalues in a way that completely defines its eigenvalue decomposition, as this simplifies both the solution of the trust region subproblem and the nearest limited memory matrix problem. Our method is compared to a reference trust region method with the usual limited memory BFGS updates, and is shown to require fewer iterations and the storage of fewer vectors for a variety of test problems.","sentences":["Update formulas for the Hessian approximations in quasi-Newton methods such as BFGS can be derived as analytical solutions to certain nearest-matrix problems.","In this article, we propose a similar idea for deriving new limited memory versions of quasi-Newton methods.","Most limited memory quasi-Newton methods make use of Hessian approximations that can be written as a scaled identity matrix plus a symmetric matrix with limited rank.","We derive a way of finding the nearest matrix of this type to an arbitrary symmetric matrix, in either the Frobenius norm, the induced $l^2$ norm, or a dissimilarity measure for positive definite matrices in terms of trace and determinant.","In doing so, we lay down a framework for more general matrix optimization problems with unitarily invariant matrix norms and arbitrary constraints on the set of eigenvalues.","We then propose a trust region method in which the Hessian approximation, after having been updated by a Broyden class formula and used to solve a trust-region problem, is replaced by one of its closest limited memory approximations.","We propose to store the Hessian approximation in terms of its eigenvectors and eigenvalues in a way that completely defines its eigenvalue decomposition, as this simplifies both the solution of the trust region subproblem and the nearest limited memory matrix problem.","Our method is compared to a reference trust region method with the usual limited memory BFGS updates, and is shown to require fewer iterations and the storage of fewer vectors for a variety of test problems."],"url":"http://arxiv.org/abs/2403.02860v1","category":"math.OC"}
{"created":"2024-03-05 10:56:25","title":"Quantum Data Management: From Theory to Opportunities","abstract":"Quantum computing has emerged as a transformative tool for future data management. Classical problems in database domains, including query optimization, data integration, and transaction management, have recently been addressed using quantum computing techniques. This tutorial aims to establish the theoretical foundation essential for enhancing methodologies and practical implementations in this line of research. Moreover, this tutorial takes a forward-looking approach by delving into recent strides in quantum internet technologies and the nonlocality theory. We aim to shed light on the uncharted territory of future data systems tailored for the quantum internet.","sentences":["Quantum computing has emerged as a transformative tool for future data management.","Classical problems in database domains, including query optimization, data integration, and transaction management, have recently been addressed using quantum computing techniques.","This tutorial aims to establish the theoretical foundation essential for enhancing methodologies and practical implementations in this line of research.","Moreover, this tutorial takes a forward-looking approach by delving into recent strides in quantum internet technologies and the nonlocality theory.","We aim to shed light on the uncharted territory of future data systems tailored for the quantum internet."],"url":"http://arxiv.org/abs/2403.02856v1","category":"cs.DB"}
{"created":"2024-03-05 10:19:37","title":"Trajectory stabilization of nonlocal continuity equations by localized controls","abstract":"We discuss stabilization around trajectories of the continuity equation with nonlocal vector fields, where the control is localized, i.e., it acts on a fixed subset of the configuration space. We first show that the correct definition of stabilization is the following: given an initial error of order $\\varepsilon$, measured in Wasserstein distance, one can improve the final error to an order $\\varepsilon^{1+\\kappa}$ with $\\kappa>0$. We then prove the main result: assuming that the trajectory crosses the subset of control action, stabilization can be achieved. The key problem lies in regularity issues: the reference trajectory needs to be absolutely continuous, while the initial state to be stabilized needs to be realized by a small Lipschitz perturbation or being in a very small neighborhood of it.","sentences":["We discuss stabilization around trajectories of the continuity equation with nonlocal vector fields, where the control is localized, i.e., it acts on a fixed subset of the configuration space.","We first show that the correct definition of stabilization is the following: given an initial error of order $\\varepsilon$, measured in Wasserstein distance, one can improve the final error to an order $\\varepsilon^{1+\\kappa}$ with $\\kappa>0$. We then prove the main result: assuming that the trajectory crosses the subset of control action, stabilization can be achieved.","The key problem lies in regularity issues: the reference trajectory needs to be absolutely continuous, while the initial state to be stabilized needs to be realized by a small Lipschitz perturbation or being in a very small neighborhood of it."],"url":"http://arxiv.org/abs/2403.02837v1","category":"math.OC"}
{"created":"2024-03-05 10:17:18","title":"Forced Symmetric Formation Control","abstract":"This work considers the distance constrained formation control problem with an additional constraint requiring that the formation exhibits a specified spatial symmetry. We employ recent results from the theory of symmetry-forced rigidity to construct an appropriate potential function that leads to a gradient dynamical system driving the agents to the desired formation. We show that only $(1+1/|\\Gamma|)n$ edges are sufficient to implement the control strategy when there are $n$ agents and the underlying symmetry group is $\\Gamma$. This number is considerably smaller than what is typically required from classic rigidity-theory based strategies ($2n-3$ edges). We also provide an augmented control strategy that ensures the agents can converge to a formation with respect to an arbitrary centroid. Numerous numerical examples are provided to illustrate the main results.","sentences":["This work considers the distance constrained formation control problem with an additional constraint requiring that the formation exhibits a specified spatial symmetry.","We employ recent results from the theory of symmetry-forced rigidity to construct an appropriate potential function that leads to a gradient dynamical system driving the agents to the desired formation.","We show that only $(1+1/|\\Gamma|)n$ edges are sufficient to implement the control strategy when there are $n$ agents and the underlying symmetry group is $\\Gamma$.","This number is considerably smaller than what is typically required from classic rigidity-theory based strategies ($2n-3$ edges).","We also provide an augmented control strategy that ensures the agents can converge to a formation with respect to an arbitrary centroid.","Numerous numerical examples are provided to illustrate the main results."],"url":"http://arxiv.org/abs/2403.02836v1","category":"math.OC"}
{"created":"2024-03-05 10:14:29","title":"Low-rank Tensor Autoregressive Predictor for Third-Order Time-Series Forecasting","abstract":"Recently, tensor time-series forecasting has gained increasing attention, whose core requirement is how to perform dimensionality reduction. Among all multidimensional data, third-order tensor is the most prevalent structure in real-world scenarios, such as RGB images and network traffic data. Previous studies in this field are mainly based on tensor Tucker decomposition and such methods have limitations in terms of computational cost, with iteration complexity of approximately $O(2n^3r)$, where $n$ and $r$ are the dimension and rank of original tensor data. Moreover, many real-world data does not exhibit the low-rank property under Tucker decomposition, which may fail the dimensionality reduction. In this paper, we pioneer the application of tensor singular value decomposition (t-SVD) to third-order time-series, which builds an efficient forecasting algorithm, called Low-rank Tensor Autoregressive Predictor (LOTAP). We observe that tensor tubal rank in t-SVD is always less than Tucker rank, which leads to great benefit in computational complexity. By combining it with the autoregressive (AR) model, the forecasting problem is formulated as a least squares optimization. We divide such an optimization problem by fast Fourier transformation into four decoupled subproblems, whose variables include regressive coefficient, f-diagonal tensor, left and right orthogonal tensors. The alternating minimization algorithm is proposed with iteration complexity of about $O(n^3 + n^2r^2)$, in which each subproblem has a closed-form solution. Numerical experiments show that, compared to Tucker-decomposition-based algorithms, LOTAP achieves a speed improvement ranging from 2 to 6 times while maintaining accurate forecasting performance in all four baseline tasks. In addition, LOTAP is applicable to a wider range of tensor forecasting tasks due to its more effective dimensionality reduction ability.","sentences":["Recently, tensor time-series forecasting has gained increasing attention, whose core requirement is how to perform dimensionality reduction.","Among all multidimensional data, third-order tensor is the most prevalent structure in real-world scenarios, such as RGB images and network traffic data.","Previous studies in this field are mainly based on tensor Tucker decomposition and such methods have limitations in terms of computational cost, with iteration complexity of approximately $O(2n^3r)$, where $n$ and $r$ are the dimension and rank of original tensor data.","Moreover, many real-world data does not exhibit the low-rank property under Tucker decomposition, which may fail the dimensionality reduction.","In this paper, we pioneer the application of tensor singular value decomposition (t-SVD) to third-order time-series, which builds an efficient forecasting algorithm, called Low-rank Tensor Autoregressive Predictor (LOTAP).","We observe that tensor tubal rank in t-SVD is always less than Tucker rank, which leads to great benefit in computational complexity.","By combining it with the autoregressive (AR) model, the forecasting problem is formulated as a least squares optimization.","We divide such an optimization problem by fast Fourier transformation into four decoupled subproblems, whose variables include regressive coefficient, f-diagonal tensor, left and right orthogonal tensors.","The alternating minimization algorithm is proposed with iteration complexity of about $O(n^3 +","n^2r^2)$, in which each subproblem has a closed-form solution.","Numerical experiments show that, compared to Tucker-decomposition-based algorithms, LOTAP achieves a speed improvement ranging from 2 to 6 times while maintaining accurate forecasting performance in all four baseline tasks.","In addition, LOTAP is applicable to a wider range of tensor forecasting tasks due to its more effective dimensionality reduction ability."],"url":"http://arxiv.org/abs/2403.02835v1","category":"math.OC"}
{"created":"2024-03-05 10:09:31","title":"SOFIM: Stochastic Optimization Using Regularized Fisher Information Matrix","abstract":"This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models. It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion. Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data. The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (SGD) with momentum. The extensive experiments on training deep learning models on several benchmark image classification datasets demonstrate that the proposed SOFIM outperforms SGD with momentum and several state-of-the-art Newton optimization methods, such as Nystrom-SGD, L-BFGS, and AdaHessian, in term of the convergence speed for achieving the pre-specified objectives of training and test losses as well as test accuracy.","sentences":["This paper introduces a new stochastic optimization method based on the regularized Fisher information matrix (FIM), named SOFIM, which can efficiently utilize the FIM to approximate the Hessian matrix for finding Newton's gradient update in large-scale stochastic optimization of machine learning models.","It can be viewed as a variant of natural gradient descent (NGD), where the challenge of storing and calculating the full FIM is addressed through making use of the regularized FIM and directly finding the gradient update direction via Sherman-Morrison matrix inversion.","Additionally, like the popular Adam method, SOFIM uses the first moment of the gradient to address the issue of non-stationary objectives across mini-batches due to heterogeneous data.","The utilization of the regularized FIM and Sherman-Morrison matrix inversion leads to the improved convergence rate with the same space and time complexities as stochastic gradient descent (SGD) with momentum.","The extensive experiments on training deep learning models on several benchmark image classification datasets demonstrate that the proposed SOFIM outperforms SGD with momentum and several state-of-the-art Newton optimization methods, such as Nystrom-SGD, L-BFGS, and AdaHessian, in term of the convergence speed for achieving the pre-specified objectives of training and test losses as well as test accuracy."],"url":"http://arxiv.org/abs/2403.02833v1","category":"cs.LG"}
{"created":"2024-03-05 10:01:18","title":"Targeted optimization in small-scale atomic structure calculations: application to Au I","abstract":"The lack of reliable atomic data can be a severe limitation in astrophysical modelling, in particular of events such as kilonovae that require information on all neutron-capture elements across a wide range of ionization stages. Notably, the presence of non-orthonormalities between electron orbitals representing configurations that are close in energy can introduce significant inaccuracies in computed energies and transition probabilities. Here, we propose an explicit targeted optimization method that can effectively circumvent this concern while retaining an orthonormal orbital basis set. We illustrate this method within the framework of small-scale atomic structure models of Au I, using the GRASP2018 multiconfigurational Dirac-Hartree-Fock atomic structure code. By comparing to conventional optimization schemes we show how a targeted optimization approach improves the energy level positioning and ordering. Targeted optimization also leads to better agreement with experimental data for the strongest E1 transitions. This illustrates how small-scale models can be significantly improved with minor computational costs if orbital non-orthonormalities are considered carefully. These results should prove useful to multi-element atomic structure calculations in, for example, astrophysical opacity applications involving neutron-capture elements.","sentences":["The lack of reliable atomic data can be a severe limitation in astrophysical modelling, in particular of events such as kilonovae that require information on all neutron-capture elements across a wide range of ionization stages.","Notably, the presence of non-orthonormalities between electron orbitals representing configurations that are close in energy can introduce significant inaccuracies in computed energies and transition probabilities.","Here, we propose an explicit targeted optimization method that can effectively circumvent this concern while retaining an orthonormal orbital basis set.","We illustrate this method within the framework of small-scale atomic structure models of Au I, using the GRASP2018 multiconfigurational Dirac-Hartree-Fock atomic structure code.","By comparing to conventional optimization schemes we show how a targeted optimization approach improves the energy level positioning and ordering.","Targeted optimization also leads to better agreement with experimental data for the strongest E1 transitions.","This illustrates how small-scale models can be significantly improved with minor computational costs if orbital non-orthonormalities are considered carefully.","These results should prove useful to multi-element atomic structure calculations in, for example, astrophysical opacity applications involving neutron-capture elements."],"url":"http://arxiv.org/abs/2403.02829v1","category":"physics.atom-ph"}
{"created":"2024-03-05 09:48:59","title":"Impact of domain reduction techniques in polynomial optimization: A computational study","abstract":"Domain reduction techniques are at the core of any global optimization solver for NLP or MINLP problems. In this paper, we delve into several of these techniques and assess the impact they may have in the performance of an RLT-based algorithm for polynomial optimization problems. These techniques include i) the use of (nonlinear) conic relaxations for optimality-based bound tightening, ii) the use of Lagrangian dual information to enhance feasibility-based bound tightening, and iii) different strategies for branching point selection. We also explore how a solver equipped with these domain reduction enhancements can further improve its performance by using machine learning to better choose the best domain reduction approach to use on a given instance.","sentences":["Domain reduction techniques are at the core of any global optimization solver for NLP or MINLP problems.","In this paper, we delve into several of these techniques and assess the impact they may have in the performance of an RLT-based algorithm for polynomial optimization problems.","These techniques include i) the use of (nonlinear) conic relaxations for optimality-based bound tightening, ii) the use of Lagrangian dual information to enhance feasibility-based bound tightening, and iii) different strategies for branching point selection.","We also explore how a solver equipped with these domain reduction enhancements can further improve its performance by using machine learning to better choose the best domain reduction approach to use on a given instance."],"url":"http://arxiv.org/abs/2403.02823v1","category":"math.OC"}
{"created":"2024-03-05 09:48:11","title":"Highly Reproducible and CMOS-compatible VO2-based Oscillators for Brain-inspired Computing","abstract":"With remarkable electrical and optical switching properties induced at low power and near room temperature (68C), vanadium dioxide (VO2) has sparked rising interest in unconventional computing among the phase-change materials research community. The scalability and the potential to compute beyond the von Neumann model make VO2 especially appealing for implementation in oscillating neural networks for artificial intelligence (AI) applications, to solve constraint satisfaction problems, and for pattern recognition. Its integration into large networks of oscillators on a Silicon platform still poses challenges associated with the stabilization in the correct oxidation state and the ability to fabricate a structure with predictable electrical behavior showing very low variability. In this work, the role played by the different annealing parameters applied by three methods (slow thermal annealing, flash annealing, and rapid thermal annealing), following the vanadium oxide atomic layer deposition (ALD), on the formation of VO2 grains is studied and an optimal substrate stack configuration that minimizes variability between devices is proposed. Material and electrical characterizations are performed on the different films and a step-by-step recipe to build reproducible VO2-based oscillators is presented, which is argued to be made possible thanks to the introduction of a hafnium oxide (HfO2) layer between the silicon substrate and the vanadium oxide layer. Up to seven nearly identical VO2-based devices are contacted simultaneously to create a network of oscillators, paving the way for large-scale implementation of VO2 oscillating neural networks.","sentences":["With remarkable electrical and optical switching properties induced at low power and near room temperature (68C), vanadium dioxide (VO2) has sparked rising interest in unconventional computing among the phase-change materials research community.","The scalability and the potential to compute beyond the von Neumann model make VO2 especially appealing for implementation in oscillating neural networks for artificial intelligence (AI) applications, to solve constraint satisfaction problems, and for pattern recognition.","Its integration into large networks of oscillators on a Silicon platform still poses challenges associated with the stabilization in the correct oxidation state and the ability to fabricate a structure with predictable electrical behavior showing very low variability.","In this work, the role played by the different annealing parameters applied by three methods (slow thermal annealing, flash annealing, and rapid thermal annealing), following the vanadium oxide atomic layer deposition (ALD), on the formation of VO2 grains is studied and an optimal substrate stack configuration that minimizes variability between devices is proposed.","Material and electrical characterizations are performed on the different films and a step-by-step recipe to build reproducible VO2-based oscillators is presented, which is argued to be made possible thanks to the introduction of a hafnium oxide (HfO2) layer between the silicon substrate and the vanadium oxide layer.","Up to seven nearly identical VO2-based devices are contacted simultaneously to create a network of oscillators, paving the way for large-scale implementation of VO2 oscillating neural networks."],"url":"http://arxiv.org/abs/2403.02822v1","category":"physics.app-ph"}
{"created":"2024-03-05 09:28:40","title":"Linear quadratic control of nonlinear systems with Koopman operator learning and the Nystr\u00f6m method","abstract":"In this paper, we study how the Koopman operator framework can be combined with kernel methods to effectively control nonlinear dynamical systems. While kernel methods have typically large computational requirements, we show how random subspaces (Nystr\\\"om approximation) can be used to achieve huge computational savings while preserving accuracy. Our main technical contribution is deriving theoretical guarantees on the effect of the Nystr\\\"om approximation. More precisely, we study the linear quadratic regulator problem, showing that both the approximated Riccati operator and the regulator objective, for the associated solution of the optimal control problem, converge at the rate $m^{-1/2}$, where $m$ is the random subspace size. Theoretical findings are complemented by numerical experiments corroborating our results.","sentences":["In this paper, we study how the Koopman operator framework can be combined with kernel methods to effectively control nonlinear dynamical systems.","While kernel methods have typically large computational requirements, we show how random subspaces (Nystr\\\"om approximation) can be used to achieve huge computational savings while preserving accuracy.","Our main technical contribution is deriving theoretical guarantees on the effect of the Nystr\\\"om approximation.","More precisely, we study the linear quadratic regulator problem, showing that both the approximated Riccati operator and the regulator objective, for the associated solution of the optimal control problem, converge at the rate $m^{-1/2}$, where $m$ is the random subspace size.","Theoretical findings are complemented by numerical experiments corroborating our results."],"url":"http://arxiv.org/abs/2403.02811v1","category":"math.OC"}
{"created":"2024-03-05 09:09:15","title":"Evaluating and Optimizing Educational Content with Large Language Model Judgments","abstract":"Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes. To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials. However, it is difficult to model the cognitive processes of learning dynamics. We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes. Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect. This demonstrates the potential of LMs as reliable evaluators of educational content. Building on this insight, we introduce an instruction optimization approach in which one LM generates instructional materials using the judgments of another LM as a reward function. We apply this approach to create math word problem worksheets aimed at maximizing student learning gains. Human teachers' evaluations of these LM-generated worksheets show a significant alignment between the LM judgments and human teacher preferences. We conclude by discussing potential divergences between human and LM opinions and the resulting pitfalls of automating instructional design.","sentences":["Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes.","To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials.","However, it is difficult to model the cognitive processes of learning dynamics.","We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes.","Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect.","This demonstrates the potential of LMs as reliable evaluators of educational content.","Building on this insight, we introduce an instruction optimization approach in which one LM generates instructional materials using the judgments of another LM as a reward function.","We apply this approach to create math word problem worksheets aimed at maximizing student learning gains.","Human teachers' evaluations of these LM-generated worksheets show a significant alignment between the LM judgments and human teacher preferences.","We conclude by discussing potential divergences between human and LM opinions and the resulting pitfalls of automating instructional design."],"url":"http://arxiv.org/abs/2403.02795v1","category":"cs.AI"}
{"created":"2024-03-05 09:06:20","title":"Intelligent Traffic Monitoring with Distributed Acoustic Sensing","abstract":"Distributed Acoustic Sensing (DAS) is promising for traffic monitoring, but its extensive data and sensitivity to vibrations, causing noise, pose computational challenges. To address this, we propose a two-step deep-learning workflow with high efficiency and noise immunity for DAS-based traffic monitoring, focusing on instance vehicle trajectory segmentation and velocity estimation. Our approach begins by generating a diverse synthetic DAS dataset with labeled vehicle signals, tackling the issue of missing training labels in this field. This dataset is used to train a Convolutional Neural Network (CNN) to detect linear vehicle trajectories from the noisy DAS data in the time-space domain. However, due to significant noise, these trajectories are often fragmented and incomplete. To enhance accuracy, we introduce a second step involving the Hough transform. This converts detected linear features into point-like energy clusters in the Hough domain. Another CNN is then employed to focus on these energies, identifying the most significant points. The inverse Hough transform is applied to these points to reconstruct complete, distinct, and noise-free linear vehicle trajectories in the time-space domain. The Hough transform plays a crucial role by enforcing a local linearity constraint on the trajectories, enhancing continuity and noise immunity, and facilitating the separation of individual trajectories and estimation of vehicle velocities (indicated by trajectory slopes in the Hough domain). Our method has shown effectiveness in real-world datasets, proving its value in real-time processing of DAS data and applicability in similar traffic monitoring scenarios. All related codes and data are available at https://github.com/TTMuTian/itm/.","sentences":["Distributed Acoustic Sensing (DAS) is promising for traffic monitoring, but its extensive data and sensitivity to vibrations, causing noise, pose computational challenges.","To address this, we propose a two-step deep-learning workflow with high efficiency and noise immunity for DAS-based traffic monitoring, focusing on instance vehicle trajectory segmentation and velocity estimation.","Our approach begins by generating a diverse synthetic DAS dataset with labeled vehicle signals, tackling the issue of missing training labels in this field.","This dataset is used to train a Convolutional Neural Network (CNN) to detect linear vehicle trajectories from the noisy DAS data in the time-space domain.","However, due to significant noise, these trajectories are often fragmented and incomplete.","To enhance accuracy, we introduce a second step involving the Hough transform.","This converts detected linear features into point-like energy clusters in the Hough domain.","Another CNN is then employed to focus on these energies, identifying the most significant points.","The inverse Hough transform is applied to these points to reconstruct complete, distinct, and noise-free linear vehicle trajectories in the time-space domain.","The Hough transform plays a crucial role by enforcing a local linearity constraint on the trajectories, enhancing continuity and noise immunity, and facilitating the separation of individual trajectories and estimation of vehicle velocities (indicated by trajectory slopes in the Hough domain).","Our method has shown effectiveness in real-world datasets, proving its value in real-time processing of DAS data and applicability in similar traffic monitoring scenarios.","All related codes and data are available at https://github.com/TTMuTian/itm/."],"url":"http://arxiv.org/abs/2403.02791v1","category":"physics.geo-ph"}
{"created":"2024-03-05 08:56:30","title":"Where the Really Hard Quadratic Assignment Problems Are: the QAP-SAT instances","abstract":"The Quadratic Assignment Problem (QAP) is one of the major domains in the field of evolutionary computation, and more widely in combinatorial optimization. This paper studies the phase transition of the QAP, which can be described as a dramatic change in the problem's computational complexity and satisfiability, within a narrow range of the problem parameters. To approach this phenomenon, we introduce a new QAP-SAT design of the initial problem based on submodularity to capture its difficulty with new features. This decomposition is studied experimentally using branch-and-bound and tabu search solvers. A phase transition parameter is then proposed. The critical parameter of phase transition satisfaction and that of the solving effort are shown to be highly correlated for tabu search, thus allowing the prediction of difficult instances.","sentences":["The Quadratic Assignment Problem (QAP) is one of the major domains in the field of evolutionary computation, and more widely in combinatorial optimization.","This paper studies the phase transition of the QAP, which can be described as a dramatic change in the problem's computational complexity and satisfiability, within a narrow range of the problem parameters.","To approach this phenomenon, we introduce a new QAP-SAT design of the initial problem based on submodularity to capture its difficulty with new features.","This decomposition is studied experimentally using branch-and-bound and tabu search solvers.","A phase transition parameter is then proposed.","The critical parameter of phase transition satisfaction and that of the solving effort are shown to be highly correlated for tabu search, thus allowing the prediction of difficult instances."],"url":"http://arxiv.org/abs/2403.02783v1","category":"cs.AI"}
{"created":"2024-03-05 08:52:16","title":"Data Collaboration Analysis Over Matrix Manifolds","abstract":"The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of this data in developing robust ML models. Within this realm, the Non-Readily Identifiable Data Collaboration (NRI-DC) framework emerges as an innovative approach, potentially resolving the 'data island' issue among institutions through non-iterative communication and robust privacy protections. However, in its current state, the NRI-DC framework faces model performance instability due to theoretical unsteadiness in creating collaboration functions. This study establishes a rigorous theoretical foundation for these collaboration functions and introduces new formulations through optimization problems on matrix manifolds and efficient solutions. Empirical analyses demonstrate that the proposed approach, particularly the formulation over orthogonal matrix manifolds, significantly enhances performance, maintaining consistency and efficiency without compromising communication efficiency or privacy protections.","sentences":["The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets.","Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios.","Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets.","However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure.","Various global legislative frameworks have been established to address these privacy issues.","While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies.","Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of this data in developing robust ML models.","Within this realm, the Non-Readily Identifiable Data Collaboration (NRI-DC) framework emerges as an innovative approach, potentially resolving the 'data island' issue among institutions through non-iterative communication and robust privacy protections.","However, in its current state, the NRI-DC framework faces model performance instability due to theoretical unsteadiness in creating collaboration functions.","This study establishes a rigorous theoretical foundation for these collaboration functions and introduces new formulations through optimization problems on matrix manifolds and efficient solutions.","Empirical analyses demonstrate that the proposed approach, particularly the formulation over orthogonal matrix manifolds, significantly enhances performance, maintaining consistency and efficiency without compromising communication efficiency or privacy protections."],"url":"http://arxiv.org/abs/2403.02780v1","category":"cs.LG"}
{"created":"2024-03-05 08:45:30","title":"EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs","abstract":"Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods.","sentences":["Large language models (LLMs) have proven to be very superior to conventional methods in various tasks.","However, their expensive computations and high memory requirements are prohibitive for deployment.","Model quantization is an effective method for reducing this overhead.","The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks.","Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance?","In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs.","Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error.","Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the quantization range to reduce the reconstruction error.","With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model.","Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs is safely guaranteed.","Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods."],"url":"http://arxiv.org/abs/2403.02775v1","category":"cs.AI"}
{"created":"2024-03-05 08:35:09","title":"DeconfuseTrack:Dealing with Confusion for Multi-Object Tracking","abstract":"Accurate data association is crucial in reducing confusion, such as ID switches and assignment errors, in multi-object tracking (MOT). However, existing advanced methods often overlook the diversity among trajectories and the ambiguity and conflicts present in motion and appearance cues, leading to confusion among detections, trajectories, and associations when performing simple global data association. To address this issue, we propose a simple, versatile, and highly interpretable data association approach called Decomposed Data Association (DDA). DDA decomposes the traditional association problem into multiple sub-problems using a series of non-learning-based modules and selectively addresses the confusion in each sub-problem by incorporating targeted exploitation of new cues. Additionally, we introduce Occlusion-aware Non-Maximum Suppression (ONMS) to retain more occluded detections, thereby increasing opportunities for association with trajectories and indirectly reducing the confusion caused by missed detections. Finally, based on DDA and ONMS, we design a powerful multi-object tracker named DeconfuseTrack, specifically focused on resolving confusion in MOT. Extensive experiments conducted on the MOT17 and MOT20 datasets demonstrate that our proposed DDA and ONMS significantly enhance the performance of several popular trackers. Moreover, DeconfuseTrack achieves state-of-the-art performance on the MOT17 and MOT20 test sets, significantly outperforms the baseline tracker ByteTrack in metrics such as HOTA, IDF1, AssA. This validates that our tracking design effectively reduces confusion caused by simple global association.","sentences":["Accurate data association is crucial in reducing confusion, such as ID switches and assignment errors, in multi-object tracking (MOT).","However, existing advanced methods often overlook the diversity among trajectories and the ambiguity and conflicts present in motion and appearance cues, leading to confusion among detections, trajectories, and associations when performing simple global data association.","To address this issue, we propose a simple, versatile, and highly interpretable data association approach called Decomposed Data Association (DDA).","DDA decomposes the traditional association problem into multiple sub-problems using a series of non-learning-based modules and selectively addresses the confusion in each sub-problem by incorporating targeted exploitation of new cues.","Additionally, we introduce Occlusion-aware Non-Maximum Suppression (ONMS) to retain more occluded detections, thereby increasing opportunities for association with trajectories and indirectly reducing the confusion caused by missed detections.","Finally, based on DDA and ONMS, we design a powerful multi-object tracker named DeconfuseTrack, specifically focused on resolving confusion in MOT.","Extensive experiments conducted on the MOT17 and MOT20 datasets demonstrate that our proposed DDA and ONMS significantly enhance the performance of several popular trackers.","Moreover, DeconfuseTrack achieves state-of-the-art performance on the MOT17 and MOT20 test sets, significantly outperforms the baseline tracker ByteTrack in metrics such as HOTA, IDF1, AssA. This validates that our tracking design effectively reduces confusion caused by simple global association."],"url":"http://arxiv.org/abs/2403.02767v1","category":"cs.CV"}
{"created":"2024-03-05 08:33:55","title":"Unbalanced L1 optimal transport for vector valued measures and application to Full Waveform Inversion","abstract":"Optimal transport has recently started to be successfully employed to define misfit or loss functions in inverse problems. However, it is a problem intrinsically defined for positive (probability) measures and therefore strategies are needed for its applications in more general settings of interest. In this paper we introduce an unbalanced optimal transport problem for vector valued measures starting from the $L^1$ optimal transport. By lifting data in a self-dual cone of a higher dimensional vector space, we show that one can recover a meaningful transport problem. We show that the favorable computational complexity of the $L^1$ problem, an advantage compared to other formulations of optimal transport, is inherited by our vector extension. We consider both a one-homogeneous and a two-homogeneous penalization for the imbalance of mass, the latter being potentially relevant for applications to physics based problems. In particular, we demonstrate the potential of our strategy for full waveform inversion, an inverse problem for high resolution seismic imaging.","sentences":["Optimal transport has recently started to be successfully employed to define misfit or loss functions in inverse problems.","However, it is a problem intrinsically defined for positive (probability) measures and therefore strategies are needed for its applications in more general settings of interest.","In this paper we introduce an unbalanced optimal transport problem for vector valued measures starting from the $L^1$ optimal transport.","By lifting data in a self-dual cone of a higher dimensional vector space, we show that one can recover a meaningful transport problem.","We show that the favorable computational complexity of the $L^1$ problem, an advantage compared to other formulations of optimal transport, is inherited by our vector extension.","We consider both a one-homogeneous and a two-homogeneous penalization for the imbalance of mass, the latter being potentially relevant for applications to physics based problems.","In particular, we demonstrate the potential of our strategy for full waveform inversion, an inverse problem for high resolution seismic imaging."],"url":"http://arxiv.org/abs/2403.02764v1","category":"math.OC"}
{"created":"2024-03-05 08:32:17","title":"Quantum Zeno Monte Carlo for observable measurement","abstract":"The advent of logical quantum processors marks the beginning of the early stages of error-corrected quantum computation. As a bridge between the noisy intermediate scale quantum (NISQ) era and the fault-tolerant quantum computing (FTQC) era, these devices and their successors have the potential to revolutionize the solution of classically challenging problems. An important application of quantum computers is to calculate observables of quantum systems. This problem is crucial for solving quantum many-body and optimization problems. However, due to limited error correction capabilities, this new era are still susceptible to noise, thereby necessitating new quantum algorithms with polynomial complexity as well as noisy-resilency. This paper proposes a new noise-resilient and ansatz-free algorithm, called Quantum Zeno Monte Carlo. It utilizes the quantum Zeno effect and Monte Carlo integration for multi-step adiabatic transitions to the target eigenstates. It can efficiently find static as well as dynamic physical properties such as ground state energy, excited state energies, and Green's function without the use of variational parameters. This algorithm offers a polynomial computational cost and quantum circuit depth that is significantly lower than the quantum phase estimation.","sentences":["The advent of logical quantum processors marks the beginning of the early stages of error-corrected quantum computation.","As a bridge between the noisy intermediate scale quantum (NISQ) era and the fault-tolerant quantum computing (FTQC) era, these devices and their successors have the potential to revolutionize the solution of classically challenging problems.","An important application of quantum computers is to calculate observables of quantum systems.","This problem is crucial for solving quantum many-body and optimization problems.","However, due to limited error correction capabilities, this new era are still susceptible to noise, thereby necessitating new quantum algorithms with polynomial complexity as well as noisy-resilency.","This paper proposes a new noise-resilient and ansatz-free algorithm, called Quantum Zeno Monte Carlo.","It utilizes the quantum Zeno effect and Monte Carlo integration for multi-step adiabatic transitions to the target eigenstates.","It can efficiently find static as well as dynamic physical properties such as ground state energy, excited state energies, and Green's function without the use of variational parameters.","This algorithm offers a polynomial computational cost and quantum circuit depth that is significantly lower than the quantum phase estimation."],"url":"http://arxiv.org/abs/2403.02763v1","category":"quant-ph"}
{"created":"2024-03-05 08:31:49","title":"Noise-induced transition in optimal solutions of variational quantum algorithms","abstract":"Variational quantum algorithms are promising candidates for delivering practical quantum advantage on noisy intermediate-scale quantum (NISQ) hardware. However, optimizing the noisy cost functions associated with these algorithms is challenging for system sizes relevant to quantum advantage. In this work, we investigate the effect of noise on optimization by studying a variational quantum eigensolver (VQE) algorithm calculating the ground state of a spin chain model, and we observe an abrupt transition induced by noise to the optimal solutions. We will present numerical simulations, a demonstration using an IBM quantum processor unit (QPU), and a theoretical analysis indicating the origin of this transition. Our findings suggest that careful analysis is crucial to avoid misinterpreting the noise-induced features as genuine algorithm results.","sentences":["Variational quantum algorithms are promising candidates for delivering practical quantum advantage on noisy intermediate-scale quantum (NISQ) hardware.","However, optimizing the noisy cost functions associated with these algorithms is challenging for system sizes relevant to quantum advantage.","In this work, we investigate the effect of noise on optimization by studying a variational quantum eigensolver (VQE) algorithm calculating the ground state of a spin chain model, and we observe an abrupt transition induced by noise to the optimal solutions.","We will present numerical simulations, a demonstration using an IBM quantum processor unit (QPU), and a theoretical analysis indicating the origin of this transition.","Our findings suggest that careful analysis is crucial to avoid misinterpreting the noise-induced features as genuine algorithm results."],"url":"http://arxiv.org/abs/2403.02762v1","category":"quant-ph"}
{"created":"2024-03-05 08:10:11","title":"Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps","abstract":"We present Splat-Nav, a navigation pipeline that consists of a real-time safe planning module and a robust state estimation module designed to operate in the Gaussian Splatting (GSplat) environment representation, a popular emerging 3D scene representation from computer vision. We formulate rigorous collision constraints that can be computed quickly to build a guaranteed-safe polytope corridor through the map. We then optimize a B-spline trajectory through this corridor. We also develop a real-time, robust state estimation module by interpreting the GSplat representation as a point cloud. The module enables the robot to localize its global pose with zero prior knowledge from RGB-D images using point cloud alignment, and then track its own pose as it moves through the scene from RGB images using image-to-point cloud localization. We also incorporate semantics into the GSplat in order to obtain better images for localization. All of these modules operate mainly on CPU, freeing up GPU resources for tasks like real-time scene reconstruction. We demonstrate the safety and robustness of our pipeline in both simulation and hardware, where we show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based navigation methods, thereby enabling real-time navigation.","sentences":["We present Splat-Nav, a navigation pipeline that consists of a real-time safe planning module and a robust state estimation module designed to operate in the Gaussian Splatting (GSplat) environment representation, a popular emerging 3D scene representation from computer vision.","We formulate rigorous collision constraints that can be computed quickly to build a guaranteed-safe polytope corridor through the map.","We then optimize a B-spline trajectory through this corridor.","We also develop a real-time, robust state estimation module by interpreting the GSplat representation as a point cloud.","The module enables the robot to localize its global pose with zero prior knowledge from RGB-D images using point cloud alignment, and then track its own pose as it moves through the scene from RGB images using image-to-point cloud localization.","We also incorporate semantics into the GSplat in order to obtain better images for localization.","All of these modules operate mainly on CPU, freeing up GPU resources for tasks like real-time scene reconstruction.","We demonstrate the safety and robustness of our pipeline in both simulation and hardware, where we show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude faster than Neural Radiance Field (NeRF)-based navigation methods, thereby enabling real-time navigation."],"url":"http://arxiv.org/abs/2403.02751v1","category":"cs.RO"}
{"created":"2024-03-05 07:37:47","title":"A Two-Stage Training Method for Modeling Constrained Systems With Neural Networks","abstract":"Real-world systems are often formulated as constrained optimization problems. Techniques to incorporate constraints into Neural Networks (NN), such as Neural Ordinary Differential Equations (Neural ODEs), have been used. However, these introduce hyperparameters that require manual tuning through trial and error, raising doubts about the successful incorporation of constraints into the generated model. This paper describes in detail the two-stage training method for Neural ODEs, a simple, effective, and penalty parameter-free approach to model constrained systems. In this approach the constrained optimization problem is rewritten as two unconstrained sub-problems that are solved in two stages. The first stage aims at finding feasible NN parameters by minimizing a measure of constraints violation. The second stage aims to find the optimal NN parameters by minimizing the loss function while keeping inside the feasible region. We experimentally demonstrate that our method produces models that satisfy the constraints and also improves their predictive performance. Thus, ensuring compliance with critical system properties and also contributing to reducing data quantity requirements. Furthermore, we show that the proposed method improves the convergence to an optimal solution and improves the explainability of Neural ODE models. Our proposed two-stage training method can be used with any NN architectures.","sentences":["Real-world systems are often formulated as constrained optimization problems.","Techniques to incorporate constraints into Neural Networks (NN), such as Neural Ordinary Differential Equations (Neural ODEs), have been used.","However, these introduce hyperparameters that require manual tuning through trial and error, raising doubts about the successful incorporation of constraints into the generated model.","This paper describes in detail the two-stage training method for Neural ODEs, a simple, effective, and penalty parameter-free approach to model constrained systems.","In this approach the constrained optimization problem is rewritten as two unconstrained sub-problems that are solved in two stages.","The first stage aims at finding feasible NN parameters by minimizing a measure of constraints violation.","The second stage aims to find the optimal NN parameters by minimizing the loss function while keeping inside the feasible region.","We experimentally demonstrate that our method produces models that satisfy the constraints and also improves their predictive performance.","Thus, ensuring compliance with critical system properties and also contributing to reducing data quantity requirements.","Furthermore, we show that the proposed method improves the convergence to an optimal solution and improves the explainability of Neural ODE models.","Our proposed two-stage training method can be used with any NN architectures."],"url":"http://arxiv.org/abs/2403.02730v1","category":"cs.LG"}
{"created":"2024-03-05 07:07:23","title":"Reducing computational effort in topology optimization considering the deformation in additive manufacturing","abstract":"Integrating topology optimization and additive manufacturing (AM) technology can facilitate innovative product development. However, laser powder bed fusion, which is the predominant method in metal AM, can lead to issues such as residual stress and deformation. Recently, topology optimization methods considering these stresses and deformations have been proposed; however, they suffer from challenges caused by an increased computational cost. In this study, we propose a method for reducing computational cost in topology optimization considering the deformation in AM. An inherent strain method-based analytical model is presented for simulating the residual stress and deformation in the AM process. Subsequently, a constraint condition to suppress the deformation is formulated, and a method to reduce the computational cost of the adjoint analysis in deriving sensitivity is proposed. The minimum mean compliance problem considering AM deformation and self-support constraints can then be incorporated into the level set-based topology optimization framework. Finally, numerical examples are presented for validating the effectiveness of the proposed topology optimization method.","sentences":["Integrating topology optimization and additive manufacturing (AM) technology can facilitate innovative product development.","However, laser powder bed fusion, which is the predominant method in metal AM, can lead to issues such as residual stress and deformation.","Recently, topology optimization methods considering these stresses and deformations have been proposed; however, they suffer from challenges caused by an increased computational cost.","In this study, we propose a method for reducing computational cost in topology optimization considering the deformation in AM.","An inherent strain method-based analytical model is presented for simulating the residual stress and deformation in the AM process.","Subsequently, a constraint condition to suppress the deformation is formulated, and a method to reduce the computational cost of the adjoint analysis in deriving sensitivity is proposed.","The minimum mean compliance problem considering AM deformation and self-support constraints can then be incorporated into the level set-based topology optimization framework.","Finally, numerical examples are presented for validating the effectiveness of the proposed topology optimization method."],"url":"http://arxiv.org/abs/2403.02711v1","category":"cs.CE"}
{"created":"2024-03-05 07:00:46","title":"RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches","abstract":"Natural language and images are commonly used as goal representations in goal-conditioned imitation learning (IL). However, natural language can be ambiguous and images can be over-specified. In this work, we propose hand-drawn sketches as a modality for goal specification in visual imitation learning. Sketches are easy for users to provide on the fly like language, but similar to images they can also help a downstream policy to be spatially-aware and even go beyond images to disambiguate task-relevant from task-irrelevant objects. We present RT-Sketch, a goal-conditioned policy for manipulation that takes a hand-drawn sketch of the desired scene as input, and outputs actions. We train RT-Sketch on a dataset of paired trajectories and corresponding synthetically generated goal sketches. We evaluate this approach on six manipulation skills involving tabletop object rearrangements on an articulated countertop. Experimentally we find that RT-Sketch is able to perform on a similar level to image or language-conditioned agents in straightforward settings, while achieving greater robustness when language goals are ambiguous or visual distractors are present. Additionally, we show that RT-Sketch has the capacity to interpret and act upon sketches with varied levels of specificity, ranging from minimal line drawings to detailed, colored drawings. For supplementary material and videos, please refer to our website: http://rt-sketch.github.io.","sentences":["Natural language and images are commonly used as goal representations in goal-conditioned imitation learning (IL).","However, natural language can be ambiguous and images can be over-specified.","In this work, we propose hand-drawn sketches as a modality for goal specification in visual imitation learning.","Sketches are easy for users to provide on the fly like language, but similar to images they can also help a downstream policy to be spatially-aware and even go beyond images to disambiguate task-relevant from task-irrelevant objects.","We present RT-Sketch, a goal-conditioned policy for manipulation that takes a hand-drawn sketch of the desired scene as input, and outputs actions.","We train RT-Sketch on a dataset of paired trajectories and corresponding synthetically generated goal sketches.","We evaluate this approach on six manipulation skills involving tabletop object rearrangements on an articulated countertop.","Experimentally we find that RT-Sketch is able to perform on a similar level to image or language-conditioned agents in straightforward settings, while achieving greater robustness when language goals are ambiguous or visual distractors are present.","Additionally, we show that RT-Sketch has the capacity to interpret and act upon sketches with varied levels of specificity, ranging from minimal line drawings to detailed, colored drawings.","For supplementary material and videos, please refer to our website: http://rt-sketch.github.io."],"url":"http://arxiv.org/abs/2403.02709v1","category":"cs.RO"}
