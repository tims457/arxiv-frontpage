{"created":"2024-03-11 17:59:57","title":"Tidal synchronization trapping in stars and planets with convective envelopes","abstract":"Tidal torques can alter the spins of tidally interacting stars and planets, usually over shorter timescales than the tidal damping of orbital separations or eccentricities. Simple tidal models predict that in eccentric binary or planetary systems, rotation periods will evolve toward a \"pseudosynchronous\" ratio with the orbital period. However, this prediction does not account for \"inertial\" waves that are present in stars or gaseous planets with (i) convective envelopes, and (ii) even very slow rotation. We demonstrate that tidal driving of inertial oscillations in eccentric systems generically produces a network of stable \"synchronization traps\" at ratios of orbital to rotation period that are simple to predict, but can deviate significantly from pseudosynchronization. The mechanism underlying spin synchronization trapping is similar to tidal resonance locking, involving a balance between torques that is maintained automatically by the scaling of inertial mode frequencies with the rotation rate. In contrast with many resonance locking scenarios, however, the torque balance required for synchronization trapping need not drive mode amplitudes to nonlinearity. Synchronization traps may provide an explanation for low-mass stars and hot Jupiters with observed rotation rates that deviate from pseudosynchronous or synchronous expectations.","sentences":["Tidal torques can alter the spins of tidally interacting stars and planets, usually over shorter timescales than the tidal damping of orbital separations or eccentricities.","Simple tidal models predict that in eccentric binary or planetary systems, rotation periods will evolve toward a \"pseudosynchronous\" ratio with the orbital period.","However, this prediction does not account for \"inertial\" waves that are present in stars or gaseous planets with (i) convective envelopes, and (ii) even very slow rotation.","We demonstrate that tidal driving of inertial oscillations in eccentric systems generically produces a network of stable \"synchronization traps\" at ratios of orbital to rotation period that are simple to predict, but can deviate significantly from pseudosynchronization.","The mechanism underlying spin synchronization trapping is similar to tidal resonance locking, involving a balance between torques that is maintained automatically by the scaling of inertial mode frequencies with the rotation rate.","In contrast with many resonance locking scenarios, however, the torque balance required for synchronization trapping need not drive mode amplitudes to nonlinearity.","Synchronization traps may provide an explanation for low-mass stars and hot Jupiters with observed rotation rates that deviate from pseudosynchronous or synchronous expectations."],"url":"http://arxiv.org/abs/2403.06979v1","category":"astro-ph.SR"}
{"created":"2024-03-11 17:58:43","title":"Ehrhart polynomials of partial permutohedra","abstract":"For positive integers $m$ and $n$, the partial permutohedron $\\mathcal{P}(m,n)$ is a certain integral polytope in $\\mathbb{R}^m$, which can be defined as the convex hull of the vectors from $\\{0,1,\\ldots,n\\}^m$ whose nonzero entries are distinct. For $n=m-1$, $\\mathcal{P}(m,m-1)$ is (after translation by $(1,\\ldots,1)$) the polytope $P_m$ of parking functions of length $m$, and for $n\\ge m$, $\\mathcal{P}(m,n)$ is combinatorially equivalent to an $m$-stellohedron. The main result of this paper is an explicit expression for the Ehrhart polynomial of $\\mathcal{P}(m,n)$ for any $m$ and $n$ with $n\\ge m-1$. The result confirms the validity of a conjecture for this Ehrhart polynomial in arXiv:2207.14253, and the $n=m-1$ case also answers a question of Stanley regarding the number of integer points in $P_m$. The proof of the result involves transforming $\\mathcal{P}(m,n)$ to a unimodularly equivalent polytope in $\\mathbb{R}^{m+1}$, obtaining a decomposition of this lifted version of $\\mathcal{P}(m,n)$ with $n\\ge m-1$ as a Minkowski sum of dilated coordinate simplices, applying a result of Postnikov for the number of integer points in generalized permutohedra of this form, observing that this gives an expression for the Ehrhart polynomial of $\\mathcal{P}(m,n)$ with $n\\ge m-1$ as an edge-weighted sum over graphs (with loops and multiple edges permitted) on $m$ labelled vertices in which each connected component contains at most one cycle, and then applying standard techniques for the enumeration of such graphs.","sentences":["For positive integers $m$ and $n$, the partial permutohedron $\\mathcal{P}(m,n)$ is a certain integral polytope in $\\mathbb{R}^m$, which can be defined as the convex hull of the vectors from $\\{0,1,\\ldots,n\\}^m$ whose nonzero entries are distinct.","For $n=m-1$, $\\mathcal{P}(m,m-1)$ is (after translation by $(1,\\ldots,1)$) the polytope $P_m$ of parking functions of length $m$, and for $n\\ge m$, $\\mathcal{P}(m,n)$ is combinatorially equivalent to an $m$-stellohedron.","The main result of this paper is an explicit expression for the Ehrhart polynomial of $\\mathcal{P}(m,n)$ for any $m$ and $n$ with $n\\ge m-1$. The result confirms the validity of a conjecture for this Ehrhart polynomial in arXiv:2207.14253, and the $n=m-1$ case also answers a question of Stanley regarding the number of integer points in $P_m$. The proof of the result involves transforming $\\mathcal{P}(m,n)$ to a unimodularly equivalent polytope in $\\mathbb{R}^{m+1}$, obtaining a decomposition of this lifted version of $\\mathcal{P}(m,n)$ with $n\\ge m-1$ as a Minkowski sum of dilated coordinate simplices, applying a result of Postnikov for the number of integer points in generalized permutohedra of this form, observing that this gives an expression for the Ehrhart polynomial of $\\mathcal{P}(m,n)$ with $n\\ge m-1$ as an edge-weighted sum over graphs (with loops and multiple edges permitted) on $m$ labelled vertices in which each connected component contains at most one cycle, and then applying standard techniques for the enumeration of such graphs."],"url":"http://arxiv.org/abs/2403.06975v1","category":"math.CO"}
{"created":"2024-03-11 17:55:02","title":"Investigating model dependencies for obscured Active Galactic Nuclei: a case study of NGC 3982","abstract":"X-ray spectroscopy of heavily obscured Active Galactic Nuclei (AGN) offers a unique opportunity to study the circum-nuclear environment of accreting supermassive black holes (SMBHs). However, individual models describing the obscurer have unique parameter spaces that give distinct parameter posterior distributions when fit to the same data. To assess the impact of model-specific parameter dependencies, we present a case study of the nearby heavily obscured low-luminosity AGN NGC 3982, which has a variety of column density estimations reported in the literature. We fit the same broadband XMM-Newton + NuSTAR spectra of the source with five unique obscuration models and generate posterior parameter distributions for each. By using global parameter exploration, we traverse the full prior-defined parameter space to accurately reproduce complex posterior shapes and inter-parameter degeneracies. The unique model posteriors for the line-of-sight column density are broadly consistent, predicting Compton-thick $N_{\\rm H}$ $>1.5\\times10^{24}\\rm cm^{-2}$ at the 3$\\sigma$ confidence level. The posterior median intrinsic X-ray luminosity in the 2-10 keV band however was found to differ substantially, with values in the range log $L_{ 2-10\\,{\\rm keV}}$ergs$^{-1}$ = 40.9-42.1 for the individual models. We additionally show that the posterior distributions for each model occupy unique regions of their respective multi-dimensional parameters spaces, and how such differences can propagate into the inferred properties of the central engine. We conclude by showcasing the improvement in parameter inference attainable with the High Energy X-ray Probe (HEX-P) with a uniquely broad simultaneous and high-sensitivity bandpass of 0.2-80 keV.","sentences":["X-ray spectroscopy of heavily obscured Active Galactic Nuclei (AGN) offers a unique opportunity to study the circum-nuclear environment of accreting supermassive black holes (SMBHs).","However, individual models describing the obscurer have unique parameter spaces that give distinct parameter posterior distributions when fit to the same data.","To assess the impact of model-specific parameter dependencies, we present a case study of the nearby heavily obscured low-luminosity AGN NGC 3982, which has a variety of column density estimations reported in the literature.","We fit the same broadband XMM-Newton + NuSTAR spectra of the source with five unique obscuration models and generate posterior parameter distributions for each.","By using global parameter exploration, we traverse the full prior-defined parameter space to accurately reproduce complex posterior shapes and inter-parameter degeneracies.","The unique model posteriors for the line-of-sight column density are broadly consistent, predicting Compton-thick $N_{\\rm H}$ $>1.5\\times10^{24}\\rm cm^{-2}$ at the 3$\\sigma$ confidence level.","The posterior median intrinsic X-ray luminosity in the 2-10 keV band however was found to differ substantially, with values in the range log $L_{ 2-10\\,{\\rm keV}}$ergs$^{-1}$ = 40.9-42.1 for the individual models.","We additionally show that the posterior distributions for each model occupy unique regions of their respective multi-dimensional parameters spaces, and how such differences can propagate into the inferred properties of the central engine.","We conclude by showcasing the improvement in parameter inference attainable with the High Energy X-ray Probe (HEX-P) with a uniquely broad simultaneous and high-sensitivity bandpass of 0.2-80 keV."],"url":"http://arxiv.org/abs/2403.06972v1","category":"astro-ph.HE"}
{"created":"2024-03-11 17:54:42","title":"A representation-learning game for classes of prediction tasks","abstract":"We propose a game-based formulation for learning dimensionality-reducing representations of feature vectors, when only a prior knowledge on future prediction tasks is available. In this game, the first player chooses a representation, and then the second player adversarially chooses a prediction task from a given class, representing the prior knowledge. The first player aims is to minimize, and the second player to maximize, the regret: The minimal prediction loss using the representation, compared to the same loss using the original features. For the canonical setting in which the representation, the response to predict and the predictors are all linear functions, and under the mean squared error loss function, we derive the theoretically optimal representation in pure strategies, which shows the effectiveness of the prior knowledge, and the optimal regret in mixed strategies, which shows the usefulness of randomizing the representation. For general representations and loss functions, we propose an efficient algorithm to optimize a randomized representation. The algorithm only requires the gradients of the loss function, and is based on incrementally adding a representation rule to a mixture of such rules.","sentences":["We propose a game-based formulation for learning dimensionality-reducing representations of feature vectors, when only a prior knowledge on future prediction tasks is available.","In this game, the first player chooses a representation, and then the second player adversarially chooses a prediction task from a given class, representing the prior knowledge.","The first player aims is to minimize, and the second player to maximize, the regret: The minimal prediction loss using the representation, compared to the same loss using the original features.","For the canonical setting in which the representation, the response to predict and the predictors are all linear functions, and under the mean squared error loss function, we derive the theoretically optimal representation in pure strategies, which shows the effectiveness of the prior knowledge, and the optimal regret in mixed strategies, which shows the usefulness of randomizing the representation.","For general representations and loss functions, we propose an efficient algorithm to optimize a randomized representation.","The algorithm only requires the gradients of the loss function, and is based on incrementally adding a representation rule to a mixture of such rules."],"url":"http://arxiv.org/abs/2403.06971v1","category":"cs.LG"}
{"created":"2024-03-11 17:47:30","title":"The pitfalls of next-token prediction","abstract":"Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn. We provide preliminary evidence that this failure can be resolved when training to predict multiple tokens in advance. We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm. We make our code available under https://github.com/gregorbachmann/Next-Token-Failures","sentences":["Can a mere next-token predictor faithfully model human intelligence?","We crystallize this intuitive concern, which is fragmented in the literature.","As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly.","The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor.","This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place.","We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn.","We provide preliminary evidence that this failure can be resolved when training to predict multiple tokens in advance.","We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm.","We make our code available under https://github.com/gregorbachmann/Next-Token-Failures"],"url":"http://arxiv.org/abs/2403.06963v1","category":"cs.CL"}
{"created":"2024-03-11 17:47:08","title":"Fully non-Gaussian Scalar-Induced Gravitational Waves","abstract":"Scalar-induced Gravitational Waves (SIGWs) represent a particular class of primordial signals which are sourced at second-order in perturbation theory whenever a scalar fluctuation of the metric is present. They form a guaranteed Stochastic Gravitational Wave Background (SGWB) that, depending on the amplification of primordial scalar fluctuations, can be detected by GW detectors. The amplitude and the frequency shape of the scalar-induced SGWB can be influenced by the statistical properties of the scalar density perturbations. In this work we study the intuitive physics behind SIGWs and we analyze the imprints of local non-Gaussianity of the primordial curvature perturbation on the GW spectrum. We consider all the relevant non-Gaussian contributions up to fifth-order in the scalar seeds without any hierarchy, and we derive the related GW energy density $\\Omega_{\\rm GW}(f)$. We perform a Fisher matrix analysis to understand to which accuracy non-Gaussianity can be constrained with the LISA detector, which will be sensitive in the milli-Hertz frequency band. We find that LISA, neglecting the impact of astrophysical foregrounds, will be able to measure the amplitude, the width and the peak of the spectrum with an accuracy up to $\\mathcal{O}(10^{-4})$, while non-Gaussianity can be measured up to $\\mathcal{O}(10^{-3})$. Finally, we discuss the implications of our non-Gaussianity expansion on the fraction of Primordial Black Holes.","sentences":["Scalar-induced Gravitational Waves (SIGWs) represent a particular class of primordial signals which are sourced at second-order in perturbation theory whenever a scalar fluctuation of the metric is present.","They form a guaranteed Stochastic Gravitational Wave Background (SGWB) that, depending on the amplification of primordial scalar fluctuations, can be detected by GW detectors.","The amplitude and the frequency shape of the scalar-induced SGWB can be influenced by the statistical properties of the scalar density perturbations.","In this work we study the intuitive physics behind SIGWs and we analyze the imprints of local non-Gaussianity of the primordial curvature perturbation on the GW spectrum.","We consider all the relevant non-Gaussian contributions up to fifth-order in the scalar seeds without any hierarchy, and we derive the related GW energy density $\\Omega_{\\rm GW}(f)$. We perform a Fisher matrix analysis to understand to which accuracy non-Gaussianity can be constrained with the LISA detector, which will be sensitive in the milli-Hertz frequency band.","We find that LISA, neglecting the impact of astrophysical foregrounds, will be able to measure the amplitude, the width and the peak of the spectrum with an accuracy up to $\\mathcal{O}(10^{-4})$, while non-Gaussianity can be measured up to $\\mathcal{O}(10^{-3})$. Finally, we discuss the implications of our non-Gaussianity expansion on the fraction of Primordial Black Holes."],"url":"http://arxiv.org/abs/2403.06962v1","category":"astro-ph.CO"}
{"created":"2024-03-11 17:46:21","title":"Explainable Transformer Prototypes for Medical Diagnoses","abstract":"Deployments of artificial intelligence in medical diagnostics mandate not just accuracy and efficacy but also trust, emphasizing the need for explainability in machine decisions. The recent trend in automated medical image diagnostics leans towards the deployment of Transformer-based architectures, credited to their impressive capabilities. Since the self-attention feature of transformers contributes towards identifying crucial regions during the classification process, they enhance the trustability of the methods. However, the complex intricacies of these attention mechanisms may fall short of effectively pinpointing the regions of interest directly influencing AI decisions. Our research endeavors to innovate a unique attention block that underscores the correlation between 'regions' rather than 'pixels'. To address this challenge, we introduce an innovative system grounded in prototype learning, featuring an advanced self-attention mechanism that goes beyond conventional ad-hoc visual explanation techniques by offering comprehensible visual insights. A combined quantitative and qualitative methodological approach was used to demonstrate the effectiveness of the proposed method on the large-scale NIH chest X-ray dataset. Experimental results showed that our proposed method offers a promising direction for explainability, which can lead to the development of more trustable systems, which can facilitate easier and rapid adoption of such technology into routine clinics. The code is available at www.github.com/NUBagcilab/r2r_proto.","sentences":["Deployments of artificial intelligence in medical diagnostics mandate not just accuracy and efficacy but also trust, emphasizing the need for explainability in machine decisions.","The recent trend in automated medical image diagnostics leans towards the deployment of Transformer-based architectures, credited to their impressive capabilities.","Since the self-attention feature of transformers contributes towards identifying crucial regions during the classification process, they enhance the trustability of the methods.","However, the complex intricacies of these attention mechanisms may fall short of effectively pinpointing the regions of interest directly influencing AI decisions.","Our research endeavors to innovate a unique attention block that underscores the correlation between 'regions' rather than 'pixels'.","To address this challenge, we introduce an innovative system grounded in prototype learning, featuring an advanced self-attention mechanism that goes beyond conventional ad-hoc visual explanation techniques by offering comprehensible visual insights.","A combined quantitative and qualitative methodological approach was used to demonstrate the effectiveness of the proposed method on the large-scale NIH chest X-ray dataset.","Experimental results showed that our proposed method offers a promising direction for explainability, which can lead to the development of more trustable systems, which can facilitate easier and rapid adoption of such technology into routine clinics.","The code is available at www.github.com/NUBagcilab/r2r_proto."],"url":"http://arxiv.org/abs/2403.06961v1","category":"cs.CV"}
{"created":"2024-03-11 17:45:10","title":"Notes on solitary-wave solutions of Rosenau-type equations","abstract":"The present paper is concerned with the existence of solitary wave solutions of Rosenau-type equations. By using two standard theories, Normal Form Theory and Concentration-Compactness Theory, some results of existence of solitary waves of three different forms are derived. The results depend on some conditions on the speed of the waves with respect to the parameters of the equations. They are discussed for several families of Rosenau equations present in the literature. The analysis is illustrated with a numerical study of generation of approximate solitary-wave profiles from a numerical procedure based on the Petviashvili iteration.","sentences":["The present paper is concerned with the existence of solitary wave solutions of Rosenau-type equations.","By using two standard theories, Normal Form Theory and Concentration-Compactness Theory, some results of existence of solitary waves of three different forms are derived.","The results depend on some conditions on the speed of the waves with respect to the parameters of the equations.","They are discussed for several families of Rosenau equations present in the literature.","The analysis is illustrated with a numerical study of generation of approximate solitary-wave profiles from a numerical procedure based on the Petviashvili iteration."],"url":"http://arxiv.org/abs/2403.06958v1","category":"math.AP"}
{"created":"2024-03-11 17:36:11","title":"Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot Domain Transfer","abstract":"Purpose: Advances in deep learning have resulted in effective models for surgical video analysis; however, these models often fail to generalize across medical centers due to domain shift caused by variations in surgical workflow, camera setups, and patient demographics. Recently, object-centric learning has emerged as a promising approach for improved surgical scene understanding, capturing and disentangling visual and semantic properties of surgical tools and anatomy to improve downstream task performance. In this work, we conduct a multi-centric performance benchmark of object-centric approaches, focusing on Critical View of Safety assessment in laparoscopic cholecystectomy, then propose an improved approach for unseen domain generalization.   Methods: We evaluate four object-centric approaches for domain generalization, establishing baseline performance. Next, leveraging the disentangled nature of object-centric representations, we dissect one of these methods through a series of ablations (e.g. ignoring either visual or semantic features for downstream classification). Finally, based on the results of these ablations, we develop an optimized method specifically tailored for domain generalization, LG-DG, that includes a novel disentanglement loss function.   Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over the best baseline approach. More broadly, we show that object-centric approaches are highly effective for domain generalization thanks to their modular approach to representation learning.   Conclusion: We investigate the use of object-centric methods for unseen domain generalization, identify method-agnostic factors critical for performance, and present an optimized approach that substantially outperforms existing methods.","sentences":["Purpose:","Advances in deep learning have resulted in effective models for surgical video analysis; however, these models often fail to generalize across medical centers due to domain shift caused by variations in surgical workflow, camera setups, and patient demographics.","Recently, object-centric learning has emerged as a promising approach for improved surgical scene understanding, capturing and disentangling visual and semantic properties of surgical tools and anatomy to improve downstream task performance.","In this work, we conduct a multi-centric performance benchmark of object-centric approaches, focusing on Critical View of Safety assessment in laparoscopic cholecystectomy, then propose an improved approach for unseen domain generalization.   ","Methods: We evaluate four object-centric approaches for domain generalization, establishing baseline performance.","Next, leveraging the disentangled nature of object-centric representations, we dissect one of these methods through a series of ablations (e.g. ignoring either visual or semantic features for downstream classification).","Finally, based on the results of these ablations, we develop an optimized method specifically tailored for domain generalization, LG-DG, that includes a novel disentanglement loss function.   ","Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over the best baseline approach.","More broadly, we show that object-centric approaches are highly effective for domain generalization thanks to their modular approach to representation learning.   ","Conclusion: We investigate the use of object-centric methods for unseen domain generalization, identify method-agnostic factors critical for performance, and present an optimized approach that substantially outperforms existing methods."],"url":"http://arxiv.org/abs/2403.06953v1","category":"cs.CV"}
{"created":"2024-03-11 17:35:33","title":"SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data","abstract":"Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models.","sentences":["Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions.","However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects.","In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging.","First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts.","Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging.","Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets.","We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation.","Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data.","Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models."],"url":"http://arxiv.org/abs/2403.06952v1","category":"cs.CV"}
{"created":"2024-03-11 17:34:25","title":"Materials science in the era of large language models: a perspective","abstract":"Large Language Models (LLMs) have garnered considerable interest due to their impressive natural language capabilities, which in conjunction with various emergent properties make them versatile tools in workflows ranging from complex code generation to heuristic finding for combinatorial problems. In this paper we offer a perspective on their applicability to materials science research, arguing their ability to handle ambiguous requirements across a range of tasks and disciplines mean they could be a powerful tool to aid researchers. We qualitatively examine basic LLM theory, connecting it to relevant properties and techniques in the literature before providing two case studies that demonstrate their use in task automation and knowledge extraction at-scale. At their current stage of development, we argue LLMs should be viewed less as oracles of novel insight, and more as tireless workers that can accelerate and unify exploration across domains. It is our hope that this paper can familiarise material science researchers with the concepts needed to leverage these tools in their own research.","sentences":["Large Language Models (LLMs) have garnered considerable interest due to their impressive natural language capabilities, which in conjunction with various emergent properties make them versatile tools in workflows ranging from complex code generation to heuristic finding for combinatorial problems.","In this paper we offer a perspective on their applicability to materials science research, arguing their ability to handle ambiguous requirements across a range of tasks and disciplines mean they could be a powerful tool to aid researchers.","We qualitatively examine basic LLM theory, connecting it to relevant properties and techniques in the literature before providing two case studies that demonstrate their use in task automation and knowledge extraction at-scale.","At their current stage of development, we argue LLMs should be viewed less as oracles of novel insight, and more as tireless workers that can accelerate and unify exploration across domains.","It is our hope that this paper can familiarise material science researchers with the concepts needed to leverage these tools in their own research."],"url":"http://arxiv.org/abs/2403.06949v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-11 17:33:25","title":"Advancing Generalizable Remote Physiological Measurement through the Integration of Explicit and Implicit Prior Knowledge","abstract":"Remote photoplethysmography (rPPG) is a promising technology that captures physiological signals from face videos, with potential applications in medical health, emotional computing, and biosecurity recognition. The demand for rPPG tasks has expanded from demonstrating good performance on intra-dataset testing to cross-dataset testing (i.e., domain generalization). However, most existing methods have overlooked the prior knowledge of rPPG, resulting in poor generalization ability. In this paper, we propose a novel framework that simultaneously utilizes explicit and implicit prior knowledge in the rPPG task. Specifically, we systematically analyze the causes of noise sources (e.g., different camera, lighting, skin types, and movement) across different domains and incorporate these prior knowledge into the network. Additionally, we leverage a two-branch network to disentangle the physiological feature distribution from noises through implicit label correlation. Our extensive experiments demonstrate that the proposed method not only outperforms state-of-the-art methods on RGB cross-dataset evaluation but also generalizes well from RGB datasets to NIR datasets. The code is available at https://github.com/keke-nice/Greip.","sentences":["Remote photoplethysmography (rPPG) is a promising technology that captures physiological signals from face videos, with potential applications in medical health, emotional computing, and biosecurity recognition.","The demand for rPPG tasks has expanded from demonstrating good performance on intra-dataset testing to cross-dataset testing (i.e., domain generalization).","However, most existing methods have overlooked the prior knowledge of rPPG, resulting in poor generalization ability.","In this paper, we propose a novel framework that simultaneously utilizes explicit and implicit prior knowledge in the rPPG task.","Specifically, we systematically analyze the causes of noise sources (e.g., different camera, lighting, skin types, and movement) across different domains and incorporate these prior knowledge into the network.","Additionally, we leverage a two-branch network to disentangle the physiological feature distribution from noises through implicit label correlation.","Our extensive experiments demonstrate that the proposed method not only outperforms state-of-the-art methods on RGB cross-dataset evaluation but also generalizes well from RGB datasets to NIR datasets.","The code is available at https://github.com/keke-nice/Greip."],"url":"http://arxiv.org/abs/2403.06947v1","category":"cs.CV"}
{"created":"2024-03-11 17:31:49","title":"Sj$\\ddot{\\text{o}}$qvist quantum geometric tensor of finite-temperature mixed states","abstract":"The quantum geometric tensor (QGT) reveals local geometric properties and associated topological information of quantum states. Here a generalization of the QGT to mixed quantum states at finite temperatures based on the Sj$\\ddot{\\text{o}}$qvist distance is developed. The resulting Sj$\\ddot{\\text{o}}$qvist QGT is invariant under gauge transformations of individual spectrum levels. A Pythagorean-like relation connects the distances and gauge transformations, which clarifies the role of the parallel-transport condition. The real part of the QGT naturally decomposes into a sum of the Fisher-Rao metric and Fubini-Study metrics, allowing a distinction between different contributions to the quantum distance. The imaginary part of the QGT is proportional to the weighted summation of the Berry curvatures, which leads to a geometric phase for mixed states under certain conditions. We present three examples of different dimensions to illustrate the temperature dependence of the QGT and a discussion on possible implications.","sentences":["The quantum geometric tensor (QGT) reveals local geometric properties and associated topological information of quantum states.","Here a generalization of the QGT to mixed quantum states at finite temperatures based on the Sj$\\ddot{\\text{o}}$qvist distance is developed.","The resulting Sj$\\ddot{\\text{o}}$qvist QGT is invariant under gauge transformations of individual spectrum levels.","A Pythagorean-like relation connects the distances and gauge transformations, which clarifies the role of the parallel-transport condition.","The real part of the QGT naturally decomposes into a sum of the Fisher-Rao metric and Fubini-Study metrics, allowing a distinction between different contributions to the quantum distance.","The imaginary part of the QGT is proportional to the weighted summation of the Berry curvatures, which leads to a geometric phase for mixed states under certain conditions.","We present three examples of different dimensions to illustrate the temperature dependence of the QGT and a discussion on possible implications."],"url":"http://arxiv.org/abs/2403.06944v1","category":"quant-ph"}
{"created":"2024-03-11 17:28:46","title":"Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI","abstract":"Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference. Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection.   Methods and Results: The architecture of the proposed design originates from the Wiener-Kallianpur innovation representation of a random process that transforms causally a stationary random process into an innovation sequence with independent and identically distributed random variables. This work presents a generative AI approach that (i) learns an innovation autoencoder that extracts innovation sequence from CPOW time series, (ii) compresses the CPOW streaming data with innovation autoencoder and subband coding, and (iii) detects unknown faults and novel trends via nonparametric sequential hypothesis testing.   Conclusion: This work argues that conventional monitoring using SCADA and phasor measurement unit (PMU) technologies is ill-suited for a future grid with deep penetration of inverter-based renewable generations and distributed energy resources. A monitoring system based on CPOW data streaming and AI data analytics should be the basic building blocks for situational awareness of a highly dynamic future grid.","sentences":["Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference.","Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection.   Methods and Results: The architecture of the proposed design originates from the Wiener-Kallianpur innovation representation of a random process that transforms causally a stationary random process into an innovation sequence with independent and identically distributed random variables.","This work presents a generative AI approach that (i) learns an innovation autoencoder that extracts innovation sequence from CPOW time series, (ii) compresses the CPOW streaming data with innovation autoencoder and subband coding, and (iii) detects unknown faults and novel trends via nonparametric sequential hypothesis testing.   ","Conclusion: This work argues that conventional monitoring using SCADA and phasor measurement unit (PMU) technologies is ill-suited for a future grid with deep penetration of inverter-based renewable generations and distributed energy resources.","A monitoring system based on CPOW data streaming and AI data analytics should be the basic building blocks for situational awareness of a highly dynamic future grid."],"url":"http://arxiv.org/abs/2403.06942v1","category":"eess.SY"}
{"created":"2024-03-11 17:26:18","title":"Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction","abstract":"Alzheimer's Disease (AD) is a neurodegenerative condition characterized by diverse progression rates among individuals, with changes in cortical thickness (CTh) closely linked to its progression. Accurately forecasting CTh trajectories can significantly enhance early diagnosis and intervention strategies, providing timely care. However, the longitudinal data essential for these studies often suffer from temporal sparsity and incompleteness, presenting substantial challenges in modeling the disease's progression accurately. Existing methods are limited, focusing primarily on datasets without missing entries or requiring predefined assumptions about CTh progression. To overcome these obstacles, we propose a conditional score-based diffusion model specifically designed to generate CTh trajectories with the given baseline information, such as age, sex, and initial diagnosis. Our conditional diffusion model utilizes all available data during the training phase to make predictions based solely on baseline information during inference without needing prior history about CTh progression. The prediction accuracy of the proposed CTh prediction pipeline using a conditional score-based model was compared for sub-groups consisting of cognitively normal, mild cognitive impairment, and AD subjects. The Bland-Altman analysis shows our diffusion-based prediction model has a near-zero bias with narrow 95% confidential interval compared to the ground-truth CTh in 6-36 months. In addition, our conditional diffusion model has a stochastic generative nature, therefore, we demonstrated an uncertainty analysis of patient-specific CTh prediction through multiple realizations.","sentences":["Alzheimer's Disease (AD) is a neurodegenerative condition characterized by diverse progression rates among individuals, with changes in cortical thickness (CTh) closely linked to its progression.","Accurately forecasting CTh trajectories can significantly enhance early diagnosis and intervention strategies, providing timely care.","However, the longitudinal data essential for these studies often suffer from temporal sparsity and incompleteness, presenting substantial challenges in modeling the disease's progression accurately.","Existing methods are limited, focusing primarily on datasets without missing entries or requiring predefined assumptions about CTh progression.","To overcome these obstacles, we propose a conditional score-based diffusion model specifically designed to generate CTh trajectories with the given baseline information, such as age, sex, and initial diagnosis.","Our conditional diffusion model utilizes all available data during the training phase to make predictions based solely on baseline information during inference without needing prior history about CTh progression.","The prediction accuracy of the proposed CTh prediction pipeline using a conditional score-based model was compared for sub-groups consisting of cognitively normal, mild cognitive impairment, and AD subjects.","The Bland-Altman analysis shows our diffusion-based prediction model has a near-zero bias with narrow 95% confidential interval compared to the ground-truth CTh in 6-36 months.","In addition, our conditional diffusion model has a stochastic generative nature, therefore, we demonstrated an uncertainty analysis of patient-specific CTh prediction through multiple realizations."],"url":"http://arxiv.org/abs/2403.06940v1","category":"eess.IV"}
{"created":"2024-03-11 17:21:39","title":"Counterfactual Reasoning with Knowledge Graph Embeddings","abstract":"Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories. In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR. We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules. We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained. We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark. Our results indicate that KGEs learn patterns in the graph without explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow these patterns. An evaluation on human-annotated data reveals that KGEs adapted with COULDD are mostly unable to recognize changes to the graph that do not follow learned inference rules. In contrast, ChatGPT mostly outperforms KGEs in detecting plausible changes to the graph but has poor knowledge retention. In summary, CFKGR connects two previously distinct areas, namely KG completion and counterfactual reasoning.","sentences":["Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories.","In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR.","We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules.","We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained.","We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark.","Our results indicate that KGEs learn patterns in the graph without explicit training.","We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow these patterns.","An evaluation on human-annotated data reveals that KGEs adapted with COULDD are mostly unable to recognize changes to the graph that do not follow learned inference rules.","In contrast, ChatGPT mostly outperforms KGEs in detecting plausible changes to the graph but has poor knowledge retention.","In summary, CFKGR connects two previously distinct areas, namely KG completion and counterfactual reasoning."],"url":"http://arxiv.org/abs/2403.06936v1","category":"cs.LG"}
{"created":"2024-03-11 17:17:01","title":"3D simulations of TRAPPIST-1e with varying CO2, CH4 and haze profiles","abstract":"Using a 3D General Circulation Model, the Unified Model, we present results from simulations of a tidally-locked TRAPPIST-1e with varying carbon dioxide CO2 and methane CH4 gas concentrations, and their corresponding prescribed spherical haze profiles. Our results show that the presence of CO2 leads to a warmer atmosphere globally due to its greenhouse effect, with the increase of surface temperature on the dayside surface reaching up to ~14.1 K, and on the nightside up to ~21.2 K. Increasing presence of CH4 first elevates the surface temperature on the dayside, followed by a decrease due to the balance of tropospheric warming and stratospheric cooling. A thin layer of haze, formed when the partial pressures of CH4 to CO2 (pCH4/pCO2) = 0.1, leads to a dayside warming of ~4.9K due to a change in the water vapour H2O distribution. The presence of a haze layer that formed beyond the ratio of 0.1 leads to dayside cooling. The haze reaches an optical threshold thickness when pCH4/pCO2 ~0.4 beyond which the dayside mean surface temperature does not vary much. The planet is more favourable to maintaining liquid water on the surface (mean surface temperature above 273.15 K) when pCO2 is high, pCH4 is low and the haze layer is thin. The effect of CO2, CH4 and haze on the dayside is similar to that for a rapidly-rotating planet. On the contrary, their effect on the nightside depends on the wind structure and the wind speed in the simulation.","sentences":["Using a 3D General Circulation Model, the Unified Model, we present results from simulations of a tidally-locked TRAPPIST-1e with varying carbon dioxide CO2 and methane CH4 gas concentrations, and their corresponding prescribed spherical haze profiles.","Our results show that the presence of CO2 leads to a warmer atmosphere globally due to its greenhouse effect, with the increase of surface temperature on the dayside surface reaching up to ~14.1 K, and on the nightside up to ~21.2 K. Increasing presence of CH4 first elevates the surface temperature on the dayside, followed by a decrease due to the balance of tropospheric warming and stratospheric cooling.","A thin layer of haze, formed when the partial pressures of CH4 to CO2 (pCH4/pCO2) = 0.1, leads to a dayside warming of ~4.9K due to a change in the water vapour H2O distribution.","The presence of a haze layer that formed beyond the ratio of 0.1 leads to dayside cooling.","The haze reaches an optical threshold thickness when pCH4/pCO2 ~0.4 beyond which the dayside mean surface temperature does not vary much.","The planet is more favourable to maintaining liquid water on the surface (mean surface temperature above 273.15 K) when pCO2 is high, pCH4 is low and the haze layer is thin.","The effect of CO2, CH4 and haze on the dayside is similar to that for a rapidly-rotating planet.","On the contrary, their effect on the nightside depends on the wind structure and the wind speed in the simulation."],"url":"http://arxiv.org/abs/2403.06928v1","category":"astro-ph.EP"}
{"created":"2024-03-11 17:15:58","title":"Effective multiband synthetic four-wave mixing by cascading quadratic processes","abstract":"Four wave mixing (FWM) is an important way to generate supercontinuum and frequency combs in the mid-infrared band. Here, we obtain simultaneous synthetic FWM in the visible and mid-infrared bands by cascading quadratic nonlinear processes in a periodically poled lithium niobate crystal (PPLN), which has a 110dB(at 3000nm) higher conversion efficiency than the FWM directly generated by third-order susceptibilities in bulk PPLN crystals. A general model of this process is developed that is in full agreement with the experimental verifications. The frequency difference between the new frequency components can be freely tuned by changing the frequency difference of the dual pump lasers. Furthermore, by increasing the conversion bandwidth and efficiency of the cascaded processes, it is feasible to generate frequency combs in three bands the visible, near-infrared and mid-infrared bands simultaneously through high-order cascaded processes. This work opens up a new avenue toward free-tuning multiband frequency comb generation with multi-octaves frequency spanning, which will have significant applications in fields such as mid-infrared gas sensing, lidar and precision spectroscopy.","sentences":["Four wave mixing (FWM) is an important way to generate supercontinuum and frequency combs in the mid-infrared band.","Here, we obtain simultaneous synthetic FWM in the visible and mid-infrared bands by cascading quadratic nonlinear processes in a periodically poled lithium niobate crystal (PPLN), which has a 110dB(at 3000nm) higher conversion efficiency than the FWM directly generated by third-order susceptibilities in bulk PPLN crystals.","A general model of this process is developed that is in full agreement with the experimental verifications.","The frequency difference between the new frequency components can be freely tuned by changing the frequency difference of the dual pump lasers.","Furthermore, by increasing the conversion bandwidth and efficiency of the cascaded processes, it is feasible to generate frequency combs in three bands the visible, near-infrared and mid-infrared bands simultaneously through high-order cascaded processes.","This work opens up a new avenue toward free-tuning multiband frequency comb generation with multi-octaves frequency spanning, which will have significant applications in fields such as mid-infrared gas sensing, lidar and precision spectroscopy."],"url":"http://arxiv.org/abs/2403.06927v1","category":"physics.optics"}
{"created":"2024-03-11 17:12:09","title":"Simplicity Bias of Transformers to Learn Low Sensitivity Functions","abstract":"Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with improved robustness; furthermore, it can also be used as an efficient intervention to further improve the robustness of transformers.","sentences":["Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive.","Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space.","In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities.","We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks.","We also show that low-sensitivity bias correlates with improved robustness; furthermore, it can also be used as an efficient intervention to further improve the robustness of transformers."],"url":"http://arxiv.org/abs/2403.06925v1","category":"cs.LG"}
{"created":"2024-03-11 17:11:31","title":"A unified diagrammatic approach in Liouville space to quantum transport for bosonic and fermionic reservoirs","abstract":"We present a diagrammatic approach to quantum transport based on a master equation formalism in Liouville space. It can be applied to linear and nonlinear transport in generic multi-level junctions coupled to bosonic or fermionic reservoirs and presents a convenient perturbation expansion in the strength of the coupling between the reservoirs and the junction. The Redfield theory is recovered at second order, with the partial and full secular master equations discussed. Analytical, approximate expressions are provided up to fourth order for the steady-state boson transport that generalize to multi-level systems the known formula for the low-temperature thermal conductance in the spin-boson model. The formalism is applied to the problem of heat transport in a qubit-resonator junction modeled by the quantum Rabi model. Nontrivial transport features emerge as a result of the interplay between the qubit-oscillator detuning and coupling strength. For quasi-degenerate spectra, nonvanishing steady-state coherences cause a suppression of the thermal conductance.","sentences":["We present a diagrammatic approach to quantum transport based on a master equation formalism in Liouville space.","It can be applied to linear and nonlinear transport in generic multi-level junctions coupled to bosonic or fermionic reservoirs and presents a convenient perturbation expansion in the strength of the coupling between the reservoirs and the junction.","The Redfield theory is recovered at second order, with the partial and full secular master equations discussed.","Analytical, approximate expressions are provided up to fourth order for the steady-state boson transport that generalize to multi-level systems the known formula for the low-temperature thermal conductance in the spin-boson model.","The formalism is applied to the problem of heat transport in a qubit-resonator junction modeled by the quantum Rabi model.","Nontrivial transport features emerge as a result of the interplay between the qubit-oscillator detuning and coupling strength.","For quasi-degenerate spectra, nonvanishing steady-state coherences cause a suppression of the thermal conductance."],"url":"http://arxiv.org/abs/2403.06923v1","category":"quant-ph"}
{"created":"2024-03-11 17:10:40","title":"Kantowski-Sachs and Bianchi III dynamics in $f\\left(Q\\right)$-gravity","abstract":"We explore the phase-space of homogeneous and anisotropic spacetimes within symmetric teleparallel $f(Q)$-gravity. Specifically, we consider the Kantowski-Sachs and locally rotational Bianchi III geometries to describe the physical space. By analyzing the phase-space, we reconstruct the cosmological history dictated by $f(Q)$-gravity and comment about the theory's viability. Our findings suggest that the free parameters of the connection must be constrained to eliminate nonlinear terms in the field equations. Consequently, new stationary points emerge, rendering the theory cosmologically viable. We identify the existence of anisotropic accelerated universes, which may correspond to the pre-inflationary epoch.","sentences":["We explore the phase-space of homogeneous and anisotropic spacetimes within symmetric teleparallel $f(Q)$-gravity.","Specifically, we consider the Kantowski-Sachs and locally rotational Bianchi III geometries to describe the physical space.","By analyzing the phase-space, we reconstruct the cosmological history dictated by $f(Q)$-gravity and comment about the theory's viability.","Our findings suggest that the free parameters of the connection must be constrained to eliminate nonlinear terms in the field equations.","Consequently, new stationary points emerge, rendering the theory cosmologically viable.","We identify the existence of anisotropic accelerated universes, which may correspond to the pre-inflationary epoch."],"url":"http://arxiv.org/abs/2403.06922v1","category":"gr-qc"}
{"created":"2024-03-11 17:08:55","title":"Synthesis of Robust Optimal Strategies in Weighted Timed Games","abstract":"Weighted Timed Games (WTG for short) are the most widely used model to describe controller synthesis problems involving real-time issues. The synthesized strategies rely on a perfect measure of time elapse, which is not realistic in practice. In order to produce strategies tolerant to timing imprecisions, we rely on a notion of robustness first introduced for timed automata. More precisely, WTGs are two-player zero-sum games played in a timed automaton equipped with integer weights in which one of the players, that we call Min, wants to reach a target location while minimising the cumulated weight. In this work, we equip the underlying timed automaton with a semantics depending on some parameter (representing the maximal possible perturbation) in which the opponent of Min can in addition perturb delays chosen by Min.   The robust value problem can then be stated as follows: given some threshold, determine whether there exists a positive perturbation and a strategy for Min ensuring to reach the target, with an accumulated weight below the threshold, whatever the opponent does.   We provide the first decidability result for this robust value problem by computing the robust value function, in a parametric way, for the class of divergent WTGs (introduced to obtain decidability of the (classical) value problem in WTGs without bounding the number of clocks). To this end, we show that the robust value is the fixpoint of some operators, as is classically done for value iteration algorithms. We then combine in a very careful way two representations: piecewise affine functions introduced in [1] to analyse WTGs, and shrunk Difference Bound Matrices considered in [29] to analyse robustness in timed automata. Last, we also study qualitative decision problems and close an open problem on robust reachability, showing it is EXPTIME-complete for general WTGs.","sentences":["Weighted Timed Games (WTG for short) are the most widely used model to describe controller synthesis problems involving real-time issues.","The synthesized strategies rely on a perfect measure of time elapse, which is not realistic in practice.","In order to produce strategies tolerant to timing imprecisions, we rely on a notion of robustness first introduced for timed automata.","More precisely, WTGs are two-player zero-sum games played in a timed automaton equipped with integer weights in which one of the players, that we call Min, wants to reach a target location while minimising the cumulated weight.","In this work, we equip the underlying timed automaton with a semantics depending on some parameter (representing the maximal possible perturbation) in which the opponent of Min can in addition perturb delays chosen by Min.   ","The robust value problem can then be stated as follows: given some threshold, determine whether there exists a positive perturbation and a strategy for Min ensuring to reach the target, with an accumulated weight below the threshold, whatever the opponent does.   ","We provide the first decidability result for this robust value problem by computing the robust value function, in a parametric way, for the class of divergent WTGs (introduced to obtain decidability of the (classical) value problem in WTGs without bounding the number of clocks).","To this end, we show that the robust value is the fixpoint of some operators, as is classically done for value iteration algorithms.","We then combine in a very careful way two representations: piecewise affine functions introduced in [1] to analyse WTGs, and shrunk Difference Bound Matrices considered in [29] to analyse robustness in timed automata.","Last, we also study qualitative decision problems and close an open problem on robust reachability, showing it is EXPTIME-complete for general WTGs."],"url":"http://arxiv.org/abs/2403.06921v1","category":"cs.GT"}
{"created":"2024-03-11 17:05:47","title":"Double Eisenstein series and modular forms of level $4$","abstract":"We study the $\\mthbb{Q}$-vector space generated by the double zeta values with character of conductor $4$. For this purpose, we define associated double Eisenstein series and investigate their relation with modular forms of level $4$.","sentences":["We study the $\\mthbb{Q}$-vector space generated by the double zeta values with character of conductor $4$. For this purpose, we define associated double Eisenstein series and investigate their relation with modular forms of level $4$."],"url":"http://arxiv.org/abs/2403.06917v1","category":"math.NT"}
{"created":"2024-03-11 17:03:04","title":"MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning","abstract":"Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism. Existing solutions attempt to distill lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise LLM's in-context learning performance. To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of distilling demonstrations through a two-stage training process, which includes meta-distillation pretraining and fine-tuning. Comprehensive evaluations across seven diverse ICL task partitions using decoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not only matches but often outperforms the Vanilla ICL as well as other state-of-the-art distillation models, while significantly reducing the computational demands. This innovation promises enhanced scalability and efficiency for the practical deployment of large language models","sentences":["Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations).","Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the self-attention mechanism.","Existing solutions attempt to distill lengthy demonstrations into compact vectors.","However, they often require task-specific retraining or compromise LLM's in-context learning performance.","To mitigate these challenges, we present Meta dEmonstratioN Distillation (MEND), where a language model learns to distill any lengthy demonstrations into vectors without retraining for a new downstream task.","We exploit the knowledge distillation to enhance alignment between MEND and LLM, achieving both efficiency and effectiveness simultaneously.","MEND is endowed with the meta-knowledge of distilling demonstrations through a two-stage training process, which includes meta-distillation pretraining and fine-tuning.","Comprehensive evaluations across seven diverse ICL task partitions using decoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess.","It not only matches but often outperforms the Vanilla ICL as well as other state-of-the-art distillation models, while significantly reducing the computational demands.","This innovation promises enhanced scalability and efficiency for the practical deployment of large language models"],"url":"http://arxiv.org/abs/2403.06914v1","category":"cs.CL"}
{"created":"2024-03-11 17:02:14","title":"Accurate and Interpretable Representation of Correlated Electronic Structure via Tensor Product Selected CI","abstract":"The task of computing wavefunctions that are accurate, yet simple enough mathematical objects to use for reasoning has long been a challenge in quantum chemistry. The difficulty in drawing physical conclusions from a wavefunction is often related to the generally large number of configurations with similar weights. In Tensor Product Selected CI, we use a locally correlated tensor product state basis, which has the effect of concentrating the weight of a state onto a smaller number of physically interpretable degrees of freedom. In this paper, we apply TPSCI to a series of three molecular systems ranging in separability, one of which is the first application of TPSCI to an open-shell bimetallic system. For each of these systems, we obtain accurate solutions to large active spaces, and analyze the resulting wavefunctions through a series of different approaches including (i) direct inspection of the TPS basis coefficients, (ii) construction of Bloch effective Hamiltonians, and (iii) computation of cluster correlation functions.","sentences":["The task of computing wavefunctions that are accurate, yet simple enough mathematical objects to use for reasoning has long been a challenge in quantum chemistry.","The difficulty in drawing physical conclusions from a wavefunction is often related to the generally large number of configurations with similar weights.","In Tensor Product Selected CI, we use a locally correlated tensor product state basis, which has the effect of concentrating the weight of a state onto a smaller number of physically interpretable degrees of freedom.","In this paper, we apply TPSCI to a series of three molecular systems ranging in separability, one of which is the first application of TPSCI to an open-shell bimetallic system.","For each of these systems, we obtain accurate solutions to large active spaces, and analyze the resulting wavefunctions through a series of different approaches including (i) direct inspection of the TPS basis coefficients, (ii) construction of Bloch effective Hamiltonians, and (iii) computation of cluster correlation functions."],"url":"http://arxiv.org/abs/2403.06913v1","category":"physics.chem-ph"}
{"created":"2024-03-11 17:01:24","title":"Homotopical commutative rings and bispans","abstract":"We prove that commutative semirings in a cartesian closed presentable $\\infty$-category, as defined by Groth, Gepner, and Nikolaus, are equivalent to product-preserving functors from the $(2,1)$-category of bispans of finite sets. In other words, we identify the latter as the Lawvere theory for commutative semirings in the $\\infty$-categorical context. This implies that connective commutative ring spectra can be described as grouplike product-preserving functors from bispans of finite sets to spaces. A key part of the proof is a localization result for $\\infty$-categories of spans, and more generally for $\\infty$-categories with factorization systems, that may be of independent interest.","sentences":["We prove that commutative semirings in a cartesian closed presentable $\\infty$-category, as defined by Groth, Gepner, and Nikolaus, are equivalent to product-preserving functors from the $(2,1)$-category of bispans of finite sets.","In other words, we identify the latter as the Lawvere theory for commutative semirings in the $\\infty$-categorical context.","This implies that connective commutative ring spectra can be described as grouplike product-preserving functors from bispans of finite sets to spaces.","A key part of the proof is a localization result for $\\infty$-categories of spans, and more generally for $\\infty$-categories with factorization systems, that may be of independent interest."],"url":"http://arxiv.org/abs/2403.06911v1","category":"math.CT"}
{"created":"2024-03-11 17:01:13","title":"Responsible Artificial Intelligence: A Structured Literature Review","abstract":"Our research endeavors to advance the concept of responsible artificial intelligence (AI), a topic of increasing importance within EU policy discussions. The EU has recently issued several publications emphasizing the necessity of trust in AI, underscoring the dual nature of AI as both a beneficial tool and a potential weapon. This dichotomy highlights the urgent need for international regulation. Concurrently, there is a need for frameworks that guide companies in AI development, ensuring compliance with such regulations. Our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of AI regulation, identifying focal areas for future attention. This paper introduces a comprehensive and, to our knowledge, the first unified definition of responsible AI. Through a structured literature review, we elucidate the current understanding of responsible AI. Drawing from this analysis, we propose an approach for developing a future framework centered around this concept. Our findings advocate for a human-centric approach to Responsible AI. This approach encompasses the implementation of AI methods with a strong emphasis on ethics, model explainability, and the pillars of privacy, security, and trust.","sentences":["Our research endeavors to advance the concept of responsible artificial intelligence (AI), a topic of increasing importance within EU policy discussions.","The EU has recently issued several publications emphasizing the necessity of trust in AI, underscoring the dual nature of AI as both a beneficial tool and a potential weapon.","This dichotomy highlights the urgent need for international regulation.","Concurrently, there is a need for frameworks that guide companies in AI development, ensuring compliance with such regulations.","Our research aims to assist lawmakers and machine learning practitioners in navigating the evolving landscape of AI regulation, identifying focal areas for future attention.","This paper introduces a comprehensive and, to our knowledge, the first unified definition of responsible AI.","Through a structured literature review, we elucidate the current understanding of responsible AI.","Drawing from this analysis, we propose an approach for developing a future framework centered around this concept.","Our findings advocate for a human-centric approach to Responsible AI.","This approach encompasses the implementation of AI methods with a strong emphasis on ethics, model explainability, and the pillars of privacy, security, and trust."],"url":"http://arxiv.org/abs/2403.06910v1","category":"cs.AI"}
{"created":"2024-03-11 16:57:20","title":"Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints","abstract":"Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series of cost-sensitive fraud detection scenarios with different teams of 9 synthetic fraud analysts, with individual work capacity constraints. The results demonstrate that our approach performs significantly better than the baselines in a wide array of scenarios, achieving an average 8.4% reduction in the misclassification cost.","sentences":["Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier.","Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints.","To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF).","DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations.","We test DeCCaF in a series of cost-sensitive fraud detection scenarios with different teams of 9 synthetic fraud analysts, with individual work capacity constraints.","The results demonstrate that our approach performs significantly better than the baselines in a wide array of scenarios, achieving an average 8.4% reduction in the misclassification cost."],"url":"http://arxiv.org/abs/2403.06906v1","category":"cs.LG"}
{"created":"2024-03-11 16:56:37","title":"FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in Human-Centric Tasks","abstract":"We propose FocusCLIP, integrating subject-level guidance--a specialized mechanism for target-specific supervision--into the CLIP framework for improved zero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP on both the vision and text sides. On the vision side, we incorporate ROI heatmaps emulating human visual attention mechanisms to emphasize subject-relevant image regions. On the text side, we introduce human pose descriptions to provide rich contextual information. For human-centric tasks, FocusCLIP is trained with images from the MPII Human Pose dataset. The proposed approach surpassed CLIP by an average of 8.61% across five previously unseen datasets covering three human-centric tasks. FocusCLIP achieved an average accuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement in activity recognition, a 14.78% improvement in age classification, and a 7.06% improvement in emotion recognition. Moreover, using our proposed single-shot LLM prompting strategy, we release a high-quality MPII Pose Descriptions dataset to encourage further research in multimodal learning for human-centric tasks. Furthermore, we also demonstrate the effectiveness of our subject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47% improvement over CLIP in zero-shot bird classification using the CUB dataset. Our findings emphasize the potential of integrating subject-level guidance with general pretraining methods for enhanced downstream performance.","sentences":["We propose FocusCLIP, integrating subject-level guidance--a specialized mechanism for target-specific supervision--into the CLIP framework for improved zero-shot transfer on human-centric tasks.","Our novel contributions enhance CLIP on both the vision and text sides.","On the vision side, we incorporate ROI heatmaps emulating human visual attention mechanisms to emphasize subject-relevant image regions.","On the text side, we introduce human pose descriptions to provide rich contextual information.","For human-centric tasks, FocusCLIP is trained with images from the MPII Human Pose dataset.","The proposed approach surpassed CLIP by an average of 8.61% across five previously unseen datasets covering three human-centric tasks.","FocusCLIP achieved an average accuracy of 33.65% compared to 25.04% by CLIP.","We observed a 3.98% improvement in activity recognition, a 14.78% improvement in age classification, and a 7.06% improvement in emotion recognition.","Moreover, using our proposed single-shot LLM prompting strategy, we release a high-quality MPII Pose Descriptions dataset to encourage further research in multimodal learning for human-centric tasks.","Furthermore, we also demonstrate the effectiveness of our subject-level supervision on non-human-centric tasks.","FocusCLIP shows a 2.47% improvement over CLIP in zero-shot bird classification using the CUB dataset.","Our findings emphasize the potential of integrating subject-level guidance with general pretraining methods for enhanced downstream performance."],"url":"http://arxiv.org/abs/2403.06904v1","category":"cs.CV"}
{"created":"2024-03-11 16:56:37","title":"Biphoton State Reconstruction via Phase Retrieval Methods","abstract":"The complete measurement of the quantum state of two correlated photons requires reconstructing the amplitude and phase of the biphoton wavefunction. We show how, by means of spatially resolved single photon detection, one can infer the spatial structure of bi-photons generated by spontaneous parametric down conversion. In particular, a spatially resolved analysis of the second-order correlations allows us to isolate the moduli of the pump and phasematching contributions to the two-photon states. When carrying this analysis on different propagation planes, the free space propagation of pump and phasematching is observed. This result allows, in principle, to gain enough information to reconstruct also the phase of pump and phasematching, and thus the full biphoton wavefunction. We show this in different examples where the pump is shaped as a superposition of orbital angular momentum modes or as a smooth amplitude with a phase structure with no singularities. The corresponding phase structure is retrieved employing maximum likelihood or genetic algorithms. These findings have potential applications in fast, efficient quantum state characterisation that does not require any control over the source.","sentences":["The complete measurement of the quantum state of two correlated photons requires reconstructing the amplitude and phase of the biphoton wavefunction.","We show how, by means of spatially resolved single photon detection, one can infer the spatial structure of bi-photons generated by spontaneous parametric down conversion.","In particular, a spatially resolved analysis of the second-order correlations allows us to isolate the moduli of the pump and phasematching contributions to the two-photon states.","When carrying this analysis on different propagation planes, the free space propagation of pump and phasematching is observed.","This result allows, in principle, to gain enough information to reconstruct also the phase of pump and phasematching, and thus the full biphoton wavefunction.","We show this in different examples where the pump is shaped as a superposition of orbital angular momentum modes or as a smooth amplitude with a phase structure with no singularities.","The corresponding phase structure is retrieved employing maximum likelihood or genetic algorithms.","These findings have potential applications in fast, efficient quantum state characterisation that does not require any control over the source."],"url":"http://arxiv.org/abs/2403.06905v1","category":"quant-ph"}
{"created":"2024-03-11 16:56:01","title":"Benign overfitting in leaky ReLU networks with moderate input dimension","abstract":"The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property. In contrast to prior work we do not require near orthogonality conditions on the training data: notably, for input dimension $d$ and training sample size $n$, while prior work shows asymptotically optimal error when $d = \\Omega(n^2 \\log n)$, here we require only $d = \\Omega\\left(n \\log \\frac{1}{\\epsilon}\\right)$ to obtain error within $\\epsilon$ of optimal.","sentences":["The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well.","We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task.","We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another.","We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs.","We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property.","In contrast to prior work we do not require near orthogonality conditions on the training data: notably, for input dimension $d$ and training sample size $n$, while prior work shows asymptotically optimal error when $d = \\Omega(n^2 \\log n)$, here we require only $d = \\Omega\\left(n \\log \\frac{1}{\\epsilon}\\right)$ to obtain error within $\\epsilon$ of optimal."],"url":"http://arxiv.org/abs/2403.06903v1","category":"cs.LG"}
{"created":"2024-03-11 16:55:19","title":"Deep adaptative spectral zoom for improved remote heart rate estimation","abstract":"Recent advances in remote heart rate measurement, motivated by data-driven approaches, have notably enhanced accuracy. However, these improvements primarily focus on recovering the rPPG signal, overlooking the implicit challenges of estimating the heart rate (HR) from the derived signal. While many methods employ the Fast Fourier Transform (FFT) for HR estimation, the performance of the FFT is inherently affected by a limited frequency resolution. In contrast, the Chirp-Z Transform (CZT), a generalization form of FFT, can refine the spectrum to the narrow-band range of interest for heart rate, providing improved frequential resolution and, consequently, more accurate estimation. This paper presents the advantages of employing the CZT for remote HR estimation and introduces a novel data-driven adaptive CZT estimator. The objective of our proposed model is to tailor the CZT to match the characteristics of each specific dataset sensor, facilitating a more optimal and accurate estimation of HR from the rPPG signal without compromising generalization across diverse datasets. This is achieved through a Sparse Matrix Optimization (SMO). We validate the effectiveness of our model through exhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE, and UBFC-rPPG employing both intra- and cross-database performance metrics. The results reveal outstanding heart rate estimation capabilities, establishing the proposed approach as a robust and versatile estimator for any rPPG method.","sentences":["Recent advances in remote heart rate measurement, motivated by data-driven approaches, have notably enhanced accuracy.","However, these improvements primarily focus on recovering the rPPG signal, overlooking the implicit challenges of estimating the heart rate (HR) from the derived signal.","While many methods employ the Fast Fourier Transform (FFT) for HR estimation, the performance of the FFT is inherently affected by a limited frequency resolution.","In contrast, the Chirp-Z Transform (CZT), a generalization form of FFT, can refine the spectrum to the narrow-band range of interest for heart rate, providing improved frequential resolution and, consequently, more accurate estimation.","This paper presents the advantages of employing the CZT for remote HR estimation and introduces a novel data-driven adaptive CZT estimator.","The objective of our proposed model is to tailor the CZT to match the characteristics of each specific dataset sensor, facilitating a more optimal and accurate estimation of HR from the rPPG signal without compromising generalization across diverse datasets.","This is achieved through a Sparse Matrix Optimization (SMO).","We validate the effectiveness of our model through exhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE, and UBFC-rPPG employing both intra- and cross-database performance metrics.","The results reveal outstanding heart rate estimation capabilities, establishing the proposed approach as a robust and versatile estimator for any rPPG method."],"url":"http://arxiv.org/abs/2403.06902v1","category":"cs.CV"}
{"created":"2024-03-11 16:54:44","title":"LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration","abstract":"The surgical environment imposes unique challenges to the intraoperative registration of organ shapes to their preoperatively-imaged geometry. Biomechanical model-based registration remains popular, while deep learning solutions remain limited due to the sparsity and variability of intraoperative measurements and the limited ground-truth deformation of an organ that can be obtained during the surgery. In this paper, we propose a novel \\textit{hybrid} registration approach that leverage a linearized iterative boundary reconstruction (LIBR) method based on linear elastic biomechanics, and use deep neural networks to learn its residual to the ground-truth deformation (LIBR+). We further formulate a dual-branch spline-residual graph convolutional neural network (SR-GCN) to assimilate information from sparse and variable intraoperative measurements and effectively propagate it through the geometry of the 3D organ. Experiments on a large intraoperative liver registration dataset demonstrated the consistent improvements achieved by LIBR+ in comparison to existing rigid, biomechnical model-based non-rigid, and deep-learning based non-rigid approaches to intraoperative liver registration.","sentences":["The surgical environment imposes unique challenges to the intraoperative registration of organ shapes to their preoperatively-imaged geometry.","Biomechanical model-based registration remains popular, while deep learning solutions remain limited due to the sparsity and variability of intraoperative measurements and the limited ground-truth deformation of an organ that can be obtained during the surgery.","In this paper, we propose a novel \\textit{hybrid} registration approach that leverage a linearized iterative boundary reconstruction (LIBR) method based on linear elastic biomechanics, and use deep neural networks to learn its residual to the ground-truth deformation (LIBR+).","We further formulate a dual-branch spline-residual graph convolutional neural network (SR-GCN) to assimilate information from sparse and variable intraoperative measurements and effectively propagate it through the geometry of the 3D organ.","Experiments on a large intraoperative liver registration dataset demonstrated the consistent improvements achieved by LIBR+ in comparison to existing rigid, biomechnical model-based non-rigid, and deep-learning based non-rigid approaches to intraoperative liver registration."],"url":"http://arxiv.org/abs/2403.06901v1","category":"eess.IV"}
{"created":"2024-03-11 16:52:17","title":"SFVInt: Simple, Fast and Generic Variable-Length Integer Decoding using Bit Manipulation Instructions","abstract":"The ubiquity of variable-length integers in data storage and communication necessitates efficient decoding techniques. In this paper, we present SFVInt, a simple and fast approach to decode the prevalent Little Endian Base-128 (LEB128) varints. Our approach, distilled into a mere 500 lines of code, effectively utilizes the Bit Manipulation Instruction Set 2 (BMI2) in modern Intel and AMD processors, achieving significant performance improvement while maintaining simplicity and avoiding overengineering. SFVInt, with its generic design, effectively processes both 32-bit and 64-bit unsigned integers using a unified code template, marking a significant leap forward in varint decoding efficiency. We thoroughly evaluate SFVInt's performance across various datasets and scenarios, demonstrating that it achieves up to a 2x increase in decoding speed when compared to varint decoding methods used in established frameworks like Facebook Folly and Google Protobuf.","sentences":["The ubiquity of variable-length integers in data storage and communication necessitates efficient decoding techniques.","In this paper, we present SFVInt, a simple and fast approach to decode the prevalent Little Endian Base-128 (LEB128) varints.","Our approach, distilled into a mere 500 lines of code, effectively utilizes the Bit Manipulation Instruction Set 2 (BMI2) in modern Intel and AMD processors, achieving significant performance improvement while maintaining simplicity and avoiding overengineering.","SFVInt, with its generic design, effectively processes both 32-bit and 64-bit unsigned integers using a unified code template, marking a significant leap forward in varint decoding efficiency.","We thoroughly evaluate SFVInt's performance across various datasets and scenarios, demonstrating that it achieves up to a 2x increase in decoding speed when compared to varint decoding methods used in established frameworks like Facebook Folly and Google Protobuf."],"url":"http://arxiv.org/abs/2403.06898v1","category":"cs.DB"}
{"created":"2024-03-11 16:50:58","title":"4-torsion classes in the integral cohomology of oriented Grassmannians","abstract":"We investigate the existence of 4-torsion in the integral cohomology of oriented Grassmannians. We prove a general criterion for the appearance of 4-torsion classes based on (twisted) Steenrod squares and show that there are many cases where this criterion is satisfied for minimal-degree anomalous classes, assuming a conjecture on the characteristic rank. We also establish the upper bound in the characteristic rank conjecture for oriented Grassmannians $\\tilde{Gr}_k(n)$, and prove the equality in the cases $k=5, n=2^t-1,2^t$ and $k=6, n=2^t$. This provides infinitely many examples of oriented Grassmannians having 4-torsion in their integral cohomology. On the way, we clarify the relation between minimal-degree anomalous classes and results of Stong on the height of the first Stiefel-Whitney class $w_1$ in the mod 2 cohomology of real Grassmannians, for which we give an independent proof. We also establish some bounds on torsion exponents for the integral cohomology of oriented flag manifolds. Based on these findings and further computational evidence, we formulate a conjectural relationship between the torsion exponent in the integral cohomology of homogeneous spaces and their deficiency.","sentences":["We investigate the existence of 4-torsion in the integral cohomology of oriented Grassmannians.","We prove a general criterion for the appearance of 4-torsion classes based on (twisted) Steenrod squares and show that there are many cases where this criterion is satisfied for minimal-degree anomalous classes, assuming a conjecture on the characteristic rank.","We also establish the upper bound in the characteristic rank conjecture for oriented Grassmannians $\\tilde{Gr}_k(n)$, and prove the equality in the cases $k=5, n=2^t-1,2^t$ and $k=6, n=2^t$. This provides infinitely many examples of oriented Grassmannians having 4-torsion in their integral cohomology.","On the way, we clarify the relation between minimal-degree anomalous classes and results of Stong on the height of the first Stiefel-Whitney class $w_1$ in the mod 2 cohomology of real Grassmannians, for which we give an independent proof.","We also establish some bounds on torsion exponents for the integral cohomology of oriented flag manifolds.","Based on these findings and further computational evidence, we formulate a conjectural relationship between the torsion exponent in the integral cohomology of homogeneous spaces and their deficiency."],"url":"http://arxiv.org/abs/2403.06897v1","category":"math.AT"}
{"created":"2024-03-11 16:49:56","title":"Scalable multi-qubit intrinsic gates in quantum dot arrays","abstract":"We study the multi-qubit quantum gates intrinsic to a general array of semiconductor quantum dots and investigate how they can be implemented in a scalable way. The intrinsic quantum gates refer to the class of natural-forming transformations in the qubit rotating-frame under direct exchange coupling, and can be recognized as the instruction set of a spin-qubit chip. Adopting an perturbative treatment, we can model intrinsic gates by first-order dynamics in the coupling strength. A general formalism is developed for identifying the multi-qubit intrinsic gates under arbitrary array connectivity. Factors influencing the fidelities of the multi-qubit intrinsic gates are discussed. The advantageous applications of intrinsic gates in quantum computing and quantum error correction are explored. We also propose a theoretical scheme to overcome the problem of inhomogeneous coupling using dynamical calibration of the connecting bonds. This scheme can be further combined with periodic dynamical decoupling for robust implementations of multi-qubit gates in large-scale quantum computers.","sentences":["We study the multi-qubit quantum gates intrinsic to a general array of semiconductor quantum dots and investigate how they can be implemented in a scalable way.","The intrinsic quantum gates refer to the class of natural-forming transformations in the qubit rotating-frame under direct exchange coupling, and can be recognized as the instruction set of a spin-qubit chip.","Adopting an perturbative treatment, we can model intrinsic gates by first-order dynamics in the coupling strength.","A general formalism is developed for identifying the multi-qubit intrinsic gates under arbitrary array connectivity.","Factors influencing the fidelities of the multi-qubit intrinsic gates are discussed.","The advantageous applications of intrinsic gates in quantum computing and quantum error correction are explored.","We also propose a theoretical scheme to overcome the problem of inhomogeneous coupling using dynamical calibration of the connecting bonds.","This scheme can be further combined with periodic dynamical decoupling for robust implementations of multi-qubit gates in large-scale quantum computers."],"url":"http://arxiv.org/abs/2403.06894v1","category":"quant-ph"}
{"created":"2024-03-11 16:45:19","title":"HiRA-Pro: High resolution alignment of multimodal spatio-temporal data: a process physics driven approach","abstract":"We present HiRA-Pro, a novel procedure to align, at high spatio-temporal resolutions, multimodal signals from real-world processes and systems that exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing machines. It is based on discerning and synchronizing the process signatures of salient kinematic and dynamic events in these disparate signals. HiRA-Pro addresses the challenge of aligning data with sub-millisecond phenomena, where traditional timestamp, external trigger, or clock-based alignment methods fall short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing context, where it aligns data from 13+ channels acquired during 3D-printing and milling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data is then voxelized to generate 0.25 second aligned data chunks that correspond to physical voxels on the produced part. The superiority of HiRA-Pro is further showcased through case studies in additive manufacturing, demonstrating improved machine learning-based predictive performance due to precise multimodal data alignment. Specifically, testing classification accuracies improved by almost 35% with the application of HiRA-Pro, even with limited data, allowing for precise localization of artifacts. The paper also provides a comprehensive discussion on the proposed method, its applications, and comparative qualitative analysis with a few other alignment methods. HiRA-Pro achieves temporal-spatial resolutions of 10-1000 us and 100 um in order to generate datasets that register with physical voxels on the 3D-printed and milled part. These resolutions are at least an order of magnitude finer than the existing alignment methods that employ individual timestamps, statistical correlations, or common clocks, which achieve precision of hundreds of milliseconds.","sentences":["We present HiRA-Pro, a novel procedure to align, at high spatio-temporal resolutions, multimodal signals from real-world processes and systems that exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing machines.","It is based on discerning and synchronizing the process signatures of salient kinematic and dynamic events in these disparate signals.","HiRA-Pro addresses the challenge of aligning data with sub-millisecond phenomena, where traditional timestamp, external trigger, or clock-based alignment methods fall short.","The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing context, where it aligns data from 13+ channels acquired during 3D-printing and milling operations on an Optomec-LENS MTS 500 hybrid machine.","The aligned data is then voxelized to generate 0.25 second aligned data chunks that correspond to physical voxels on the produced part.","The superiority of HiRA-Pro is further showcased through case studies in additive manufacturing, demonstrating improved machine learning-based predictive performance due to precise multimodal data alignment.","Specifically, testing classification accuracies improved by almost 35% with the application of HiRA-Pro, even with limited data, allowing for precise localization of artifacts.","The paper also provides a comprehensive discussion on the proposed method, its applications, and comparative qualitative analysis with a few other alignment methods.","HiRA-Pro achieves temporal-spatial resolutions of 10-1000 us and 100 um in order to generate datasets that register with physical voxels on the 3D-printed and milled part.","These resolutions are at least an order of magnitude finer than the existing alignment methods that employ individual timestamps, statistical correlations, or common clocks, which achieve precision of hundreds of milliseconds."],"url":"http://arxiv.org/abs/2403.06888v1","category":"physics.data-an"}
{"created":"2024-03-11 16:44:41","title":"Admissibility of the Structural Rules in the Sequent Calculus with Equality","abstract":"On the ground of a general theorem concerning the admissibility of the structural rules in sequent calculi with additional atomic rules, we develop a proof theoretic analysis for several extensions of the ${\\bf G3[mic]}$ sequent calculi with rules for equality, including the one originally proposed by H.Wang. In the classical case we relate our results with the semantic tableau method for first order logic with equality. In particular we establish that, for languages without function symbols, in Fitting's alternative semantic tableau method, strictness (which does not allow the repetition of equalities which are modified) can be imposed together with the orientation of the replacement of equals. A significant progress is made toward extending that result to languages with function symbols although whether that is possible or not remains to be settled. We also briefly consider systems that, in the classical case, are related to the semantic tableau method in which one can expand branches by adding identities at will, obtaining that also in that case strictness can be imposed. Furthermore we discuss to what extent the strengthened form of the nonlengthening property of Orevkov known to hold for the sequent calculi with the structural rules applies also to the present context.","sentences":["On the ground of a general theorem concerning the admissibility of the structural rules in sequent calculi with additional atomic rules, we develop a proof theoretic analysis for several extensions of the ${\\bf G3[mic]}$ sequent calculi with rules for equality, including the one originally proposed by H.Wang.","In the classical case we relate our results with the semantic tableau method for first order logic with equality.","In particular we establish that, for languages without function symbols, in Fitting's alternative semantic tableau method, strictness (which does not allow the repetition of equalities which are modified) can be imposed together with the orientation of the replacement of equals.","A significant progress is made toward extending that result to languages with function symbols although whether that is possible or not remains to be settled.","We also briefly consider systems that, in the classical case, are related to the semantic tableau method in which one can expand branches by adding identities at will, obtaining that also in that case strictness can be imposed.","Furthermore we discuss to what extent the strengthened form of the nonlengthening property of Orevkov known to hold for the sequent calculi with the structural rules applies also to the present context."],"url":"http://arxiv.org/abs/2403.06887v1","category":"math.LO"}
{"created":"2024-03-11 16:43:09","title":"QED Effects on Kerr-Newman Black Hole Shadows","abstract":"Incorporating first-order QED effects, we explore the shadows of Kerr-Newman black holes with a magnetic charge through the numerical backward ray-tracing method. Our investigation accounts for both the direct influence of the electromagnetic field on light rays and the distortion of the background spacetime metric due to QED corrections. We notice that the area of the shadow increases with the QED effect, mainly due to the fact that the photons move more slowly in the effective medium and become easier to be trapped by the black hole.","sentences":["Incorporating first-order QED effects, we explore the shadows of Kerr-Newman black holes with a magnetic charge through the numerical backward ray-tracing method.","Our investigation accounts for both the direct influence of the electromagnetic field on light rays and the distortion of the background spacetime metric due to QED corrections.","We notice that the area of the shadow increases with the QED effect, mainly due to the fact that the photons move more slowly in the effective medium and become easier to be trapped by the black hole."],"url":"http://arxiv.org/abs/2403.06886v1","category":"gr-qc"}
{"created":"2024-03-11 16:34:23","title":"Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning","abstract":"Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting wide minima that enhance generalization in RL models.","sentences":["Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards.","Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks.","Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes.","Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates.","Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition.","Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting wide minima that enhance generalization in RL models."],"url":"http://arxiv.org/abs/2403.06880v1","category":"cs.LG"}
{"created":"2024-03-11 16:31:25","title":"SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields for Robotic Inspection","abstract":"We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures. This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals. We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss. We use submapping to scale the system to large-scale environments captured over long trajectories. We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building. Website: https://ori-drs.github.io/projects/silvr/","sentences":["We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures.","This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals.","We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss.","We use submapping to scale the system to large-scale environments captured over long trajectories.","We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building.","Website: https://ori-drs.github.io/projects/silvr/"],"url":"http://arxiv.org/abs/2403.06877v1","category":"cs.RO"}
{"created":"2024-03-11 16:26:35","title":"COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification","abstract":"High-performing out-of-distribution (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models. In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality. We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a supervised model. The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind. COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection. We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority of experiments, e.g., improving detecting ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset. SHAP (feature contribution) analysis shows that different individual OOD measures are essential for various tasks, indicating that multiple OOD measures and combinations are needed to generalize. Additionally, we show that explicitly considering ID images that are incorrectly classified for the original (species) recognition task is important for constructing high-performing OOD detection methods and for practical applicability. The framework can easily be extended or adapted to other tasks and media modalities.","sentences":["High-performing out-of-distribution (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models.","In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality.","We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a supervised model.","The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind.","COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection.","We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority of experiments, e.g., improving detecting ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.","SHAP (feature contribution) analysis shows that different individual OOD measures are essential for various tasks, indicating that multiple OOD measures and combinations are needed to generalize.","Additionally, we show that explicitly considering ID images that are incorrectly classified for the original (species) recognition task is important for constructing high-performing OOD detection methods and for practical applicability.","The framework can easily be extended or adapted to other tasks and media modalities."],"url":"http://arxiv.org/abs/2403.06874v1","category":"cs.CV"}
{"created":"2024-03-11 16:24:26","title":"Last Iterate Convergence of Incremental Methods and Applications in Continual Learning","abstract":"Incremental gradient methods and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature. Yet, when it comes to their convergence guarantees, nonasymptotic (first-order or proximal) oracle complexity bounds have been obtained fairly recently, almost exclusively applying to the average iterate. Motivated by applications in continual learning, we obtain the first convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings. Our oracle complexity bounds for the last iterate nearly match (i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods. We further obtain generalizations of our results to weighted averaging of the iterates with increasing weights, which can be seen as interpolating between the last iterate and the average iterate guarantees. Additionally, we discuss how our results can be generalized to variants of studied incremental methods with permuted ordering of updates. Our results generalize last iterate guarantees for incremental methods compared to state of the art, as such results were previously known only for overparameterized linear models, which correspond to convex quadratic problems with infinitely many solutions.","sentences":["Incremental gradient methods and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature.","Yet, when it comes to their convergence guarantees, nonasymptotic (first-order or proximal) oracle complexity bounds have been obtained fairly recently, almost exclusively applying to the average iterate.","Motivated by applications in continual learning, we obtain the first convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings.","Our oracle complexity bounds for the last iterate nearly match (i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods.","We further obtain generalizations of our results to weighted averaging of the iterates with increasing weights, which can be seen as interpolating between the last iterate and the average iterate guarantees.","Additionally, we discuss how our results can be generalized to variants of studied incremental methods with permuted ordering of updates.","Our results generalize last iterate guarantees for incremental methods compared to state of the art, as such results were previously known only for overparameterized linear models, which correspond to convex quadratic problems with infinitely many solutions."],"url":"http://arxiv.org/abs/2403.06873v1","category":"math.OC"}
{"created":"2024-03-11 16:24:08","title":"Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents","abstract":"Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation. We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the hierarchical framework of MESc and compare them with their standalone performance on legal texts. We also study their intra-domain(legal) transfer learning capability and the impact of combining embeddings from their last layers in MESc. We test these methods and their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset. Our approach achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art methods.","sentences":["Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure.","Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation.","We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment prediction.","Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering.","Which we use in another set of transformer encoder layers to learn the inter-chunk representations.","We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the hierarchical framework of MESc and compare them with their standalone performance on legal texts.","We also study their intra-domain(legal) transfer learning capability and the impact of combining embeddings from their last layers in MESc.","We test these methods and their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset.","Our approach achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.06872v1","category":"cs.CL"}
{"created":"2024-03-11 16:23:42","title":"On the Generalization Ability of Unsupervised Pretraining","abstract":"Recent advances in unsupervised learning have shown that unsupervised pre-training, followed by fine-tuning, can improve model generalization. However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the fine-tuned model is lacking. Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and fine-tuning stage. To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during unsupervised pre-training to the subsequent fine-tuning phase, ultimately affecting the generalization capabilities of the fine-tuned model on downstream tasks. We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked Autoencoder pre-training with deep transformers, followed by fine-tuning on a binary classification task. Finally, inspired by our findings, we propose a novel regularization method during pre-training to further enhances the generalization of fine-tuned model. Overall, our results contribute to a better understanding of unsupervised pre-training and fine-tuning paradigm, and can shed light on the design of more effective pre-training algorithms.","sentences":["Recent advances in unsupervised learning have shown that unsupervised pre-training, followed by fine-tuning, can improve model generalization.","However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the fine-tuned model is lacking.","Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and fine-tuning stage.","To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during unsupervised pre-training to the subsequent fine-tuning phase, ultimately affecting the generalization capabilities of the fine-tuned model on downstream tasks.","We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked Autoencoder pre-training with deep transformers, followed by fine-tuning on a binary classification task.","Finally, inspired by our findings, we propose a novel regularization method during pre-training to further enhances the generalization of fine-tuned model.","Overall, our results contribute to a better understanding of unsupervised pre-training and fine-tuning paradigm, and can shed light on the design of more effective pre-training algorithms."],"url":"http://arxiv.org/abs/2403.06871v1","category":"cs.LG"}
{"created":"2024-03-11 16:22:41","title":"Learning with Noisy Foundation Models","abstract":"Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning.","sentences":["Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning.","However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks.","This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks.","Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different.","These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications.","We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently.","We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners.","We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation.","Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning."],"url":"http://arxiv.org/abs/2403.06869v1","category":"cs.LG"}
{"created":"2024-03-11 16:21:55","title":"Exploring Unique Quasinormal Modes of a Massive Scalar Field in Brane-World Scenarios","abstract":"We compute precise values of quasinormal modes of a massive scalar field in the background of the Schwarzschild-like brane-localised black holes. It is shown that the quasinormal spectrum of the massive field differs qualitatively from that previously known for other black hole models, due to the presence of two kinds of modes: those whose damping rate vanishes as the mass of the field $\\mu$ increases up to some critical value, and those whose real oscillation frequency vanishes at a certain value of $\\mu$. While the first type of modes, which are arbitrarily long-lived, are recognized in various four-dimensional backgrounds as quasi-resonances, the second type is a novel feature for asymptotically flat black holes. When $Re (\\omega)$ reaches zero, the fundamental mode disappears from the spectrum and the first overtone becomes the fundamental mode. We also demonstrate that quasi-resonances may not exist for brane-localised black holes immersed in $D\\geq 6$ - dimensional bulk.","sentences":["We compute precise values of quasinormal modes of a massive scalar field in the background of the Schwarzschild-like brane-localised black holes.","It is shown that the quasinormal spectrum of the massive field differs qualitatively from that previously known for other black hole models, due to the presence of two kinds of modes: those whose damping rate vanishes as the mass of the field $\\mu$ increases up to some critical value, and those whose real oscillation frequency vanishes at a certain value of $\\mu$. While the first type of modes, which are arbitrarily long-lived, are recognized in various four-dimensional backgrounds as quasi-resonances, the second type is a novel feature for asymptotically flat black holes.","When $Re (\\omega)$ reaches zero, the fundamental mode disappears from the spectrum and the first overtone becomes the fundamental mode.","We also demonstrate that quasi-resonances may not exist for brane-localised black holes immersed in $D\\geq 6$ - dimensional bulk."],"url":"http://arxiv.org/abs/2403.06867v1","category":"gr-qc"}
{"created":"2024-03-11 16:18:40","title":"On the Preservation of Africa's Cultural Heritage in the Age of Artificial Intelligence","abstract":"In this paper we delve into the historical evolution of data as a fundamental element in communication and knowledge transmission. The paper traces the stages of knowledge dissemination from oral traditions to the digital era, highlighting the significance of languages and cultural diversity in this progression. It also explores the impact of digital technologies on memory, communication, and cultural preservation, emphasizing the need for promoting a culture of the digital (rather than a digital culture) in Africa and beyond. Additionally, it discusses the challenges and opportunities presented by data biases in AI development, underscoring the importance of creating diverse datasets for equitable representation. We advocate for investing in data as a crucial raw material for fostering digital literacy, economic development, and, above all, cultural preservation in the digital age.","sentences":["In this paper we delve into the historical evolution of data as a fundamental element in communication and knowledge transmission.","The paper traces the stages of knowledge dissemination from oral traditions to the digital era, highlighting the significance of languages and cultural diversity in this progression.","It also explores the impact of digital technologies on memory, communication, and cultural preservation, emphasizing the need for promoting a culture of the digital (rather than a digital culture) in Africa and beyond.","Additionally, it discusses the challenges and opportunities presented by data biases in AI development, underscoring the importance of creating diverse datasets for equitable representation.","We advocate for investing in data as a crucial raw material for fostering digital literacy, economic development, and, above all, cultural preservation in the digital age."],"url":"http://arxiv.org/abs/2403.06865v1","category":"cs.CY"}
{"created":"2024-03-11 16:15:51","title":"Real-Time Simulated Avatar from Head-Mounted Sensors","abstract":"We present SimXR, a method for controlling a simulated avatar from information (headset pose and cameras) obtained from AR / VR headsets. Due to the challenging viewpoint of head-mounted cameras, the human body is often clipped out of view, making traditional image-based egocentric pose estimation challenging. On the other hand, headset poses provide valuable information about overall body motion, but lack fine-grained details about the hands and feet. To synergize headset poses with cameras, we control a humanoid to track headset movement while analyzing input images to decide body movement. When body parts are seen, the movements of hands and feet will be guided by the images; when unseen, the laws of physics guide the controller to generate plausible motion. We design an end-to-end method that does not rely on any intermediate representations and learns to directly map from images and headset poses to humanoid control signals. To train our method, we also propose a large-scale synthetic dataset created using camera configurations compatible with a commercially available VR headset (Quest 2) and show promising results on real-world captures. To demonstrate the applicability of our framework, we also test it on an AR headset with a forward-facing camera.","sentences":["We present SimXR, a method for controlling a simulated avatar from information (headset pose and cameras) obtained from AR / VR headsets.","Due to the challenging viewpoint of head-mounted cameras, the human body is often clipped out of view, making traditional image-based egocentric pose estimation challenging.","On the other hand, headset poses provide valuable information about overall body motion, but lack fine-grained details about the hands and feet.","To synergize headset poses with cameras, we control a humanoid to track headset movement while analyzing input images to decide body movement.","When body parts are seen, the movements of hands and feet will be guided by the images; when unseen, the laws of physics guide the controller to generate plausible motion.","We design an end-to-end method that does not rely on any intermediate representations and learns to directly map from images and headset poses to humanoid control signals.","To train our method, we also propose a large-scale synthetic dataset created using camera configurations compatible with a commercially available VR headset (Quest 2) and show promising results on real-world captures.","To demonstrate the applicability of our framework, we also test it on an AR headset with a forward-facing camera."],"url":"http://arxiv.org/abs/2403.06862v1","category":"cs.CV"}
{"created":"2024-03-11 16:12:58","title":"Estimation of parameters and local times in a discretely observed threshold diffusion model","abstract":"We consider a simple mean reverting diffusion process, with piecewise constant drift and diffusion coefficients, discontinuous at a fixed threshold. We discuss estimation of drift and diffusion parameters from discrete observations of the process, with a generalized moment estimator and a maximum likelihood estimator. We develop the asymptotic theory of the estimators when the time horizon of the observations goes to infinity, considering both cases of a fixed time lag (low frequency) and a vanishing time lag (high frequency) between consecutive observations. In the setting of low frequency observations and infinite time horizon we also study the convergence of three local time estimators, that are already known to converge to the local time in the setting of high frequency observations and fixed time horizon. We find that these estimators can behave differently, depending on the assumptions on the time lag between observations.","sentences":["We consider a simple mean reverting diffusion process, with piecewise constant drift and diffusion coefficients, discontinuous at a fixed threshold.","We discuss estimation of drift and diffusion parameters from discrete observations of the process, with a generalized moment estimator and a maximum likelihood estimator.","We develop the asymptotic theory of the estimators when the time horizon of the observations goes to infinity, considering both cases of a fixed time lag (low frequency) and a vanishing time lag (high frequency) between consecutive observations.","In the setting of low frequency observations and infinite time horizon we also study the convergence of three local time estimators, that are already known to converge to the local time in the setting of high frequency observations and fixed time horizon.","We find that these estimators can behave differently, depending on the assumptions on the time lag between observations."],"url":"http://arxiv.org/abs/2403.06858v1","category":"math.ST"}
{"created":"2024-03-11 16:12:34","title":"Development of a Reliable and Accessible Caregiving Language Model (CaLM)","abstract":"Unlike professional caregivers, family caregivers often assume this role without formal preparation or training. Because of this, there is an urgent need to enhance the capacity of family caregivers to provide quality care. Large language models can potentially be used as a foundation technology for supporting caregivers as educational tools or as adjunct to care. This study aimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a caregiving knowledge base, develop an accessible CaLM using a small FM that requires fewer computing resources, and evaluate the performance of the model compared to a large FM. We developed CaLM using the Retrieval Augmented Generation (RAG) framework combined with FM fine-tuning for improving the quality of FM answers by grounding the model on a caregiving knowledge base. We used two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B parameters) and larger FM GPT-3.5 as a benchmark. We developed the caregiving knowledge base by gathering various types of documents from the Internet. In this study, we focused on caregivers of individuals with Alzheimer's Disease Related Dementias. We evaluated the models' performance using the benchmark metrics commonly used in evaluating language models and their reliability to provide accurate references with the answers. The RAG framework improved the performance of all FMs used in this study across all measures. As expected, the large FM performed better than small FMs across all metrics. The most interesting result is that small fine-tuned FMs with RAG performed significantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2 small FM performed better than GPT 3.5 (even with RAG) in returning references with the answers. The study shows that reliable and accessible CaLM can be developed by using small FMs with a knowledge base specific to the caregiving domain.","sentences":["Unlike professional caregivers, family caregivers often assume this role without formal preparation or training.","Because of this, there is an urgent need to enhance the capacity of family caregivers to provide quality care.","Large language models can potentially be used as a foundation technology for supporting caregivers as educational tools or as adjunct to care.","This study aimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a caregiving knowledge base, develop an accessible CaLM using a small FM that requires fewer computing resources, and evaluate the performance of the model compared to a large FM.","We developed CaLM using the Retrieval Augmented Generation (RAG) framework combined with FM fine-tuning for improving the quality of FM answers by grounding the model on a caregiving knowledge base.","We used two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B parameters) and larger FM GPT-3.5 as a benchmark.","We developed the caregiving knowledge base by gathering various types of documents from the Internet.","In this study, we focused on caregivers of individuals with Alzheimer's Disease Related Dementias.","We evaluated the models' performance using the benchmark metrics commonly used in evaluating language models and their reliability to provide accurate references with the answers.","The RAG framework improved the performance of all FMs used in this study across all measures.","As expected, the large FM performed better than small FMs across all metrics.","The most interesting result is that small fine-tuned FMs with RAG performed significantly better than GPT 3.5 across all metrics.","The fine-tuned LLaMA-2 small FM performed better than GPT 3.5 (even with RAG) in returning references with the answers.","The study shows that reliable and accessible CaLM can be developed by using small FMs with a knowledge base specific to the caregiving domain."],"url":"http://arxiv.org/abs/2403.06857v1","category":"cs.CL"}
{"created":"2024-03-11 16:11:57","title":"Surface-aware Mesh Texture Synthesis with Pre-trained 2D CNNs","abstract":"Mesh texture synthesis is a key component in the automatic generation of 3D content. Existing learning-based methods have drawbacks -- either by disregarding the shape manifold during texture generation or by requiring a large number of different views to mitigate occlusion-related inconsistencies. In this paper, we present a novel surface-aware approach for mesh texture synthesis that overcomes these drawbacks by leveraging the pre-trained weights of 2D Convolutional Neural Networks (CNNs) with the same architecture, but with convolutions designed for 3D meshes. Our proposed network keeps track of the oriented patches surrounding each texel, enabling seamless texture synthesis and retaining local similarity to classical 2D convolutions with square kernels. Our approach allows us to synthesize textures that account for the geometric content of mesh surfaces, eliminating discontinuities and achieving comparable quality to 2D image synthesis algorithms. We compare our approach with state-of-the-art methods where, through qualitative and quantitative evaluations, we demonstrate that our approach is more effective for a variety of meshes and styles, while also producing visually appealing and consistent textures on meshes.","sentences":["Mesh texture synthesis is a key component in the automatic generation of 3D content.","Existing learning-based methods have drawbacks -- either by disregarding the shape manifold during texture generation or by requiring a large number of different views to mitigate occlusion-related inconsistencies.","In this paper, we present a novel surface-aware approach for mesh texture synthesis that overcomes these drawbacks by leveraging the pre-trained weights of 2D Convolutional Neural Networks (CNNs) with the same architecture, but with convolutions designed for 3D meshes.","Our proposed network keeps track of the oriented patches surrounding each texel, enabling seamless texture synthesis and retaining local similarity to classical 2D convolutions with square kernels.","Our approach allows us to synthesize textures that account for the geometric content of mesh surfaces, eliminating discontinuities and achieving comparable quality to 2D image synthesis algorithms.","We compare our approach with state-of-the-art methods where, through qualitative and quantitative evaluations, we demonstrate that our approach is more effective for a variety of meshes and styles, while also producing visually appealing and consistent textures on meshes."],"url":"http://arxiv.org/abs/2403.06855v1","category":"cs.GR"}
{"created":"2024-03-11 16:09:06","title":"Vibrational Dynamics and Spectroscopy of Water at Porous g-C$_{3}$N$_{4}$ and C$_{2}$N Materials","abstract":"In this work, the vibrational dynamics and spectroscopy of deuterated water molecules (D$_{2}$O) mimicking dense water layers at room temperature on the surfaces of two different C/N based materials with different N content and pore size, namely graphitic C$_{3}$N$_{4}$ (g-C$_{3}$N$_{4}$) and C$_{2}$N are studied using Ab Initio Molecular Dynamics (AIMD). In particular, Time-Dependent vibrational Sum-Frequency Generation spectra (TD-vSFG) of the OD modes and also time-averaged vSFG spectra and OD frequency distributions are computed.","sentences":["In this work, the vibrational dynamics and spectroscopy of deuterated water molecules (D$_{2}$O) mimicking dense water layers at room temperature on the surfaces of two different C/N based materials with different N content and pore size, namely graphitic C$_{3}$N$_{4}$ (g-C$_{3}$N$_{4}$) and C$_{2}$N are studied using Ab Initio Molecular Dynamics (AIMD).","In particular, Time-Dependent vibrational Sum-Frequency Generation spectra (TD-vSFG) of the OD modes and also time-averaged vSFG spectra and OD frequency distributions are computed."],"url":"http://arxiv.org/abs/2403.06853v1","category":"physics.chem-ph"}
{"created":"2024-03-11 16:07:50","title":"The Berge-F\u00fcredi conjecture on the chromatic index of hypergraphs with large hyperedges","abstract":"This paper is concerned with two conjectures which are intimately related. The first is a generalization to hypergraphs of Vizing's Theorem on the chromatic index of a graph and the second is the well-known conjecture of Erd\\H{o}s, Faber and Lov\\'asz which deals with the problem of coloring a family of cliques intersecting in at most one vertex. We are led to study a special class of uniform and linear hypergraphs for which a number of properties are established.","sentences":["This paper is concerned with two conjectures which are intimately related.","The first is a generalization to hypergraphs of Vizing's Theorem on the chromatic index of a graph and the second is the well-known conjecture of Erd\\H{o}s, Faber and Lov\\'asz which deals with the problem of coloring a family of cliques intersecting in at most one vertex.","We are led to study a special class of uniform and linear hypergraphs for which a number of properties are established."],"url":"http://arxiv.org/abs/2403.06850v1","category":"math.CO"}
{"created":"2024-03-11 16:03:35","title":"ExoCubed: A Riemann-Solver based Cubed-Sphere Dynamic Core for Planetary Atmospheres","abstract":"The computational fluid dynamics on a sphere is relevant to global simulations of geophysical fluid dynamics. Using the conventional spherical-polar (or lat-lon) grid results in a singularity at the poles, with orders of magnitude smaller cell sizes at the poles in comparison to the equator. To address this problem, we developed a general circulation model (dynamic core) with a gnomonic equiangular cubed-sphere configuration. This model is developed based on the Simulating Nonhydrostatic Atmospheres on Planets (SNAP) model, using a finite volume numerical scheme with a Riemann-solver-based dynamic core and the vertical implicit correction (VIC) scheme. This change of the horizontal configuration gives a 20-time acceleration of global simulations compared to the lat-lon grid with a similar number of cells at medium resolution. We presented standard tests ranging from 2D shallow-water models to 3D general circulation tests, including earth-like planets and shallow hot Jupiters, to validate the accuracy of the model. The method described in this article is generic to transform any existing finite-volume hydrodynamic model in the Cartesian geometry to the spherical geometry.","sentences":["The computational fluid dynamics on a sphere is relevant to global simulations of geophysical fluid dynamics.","Using the conventional spherical-polar (or lat-lon) grid results in a singularity at the poles, with orders of magnitude smaller cell sizes at the poles in comparison to the equator.","To address this problem, we developed a general circulation model (dynamic core) with a gnomonic equiangular cubed-sphere configuration.","This model is developed based on the Simulating Nonhydrostatic Atmospheres on Planets (SNAP) model, using a finite volume numerical scheme with a Riemann-solver-based dynamic core and the vertical implicit correction (VIC) scheme.","This change of the horizontal configuration gives a 20-time acceleration of global simulations compared to the lat-lon grid with a similar number of cells at medium resolution.","We presented standard tests ranging from 2D shallow-water models to 3D general circulation tests, including earth-like planets and shallow hot Jupiters, to validate the accuracy of the model.","The method described in this article is generic to transform any existing finite-volume hydrodynamic model in the Cartesian geometry to the spherical geometry."],"url":"http://arxiv.org/abs/2403.06844v1","category":"astro-ph.EP"}
{"created":"2024-03-11 16:03:35","title":"DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation","abstract":"World models have demonstrated superiority in autonomous driving, particularly in the generation of multi-view driving videos. However, significant challenges still exist in generating customized driving videos. In this paper, we propose DriveDreamer-2, which builds upon the framework of DriveDreamer and incorporates a Large Language Model (LLM) to generate user-defined driving videos. Specifically, an LLM interface is initially incorporated to convert a user's query into agent trajectories. Subsequently, a HDMap, adhering to traffic regulations, is generated based on the trajectories. Ultimately, we propose the Unified Multi-View Model to enhance temporal and spatial coherence in the generated driving videos. DriveDreamer-2 is the first world model to generate customized driving videos, it can generate uncommon driving videos (e.g., vehicles abruptly cut in) in a user-friendly manner. Besides, experimental results demonstrate that the generated videos enhance the training of driving perception methods (e.g., 3D detection and tracking). Furthermore, video generation quality of DriveDreamer-2 surpasses other state-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7, representing relative improvements of 30% and 50%.","sentences":["World models have demonstrated superiority in autonomous driving, particularly in the generation of multi-view driving videos.","However, significant challenges still exist in generating customized driving videos.","In this paper, we propose DriveDreamer-2, which builds upon the framework of DriveDreamer and incorporates a Large Language Model (LLM) to generate user-defined driving videos.","Specifically, an LLM interface is initially incorporated to convert a user's query into agent trajectories.","Subsequently, a HDMap, adhering to traffic regulations, is generated based on the trajectories.","Ultimately, we propose the Unified Multi-View Model to enhance temporal and spatial coherence in the generated driving videos.","DriveDreamer-2 is the first world model to generate customized driving videos, it can generate uncommon driving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.","Besides, experimental results demonstrate that the generated videos enhance the training of driving perception methods (e.g., 3D detection and tracking).","Furthermore, video generation quality of DriveDreamer-2 surpasses other state-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7, representing relative improvements of 30% and 50%."],"url":"http://arxiv.org/abs/2403.06845v1","category":"cs.CV"}
{"created":"2024-03-11 16:03:21","title":"Towards an educational tool for supporting neonatologists in the delivery room","abstract":"Nowadays, there is evidence that several factors may increase the risk, for an infant, to require stabilisation or resuscitation manoeuvres at birth. However, this risk factors are not completely known, and a universally applicable model for predicting high-risk situations is not available yet. Considering both these limitations and the fact that the need for resuscitation at birth is a rare event, periodic training of the healthcare personnel responsible for newborn caring in the delivery room is mandatory.   In this paper, we propose a machine learning approach for identifying risk factors and their impact on the birth event from real data, which can be used by personnel to progressively increase and update their knowledge. Our final goal will be the one of designing a user-friendly mobile application, able to improve the recognition rate and the planning of the appropriate interventions on high-risk patients.","sentences":["Nowadays, there is evidence that several factors may increase the risk, for an infant, to require stabilisation or resuscitation manoeuvres at birth.","However, this risk factors are not completely known, and a universally applicable model for predicting high-risk situations is not available yet.","Considering both these limitations and the fact that the need for resuscitation at birth is a rare event, periodic training of the healthcare personnel responsible for newborn caring in the delivery room is mandatory.   ","In this paper, we propose a machine learning approach for identifying risk factors and their impact on the birth event from real data, which can be used by personnel to progressively increase and update their knowledge.","Our final goal will be the one of designing a user-friendly mobile application, able to improve the recognition rate and the planning of the appropriate interventions on high-risk patients."],"url":"http://arxiv.org/abs/2403.06843v1","category":"cs.AI"}
{"created":"2024-03-11 16:01:07","title":"Inverse Garment and Pattern Modeling with a Differentiable Simulator","abstract":"The capability to generate simulation-ready garment models from 3D shapes of clothed humans will significantly enhance the interpretability of captured geometry of real garments, as well as their faithful reproduction in the virtual world. This will have notable impact on fields like shape capture in social VR, and virtual try-on in the fashion industry. To align with the garment modeling process standardized by the fashion industry as well as cloth simulation softwares, it is required to recover 2D patterns. This involves an inverse garment design problem, which is the focus of our work here: Starting with an arbitrary target garment geometry, our system estimates an animatable garment model by automatically adjusting its corresponding 2D template pattern, along with the material parameters of the physics-based simulation (PBS). Built upon a differentiable cloth simulator, the optimization process is directed towards minimizing the deviation of the simulated garment shape from the target geometry. Moreover, our produced patterns meet manufacturing requirements such as left-to-right-symmetry, making them suited for reverse garment fabrication. We validate our approach on examples of different garment types, and show that our method faithfully reproduces both the draped garment shape and the sewing pattern.","sentences":["The capability to generate simulation-ready garment models from 3D shapes of clothed humans will significantly enhance the interpretability of captured geometry of real garments, as well as their faithful reproduction in the virtual world.","This will have notable impact on fields like shape capture in social VR, and virtual try-on in the fashion industry.","To align with the garment modeling process standardized by the fashion industry as well as cloth simulation softwares, it is required to recover 2D patterns.","This involves an inverse garment design problem, which is the focus of our work here: Starting with an arbitrary target garment geometry, our system estimates an animatable garment model by automatically adjusting its corresponding 2D template pattern, along with the material parameters of the physics-based simulation (PBS).","Built upon a differentiable cloth simulator, the optimization process is directed towards minimizing the deviation of the simulated garment shape from the target geometry.","Moreover, our produced patterns meet manufacturing requirements such as left-to-right-symmetry, making them suited for reverse garment fabrication.","We validate our approach on examples of different garment types, and show that our method faithfully reproduces both the draped garment shape and the sewing pattern."],"url":"http://arxiv.org/abs/2403.06841v1","category":"cs.GR"}
{"created":"2024-03-11 16:01:05","title":"RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback","abstract":"Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities and reducing hallucinations.","sentences":["Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters.","Moreover, updating this knowledge incurs high training costs.","Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge.","The model can answer questions","it couldn't previously by retrieving knowledge relevant to the query.","This approach improves performance in certain scenarios for specific tasks.","However, if irrelevant texts are retrieved, it may impair model performance.","In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities.","Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities and reducing hallucinations."],"url":"http://arxiv.org/abs/2403.06840v1","category":"cs.CL"}
{"created":"2024-03-11 15:59:59","title":"ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts","abstract":"Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical. While existing research has proposed multiple detection tools, the automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge. Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of AC lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence.   Leveraging recent advancements in large language models (LLMs), we employ the state-of-the-art GPT-4 model and enhance it with a novel approach called ACFIX. The key insight is that we can mine common AC practices for major categories of code functionality and use them to guide LLMs in fixing code with similar functionality. To this end, ACFIX involves both offline and online phases. First, during the offline phase, ACFIX mines a tax- onomy of common Role-based Access Control (RBAC) practices from 344,251 on-chain contracts, categorizing 49 role-permission pairs from the top 1,000 pairs mined. Second, during the online phase, ACFIX tracks AC-related elements across the contract and uses this context information along with a Chain-of-Thought pipeline to guide LLMs in identifying the most appropriate role-permission pair for the subject contract and subsequently generating a suitable patch. This patch will then undergo a validity and effectiveness check. To evaluate ACFIX, we built the first benchmark dataset of 118 real-world AC vulnerabilities, and our evaluation revealed that ACFIX successfully repaired 94.92% of them. This represents a significant improvement compared to the baseline GPT-4, which achieved only 52.54%.","sentences":["Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical.","While existing research has proposed multiple detection tools, the automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge.","Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of AC lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence.   ","Leveraging recent advancements in large language models (LLMs), we employ the state-of-the-art GPT-4 model and enhance it with a novel approach called ACFIX.","The key insight is that we can mine common AC practices for major categories of code functionality and use them to guide LLMs in fixing code with similar functionality.","To this end, ACFIX involves both offline and online phases.","First, during the offline phase, ACFIX mines a tax- onomy of common Role-based Access Control (RBAC) practices from 344,251 on-chain contracts, categorizing 49 role-permission pairs from the top 1,000 pairs mined.","Second, during the online phase, ACFIX tracks AC-related elements across the contract and uses this context information along with a Chain-of-Thought pipeline to guide LLMs in identifying the most appropriate role-permission pair for the subject contract and subsequently generating a suitable patch.","This patch will then undergo a validity and effectiveness check.","To evaluate ACFIX, we built the first benchmark dataset of 118 real-world AC vulnerabilities, and our evaluation revealed that ACFIX successfully repaired 94.92% of them.","This represents a significant improvement compared to the baseline GPT-4, which achieved only 52.54%."],"url":"http://arxiv.org/abs/2403.06838v1","category":"cs.SE"}
{"created":"2024-03-11 15:59:35","title":"Stochastic Cortical Self-Reconstruction","abstract":"Magnetic resonance imaging (MRI) is critical for diagnosing neurodegenerative diseases, yet accurately assessing mild cortical atrophy remains a challenge due to its subtlety. Automated cortex reconstruction, paired with healthy reference ranges, aids in pinpointing pathological atrophy, yet their generalization is limited by biases from image acquisition and processing. We introduce the concept of stochastic cortical self-reconstruction (SCSR) that creates a subject-specific healthy reference by taking MRI-derived thicknesses as input and, therefore, implicitly accounting for potential confounders. SCSR randomly corrupts parts of the cortex and self-reconstructs them from the remaining information. Trained exclusively on healthy individuals, repeated self-reconstruction generates a stochastic reference cortex for assessing deviations from the norm. We present three implementations of this concept: XGBoost applied on parcels, and two autoencoders on vertex level -- one based on a multilayer perceptron and the other using a spherical U-Net. These models were trained on healthy subjects from the UK Biobank and subsequently evaluated across four public Alzheimer's datasets. Finally, we deploy the model on clinical in-house data, where deviation maps' high spatial resolution aids in discriminating between four types of dementia.","sentences":["Magnetic resonance imaging (MRI) is critical for diagnosing neurodegenerative diseases, yet accurately assessing mild cortical atrophy remains a challenge due to its subtlety.","Automated cortex reconstruction, paired with healthy reference ranges, aids in pinpointing pathological atrophy, yet their generalization is limited by biases from image acquisition and processing.","We introduce the concept of stochastic cortical self-reconstruction (SCSR) that creates a subject-specific healthy reference by taking MRI-derived thicknesses as input and, therefore, implicitly accounting for potential confounders.","SCSR randomly corrupts parts of the cortex and self-reconstructs them from the remaining information.","Trained exclusively on healthy individuals, repeated self-reconstruction generates a stochastic reference cortex for assessing deviations from the norm.","We present three implementations of this concept: XGBoost applied on parcels, and two autoencoders on vertex level -- one based on a multilayer perceptron and the other using a spherical U-Net.","These models were trained on healthy subjects from the UK Biobank and subsequently evaluated across four public Alzheimer's datasets.","Finally, we deploy the model on clinical in-house data, where deviation maps' high spatial resolution aids in discriminating between four types of dementia."],"url":"http://arxiv.org/abs/2403.06837v1","category":"cs.CV"}
{"created":"2024-03-11 15:56:17","title":"Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting","abstract":"Data scarcity and privacy concerns limit the availability of high-quality medical images for public use, which can be mitigated through medical image synthesis. However, current medical image synthesis methods often struggle to accurately capture the complexity of detailed anatomical structures and pathological conditions. To address these challenges, we propose a novel medical image synthesis model that leverages fine-grained image-text alignment and anatomy-pathology prompts to generate highly detailed and accurate synthetic medical images. Our method integrates advanced natural language processing techniques with image generative modeling, enabling precise alignment between descriptive text prompts and the synthesized images' anatomical and pathological details. The proposed approach consists of two key components: an anatomy-pathology prompting module and a fine-grained alignment-based synthesis module. The anatomy-pathology prompting module automatically generates descriptive prompts for high-quality medical images. To further synthesize high-quality medical images from the generated prompts, the fine-grained alignment-based synthesis module pre-defines a visual codebook for the radiology dataset and performs fine-grained alignment between the codebook and generated prompts to obtain key patches as visual clues, facilitating accurate image synthesis. We validate the superiority of our method through experiments on public chest X-ray datasets and demonstrate that our synthetic images preserve accurate semantic information, making them valuable for various medical applications.","sentences":["Data scarcity and privacy concerns limit the availability of high-quality medical images for public use, which can be mitigated through medical image synthesis.","However, current medical image synthesis methods often struggle to accurately capture the complexity of detailed anatomical structures and pathological conditions.","To address these challenges, we propose a novel medical image synthesis model that leverages fine-grained image-text alignment and anatomy-pathology prompts to generate highly detailed and accurate synthetic medical images.","Our method integrates advanced natural language processing techniques with image generative modeling, enabling precise alignment between descriptive text prompts and the synthesized images' anatomical and pathological details.","The proposed approach consists of two key components: an anatomy-pathology prompting module and a fine-grained alignment-based synthesis module.","The anatomy-pathology prompting module automatically generates descriptive prompts for high-quality medical images.","To further synthesize high-quality medical images from the generated prompts, the fine-grained alignment-based synthesis module pre-defines a visual codebook for the radiology dataset and performs fine-grained alignment between the codebook and generated prompts to obtain key patches as visual clues, facilitating accurate image synthesis.","We validate the superiority of our method through experiments on public chest X-ray datasets and demonstrate that our synthetic images preserve accurate semantic information, making them valuable for various medical applications."],"url":"http://arxiv.org/abs/2403.06835v1","category":"cs.CV"}
{"created":"2024-03-11 15:51:56","title":"Relativistic Roche problem for stars in precessing orbits around a spinning black hole","abstract":"Tidal disruptions of stars on the equatorial plane orbiting Kerr black holes have been widely studied. However thus far, there have been fewer studies of stars in inclined precessing orbits around a Kerr black hole. In this paper, by using tensor virial equations, we show the presence of possible resonances in these systems for typical physical parameters of black hole-neutron star binaries in close orbits or of a white dwarf/an ordinary star orbiting a supermassive black hole. This suggests the presence of a new instability before the tidal disruption limit is encountered in such systems.","sentences":["Tidal disruptions of stars on the equatorial plane orbiting Kerr black holes have been widely studied.","However thus far, there have been fewer studies of stars in inclined precessing orbits around a Kerr black hole.","In this paper, by using tensor virial equations, we show the presence of possible resonances in these systems for typical physical parameters of black hole-neutron star binaries in close orbits or of a white dwarf/an ordinary star orbiting a supermassive black hole.","This suggests the presence of a new instability before the tidal disruption limit is encountered in such systems."],"url":"http://arxiv.org/abs/2403.06834v1","category":"astro-ph.HE"}
{"created":"2024-03-11 15:48:43","title":"The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework","abstract":"The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework. This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations. In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and seven for MEMA), demonstrating its robustness and versatility. Besides, SNAG can not only function as a standalone model but also enhance other existing methods, providing stable performance improvements. Our code and data are available at: https://github.com/zjukg/SNAG.","sentences":["The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework.","This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations.","In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA).","Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs.","By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and seven for MEMA), demonstrating its robustness and versatility.","Besides, SNAG can not only function as a standalone model but also enhance other existing methods, providing stable performance improvements.","Our code and data are available at: https://github.com/zjukg/SNAG."],"url":"http://arxiv.org/abs/2403.06832v1","category":"cs.CL"}
{"created":"2024-03-11 15:48:17","title":"HDRTransDC: High Dynamic Range Image Reconstruction with Transformer Deformation Convolution","abstract":"High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image with realistic details by fusing multi-exposure Low Dynamic Range (LDR) images. Caused by large motion and severe under-/over-exposure among input LDR images, HDR imaging suffers from ghosting artifacts and fusion distortions. To address these critical issues, we propose an HDR Transformer Deformation Convolution (HDRTransDC) network to generate high-quality HDR images, which consists of the Transformer Deformable Convolution Alignment Module (TDCAM) and the Dynamic Weight Fusion Block (DWFB). To solve the ghosting artifacts, the proposed TDCAM extracts long-distance content similar to the reference feature in the entire non-reference features, which can accurately remove misalignment and fill the content occluded by moving objects. For the purpose of eliminating fusion distortions, we propose DWFB to spatially adaptively select useful information across frames to effectively fuse multi-exposed features. Extensive experiments show that our method quantitatively and qualitatively achieves state-of-the-art performance.","sentences":["High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image with realistic details by fusing multi-exposure Low Dynamic Range (LDR) images.","Caused by large motion and severe under-/over-exposure among input LDR images, HDR imaging suffers from ghosting artifacts and fusion distortions.","To address these critical issues, we propose an HDR Transformer Deformation Convolution (HDRTransDC) network to generate high-quality HDR images, which consists of the Transformer Deformable Convolution Alignment Module (TDCAM) and the Dynamic Weight Fusion Block (DWFB).","To solve the ghosting artifacts, the proposed TDCAM extracts long-distance content similar to the reference feature in the entire non-reference features, which can accurately remove misalignment and fill the content occluded by moving objects.","For the purpose of eliminating fusion distortions, we propose DWFB to spatially adaptively select useful information across frames to effectively fuse multi-exposed features.","Extensive experiments show that our method quantitatively and qualitatively achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.06831v1","category":"cs.CV"}
{"created":"2024-03-11 15:44:40","title":"Constructing Variables Using Classifiers as an Aid to Regression: An Empirical Assessment","abstract":"This paper proposes a method for the automatic creation of variables (in the case of regression) that complement the information contained in the initial input vector. The method works as a pre-processing step in which the continuous values of the variable to be regressed are discretized into a set of intervals which are then used to define value thresholds. Then classifiers are trained to predict whether the value to be regressed is less than or equal to each of these thresholds. The different outputs of the classifiers are then concatenated in the form of an additional vector of variables that enriches the initial vector of the regression problem. The implemented system can thus be considered as a generic pre-processing tool. We tested the proposed enrichment method with 5 types of regressors and evaluated it in 33 regression datasets. Our experimental results confirm the interest of the approach.","sentences":["This paper proposes a method for the automatic creation of variables (in the case of regression) that complement the information contained in the initial input vector.","The method works as a pre-processing step in which the continuous values of the variable to be regressed are discretized into a set of intervals which are then used to define value thresholds.","Then classifiers are trained to predict whether the value to be regressed is less than or equal to each of these thresholds.","The different outputs of the classifiers are then concatenated in the form of an additional vector of variables that enriches the initial vector of the regression problem.","The implemented system can thus be considered as a generic pre-processing tool.","We tested the proposed enrichment method with 5 types of regressors and evaluated it in 33 regression datasets.","Our experimental results confirm the interest of the approach."],"url":"http://arxiv.org/abs/2403.06829v1","category":"cs.LG"}
{"created":"2024-03-11 15:44:38","title":"NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning","abstract":"Navigating a nonholonomic robot in a cluttered environment requires extremely accurate perception and locomotion for collision avoidance. This paper presents NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and environment-invariant robot navigation solution. Leveraging a tightly-coupled perception-locomotion framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw points to a learned multi-frame distance space, avoiding error propagation from perception to control; 2) it is interpretable from an end-to-end model-based learning perspective, enabling provable convergence. The crux of NeuPAN is to solve a high-dimensional end-to-end mathematical model with various point-level constraints using the plug-and-play (PnP) proximal alternating-minimization network (PAN) with neurons in the loop. This allows NeuPAN to generate real-time, end-to-end, physically-interpretable motions directly from point clouds, which seamlessly integrates data- and knowledge-engines, where its network parameters are adjusted via back propagation. We evaluate NeuPAN on car-like robot, wheel-legged robot, and passenger autonomous vehicle, in both simulated and real-world environments. Experiments demonstrate that NeuPAN outperforms various benchmarks, in terms of accuracy, efficiency, robustness, and generalization capability across various environments, including the cluttered sandbox, office, corridor, and parking lot. We show that NeuPAN works well in unstructured environments with arbitrary-shape undetectable objects, making impassable ways passable.","sentences":["Navigating a nonholonomic robot in a cluttered environment requires extremely accurate perception and locomotion for collision avoidance.","This paper presents NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and environment-invariant robot navigation solution.","Leveraging a tightly-coupled perception-locomotion framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw points to a learned multi-frame distance space, avoiding error propagation from perception to control; 2) it is interpretable from an end-to-end model-based learning perspective, enabling provable convergence.","The crux of NeuPAN is to solve a high-dimensional end-to-end mathematical model with various point-level constraints using the plug-and-play (PnP) proximal alternating-minimization network (PAN) with neurons in the loop.","This allows NeuPAN to generate real-time, end-to-end, physically-interpretable motions directly from point clouds, which seamlessly integrates data- and knowledge-engines, where its network parameters are adjusted via back propagation.","We evaluate NeuPAN on car-like robot, wheel-legged robot, and passenger autonomous vehicle, in both simulated and real-world environments.","Experiments demonstrate that NeuPAN outperforms various benchmarks, in terms of accuracy, efficiency, robustness, and generalization capability across various environments, including the cluttered sandbox, office, corridor, and parking lot.","We show that NeuPAN works well in unstructured environments with arbitrary-shape undetectable objects, making impassable ways passable."],"url":"http://arxiv.org/abs/2403.06828v1","category":"cs.RO"}
{"created":"2024-03-11 15:43:28","title":"Orbital relaxation length from first-principles scattering calculations","abstract":"The orbital Hall effect generates a current of orbital angular momentum perpendicular to a charge current. Experiments suggest that this orbital current decays on a long length scale that is of the order of the spin flip diffusion length or longer. We examine this suggestion using first-principles quantum mechanical scattering calculations to study the decay of orbital currents injected from an orbitally-polarized lead into thermally disordered bulk systems of selected transition metals. We find that the decay occurs over only a few atomic layers. On this length scale the orbital current may be converted into a spin current if the spin Hall angle is sufficiently large, as for Pt. In Cu, Cr and V with small spin Hall angles, the conversion into a spin current is negligible in the bulk and significant conversion only occurs at interfaces.","sentences":["The orbital Hall effect generates a current of orbital angular momentum perpendicular to a charge current.","Experiments suggest that this orbital current decays on a long length scale that is of the order of the spin flip diffusion length or longer.","We examine this suggestion using first-principles quantum mechanical scattering calculations to study the decay of orbital currents injected from an orbitally-polarized lead into thermally disordered bulk systems of selected transition metals.","We find that the decay occurs over only a few atomic layers.","On this length scale the orbital current may be converted into a spin current if the spin Hall angle is sufficiently large, as for Pt.","In Cu, Cr and V with small spin Hall angles, the conversion into a spin current is negligible in the bulk and significant conversion only occurs at interfaces."],"url":"http://arxiv.org/abs/2403.06827v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 15:43:14","title":"In-context Exploration-Exploitation for Reinforcement Learning","abstract":"In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.","sentences":["In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization.","However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models.","We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning.","Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference.","Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time.","Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method."],"url":"http://arxiv.org/abs/2403.06826v1","category":"cs.LG"}
{"created":"2024-03-11 15:42:19","title":"Entanglement and logarithmic spirals in a quantum spin-1 many-body system with competing dimer and trimer interactions","abstract":"Spontaneous symmetry breaking (SSB) with type-B Goldstone modes is investigated in the macroscopically degenerate phase for a quantum spin-1 many-body system with competing dimer and trimer interactions. The SSB involves three distinct patterns. The first occurs at the dimer point, with the pattern from staggered ${\\rm SU}(3)$ to ${\\rm U}(1)\\times{\\rm U}(1)$. The second occurs at the trimer point, with the pattern from uniform ${\\rm SU}(3)$ to ${\\rm U}(1)\\times{\\rm U}(1)$. The third occurs in the dimer-trimer regime, with the pattern from uniform ${\\rm SU}(2)$ to ${\\rm U}(1)$. The number of type-B Goldstone modes is thus two, two and one for the three patterns, respectively. The ground state degeneracies arising from the three patterns are exponential with the system size, which may be recognized as sequences of integers relevant to self-similar logarithmic spirals. This in turn is attributed to the presence of an emergent symmetry operation tailored to a specific degenerate ground state. As a consequence, the residual entropy is non-zero, which measures the disorder present in a unit cell of highly degenerate ground state generated from a generalized highest weight state. An exact Schmidt decomposition exists for the highly degenerate ground states, thus exposing the self-similarities underlying an abstract fractal, described by the fractal dimension. The latter is extracted from performing a universal finite system-size scaling analysis of the entanglement entropy, which is identical to the number of type-B Goldstone modes. The model under investigation thus accommodates an exotic scale invariant quantum state of matter.","sentences":["Spontaneous symmetry breaking (SSB) with type-B Goldstone modes is investigated in the macroscopically degenerate phase for a quantum spin-1","many-body system with competing dimer and trimer interactions.","The SSB involves three distinct patterns.","The first occurs at the dimer point, with the pattern from staggered ${\\rm SU}(3)$ to ${\\rm U}(1)\\times{\\rm U}(1)$. The second occurs at the trimer point, with the pattern from uniform ${\\rm SU}(3)$ to ${\\rm U}(1)\\times{\\rm U}(1)$.","The third occurs in the dimer-trimer regime, with the pattern from uniform ${\\rm SU}(2)$ to","${\\rm U}(1)$.","The number of type-B Goldstone modes is thus two, two and one for the three patterns, respectively.","The ground state degeneracies arising from the three patterns are exponential with the system size, which may be recognized as sequences of integers relevant to self-similar logarithmic spirals.","This in turn is attributed to the presence of an emergent symmetry operation tailored to a specific degenerate ground state.","As a consequence, the residual entropy is non-zero, which measures the disorder present in a unit cell of highly degenerate ground state generated from a generalized highest weight state.","An exact Schmidt decomposition exists for the highly degenerate ground states, thus exposing the self-similarities underlying an abstract fractal, described by the fractal dimension.","The latter is extracted from performing a universal finite system-size scaling analysis of the entanglement entropy, which is identical to the number of type-B Goldstone modes.","The model under investigation thus accommodates an exotic scale invariant quantum state of matter."],"url":"http://arxiv.org/abs/2403.06825v1","category":"cond-mat.str-el"}
{"created":"2024-03-11 15:40:40","title":"Noise-induced transitions past the onset of a steady symmetry-breaking bifurcation: the case of the sudden expansion","abstract":"We consider fluid flows, governed by the Navier-Stokes equations, subject to a steady symmetry-breaking bifurcation and forced by a weak noise acting on a slow time scale. By generalizing the multiple-scale weakly nonlinear expansion technique employed in the literature for the response of the Duffing oscillator, we rigorously derive a stochastically forced Stuart-Landau equation for the dominant symmetry-breaking mode. The probability density function of the solution, and of the escape time from one attractor to the other, are then determined by solving the associated Fokker-Planck equation. The validity of this reduced order model is tested on the flow past a sudden expansion, for a given Reynolds number and different noise amplitudes. At a very low numerical cost, the statistics obtained from the amplitude equation accurately reproduce those of long-time direct numerical simulations.","sentences":["We consider fluid flows, governed by the Navier-Stokes equations, subject to a steady symmetry-breaking bifurcation and forced by a weak noise acting on a slow time scale.","By generalizing the multiple-scale weakly nonlinear expansion technique employed in the literature for the response of the Duffing oscillator, we rigorously derive a stochastically forced Stuart-Landau equation for the dominant symmetry-breaking mode.","The probability density function of the solution, and of the escape time from one attractor to the other, are then determined by solving the associated Fokker-Planck equation.","The validity of this reduced order model is tested on the flow past a sudden expansion, for a given Reynolds number and different noise amplitudes.","At a very low numerical cost, the statistics obtained from the amplitude equation accurately reproduce those of long-time direct numerical simulations."],"url":"http://arxiv.org/abs/2403.06824v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 15:40:36","title":"Transparent AI Disclosure Obligations: Who, What, When, Where, Why, How","abstract":"Advances in Generative Artificial Intelligence (AI) are resulting in AI-generated media output that is (nearly) indistinguishable from human-created content. This can drastically impact users and the media sector, especially given global risks of misinformation. While the currently discussed European AI Act aims at addressing these risks through Article 52's AI transparency obligations, its interpretation and implications remain unclear. In this early work, we adopt a participatory AI approach to derive key questions based on Article 52's disclosure obligations. We ran two workshops with researchers, designers, and engineers across disciplines (N=16), where participants deconstructed Article 52's relevant clauses using the 5W1H framework. We contribute a set of 149 questions clustered into five themes and 18 sub-themes. We believe these can not only help inform future legal developments and interpretations of Article 52, but also provide a starting point for Human-Computer Interaction research to (re-)examine disclosure transparency from a human-centered AI lens.","sentences":["Advances in Generative Artificial Intelligence (AI) are resulting in AI-generated media output that is (nearly) indistinguishable from human-created content.","This can drastically impact users and the media sector, especially given global risks of misinformation.","While the currently discussed European AI Act aims at addressing these risks through Article 52's AI transparency obligations, its interpretation and implications remain unclear.","In this early work, we adopt a participatory AI approach to derive key questions based on Article 52's disclosure obligations.","We ran two workshops with researchers, designers, and engineers across disciplines (N=16), where participants deconstructed Article 52's relevant clauses using the 5W1H framework.","We contribute a set of 149 questions clustered into five themes and 18 sub-themes.","We believe these can not only help inform future legal developments and interpretations of Article 52, but also provide a starting point for Human-Computer Interaction research to (re-)examine disclosure transparency from a human-centered AI lens."],"url":"http://arxiv.org/abs/2403.06823v1","category":"cs.HC"}
{"created":"2024-03-11 15:38:11","title":"Geometric and topological corrections to Schwarzschild black hole","abstract":"In this paper, we compute departures in the black hole thermodynamics induced by either geometric or topological corrections to general relativity. Specifically, we analyze the spherically symmetric spacetime solutions of two modified gravity scenarios with Lagrangians $\\mathcal{L}\\sim R^{1+\\epsilon}$ and $\\mathcal{L}\\sim R+\\epsilon\\, \\mathcal{G}^2$, where $\\mathcal{G}$ is the Euler density in four dimensions, while $ 0<\\epsilon\\ll 1$ measures the perturbation around the Hilbert-Einstein action. Accordingly, we find the expressions of the Bekenstein-Hawking entropy by the Penrose formula, and the black hole temperature and horizon of the obtained solutions. We then investigate the heat capacities in terms of the free parameters of the theories under study. In doing so, we show that healing the problem of negative heat capacities can be possible under particular choices of the free constants, albeit with limitations on the masses allowed for the black hole solutions.","sentences":["In this paper, we compute departures in the black hole thermodynamics induced by either geometric or topological corrections to general relativity.","Specifically, we analyze the spherically symmetric spacetime solutions of two modified gravity scenarios with Lagrangians $\\mathcal{L}\\sim R^{1+\\epsilon}$ and $\\mathcal{L}\\sim R+\\epsilon\\, \\mathcal{G}^2$, where $\\mathcal{G}$ is the Euler density in four dimensions, while $ 0<\\epsilon\\ll 1$ measures the perturbation around the Hilbert-Einstein action.","Accordingly, we find the expressions of the Bekenstein-Hawking entropy by the Penrose formula, and the black hole temperature and horizon of the obtained solutions.","We then investigate the heat capacities in terms of the free parameters of the theories under study.","In doing so, we show that healing the problem of negative heat capacities can be possible under particular choices of the free constants, albeit with limitations on the masses allowed for the black hole solutions."],"url":"http://arxiv.org/abs/2403.06819v1","category":"gr-qc"}
{"created":"2024-03-11 15:37:25","title":"User Tracking and Direction Estimation Codebook Design for IRS-Assisted mmWave Communication","abstract":"Future communication systems are envisioned to employ intelligent reflecting surfaces (IRSs) and the millimeter wave (mmWave) frequency band to provide reliable high-rate services. For mobile users, the time-varying channel state information (CSI) requires adequate adjustment of the reflection pattern of the IRS. We propose a novel codebook-based user tracking (UT) algorithm for IRS-assisted mmWave communication, allowing suitable reconfiguration of the IRS unit cell phase shifts, resulting in a high reflection gain. The presented algorithm acquires the direction information of the user based on a peak likelihood-based direction estimation. Using the direction information, the user's trajectory is extrapolated to proactively update the adopted codeword and adjust the IRS phase shift configuration accordingly. Furthermore, we conduct a theoretical analysis of the direction estimation error and utilize the obtained insights to design a codebook specifically optimized for direction estimation. Our numerical results reveal a lower direction estimation error of the proposed UT algorithm when employing our designed codebook compared to codebooks from the literature. Furthermore, the average achieved signal-to-noise ratio (SNR) as well as the average effective rate of the proposed UT algorithm are analyzed. The proposed UT algorithm requires only a low overhead for direction and channel estimation and avoids outdated IRS phase shifts. Furthermore, it is shown to outperform two benchmark schemes based on direct phase shift optimization and hierarchical codebook search, respectively, via computer simulations.","sentences":["Future communication systems are envisioned to employ intelligent reflecting surfaces (IRSs) and the millimeter wave (mmWave) frequency band to provide reliable high-rate services.","For mobile users, the time-varying channel state information (CSI) requires adequate adjustment of the reflection pattern of the IRS.","We propose a novel codebook-based user tracking (UT) algorithm for IRS-assisted mmWave communication, allowing suitable reconfiguration of the IRS unit cell phase shifts, resulting in a high reflection gain.","The presented algorithm acquires the direction information of the user based on a peak likelihood-based direction estimation.","Using the direction information, the user's trajectory is extrapolated to proactively update the adopted codeword and adjust the IRS phase shift configuration accordingly.","Furthermore, we conduct a theoretical analysis of the direction estimation error and utilize the obtained insights to design a codebook specifically optimized for direction estimation.","Our numerical results reveal a lower direction estimation error of the proposed UT algorithm when employing our designed codebook compared to codebooks from the literature.","Furthermore, the average achieved signal-to-noise ratio (SNR) as well as the average effective rate of the proposed UT algorithm are analyzed.","The proposed UT algorithm requires only a low overhead for direction and channel estimation and avoids outdated IRS phase shifts.","Furthermore, it is shown to outperform two benchmark schemes based on direct phase shift optimization and hierarchical codebook search, respectively, via computer simulations."],"url":"http://arxiv.org/abs/2403.06818v1","category":"eess.SP"}
{"created":"2024-03-11 15:34:57","title":"Are Targeted Messages More Effective?","abstract":"Graph neural networks (GNN) are deep learning architectures for graphs. Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   The core GNN architecture comes in two different versions. In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices. In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one. On the logical side, the two versions correspond to two fragments of first-order logic with counting that we call modal and guarded.   The question whether the two versions differ in their expressivity has been mostly overlooked in the GNN literature and has only been asked recently (Grohe, LICS'23). We answer this question here. It turns out that the answer is not as straightforward as one might expect. By proving that the modal and guarded fragment of first-order logic with counting have the same expressivity over labelled undirected graphs, we show that in a non-uniform setting the two GNN versions have the same expressivity. However, we also prove that in a uniform setting the second version is strictly more expressive.","sentences":["Graph neural networks (GNN) are deep learning architectures for graphs.","Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data.","It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages.","The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   ","The core GNN architecture comes in two different versions.","In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices.","In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one.","On the logical side, the two versions correspond to two fragments of first-order logic with counting that we call modal and guarded.   ","The question whether the two versions differ in their expressivity has been mostly overlooked in the GNN literature and has only been asked recently (Grohe, LICS'23).","We answer this question here.","It turns out that the answer is not as straightforward as one might expect.","By proving that the modal and guarded fragment of first-order logic with counting have the same expressivity over labelled undirected graphs, we show that in a non-uniform setting the two GNN versions have the same expressivity.","However, we also prove that in a uniform setting the second version is strictly more expressive."],"url":"http://arxiv.org/abs/2403.06817v1","category":"cs.LO"}
{"created":"2024-03-11 15:33:40","title":"\u03b5-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment","abstract":"Deep Brain Stimulation (DBS) stands as an effective intervention for alleviating the motor symptoms of Parkinson's disease (PD). Traditional commercial DBS devices are only able to deliver fixed-frequency periodic pulses to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS). However, they in general suffer from energy inefficiency and side effects, such as speech impairment. Recent research has focused on adaptive DBS (aDBS) to resolve the limitations of cDBS. Specifically, reinforcement learning (RL) based approaches have been developed to adapt the frequencies of the stimuli in order to achieve both energy efficiency and treatment efficacy. However, RL approaches in general require significant amount of training data and computational resources, making it intractable to integrate RL policies into real-time embedded systems as needed in aDBS. In contrast, contextual multi-armed bandits (CMAB) in general lead to better sample efficiency compared to RL. In this study, we propose a CMAB solution for aDBS. Specifically, we define the context as the signals capturing irregular neuronal firing activities in the BG regions (i.e., beta-band power spectral density), while each arm signifies the (discretized) pulse frequency of the stimulation. Moreover, an {\\epsilon}-exploring strategy is introduced on top of the classic Thompson sampling method, leading to an algorithm called {\\epsilon}-Neural Thompson sampling ({\\epsilon}-NeuralTS), such that the learned CMAB policy can better balance exploration and exploitation of the BG environment. The {\\epsilon}-NeuralTS algorithm is evaluated using a computation BG model that captures the neuronal activities in PD patients' brains. The results show that our method outperforms both existing cDBS methods and CMAB baselines.","sentences":["Deep Brain Stimulation (DBS) stands as an effective intervention for alleviating the motor symptoms of Parkinson's disease (PD).","Traditional commercial DBS devices are only able to deliver fixed-frequency periodic pulses to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS).","However, they in general suffer from energy inefficiency and side effects, such as speech impairment.","Recent research has focused on adaptive DBS (aDBS) to resolve the limitations of cDBS.","Specifically, reinforcement learning (RL) based approaches have been developed to adapt the frequencies of the stimuli in order to achieve both energy efficiency and treatment efficacy.","However, RL approaches in general require significant amount of training data and computational resources, making it intractable to integrate RL policies into real-time embedded systems as needed in aDBS.","In contrast, contextual multi-armed bandits (CMAB) in general lead to better sample efficiency compared to RL.","In this study, we propose a CMAB solution for aDBS.","Specifically, we define the context as the signals capturing irregular neuronal firing activities in the BG regions (i.e., beta-band power spectral density), while each arm signifies the (discretized) pulse frequency of the stimulation.","Moreover, an {\\epsilon}-exploring strategy is introduced on top of the classic Thompson sampling method, leading to an algorithm called {\\epsilon}-Neural Thompson sampling ({\\epsilon}-NeuralTS), such that the learned CMAB policy can better balance exploration and exploitation of the BG environment.","The {\\epsilon}-NeuralTS algorithm is evaluated using a computation BG model that captures the neuronal activities in PD patients' brains.","The results show that our method outperforms both existing cDBS methods and CMAB baselines."],"url":"http://arxiv.org/abs/2403.06814v1","category":"cs.LG"}
{"created":"2024-03-11 15:32:56","title":"Monotone Individual Fairness","abstract":"We revisit the problem of online learning with individual fairness, where an online learner strives to maximize predictive accuracy while ensuring that similar individuals are treated similarly. We first extend the frameworks of Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human auditors regarding fairness violations, as we consider auditing schemes that are capable of aggregating feedback from any number of auditors, using a rich class we term monotone aggregation functions. We then prove a characterization for such auditing schemes, practically reducing the analysis of auditing for individual fairness by multiple auditors to that of auditing by (instance-specific) single auditors. Using our generalized framework, we present an oracle-efficient algorithm achieving an upper bound frontier of $(\\mathcal{O}(T^{1/2+2b}),\\mathcal{O}(T^{3/4-b}))$ respectively for regret, number of fairness violations, for $0\\leq b \\leq 1/4$. We then study an online classification setting where label feedback is available for positively-predicted individuals only, and present an oracle-efficient algorithm achieving an upper bound frontier of $(\\mathcal{O}(T^{2/3+2b}),\\mathcal{O}(T^{5/6-b}))$ for regret, number of fairness violations, for $0\\leq b \\leq 1/6$. In both settings, our algorithms improve on the best known bounds for oracle-efficient algorithms. Furthermore, our algorithms offer significant improvements in computational efficiency, greatly reducing the number of required calls to an (offline) optimization oracle per round, to $\\tilde{\\mathcal{O}}(\\alpha^{-2})$ in the full information setting, and $\\tilde{\\mathcal{O}}(\\alpha^{-2} + k^2T^{1/3})$ in the partial information setting, where $\\alpha$ is the sensitivity for reporting fairness violations, and $k$ is the number of individuals in a round.","sentences":["We revisit the problem of online learning with individual fairness, where an online learner strives to maximize predictive accuracy while ensuring that similar individuals are treated similarly.","We first extend the frameworks of Gillen et al.","(2018);","Bechavod et al. (2020), which rely on feedback from human auditors regarding fairness violations, as we consider auditing schemes that are capable of aggregating feedback from any number of auditors, using a rich class we term monotone aggregation functions.","We then prove a characterization for such auditing schemes, practically reducing the analysis of auditing for individual fairness by multiple auditors to that of auditing by (instance-specific) single auditors.","Using our generalized framework, we present an oracle-efficient algorithm achieving an upper bound frontier of $(\\mathcal{O}(T^{1/2+2b}),\\mathcal{O}(T^{3/4-b}))$ respectively for regret, number of fairness violations, for $0\\leq b \\leq","1/4$. We then study an online classification setting where label feedback is available for positively-predicted individuals only, and present an oracle-efficient algorithm achieving an upper bound frontier of $(\\mathcal{O}(T^{2/3+2b}),\\mathcal{O}(T^{5/6-b}))$ for regret, number of fairness violations, for $0\\leq","b \\leq 1/6$.","In both settings, our algorithms improve on the best known bounds for oracle-efficient algorithms.","Furthermore, our algorithms offer significant improvements in computational efficiency, greatly reducing the number of required calls to an (offline) optimization oracle per round, to $\\tilde{\\mathcal{O}}(\\alpha^{-2})$ in the full information setting, and $\\tilde{\\mathcal{O}}(\\alpha^{-2} + k^2T^{1/3})$ in the partial information setting, where $\\alpha$ is the sensitivity for reporting fairness violations, and $k$ is the number of individuals in a round."],"url":"http://arxiv.org/abs/2403.06812v1","category":"cs.LG"}
{"created":"2024-03-11 15:26:34","title":"Multistep Consistency Models","abstract":"Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step.   In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas we show that a $\\infty$-step consistency model is a diffusion model.   Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 steps with consistency distillation. We also show that our method scales to a text-to-image diffusion model, generating samples that are very close to the quality of the original model.","sentences":["Diffusion models are relatively easy to train but require many steps to generate samples.","Consistency models are far more difficult to train, but generate samples in a single step.   ","In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality.","Specifically, a 1-step consistency model is a conventional consistency model whereas we show that a $\\infty$-step consistency model is a diffusion model.   ","Multistep Consistency Models work really well in practice.","By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits.","Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 steps with consistency distillation.","We also show that our method scales to a text-to-image diffusion model, generating samples that are very close to the quality of the original model."],"url":"http://arxiv.org/abs/2403.06807v1","category":"cs.LG"}
{"created":"2024-03-11 15:23:35","title":"On the Robustness of Lexicase Selection to Contradictory Objectives","abstract":"Lexicase and epsilon-lexicase selection are state of the art parent selection techniques for problems featuring multiple selection criteria. Originally, lexicase selection was developed for cases where these selection criteria are unlikely to be in conflict with each other, but preliminary work suggests it is also a highly effective many-objective optimization algorithm. However, to predict whether these results generalize, we must understand lexicase selection's performance on contradictory objectives. Prior work has shown mixed results on this question. Here, we develop theory identifying circumstances under which lexicase selection will succeed or fail to find a Pareto-optimal solution. To make this analysis tractable, we restrict our investigation to a theoretical problem with maximally contradictory objectives. Ultimately, we find that lexicase and epsilon-lexicase selection each have a region of parameter space where they are incapable of optimizing contradictory objectives. Outside of this region, however, they perform well despite the presence of contradictory objectives. Based on these findings, we propose theoretically-backed guidelines for parameter choice. Additionally, we identify other properties that may affect whether a many-objective optimization problem is a good fit for lexicase or epsilon-lexicase selection.","sentences":["Lexicase and epsilon-lexicase selection are state of the art parent selection techniques for problems featuring multiple selection criteria.","Originally, lexicase selection was developed for cases where these selection criteria are unlikely to be in conflict with each other, but preliminary work suggests it is also a highly effective many-objective optimization algorithm.","However, to predict whether these results generalize, we must understand lexicase selection's performance on contradictory objectives.","Prior work has shown mixed results on this question.","Here, we develop theory identifying circumstances under which lexicase selection will succeed or fail to find a Pareto-optimal solution.","To make this analysis tractable, we restrict our investigation to a theoretical problem with maximally contradictory objectives.","Ultimately, we find that lexicase and epsilon-lexicase selection each have a region of parameter space where they are incapable of optimizing contradictory objectives.","Outside of this region, however, they perform well despite the presence of contradictory objectives.","Based on these findings, we propose theoretically-backed guidelines for parameter choice.","Additionally, we identify other properties that may affect whether a many-objective optimization problem is a good fit for lexicase or epsilon-lexicase selection."],"url":"http://arxiv.org/abs/2403.06805v1","category":"cs.NE"}
{"created":"2024-03-11 15:23:11","title":"Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction","abstract":"We present Shape Non-rigid Kinematics (SNK), a novel zero-shot method for non-rigid shape matching that eliminates the need for extensive training or ground truth data. SNK operates on a single pair of shapes, and employs a reconstruction-based strategy using an encoder-decoder architecture, which deforms the source shape to closely match the target shape. During the process, an unsupervised functional map is predicted and converted into a point-to-point map, serving as a supervisory mechanism for the reconstruction. To aid in training, we have designed a new decoder architecture that generates smooth, realistic deformations. SNK demonstrates competitive results on traditional benchmarks, simplifying the shape-matching process without compromising accuracy. Our code can be found online: https://github.com/pvnieo/SNK","sentences":["We present Shape Non-rigid Kinematics (SNK), a novel zero-shot method for non-rigid shape matching that eliminates the need for extensive training or ground truth data.","SNK operates on a single pair of shapes, and employs a reconstruction-based strategy using an encoder-decoder architecture, which deforms the source shape to closely match the target shape.","During the process, an unsupervised functional map is predicted and converted into a point-to-point map, serving as a supervisory mechanism for the reconstruction.","To aid in training, we have designed a new decoder architecture that generates smooth, realistic deformations.","SNK demonstrates competitive results on traditional benchmarks, simplifying the shape-matching process without compromising accuracy.","Our code can be found online: https://github.com/pvnieo/SNK"],"url":"http://arxiv.org/abs/2403.06804v1","category":"cs.CV"}
{"created":"2024-03-11 15:22:28","title":"Data-Independent Operator: A Training-Free Artifact Representation Extractor for Generalizable Deepfake Detection","abstract":"Recently, the proliferation of increasingly realistic synthetic images generated by various generative adversarial networks has increased the risk of misuse. Consequently, there is a pressing need to develop a generalizable detector for accurately recognizing fake images. The conventional methods rely on generating diverse training sources or large pretrained models. In this work, we show that, on the contrary, the small and training-free filter is sufficient to capture more general artifact representations. Due to its unbias towards both the training and test sources, we define it as Data-Independent Operator (DIO) to achieve appealing improvements on unseen sources. In our framework, handcrafted filters and the randomly-initialized convolutional layer can be used as the training-free artifact representations extractor with excellent results. With the data-independent operator of a popular classifier, such as Resnet50, one could already reach a new state-of-the-art without bells and whistles. We evaluate the effectiveness of the DIO on 33 generation models, even DALLE and Midjourney. Our detector achieves a remarkable improvement of $13.3\\%$, establishing a new state-of-the-art performance. The DIO and its extension can serve as strong baselines for future methods. The code is available at \\url{https://github.com/chuangchuangtan/Data-Independent-Operator}.","sentences":["Recently, the proliferation of increasingly realistic synthetic images generated by various generative adversarial networks has increased the risk of misuse.","Consequently, there is a pressing need to develop a generalizable detector for accurately recognizing fake images.","The conventional methods rely on generating diverse training sources or large pretrained models.","In this work, we show that, on the contrary, the small and training-free filter is sufficient to capture more general artifact representations.","Due to its unbias towards both the training and test sources, we define it as Data-Independent Operator (DIO) to achieve appealing improvements on unseen sources.","In our framework, handcrafted filters and the randomly-initialized convolutional layer can be used as the training-free artifact representations extractor with excellent results.","With the data-independent operator of a popular classifier, such as Resnet50, one could already reach a new state-of-the-art without bells and whistles.","We evaluate the effectiveness of the DIO on 33 generation models, even DALLE and Midjourney.","Our detector achieves a remarkable improvement of $13.3\\%$, establishing a new state-of-the-art performance.","The DIO and its extension can serve as strong baselines for future methods.","The code is available at \\url{https://github.com/chuangchuangtan/Data-Independent-Operator}."],"url":"http://arxiv.org/abs/2403.06803v1","category":"cs.CV"}
{"created":"2024-03-11 15:19:52","title":"Joint Source-and-Channel Coding for Small Satellite Applications","abstract":"Small satellites are widely used today as cost effective means to perform Earth observation and other tasks that generate large amounts of high-dimensional data, such as multi-spectral imagery. These satellites typically operate in low earth orbit, which poses significant challenges for data transmission due to short contact times with ground stations, low bandwidth, and high packet loss probabilities. In this paper, we introduce JSCC-Sat, which applies joint source-and-channel coding using neural networks to provide efficient and robust transmission of compressed image data for satellite applications. We evaluate our mechanism against traditional transmission schemes with separate source and channel coding and demonstrate that it outperforms the existing approaches when applied to Earth observation data of the Sentinel-2 mission.","sentences":["Small satellites are widely used today as cost effective means to perform Earth observation and other tasks that generate large amounts of high-dimensional data, such as multi-spectral imagery.","These satellites typically operate in low earth orbit, which poses significant challenges for data transmission due to short contact times with ground stations, low bandwidth, and high packet loss probabilities.","In this paper, we introduce JSCC-Sat, which applies joint source-and-channel coding using neural networks to provide efficient and robust transmission of compressed image data for satellite applications.","We evaluate our mechanism against traditional transmission schemes with separate source and channel coding and demonstrate that it outperforms the existing approaches when applied to Earth observation data of the Sentinel-2 mission."],"url":"http://arxiv.org/abs/2403.06802v1","category":"cs.NI"}
{"created":"2024-03-11 15:17:45","title":"CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging","abstract":"Medical imaging plays a crucial role in diagnosis, with radiology reports serving as vital documentation. Automating report generation has emerged as a critical need to alleviate the workload of radiologists. While machine learning has facilitated report generation for 2D medical imaging, extending this to 3D has been unexplored due to computational complexity and data scarcity. We introduce the first method to generate radiology reports for 3D medical imaging, specifically targeting chest CT volumes. Given the absence of comparable methods, we establish a baseline using an advanced 3D vision encoder in medical imaging to demonstrate our method's effectiveness, which leverages a novel auto-regressive causal transformer. Furthermore, recognizing the benefits of leveraging information from previous visits, we augment CT2Rep with a cross-attention-based multi-modal fusion module and hierarchical memory, enabling the incorporation of longitudinal multimodal data. Access our code at: https://github.com/ibrahimethemhamamci/CT2Rep","sentences":["Medical imaging plays a crucial role in diagnosis, with radiology reports serving as vital documentation.","Automating report generation has emerged as a critical need to alleviate the workload of radiologists.","While machine learning has facilitated report generation for 2D medical imaging, extending this to 3D has been unexplored due to computational complexity and data scarcity.","We introduce the first method to generate radiology reports for 3D medical imaging, specifically targeting chest CT volumes.","Given the absence of comparable methods, we establish a baseline using an advanced 3D vision encoder in medical imaging to demonstrate our method's effectiveness, which leverages a novel auto-regressive causal transformer.","Furthermore, recognizing the benefits of leveraging information from previous visits, we augment CT2Rep with a cross-attention-based multi-modal fusion module and hierarchical memory, enabling the incorporation of longitudinal multimodal data.","Access our code at: https://github.com/ibrahimethemhamamci/CT2Rep"],"url":"http://arxiv.org/abs/2403.06801v1","category":"eess.IV"}
{"created":"2024-03-11 15:16:20","title":"Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification","abstract":"Remarkable successes were made in Medical Image Classification (MIC) recently, mainly due to wide applications of convolutional neural networks (CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity with raw data, raising serious concerns on network robustness. Although adversarial training (AT), in responding to malevolent AEs, was recognized as an effective approach to improve robustness, it was challenging to overcome generalization decline of networks caused by the AT. In this paper, in order to reserve high generalization while improving robustness, we proposed a dynamic perturbation-adaptive adversarial training (DPAAT) method, which placed AT in a dynamic learning environment to generate adaptive data-level perturbations and provided a dynamically updated criterion by loss information collections to handle the disadvantage of fixed perturbation sizes in conventional AT methods and the dependence on external transference. Comprehensive testing on dermatology HAM10000 dataset showed that the DPAAT not only achieved better robustness improvement and generalization preservation but also significantly enhanced mean average precision and interpretability on various CNNs, indicating its great potential as a generic adversarial training method on the MIC.","sentences":["Remarkable successes were made in Medical Image Classification (MIC) recently, mainly due to wide applications of convolutional neural networks (CNNs).","However, adversarial examples (AEs) exhibited imperceptible similarity with raw data, raising serious concerns on network robustness.","Although adversarial training (AT), in responding to malevolent AEs, was recognized as an effective approach to improve robustness, it was challenging to overcome generalization decline of networks caused by the AT.","In this paper, in order to reserve high generalization while improving robustness, we proposed a dynamic perturbation-adaptive adversarial training (DPAAT) method, which placed AT in a dynamic learning environment to generate adaptive data-level perturbations and provided a dynamically updated criterion by loss information collections to handle the disadvantage of fixed perturbation sizes in conventional AT methods and the dependence on external transference.","Comprehensive testing on dermatology HAM10000 dataset showed that the DPAAT not only achieved better robustness improvement and generalization preservation but also significantly enhanced mean average precision and interpretability on various CNNs, indicating its great potential as a generic adversarial training method on the MIC."],"url":"http://arxiv.org/abs/2403.06798v1","category":"eess.IV"}
{"created":"2024-03-11 15:15:50","title":"Leveraging Internal Representations of Model for Magnetic Image Classification","abstract":"Data generated by edge devices has the potential to train intelligent autonomous systems across various domains. Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations. This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available. We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity. Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results. This methodology presents a promising avenue for training machine learning models with minimal data.","sentences":["Data generated by edge devices has the potential to train intelligent autonomous systems across various domains.","Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations.","This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available.","We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity.","Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results.","This methodology presents a promising avenue for training machine learning models with minimal data."],"url":"http://arxiv.org/abs/2403.06797v1","category":"cs.LG"}
{"created":"2024-03-11 15:11:57","title":"Boosting Image Restoration via Priors from Pre-trained Models","abstract":"Pre-trained models with large-scale training data, such as CLIP and Stable Diffusion, have demonstrated remarkable performance in various high-level computer vision tasks such as image understanding and generation from language descriptions. Yet, their potential for low-level tasks such as image restoration remains relatively unexplored. In this paper, we explore such models to enhance image restoration. As off-the-shelf features (OSF) from pre-trained models do not directly serve image restoration, we propose to learn an additional lightweight module called Pre-Train-Guided Refinement Module (PTG-RM) to refine restoration results of a target restoration network with OSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying Enhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention (PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations, while PTG-CSA enhances spatial-channel attention for restoration-related learning. Extensive experiments demonstrate that PTG-RM, with its compact size ($<$1M parameters), effectively enhances restoration performance of various models across different tasks, including low-light enhancement, deraining, deblurring, and denoising.","sentences":["Pre-trained models with large-scale training data, such as CLIP and Stable Diffusion, have demonstrated remarkable performance in various high-level computer vision tasks such as image understanding and generation from language descriptions.","Yet, their potential for low-level tasks such as image restoration remains relatively unexplored.","In this paper, we explore such models to enhance image restoration.","As off-the-shelf features (OSF) from pre-trained models do not directly serve image restoration, we propose to learn an additional lightweight module called Pre-Train-Guided Refinement Module (PTG-RM) to refine restoration results of a target restoration network with OSF.","PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying Enhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention (PTG-CSA).","PTG-SVE enables optimal short- and long-range neural operations, while PTG-CSA enhances spatial-channel attention for restoration-related learning.","Extensive experiments demonstrate that PTG-RM, with its compact size ($<$1M parameters), effectively enhances restoration performance of various models across different tasks, including low-light enhancement, deraining, deblurring, and denoising."],"url":"http://arxiv.org/abs/2403.06793v1","category":"cs.CV"}
{"created":"2024-03-11 15:04:47","title":"Efficient dual-scale generalized Radon-Fourier transform detector family for long time coherent integration","abstract":"Long Time Coherent Integration (LTCI) aims to accumulate target energy through long time integration, which is an effective method for the detection of a weak target. However, for a moving target, defocusing can occur due to range migration (RM) and Doppler frequency migration (DFM). To address this issue, RM and DFM corrections are required in order to achieve a well-focused image for the subsequent detection. Since RM and DFM are induced by the same motion parameters, existing approaches such as the generalized Radon-Fourier transform (GRFT) or the keystone transform (KT)-matching filter process (MFP) adopt the same search space for the motion parameters in order to eliminate both effects, thus leading to large redundancy in computation. To this end, this paper first proposes a dual-scale decomposition of the target motion parameters, consisting of well designed coarse and fine motion parameters. Then, utilizing this decomposition, the joint correction of the RM and DFM effects is decoupled into a cascade procedure, first RM correction on the coarse search space and then DFM correction on the fine search spaces. As such, step size of the search space can be tailored to RM and DFM corrections, respectively, thus avoiding large redundant computation effectively. The resulting algorithms are called dual-scale GRFT (DS-GRFT) or dual-scale GRFT (DS-KTMFP) which provide comparable performance while achieving significant improvement in computational efficiency compared to standard GRFT (KT-MFP). Simulation experiments verify their effectiveness and efficiency.","sentences":["Long Time Coherent Integration (LTCI) aims to accumulate target energy through long time integration, which is an effective method for the detection of a weak target.","However, for a moving target, defocusing can occur due to range migration (RM) and Doppler frequency migration (DFM).","To address this issue, RM and DFM corrections are required in order to achieve a well-focused image for the subsequent detection.","Since RM and DFM are induced by the same motion parameters, existing approaches such as the generalized Radon-Fourier transform (GRFT) or the keystone transform (KT)-matching filter process (MFP) adopt the same search space for the motion parameters in order to eliminate both effects, thus leading to large redundancy in computation.","To this end, this paper first proposes a dual-scale decomposition of the target motion parameters, consisting of well designed coarse and fine motion parameters.","Then, utilizing this decomposition, the joint correction of the RM and DFM effects is decoupled into a cascade procedure, first RM correction on the coarse search space and then DFM correction on the fine search spaces.","As such, step size of the search space can be tailored to RM and DFM corrections, respectively, thus avoiding large redundant computation effectively.","The resulting algorithms are called dual-scale GRFT (DS-GRFT) or dual-scale GRFT (DS-KTMFP) which provide comparable performance while achieving significant improvement in computational efficiency compared to standard GRFT (KT-MFP).","Simulation experiments verify their effectiveness and efficiency."],"url":"http://arxiv.org/abs/2403.06788v1","category":"eess.SY"}
{"created":"2024-03-11 15:03:33","title":"Bjorken and threshold limit of a space-like structure function in the 2D $U(N)$ Gross-Neveu model","abstract":"In this note, we investigate a simple coordinate-space structure function $f_a(z^2m^2,\\lambda)$ in the 2D $U(N)$ Gross-Neveu model to next-to-leading order in the large $N$ expansion. We analytically perform the twist expansion in the Bjorken limit through double Mellin-representations. Hard and non-perturbative scaling functions at leading and next-to-leading powers are naturally generated in their Borel representations. At leading power (LP), the factorization formula is explicitly verified, and the issue of ``scale-dependency'' of perturbative and non-perturbative functions is explained naturally. At NLP, there are three series of non-perturbative functions and the related hard functions. The renormalon cancellation pattern between the hard function at LP and the non-perturbative function at NLP is verified explicitly. We also investigate the threshold expansion of the structure function and its relation to the twist expansion.","sentences":["In this note, we investigate a simple coordinate-space structure function $f_a(z^2m^2,\\lambda)$ in the 2D $U(N)$ Gross-Neveu model to next-to-leading order in the large $N$ expansion.","We analytically perform the twist expansion in the Bjorken limit through double Mellin-representations.","Hard and non-perturbative scaling functions at leading and next-to-leading powers are naturally generated in their Borel representations.","At leading power (LP), the factorization formula is explicitly verified, and the issue of ``scale-dependency'' of perturbative and non-perturbative functions is explained naturally.","At NLP, there are three series of non-perturbative functions and the related hard functions.","The renormalon cancellation pattern between the hard function at LP and the non-perturbative function at NLP is verified explicitly.","We also investigate the threshold expansion of the structure function and its relation to the twist expansion."],"url":"http://arxiv.org/abs/2403.06787v1","category":"hep-th"}
{"created":"2024-03-11 15:00:56","title":"Genetic Learning for Designing Sim-to-Real Data Augmentations","abstract":"Data augmentations are useful in closing the sim-to-real domain gap when training on synthetic data. This is because they widen the training data distribution, thus encouraging the model to generalize better to other domains. Many image augmentation techniques exist, parametrized by different settings, such as strength and probability. This leads to a large space of different possible augmentation policies. Some policies work better than others for overcoming the sim-to-real gap for specific datasets, and it is unclear why. This paper presents two different interpretable metrics that can be combined to predict how well a certain augmentation policy will work for a specific sim-to-real setting, focusing on object detection. We validate our metrics by training many models with different augmentation policies and showing a strong correlation with performance on real data. Additionally, we introduce GeneticAugment, a genetic programming method that can leverage these metrics to automatically design an augmentation policy for a specific dataset without needing to train a model.","sentences":["Data augmentations are useful in closing the sim-to-real domain gap when training on synthetic data.","This is because they widen the training data distribution, thus encouraging the model to generalize better to other domains.","Many image augmentation techniques exist, parametrized by different settings, such as strength and probability.","This leads to a large space of different possible augmentation policies.","Some policies work better than others for overcoming the sim-to-real gap for specific datasets, and it is unclear why.","This paper presents two different interpretable metrics that can be combined to predict how well a certain augmentation policy will work for a specific sim-to-real setting, focusing on object detection.","We validate our metrics by training many models with different augmentation policies and showing a strong correlation with performance on real data.","Additionally, we introduce GeneticAugment, a genetic programming method that can leverage these metrics to automatically design an augmentation policy for a specific dataset without needing to train a model."],"url":"http://arxiv.org/abs/2403.06786v1","category":"cs.CV"}
{"created":"2024-03-11 14:56:44","title":"Mass from an extrinsic point of view","abstract":"We express the $q$-th Gauss--Bonnet--Chern mass of an immersed submanifold of Euclidean space as a linear combination of two terms: the total $(2q)$-th mean curvature and the integral, over the entire manifold, of the inner product between the $(2q+1)$-th mean curvature vector and the position vector of the immersion. As a consequence, we obtain, for each $q$, a geometric inequality that holds whenever the positive mass theorem (for the $q$-th Gauss--Bonnet--Chern mass) holds.","sentences":["We express the $q$-th Gauss--Bonnet--Chern mass of an immersed submanifold of Euclidean space as a linear combination of two terms: the total $(2q)$-th mean curvature and the integral, over the entire manifold, of the inner product between the $(2q+1)$-th mean curvature vector and the position vector of the immersion.","As a consequence, we obtain, for each $q$, a geometric inequality that holds whenever the positive mass theorem (for the $q$-th Gauss--Bonnet--Chern mass) holds."],"url":"http://arxiv.org/abs/2403.06782v1","category":"math.DG"}
{"created":"2024-03-11 14:55:40","title":"Weak multiset sequenceability and weak BHR conjecture","abstract":"A subset $S$ of a group $(G,+)$ is $t$-weakly sequenceable if there is an ordering $(y_1, \\ldots, y_k)$ of its elements such that the partial sums~$s_0, s_1, \\ldots, s_k$, given by $s_0 = 0$ and $s_i = \\sum_{j=1}^i y_j$ for $1 \\leq i \\leq k$, satisfy $s_i \\neq s_j$ whenever and $1 \\leq |i-j|\\leq t$. In this paper, we consider the weak sequenceability problem on multisets. In particular, we are able to prove that a multiset $M=[a_1^{\\lambda_1},a_2^{\\lambda_2},\\dots,a_n^{\\lambda_n}]$ of non-identity elements of a generic group $G$ is $t$-weakly sequenceable whenever the underlying set $\\{a_1,a_2,\\dots,a_n\\}$ is sufficiently large (with respect to $t$) and the smallest prime divisor $p$ of $|G|$ is larger than $t$.   A related question is the one posed by the Buratti, Horak, and Rosa (briefly BHR) conjecture here considered again in the weak sense. Given a multiset $M$ and a walk $W$ in $Cay[G: \\pm M]$, we say that $W$ is a realization of $M$ if $\\Delta(W)=\\pm M$. Here we prove that a multiset $M=[a_1^{\\lambda_1},a_2^{\\lambda_2},\\dots,a_n^{\\lambda_n}]$ of non-identity elements of $G$ admits a realization $W=(w_0,\\dots,w_{\\ell})$ such that $w_i\\neq w_j$ whenever and $1 \\leq |i-j|\\leq t$ assuming that $|M|=\\lambda_1+\\lambda_2+\\dots+\\lambda_n$ is sufficiently large and the smallest prime divisor $p$ of $|G|$ is larger than $t(2t+1)$.","sentences":["A subset $S$ of a group $(G,+)$ is $t$-weakly sequenceable if there is an ordering $(y_1, \\ldots, y_k)$ of its elements such that the partial sums~$s_0, s_1, \\ldots, s_k$, given by $s_0 = 0$ and $s_i = \\sum_{j=1}^i y_j$ for $1 \\leq i \\leq k$, satisfy $s_i \\neq s_j$ whenever and $1 \\leq |i-j|\\leq t$.","In this paper, we consider the weak sequenceability problem on multisets.","In particular, we are able to prove that a multiset $M=[a_1^{\\lambda_1},a_2^{\\lambda_2},\\dots,a_n^{\\lambda_n}]$ of non-identity elements of a generic group $G$ is $t$-weakly sequenceable whenever the underlying set $\\{a_1,a_2,\\dots,a_n\\}$ is sufficiently large (with respect to $t$) and the smallest prime divisor $p$ of $|G|$ is larger than $t$.   A related question is the one posed by the Buratti, Horak, and Rosa (briefly BHR) conjecture here considered again in the weak sense.","Given a multiset $M$ and a walk $W$ in $Cay[G: \\pm M]$, we say that $W$ is a realization of $M$ if $\\Delta(W)=\\pm M$. Here we prove that a multiset $M=[a_1^{\\lambda_1},a_2^{\\lambda_2},\\dots,a_n^{\\lambda_n}]$ of non-identity elements of $G$ admits a realization $W=(w_0,\\dots,w_{\\ell})$ such that $w_i\\neq w_j$ whenever and $1 \\leq |i-j|\\leq t$ assuming that $|M|=\\lambda_1+\\lambda_2+\\dots+\\lambda_n$ is sufficiently large and the smallest prime divisor $p$ of $|G|$ is larger than $t(2t+1)$."],"url":"http://arxiv.org/abs/2403.06781v1","category":"math.CO"}
{"created":"2024-03-11 14:43:40","title":"FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation","abstract":"Subject-driven generation has garnered significant interest recently due to its ability to personalize text-to-image generation. Typical works focus on learning the new subject's private attributes. However, an important fact has not been taken seriously that a subject is not an isolated new concept but should be a specialization of a certain category in the pre-trained model. This results in the subject failing to comprehensively inherit the attributes in its category, causing poor attribute-related generations. In this paper, motivated by object-oriented programming, we model the subject as a derived class whose base class is its semantic category. This modeling enables the subject to inherit public attributes from its category while learning its private attributes from the user-provided example. Specifically, we propose a plug-and-play method, Subject-Derived regularization (SuDe). It constructs the base-derived class modeling by constraining the subject-driven generated images to semantically belong to the subject's category. Extensive experiments under three baselines and two backbones on various subjects show that our SuDe enables imaginative attribute-related generations while maintaining subject fidelity. Codes will be open sourced soon at FaceChain (https://github.com/modelscope/facechain).","sentences":["Subject-driven generation has garnered significant interest recently due to its ability to personalize text-to-image generation.","Typical works focus on learning the new subject's private attributes.","However, an important fact has not been taken seriously that a subject is not an isolated new concept but should be a specialization of a certain category in the pre-trained model.","This results in the subject failing to comprehensively inherit the attributes in its category, causing poor attribute-related generations.","In this paper, motivated by object-oriented programming, we model the subject as a derived class whose base class is its semantic category.","This modeling enables the subject to inherit public attributes from its category while learning its private attributes from the user-provided example.","Specifically, we propose a plug-and-play method, Subject-Derived regularization (SuDe).","It constructs the base-derived class modeling by constraining the subject-driven generated images to semantically belong to the subject's category.","Extensive experiments under three baselines and two backbones on various subjects show that our SuDe enables imaginative attribute-related generations while maintaining subject fidelity.","Codes will be open sourced soon at FaceChain (https://github.com/modelscope/facechain)."],"url":"http://arxiv.org/abs/2403.06775v1","category":"cs.CV"}
{"created":"2024-03-11 14:42:21","title":"Real Nullstellensatz for 2-step nilpotent Lie algebras","abstract":"We prove a noncommutative real Nullstellensatz for 2-step nilpotent Lie algebras that extends the classical, commutative real Nullstellensatz as follows: Instead of the real polynomial algebra $\\mathbb R[x_1, \\dots, x_d]$ we consider the universal enveloping *-algebra of a 2-step nilpotent real Lie algebra (i.e. the universal enveloping algebra of its complexification with the canonical *-involution). Evaluation at points of $\\mathbb R^d$ is then generalized to evaluation through integrable *-representations, which in this case are equivalent to filtered *-algebra morphisms from the universal enveloping *-algebra to a Weyl algebra. Our Nullstellensatz characterizes the common kernels of a set of such *-algebra morphisms as the real ideals of the universal enveloping *-algebra.","sentences":["We prove a noncommutative real Nullstellensatz for 2-step nilpotent Lie algebras that extends the classical, commutative real Nullstellensatz as follows: Instead of the real polynomial algebra $\\mathbb R[x_1, \\dots, x_d]$ we consider the universal enveloping *-algebra of a 2-step nilpotent real Lie algebra (i.e. the universal enveloping algebra of its complexification with the canonical *-involution).","Evaluation at points of $\\mathbb R^d$ is then generalized to evaluation through integrable *-representations, which in this case are equivalent to filtered *-algebra morphisms from the universal enveloping *-algebra to a Weyl algebra.","Our Nullstellensatz characterizes the common kernels of a set of such *-algebra morphisms as the real ideals of the universal enveloping *-algebra."],"url":"http://arxiv.org/abs/2403.06773v1","category":"math.AG"}
{"created":"2024-03-11 14:38:16","title":"Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning","abstract":"We investigate non-collaborative dialogue agents that must engage in tailored strategic planning for diverse users to secure a favorable agreement. This poses challenges for existing dialogue agents due to two main reasons: their inability to integrate user-specific characteristics into their strategic planning and their training paradigm's failure to produce strategic planners that can generalize to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users.","sentences":["We investigate non-collaborative dialogue agents that must engage in tailored strategic planning for diverse users to secure a favorable agreement.","This poses challenges for existing dialogue agents due to two main reasons: their inability to integrate user-specific characteristics into their strategic planning and their training paradigm's failure to produce strategic planners that can generalize to diverse users.","To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm.","Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users."],"url":"http://arxiv.org/abs/2403.06769v1","category":"cs.CL"}
{"created":"2024-03-11 14:37:57","title":"XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage","abstract":"Meta-learning, which pursues an effective initialization model, has emerged as a promising approach to handling unseen tasks. However, a limitation remains to be evident when a meta-learner tries to encompass a wide range of task distribution, e.g., learning across distinctive datasets or domains. Recently, a group of works has attempted to employ multiple model initializations to cover widely-ranging tasks, but they are limited in adaptively expanding initializations. We introduce XB-MAML, which learns expandable basis parameters, where they are linearly combined to form an effective initialization to a given task. XB-MAML observes the discrepancy between the vector space spanned by the basis and fine-tuned parameters to decide whether to expand the basis. Our method surpasses the existing works in the multi-domain meta-learning benchmarks and opens up new chances of meta-learning for obtaining the diverse inductive bias that can be combined to stretch toward the effective initialization for diverse unseen tasks.","sentences":["Meta-learning, which pursues an effective initialization model, has emerged as a promising approach to handling unseen tasks.","However, a limitation remains to be evident when a meta-learner tries to encompass a wide range of task distribution, e.g., learning across distinctive datasets or domains.","Recently, a group of works has attempted to employ multiple model initializations to cover widely-ranging tasks, but they are limited in adaptively expanding initializations.","We introduce XB-MAML, which learns expandable basis parameters, where they are linearly combined to form an effective initialization to a given task.","XB-MAML observes the discrepancy between the vector space spanned by the basis and fine-tuned parameters to decide whether to expand the basis.","Our method surpasses the existing works in the multi-domain meta-learning benchmarks and opens up new chances of meta-learning for obtaining the diverse inductive bias that can be combined to stretch toward the effective initialization for diverse unseen tasks."],"url":"http://arxiv.org/abs/2403.06768v1","category":"cs.LG"}
{"created":"2024-03-11 14:35:45","title":"ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model","abstract":"The internet has brought both benefits and harms to society. A prime example of the latter is misinformation, including conspiracy theories, which flood the web. Recent advances in natural language processing, particularly the emergence of large language models (LLMs), have improved the prospects of accurate misinformation detection. However, most LLM-based approaches to conspiracy theory detection focus only on binary classification and fail to account for the important relationship between misinformation and affective features (i.e., sentiment and emotions). Driven by a comprehensive analysis of conspiracy text that reveals its distinctive affective features, we propose ConspEmoLLM, the first open-source LLM that integrates affective information and is able to perform diverse tasks relating to conspiracy theories. These tasks include not only conspiracy theory detection, but also classification of theory type and detection of related discussion (e.g., opinions towards theories). ConspEmoLLM is fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset, which includes five tasks to support LLM instruction tuning and evaluation. We demonstrate that when applied to these tasks, ConspEmoLLM largely outperforms several open-source general domain LLMs and ChatGPT, as well as an LLM that has been fine-tuned using ConDID, but which does not use affective features. This project will be released on https://github.com/lzw108/ConspEmoLLM/.","sentences":["The internet has brought both benefits and harms to society.","A prime example of the latter is misinformation, including conspiracy theories, which flood the web.","Recent advances in natural language processing, particularly the emergence of large language models (LLMs), have improved the prospects of accurate misinformation detection.","However, most LLM-based approaches to conspiracy theory detection focus only on binary classification and fail to account for the important relationship between misinformation and affective features (i.e., sentiment and emotions).","Driven by a comprehensive analysis of conspiracy text that reveals its distinctive affective features, we propose ConspEmoLLM, the first open-source LLM that integrates affective information and is able to perform diverse tasks relating to conspiracy theories.","These tasks include not only conspiracy theory detection, but also classification of theory type and detection of related discussion (e.g., opinions towards theories).","ConspEmoLLM is fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset, which includes five tasks to support LLM instruction tuning and evaluation.","We demonstrate that when applied to these tasks, ConspEmoLLM largely outperforms several open-source general domain LLMs and ChatGPT, as well as an LLM that has been fine-tuned using ConDID, but which does not use affective features.","This project will be released on https://github.com/lzw108/ConspEmoLLM/."],"url":"http://arxiv.org/abs/2403.06765v1","category":"cs.CL"}
{"created":"2024-03-11 14:35:32","title":"An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models","abstract":"In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve a lower budget than that of a 7B-parameter model, while still maintaining superior performance. We believe FastV has practical values for deployment of LVLMs in edge devices and commercial models. Code is released at https://github.com/pkunlp-icler/FastV.","sentences":["In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA.","We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling.","To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones.","Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks.","The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient.","It can compress the FLOPs of a 13B-parameter model to achieve a lower budget than that of a 7B-parameter model, while still maintaining superior performance.","We believe FastV has practical values for deployment of LVLMs in edge devices and commercial models.","Code is released at https://github.com/pkunlp-icler/FastV."],"url":"http://arxiv.org/abs/2403.06764v1","category":"cs.CV"}
{"created":"2024-03-11 14:28:40","title":"ALaRM: Align Language Models via Hierarchical Rewards Modeling","abstract":"We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment. We release our code at https://ALaRM-fdu.github.io.","sentences":["We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences.","The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards.","This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks.","By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment.","We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over existing baselines.","Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment.","We release our code at https://ALaRM-fdu.github.io."],"url":"http://arxiv.org/abs/2403.06754v1","category":"cs.CL"}
{"created":"2024-03-11 14:14:52","title":"Shortcut Learning in Medical Image Segmentation","abstract":"Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights and methodologies for evaluating and overcoming this pervasive challenge and call for attention in the community for shortcuts in segmentation.","sentences":["Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set.","While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation.","We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy.","We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks.","In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models.","By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights and methodologies for evaluating and overcoming this pervasive challenge and call for attention in the community for shortcuts in segmentation."],"url":"http://arxiv.org/abs/2403.06748v1","category":"eess.IV"}
{"created":"2024-03-11 14:10:57","title":"ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation","abstract":"Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning. However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation. For this issue, this paper introduces an \\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction \\textbf{\\underline{T}}urning mechanism for \\textbf{\\underline{M}}ultilingual \\textbf{\\underline{N}}eural \\textbf{\\underline{M}}achine \\textbf{\\underline{T}}ranslation (\\model), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods. In this method, \\model automatically constructs a constrained template in the target side by adding trigger tokens ahead of the ground truth. Furthermore, trigger tokens can be arranged and combined freely to represent different task semantics, and they can be iteratively updated to maximize the label likelihood. Experiments are performed on WMT test sets with multiple metrics, and the experimental results demonstrate that \\model achieves substantially improved performance across multiple translation directions and reduce the off-target phenomena in the translation.","sentences":["Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning.","However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation.","For this issue, this paper introduces an \\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction \\textbf{\\underline{T}}urning mechanism for \\textbf{\\underline{M}}ultilingual \\textbf{\\underline{N}}eural \\textbf{\\underline{M}}achine \\textbf{\\underline{T}}ranslation (\\model), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods.","In this method, \\model automatically constructs a constrained template in the target side by adding trigger tokens ahead of the ground truth.","Furthermore, trigger tokens can be arranged and combined freely to represent different task semantics, and they can be iteratively updated to maximize the label likelihood.","Experiments are performed on WMT test sets with multiple metrics, and the experimental results demonstrate that \\model achieves substantially improved performance across multiple translation directions and reduce the off-target phenomena in the translation."],"url":"http://arxiv.org/abs/2403.06745v1","category":"cs.CL"}
{"created":"2024-03-11 14:08:10","title":"Search for charged-lepton-flavour violating $\u03bc\u03c4qt$ interactions in top-quark production and decay in $pp$ collisions at $\\sqrt{s}= 13$ TeV with the ATLAS detector at the LHC","abstract":"A search for charged-lepton-flavour violating $\\mu\\tau qt$ ($q=u,c$) interactions is presented, considering both top-quark production and decay. The data analysed correspond to 140 $\\textrm{fb}^{-1}$ of proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}= $13 TeV recorded with the ATLAS detector at the Large Hadron Collider. The analysis targets events containing two muons with the same electric charge, a hadronically decaying $\\tau$-lepton and at least one jet, with exactly one $b$-tagged jet, produced by a $\\mu\\tau qt$ interaction. Agreement with the Standard Model expectation within $1.6\\sigma$ is observed, and limits are set at the 95% CL on the charged-lepton-flavour violation branching ratio of $\\mathcal{B}(t \\to \\mu\\tau q) < 8.7 \\times 10^{-7}$. An Effective Field Theory interpretation is performed yielding 95% CL limits on Wilson coefficients, dependent on the flavour of the associated light quark and the Lorentz structure of the coupling. These range from $|c_{\\mathsf{lequ}}^{3(2313)}| / \\Lambda^{2} < 0.10\\textrm{ TeV}^{-2}$ for $\\mu\\tau ut$ to $|c_{\\mathsf{ lequ}}^{1(2323)}| / \\Lambda^{2} < 1.8\\textrm{ TeV}^{-2}$ for $\\mu\\tau ct$. An additional interpretation is performed for scalar leptoquark production inducing charged lepton flavour violation, with fixed inter-generational couplings. Upper limits on leptoquark coupling strengths are set at the 95% CL, ranging from $\\lambda^{\\textrm{LQ}} = $1.3 to $\\lambda^{\\textrm{LQ}} = $3.7 for leptoquark masses between 0.5 and 2.0 TeV.","sentences":["A search for charged-lepton-flavour violating $\\mu\\tau qt$ ($q=u,c$) interactions is presented, considering both top-quark production and decay.","The data analysed correspond to 140 $\\textrm{fb}^{-1}$ of proton-proton collisions at a centre-of-mass energy of $\\sqrt{s}= $13 TeV recorded with the ATLAS detector at the Large Hadron Collider.","The analysis targets events containing two muons with the same electric charge, a hadronically decaying $\\tau$-lepton and at least one jet, with exactly one $b$-tagged jet, produced by a $\\mu\\tau qt$ interaction.","Agreement with the Standard Model expectation within $1.6\\sigma$ is observed, and limits are set at the 95% CL on the charged-lepton-flavour violation branching ratio of $\\mathcal{B}(t \\to \\mu\\tau q) < 8.7 \\times 10^{-7}$.","An Effective Field Theory interpretation is performed yielding 95% CL limits on Wilson coefficients, dependent on the flavour of the associated light quark and the Lorentz structure of the coupling.","These range from $|c_{\\mathsf{lequ}}^{3(2313)}| / \\Lambda^{2} < 0.10\\textrm{ TeV}^{-2}$ for $\\mu\\tau ut$ to $|c_{\\mathsf{ lequ}}^{1(2323)}| / \\Lambda^{2} < 1.8\\textrm{ TeV}^{-2}$ for $\\mu\\tau ct$.","An additional interpretation is performed for scalar leptoquark production inducing charged lepton flavour violation, with fixed inter-generational couplings.","Upper limits on leptoquark coupling strengths are set at the 95% CL, ranging from $\\lambda^{\\textrm{LQ}} = $1.3 to $\\lambda^{\\textrm{LQ}} = $3.7 for leptoquark masses between 0.5 and 2.0 TeV."],"url":"http://arxiv.org/abs/2403.06742v1","category":"hep-ex"}
{"created":"2024-03-11 14:07:53","title":"Distribution-Aware Data Expansion with Diffusion Models","abstract":"The scale and quality of a dataset significantly impact the performance of deep models. However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor. To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models. Current data expansion methods encompass image transformation-based and synthesis-based methods. The transformation-based methods introduce only local variations, resulting in poor diversity. While image synthesis-based methods can create entirely new content, significantly enhancing informativeness. However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples. In this paper, we propose DistDiff, an effective data expansion framework based on the distribution-aware diffusion model. DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance. We demonstrate its ability to generate distribution-consistent samples, achieving substantial improvements in data expansion tasks. Specifically, without additional training, DistDiff achieves a 30.7% improvement in accuracy across six image datasets compared to the model trained on original datasets and a 9.8% improvement compared to the state-of-the-art diffusion-based method. Our code is available at https://github.com/haoweiz23/DistDiff","sentences":["The scale and quality of a dataset significantly impact the performance of deep models.","However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor.","To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models.","Current data expansion methods encompass image transformation-based and synthesis-based methods.","The transformation-based methods introduce only local variations, resulting in poor diversity.","While image synthesis-based methods can create entirely new content, significantly enhancing informativeness.","However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples.","In this paper, we propose DistDiff, an effective data expansion framework based on the distribution-aware diffusion model.","DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance.","We demonstrate its ability to generate distribution-consistent samples, achieving substantial improvements in data expansion tasks.","Specifically, without additional training, DistDiff achieves a 30.7% improvement in accuracy across six image datasets compared to the model trained on original datasets and a 9.8% improvement compared to the state-of-the-art diffusion-based method.","Our code is available at https://github.com/haoweiz23/DistDiff"],"url":"http://arxiv.org/abs/2403.06741v1","category":"cs.CV"}
{"created":"2024-03-11 14:03:36","title":"V3D: Video Diffusion Models are Effective 3D Generators","abstract":"Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at https://github.com/heheyas/V3D","sentences":["Automatic 3D generation has recently attracted widespread attention.","Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data.","Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation.","To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator.","Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image.","With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes.","Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views.","Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency.","Our code is available at https://github.com/heheyas/V3D"],"url":"http://arxiv.org/abs/2403.06738v1","category":"cs.CV"}
{"created":"2024-03-11 13:57:05","title":"Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback","abstract":"Research on generative models to produce human-aligned / human-preferred outputs has seen significant recent contributions. Between text and image-generative models, we narrowed our focus to text-based generative models, particularly to produce captions for images that align with human preferences. In this research, we explored a potential method to amplify the performance of the Deep Neural Network Model to generate captions that are preferred by humans. This was achieved by integrating Supervised Learning and Reinforcement Learning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel loss function that is capable of optimizing the model based on human feedback is introduced. In this paper, we provide a concise sketch of our approach and results, hoping to contribute to the ongoing advances in the field of human-aligned generative AI models.","sentences":["Research on generative models to produce human-aligned / human-preferred outputs has seen significant recent contributions.","Between text and image-generative models, we narrowed our focus to text-based generative models, particularly to produce captions for images that align with human preferences.","In this research, we explored a potential method to amplify the performance of the Deep Neural Network Model to generate captions that are preferred by humans.","This was achieved by integrating Supervised Learning and Reinforcement Learning with Human Feedback (RLHF) using the Flickr8k dataset.","Also, a novel loss function that is capable of optimizing the model based on human feedback is introduced.","In this paper, we provide a concise sketch of our approach and results, hoping to contribute to the ongoing advances in the field of human-aligned generative AI models."],"url":"http://arxiv.org/abs/2403.06735v1","category":"cs.CV"}
{"created":"2024-03-11 13:56:57","title":"Real-Time Multimodal Cognitive Assistant for Emergency Medical Services","abstract":"Emergency Medical Services (EMS) responders often operate under time-sensitive conditions, facing cognitive overload and inherent risks, requiring essential skills in critical thinking and rapid decision-making. This paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system that can act as a collaborative virtual partner engaging in the real-time acquisition and analysis of multimodal data from an emergency scene and interacting with EMS responders through Augmented Reality (AR) smart glasses. CognitiveEMS processes the continuous streams of data in real-time and leverages edge computing to provide assistance in EMS protocol selection and intervention recognition. We address key technical challenges in real-time cognitive assistance by introducing three novel components: (i) a Speech Recognition model that is fine-tuned for real-world medical emergency conversations using simulated EMS audio recordings, augmented with synthetic data generated by large language models (LLMs); (ii) an EMS Protocol Prediction model that combines state-of-the-art (SOTA) tiny language models with EMS domain knowledge using graph-based attention mechanisms; (iii) an EMS Action Recognition module which leverages multimodal audio and video data and protocol predictions to infer the intervention/treatment actions taken by the responders at the incident scene. Our results show that for speech recognition we achieve superior performance compared to SOTA (WER of 0.290 vs. 0.618) on conversational data. Our protocol prediction component also significantly outperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition achieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s for protocol prediction on the edge and 0.31s on the server.","sentences":["Emergency Medical Services (EMS) responders often operate under time-sensitive conditions, facing cognitive overload and inherent risks, requiring essential skills in critical thinking and rapid decision-making.","This paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system that can act as a collaborative virtual partner engaging in the real-time acquisition and analysis of multimodal data from an emergency scene and interacting with EMS responders through Augmented Reality (AR) smart glasses.","CognitiveEMS processes the continuous streams of data in real-time and leverages edge computing to provide assistance in EMS protocol selection and intervention recognition.","We address key technical challenges in real-time cognitive assistance by introducing three novel components: (i) a Speech Recognition model that is fine-tuned for real-world medical emergency conversations using simulated EMS audio recordings, augmented with synthetic data generated by large language models (LLMs); (ii) an EMS Protocol Prediction model that combines state-of-the-art (SOTA) tiny language models with EMS domain knowledge using graph-based attention mechanisms; (iii) an EMS Action Recognition module which leverages multimodal audio and video data and protocol predictions to infer the intervention/treatment actions taken by the responders at the incident scene.","Our results show that for speech recognition we achieve superior performance compared to SOTA (WER of 0.290 vs. 0.618) on conversational data.","Our protocol prediction component also significantly outperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition achieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s for protocol prediction on the edge and 0.31s on the server."],"url":"http://arxiv.org/abs/2403.06734v1","category":"cs.AI"}
{"created":"2024-03-11 13:55:03","title":"On the construction of a quantum channel corresponding to non-commutative graph for a qubit interacting with quantum oscillator","abstract":"We consider error correction, based on the theory of non-commutative graphs, for a model of a qubit interacting with quantum oscillator. The dynamics of the composite system is governed by the Schr\\\"odinger equation which generates positive operator-valued measure (POVM) for the system dynamics. We construct a quantum channel generating the non-commutative graph as a linear envelope of the POVM. The idea is based on applying a generalized version of a quantum channel using the apparatus of von Neumann algebras. The results are analyzes for a non-commutative graph generated by a qubit interacting with quantum oscillator. For this model the quantum anticlique which determines the error correcting subspace has an explicit expression.","sentences":["We consider error correction, based on the theory of non-commutative graphs, for a model of a qubit interacting with quantum oscillator.","The dynamics of the composite system is governed by the Schr\\\"odinger equation which generates positive operator-valued measure (POVM) for the system dynamics.","We construct a quantum channel generating the non-commutative graph as a linear envelope of the POVM.","The idea is based on applying a generalized version of a quantum channel using the apparatus of von Neumann algebras.","The results are analyzes for a non-commutative graph generated by a qubit interacting with quantum oscillator.","For this model the quantum anticlique which determines the error correcting subspace has an explicit expression."],"url":"http://arxiv.org/abs/2403.06733v1","category":"quant-ph"}
{"created":"2024-03-11 13:47:51","title":"Galaxy Morphologies Revealed with Subaru HSC and Super-Resolution Techniques II: Environmental Dependence of Galaxy Mergers at z~2-5","abstract":"We super-resolve the seeing-limited Subaru Hyper Suprime-Cam (HSC) images for 32,187 galaxies at z~2-5 in three techniques, namely, the classical Richardson-Lucy (RL) point spread function (PSF) deconvolution, sparse modeling, and generative adversarial networks to investigate the environmental dependence of galaxy mergers. These three techniques generate overall similar high spatial resolution images but with some slight differences in galaxy structures, for example, more residual noises are seen in the classical RL PSF deconvolution. To alleviate disadvantages of each technique, we create combined images by averaging over the three types of super-resolution images, which result in galaxy sub-structures resembling those seen in the Hubble Space Telescope images. Using the combined super-resolution images, we measure the relative galaxy major merger fraction corrected for the chance projection effect, f_merg, for galaxies in the ~300 deg^2-area data of the HSC Strategic Survey Program and the CFHT Large Area U-band Survey. Our f_merg measurements at z~3 validate previous findings showing that f_merg is higher in regions with a higher galaxy overdensity delta at z~2-3. Thanks to the large galaxy sample, we identify a nearly linear increase in f_merg with increasing delta at z~4-5, providing the highest-z observational evidence that galaxy mergers are related to delta. In addition to our f_merg measurements, we find that the galaxy merger fractions in the literature also broadly align with the linear f_merg-delta relation across a wide redshift range of z~2-5. This alignment suggests that the linear f_merg-delta relation can serve as a valuable tool for quantitatively estimating the contributions of galaxy mergers to various environmental dependences. This super-resolution analysis can be readily applied to datasets from wide field-of-view space telescopes such as Euclid and Roman.","sentences":["We super-resolve the seeing-limited Subaru Hyper Suprime-Cam (HSC) images for 32,187 galaxies at z~2-5 in three techniques, namely, the classical Richardson-Lucy (RL) point spread function (PSF) deconvolution, sparse modeling, and generative adversarial networks to investigate the environmental dependence of galaxy mergers.","These three techniques generate overall similar high spatial resolution images but with some slight differences in galaxy structures, for example, more residual noises are seen in the classical RL PSF deconvolution.","To alleviate disadvantages of each technique, we create combined images by averaging over the three types of super-resolution images, which result in galaxy sub-structures resembling those seen in the Hubble Space Telescope images.","Using the combined super-resolution images, we measure the relative galaxy major merger fraction corrected for the chance projection effect, f_merg, for galaxies in the ~300 deg^2-area data of the HSC Strategic Survey Program and the CFHT Large Area U-band Survey.","Our f_merg measurements at z~3 validate previous findings showing that f_merg is higher in regions with a higher galaxy overdensity delta at z~2-3.","Thanks to the large galaxy sample, we identify a nearly linear increase in f_merg with increasing delta at z~4-5, providing the highest-z observational evidence that galaxy mergers are related to delta.","In addition to our f_merg measurements, we find that the galaxy merger fractions in the literature also broadly align with the linear f_merg-delta relation across a wide redshift range of z~2-5.","This alignment suggests that the linear f_merg-delta relation can serve as a valuable tool for quantitatively estimating the contributions of galaxy mergers to various environmental dependences.","This super-resolution analysis can be readily applied to datasets from wide field-of-view space telescopes such as Euclid and Roman."],"url":"http://arxiv.org/abs/2403.06729v1","category":"astro-ph.GA"}
{"created":"2024-03-11 13:47:11","title":"Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning","abstract":"Radiology report generation (RRG) has attracted significant attention due to its potential to reduce the workload of radiologists. Current RRG approaches are still unsatisfactory against clinical standards. This paper introduces a novel RRG method, \\textbf{LM-RRG}, that integrates large models (LMs) with clinical quality reinforcement learning to generate accurate and comprehensive chest X-ray radiology reports. Our method first designs a large language model driven feature extractor to analyze and interpret different regions of the chest X-ray image, emphasizing specific regions with medical significance. Next, based on the large model's decoder, we develop a multimodal report generator that leverages multimodal prompts from visual features and textual instruction to produce the radiology report in an auto-regressive way. Finally, to better reflect the clinical significant and insignificant errors that radiologists would normally assign in the report, we introduce a novel clinical quality reinforcement learning strategy. It utilizes the radiology report clinical quality (RadCliQ) metric as a reward function in the learning process. Extensive experiments on the MIMIC-CXR and IU-Xray datasets demonstrate the superiority of our method over the state of the art.","sentences":["Radiology report generation (RRG) has attracted significant attention due to its potential to reduce the workload of radiologists.","Current RRG approaches are still unsatisfactory against clinical standards.","This paper introduces a novel RRG method, \\textbf{LM-RRG}, that integrates large models (LMs) with clinical quality reinforcement learning to generate accurate and comprehensive chest X-ray radiology reports.","Our method first designs a large language model driven feature extractor to analyze and interpret different regions of the chest X-ray image, emphasizing specific regions with medical significance.","Next, based on the large model's decoder, we develop a multimodal report generator that leverages multimodal prompts from visual features and textual instruction to produce the radiology report in an auto-regressive way.","Finally, to better reflect the clinical significant and insignificant errors that radiologists would normally assign in the report, we introduce a novel clinical quality reinforcement learning strategy.","It utilizes the radiology report clinical quality (RadCliQ) metric as a reward function in the learning process.","Extensive experiments on the MIMIC-CXR and IU-Xray datasets demonstrate the superiority of our method over the state of the art."],"url":"http://arxiv.org/abs/2403.06728v1","category":"cs.CV"}
{"created":"2024-03-11 13:44:49","title":"Probabilistic Contrastive Learning for Long-Tailed Visual Recognition","abstract":"Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples. Such imbalance issue considerably impairs the performance of standard supervised learning algorithms, which are mainly designed for balanced training sets. Recent investigations have revealed that supervised contrastive learning exhibits promising potential in alleviating the data imbalance. However, the performance of supervised contrastive learning is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct contrastive pairs that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data. To overcome this obstacle, we propose a novel probabilistic contrastive (ProCo) learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples contrastive pairs accordingly. In fact, estimating the distributions of all classes using features in a small batch, particularly for imbalanced data, is not feasible. Our key idea is to introduce a reasonable and simple assumption that the normalized features in contrastive learning follow a mixture of von Mises-Fisher (vMF) distributions on unit space, which brings two-fold benefits. First, the distribution parameters can be estimated using only the first sample moment, which can be efficiently computed in an online manner across different batches. Second, based on the estimated distribution, the vMF distribution allows us to sample an infinite number of contrastive pairs and derive a closed form of the expected contrastive loss for efficient optimization. Our code is available at https://github.com/LeapLabTHU/ProCo.","sentences":["Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples.","Such imbalance issue considerably impairs the performance of standard supervised learning algorithms, which are mainly designed for balanced training sets.","Recent investigations have revealed that supervised contrastive learning exhibits promising potential in alleviating the data imbalance.","However, the performance of supervised contrastive learning is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct contrastive pairs that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data.","To overcome this obstacle, we propose a novel probabilistic contrastive (ProCo) learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples contrastive pairs accordingly.","In fact, estimating the distributions of all classes using features in a small batch, particularly for imbalanced data, is not feasible.","Our key idea is to introduce a reasonable and simple assumption that the normalized features in contrastive learning follow a mixture of von Mises-Fisher (vMF) distributions on unit space, which brings two-fold benefits.","First, the distribution parameters can be estimated using only the first sample moment, which can be efficiently computed in an online manner across different batches.","Second, based on the estimated distribution, the vMF distribution allows us to sample an infinite number of contrastive pairs and derive a closed form of the expected contrastive loss for efficient optimization.","Our code is available at https://github.com/LeapLabTHU/ProCo."],"url":"http://arxiv.org/abs/2403.06726v1","category":"cs.LG"}
{"created":"2024-03-11 13:44:43","title":"Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning","abstract":"Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent \"pre-training and fine-tuning\" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subsequently facilitate effective adaptation to low-resource KT datasets. Specifically, we simplify existing sophisticated DLKT model architectures with purely a stack of transformer decoders. We design an encoding mechanism to incorporate student interactions from multiple KT data sources and develop an importance mechanism to prioritize updating parameters with high importance while constraining less important ones during the fine-tuning stage. We evaluate LoReKT on six public KT datasets and experimental results demonstrate the superiority of our approach in terms of AUC and Accuracy. To encourage reproducible research, we make our data and code publicly available at https://anonymous.4open.science/r/LoReKT-C619.","sentences":["Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions.","Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task.","These DLKT models heavily rely on the large number of available student interactions.","However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets.","Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture.","Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges.","Inspired by the prevalent \"pre-training and fine-tuning\" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subsequently facilitate effective adaptation to low-resource KT datasets.","Specifically, we simplify existing sophisticated DLKT model architectures with purely a stack of transformer decoders.","We design an encoding mechanism to incorporate student interactions from multiple KT data sources and develop an importance mechanism to prioritize updating parameters with high importance while constraining less important ones during the fine-tuning stage.","We evaluate LoReKT on six public KT datasets and experimental results demonstrate the superiority of our approach in terms of AUC and Accuracy.","To encourage reproducible research, we make our data and code publicly available at https://anonymous.4open.science/r/LoReKT-C619."],"url":"http://arxiv.org/abs/2403.06725v1","category":"cs.CY"}
{"created":"2024-03-11 13:43:07","title":"Fundamental Theorems for Timelike Surfaces in the Minkowski 4-Space","abstract":"In the present paper, we study timelike surfaces free of minimal points in the four-dimensional Minkowski space. For each such surface we introduce a geometrically determined pseudo-orthonormal frame field and writing the derivative formulas with respect to this moving frame field and using the integrability conditions, we obtain a system of six functions satisfying some natural conditions. In the general case, we prove a Fundamental Bonnet-type theorem (existence and uniqueness theorem) stating that these six functions, satisfying the natural conditions, determine the surface up to a motion. In some particular cases, we reduce the number of functions and give the fundamental theorems.","sentences":["In the present paper, we study timelike surfaces free of minimal points in the four-dimensional Minkowski space.","For each such surface we introduce a geometrically determined pseudo-orthonormal frame field and writing the derivative formulas with respect to this moving frame field and using the integrability conditions, we obtain a system of six functions satisfying some natural conditions.","In the general case, we prove a Fundamental Bonnet-type theorem (existence and uniqueness theorem) stating that these six functions, satisfying the natural conditions, determine the surface up to a motion.","In some particular cases, we reduce the number of functions and give the fundamental theorems."],"url":"http://arxiv.org/abs/2403.06721v1","category":"math.DG"}
{"created":"2024-03-11 13:42:18","title":"On the Secrecy Rate of In-Band Full-duplex Two-way Wiretap Channel","abstract":"In this paper, we consider a two-way wiretap Multi-Input Multi-Output Multi-antenna Eve (MIMOME) channel, where both nodes (Alice and Bob) transmit and receive in an in-band full-duplex (IBFD) manner. For this system with keyless security, we provide a novel artificial noise (AN) based signal design, where the AN is injected in both signal and null spaces. We present an ergodic secrecy rate approximation to derive the power allocation algorithm. We consider scenarios where AN is known and unknown to legitimate users and include imperfect channel information effects. To maximize secrecy rates subject to the transmit power constraint, a two-step power allocation solution is proposed, where the first step is known at Eve, and the second step helps to improve the secrecy further. We also consider scenarios where partial information is known by Eve and the effects of non-ideal self-interference cancellation. The usefulness and limitations of the resulting power allocation solution are analyzed and verified via simulations. Results show that secrecy rates are less when AN is unknown to receivers or Eve has more information about legitimate users. Since the ergodic approximation only considers Eves distance, the resulting power allocation provides secrecy rates close to the actual ones.","sentences":["In this paper, we consider a two-way wiretap Multi-Input Multi-Output Multi-antenna Eve (MIMOME) channel, where both nodes (Alice and Bob) transmit and receive in an in-band full-duplex (IBFD) manner.","For this system with keyless security, we provide a novel artificial noise (AN) based signal design, where the AN is injected in both signal and null spaces.","We present an ergodic secrecy rate approximation to derive the power allocation algorithm.","We consider scenarios where AN is known and unknown to legitimate users and include imperfect channel information effects.","To maximize secrecy rates subject to the transmit power constraint, a two-step power allocation solution is proposed, where the first step is known at Eve, and the second step helps to improve the secrecy further.","We also consider scenarios where partial information is known by Eve and the effects of non-ideal self-interference cancellation.","The usefulness and limitations of the resulting power allocation solution are analyzed and verified via simulations.","Results show that secrecy rates are less when AN is unknown to receivers or Eve has more information about legitimate users.","Since the ergodic approximation only considers Eves distance, the resulting power allocation provides secrecy rates close to the actual ones."],"url":"http://arxiv.org/abs/2403.06720v1","category":"cs.IT"}
{"created":"2024-03-11 13:37:56","title":"Influence of Li-stoichiometry on electrical and acoustic properties and temperature stability of Li(Nb,Ta)O$_{3}$ solid solutions up to 900 \u00b0C","abstract":"The current work is focused on the impact of the lithium stoichiometry on electrical conductivity, acoustic properties and high-temperature stability of single crystalline Li(Nb,Ta)O$_{3}$ at high temperatures. The crystals grown from Li-deficient melts were treated by the vapor transport equilibration (VTE) method, achieving near stoichiometric Li-content. It is shown, that the VTE-treated specimens generally exhibit lower conductivity at temperatures below 800 {\\deg}C, which is attributed to the reduced number of Li-vacancies in near stoichiometric Li(Nb,Ta)O$_{3}$, provided that the Li-ion migration dominates the conductivity in this temperature range. Further, it is shown, that above 600-650 {\\deg}C different mechanism increasingly contributes to the conductivity, which is consequently attributed to the electronic conduction. Further, it is shown that losses in LNT strongly increase above about 500 {\\deg}C, which is interpreted to originate from conductivity-related relaxation mechanism. Finally, the thermal stability of Li(Nb,Ta)O$_{3}$ is evaluated by the measurement of the conductivity and resonance frequency as a function of time. It is found that during annealing at 700 {\\deg}C for 350 hours, the resonance frequency of LiNbO$_{3}$ remains in a {\\textpm} 100 ppm range of the initial value of 3.5 MHz.","sentences":["The current work is focused on the impact of the lithium stoichiometry on electrical conductivity, acoustic properties and high-temperature stability of single crystalline Li(Nb,Ta)O$_{3}$ at high temperatures.","The crystals grown from Li-deficient melts were treated by the vapor transport equilibration (VTE) method, achieving near stoichiometric Li-content.","It is shown, that the VTE-treated specimens generally exhibit lower conductivity at temperatures below 800 {\\deg}C, which is attributed to the reduced number of Li-vacancies in near stoichiometric Li(Nb,Ta)O$_{3}$, provided that the Li-ion migration dominates the conductivity in this temperature range.","Further, it is shown, that above 600-650 {\\deg}C different mechanism increasingly contributes to the conductivity, which is consequently attributed to the electronic conduction.","Further, it is shown that losses in LNT strongly increase above about 500 {\\deg}C, which is interpreted to originate from conductivity-related relaxation mechanism.","Finally, the thermal stability of Li(Nb,Ta)O$_{3}$ is evaluated by the measurement of the conductivity and resonance frequency as a function of time.","It is found that during annealing at 700 {\\deg}C for 350 hours, the resonance frequency of LiNbO$_{3}$ remains in a {\\textpm} 100 ppm range of the initial value of 3.5 MHz."],"url":"http://arxiv.org/abs/2403.06713v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-11 13:17:55","title":"Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization","abstract":"Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging research hot spot in machine learning, which still suffers from low efficiency and poor quality. In this paper, we propose an End-to-End Efficient and Effective network for fast and accurate T3D face generation and manipulation, termed $E^3$-FaceNet. Different from existing complex generation paradigms, $E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware visual space. We introduce a novel Style Code Enhancer to enhance cross-modal semantic alignment, alongside an innovative Geometric Regularization objective to maintain consistency across multi-view generations. Extensive experiments on three benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve picture-like 3D face generation and manipulation, but also improve inference speed by orders of magnitudes. For instance, compared with Latent3D, $E^3$-FaceNet speeds up the five-view generations by almost 470 times, while still exceeding in generation quality. Our code are released at https://github.com/Aria-Zhangjl/E3-FaceNet.","sentences":["Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging research hot spot in machine learning, which still suffers from low efficiency and poor quality.","In this paper, we propose an End-to-End Efficient and Effective network for fast and accurate T3D face generation and manipulation, termed $E^3$-FaceNet.","Different from existing complex generation paradigms, $E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware visual space.","We introduce a novel Style Code Enhancer to enhance cross-modal semantic alignment, alongside an innovative Geometric Regularization objective to maintain consistency across multi-view generations.","Extensive experiments on three benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve picture-like 3D face generation and manipulation, but also improve inference speed by orders of magnitudes.","For instance, compared with Latent3D, $E^3$-FaceNet speeds up the five-view generations by almost 470 times, while still exceeding in generation quality.","Our code are released at https://github.com/Aria-Zhangjl/E3-FaceNet."],"url":"http://arxiv.org/abs/2403.06702v1","category":"cs.CV"}
{"created":"2024-03-11 13:16:44","title":"Enhancing Adversarial Training with Prior Knowledge Distillation for Robust Image Compression","abstract":"Deep neural network-based image compression (NIC) has achieved excellent performance, but NIC method models have been shown to be susceptible to backdoor attacks. Adversarial training has been validated in image compression models as a common method to enhance model robustness. However, the improvement effect of adversarial training on model robustness is limited. In this paper, we propose a prior knowledge-guided adversarial training framework for image compression models. Specifically, first, we propose a gradient regularization constraint for training robust teacher models. Subsequently, we design a knowledge distillation based strategy to generate a priori knowledge from the teacher model to the student model for guiding adversarial training. Experimental results show that our method improves the reconstruction quality by about 9dB when the Kodak dataset is elected as the backdoor attack object for psnr attack. Compared with Ma2023, our method has a 5dB higher PSNR output at high bitrate points.","sentences":["Deep neural network-based image compression (NIC) has achieved excellent performance, but NIC method models have been shown to be susceptible to backdoor attacks.","Adversarial training has been validated in image compression models as a common method to enhance model robustness.","However, the improvement effect of adversarial training on model robustness is limited.","In this paper, we propose a prior knowledge-guided adversarial training framework for image compression models.","Specifically, first, we propose a gradient regularization constraint for training robust teacher models.","Subsequently, we design a knowledge distillation based strategy to generate a priori knowledge from the teacher model to the student model for guiding adversarial training.","Experimental results show that our method improves the reconstruction quality by about 9dB when the Kodak dataset is elected as the backdoor attack object for psnr attack.","Compared with Ma2023, our method has a 5dB higher PSNR output at high bitrate points."],"url":"http://arxiv.org/abs/2403.06700v1","category":"eess.IV"}
{"created":"2024-03-11 13:13:10","title":"PCLD: Point Cloud Layerwise Diffusion for Adversarial Purification","abstract":"Point clouds are extensively employed in a variety of real-world applications such as robotics, autonomous driving and augmented reality. Despite the recent success of point cloud neural networks, especially for safety-critical tasks, it is essential to also ensure the robustness of the model. A typical way to assess a model's robustness is through adversarial attacks, where test-time examples are generated based on gradients to deceive the model. While many different defense mechanisms are studied in 2D, studies on 3D point clouds have been relatively limited in the academic field. Inspired from PointDP, which denoises the network inputs by diffusion, we propose Point Cloud Layerwise Diffusion (PCLD), a layerwise diffusion based 3D point cloud defense strategy. Unlike PointDP, we propagated the diffusion denoising after each layer to incrementally enhance the results. We apply our defense method to different types of commonly used point cloud models and adversarial attacks to evaluate its robustness. Our experiments demonstrate that the proposed defense method achieved results that are comparable to or surpass those of existing methodologies, establishing robustness through a novel technique. Code is available at https://github.com/batuceng/diffusion-layer-robustness-pc.","sentences":["Point clouds are extensively employed in a variety of real-world applications such as robotics, autonomous driving and augmented reality.","Despite the recent success of point cloud neural networks, especially for safety-critical tasks, it is essential to also ensure the robustness of the model.","A typical way to assess a model's robustness is through adversarial attacks, where test-time examples are generated based on gradients to deceive the model.","While many different defense mechanisms are studied in 2D, studies on 3D point clouds have been relatively limited in the academic field.","Inspired from PointDP, which denoises the network inputs by diffusion, we propose Point Cloud Layerwise Diffusion (PCLD), a layerwise diffusion based 3D point cloud defense strategy.","Unlike PointDP, we propagated the diffusion denoising after each layer to incrementally enhance the results.","We apply our defense method to different types of commonly used point cloud models and adversarial attacks to evaluate its robustness.","Our experiments demonstrate that the proposed defense method achieved results that are comparable to or surpass those of existing methodologies, establishing robustness through a novel technique.","Code is available at https://github.com/batuceng/diffusion-layer-robustness-pc."],"url":"http://arxiv.org/abs/2403.06698v1","category":"cs.CV"}
{"created":"2024-03-11 13:07:58","title":"$C_{2k+1}$-coloring of bounded-diameter graphs","abstract":"For a fixed graph $H$, in the graph homomorphism problem, denoted by $Hom(H)$, we are given a graph $G$ and we have to determine whether there exists an edge-preserving mapping $\\varphi: V(G) \\to V(H)$. Note that $Hom(C_3)$, where $C_3$ is the cycle of length $3$, is equivalent to $3$-Coloring. The question whether $3$-Coloring is polynomial-time solvable on diameter-$2$ graphs is a well-known open problem. In this paper we study the $Hom(C_{2k+1})$ problem on bounded-diameter graphs for $k\\geq 2$, so we consider all other odd cycles than $C_3$. We prove that for $k\\geq 2$, the $Hom(C_{2k+1})$ problem is polynomial-time solvable on diameter-$(k+1)$ graphs -- note that such a result for $k=1$ would be precisely a polynomial-time algorithm for $3$-Coloring of diameter-$2$ graphs.   Furthermore, we give subexponential-time algorithms for diameter-$(k+2)$ and -$(k+3)$ graphs.   We complement these results with a lower bound for diameter-$(2k+2)$ graphs -- in this class of graphs the $Hom(C_{2k+1})$ problem is NP-hard and cannot be solved in subexponential-time, unless the ETH fails.   Finally, we consider another direction of generalizing $3$-Coloring on diameter-$2$ graphs. We consider other target graphs $H$ than odd cycles but we restrict ourselves to diameter $2$. We show that if $H$ is triangle-free, then $Hom(H)$ is polynomial-time solvable on diameter-$2$ graphs.","sentences":["For a fixed graph $H$, in the graph homomorphism problem, denoted by $Hom(H)$, we are given a graph $G$ and we have to determine whether there exists an edge-preserving mapping $\\varphi: V(G)","\\to V(H)$. Note that $Hom(C_3)$, where $C_3$ is the cycle of length $3$, is equivalent to $3$-Coloring.","The question whether $3$-Coloring is polynomial-time solvable on diameter-$2$ graphs is a well-known open problem.","In this paper we study the $Hom(C_{2k+1})$ problem on bounded-diameter graphs for $k\\geq 2$, so we consider all other odd cycles than $C_3$. We prove that for $k\\geq 2$, the $Hom(C_{2k+1})$ problem is polynomial-time solvable on diameter-$(k+1)$ graphs -- note that such a result for $k=1$ would be precisely a polynomial-time algorithm for $3$-Coloring of diameter-$2$ graphs.   ","Furthermore, we give subexponential-time algorithms for diameter-$(k+2)$ and -$(k+3)$ graphs.   ","We complement these results with a lower bound for diameter-$(2k+2)$ graphs -- in this class of graphs the $Hom(C_{2k+1})$ problem is NP-hard and cannot be solved in subexponential-time, unless the ETH fails.   ","Finally, we consider another direction of generalizing $3$-Coloring on diameter-$2$ graphs.","We consider other target graphs $H$ than odd cycles but we restrict ourselves to diameter $2$. We show that if $H$ is triangle-free, then $Hom(H)$ is polynomial-time solvable on diameter-$2$ graphs."],"url":"http://arxiv.org/abs/2403.06694v1","category":"math.CO"}
{"created":"2024-03-11 13:07:46","title":"Chart4Blind: An Intelligent Interface for Chart Accessibility Conversion","abstract":"In a world driven by data visualization, ensuring the inclusive accessibility of charts for Blind and Visually Impaired (BVI) individuals remains a significant challenge. Charts are usually presented as raster graphics without textual and visual metadata needed for an equivalent exploration experience for BVI people. Additionally, converting these charts into accessible formats requires considerable effort from sighted individuals. Digitizing charts with metadata extraction is just one aspect of the issue; transforming it into accessible modalities, such as tactile graphics, presents another difficulty. To address these disparities, we propose Chart4Blind, an intelligent user interface that converts bitmap image representations of line charts into universally accessible formats. Chart4Blind achieves this transformation by generating Scalable Vector Graphics (SVG), Comma-Separated Values (CSV), and alternative text exports, all comply with established accessibility standards. Through interviews and a formal user study, we demonstrate that even inexperienced sighted users can make charts accessible in an average of 4 minutes using Chart4Blind, achieving a System Usability Scale rating of 90%. In comparison to existing approaches, Chart4Blind provides a comprehensive solution, generating end-to-end accessible SVGs suitable for assistive technologies such as embossed prints (papers and laser cut), 2D tactile displays, and screen readers. For additional information, including open-source codes and demos, please visit our project page https://moured.github.io/chart4blind/.","sentences":["In a world driven by data visualization, ensuring the inclusive accessibility of charts for Blind and Visually Impaired (BVI) individuals remains a significant challenge.","Charts are usually presented as raster graphics without textual and visual metadata needed for an equivalent exploration experience for BVI people.","Additionally, converting these charts into accessible formats requires considerable effort from sighted individuals.","Digitizing charts with metadata extraction is just one aspect of the issue; transforming it into accessible modalities, such as tactile graphics, presents another difficulty.","To address these disparities, we propose Chart4Blind, an intelligent user interface that converts bitmap image representations of line charts into universally accessible formats.","Chart4Blind achieves this transformation by generating Scalable Vector Graphics (SVG), Comma-Separated Values (CSV), and alternative text exports, all comply with established accessibility standards.","Through interviews and a formal user study, we demonstrate that even inexperienced sighted users can make charts accessible in an average of 4 minutes using Chart4Blind, achieving a System Usability Scale rating of 90%.","In comparison to existing approaches, Chart4Blind provides a comprehensive solution, generating end-to-end accessible SVGs suitable for assistive technologies such as embossed prints (papers and laser cut), 2D tactile displays, and screen readers.","For additional information, including open-source codes and demos, please visit our project page https://moured.github.io/chart4blind/."],"url":"http://arxiv.org/abs/2403.06693v1","category":"cs.HC"}
{"created":"2024-03-11 13:05:08","title":"From S-matrix theory to strings: Scattering data and the commitment to non-arbitrariness","abstract":"The early history of string theory is marked by a shift from strong interaction physics to quantum gravity. The first string models and associated theoretical framework were formulated in the late 1960s and early 1970s in the context of the S-matrix program for the strong interactions. In the mid-1970s, the models were reinterpreted as a potential theory unifying the four fundamental forces. This paper provides a historical analysis of how string theory was developed out of S-matrix physics, aiming to clarify how modern string theory, as a theory detached from experimental data, grew out of an S-matrix program that was strongly dependent upon observable quantities. Surprisingly, the theoretical practice of physicists already turned away from experiment before string theory was recast as a potential unified quantum gravity theory. With the formulation of dual resonance models (the \"hadronic string theory\"), physicists were able to determine almost all of the models' parameters on the basis of theoretical reasoning. It was this commitment to \"non-arbitrariness\", i.e., a lack of free parameters in the theory, that initially drove string theorists away from experimental input, and not the practical inaccessibility of experimental data in the context of quantum gravity physics. This is an important observation when assessing the role of experimental data in string theory.","sentences":["The early history of string theory is marked by a shift from strong interaction physics to quantum gravity.","The first string models and associated theoretical framework were formulated in the late 1960s and early 1970s in the context of the S-matrix program for the strong interactions.","In the mid-1970s, the models were reinterpreted as a potential theory unifying the four fundamental forces.","This paper provides a historical analysis of how string theory was developed out of S-matrix physics, aiming to clarify how modern string theory, as a theory detached from experimental data, grew out of an S-matrix program that was strongly dependent upon observable quantities.","Surprisingly, the theoretical practice of physicists already turned away from experiment before string theory was recast as a potential unified quantum gravity theory.","With the formulation of dual resonance models (the \"hadronic string theory\"), physicists were able to determine almost all of the models' parameters on the basis of theoretical reasoning.","It was this commitment to \"non-arbitrariness\", i.e., a lack of free parameters in the theory, that initially drove string theorists away from experimental input, and not the practical inaccessibility of experimental data in the context of quantum gravity physics.","This is an important observation when assessing the role of experimental data in string theory."],"url":"http://arxiv.org/abs/2403.06690v1","category":"physics.hist-ph"}
{"created":"2024-03-11 13:04:48","title":"Dynamic of frustrated Kuramoto oscillators with modular connections","abstract":"Synchronization and collective movement are phenomena of a highly interdisciplinary nature, with examples ranging from neuronal activation to walking pedestrians. As of today, the Kuramoto model stands as the quintessential framework for investigating synchronization phenomena, displaying a second order phase transition from disordered motion to synchronization as the coupling between oscillators increases. The model was recently extended to higher dimensions allowing for the coupling parameter to be promoted to a matrix, leading to generalized frustration and new synchronized states. This model was previously investigated in the case of all-to-all and homogeneous interactions. Here, we extend the analysis to modular graphs, which mimic the community structure presented in many real systems. We investigated, both numerically and analytically, the matrix coupled Kuramoto model with oscillators divided into two groups with distinct coupling parameters to understand in which conditions they synchronize independently or globally. We discovered a very rich and complex dynamic, including an extended region in the parameter space in which the interactions between modules were destructive, leading to a global disordered motion even tough the uncoupled dynamic presented higher levels of synchronization. Additional simulations considering synthetic modular networks were performed to assess the robustness of our findings.","sentences":["Synchronization and collective movement are phenomena of a highly interdisciplinary nature, with examples ranging from neuronal activation to walking pedestrians.","As of today, the Kuramoto model stands as the quintessential framework for investigating synchronization phenomena, displaying a second order phase transition from disordered motion to synchronization as the coupling between oscillators increases.","The model was recently extended to higher dimensions allowing for the coupling parameter to be promoted to a matrix, leading to generalized frustration and new synchronized states.","This model was previously investigated in the case of all-to-all and homogeneous interactions.","Here, we extend the analysis to modular graphs, which mimic the community structure presented in many real systems.","We investigated, both numerically and analytically, the matrix coupled Kuramoto model with oscillators divided into two groups with distinct coupling parameters to understand in which conditions they synchronize independently or globally.","We discovered a very rich and complex dynamic, including an extended region in the parameter space in which the interactions between modules were destructive, leading to a global disordered motion even tough the uncoupled dynamic presented higher levels of synchronization.","Additional simulations considering synthetic modular networks were performed to assess the robustness of our findings."],"url":"http://arxiv.org/abs/2403.06689v1","category":"physics.soc-ph"}
{"created":"2024-03-11 13:04:36","title":"Ground-state chiral current via periodic modulation","abstract":"In this study, we engineer the Dzyaloshinskii-Moriya interaction mediated by photons to emulate ground-state chiral current based on three-level atoms driven by quantum and classical fields. We employ adiabatic elimination techniques to derive an effective Dzyaloshinskii-Moriya interaction Hamiltonian of two-level systems, which can address the challenges arising from the finite lifetime of excited states. Furthermore, we can ensure to achieve desired dynamics through the implementation of periodic modulation on the atomic ground states. Besides, three-state and multi-state chiral current can be obtained by choosing appropriate driving frequencies and phases. We also design the Dzyaloshinskii-Moriya interaction for the other components based on a toggling frame. The numerical simulation results further indicate that our proposal can generate a perfectly reliable ground-state chiral current and open up possibilities for quantum state transfer and the development of future quantum networks.","sentences":["In this study, we engineer the Dzyaloshinskii-Moriya interaction mediated by photons to emulate ground-state chiral current based on three-level atoms driven by quantum and classical fields.","We employ adiabatic elimination techniques to derive an effective Dzyaloshinskii-Moriya interaction Hamiltonian of two-level systems, which can address the challenges arising from the finite lifetime of excited states.","Furthermore, we can ensure to achieve desired dynamics through the implementation of periodic modulation on the atomic ground states.","Besides, three-state and multi-state chiral current can be obtained by choosing appropriate driving frequencies and phases.","We also design the Dzyaloshinskii-Moriya interaction for the other components based on a toggling frame.","The numerical simulation results further indicate that our proposal can generate a perfectly reliable ground-state chiral current and open up possibilities for quantum state transfer and the development of future quantum networks."],"url":"http://arxiv.org/abs/2403.06688v1","category":"quant-ph"}
{"created":"2024-03-11 12:57:28","title":"Restoring Ancient Ideograph: A Multimodal Multitask Neural Network Approach","abstract":"Cultural heritage serves as the enduring record of human thought and history. Despite significant efforts dedicated to the preservation of cultural relics, many ancient artefacts have been ravaged irreversibly by natural deterioration and human actions. Deep learning technology has emerged as a valuable tool for restoring various kinds of cultural heritages, including ancient text restoration. Previous research has approached ancient text restoration from either visual or textual perspectives, often overlooking the potential of synergizing multimodal information. This paper proposes a novel Multimodal Multitask Restoring Model (MMRM) to restore ancient texts, particularly emphasising the ideograph. This model combines context understanding with residual visual information from damaged ancient artefacts, enabling it to predict damaged characters and generate restored images simultaneously. We tested the MMRM model through experiments conducted on both simulated datasets and authentic ancient inscriptions. The results show that the proposed method gives insightful restoration suggestions in both simulation experiments and real-world scenarios. To the best of our knowledge, this work represents the pioneering application of multimodal deep learning in ancient text restoration, which will contribute to the understanding of ancient society and culture in digital humanities fields.","sentences":["Cultural heritage serves as the enduring record of human thought and history.","Despite significant efforts dedicated to the preservation of cultural relics, many ancient artefacts have been ravaged irreversibly by natural deterioration and human actions.","Deep learning technology has emerged as a valuable tool for restoring various kinds of cultural heritages, including ancient text restoration.","Previous research has approached ancient text restoration from either visual or textual perspectives, often overlooking the potential of synergizing multimodal information.","This paper proposes a novel Multimodal Multitask Restoring Model (MMRM) to restore ancient texts, particularly emphasising the ideograph.","This model combines context understanding with residual visual information from damaged ancient artefacts, enabling it to predict damaged characters and generate restored images simultaneously.","We tested the MMRM model through experiments conducted on both simulated datasets and authentic ancient inscriptions.","The results show that the proposed method gives insightful restoration suggestions in both simulation experiments and real-world scenarios.","To the best of our knowledge, this work represents the pioneering application of multimodal deep learning in ancient text restoration, which will contribute to the understanding of ancient society and culture in digital humanities fields."],"url":"http://arxiv.org/abs/2403.06682v1","category":"cs.CL"}
{"created":"2024-03-11 12:56:36","title":"Trustworthy Partial Label Learning with Out-of-distribution Detection","abstract":"Partial Label Learning (PLL) grapples with learning from ambiguously labelled data, and it has been successfully applied in fields such as image recognition. Nevertheless, traditional PLL methods rely on the closed-world assumption, which can be limiting in open-world scenarios and negatively impact model performance and generalization. To tackle these challenges, our study introduces a novel method called PLL-OOD, which is the first to incorporate Out-of-Distribution (OOD) detection into the PLL framework. PLL-OOD significantly enhances model adaptability and accuracy by merging self-supervised learning with partial label loss and pioneering the Partial-Energy (PE) score for OOD detection. This approach improves data feature representation and effectively disambiguates candidate labels, using a dynamic label confidence matrix to refine predictions. The PE score, adjusted by label confidence, precisely identifies OOD instances, optimizing model training towards in-distribution data. This innovative method markedly boosts PLL model robustness and performance in open-world settings. To validate our approach, we conducted a comprehensive comparative experiment combining the existing state-of-the-art PLL model with multiple OOD scores on the CIFAR-10 and CIFAR-100 datasets with various OOD datasets. The results demonstrate that the proposed PLL-OOD framework is highly effective and effectiveness outperforms existing models, showcasing its superiority and effectiveness.","sentences":["Partial Label Learning (PLL) grapples with learning from ambiguously labelled data, and it has been successfully applied in fields such as image recognition.","Nevertheless, traditional PLL methods rely on the closed-world assumption, which can be limiting in open-world scenarios and negatively impact model performance and generalization.","To tackle these challenges, our study introduces a novel method called PLL-OOD, which is the first to incorporate Out-of-Distribution (OOD) detection into the PLL framework.","PLL-OOD significantly enhances model adaptability and accuracy by merging self-supervised learning with partial label loss and pioneering the Partial-Energy (PE) score for OOD detection.","This approach improves data feature representation and effectively disambiguates candidate labels, using a dynamic label confidence matrix to refine predictions.","The PE score, adjusted by label confidence, precisely identifies OOD instances, optimizing model training towards in-distribution data.","This innovative method markedly boosts PLL model robustness and performance in open-world settings.","To validate our approach, we conducted a comprehensive comparative experiment combining the existing state-of-the-art PLL model with multiple OOD scores on the CIFAR-10 and CIFAR-100 datasets with various OOD datasets.","The results demonstrate that the proposed PLL-OOD framework is highly effective and effectiveness outperforms existing models, showcasing its superiority and effectiveness."],"url":"http://arxiv.org/abs/2403.06681v1","category":"cs.CV"}
{"created":"2024-03-11 12:51:37","title":"Answering Diverse Questions via Text Attached with Key Audio-Visual Clues","abstract":"Audio-visual question answering (AVQA) requires reference to video content and auditory information, followed by correlating the question to predict the most precise answer. Although mining deeper layers of audio-visual information to interact with questions facilitates the multimodal fusion process, the redundancy of audio-visual parameters tends to reduce the generalization of the inference engine to multiple question-answer pairs in a single video. Indeed, the natural heterogeneous relationship between audiovisuals and text makes the perfect fusion challenging, to prevent high-level audio-visual semantics from weakening the network's adaptability to diverse question types, we propose a framework for performing mutual correlation distillation (MCD) to aid question inference. MCD is divided into three main steps: 1) firstly, the residual structure is utilized to enhance the audio-visual soft associations based on self-attention, then key local audio-visual features relevant to the question context are captured hierarchically by shared aggregators and coupled in the form of clues with specific question vectors. 2) Secondly, knowledge distillation is enforced to align audio-visual-text pairs in a shared latent space to narrow the cross-modal semantic gap. 3) And finally, the audio-visual dependencies are decoupled by discarding the decision-level integrations. We evaluate the proposed method on two publicly available datasets containing multiple question-and-answer pairs, i.e., Music-AVQA and AVQA. Experiments show that our method outperforms other state-of-the-art methods, and one interesting finding behind is that removing deep audio-visual features during inference can effectively mitigate overfitting. The source code is released at http://github.com/rikeilong/MCD-forAVQA.","sentences":["Audio-visual question answering (AVQA) requires reference to video content and auditory information, followed by correlating the question to predict the most precise answer.","Although mining deeper layers of audio-visual information to interact with questions facilitates the multimodal fusion process, the redundancy of audio-visual parameters tends to reduce the generalization of the inference engine to multiple question-answer pairs in a single video.","Indeed, the natural heterogeneous relationship between audiovisuals and text makes the perfect fusion challenging, to prevent high-level audio-visual semantics from weakening the network's adaptability to diverse question types, we propose a framework for performing mutual correlation distillation (MCD) to aid question inference.","MCD is divided into three main steps: 1) firstly, the residual structure is utilized to enhance the audio-visual soft associations based on self-attention, then key local audio-visual features relevant to the question context are captured hierarchically by shared aggregators and coupled in the form of clues with specific question vectors.","2) Secondly, knowledge distillation is enforced to align audio-visual-text pairs in a shared latent space to narrow the cross-modal semantic gap.","3) And finally, the audio-visual dependencies are decoupled by discarding the decision-level integrations.","We evaluate the proposed method on two publicly available datasets containing multiple question-and-answer pairs, i.e., Music-AVQA and AVQA.","Experiments show that our method outperforms other state-of-the-art methods, and one interesting finding behind is that removing deep audio-visual features during inference can effectively mitigate overfitting.","The source code is released at http://github.com/rikeilong/MCD-forAVQA."],"url":"http://arxiv.org/abs/2403.06679v1","category":"cs.CV"}
{"created":"2024-03-11 12:49:37","title":"Streamlining in the Riemannian Realm: Efficient Riemannian Optimization with Loopless Variance Reduction","abstract":"In this study, we investigate stochastic optimization on Riemannian manifolds, focusing on the crucial variance reduction mechanism used in both Euclidean and Riemannian settings. Riemannian variance-reduced methods usually involve a double-loop structure, computing a full gradient at the start of each loop. Determining the optimal inner loop length is challenging in practice, as it depends on strong convexity or smoothness constants, which are often unknown or hard to estimate. Motivated by Euclidean methods, we introduce the Riemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods. These methods replace the outer loop with probabilistic gradient computation triggered by a coin flip in each iteration, ensuring simpler proofs, efficient hyperparameter selection, and sharp convergence guarantees. Using R-PAGE as a framework for non-convex Riemannian optimization, we demonstrate its applicability to various important settings. For example, we derive Riemannian MARINA (R-MARINA) for distributed settings with communication compression, providing the best theoretical communication complexity guarantees for non-convex distributed optimization over Riemannian manifolds. Experimental results support our theoretical findings.","sentences":["In this study, we investigate stochastic optimization on Riemannian manifolds, focusing on the crucial variance reduction mechanism used in both Euclidean and Riemannian settings.","Riemannian variance-reduced methods usually involve a double-loop structure, computing a full gradient at the start of each loop.","Determining the optimal inner loop length is challenging in practice, as it depends on strong convexity or smoothness constants, which are often unknown or hard to estimate.","Motivated by Euclidean methods, we introduce the Riemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods.","These methods replace the outer loop with probabilistic gradient computation triggered by a coin flip in each iteration, ensuring simpler proofs, efficient hyperparameter selection, and sharp convergence guarantees.","Using R-PAGE as a framework for non-convex Riemannian optimization, we demonstrate its applicability to various important settings.","For example, we derive Riemannian MARINA (R-MARINA) for distributed settings with communication compression, providing the best theoretical communication complexity guarantees for non-convex distributed optimization over Riemannian manifolds.","Experimental results support our theoretical findings."],"url":"http://arxiv.org/abs/2403.06677v1","category":"cs.LG"}
{"created":"2024-03-11 12:47:04","title":"Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code","abstract":"AI-based code generators have gained a fundamental role in assisting developers in writing software starting from natural language (NL). However, since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples. In this position paper, we address the security of AI code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code. Next, we devise an extensive evaluation of how these attacks impact state-of-the-art models for code generation. Lastly, we discuss potential solutions to overcome this threat.","sentences":["AI-based code generators have gained a fundamental role in assisting developers in writing software starting from natural language (NL).","However, since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples.","In this position paper, we address the security of AI code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code.","Next, we devise an extensive evaluation of how these attacks impact state-of-the-art models for code generation.","Lastly, we discuss potential solutions to overcome this threat."],"url":"http://arxiv.org/abs/2403.06675v1","category":"cs.CR"}
{"created":"2024-03-11 12:46:53","title":"Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment","abstract":"Most computer vision applications aim to identify pixels in a scene and use them for diverse purposes. One intriguing application is car damage detection for insurance carriers which tends to detect all car damages by comparing both pre-trip and post-trip images, even requiring two components: (i) car damage detection; (ii) image alignment. Firstly, we implemented a Mask R-CNN model to detect car damages on custom images. Whereas for the image alignment section, we especially propose a novel self-supervised Patch-to-Patch SimCLR inspired alignment approach to find perspective transformations between custom pre/post car rental images except for traditional computer vision methods.","sentences":["Most computer vision applications aim to identify pixels in a scene and use them for diverse purposes.","One intriguing application is car damage detection for insurance carriers which tends to detect all car damages by comparing both pre-trip and post-trip images, even requiring two components: (i) car damage detection; (ii) image alignment.","Firstly, we implemented a Mask R-CNN model to detect car damages on custom images.","Whereas for the image alignment section, we especially propose a novel self-supervised Patch-to-Patch SimCLR inspired alignment approach to find perspective transformations between custom pre/post car rental images except for traditional computer vision methods."],"url":"http://arxiv.org/abs/2403.06674v1","category":"cs.CV"}
{"created":"2024-03-11 12:40:12","title":"CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learnin","abstract":"In real-world applications, dynamic scenarios require the models to possess the capability to learn new tasks continuously without forgetting the old knowledge. Experience-Replay methods store a subset of the old images for joint training. In the scenario of more strict privacy protection, storing the old images becomes infeasible, which leads to a more severe plasticity-stability dilemma and classifier bias. To meet the above challenges, we propose a new architecture, named continual expansion and absorption transformer~(CEAT). The model can learn the novel knowledge by extending the expanded-fusion layers in parallel with the frozen previous parameters. After the task ends, we losslessly absorb the extended parameters into the backbone to ensure that the number of parameters remains constant. To improve the learning ability of the model, we designed a novel prototype contrastive loss to reduce the overlap between old and new classes in the feature space. Besides, to address the classifier bias towards the new classes, we propose a novel approach to generate the pseudo-features to correct the classifier. We experiment with our methods on three standard Non-Exemplar Class-Incremental Learning~(NECIL) benchmarks. Extensive experiments demonstrate that our model gets a significant improvement compared with the previous works and achieves 5.38%, 5.20%, and 4.92% improvement on CIFAR-100, TinyImageNet, and ImageNet-Subset.","sentences":["In real-world applications, dynamic scenarios require the models to possess the capability to learn new tasks continuously without forgetting the old knowledge.","Experience-Replay methods store a subset of the old images for joint training.","In the scenario of more strict privacy protection, storing the old images becomes infeasible, which leads to a more severe plasticity-stability dilemma and classifier bias.","To meet the above challenges, we propose a new architecture, named continual expansion and absorption transformer~(CEAT).","The model can learn the novel knowledge by extending the expanded-fusion layers in parallel with the frozen previous parameters.","After the task ends, we losslessly absorb the extended parameters into the backbone to ensure that the number of parameters remains constant.","To improve the learning ability of the model, we designed a novel prototype contrastive loss to reduce the overlap between old and new classes in the feature space.","Besides, to address the classifier bias towards the new classes, we propose a novel approach to generate the pseudo-features to correct the classifier.","We experiment with our methods on three standard Non-Exemplar Class-Incremental Learning~(NECIL) benchmarks.","Extensive experiments demonstrate that our model gets a significant improvement compared with the previous works and achieves 5.38%, 5.20%, and 4.92% improvement on CIFAR-100, TinyImageNet, and ImageNet-Subset."],"url":"http://arxiv.org/abs/2403.06670v1","category":"cs.CV"}
{"created":"2024-03-11 12:32:22","title":"Experimental realization of universal quantum gates and six-qubit state using photonic quantum walk","abstract":"Controlled quantum walk forms the basis for various quantum algorithm and quantum simulation schemes. Though theoretical proposals are also available to realize universal quantum computation using quantum walks, no experimental demonstration of universal set of gates has been reported. Here we report the experimental realize of universal set of quantum gates using photonic quantum walk. Taking cue from the discrete-time quantum walk formalism, we encode multiple qubits using polarization and paths degree of freedom for photon and demonstrate realization of universal set of gates with 100\\% success probability and high fidelity, as characterised by quantum state tomography. For a 3-qubit system we encode first qubit with $H$ and $V-$polarization of photon and path information for the second and third qubit, closely resembling a Mach-Zehnder interference setup. To generate a 6-qubit system and demonstrate 6-qubit GHZ state, entangled photon pairs are used as source to two 3-qubit systems. We also provide insights into the mapping of quantum circuits to quantum walk operations on photons and way to resourcefully scale. This work marks a significant progress towards using photonic quantum walk for quantum computing. It also provides a framework for photonic quantum computing using lesser number of photons in combination with path degree of freedom to increase the success rate of multi-qubit gate operations.","sentences":["Controlled quantum walk forms the basis for various quantum algorithm and quantum simulation schemes.","Though theoretical proposals are also available to realize universal quantum computation using quantum walks, no experimental demonstration of universal set of gates has been reported.","Here we report the experimental realize of universal set of quantum gates using photonic quantum walk.","Taking cue from the discrete-time quantum walk formalism, we encode multiple qubits using polarization and paths degree of freedom for photon and demonstrate realization of universal set of gates with 100\\% success probability and high fidelity, as characterised by quantum state tomography.","For a 3-qubit system we encode first qubit with $H$ and $V-$polarization of photon and path information for the second and third qubit, closely resembling a Mach-Zehnder interference setup.","To generate a 6-qubit system and demonstrate 6-qubit GHZ state, entangled photon pairs are used as source to two 3-qubit systems.","We also provide insights into the mapping of quantum circuits to quantum walk operations on photons and way to resourcefully scale.","This work marks a significant progress towards using photonic quantum walk for quantum computing.","It also provides a framework for photonic quantum computing using lesser number of photons in combination with path degree of freedom to increase the success rate of multi-qubit gate operations."],"url":"http://arxiv.org/abs/2403.06665v1","category":"quant-ph"}
{"created":"2024-03-11 12:29:55","title":"epsilon-Mesh Attack: A Surface-based Adversarial Point Cloud Attack for Facial Expression Recognition","abstract":"Point clouds and meshes are widely used 3D data structures for many computer vision applications. While the meshes represent the surfaces of an object, point cloud represents sampled points from the surface which is also the output of modern sensors such as LiDAR and RGB-D cameras. Due to the wide application area of point clouds and the recent advancements in deep neural networks, studies focusing on robust classification of the 3D point cloud data emerged. To evaluate the robustness of deep classifier networks, a common method is to use adversarial attacks where the gradient direction is followed to change the input slightly. The previous studies on adversarial attacks are generally evaluated on point clouds of daily objects. However, considering 3D faces, these adversarial attacks tend to affect the person's facial structure more than the desired amount and cause malformation. Specifically for facial expressions, even a small adversarial attack can have a significant effect on the face structure. In this paper, we suggest an adversarial attack called $\\epsilon$-Mesh Attack, which operates on point cloud data via limiting perturbations to be on the mesh surface. We also parameterize our attack by $\\epsilon$ to scale the perturbation mesh. Our surface-based attack has tighter perturbation bounds compared to $L_2$ and $L_\\infty$ norm bounded attacks that operate on unit-ball. Even though our method has additional constraints, our experiments on CoMA, Bosphorus and FaceWarehouse datasets show that $\\epsilon$-Mesh Attack (Perpendicular) successfully confuses trained DGCNN and PointNet models $99.72\\%$ and $97.06\\%$ of the time, with indistinguishable facial deformations. The code is available at https://github.com/batuceng/e-mesh-attack.","sentences":["Point clouds and meshes are widely used 3D data structures for many computer vision applications.","While the meshes represent the surfaces of an object, point cloud represents sampled points from the surface which is also the output of modern sensors such as LiDAR and RGB-D cameras.","Due to the wide application area of point clouds and the recent advancements in deep neural networks, studies focusing on robust classification of the 3D point cloud data emerged.","To evaluate the robustness of deep classifier networks, a common method is to use adversarial attacks where the gradient direction is followed to change the input slightly.","The previous studies on adversarial attacks are generally evaluated on point clouds of daily objects.","However, considering 3D faces, these adversarial attacks tend to affect the person's facial structure more than the desired amount and cause malformation.","Specifically for facial expressions, even a small adversarial attack can have a significant effect on the face structure.","In this paper, we suggest an adversarial attack called $\\epsilon$-Mesh Attack, which operates on point cloud data via limiting perturbations to be on the mesh surface.","We also parameterize our attack by $\\epsilon$ to scale the perturbation mesh.","Our surface-based attack has tighter perturbation bounds compared to $L_2$ and $L_\\infty$ norm bounded attacks that operate on unit-ball.","Even though our method has additional constraints, our experiments on CoMA, Bosphorus and FaceWarehouse datasets show that $\\epsilon$-Mesh Attack (Perpendicular) successfully confuses trained DGCNN and PointNet models $99.72\\%$ and $97.06\\%$ of the time, with indistinguishable facial deformations.","The code is available at https://github.com/batuceng/e-mesh-attack."],"url":"http://arxiv.org/abs/2403.06661v1","category":"cs.CV"}
{"created":"2024-03-11 12:29:35","title":"FashionReGen: LLM-Empowered Fashion Report Generation","abstract":"Fashion analysis refers to the process of examining and evaluating trends, styles, and elements within the fashion industry to understand and interpret its current state, generating fashion reports. It is traditionally performed by fashion professionals based on their expertise and experience, which requires high labour cost and may also produce biased results for relying heavily on a small group of people. In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced Large Language Models (LLMs), debbed as GPT-FAR. Specifically, it tries to deliver FashionReGen based on effective catwalk analysis, which is equipped with several key procedures, namely, catwalk understanding, collective organization and analysis, and report generation. By posing and exploring such an open-ended, complex and domain-specific task of FashionReGen, it is able to test the general capability of LLMs in fashion domain. It also inspires the explorations of more high-level tasks with industrial significance in other domains. Video illustration and more materials of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.","sentences":["Fashion analysis refers to the process of examining and evaluating trends, styles, and elements within the fashion industry to understand and interpret its current state, generating fashion reports.","It is traditionally performed by fashion professionals based on their expertise and experience, which requires high labour cost and may also produce biased results for relying heavily on a small group of people.","In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced Large Language Models (LLMs), debbed as GPT-FAR.","Specifically, it tries to deliver FashionReGen based on effective catwalk analysis, which is equipped with several key procedures, namely, catwalk understanding, collective organization and analysis, and report generation.","By posing and exploring such an open-ended, complex and domain-specific task of FashionReGen, it is able to test the general capability of LLMs in fashion domain.","It also inspires the explorations of more high-level tasks with industrial significance in other domains.","Video illustration and more materials of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen."],"url":"http://arxiv.org/abs/2403.06660v1","category":"cs.MM"}
{"created":"2024-03-11 12:28:55","title":"Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement","abstract":"Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating more descriptive prompts and reducing hallucinations in LLM-generated content to boost zero-shot classification. Based on MERL, we perform the first benchmark across six public ECG datasets, showing the superior performance of MERL compared against eSSL methods. Notably, MERL achieves an average AUC score of 75.2% in zero-shot classification (without training data), 3.2% higher than linear probed eSSL methods with 10\\% annotated training data, averaged across all six datasets.","sentences":["Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice.","While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports.","This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility.","In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework.","Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks.","At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating more descriptive prompts and reducing hallucinations in LLM-generated content to boost zero-shot classification.","Based on MERL, we perform the first benchmark across six public ECG datasets, showing the superior performance of MERL compared against eSSL methods.","Notably, MERL achieves an average AUC score of 75.2% in zero-shot classification (without training data), 3.2% higher than linear probed eSSL methods with 10\\% annotated training data, averaged across all six datasets."],"url":"http://arxiv.org/abs/2403.06659v1","category":"eess.SP"}
{"created":"2024-03-11 12:27:20","title":"Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration Framework","abstract":"Large vision models based in deep learning architectures have been consistently advancing the state-of-the-art in biometric recognition. However, three weaknesses are commonly reported for such kind of approaches: 1) their extreme demands in terms of learning data; 2) the difficulties in generalising between different domains; and 3) the lack of interpretability/explainability, with biometrics being of particular interest, as it is important to provide evidence able to be used for forensics/legal purposes (e.g., in courts). To the best of our knowledge, this paper describes the first recognition framework/strategy that aims at addressing the three weaknesses simultaneously. At first, it relies exclusively in synthetic samples for learning purposes. Instead of requiring a large amount and variety of samples for each subject, the idea is to exclusively enroll a 3D point cloud per identity. Then, using generative strategies, we synthesize a very large (potentially infinite) number of samples, containing all the desired covariates (poses, clothing, distances, perspectives, lighting, occlusions,...). Upon the synthesizing method used, it is possible to adapt precisely to different kind of domains, which accounts for generalization purposes. Such data are then used to learn a model that performs local registration between image pairs, establishing positive correspondences between body parts that are the key, not only to recognition (according to cardinality and distribution), but also to provide an interpretable description of the response (e.g.: \"both samples are from the same person, as they have similar facial shape, hair color and legs thickness\").","sentences":["Large vision models based in deep learning architectures have been consistently advancing the state-of-the-art in biometric recognition.","However, three weaknesses are commonly reported for such kind of approaches: 1) their extreme demands in terms of learning data; 2) the difficulties in generalising between different domains; and 3) the lack of interpretability/explainability, with biometrics being of particular interest, as it is important to provide evidence able to be used for forensics/legal purposes (e.g., in courts).","To the best of our knowledge, this paper describes the first recognition framework/strategy that aims at addressing the three weaknesses simultaneously.","At first, it relies exclusively in synthetic samples for learning purposes.","Instead of requiring a large amount and variety of samples for each subject, the idea is to exclusively enroll a 3D point cloud per identity.","Then, using generative strategies, we synthesize a very large (potentially infinite) number of samples, containing all the desired covariates (poses, clothing, distances, perspectives, lighting, occlusions,...).","Upon the synthesizing method used, it is possible to adapt precisely to different kind of domains, which accounts for generalization purposes.","Such data are then used to learn a model that performs local registration between image pairs, establishing positive correspondences between body parts that are the key, not only to recognition (according to cardinality and distribution), but also to provide an interpretable description of the response (e.g.: \"both samples are from the same person, as they have similar facial shape, hair color and legs thickness\")."],"url":"http://arxiv.org/abs/2403.06658v1","category":"cs.CV"}
{"created":"2024-03-11 12:19:41","title":"A preconditioning for the spectral solution of incompressible variable-density flows","abstract":"In the present study, the efficiency of preconditioners for solving linear systems associated with the discretized variable-density incompressible Navier-Stokes equations with semiimplicit second-order accuracy in time and spectral accuracy in space is investigated. The method, in which the inverse operator for the constant-density flow system acts as preconditioner, is implemented for three iterative solvers: the General Minimal Residual, the Conjugate Gradient and the Richardson Minimal Residual. We discuss the method, first, in the context of the one-dimensional flow case where a top-hat like profile for the density is used. Numerical evidence shows that the convergence is significantly improved due to the notable decrease in the condition number of the operators. Most importantly, we then validate the robustness and convergence properties of the method on two more realistic problems: the two-dimensional Rayleigh-Taylor instability problem and the three-dimensional variable-density swirling jet.","sentences":["In the present study, the efficiency of preconditioners for solving linear systems associated with the discretized variable-density incompressible Navier-Stokes equations with semiimplicit second-order accuracy in time and spectral accuracy in space is investigated.","The method, in which the inverse operator for the constant-density flow system acts as preconditioner, is implemented for three iterative solvers: the General Minimal Residual, the Conjugate Gradient and the Richardson Minimal Residual.","We discuss the method, first, in the context of the one-dimensional flow case where a top-hat like profile for the density is used.","Numerical evidence shows that the convergence is significantly improved due to the notable decrease in the condition number of the operators.","Most importantly, we then validate the robustness and convergence properties of the method on two more realistic problems: the two-dimensional Rayleigh-Taylor instability problem and the three-dimensional variable-density swirling jet."],"url":"http://arxiv.org/abs/2403.06654v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 12:19:11","title":"UAV-Enabled Asynchronous Federated Learning","abstract":"To exploit unprecedented data generation in mobile edge networks, federated learning (FL) has emerged as a promising alternative to the conventional centralized machine learning (ML).   However, there are some critical challenges for FL deployment.   One major challenge called straggler issue severely limits FL's coverage where the device with the weakest channel condition becomes the bottleneck of the model aggregation performance.   Besides, the huge uplink communication overhead compromises the effectiveness of FL, which is particularly pronounced in large-scale systems.   To address the straggler issue, we propose the integration of an unmanned aerial vehicle (UAV) as the parameter server (UAV-PS) to coordinate the FL implementation.   We further employ over-the-air computation technique that leverages the superposition property of wireless channels for efficient uplink communication.   Specifically, in this paper, we develop a novel UAV-enabled over-the-air asynchronous FL (UAV-AFL) framework which supports the UAV-PS in updating the model continuously to enhance the learning performance. Moreover, we conduct a convergence analysis to quantitatively capture the impact of model asynchrony, device selection and communication errors on the UAV-AFL learning performance. Based on this, a unified communication-learning problem is formulated to maximize asymptotical learning performance by optimizing the UAV-PS trajectory, device selection and over-the-air transceiver design. Simulation results demonstrate that the proposed scheme achieves substantially learning efficiency improvement compared with the state-of-the-art approaches.","sentences":["To exploit unprecedented data generation in mobile edge networks, federated learning (FL) has emerged as a promising alternative to the conventional centralized machine learning (ML).   ","However, there are some critical challenges for FL deployment.   ","One major challenge called straggler issue severely limits FL's coverage where the device with the weakest channel condition becomes the bottleneck of the model aggregation performance.   ","Besides, the huge uplink communication overhead compromises the effectiveness of FL, which is particularly pronounced in large-scale systems.   ","To address the straggler issue, we propose the integration of an unmanned aerial vehicle (UAV) as the parameter server (UAV-PS) to coordinate the FL implementation.   ","We further employ over-the-air computation technique that leverages the superposition property of wireless channels for efficient uplink communication.   ","Specifically, in this paper, we develop a novel UAV-enabled over-the-air asynchronous FL (UAV-AFL) framework which supports the UAV-PS in updating the model continuously to enhance the learning performance.","Moreover, we conduct a convergence analysis to quantitatively capture the impact of model asynchrony, device selection and communication errors on the UAV-AFL learning performance.","Based on this, a unified communication-learning problem is formulated to maximize asymptotical learning performance by optimizing the UAV-PS trajectory, device selection and over-the-air transceiver design.","Simulation results demonstrate that the proposed scheme achieves substantially learning efficiency improvement compared with the state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.06653v1","category":"eess.SP"}
{"created":"2024-03-11 12:11:19","title":"Ray Launching-Based Computation of Exact Paths with Noisy Dense Point Clouds","abstract":"Point clouds have been a recent interest for ray tracing-based radio channel characterization, as sensors such as RGB-D cameras and laser scanners can be utilized to generate an accurate virtual copy of a physical environment. In this paper, a novel ray launching algorithm is presented, which operates directly on noisy point clouds acquired from sensor data. It produces coarse paths that are further refined to exact paths consisting of reflections and diffractions. A commercial ray tracing tool is utilized as the baseline for validating the simulated paths. A significant majority of the baseline paths is found. The robustness to noise is examined by artificially applying noise along the normal vector of each point. It is observed that the proposed method is capable of adapting to noise and finds similar paths compared to the baseline path trajectories with noisy point clouds. This is prevalent especially if the normal vectors of the points are estimated accurately. Lastly, a simulation is performed with a reconstructed point cloud and compared against channel measurements and the baseline paths. The resulting paths demonstrate similarity with the baseline path trajectories and exhibit an analogous pattern to the aggregated impulse response extracted from the measurements. Code available at https://github.com/nvaara/NimbusRT","sentences":["Point clouds have been a recent interest for ray tracing-based radio channel characterization, as sensors such as RGB-D cameras and laser scanners can be utilized to generate an accurate virtual copy of a physical environment.","In this paper, a novel ray launching algorithm is presented, which operates directly on noisy point clouds acquired from sensor data.","It produces coarse paths that are further refined to exact paths consisting of reflections and diffractions.","A commercial ray tracing tool is utilized as the baseline for validating the simulated paths.","A significant majority of the baseline paths is found.","The robustness to noise is examined by artificially applying noise along the normal vector of each point.","It is observed that the proposed method is capable of adapting to noise and finds similar paths compared to the baseline path trajectories with noisy point clouds.","This is prevalent especially if the normal vectors of the points are estimated accurately.","Lastly, a simulation is performed with a reconstructed point cloud and compared against channel measurements and the baseline paths.","The resulting paths demonstrate similarity with the baseline path trajectories and exhibit an analogous pattern to the aggregated impulse response extracted from the measurements.","Code available at https://github.com/nvaara/NimbusRT"],"url":"http://arxiv.org/abs/2403.06648v1","category":"eess.SP"}
{"created":"2024-03-11 12:11:04","title":"Positivity and asymptotic behaviour of solutions to a generalized nonlocal fast diffusion equation","abstract":"We study the positivity and asymptotic behaviour of nonnegative solutions of a general nonlocal fast diffusion equation,   \\[\\partial_t u + \\mathcal{L}\\varphi(u) = 0,\\] and the interplay between these two properties. Here $\\mathcal{L}$ is a stable-like operator and $\\varphi$ is a singular nonlinearity.   We start by analysing positivity by means of a weak Harnack inequality satisfied by a related elliptic (nonlocal) equation. Then we use this positivity to establish the asymptotic behaviour: under certain hypotheses on the nonlocal operator and nonlinearity, our solutions behave asymptotically as the Barenblatt solution of the standard fractional fast diffusion equation.   The main difficulty stems from the generality of the operator, which does not allow the use of the methods that were available for the fractional Laplacian. Our results are new even in the case where $\\varphi$ is a power.","sentences":["We study the positivity and asymptotic behaviour of nonnegative solutions of a general nonlocal fast diffusion equation,   \\[\\partial_t u + \\mathcal{L}\\varphi(u) = 0,\\] and the interplay between these two properties.","Here $\\mathcal{L}$ is a stable-like operator and $\\varphi$ is a singular nonlinearity.   ","We start by analysing positivity by means of a weak Harnack inequality satisfied by a related elliptic (nonlocal) equation.","Then we use this positivity to establish the asymptotic behaviour: under certain hypotheses on the nonlocal operator and nonlinearity, our solutions behave asymptotically as the Barenblatt solution of the standard fractional fast diffusion equation.   ","The main difficulty stems from the generality of the operator, which does not allow the use of the methods that were available for the fractional Laplacian.","Our results are new even in the case where $\\varphi$ is a power."],"url":"http://arxiv.org/abs/2403.06647v1","category":"math.AP"}
{"created":"2024-03-11 12:04:28","title":"Spatial features of CO2 for occupancy detection in a naturally ventilated school building","abstract":"Accurate occupancy information helps to improve building energy efficiency and occupant comfort. Occupancy detection methods based on CO2 sensors have received attention due to their low cost and low intrusiveness. In naturally ventilated buildings, the accuracy of CO2-based occupancy detection is generally low in related studies due to the complex ventilation behavior and the difficulty in measuring the actual air exchange through windows. In this study, we present two novel features for occupancy detection based on the spatial distribution of the CO2 concentration. After a quantitative analysis with Support Vector Machine (SVM) as classifier, it was found that the accuracy of occupancy state detection in naturally ventilated rooms could be improved by up to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1 score 0.84) without any ventilation information. With ventilation information, the accuracy reached 87.6 % (F1 score 0.89). The performance of occupancy quantity detection was significantly improved by up to 25.3 percentage points versus baseline, reaching 56 %, with root mean square error (RMSE) of 11.44 occupants, using only CO2-related features. Additional ventilation information further enhanced the performance to 61.8 % (RMSE 9.02 occupants). By incorporating spatial features, the model using only CO2-related features revealed similar performance as the model containing additional ventilation information, resulting in a better low-cost occupancy detection method for naturally ventilated buildings.","sentences":["Accurate occupancy information helps to improve building energy efficiency and occupant comfort.","Occupancy detection methods based on CO2 sensors have received attention due to their low cost and low intrusiveness.","In naturally ventilated buildings, the accuracy of CO2-based occupancy detection is generally low in related studies due to the complex ventilation behavior and the difficulty in measuring the actual air exchange through windows.","In this study, we present two novel features for occupancy detection based on the spatial distribution of the CO2 concentration.","After a quantitative analysis with Support Vector Machine (SVM) as classifier, it was found that the accuracy of occupancy state detection in naturally ventilated rooms could be improved by up to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1 score 0.84) without any ventilation information.","With ventilation information, the accuracy reached 87.6 % (F1 score 0.89).","The performance of occupancy quantity detection was significantly improved by up to 25.3 percentage points versus baseline, reaching 56 %, with root mean square error (RMSE) of 11.44 occupants, using only CO2-related features.","Additional ventilation information further enhanced the performance to 61.8 % (RMSE 9.02 occupants).","By incorporating spatial features, the model using only CO2-related features revealed similar performance as the model containing additional ventilation information, resulting in a better low-cost occupancy detection method for naturally ventilated buildings."],"url":"http://arxiv.org/abs/2403.06643v1","category":"cs.LG"}
{"created":"2024-03-11 12:04:20","title":"KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation","abstract":"The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training. Experiments on public datasets and in-enterprise datasets validate the effectiveness of the proposed method.","sentences":["The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches.","With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction.","However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination.","A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text.","Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method.","In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training.","Experiments on public datasets and in-enterprise datasets validate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.06642v1","category":"cs.IR"}
{"created":"2024-03-11 11:54:55","title":"Visualizing, Analyzing and Constructing L-System from Arborized 3D Model Using a Web Application","abstract":"In biology, arborized structures are well represented and typically complex for visualization and analysis. In order to have a profound understanding of the topology of arborized 3D biological model, higher level abstraction is needed. We aim at constructing an abstraction of arborized 3D biological model to an L-system that provides a generalized formalization in a grammar to represent complex structures. The focus of this paper is to combine 3D visualization, analysis and L-system abstraction into a single web application. We designed a front-end user interface and a back-end. In the front-end, we used A-Frame and defined algorithms to generate and visualize L-systems. In the back-end, we utilized the Vascular Modelling Toolkit's (VMTK) centerline analysis methods to extract important features from the arborized 3D models, which can be applied to L-system generation. In addition, two 3D biological models: lactiferous duct and artery are used as two case studies to verify the functionality of this web application. In conclusion, our web application is able to visualize, analyse and create L-system abstractions of arborized 3D models. This in turn provides workflow-improving benefits, easy accessibility and extensibility.","sentences":["In biology, arborized structures are well represented and typically complex for visualization and analysis.","In order to have a profound understanding of the topology of arborized 3D biological model, higher level abstraction is needed.","We aim at constructing an abstraction of arborized 3D biological model to an L-system that provides a generalized formalization in a grammar to represent complex structures.","The focus of this paper is to combine 3D visualization, analysis and L-system abstraction into a single web application.","We designed a front-end user interface and a back-end.","In the front-end, we used A-Frame and defined algorithms to generate and visualize L-systems.","In the back-end, we utilized the Vascular Modelling Toolkit's (VMTK) centerline analysis methods to extract important features from the arborized 3D models, which can be applied to L-system generation.","In addition, two 3D biological models: lactiferous duct and artery are used as two case studies to verify the functionality of this web application.","In conclusion, our web application is able to visualize, analyse and create L-system abstractions of arborized 3D models.","This in turn provides workflow-improving benefits, easy accessibility and extensibility."],"url":"http://arxiv.org/abs/2403.06638v1","category":"q-bio.TO"}
{"created":"2024-03-11 11:52:26","title":"Tur\u00e1n problems for star-path forests in hypergraphs","abstract":"An $r$-uniform hypergraph ($r$-graph) is linear if any two edges intersect at most one vertex. Let $\\mathcal{F}$ be a given family of $r$-graphs. A hypergraph $H$ is called $\\mathcal{F}$-free if $H$ does not contain any hypergraphs in $\\mathcal{F}$. The Tur\\'{a}n number ${\\rm{ex}}_r(n,\\mathcal{F})$ of $\\mathcal{F}$ is defined as the maximum number of edges of all $\\mathcal{F}$-free $r$-graphs on $n$ vertices, and the linear Tur\\'{a}n number ${\\rm{ex}}^{\\rm{lin}}_r(n,\\mathcal{F})$ of $\\mathcal{F}$ is defined as the Tur\\'{a}n number of $\\mathcal{F}$ in linear host hypergraphs. An $r$-uniform linear path $P^r_\\ell$ of length $\\ell$ is an $r$-graph with edges $e_1,\\cdots,e_\\ell$ such that $|V(e_i)\\cap V(e_j)|=1$ if $|i-j|=1$, and $V(e_i)\\cap V(e_j)=\\emptyset$ for $i\\neq j$ otherwise. Gy\\'{a}rf\\'{a}s et al. [Linear Tur\\'{a}n numbers of acyclic triple systems, European J. Combin., 2022, 103435] obtained an upper bound for the linear Tur\\'{a}n number of $P_\\ell^3$. In this paper, an upper bound for the linear Tur\\'{a}n number of $P_\\ell^r$ is obtained, which generalizes the result of $P_\\ell^3$ to any $P_\\ell^r$. Furthermore, some results for the linear Tur\\'{a}n number and Tur\\'{a}n number of some linear star-path forests are obtained.","sentences":["An $r$-uniform hypergraph ($r$-graph) is linear if any two edges intersect at most one vertex.","Let $\\mathcal{F}$ be a given family of $r$-graphs.","A hypergraph $H$ is called $\\mathcal{F}$-free if $H$ does not contain any hypergraphs in $\\mathcal{F}$. The Tur\\'{a}n number ${\\rm{ex}}_r(n,\\mathcal{F})$ of $\\mathcal{F}$ is defined as the maximum number of edges of all $\\mathcal{F}$-free $r$-graphs on $n$ vertices, and the linear Tur\\'{a}n number ${\\rm{ex}}^{\\rm{lin}}_r(n,\\mathcal{F})$ of $\\mathcal{F}$ is defined as the Tur\\'{a}n number of $\\mathcal{F}$ in linear host hypergraphs.","An $r$-uniform linear path $P^r_\\ell$ of length $\\ell$ is an $r$-graph with edges $e_1,\\cdots,e_\\ell$ such that $|V(e_i)\\cap V(e_j)|=1$ if $|i-j|=1$, and $V(e_i)\\cap V(e_j)=\\emptyset$ for $i\\neq j$ otherwise.","Gy\\'{a}rf\\'{a}s et al.","[Linear Tur\\'{a}n numbers of acyclic triple systems, European J. Combin., 2022, 103435] obtained an upper bound for the linear Tur\\'{a}n number of $P_\\ell^3$. In this paper, an upper bound for the linear Tur\\'{a}n number of $P_\\ell^r$ is obtained, which generalizes the result of $P_\\ell^3$ to any $P_\\ell^r$. Furthermore, some results for the linear Tur\\'{a}n number and Tur\\'{a}n number of some linear star-path forests are obtained."],"url":"http://arxiv.org/abs/2403.06637v1","category":"math.CO"}
{"created":"2024-03-11 11:41:30","title":"Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings","abstract":"In the ever-evolving era of Artificial Intelligence (AI), model performance has constituted a key metric driving innovation, leading to an exponential growth in model size and complexity. However, sustainability and energy efficiency have been critical requirements during deployment in contemporary industrial settings, necessitating the use of data-efficient approaches such as few-shot learning. In this paper, to alleviate the burden of lengthy model training and minimize energy consumption, a finetuning approach to adapt standard object detection models to downstream tasks is examined. Subsequently, a thorough case study and evaluation of the energy demands of the developed models, applied in object detection benchmark datasets from volatile industrial environments is presented. Specifically, different finetuning strategies as well as utilization of ancillary evaluation data during training are examined, and the trade-off between performance and efficiency is highlighted in this low-data regime. Finally, this paper introduces a novel way to quantify this trade-off through a customized Efficiency Factor metric.","sentences":["In the ever-evolving era of Artificial Intelligence (AI), model performance has constituted a key metric driving innovation, leading to an exponential growth in model size and complexity.","However, sustainability and energy efficiency have been critical requirements during deployment in contemporary industrial settings, necessitating the use of data-efficient approaches such as few-shot learning.","In this paper, to alleviate the burden of lengthy model training and minimize energy consumption, a finetuning approach to adapt standard object detection models to downstream tasks is examined.","Subsequently, a thorough case study and evaluation of the energy demands of the developed models, applied in object detection benchmark datasets from volatile industrial environments is presented.","Specifically, different finetuning strategies as well as utilization of ancillary evaluation data during training are examined, and the trade-off between performance and efficiency is highlighted in this low-data regime.","Finally, this paper introduces a novel way to quantify this trade-off through a customized Efficiency Factor metric."],"url":"http://arxiv.org/abs/2403.06631v1","category":"cs.LG"}
{"created":"2024-03-11 11:39:08","title":"Assembly Theory is a weak version of algorithmic complexity based on LZ compression that does not explain or quantify selection or evolution","abstract":"We demonstrate that Assembly Theory, pathway complexity, the assembly index, and the assembly number are subsumed and constitute a weak version of algorithmic (Kolmogorov-Solomonoff-Chaitin) complexity reliant on an approximation method based upon statistical compression, their results obtained due to the use of methods strictly equivalent to the LZ family of compression algorithms used in compressing algorithms such as ZIP, GZIP, or JPEG. Such popular algorithms have been shown to empirically reproduce the results of AT's assembly index and their use had already been reported in successful application to separating organic from non-organic molecules, and the study of selection and evolution. Here we exhibit and prove the connections and full equivalence of Assembly Theory to Shannon Entropy and statistical compression, and AT's disconnection as a statistical approach from causality. We demonstrate that formulating a traditional statistically compressed description of molecules, or the theory underlying it, does not imply an explanation or quantification of biases in generative (physical or biological) processes, including those brought about by selection and evolution, when lacking in logical consistency and empirical evidence. We argue that in their basic arguments, the authors of AT conflate how objects may assemble with causal directionality, and conclude that Assembly Theory does nothing to explain selection or evolution beyond known and previously established connections, some of which are reviewed here, based on sounder theory and better experimental evidence.","sentences":["We demonstrate that Assembly Theory, pathway complexity, the assembly index, and the assembly number are subsumed and constitute a weak version of algorithmic (Kolmogorov-Solomonoff-Chaitin) complexity reliant on an approximation method based upon statistical compression, their results obtained due to the use of methods strictly equivalent to the LZ family of compression algorithms used in compressing algorithms such as ZIP, GZIP, or JPEG.","Such popular algorithms have been shown to empirically reproduce the results of AT's assembly index and their use had already been reported in successful application to separating organic from non-organic molecules, and the study of selection and evolution.","Here we exhibit and prove the connections and full equivalence of Assembly Theory to Shannon Entropy and statistical compression, and AT's disconnection as a statistical approach from causality.","We demonstrate that formulating a traditional statistically compressed description of molecules, or the theory underlying it, does not imply an explanation or quantification of biases in generative (physical or biological) processes, including those brought about by selection and evolution, when lacking in logical consistency and empirical evidence.","We argue that in their basic arguments, the authors of AT conflate how objects may assemble with causal directionality, and conclude that Assembly Theory does nothing to explain selection or evolution beyond known and previously established connections, some of which are reviewed here, based on sounder theory and better experimental evidence."],"url":"http://arxiv.org/abs/2403.06629v1","category":"cs.IT"}
{"created":"2024-03-11 11:36:14","title":"Bulk and interface spin-orbit torques in Pt/Co/MgO thin film structures","abstract":"We investigate the origin of spin-orbit torques (SOTs) in archetypical Pt/Co/MgO thin films structures by performing harmonic Hall measurements. The behaviour of the damping like (DL) effective field ($h_{DL}$) with varying the Pt layer thickness and the Co layer thickness indicates that bulk spin-Hall effect (SHE) in Pt is mainly responsible for DL-SOT. The insertion of a Pd ultrathin layer at the Pt/Co interface leads to a step decrease in $h_{DL}$, attributed to the modification of interfacial spin transparency. Further increase in Pd thickness led to a reduction of the interfacial spin-orbit coupling (iSOC) quantified by the decrease in the surface magnetic anisotropy. The consistent insensitivity of $h_{DL}$ to variations in iSOC at the bottom Pt/Co interface and oxidation at the top Co/MgO interface provides additional evidence for the bulk SHE origin of DL-SOT. The strong reduction in the field-like (FL) torque effective field ($h_{FL}$) with decreasing iSOC at the Pt/Co interface points to the interfacial nature of FL-SOT, either due to iSOC induced interfacial spin-currents or to the Rashba-Edelstein effect at the Pt/Co interface. Furthermore, we demonstrate that a FL-SOT develops at the top Co/MgO interface opposing the one generated at the bottom Pt/Co interface, whose strength increases with Co/MgO interfacial oxidation, and attributed to the Rashba-Edelstein effect.","sentences":["We investigate the origin of spin-orbit torques (SOTs) in archetypical Pt/Co/MgO thin films structures by performing harmonic Hall measurements.","The behaviour of the damping like (DL) effective field ($h_{DL}$) with varying the Pt layer thickness and the Co layer thickness indicates that bulk spin-Hall effect (SHE) in Pt is mainly responsible for DL-SOT.","The insertion of a Pd ultrathin layer at the Pt/Co interface leads to a step decrease in $h_{DL}$, attributed to the modification of interfacial spin transparency.","Further increase in Pd thickness led to a reduction of the interfacial spin-orbit coupling (iSOC) quantified by the decrease in the surface magnetic anisotropy.","The consistent insensitivity of $h_{DL}$ to variations in iSOC at the bottom Pt/Co interface and oxidation at the top Co/MgO interface provides additional evidence for the bulk SHE origin of DL-SOT.","The strong reduction in the field-like (FL) torque effective field ($h_{FL}$) with decreasing iSOC at the Pt/Co interface points to the interfacial nature of FL-SOT, either due to iSOC induced interfacial spin-currents or to the Rashba-Edelstein effect at the Pt/Co interface.","Furthermore, we demonstrate that a FL-SOT develops at the top Co/MgO interface opposing the one generated at the bottom Pt/Co interface, whose strength increases with Co/MgO interfacial oxidation, and attributed to the Rashba-Edelstein effect."],"url":"http://arxiv.org/abs/2403.06627v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-11 11:33:53","title":"AC/DC optimal power flow and techno-economic assessment for hybrid microgrids: TIGON CEDER demonstrator","abstract":"In the recent years, the interest in electric direct current (DC) technologies (such as converters, batteries, electric vehicles, etc.) is increasing due to its potential on energy efficiency and sustainability. However, the vast majority of electric systems and networks are based on alternating current (AC), as they also have certain advantages regarding cost-effective transport and robustness. In this paper, an AC/DC optimal power flow method for hybrid microgrids and several key performance indicators (KPIs) for its techno-economic assessment are presented. The combination of both calculations allows users to clearly determine the viability of their hybrid microgrids. AC/DC networks have been modelled considering their most common elements. For the power flow method, a polynomial optimisation is formulated considering four different objective functions: the minimisation of energy losses, voltage deviation and operational costs, and also the maximisation of the microgrid generation. The power flow method and the techno-economic analysis have been implemented in Python and validated in the Centro de Desarrollo de Energ\\'ias Renovables (CEDER) demonstrator for TIGON. The results show that the calculated power flow variables and the ones measured at CEDER are practically the same. In addition, the KPIs have been obtained and compared for four operating scenarios: baseline, no battery, battery flexibility and virtual battery (VB) flexibility. The last one result in the most profitable option.","sentences":["In the recent years, the interest in electric direct current (DC) technologies (such as converters, batteries, electric vehicles, etc.) is increasing due to its potential on energy efficiency and sustainability.","However, the vast majority of electric systems and networks are based on alternating current (AC), as they also have certain advantages regarding cost-effective transport and robustness.","In this paper, an AC/DC optimal power flow method for hybrid microgrids and several key performance indicators (KPIs) for its techno-economic assessment are presented.","The combination of both calculations allows users to clearly determine the viability of their hybrid microgrids.","AC/DC networks have been modelled considering their most common elements.","For the power flow method, a polynomial optimisation is formulated considering four different objective functions: the minimisation of energy losses, voltage deviation and operational costs, and also the maximisation of the microgrid generation.","The power flow method and the techno-economic analysis have been implemented in Python and validated in the Centro de Desarrollo de Energ\\'ias Renovables (CEDER) demonstrator for TIGON.","The results show that the calculated power flow variables and the ones measured at CEDER are practically the same.","In addition, the KPIs have been obtained and compared for four operating scenarios: baseline, no battery, battery flexibility and virtual battery (VB) flexibility.","The last one result in the most profitable option."],"url":"http://arxiv.org/abs/2403.06625v1","category":"eess.SY"}
{"created":"2024-03-11 11:29:21","title":"Feasibility study on solving the Helmholtz equation in 3D with PINNs","abstract":"Room acoustic simulations at low frequencies often face significant uncertainties of material parameters and boundary conditions due to absorbing material. We discuss the application of Physics-Informed Neural Networks (PINNs) to solve the (forward) Helmholtz equation in three dimensions (3D), employing mini-batch stochastic gradient descent with periodic resampling every 100 iterations for memory-efficient training. Addressing the computational challenges posed by the extension of PINNs from 2D to 3D for acoustics, DeepXDE is used for implementing the forward PINN. The proposed numerical method is benchmarked against an analytical solution of a standing wave field in 3D. The PINN results are also compared to the Finite Element Method (FEM) solutions for a 3D wave field computed with openCFS. The alignment between PINN-generated solutions and analytical/FEM solutions shows the feasibility of PINNs modeling 3D acoustic applications for future inverse problems, and validating the accuracy and reliability of the proposed approach. Compared to FEM, establishing the PINN model took few hours (similar to the setup of a FEM simulation), the training took 38h to 42.8h (which is longer than the solution of the FEM simulation, which took 17min-19min), and the inference took 0.05 seconds being more than 20,000 times faster than the FEM benchmark openCFS using the same number of degrees of freedomwhen producing the results. Thereby, the insight is gained that 3D acoustic wave simulations in the frequency domain are feasible for forward PINNs and can predict complex wave behaviors in real-world applications.","sentences":["Room acoustic simulations at low frequencies often face significant uncertainties of material parameters and boundary conditions due to absorbing material.","We discuss the application of Physics-Informed Neural Networks (PINNs) to solve the (forward) Helmholtz equation in three dimensions (3D), employing mini-batch stochastic gradient descent with periodic resampling every 100 iterations for memory-efficient training.","Addressing the computational challenges posed by the extension of PINNs from 2D to 3D for acoustics, DeepXDE is used for implementing the forward PINN.","The proposed numerical method is benchmarked against an analytical solution of a standing wave field in 3D.","The PINN results are also compared to the Finite Element Method (FEM) solutions for a 3D wave field computed with openCFS.","The alignment between PINN-generated solutions and analytical/FEM solutions shows the feasibility of PINNs modeling 3D acoustic applications for future inverse problems, and validating the accuracy and reliability of the proposed approach.","Compared to FEM, establishing the PINN model took few hours (similar to the setup of a FEM simulation), the training took 38h to 42.8h (which is longer than the solution of the FEM simulation, which took 17min-19min), and the inference took 0.05 seconds being more than 20,000 times faster than the FEM benchmark openCFS using the same number of degrees of freedomwhen producing the results.","Thereby, the insight is gained that 3D acoustic wave simulations in the frequency domain are feasible for forward PINNs and can predict complex wave behaviors in real-world applications."],"url":"http://arxiv.org/abs/2403.06623v1","category":"physics.comp-ph"}
{"created":"2024-03-11 11:26:44","title":"Forest Inspection Dataset for Aerial Semantic Segmentation and Depth Estimation","abstract":"Humans use UAVs to monitor changes in forest environments since they are lightweight and provide a large variety of surveillance data. However, their information does not present enough details for understanding the scene which is needed to assess the degree of deforestation. Deep learning algorithms must be trained on large amounts of data to output accurate interpretations, but ground truth recordings of annotated forest imagery are not available. To solve this problem, we introduce a new large aerial dataset for forest inspection which contains both real-world and virtual recordings of natural environments, with densely annotated semantic segmentation labels and depth maps, taken in different illumination conditions, at various altitudes and recording angles. We test the performance of two multi-scale neural networks for solving the semantic segmentation task (HRNet and PointFlow network), studying the impact of the various acquisition conditions and the capabilities of transfer learning from virtual to real data. Our results showcase that the best results are obtained when the training is done on a dataset containing a large variety of scenarios, rather than separating the data into specific categories. We also develop a framework to assess the deforestation degree of an area.","sentences":["Humans use UAVs to monitor changes in forest environments since they are lightweight and provide a large variety of surveillance data.","However, their information does not present enough details for understanding the scene which is needed to assess the degree of deforestation.","Deep learning algorithms must be trained on large amounts of data to output accurate interpretations, but ground truth recordings of annotated forest imagery are not available.","To solve this problem, we introduce a new large aerial dataset for forest inspection which contains both real-world and virtual recordings of natural environments, with densely annotated semantic segmentation labels and depth maps, taken in different illumination conditions, at various altitudes and recording angles.","We test the performance of two multi-scale neural networks for solving the semantic segmentation task (HRNet and PointFlow network), studying the impact of the various acquisition conditions and the capabilities of transfer learning from virtual to real data.","Our results showcase that the best results are obtained when the training is done on a dataset containing a large variety of scenarios, rather than separating the data into specific categories.","We also develop a framework to assess the deforestation degree of an area."],"url":"http://arxiv.org/abs/2403.06621v1","category":"cs.CV"}
{"created":"2024-03-11 11:12:38","title":"Mapping a dissipative quantum spin chain onto a generalized Coulomb gas","abstract":"An XXZ spin chain at zero magnetization is subject to spatially correlated baths acting as dissipation. We show that the low-energy excitations of this model are described by a dissipative sine-Gordon field theory, i.e. a sine-Gordon action with an additional long-range interaction emerging from dissipation. The field theory is then exactly mapped onto a generalized Coulomb gas which, in addition to the usual integer charges, displays half-integer charges that originate from the dissipative baths. These new charges come in pairs linked by a charge-independent logarithmic interaction. In the Coulomb gas picture, we identify a Berezinsky-Kosterlitz-Thouless-like phase transition corresponding to the binding of charges and derive the associated perturbative renormalization group equations. For superohmic baths, the transition is due to the binding of the integer charges, while for subohmic baths, it is due to the binding of the half-integer charges, thereby signaling a dissipation-induced transition.","sentences":["An XXZ spin chain at zero magnetization is subject to spatially correlated baths acting as dissipation.","We show that the low-energy excitations of this model are described by a dissipative sine-Gordon field theory, i.e. a sine-Gordon action with an additional long-range interaction emerging from dissipation.","The field theory is then exactly mapped onto a generalized Coulomb gas which, in addition to the usual integer charges, displays half-integer charges that originate from the dissipative baths.","These new charges come in pairs linked by a charge-independent logarithmic interaction.","In the Coulomb gas picture, we identify a Berezinsky-Kosterlitz-Thouless-like phase transition corresponding to the binding of charges and derive the associated perturbative renormalization group equations.","For superohmic baths, the transition is due to the binding of the integer charges, while for subohmic baths, it is due to the binding of the half-integer charges, thereby signaling a dissipation-induced transition."],"url":"http://arxiv.org/abs/2403.06618v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-11 11:10:42","title":"Breaking Abbe's diffraction limit with harmonic deactivation microscopy","abstract":"Nonlinear optical microscopy provides elegant means for label-free imaging of biological samples and condensed matter systems. The widespread areas of application could even be increased if resolution was improved, which is currently limited by the famous Abbe diffraction limit. Super-resolution techniques can break the diffraction limit but rely on fluorescent labeling. This makes them incompatible with (sub-)femtosecond temporal resolution and applications that demand the absence of labeling. Here, we introduce harmonic deactivation microscopy (HADES) for breaking the diffraction limit in non-fluorescent samples. By controlling the harmonic generation process on the quantum level with a second donut-shaped pulse, we confine the third harmonic generation to three times below the original focus size and use this pulse for scanning microscopy. We demonstrate that resolution improvement by deactivation is more efficient for higher harmonic orders, and only limited by the maximum applicable deactivation-pulse fluence. This provides a route towards sub-100~nm resolution in a regular nonlinear microscope. The new capability of label-free super-resolution can find immediate applications in condensed matter physics, semiconductor metrology, and biomedical imaging.","sentences":["Nonlinear optical microscopy provides elegant means for label-free imaging of biological samples and condensed matter systems.","The widespread areas of application could even be increased if resolution was improved, which is currently limited by the famous Abbe diffraction limit.","Super-resolution techniques can break the diffraction limit but rely on fluorescent labeling.","This makes them incompatible with (sub-)femtosecond temporal resolution and applications that demand the absence of labeling.","Here, we introduce harmonic deactivation microscopy (HADES) for breaking the diffraction limit in non-fluorescent samples.","By controlling the harmonic generation process on the quantum level with a second donut-shaped pulse, we confine the third harmonic generation to three times below the original focus size and use this pulse for scanning microscopy.","We demonstrate that resolution improvement by deactivation is more efficient for higher harmonic orders, and only limited by the maximum applicable deactivation-pulse fluence.","This provides a route towards sub-100~nm resolution in a regular nonlinear microscope.","The new capability of label-free super-resolution can find immediate applications in condensed matter physics, semiconductor metrology, and biomedical imaging."],"url":"http://arxiv.org/abs/2403.06617v1","category":"physics.optics"}
{"created":"2024-03-11 11:03:10","title":"The influence of different coronal hole geometries on simulations of coronal wave - coronal hole interaction","abstract":"The geometry of a coronal hole (CH) affects the density profile of the reflected part of an incoming global coronal wave (CW). In this study, we perform for the first time magnetohydrodynamic (MHD) simulations of fast-mode MHD waves interacting with CHs of different geometries, such as circular, elliptic, convex, and concave shapes. We analyse the influence these geometries have on the density profiles of the reflected waves and we generate the corresponding simulation-based time-distance plots. Within these time-distance plots we determine regions that exhibit specific density features, such as large reflected density amplitudes. In a further step, these interaction features can be compared to actual observed CW-CH interaction events which makes it possible to explain interaction parameters from the observed interaction events, such as the density structure of the reflected wave, which are usually difficult to comprehensively understand by only analysing the measurements. Moreover, we show that the interaction between a concave shaped CH and CWs, whose density profile include an enhanced as well as a depleted wave, can lead to reflected density amplitudes that are more than two times larger than the incoming ones. Another effect of the interplay between the constructive and destructive interference of the reflected wave parts is a strongly depleted region in the middle of the CW-CH interaction process. In addition, we show how important the choice of the path is that is used to generate the time-distance plots and how this choice affects the interpretation of the CW-CH interaction results.","sentences":["The geometry of a coronal hole (CH) affects the density profile of the reflected part of an incoming global coronal wave (CW).","In this study, we perform for the first time magnetohydrodynamic (MHD) simulations of fast-mode MHD waves interacting with CHs of different geometries, such as circular, elliptic, convex, and concave shapes.","We analyse the influence these geometries have on the density profiles of the reflected waves and we generate the corresponding simulation-based time-distance plots.","Within these time-distance plots we determine regions that exhibit specific density features, such as large reflected density amplitudes.","In a further step, these interaction features can be compared to actual observed CW-CH interaction events which makes it possible to explain interaction parameters from the observed interaction events, such as the density structure of the reflected wave, which are usually difficult to comprehensively understand by only analysing the measurements.","Moreover, we show that the interaction between a concave shaped CH and CWs, whose density profile include an enhanced as well as a depleted wave, can lead to reflected density amplitudes that are more than two times larger than the incoming ones.","Another effect of the interplay between the constructive and destructive interference of the reflected wave parts is a strongly depleted region in the middle of the CW-CH interaction process.","In addition, we show how important the choice of the path is that is used to generate the time-distance plots and how this choice affects the interpretation of the CW-CH interaction results."],"url":"http://arxiv.org/abs/2403.06614v1","category":"astro-ph.SR"}
{"created":"2024-03-11 11:01:31","title":"Maxitive functions with respect to general orders","abstract":"In decision-making, maxitive functions are used for worst-case and best-case evaluations. Maxitivity gives rise to a rich structure that is well-studied in the context of the pointwise order. In this article, we investigate maxitivity with respect to general preorders and provide a representation theorem for such functionals. The results are illustrated for different stochastic orders in the literature, including the usual stochastic order, the increasing convex/concave order, and the dispersive order.","sentences":["In decision-making, maxitive functions are used for worst-case and best-case evaluations.","Maxitivity gives rise to a rich structure that is well-studied in the context of the pointwise order.","In this article, we investigate maxitivity with respect to general preorders and provide a representation theorem for such functionals.","The results are illustrated for different stochastic orders in the literature, including the usual stochastic order, the increasing convex/concave order, and the dispersive order."],"url":"http://arxiv.org/abs/2403.06613v1","category":"math.ST"}
{"created":"2024-03-11 10:59:55","title":"Pulling back symmetric Riemannian geometry for data analysis","abstract":"Data sets tend to live in low-dimensional non-linear subspaces. Ideal data analysis tools for such data sets should therefore account for such non-linear geometry. The symmetric Riemannian geometry setting can be suitable for a variety of reasons. First, it comes with a rich mathematical structure to account for a wide range of non-linear geometries that has been shown to be able to capture the data geometry through empirical evidence from classical non-linear embedding. Second, many standard data analysis tools initially developed for data in Euclidean space can also be generalised efficiently to data on a symmetric Riemannian manifold. A conceptual challenge comes from the lack of guidelines for constructing a symmetric Riemannian structure on the data space itself and the lack of guidelines for modifying successful algorithms on symmetric Riemannian manifolds for data analysis to this setting. This work considers these challenges in the setting of pullback Riemannian geometry through a diffeomorphism. The first part of the paper characterises diffeomorphisms that result in proper, stable and efficient data analysis. The second part then uses these best practices to guide construction of such diffeomorphisms through deep learning. As a proof of concept, different types of pullback geometries -- among which the proposed construction -- are tested on several data analysis tasks and on several toy data sets. The numerical experiments confirm the predictions from theory, i.e., that the diffeomorphisms generating the pullback geometry need to map the data manifold into a geodesic subspace of the pulled back Riemannian manifold while preserving local isometry around the data manifold for proper, stable and efficient data analysis, and that pulling back positive curvature can be problematic in terms of stability.","sentences":["Data sets tend to live in low-dimensional non-linear subspaces.","Ideal data analysis tools for such data sets should therefore account for such non-linear geometry.","The symmetric Riemannian geometry setting can be suitable for a variety of reasons.","First, it comes with a rich mathematical structure to account for a wide range of non-linear geometries that has been shown to be able to capture the data geometry through empirical evidence from classical non-linear embedding.","Second, many standard data analysis tools initially developed for data in Euclidean space can also be generalised efficiently to data on a symmetric Riemannian manifold.","A conceptual challenge comes from the lack of guidelines for constructing a symmetric Riemannian structure on the data space itself and the lack of guidelines for modifying successful algorithms on symmetric Riemannian manifolds for data analysis to this setting.","This work considers these challenges in the setting of pullback Riemannian geometry through a diffeomorphism.","The first part of the paper characterises diffeomorphisms that result in proper, stable and efficient data analysis.","The second part then uses these best practices to guide construction of such diffeomorphisms through deep learning.","As a proof of concept, different types of pullback geometries -- among which the proposed construction -- are tested on several data analysis tasks and on several toy data sets.","The numerical experiments confirm the predictions from theory, i.e., that the diffeomorphisms generating the pullback geometry need to map the data manifold into a geodesic subspace of the pulled back Riemannian manifold while preserving local isometry around the data manifold for proper, stable and efficient data analysis, and that pulling back positive curvature can be problematic in terms of stability."],"url":"http://arxiv.org/abs/2403.06612v1","category":"math.DG"}
{"created":"2024-03-11 10:57:45","title":"MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding","abstract":"With appropriate data selection and training techniques, Large Language Models (LLMs) have demonstrated exceptional success in various medical examinations and multiple-choice questions. However, the application of LLMs in medical dialogue generation-a task more closely aligned with actual medical practice-has been less explored. This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses. In this work, we introduce the Medical dialogue with Knowledge enhancement and clinical Pathway encoding (MedKP) framework, which integrates an external knowledge enhancement module through a medical knowledge graph and an internal clinical pathway encoding via medical entities and physician actions. Evaluated with comprehensive metrics, our experiments on two large-scale, real-world online medical consultation datasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines and mitigates the incidence of hallucinations, achieving a new state-of-the-art. Extensive ablation studies further reveal the effectiveness of each component of MedKP. This enhancement advances the development of reliable, automated medical consultation responses using LLMs, thereby broadening the potential accessibility of precise and real-time medical assistance.","sentences":["With appropriate data selection and training techniques, Large Language Models (LLMs) have demonstrated exceptional success in various medical examinations and multiple-choice questions.","However, the application of LLMs in medical dialogue generation-a task more closely aligned with actual medical practice-has been less explored.","This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses.","In this work, we introduce the Medical dialogue with Knowledge enhancement and clinical Pathway encoding (MedKP) framework, which integrates an external knowledge enhancement module through a medical knowledge graph and an internal clinical pathway encoding via medical entities and physician actions.","Evaluated with comprehensive metrics, our experiments on two large-scale, real-world online medical consultation datasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines and mitigates the incidence of hallucinations, achieving a new state-of-the-art.","Extensive ablation studies further reveal the effectiveness of each component of MedKP.","This enhancement advances the development of reliable, automated medical consultation responses using LLMs, thereby broadening the potential accessibility of precise and real-time medical assistance."],"url":"http://arxiv.org/abs/2403.06611v1","category":"cs.CL"}
{"created":"2024-03-11 10:53:20","title":"Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds","abstract":"Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians. In this study, we introduce a novel framework, In-Context Padding (ICP), designed to enhance LLMs with medical knowledge. Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs. Experiments on two clinical question datasets demonstrate that ICP significantly improves the clinical reasoning ability of LLMs.","sentences":["Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients.","This process typically involves suggesting necessary examinations, diagnosing patients' diseases, and deciding on appropriate therapies, etc.","Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians.","This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches.","Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning.","However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision path of physicians.","In this study, we introduce a novel framework, In-Context Padding (ICP), designed to enhance LLMs with medical knowledge.","Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs.","Experiments on two clinical question datasets demonstrate that ICP significantly improves the clinical reasoning ability of LLMs."],"url":"http://arxiv.org/abs/2403.06609v1","category":"cs.CL"}
{"created":"2024-03-11 10:50:53","title":"Distributionally Generative Augmentation for Fair Facial Attribute Classification","abstract":"Facial Attribute Classification (FAC) holds substantial promise in widespread applications. However, FAC models trained by traditional methodologies can be unfair by exhibiting accuracy inconsistencies across varied data subpopulations. This unfairness is largely attributed to bias in data, where some spurious attributes (e.g., Male) statistically correlate with the target attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the labels of spurious attributes, which may be unavailable in practice. This work proposes a novel, generation-based two-stage framework to train a fair FAC model on biased data without additional annotation. Initially, we identify the potential spurious attributes based on generative models. Notably, it enhances interpretability by explicitly showing the spurious attributes in image space. Following this, for each image, we first edit the spurious attributes with a random degree sampled from a uniform distribution, while keeping target attribute unchanged. Then we train a fair FAC model by fostering model invariance to these augmentation. Extensive experiments on three common datasets demonstrate the effectiveness of our method in promoting fairness in FAC without compromising accuracy. Codes are in https://github.com/heqianpei/DiGA.","sentences":["Facial Attribute Classification (FAC) holds substantial promise in widespread applications.","However, FAC models trained by traditional methodologies can be unfair by exhibiting accuracy inconsistencies across varied data subpopulations.","This unfairness is largely attributed to bias in data, where some spurious attributes (e.g., Male) statistically correlate with the target attribute (e.g., Smiling).","Most of existing fairness-aware methods rely on the labels of spurious attributes, which may be unavailable in practice.","This work proposes a novel, generation-based two-stage framework to train a fair FAC model on biased data without additional annotation.","Initially, we identify the potential spurious attributes based on generative models.","Notably, it enhances interpretability by explicitly showing the spurious attributes in image space.","Following this, for each image, we first edit the spurious attributes with a random degree sampled from a uniform distribution, while keeping target attribute unchanged.","Then we train a fair FAC model by fostering model invariance to these augmentation.","Extensive experiments on three common datasets demonstrate the effectiveness of our method in promoting fairness in FAC without compromising accuracy.","Codes are in https://github.com/heqianpei/DiGA."],"url":"http://arxiv.org/abs/2403.06606v1","category":"cs.CV"}
{"created":"2024-03-11 10:50:33","title":"On the temporal resolution limits of numerical simulations in complex systems","abstract":"In this paper we formalize, using the Nyquist-Shannon theorem, a fundamental temporal resolution limit for numerical experiments in complex systems. A consequence of this limit is aliasing, the introduction of spurious frequencies due to sampling. By imposing these limits on the uncertainty principle in harmonic analysis, we show that by increasing the sampling interval $\\Delta t$, we can also artificially stretch the temporal behavior of our numerical experiment. Importantly, in limiting cases, we could even observe a new artificially created absorbing state. Our findings are validated in deterministic and stochastic simulations. In deterministic systems, we analyzed the Kuramoto model in which aliasing could be observed. In stochastic simulations, we formalized and compared different simulation approaches and showed their temporal limits. Gillespie-like simulations fully capture the continuous-time Markov chain processes, being lossless. Asynchronous cellular automata methods capture the same transitions as the continuous-time process but lose the temporal information about the process. Finally, synchronous cellular automata simulations solve a sampled chain. By comparing these methods, we show that if $\\Delta t$ is not small enough, the cellular automata approach fails to capture the original continuous-time Markov chain since the sampling is already built into the simulation method. Our results point to a fundamental limitation that cannot be overcome by traditional methods of numerical simulations.","sentences":["In this paper we formalize, using the Nyquist-Shannon theorem, a fundamental temporal resolution limit for numerical experiments in complex systems.","A consequence of this limit is aliasing, the introduction of spurious frequencies due to sampling.","By imposing these limits on the uncertainty principle in harmonic analysis, we show that by increasing the sampling interval $\\Delta t$, we can also artificially stretch the temporal behavior of our numerical experiment.","Importantly, in limiting cases, we could even observe a new artificially created absorbing state.","Our findings are validated in deterministic and stochastic simulations.","In deterministic systems, we analyzed the Kuramoto model in which aliasing could be observed.","In stochastic simulations, we formalized and compared different simulation approaches and showed their temporal limits.","Gillespie-like simulations fully capture the continuous-time Markov chain processes, being lossless.","Asynchronous cellular automata methods capture the same transitions as the continuous-time process but lose the temporal information about the process.","Finally, synchronous cellular automata simulations solve a sampled chain.","By comparing these methods, we show that if $\\Delta t$ is not small enough, the cellular automata approach fails to capture the original continuous-time Markov chain since the sampling is already built into the simulation method.","Our results point to a fundamental limitation that cannot be overcome by traditional methods of numerical simulations."],"url":"http://arxiv.org/abs/2403.06605v1","category":"physics.soc-ph"}
{"created":"2024-03-11 10:50:07","title":"Rapid state-recrossing kinetics in non-Markovian systems","abstract":"The mean first-passage time (MFPT) is one standard measure for the reaction time in thermally activated barrier-crossing processes. While the relationship between MFPTs and phenomenological rate coefficients is known for systems that satisfy Markovian dynamics, it is not clear how to interpret MFPTs for experimental and simulation time-series data generated by non-Markovian systems. Here, we simulate a one-dimensional generalized Langevin equation (GLE) in a bistable potential and compare two related numerical methods for evaluating MFPTs: one that only incorporates information about first arrivals between subsequent states and is equivalent to calculating the waiting time, or dwell time, and one that incorporates information about all first-passages associated with a given barrier-crossing event and is therefore typically employed to enhance numerical sampling. In the Markovian limit, the two methods are equivalent. However, for significant memory times, the two methods suggest dramatically different reaction kinetics. By focusing on first-passage distributions, we systematically reveal the influence of memory-induced rapid state-recrossing on the MFPTs, which we compare to various other numerical or theoretical descriptions of reaction times. Overall, we demonstrate that it is necessary to consider full first-passage distributions, rather than just the mean barrier-crossing kinetics when analyzing non-Markovian time series data.","sentences":["The mean first-passage time (MFPT) is one standard measure for the reaction time in thermally activated barrier-crossing processes.","While the relationship between MFPTs and phenomenological rate coefficients is known for systems that satisfy Markovian dynamics, it is not clear how to interpret MFPTs for experimental and simulation time-series data generated by non-Markovian systems.","Here, we simulate a one-dimensional generalized Langevin equation (GLE) in a bistable potential and compare two related numerical methods for evaluating MFPTs: one that only incorporates information about first arrivals between subsequent states and is equivalent to calculating the waiting time, or dwell time, and one that incorporates information about all first-passages associated with a given barrier-crossing event and is therefore typically employed to enhance numerical sampling.","In the Markovian limit, the two methods are equivalent.","However, for significant memory times, the two methods suggest dramatically different reaction kinetics.","By focusing on first-passage distributions, we systematically reveal the influence of memory-induced rapid state-recrossing on the MFPTs, which we compare to various other numerical or theoretical descriptions of reaction times.","Overall, we demonstrate that it is necessary to consider full first-passage distributions, rather than just the mean barrier-crossing kinetics when analyzing non-Markovian time series data."],"url":"http://arxiv.org/abs/2403.06604v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-11 10:48:56","title":"Cross-domain and Cross-dimension Learning for Image-to-Graph Transformers","abstract":"Direct image-to-graph transformation is a challenging task that solves object detection and relationship prediction in a single model. Due to the complexity of this task, large training datasets are rare in many domains, which makes the training of large networks challenging. This data sparsity necessitates the establishment of pre-training strategies akin to the state-of-the-art in computer vision. In this work, we introduce a set of methods enabling cross-domain and cross-dimension transfer learning for image-to-graph transformers. We propose (1) a regularized edge sampling loss for sampling the optimal number of object relationships (edges) across domains, (2) a domain adaptation framework for image-to-graph transformers that aligns features from different domains, and (3) a simple projection function that allows us to pretrain 3D transformers on 2D input data. We demonstrate our method's utility in cross-domain and cross-dimension experiments, where we pretrain our models on 2D satellite images before applying them to vastly different target domains in 2D and 3D. Our method consistently outperforms a series of baselines on challenging benchmarks, such as retinal or whole-brain vessel graph extraction.","sentences":["Direct image-to-graph transformation is a challenging task that solves object detection and relationship prediction in a single model.","Due to the complexity of this task, large training datasets are rare in many domains, which makes the training of large networks challenging.","This data sparsity necessitates the establishment of pre-training strategies akin to the state-of-the-art in computer vision.","In this work, we introduce a set of methods enabling cross-domain and cross-dimension transfer learning for image-to-graph transformers.","We propose (1) a regularized edge sampling loss for sampling the optimal number of object relationships (edges) across domains, (2) a domain adaptation framework for image-to-graph transformers that aligns features from different domains, and (3) a simple projection function that allows us to pretrain 3D transformers on 2D input data.","We demonstrate our method's utility in cross-domain and cross-dimension experiments, where we pretrain our models on 2D satellite images before applying them to vastly different target domains in 2D and 3D.","Our method consistently outperforms a series of baselines on challenging benchmarks, such as retinal or whole-brain vessel graph extraction."],"url":"http://arxiv.org/abs/2403.06601v1","category":"cs.CV"}
{"created":"2024-03-11 10:46:43","title":"BEV2PR: BEV-Enhanced Visual Place Recognition with Structural Cues","abstract":"In this paper, we propose a new image-based visual place recognition (VPR) framework by exploiting the structural cues in bird's-eye view (BEV) from a single monocular camera. The motivation arises from two key observations about VPR: 1) For the methods based on both camera and LiDAR sensors, the integration of LiDAR in robotic systems has led to increased expenses, while the alignment of data between different sensors is also a major challenge. 2) Other image-/camera-based methods, involving integrating RGB images and their derived variants (e.g., pseudo depth images, pseudo 3D point clouds), exhibit several limitations, such as the failure to effectively exploit the explicit spatial relationships between different objects. To tackle the above issues, we design a new BEV-enhanced VPR framework, nemely BEV2PR, which can generate a composite descriptor with both visual cues and spatial awareness solely based on a single camera. For the visual cues, any popular aggregation module for RGB global features can be integrated into our framework. The key points lie in: 1) We use BEV segmentation features as an explicit source of structural knowledge in constructing global features. 2) The lower layers of the pre-trained backbone from BEV map generation are shared for visual and structural streams in VPR, facilitating the learning of fine-grained local features in the visual stream. 3) The complementary visual features and structural features can jointly enhance VPR performance. Our BEV2PR framework enables consistent performance improvements over several popular camera-based VPR aggregation modules when integrating them. The experiments on our collected VPR-NuScenes dataset demonstrate an absolute gain of 2.47% on Recall@1 for the strong Conv-AP baseline to achieve the best performance in our setting, and notably, a 18.06% gain on the hard set.","sentences":["In this paper, we propose a new image-based visual place recognition (VPR) framework by exploiting the structural cues in bird's-eye view (BEV) from a single monocular camera.","The motivation arises from two key observations about VPR: 1) For the methods based on both camera and LiDAR sensors, the integration of LiDAR in robotic systems has led to increased expenses, while the alignment of data between different sensors is also a major challenge.","2) Other image-/camera-based methods, involving integrating RGB images and their derived variants (e.g., pseudo depth images, pseudo 3D point clouds), exhibit several limitations, such as the failure to effectively exploit the explicit spatial relationships between different objects.","To tackle the above issues, we design a new BEV-enhanced VPR framework, nemely BEV2PR, which can generate a composite descriptor with both visual cues and spatial awareness solely based on a single camera.","For the visual cues, any popular aggregation module for RGB global features can be integrated into our framework.","The key points lie in: 1) We use BEV segmentation features as an explicit source of structural knowledge in constructing global features.","2) The lower layers of the pre-trained backbone from BEV map generation are shared for visual and structural streams in VPR, facilitating the learning of fine-grained local features in the visual stream.","3)","The complementary visual features and structural features can jointly enhance VPR performance.","Our BEV2PR framework enables consistent performance improvements over several popular camera-based VPR aggregation modules when integrating them.","The experiments on our collected VPR-NuScenes dataset demonstrate an absolute gain of 2.47% on Recall@1 for the strong Conv-AP baseline to achieve the best performance in our setting, and notably, a 18.06% gain on the hard set."],"url":"http://arxiv.org/abs/2403.06600v1","category":"cs.CV"}
{"created":"2024-03-11 10:44:47","title":"Inhomogeneous probes for BCDI: Toward the imaging of dynamic and distorted crystals","abstract":"This work proposes an innovative approach to improve Bragg coherent diffraction imaging (BCDI) microscopy applied to time evolving crystals and/or non-homogeneous crystalline strain fields, identified as two major limitations of BCDI microscopy. Speckle BCDI (spBCDI), introduced here, rests on the ability of a strongly non-uniform illumination to induce a convolution of the three-dimensional (3D) frequency content associated with the finite-size crystal and a kernel acting perpendicularly to the illumination beam. In the framework of Bragg diffraction geometry, this convolution is beneficial as it encodes some 3D information about the sample in a single two-dimensional (2D) measurement, i.e., in the detector plane. With this approach, we demonstrate that we can drastically reduce the sampling frequency along the rocking curve direction and still obtain data sets with enough information to be inverted by a traditional phase retrieval algorithm.   Numerical simulations, performed for a highly distorted crystal, show that spBCDI allows a gain in the sampling ratio ranging between 4 and 20 along the rocking curve scan, for a speckle illumination with individual speckle size of 50 nm. Furthermore, spBCDI allows working at low intensity levels, leading to an additional gain for the total scanning time. Reductions of a factor of about 32 were numerically observed. Thus, measurements in the 0.3 s time scale at 4th generation synchrotrons become feasible, with a remarkable performance for the imaging of strongly distorted crystals. Practical details on the implementation of the method are also discussed.","sentences":["This work proposes an innovative approach to improve Bragg coherent diffraction imaging (BCDI) microscopy applied to time evolving crystals and/or non-homogeneous crystalline strain fields, identified as two major limitations of BCDI microscopy.","Speckle BCDI (spBCDI), introduced here, rests on the ability of a strongly non-uniform illumination to induce a convolution of the three-dimensional (3D) frequency content associated with the finite-size crystal and a kernel acting perpendicularly to the illumination beam.","In the framework of Bragg diffraction geometry, this convolution is beneficial as it encodes some 3D information about the sample in a single two-dimensional (2D) measurement, i.e., in the detector plane.","With this approach, we demonstrate that we can drastically reduce the sampling frequency along the rocking curve direction and still obtain data sets with enough information to be inverted by a traditional phase retrieval algorithm.   ","Numerical simulations, performed for a highly distorted crystal, show that spBCDI allows a gain in the sampling ratio ranging between 4 and 20 along the rocking curve scan, for a speckle illumination with individual speckle size of 50 nm.","Furthermore, spBCDI allows working at low intensity levels, leading to an additional gain for the total scanning time.","Reductions of a factor of about 32 were numerically observed.","Thus, measurements in the 0.3 s time scale at 4th generation synchrotrons become feasible, with a remarkable performance for the imaging of strongly distorted crystals.","Practical details on the implementation of the method are also discussed."],"url":"http://arxiv.org/abs/2403.06598v1","category":"physics.optics"}
{"created":"2024-03-11 10:37:39","title":"Authorship and the Politics and Ethics of LLM Watermarks","abstract":"Recently, watermarking schemes for large language models (LLMs) have been proposed to distinguish text generated by machines and by humans. The present paper explores philosophical, political, and ethical ramifications of implementing and using watermarking schemes. A definition of authorship that includes both machines (LLMs) and humans is proposed to serve as a backdrop. It is argued that private watermarks may provide private companies with sweeping rights to determine authorship, which is incompatible with traditional standards of authorship determination. Then, possible ramifications of the so-called entropy dependence of watermarking mechanisms are explored. It is argued that entropy may vary for different, socially salient groups. This could lead to group dependent rates at which machine generated text is detected. Specifically, groups more interested in low entropy text may face the challenge that it is harder to detect machine generated text that is of interest to them.","sentences":["Recently, watermarking schemes for large language models (LLMs) have been proposed to distinguish text generated by machines and by humans.","The present paper explores philosophical, political, and ethical ramifications of implementing and using watermarking schemes.","A definition of authorship that includes both machines (LLMs) and humans is proposed to serve as a backdrop.","It is argued that private watermarks may provide private companies with sweeping rights to determine authorship, which is incompatible with traditional standards of authorship determination.","Then, possible ramifications of the so-called entropy dependence of watermarking mechanisms are explored.","It is argued that entropy may vary for different, socially salient groups.","This could lead to group dependent rates at which machine generated text is detected.","Specifically, groups more interested in low entropy text may face the challenge that it is harder to detect machine generated text that is of interest to them."],"url":"http://arxiv.org/abs/2403.06593v1","category":"cs.CY"}
{"created":"2024-03-11 10:35:58","title":"Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection","abstract":"This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations. Our framework utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with content-based features, enabling the detection of visual and temporal artifacts. We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios. Through further analysis, we also validate the importance of using temporal changes of style latent vectors to improve the generality of deepfake video detection.","sentences":["This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos.","We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations.","Our framework utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors.","Additionally, we introduce a style attention module that integrates StyleGRU-generated features with content-based features, enabling the detection of visual and temporal artifacts.","We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios.","Through further analysis, we also validate the importance of using temporal changes of style latent vectors to improve the generality of deepfake video detection."],"url":"http://arxiv.org/abs/2403.06592v1","category":"cs.CV"}
{"created":"2024-03-11 10:35:53","title":"Academically intelligent LLMs are not necessarily socially intelligent","abstract":"The academic intelligence of large language models (LLMs) has made remarkable progress in recent times, but their social intelligence performance remains unclear. Inspired by established human social intelligence frameworks, particularly Daniel Goleman's social intelligence theory, we have developed a standardized social intelligence test based on real-world social scenarios to comprehensively assess the social intelligence of LLMs, termed as the Situational Evaluation of Social Intelligence (SESI). We conducted an extensive evaluation with 13 recent popular and state-of-art LLM agents on SESI. The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors. Moreover, there exists a relatively low correlation between the social intelligence and academic intelligence exhibited by LLMs, suggesting that social intelligence is distinct from academic intelligence for LLMs. Additionally, while it is observed that LLMs can't ``understand'' what social intelligence is, their social intelligence, similar to that of humans, is influenced by social factors.","sentences":["The academic intelligence of large language models (LLMs) has made remarkable progress in recent times, but their social intelligence performance remains unclear.","Inspired by established human social intelligence frameworks, particularly Daniel Goleman's social intelligence theory, we have developed a standardized social intelligence test based on real-world social scenarios to comprehensively assess the social intelligence of LLMs, termed as the Situational Evaluation of Social Intelligence (SESI).","We conducted an extensive evaluation with 13 recent popular and state-of-art LLM agents on SESI.","The results indicate the social intelligence of LLMs still has significant room for improvement, with superficially friendliness as a primary reason for errors.","Moreover, there exists a relatively low correlation between the social intelligence and academic intelligence exhibited by LLMs, suggesting that social intelligence is distinct from academic intelligence for LLMs.","Additionally, while it is observed that LLMs can't ``understand'' what social intelligence is, their social intelligence, similar to that of humans, is influenced by social factors."],"url":"http://arxiv.org/abs/2403.06591v1","category":"cs.CL"}
{"created":"2024-03-11 10:32:48","title":"The Saito vector field of a germ of complex plane curve","abstract":"In this article, we prove that an algorithm introduced by the author in a previous work and giving the generic dimension of the moduli space of a germ of curve in the complex plane that is the union of smooth curves, can be used identically to find this dimension for any kind of germ of plane curve.","sentences":["In this article, we prove that an algorithm introduced by the author in a previous work and giving the generic dimension of the moduli space of a germ of curve in the complex plane that is the union of smooth curves, can be used identically to find this dimension for any kind of germ of plane curve."],"url":"http://arxiv.org/abs/2403.06587v1","category":"math.DS"}
{"created":"2024-03-11 10:32:23","title":"ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models","abstract":"Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human activities. In this work, we propose ContextGPT: a novel prompt engineering approach to retrieve from LLMs common-sense knowledge about the relationship between human activities and the context in which they are performed. Unlike ontologies, ContextGPT requires limited human effort and expertise. An extensive evaluation carried out on two public datasets shows how a NeSy model obtained by infusing common-sense knowledge from ContextGPT is effective in data scarcity scenarios, leading to similar (and sometimes better) recognition rates than logic-based approaches with a fraction of the effort.","sentences":["Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models.","However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training.","Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers.","Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise.","Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human activities.","In this work, we propose ContextGPT: a novel prompt engineering approach to retrieve from LLMs common-sense knowledge about the relationship between human activities and the context in which they are performed.","Unlike ontologies, ContextGPT requires limited human effort and expertise.","An extensive evaluation carried out on two public datasets shows how a NeSy model obtained by infusing common-sense knowledge from ContextGPT is effective in data scarcity scenarios, leading to similar (and sometimes better) recognition rates than logic-based approaches with a fraction of the effort."],"url":"http://arxiv.org/abs/2403.06586v1","category":"cs.LG"}
{"created":"2024-03-11 10:30:09","title":"Unexpected Quantum Indeterminacy","abstract":"Recent philosophical discussions about metaphysical indeterminacy have been substantiated with the idea that quantum mechanics, one of the most successful physical theories in the history of science, provides explicit instances of worldly indefiniteness. Against this background, several philosophers underline that there are alternative formulations of quantum theory in which such indeterminacy has no room and plays no role. A typical example is Bohmian mechanics in virtue of its clear particle ontology. Contrary to these latter claims, this paper aims at showing that different pilot-wave theories do in fact instantiate diverse forms of metaphysical indeterminacy. Namely, I argue that there are various questions about worldly states of affairs that cannot be determined by looking exclusively at their ontologies and dynamical laws. Moreover, it will be claimed that Bohmian mechanics generates a new form of modal indeterminacy. Finally, it will be concluded that ontological clarity and indeterminacy are not mutually exclusive, i.e., the two can coexist in the same theory.","sentences":["Recent philosophical discussions about metaphysical indeterminacy have been substantiated with the idea that quantum mechanics, one of the most successful physical theories in the history of science, provides explicit instances of worldly indefiniteness.","Against this background, several philosophers underline that there are alternative formulations of quantum theory in which such indeterminacy has no room and plays no role.","A typical example is Bohmian mechanics in virtue of its clear particle ontology.","Contrary to these latter claims, this paper aims at showing that different pilot-wave theories do in fact instantiate diverse forms of metaphysical indeterminacy.","Namely, I argue that there are various questions about worldly states of affairs that cannot be determined by looking exclusively at their ontologies and dynamical laws.","Moreover, it will be claimed that Bohmian mechanics generates a new form of modal indeterminacy.","Finally, it will be concluded that ontological clarity and indeterminacy are not mutually exclusive, i.e., the two can coexist in the same theory."],"url":"http://arxiv.org/abs/2403.06584v1","category":"physics.hist-ph"}
{"created":"2024-03-11 10:27:30","title":"Arborescences and Shortest Path Trees when Colors Matter","abstract":"Color-constrained subgraph problems are those where we are given an edge-colored (directed or undirected) graph and the task is to find a specific type of subgraph, like a spanning tree, an arborescence, a single-source shortest path tree, a perfect matching etc., with constraints on the number of edges of each color. Some of these problems, like color-constrained spanning tree, have elegant solutions and some of them, like color-constrained perfect matching, are longstanding open questions. In this work, we study color-constrained arborescences and shortest path trees. Computing a color-constrained shortest path tree on weighted digraphs turns out to be NP-hard in general but polynomial-time solvable when all cycles have positive weight. This polynomial-time solvability is due to the fact that the solution space is essentially the set of all color-constrained arborescences of a directed acyclic subgraph of the original graph. While finding color-constrained arborescence of digraphs is NP-hard in general, we give efficient algorithms when the input graph is acyclic. Consequently, a color-constrained shortest path tree on weighted digraphs having only positive weight cycles can be efficiently computed. Our algorithms also generalize to the problem of finding a color-constrained shortest path tree with minimum total weight. En route, we sight nice connections to colored matroids and color-constrained bases.","sentences":["Color-constrained subgraph problems are those where we are given an edge-colored (directed or undirected) graph and the task is to find a specific type of subgraph, like a spanning tree, an arborescence, a single-source shortest path tree, a perfect matching etc., with constraints on the number of edges of each color.","Some of these problems, like color-constrained spanning tree, have elegant solutions and some of them, like color-constrained perfect matching, are longstanding open questions.","In this work, we study color-constrained arborescences and shortest path trees.","Computing a color-constrained shortest path tree on weighted digraphs turns out to be NP-hard in general but polynomial-time solvable when all cycles have positive weight.","This polynomial-time solvability is due to the fact that the solution space is essentially the set of all color-constrained arborescences of a directed acyclic subgraph of the original graph.","While finding color-constrained arborescence of digraphs is NP-hard in general, we give efficient algorithms when the input graph is acyclic.","Consequently, a color-constrained shortest path tree on weighted digraphs having only positive weight cycles can be efficiently computed.","Our algorithms also generalize to the problem of finding a color-constrained shortest path tree with minimum total weight.","En route, we sight nice connections to colored matroids and color-constrained bases."],"url":"http://arxiv.org/abs/2403.06580v1","category":"cs.DS"}
{"created":"2024-03-11 10:26:38","title":"Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for Distracted Driver Action Recognition","abstract":"Classification and localization of driving actions over time is important for advanced driver-assistance systems and naturalistic driving studies. Temporal localization is challenging because it requires robustness, reliability, and accuracy. In this study, we aim to improve the temporal localization and classification accuracy performance by adapting video action recognition and 2D human-pose estimation networks to one model. Therefore, we design a transformer-based fusion architecture to effectively combine 2D-pose features and spatio-temporal features. The model uses 2D-pose features as the positional embedding of the transformer architecture and spatio-temporal features as the main input to the encoder of the transformer. The proposed solution is generic and independent of the camera numbers and positions, giving frame-based class probabilities as output. Finally, the post-processing step combines information from different camera views to obtain final predictions and eliminate false positives. The model performs well on the A2 test set of the 2023 NVIDIA AI City Challenge for naturalistic driving action recognition, achieving the overlap score of the organizer-defined distracted driver behaviour metric of 0.5079.","sentences":["Classification and localization of driving actions over time is important for advanced driver-assistance systems and naturalistic driving studies.","Temporal localization is challenging because it requires robustness, reliability, and accuracy.","In this study, we aim to improve the temporal localization and classification accuracy performance by adapting video action recognition and 2D human-pose estimation networks to one model.","Therefore, we design a transformer-based fusion architecture to effectively combine 2D-pose features and spatio-temporal features.","The model uses 2D-pose features as the positional embedding of the transformer architecture and spatio-temporal features as the main input to the encoder of the transformer.","The proposed solution is generic and independent of the camera numbers and positions, giving frame-based class probabilities as output.","Finally, the post-processing step combines information from different camera views to obtain final predictions and eliminate false positives.","The model performs well on the A2 test set of the 2023 NVIDIA AI City Challenge for naturalistic driving action recognition, achieving the overlap score of the organizer-defined distracted driver behaviour metric of 0.5079."],"url":"http://arxiv.org/abs/2403.06577v1","category":"cs.CV"}
{"created":"2024-03-11 10:26:04","title":"FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing Fourier Transform and Auto-encoder","abstract":"The success of deep learning-based generative models in producing realistic images, videos, and audios has led to a crucial consideration: how to effectively assess the quality of synthetic samples. While the Fr\\'{e}chet Inception Distance (FID) serves as the standard metric for evaluating generative models in image synthesis, a comparable metric for time series data is notably absent. This gap in assessment capabilities stems from the absence of a widely accepted feature vector extractor pre-trained on benchmark time series datasets. In addressing these challenges related to assessing the quality of time series, particularly in the context of Fr\\'echet Distance, this work proposes a novel solution leveraging the Fourier transform and Auto-encoder, termed the Fr\\'{e}chet Fourier-transform Auto-encoder Distance (FFAD). Through our experimental results, we showcase the potential of FFAD for effectively distinguishing samples from different classes. This novel metric emerges as a fundamental tool for the evaluation of generative time series data, contributing to the ongoing efforts of enhancing assessment methodologies in the realm of deep learning-based generative models.","sentences":["The success of deep learning-based generative models in producing realistic images, videos, and audios has led to a crucial consideration: how to effectively assess the quality of synthetic samples.","While the Fr\\'{e}chet Inception Distance (FID) serves as the standard metric for evaluating generative models in image synthesis, a comparable metric for time series data is notably absent.","This gap in assessment capabilities stems from the absence of a widely accepted feature vector extractor pre-trained on benchmark time series datasets.","In addressing these challenges related to assessing the quality of time series, particularly in the context of Fr\\'echet Distance, this work proposes a novel solution leveraging the Fourier transform and Auto-encoder, termed the Fr\\'{e}chet Fourier-transform Auto-encoder Distance (FFAD).","Through our experimental results, we showcase the potential of FFAD for effectively distinguishing samples from different classes.","This novel metric emerges as a fundamental tool for the evaluation of generative time series data, contributing to the ongoing efforts of enhancing assessment methodologies in the realm of deep learning-based generative models."],"url":"http://arxiv.org/abs/2403.06576v1","category":"cs.LG"}
{"created":"2024-03-11 10:24:37","title":"AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models","abstract":"Given the importance of ancient Chinese in capturing the essence of rich historical and cultural heritage, the rapid advancements in Large Language Models (LLMs) necessitate benchmarks that can effectively evaluate their understanding of ancient contexts. To meet this need, we present AC-EVAL, an innovative benchmark designed to assess the advanced knowledge and reasoning capabilities of LLMs within the context of ancient Chinese. AC-EVAL is structured across three levels of difficulty reflecting different facets of language comprehension: general historical knowledge, short text understanding, and long text comprehension. The benchmark comprises 13 tasks, spanning historical facts, geography, social customs, art, philosophy, classical poetry and prose, providing a comprehensive assessment framework. Our extensive evaluation of top-performing LLMs, tailored for both English and Chinese, reveals a substantial potential for enhancing ancient text comprehension. By highlighting the strengths and weaknesses of LLMs, AC-EVAL aims to promote their development and application forward in the realms of ancient Chinese language education and scholarly research. The AC-EVAL data and evaluation code are available at https://github.com/yuting-wei/AC-EVAL.","sentences":["Given the importance of ancient Chinese in capturing the essence of rich historical and cultural heritage, the rapid advancements in Large Language Models (LLMs) necessitate benchmarks that can effectively evaluate their understanding of ancient contexts.","To meet this need, we present AC-EVAL, an innovative benchmark designed to assess the advanced knowledge and reasoning capabilities of LLMs within the context of ancient Chinese.","AC-EVAL is structured across three levels of difficulty reflecting different facets of language comprehension: general historical knowledge, short text understanding, and long text comprehension.","The benchmark comprises 13 tasks, spanning historical facts, geography, social customs, art, philosophy, classical poetry and prose, providing a comprehensive assessment framework.","Our extensive evaluation of top-performing LLMs, tailored for both English and Chinese, reveals a substantial potential for enhancing ancient text comprehension.","By highlighting the strengths and weaknesses of LLMs, AC-EVAL aims to promote their development and application forward in the realms of ancient Chinese language education and scholarly research.","The AC-EVAL data and evaluation code are available at https://github.com/yuting-wei/AC-EVAL."],"url":"http://arxiv.org/abs/2403.06574v1","category":"cs.CL"}
{"created":"2024-03-11 10:23:38","title":"Electrical Consumption Flexibility in the Cement Industry","abstract":"A method for identifying and quantifying the flexibility of electricity demand in a production plant is reported. The plant is equipped with electric machines, product storage silos, distributed generation, and electrical storage systems. The method aims to minimize production costs. To achieve this, the plant is mathematically modeled, and an economic optimization problem is formulated by managing these plant equipment. From this optimal schedule (base schedule), the feasibility of modifying it to sell or buy energy in the electricity balancing regulation markets is evaluated, thus obtaining the so called flexibility schedule. Finally, this method was successfully applied to a real case using data from a Spanish cement production plant.","sentences":["A method for identifying and quantifying the flexibility of electricity demand in a production plant is reported.","The plant is equipped with electric machines, product storage silos, distributed generation, and electrical storage systems.","The method aims to minimize production costs.","To achieve this, the plant is mathematically modeled, and an economic optimization problem is formulated by managing these plant equipment.","From this optimal schedule (base schedule), the feasibility of modifying it to sell or buy energy in the electricity balancing regulation markets is evaluated, thus obtaining the so called flexibility schedule.","Finally, this method was successfully applied to a real case using data from a Spanish cement production plant."],"url":"http://arxiv.org/abs/2403.06573v1","category":"eess.SY"}
{"created":"2024-03-11 10:14:06","title":"Scalable Online Exploration via Coverability","abstract":"Exploration is a major challenge in reinforcement learning, especially for high-dimensional domains that require function approximation. We propose exploration objectives -- policy optimization objectives that enable downstream maximization of any reward function -- as a conceptual framework to systematize the study of exploration. Within this framework, we introduce a new objective, $L_1$-Coverage, which generalizes previous exploration schemes and supports three fundamental desiderata:   1. Intrinsic complexity control. $L_1$-Coverage is associated with a structural parameter, $L_1$-Coverability, which reflects the intrinsic statistical difficulty of the underlying MDP, subsuming Block and Low-Rank MDPs.   2. Efficient planning. For a known MDP, optimizing $L_1$-Coverage efficiently reduces to standard policy optimization, allowing flexible integration with off-the-shelf methods such as policy gradient and Q-learning approaches.   3. Efficient exploration. $L_1$-Coverage enables the first computationally efficient model-based and model-free algorithms for online (reward-free or reward-driven) reinforcement learning in MDPs with low coverability.   Empirically, we find that $L_1$-Coverage effectively drives off-the-shelf policy optimization algorithms to explore the state space.","sentences":["Exploration is a major challenge in reinforcement learning, especially for high-dimensional domains that require function approximation.","We propose exploration objectives -- policy optimization objectives that enable downstream maximization of any reward function -- as a conceptual framework to systematize the study of exploration.","Within this framework, we introduce a new objective, $L_1$-Coverage, which generalizes previous exploration schemes and supports three fundamental desiderata:   1.","Intrinsic complexity control.","$L_1$-Coverage is associated with a structural parameter, $L_1$-Coverability, which reflects the intrinsic statistical difficulty of the underlying MDP, subsuming Block and Low-Rank MDPs.   ","2.","Efficient planning.","For a known MDP, optimizing $L_1$-Coverage efficiently reduces to standard policy optimization, allowing flexible integration with off-the-shelf methods such as policy gradient and Q-learning approaches.   ","3.","Efficient exploration.","$L_1$-Coverage enables the first computationally efficient model-based and model-free algorithms for online (reward-free or reward-driven) reinforcement learning in MDPs with low coverability.   ","Empirically, we find that $L_1$-Coverage effectively drives off-the-shelf policy optimization algorithms to explore the state space."],"url":"http://arxiv.org/abs/2403.06571v1","category":"cs.LG"}
{"created":"2024-03-11 10:10:35","title":"Better Understandings and Configurations in MaxSAT Local Search Solvers via Anytime Performance Analysis","abstract":"Though numerous solvers have been proposed for the MaxSAT problem, and the benchmark environment such as MaxSAT Evaluations provides a platform for the comparison of the state-of-the-art solvers, existing assessments were usually evaluated based on the quality, e.g., fitness, of the best-found solutions obtained within a given running time budget. However, concerning solely the final obtained solutions regarding specific time budgets may restrict us from comprehending the behavior of the solvers along the convergence process. This paper demonstrates that Empirical Cumulative Distribution Functions can be used to compare MaxSAT local search solvers' anytime performance across multiple problem instances and various time budgets. The assessment reveals distinctions in solvers' performance and displays that the (dis)advantages of solvers adjust along different running times. This work also exhibits that the quantitative and high variance assessment of anytime performance can guide machines, i.e., automatic configurators, to search for better parameter settings. Our experimental results show that the hyperparameter optimization tool, i.e., SMAC, generally achieves better parameter settings of local search when using the anytime performance as the cost function, compared to using the fitness of the best-found solutions.","sentences":["Though numerous solvers have been proposed for the MaxSAT problem, and the benchmark environment such as MaxSAT Evaluations provides a platform for the comparison of the state-of-the-art solvers, existing assessments were usually evaluated based on the quality, e.g., fitness, of the best-found solutions obtained within a given running time budget.","However, concerning solely the final obtained solutions regarding specific time budgets may restrict us from comprehending the behavior of the solvers along the convergence process.","This paper demonstrates that Empirical Cumulative Distribution Functions can be used to compare MaxSAT local search solvers' anytime performance across multiple problem instances and various time budgets.","The assessment reveals distinctions in solvers' performance and displays that the (dis)advantages of solvers adjust along different running times.","This work also exhibits that the quantitative and high variance assessment of anytime performance can guide machines, i.e., automatic configurators, to search for better parameter settings.","Our experimental results show that the hyperparameter optimization tool, i.e., SMAC, generally achieves better parameter settings of local search when using the anytime performance as the cost function, compared to using the fitness of the best-found solutions."],"url":"http://arxiv.org/abs/2403.06568v1","category":"cs.AI"}
{"created":"2024-03-11 10:06:45","title":"Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology","abstract":"Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. Current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. In response, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based medical image retrieval. By benchmarking these models on a comprehensive dataset of 1.6 million 2D radiological images spanning four modalities and 161 pathologies, we identify weakly-supervised models as superior, achieving a P@1 of up to 0.594. This performance not only competes with a specialized model but does so without the need for fine-tuning. Our analysis further explores the challenges in retrieving pathological versus anatomical structures, indicating that accurate retrieval of pathological features presents greater difficulty. Despite these challenges, our research underscores the vast potential of foundation models for CBIR in radiology, proposing a shift towards versatile, general-purpose medical image retrieval systems that do not require specific tuning.","sentences":["Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology.","Current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility.","In response, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based medical image retrieval.","By benchmarking these models on a comprehensive dataset of 1.6 million 2D radiological images spanning four modalities and 161 pathologies, we identify weakly-supervised models as superior, achieving a P@1 of up to 0.594.","This performance not only competes with a specialized model but does so without the need for fine-tuning.","Our analysis further explores the challenges in retrieving pathological versus anatomical structures, indicating that accurate retrieval of pathological features presents greater difficulty.","Despite these challenges, our research underscores the vast potential of foundation models for CBIR in radiology, proposing a shift towards versatile, general-purpose medical image retrieval systems that do not require specific tuning."],"url":"http://arxiv.org/abs/2403.06567v1","category":"cs.CV"}
{"created":"2024-03-11 10:06:08","title":"An Algorithm for Correct Computation of Reeb Spaces for PL Bivariate Fields","abstract":"The Reeb space is a topological structure which is a generalization of the notion of the Reeb graph to multi-fields. Its effectiveness has been established in revealing topological features in data across diverse computational domains which cannot be identified using the Reeb graph or other scalar-topology-based methods. Approximations of Reeb spaces such as the Mapper and the Joint Contour Net have been developed based on quantization of the range. However, computing the topologically correct Reeb space dispensing the range-quantization is a challenging problem. In the current paper, we develop an algorithm for computing a correct net-like approximation corresponding to the Reeb space of a generic piecewise-linear (PL) bivariate field based on a multi-dimensional Reeb graph (MDRG). First, we prove that the Reeb space is homeomorphic to its MDRG. Subsequently, we introduce an algorithm for computing the MDRG of a generic PL bivariate field through the computation of its Jacobi set and Jacobi structure, a projection of the Jacobi set into the Reeb space. This marks the first algorithm for MDRG computation without requiring the quantization of bivariate fields. Following this, we compute a net-like structure embedded in the corresponding Reeb space using the MDRG and the Jacobi structure. We provide the proof of correctness and complexity analysis of our algorithm.","sentences":["The Reeb space is a topological structure which is a generalization of the notion of the Reeb graph to multi-fields.","Its effectiveness has been established in revealing topological features in data across diverse computational domains which cannot be identified using the Reeb graph or other scalar-topology-based methods.","Approximations of Reeb spaces such as the Mapper and the Joint Contour Net have been developed based on quantization of the range.","However, computing the topologically correct Reeb space dispensing the range-quantization is a challenging problem.","In the current paper, we develop an algorithm for computing a correct net-like approximation corresponding to the Reeb space of a generic piecewise-linear (PL) bivariate field based on a multi-dimensional Reeb graph (MDRG).","First, we prove that the Reeb space is homeomorphic to its MDRG.","Subsequently, we introduce an algorithm for computing the MDRG of a generic PL bivariate field through the computation of its Jacobi set and Jacobi structure, a projection of the Jacobi set into the Reeb space.","This marks the first algorithm for MDRG computation without requiring the quantization of bivariate fields.","Following this, we compute a net-like structure embedded in the corresponding Reeb space using the MDRG and the Jacobi structure.","We provide the proof of correctness and complexity analysis of our algorithm."],"url":"http://arxiv.org/abs/2403.06564v1","category":"cs.CG"}
{"created":"2024-03-11 10:05:06","title":"Hardy inequalities and uncertainty principles in the presence of a black hole","abstract":"In this paper we establish Hardy and Heisenberg uncertainty-type inequalities for the exterior of a Schwarzschild black hole. The weights that appear in both inequalities are tailored to fit the geometry, and can both be compared to the related Riemannian distance from the event horizon to yield inequalities for that distance. Moreover, in both cases the classic Euclidean inequalities with a point singularity can be recovered in the limit where one stands ``far enough'' from the black hole, as expected from the asymptotic flatness of the metric.","sentences":["In this paper we establish Hardy and Heisenberg uncertainty-type inequalities for the exterior of a Schwarzschild black hole.","The weights that appear in both inequalities are tailored to fit the geometry, and can both be compared to the related Riemannian distance from the event horizon to yield inequalities for that distance.","Moreover, in both cases the classic Euclidean inequalities with a point singularity can be recovered in the limit where one stands ``far enough'' from the black hole, as expected from the asymptotic flatness of the metric."],"url":"http://arxiv.org/abs/2403.06562v1","category":"math.AP"}
{"created":"2024-03-11 10:01:21","title":"Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds","abstract":"While many Machine Learning methods were developed or transposed on Riemannian manifolds to tackle data with known non Euclidean geometry, Optimal Transport (OT) methods on such spaces have not received much attention. The main OT tool on these spaces is the Wasserstein distance which suffers from a heavy computational burden. On Euclidean spaces, a popular alternative is the Sliced-Wasserstein distance, which leverages a closed-form solution of the Wasserstein distance in one dimension, but which is not readily available on manifolds. In this work, we derive general constructions of Sliced-Wasserstein distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive curvature, which include among others Hyperbolic spaces or the space of Symmetric Positive Definite matrices. Then, we propose different applications. Additionally, we derive non-parametric schemes to minimize these new distances by approximating their Wasserstein gradient flows.","sentences":["While many Machine Learning methods were developed or transposed on Riemannian manifolds to tackle data with known non Euclidean geometry, Optimal Transport (OT) methods on such spaces have not received much attention.","The main OT tool on these spaces is the Wasserstein distance which suffers from a heavy computational burden.","On Euclidean spaces, a popular alternative is the Sliced-Wasserstein distance, which leverages a closed-form solution of the Wasserstein distance in one dimension, but which is not readily available on manifolds.","In this work, we derive general constructions of Sliced-Wasserstein distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive curvature, which include among others Hyperbolic spaces or the space of Symmetric Positive Definite matrices.","Then, we propose different applications.","Additionally, we derive non-parametric schemes to minimize these new distances by approximating their Wasserstein gradient flows."],"url":"http://arxiv.org/abs/2403.06560v1","category":"cs.LG"}
{"created":"2024-03-11 10:00:26","title":"Data-driven architecture to encode information in the kinematics of robots and artificial avatars","abstract":"We present a data-driven control architecture for modifying the kinematics of robots and artificial avatars to encode specific information such as the presence or not of an emotion in the movements of an avatar or robot driven by a human operator. We validate our approach on an experimental dataset obtained during the reach-to-grasp phase of a pick-and-place task.","sentences":["We present a data-driven control architecture for modifying the kinematics of robots and artificial avatars to encode specific information such as the presence or not of an emotion in the movements of an avatar or robot driven by a human operator.","We validate our approach on an experimental dataset obtained during the reach-to-grasp phase of a pick-and-place task."],"url":"http://arxiv.org/abs/2403.06557v1","category":"eess.SY"}
{"created":"2024-03-11 09:59:55","title":"Topological properties of a class of generalized Su-Schrieffer-Heeger networks: chains and meshes","abstract":"We analyze the topological properties of a family of generalized Su-Schrieffer-Heeger (SSH) chains and mesh geometries. In both the geometries the usual staggering in the distribution of the two overlap integrals is delayed (in space) by the inclusion of a third (additional) hopping term. A tight-binding Hamiltonian is used to unravel the topological phases, characterized by a topological invariant. While in the linear chains, the topological invariant (the Zak phase) always appears to be quantized, in the quasi-one dimensional strip geometries and the generalized SSH mesh patterns the quantization of the Zak phase is sensitive to the strength of the additional interaction (the `extra' hopping integral). We study its influence thoroughly and explore the edge states and their robustness against disorder in the cross-linked generalized SSH mesh geometries. The systems considered here can be taken to model (though crudely) two-dimensional polymers where the cross-linking brings in non-trivial modification of the energy bands and transport properties. In addition to the topological features studied, we provide a prescription to unravel any flat, non-dispersive energy bands in the mesh geometries, along with the structure and distribution of the compact localized eigenstates. Our results are analytically exact.","sentences":["We analyze the topological properties of a family of generalized Su-Schrieffer-Heeger (SSH) chains and mesh geometries.","In both the geometries the usual staggering in the distribution of the two overlap integrals is delayed (in space) by the inclusion of a third (additional) hopping term.","A tight-binding Hamiltonian is used to unravel the topological phases, characterized by a topological invariant.","While in the linear chains, the topological invariant (the Zak phase) always appears to be quantized, in the quasi-one dimensional strip geometries and the generalized SSH mesh patterns the quantization of the Zak phase is sensitive to the strength of the additional interaction (the `extra' hopping integral).","We study its influence thoroughly and explore the edge states and their robustness against disorder in the cross-linked generalized SSH mesh geometries.","The systems considered here can be taken to model (though crudely) two-dimensional polymers where the cross-linking brings in non-trivial modification of the energy bands and transport properties.","In addition to the topological features studied, we provide a prescription to unravel any flat, non-dispersive energy bands in the mesh geometries, along with the structure and distribution of the compact localized eigenstates.","Our results are analytically exact."],"url":"http://arxiv.org/abs/2403.06556v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 09:53:19","title":"Detection of Object Throwing Behavior in Surveillance Videos","abstract":"Anomalous behavior detection is a challenging research area within computer vision. Progress in this area enables automated detection of dangerous behavior using surveillance camera feeds. A dangerous behavior that is often overlooked in other research is the throwing action in traffic flow, which is one of the unique requirements of our Smart City project to enhance public safety. This paper proposes a solution for throwing action detection in surveillance videos using deep learning. At present, datasets for throwing actions are not publicly available. To address the use-case of our Smart City project, we first generate the novel public 'Throwing Action' dataset, consisting of 271 videos of throwing actions performed by traffic participants, such as pedestrians, bicyclists, and car drivers, and 130 normal videos without throwing actions. Second, we compare the performance of different feature extractors for our anomaly detection method on the UCF-Crime and Throwing-Action datasets. The explored feature extractors are the Convolutional 3D (C3D) network, the Inflated 3D ConvNet (I3D) network, and the Multi-Fiber Network (MFNet). Finally, the performance of the anomaly detection algorithm is improved by applying the Adam optimizer instead of Adadelta, and proposing a mean normal loss function that covers the multitude of normal situations in traffic. Both aspects yield better anomaly detection performance. Besides this, the proposed mean normal loss function lowers the false alarm rate on the combined dataset. The experimental results reach an area under the ROC curve of 86.10 for the Throwing-Action dataset, and 80.13 on the combined dataset, respectively.","sentences":["Anomalous behavior detection is a challenging research area within computer vision.","Progress in this area enables automated detection of dangerous behavior using surveillance camera feeds.","A dangerous behavior that is often overlooked in other research is the throwing action in traffic flow, which is one of the unique requirements of our Smart City project to enhance public safety.","This paper proposes a solution for throwing action detection in surveillance videos using deep learning.","At present, datasets for throwing actions are not publicly available.","To address the use-case of our Smart City project, we first generate the novel public 'Throwing Action' dataset, consisting of 271 videos of throwing actions performed by traffic participants, such as pedestrians, bicyclists, and car drivers, and 130 normal videos without throwing actions.","Second, we compare the performance of different feature extractors for our anomaly detection method on the UCF-Crime and Throwing-Action datasets.","The explored feature extractors are the Convolutional 3D (C3D) network, the Inflated 3D ConvNet (I3D) network, and the Multi-Fiber Network (MFNet).","Finally, the performance of the anomaly detection algorithm is improved by applying the Adam optimizer instead of Adadelta, and proposing a mean normal loss function that covers the multitude of normal situations in traffic.","Both aspects yield better anomaly detection performance.","Besides this, the proposed mean normal loss function lowers the false alarm rate on the combined dataset.","The experimental results reach an area under the ROC curve of 86.10 for the Throwing-Action dataset, and 80.13 on the combined dataset, respectively."],"url":"http://arxiv.org/abs/2403.06552v1","category":"cs.CV"}
{"created":"2024-03-11 09:52:32","title":"ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval","abstract":"Tool learning aims to extend the capabilities of large language models (LLMs) with external tools. A major challenge in tool learning is how to support a large number of tools, including unseen tools. To address this challenge, previous studies have proposed retrieving suitable tools for the LLM based on the user query. However, previously proposed methods do not consider the differences between seen and unseen tools, nor do they take the hierarchy of the tool library into account, which may lead to suboptimal performance for tool retrieval. Therefore, to address the aforementioned issues, we propose ToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval to further refine the retrieval results. Specifically, our proposed ToolRerank includes Adaptive Truncation, which truncates the retrieval results related to seen and unseen tools at different positions, and Hierarchy-Aware Reranking, which makes retrieval results more concentrated for single-tool queries and more diverse for multi-tool queries. Experimental results show that ToolRerank can improve the quality of the retrieval results, leading to better execution results generated by the LLM.","sentences":["Tool learning aims to extend the capabilities of large language models (LLMs) with external tools.","A major challenge in tool learning is how to support a large number of tools, including unseen tools.","To address this challenge, previous studies have proposed retrieving suitable tools for the LLM based on the user query.","However, previously proposed methods do not consider the differences between seen and unseen tools, nor do they take the hierarchy of the tool library into account, which may lead to suboptimal performance for tool retrieval.","Therefore, to address the aforementioned issues, we propose ToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval to further refine the retrieval results.","Specifically, our proposed ToolRerank includes Adaptive Truncation, which truncates the retrieval results related to seen and unseen tools at different positions, and Hierarchy-Aware Reranking, which makes retrieval results more concentrated for single-tool queries and more diverse for multi-tool queries.","Experimental results show that ToolRerank can improve the quality of the retrieval results, leading to better execution results generated by the LLM."],"url":"http://arxiv.org/abs/2403.06551v1","category":"cs.IR"}
{"created":"2024-03-11 09:48:01","title":"Differential geometry and general relativity with algebraifolds","abstract":"It is often noted that many of the basic concepts of differential geometry, such as the definition of connection, are purely algebraic in nature. Here, we review and extend existing work on fully algebraic formulations of differential geometry which eliminate the need for an underlying manifold. While the literature contains various independent approaches to this, we focus on one particular approach that we argue to be the most natural one based on the definition of \\emph{algebraifold}, by which we mean a commutative algebra $\\mathcal{A}$ for which the module of derivations of $\\mathcal{A}$ is finitely generated projective. Over $\\mathbb{R}$ as the base ring, this class of algebras includes the algebra $C^\\infty(M)$ of smooth functions on a manifold $M$, and similarly for analytic functions. An importantly different example is the Colombeau algebra of generalized functions on $M$, which makes distributional differential geometry an instance of our formalism. Another instance is a fibred version of smooth differential geometry, since any smooth submersion $M \\to N$ makes $C^\\infty(M)$ into an algebraifold with $C^\\infty(N)$ as the base ring.Over any field $k$ of characteristic zero, examples include the algebra of regular functions on a smooth affine variety as well as any function field.   Our development of differential geometry in terms of algebraifolds comprises tensors, connections, curvature, geodesics and we briefly consider general relativity.","sentences":["It is often noted that many of the basic concepts of differential geometry, such as the definition of connection, are purely algebraic in nature.","Here, we review and extend existing work on fully algebraic formulations of differential geometry which eliminate the need for an underlying manifold.","While the literature contains various independent approaches to this, we focus on one particular approach that we argue to be the most natural one based on the definition of \\emph{algebraifold}, by which we mean a commutative algebra $\\mathcal{A}$ for which the module of derivations of $\\mathcal{A}$ is finitely generated projective.","Over $\\mathbb{R}$ as the base ring, this class of algebras includes the algebra $C^\\infty(M)$ of smooth functions on a manifold $M$, and similarly for analytic functions.","An importantly different example is the Colombeau algebra of generalized functions on $M$, which makes distributional differential geometry an instance of our formalism.","Another instance is a fibred version of smooth differential geometry, since any smooth submersion $M \\to N$ makes $C^\\infty(M)$ into an algebraifold with $C^\\infty(N)$ as the base ring.","Over any field $k$ of characteristic zero, examples include the algebra of regular functions on a smooth affine variety as well as any function field.   ","Our development of differential geometry in terms of algebraifolds comprises tensors, connections, curvature, geodesics and we briefly consider general relativity."],"url":"http://arxiv.org/abs/2403.06548v1","category":"math.DG"}
{"created":"2024-03-11 09:46:41","title":"OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation","abstract":"Unsupervised Semantic Segmentation (USS) involves segmenting images without relying on predefined labels, aiming to alleviate the burden of extensive human labeling. Existing methods utilize features generated by self-supervised models and specific priors for clustering. However, their clustering objectives are not involved in the optimization of the features during training. Additionally, due to the lack of clear class definitions in USS, the resulting segments may not align well with the clustering objective. In this paper, we introduce a novel approach called Optimally Matched Hierarchy (OMH) to simultaneously address the above issues. The core of our method lies in imposing structured sparsity on the feature space, which allows the features to encode information with different levels of granularity. The structure of this sparsity stems from our hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy among parallel clusters through Optimal Transport. Our OMH yields better unsupervised segmentation performance compared to existing USS methods. Our extensive experiments demonstrate the benefits of OMH when utilizing our differentiable paradigm. We will make our code publicly available.","sentences":["Unsupervised Semantic Segmentation (USS) involves segmenting images without relying on predefined labels, aiming to alleviate the burden of extensive human labeling.","Existing methods utilize features generated by self-supervised models and specific priors for clustering.","However, their clustering objectives are not involved in the optimization of the features during training.","Additionally, due to the lack of clear class definitions in USS, the resulting segments may not align well with the clustering objective.","In this paper, we introduce a novel approach called Optimally Matched Hierarchy (OMH) to simultaneously address the above issues.","The core of our method lies in imposing structured sparsity on the feature space, which allows the features to encode information with different levels of granularity.","The structure of this sparsity stems from our hierarchy (OMH).","To achieve this, we learn a soft but sparse hierarchy among parallel clusters through Optimal Transport.","Our OMH yields better unsupervised segmentation performance compared to existing USS methods.","Our extensive experiments demonstrate the benefits of OMH when utilizing our differentiable paradigm.","We will make our code publicly available."],"url":"http://arxiv.org/abs/2403.06546v1","category":"cs.CV"}
{"created":"2024-03-11 09:45:34","title":"ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation","abstract":"The creation of in-silico datasets can expand the utility of existing annotations to new domains with different staining patterns in computational pathology. As such, it has the potential to significantly lower the cost associated with building large and pixel precise datasets needed to train supervised deep learning models. We propose a novel approach for the generation of in-silico immunohistochemistry (IHC) images by disentangling morphology specific IHC stains into separate image channels in immunofluorescence (IF) images. The proposed approach qualitatively and quantitatively outperforms baseline methods as proven by training nucleus segmentation models on the created in-silico datasets.","sentences":["The creation of in-silico datasets can expand the utility of existing annotations to new domains with different staining patterns in computational pathology.","As such, it has the potential to significantly lower the cost associated with building large and pixel precise datasets needed to train supervised deep learning models.","We propose a novel approach for the generation of in-silico immunohistochemistry (IHC) images by disentangling morphology specific IHC stains into separate image channels in immunofluorescence (IF) images.","The proposed approach qualitatively and quantitatively outperforms baseline methods as proven by training nucleus segmentation models on the created in-silico datasets."],"url":"http://arxiv.org/abs/2403.06545v1","category":"eess.IV"}
{"created":"2024-03-11 09:36:54","title":"Solving the p-Riccati Equations and Applications to the Factorisation of Differential Operators","abstract":"The solutions of the equation f^{ (p--1) }+ f^p = h^p in the unknown function f overan algebraic function field of characteristic p are very closely linked to the structure and fac-torisations of linear differential operators with coefficients in function fields of characteristic p.However, while being able to solve this equation over general algebraic function fields is necessaryeven for operators with rational coefficients, no general resolution method has been developed.We present an algorithm for testing the existence of solutions in polynomial time in the ``size''of h and an algorithm based on the computation of Riemann-Roch spaces and the selection ofelements in the divisor class group, for computing solutions of size polynomial in the ``size'' of hin polynomial time in the size of h and linear in the characteristic p, and discuss its applicationsto the factorisation of linear differential operators in positive characteristic p.","sentences":["The solutions of the equation f^{ (p--1) }","+ f^p = h^p in the unknown function f overan algebraic function field of characteristic p are very closely linked to the structure and fac-torisations of linear differential operators with coefficients in function fields of characteristic p.","However, while being able to solve this equation over general algebraic function fields is necessaryeven for operators with rational coefficients, no general resolution method has been developed.","We present an algorithm for testing the existence of solutions in polynomial time in the ``size''of h and an algorithm based on the computation of Riemann-Roch spaces and the selection ofelements in the divisor class group, for computing solutions of size polynomial in the ``size'' of hin polynomial time in the size of h and linear in the characteristic p, and discuss its applicationsto the factorisation of linear differential operators in positive characteristic p."],"url":"http://arxiv.org/abs/2403.06542v1","category":"cs.SC"}
{"created":"2024-03-11 09:23:20","title":"Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution","abstract":"Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR). Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance. Addressing this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging. MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance. Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network. We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks.","sentences":["Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR).","Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance.","Addressing this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA).","Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging.","MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance.","Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network.","We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks."],"url":"http://arxiv.org/abs/2403.06536v1","category":"cs.CV"}
{"created":"2024-03-11 09:21:11","title":"Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning","abstract":"Decentralized and lifelong-adaptive multi-agent collaborative learning aims to enhance collaboration among multiple agents without a central server, with each agent solving varied tasks over time. To achieve efficient collaboration, agents should: i) autonomously identify beneficial collaborative relationships in a decentralized manner; and ii) adapt to dynamically changing task observations. In this paper, we propose DeLAMA, a decentralized multi-agent lifelong collaborative learning algorithm with dynamic collaboration graphs. To promote autonomous collaboration relationship learning, we propose a decentralized graph structure learning algorithm, eliminating the need for external priors. To facilitate adaptation to dynamic tasks, we design a memory unit to capture the agents' accumulated learning history and knowledge, while preserving finite storage consumption. To further augment the system's expressive capabilities and computational efficiency, we apply algorithm unrolling, leveraging the advantages of both mathematical optimization and neural networks. This allows the agents to `learn to collaborate' through the supervision of training tasks. Our theoretical analysis verifies that inter-agent collaboration is communication efficient under a small number of communication rounds. The experimental results verify its ability to facilitate the discovery of collaboration strategies and adaptation to dynamic learning scenarios, achieving a 98.80% reduction in MSE and a 188.87% improvement in classification accuracy. We expect our work can serve as a foundational technique to facilitate future works towards an intelligent, decentralized, and dynamic multi-agent system. Code is available at https://github.com/ShuoTang123/DeLAMA.","sentences":["Decentralized and lifelong-adaptive multi-agent collaborative learning aims to enhance collaboration among multiple agents without a central server, with each agent solving varied tasks over time.","To achieve efficient collaboration, agents should: i) autonomously identify beneficial collaborative relationships in a decentralized manner; and ii) adapt to dynamically changing task observations.","In this paper, we propose DeLAMA, a decentralized multi-agent lifelong collaborative learning algorithm with dynamic collaboration graphs.","To promote autonomous collaboration relationship learning, we propose a decentralized graph structure learning algorithm, eliminating the need for external priors.","To facilitate adaptation to dynamic tasks, we design a memory unit to capture the agents' accumulated learning history and knowledge, while preserving finite storage consumption.","To further augment the system's expressive capabilities and computational efficiency, we apply algorithm unrolling, leveraging the advantages of both mathematical optimization and neural networks.","This allows the agents to `learn to collaborate' through the supervision of training tasks.","Our theoretical analysis verifies that inter-agent collaboration is communication efficient under a small number of communication rounds.","The experimental results verify its ability to facilitate the discovery of collaboration strategies and adaptation to dynamic learning scenarios, achieving a 98.80% reduction in MSE and a 188.87% improvement in classification accuracy.","We expect our work can serve as a foundational technique to facilitate future works towards an intelligent, decentralized, and dynamic multi-agent system.","Code is available at https://github.com/ShuoTang123/DeLAMA."],"url":"http://arxiv.org/abs/2403.06535v1","category":"cs.LG"}
{"created":"2024-03-11 09:20:40","title":"SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection","abstract":"Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at https://github.com/zcablii/SARDet_100K.","sentences":["Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities.","However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code.","To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection.","Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes.","To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created.","With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure.","To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration.","The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models.","This work aims to pave the way for further advancements in SAR object detection.","The dataset and code is available at https://github.com/zcablii/SARDet_100K."],"url":"http://arxiv.org/abs/2403.06534v1","category":"cs.CV"}
{"created":"2024-03-11 09:19:09","title":"Reconstructing Visual Stimulus Images from EEG Signals Based on Deep Visual Representation Model","abstract":"Reconstructing visual stimulus images is a significant task in neural decoding, and up to now, most studies consider the functional magnetic resonance imaging (fMRI) as the signal source. However, the fMRI-based image reconstruction methods are difficult to widely applied because of the complexity and high cost of the acquisition equipments. Considering the advantages of low cost and easy portability of the electroencephalogram (EEG) acquisition equipments, we propose a novel image reconstruction method based on EEG signals in this paper. Firstly, to satisfy the high recognizability of visual stimulus images in fast switching manner, we build a visual stimuli image dataset, and obtain the EEG dataset by a corresponding EEG signals collection experiment. Secondly, the deep visual representation model(DVRM) consisting of a primary encoder and a subordinate decoder is proposed to reconstruct visual stimuli. The encoder is designed based on the residual-in-residual dense blocks to learn the distribution characteristics between EEG signals and visual stimulus images, while the decoder is designed based on the deep neural network to reconstruct the visual stimulus image from the learned deep visual representation. The DVRM can fit the deep and multiview visual features of human natural state and make the reconstructed images more precise. Finally, we evaluate the DVRM in the quality of the generated images on our EEG dataset. The results show that the DVRM have good performance in the task of learning deep visual representation from EEG signals and generating reconstructed images that are realistic and highly resemble the original images.","sentences":["Reconstructing visual stimulus images is a significant task in neural decoding, and up to now, most studies consider the functional magnetic resonance imaging (fMRI) as the signal source.","However, the fMRI-based image reconstruction methods are difficult to widely applied because of the complexity and high cost of the acquisition equipments.","Considering the advantages of low cost and easy portability of the electroencephalogram (EEG) acquisition equipments, we propose a novel image reconstruction method based on EEG signals in this paper.","Firstly, to satisfy the high recognizability of visual stimulus images in fast switching manner, we build a visual stimuli image dataset, and obtain the EEG dataset by a corresponding EEG signals collection experiment.","Secondly, the deep visual representation model(DVRM) consisting of a primary encoder and a subordinate decoder is proposed to reconstruct visual stimuli.","The encoder is designed based on the residual-in-residual dense blocks to learn the distribution characteristics between EEG signals and visual stimulus images, while the decoder is designed based on the deep neural network to reconstruct the visual stimulus image from the learned deep visual representation.","The DVRM can fit the deep and multiview visual features of human natural state and make the reconstructed images more precise.","Finally, we evaluate the DVRM in the quality of the generated images on our EEG dataset.","The results show that the DVRM have good performance in the task of learning deep visual representation from EEG signals and generating reconstructed images that are realistic and highly resemble the original images."],"url":"http://arxiv.org/abs/2403.06532v1","category":"eess.IV"}
{"created":"2024-03-11 09:15:12","title":"Ultrafast switching of sliding ferroelectricity and dynamical magnetic field in van der Waals bilayer induced by light","abstract":"Sliding ferroelectricity is a unique type of polarity recently observed in a properly stacked van der Waals bilayer. However, electric-field control of sliding ferroelectricity is hard and could induce large coercive electric fields and serious leakage currents which corrode the ferroelectricity and electronic properties, which are essential for modern two-dimensional electronics and optoelectronics. Here, we proposed laser-pulse deterministic control of sliding ferroelectricity in bilayer h-BN by first principles and molecular dynamics simulation with machine-learned force fields. The laser pulses excite shear modes which exhibit certain directional movements of lateral sliding between bilayers. The vibration of excited modes under laser pulses is predicted to overcome the energy barrier and achieve the switching of sliding ferroelectricity. Furthermore, it is found that three possible sliding transitions - between AB (BA) and BA (AB) stacking - can lead to the occurrence of dynamical magnetic fields along three different directions. Remarkably, the magnetic fields are generated by the simple linear motion of nonmagnetic species, without any need for more exotic (circular, spiral) pathways. Such predictions of deterministic control of sliding ferroelectricity and multi-states of dynamical magnetic field thus expand the potential applications of sliding ferroelectricity in memory and electronic devices.","sentences":["Sliding ferroelectricity is a unique type of polarity recently observed in a properly stacked van der Waals bilayer.","However, electric-field control of sliding ferroelectricity is hard and could induce large coercive electric fields and serious leakage currents which corrode the ferroelectricity and electronic properties, which are essential for modern two-dimensional electronics and optoelectronics.","Here, we proposed laser-pulse deterministic control of sliding ferroelectricity in bilayer h-BN by first principles and molecular dynamics simulation with machine-learned force fields.","The laser pulses excite shear modes which exhibit certain directional movements of lateral sliding between bilayers.","The vibration of excited modes under laser pulses is predicted to overcome the energy barrier and achieve the switching of sliding ferroelectricity.","Furthermore, it is found that three possible sliding transitions - between AB (BA) and BA (AB) stacking - can lead to the occurrence of dynamical magnetic fields along three different directions.","Remarkably, the magnetic fields are generated by the simple linear motion of nonmagnetic species, without any need for more exotic (circular, spiral) pathways.","Such predictions of deterministic control of sliding ferroelectricity and multi-states of dynamical magnetic field thus expand the potential applications of sliding ferroelectricity in memory and electronic devices."],"url":"http://arxiv.org/abs/2403.06531v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-11 09:12:24","title":"Confidence-Aware RGB-D Face Recognition via Virtual Depth Synthesis","abstract":"2D face recognition encounters challenges in unconstrained environments due to varying illumination, occlusion, and pose. Recent studies focus on RGB-D face recognition to improve robustness by incorporating depth information. However, collecting sufficient paired RGB-D training data is expensive and time-consuming, hindering wide deployment. In this work, we first construct a diverse depth dataset generated by 3D Morphable Models for depth model pre-training. Then, we propose a domain-independent pre-training framework that utilizes readily available pre-trained RGB and depth models to separately perform face recognition without needing additional paired data for retraining. To seamlessly integrate the two distinct networks and harness the complementary benefits of RGB and depth information for improved accuracy, we propose an innovative Adaptive Confidence Weighting (ACW). This mechanism is designed to learn confidence estimates for each modality to achieve modality fusion at the score level. Our method is simple and lightweight, only requiring ACW training beyond the backbone models. Experiments on multiple public RGB-D face recognition benchmarks demonstrate state-of-the-art performance surpassing previous methods based on depth estimation and feature fusion, validating the efficacy of our approach.","sentences":["2D face recognition encounters challenges in unconstrained environments due to varying illumination, occlusion, and pose.","Recent studies focus on RGB-D face recognition to improve robustness by incorporating depth information.","However, collecting sufficient paired RGB-D training data is expensive and time-consuming, hindering wide deployment.","In this work, we first construct a diverse depth dataset generated by 3D Morphable Models for depth model pre-training.","Then, we propose a domain-independent pre-training framework that utilizes readily available pre-trained RGB and depth models to separately perform face recognition without needing additional paired data for retraining.","To seamlessly integrate the two distinct networks and harness the complementary benefits of RGB and depth information for improved accuracy, we propose an innovative Adaptive Confidence Weighting (ACW).","This mechanism is designed to learn confidence estimates for each modality to achieve modality fusion at the score level.","Our method is simple and lightweight, only requiring ACW training beyond the backbone models.","Experiments on multiple public RGB-D face recognition benchmarks demonstrate state-of-the-art performance surpassing previous methods based on depth estimation and feature fusion, validating the efficacy of our approach."],"url":"http://arxiv.org/abs/2403.06529v1","category":"cs.CV"}
{"created":"2024-03-11 09:06:10","title":"In-Depth Modeling of Tilt-To-Length Coupling in LISA's Interferometers and TDI Michelson Observables","abstract":"We present first-order models for tilt-to-length (TTL) coupling in LISA, both for the individual interferometers as well as in the time-delay interferometry (TDI) Michelson observables. These models include the noise contributions from angular and lateral jitter coupling of the six test masses, six movable optical subassemblies (MOSAs), and three spacecraft. We briefly discuss which terms are considered to be dominant and reduce the TTL model for the second-generation TDI Michelson X observable to these primary noise contributions to estimate the resulting noise level. We show that the expected TTL noise will initially violate the entire mission displacement noise budget, resulting in the known necessity to fit and subtract TTL noise in data post-processing. By comparing the noise levels for different assumptions prior to subtraction, we show why noise mitigation by realignment prior to subtraction is favorable. We then discuss that the TTL coupling in the individual interferometers will have noise contributions that will not be present in the TDI observables. Models for TTL coupling noise in TDI and in the individual interferometers are therefore different, and commonly made assumptions are valid as such only for TDI but not for the individual interferometers. Finally, we analyze what implications can be drawn from the presented models for the subsequent fit-and-subtraction in post-processing. We show that noise contributions from the test mass and inter-satellite interferometers are indistinguishable, such that only the combined coefficients can be fit and used for subtraction. However, a distinction is considered not necessary. Additionally, we show a correlation between coefficients for transmitter and receiver jitter couplings in each individual TDI Michelson observable. This full correlation can be resolved by using all three Michelson observables for fitting the TTL coefficients.","sentences":["We present first-order models for tilt-to-length (TTL) coupling in LISA, both for the individual interferometers as well as in the time-delay interferometry (TDI) Michelson observables.","These models include the noise contributions from angular and lateral jitter coupling of the six test masses, six movable optical subassemblies (MOSAs), and three spacecraft.","We briefly discuss which terms are considered to be dominant and reduce the TTL model for the second-generation TDI Michelson X observable to these primary noise contributions to estimate the resulting noise level.","We show that the expected TTL noise will initially violate the entire mission displacement noise budget, resulting in the known necessity to fit and subtract TTL noise in data post-processing.","By comparing the noise levels for different assumptions prior to subtraction, we show why noise mitigation by realignment prior to subtraction is favorable.","We then discuss that the TTL coupling in the individual interferometers will have noise contributions that will not be present in the TDI observables.","Models for TTL coupling noise in TDI and in the individual interferometers are therefore different, and commonly made assumptions are valid as such only for TDI but not for the individual interferometers.","Finally, we analyze what implications can be drawn from the presented models for the subsequent fit-and-subtraction in post-processing.","We show that noise contributions from the test mass and inter-satellite interferometers are indistinguishable, such that only the combined coefficients can be fit and used for subtraction.","However, a distinction is considered not necessary.","Additionally, we show a correlation between coefficients for transmitter and receiver jitter couplings in each individual TDI Michelson observable.","This full correlation can be resolved by using all three Michelson observables for fitting the TTL coefficients."],"url":"http://arxiv.org/abs/2403.06526v1","category":"astro-ph.IM"}
{"created":"2024-03-11 08:58:42","title":"Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward","abstract":"We develop a deep reinforcement learning framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario. Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models. In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using curriculum learning techniques.","sentences":["We develop a deep reinforcement learning framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario.","Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models.","In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using curriculum learning techniques."],"url":"http://arxiv.org/abs/2403.06524v1","category":"cs.LG"}
{"created":"2024-03-11 08:58:17","title":"Ultrafast and highly collimated radially polarized photons from a colloidal quantum dot in a hybrid nanoantenna at room-temperature","abstract":"To harness the potential of radially polarized single photons in applications such as high-dimensional quantum key distribution (HD-QKD) and quantum communication, we demonstrate an on-chip, room-temperature device, which generates highly directional radially polarized photons at very high rates. The photons are emitted from a giant CdSe/CdS colloidal quantum dot (gQD) accurately positioned at the tip of a metal nanocone centered inside a hybrid metal-dielectric bullseye antenna. We show that due to the large and selective Purcell enhancement specifically for the out-of-plane optical dipole of the gQD, the emitted photons can have a very high degree of radial polarization (>93%), based on a quantitative metric. Our study emphasizes the importance of accurate gQD positioning for optimal radial polarization purity through extensive experiments and simulations, which contribute to the fundamental understanding of radial polarization in nanostructured devices and pave the way for implementation of such systems in practical applications using structured quantum light.","sentences":["To harness the potential of radially polarized single photons in applications such as high-dimensional quantum key distribution (HD-QKD) and quantum communication, we demonstrate an on-chip, room-temperature device, which generates highly directional radially polarized photons at very high rates.","The photons are emitted from a giant CdSe/CdS colloidal quantum dot (gQD) accurately positioned at the tip of a metal nanocone centered inside a hybrid metal-dielectric bullseye antenna.","We show that due to the large and selective Purcell enhancement specifically for the out-of-plane optical dipole of the gQD, the emitted photons can have a very high degree of radial polarization (>93%), based on a quantitative metric.","Our study emphasizes the importance of accurate gQD positioning for optimal radial polarization purity through extensive experiments and simulations, which contribute to the fundamental understanding of radial polarization in nanostructured devices and pave the way for implementation of such systems in practical applications using structured quantum light."],"url":"http://arxiv.org/abs/2403.06523v1","category":"physics.optics"}
{"created":"2024-03-11 08:55:44","title":"Exploring spin-squeezing in the Mott insulating regime: role of anisotropy, inhomogeneity and hole doping","abstract":"Spin-squeezing in systems with single-particle control is a well-established resource of modern quantum technology. Applied in an optical lattice clock can reduce the statistical uncertainty of spectroscopic measurements. Here, we consider dynamic generation of spin-squeezing with ultra-cold bosonic atoms with two internal states loaded into an optical lattice in the strongly interacting regime as realized with state-of-the-art experiments using a quantum gas microscope. We show that anisotropic interactions and inhomogeneous magnetic fields generate scalable spin-squeezing if their magnitudes are sufficiently small, but not negligible. The effect of non-uniform filling caused by hole doping, non-zero temperature and external confinement is studied at a microscopic level demonstrating their limiting role in the dynamics and scaling of spin squeezing.","sentences":["Spin-squeezing in systems with single-particle control is a well-established resource of modern quantum technology.","Applied in an optical lattice clock can reduce the statistical uncertainty of spectroscopic measurements.","Here, we consider dynamic generation of spin-squeezing with ultra-cold bosonic atoms with two internal states loaded into an optical lattice in the strongly interacting regime as realized with state-of-the-art experiments using a quantum gas microscope.","We show that anisotropic interactions and inhomogeneous magnetic fields generate scalable spin-squeezing if their magnitudes are sufficiently small, but not negligible.","The effect of non-uniform filling caused by hole doping, non-zero temperature and external confinement is studied at a microscopic level demonstrating their limiting role in the dynamics and scaling of spin squeezing."],"url":"http://arxiv.org/abs/2403.06521v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-11 08:52:52","title":"How to Understand Named Entities: Using Common Sense for News Captioning","abstract":"News captioning aims to describe an image with its news article body as input. It greatly relies on a set of detected named entities, including real-world people, organizations, and places. This paper exploits commonsense knowledge to understand named entities for news captioning. By ``understand'', we mean correlating the news content with common sense in the wild, which helps an agent to 1) distinguish semantically similar named entities and 2) describe named entities using words outside of training corpora. Our approach consists of three modules: (a) Filter Module aims to clarify the common sense concerning a named entity from two aspects: what does it mean? and what is it related to?, which divide the common sense into explanatory knowledge and relevant knowledge, respectively. (b) Distinguish Module aggregates explanatory knowledge from node-degree, dependency, and distinguish three aspects to distinguish semantically similar named entities. (c) Enrich Module attaches relevant knowledge to named entities to enrich the entity description by commonsense information (e.g., identity and social position). Finally, the probability distributions from both modules are integrated to generate the news captions. Extensive experiments on two challenging datasets (i.e., GoodNews and NYTimes) demonstrate the superiority of our method. Ablation studies and visualization further validate its effectiveness in understanding named entities.","sentences":["News captioning aims to describe an image with its news article body as input.","It greatly relies on a set of detected named entities, including real-world people, organizations, and places.","This paper exploits commonsense knowledge to understand named entities for news captioning.","By ``understand'', we mean correlating the news content with common sense in the wild, which helps an agent to 1) distinguish semantically similar named entities and 2) describe named entities using words outside of training corpora.","Our approach consists of three modules: (a) Filter Module aims to clarify the common sense concerning a named entity from two aspects: what does it mean?","and what is it related to?, which divide the common sense into explanatory knowledge and relevant knowledge, respectively.","(b) Distinguish Module aggregates explanatory knowledge from node-degree, dependency, and distinguish three aspects to distinguish semantically similar named entities.","(c) Enrich Module attaches relevant knowledge to named entities to enrich the entity description by commonsense information (e.g., identity and social position).","Finally, the probability distributions from both modules are integrated to generate the news captions.","Extensive experiments on two challenging datasets (i.e., GoodNews and NYTimes) demonstrate the superiority of our method.","Ablation studies and visualization further validate its effectiveness in understanding named entities."],"url":"http://arxiv.org/abs/2403.06520v1","category":"cs.CL"}
{"created":"2024-03-11 08:52:48","title":"Confinement of $N$-body systems and non-integer dimensions","abstract":"The squeezing process of a three-dimensional quantum system by use of an external deformed one-body oscillator potential can also be described by the $d$-method, without external field and where the dimension can take non-integer values. In this work we first generalize both methods to $N$ particles and any transition between dimensions below $3$. Once this is done, the use of harmonic oscillator interactions between the particles allows complete analytic solutions of both methods, and a direct comparison between them is possible. Assuming that both methods describe the same process, leading to the same ground state energy and wave function, an analytic equivalence between the methods arises. The equivalence between both methods and the validity of the derived analytic relation between them is first tested for two identical bosons and for squeezing transitions from 3 to 2 and 1 dimensions, as well as from 2 to 1 dimension. We also investigate the symmetric squeezing from 3 to 1 dimensions of a system made of three identical bosons. We have in all the cases found that the derived analytic relations between the two methods work very well. This fact permits to relate both methods also for large squeezing scenarios, where the brute force numerical calculation with the external field is too much demanding from the numerical point of view, especially for systems with more than two particles.","sentences":["The squeezing process of a three-dimensional quantum system by use of an external deformed one-body oscillator potential can also be described by the $d$-method, without external field and where the dimension can take non-integer values.","In this work we first generalize both methods to $N$ particles and any transition between dimensions below $3$. Once this is done, the use of harmonic oscillator interactions between the particles allows complete analytic solutions of both methods, and a direct comparison between them is possible.","Assuming that both methods describe the same process, leading to the same ground state energy and wave function, an analytic equivalence between the methods arises.","The equivalence between both methods and the validity of the derived analytic relation between them is first tested for two identical bosons and for squeezing transitions from 3 to 2 and 1 dimensions, as well as from 2 to 1 dimension.","We also investigate the symmetric squeezing from 3 to 1 dimensions of a system made of three identical bosons.","We have in all the cases found that the derived analytic relations between the two methods work very well.","This fact permits to relate both methods also for large squeezing scenarios, where the brute force numerical calculation with the external field is too much demanding from the numerical point of view, especially for systems with more than two particles."],"url":"http://arxiv.org/abs/2403.06519v1","category":"quant-ph"}
{"created":"2024-03-11 08:48:55","title":"Activation of entanglement in generalized entanglement swapping","abstract":"We study entanglement activation in a generalized entanglement swapping process involving two Bell pairs and generalized measurements. The conventional understanding posits entangled measurements as both necessary and sufficient for establishing entanglement between distant parties. In this study, we reassess the role of measurement operators in entanglement generation within a generalized entanglement swapping process. We focus on maximally entangled two-qubit initial states and generalized measurements, investigating the necessity and sufficiency conditions for entangled measurement operators. By utilizing two Bell pairs, (1, 2) shared between Alice and Bob, and (3, 4) shared between Bob and Charlie, we demonstrate that while entangled measurements are sufficient, they are not indispensable for establishing entanglement between spatially separated observers. Through a sequential approach, if Bob performs an initial measurement which is not able to establish entanglement then followed by another measurement after post-processing the first measurement it is possible to establish entanglement. We identify specific criteria for different measurement operators that enable the potential for performing a second measurement to establish entanglement. Our findings highlight the feasibility of generating entanglement between distant parties through a combination of measurements, shedding new light on entanglement distribution in quantum networks. Additionally, we showcase through illustrative examples how successive measurements enhance entanglement compared to single measurements, underscoring the practical benefits of our approach in enhancing entanglement. Moreover, our protocol extends beyond bipartite qubit states to higher-dimensional maximally entangled states, emphasizing its versatility and applicability.","sentences":["We study entanglement activation in a generalized entanglement swapping process involving two Bell pairs and generalized measurements.","The conventional understanding posits entangled measurements as both necessary and sufficient for establishing entanglement between distant parties.","In this study, we reassess the role of measurement operators in entanglement generation within a generalized entanglement swapping process.","We focus on maximally entangled two-qubit initial states and generalized measurements, investigating the necessity and sufficiency conditions for entangled measurement operators.","By utilizing two Bell pairs, (1, 2) shared between Alice and Bob, and (3, 4) shared between Bob and Charlie, we demonstrate that while entangled measurements are sufficient, they are not indispensable for establishing entanglement between spatially separated observers.","Through a sequential approach, if Bob performs an initial measurement which is not able to establish entanglement then followed by another measurement after post-processing the first measurement it is possible to establish entanglement.","We identify specific criteria for different measurement operators that enable the potential for performing a second measurement to establish entanglement.","Our findings highlight the feasibility of generating entanglement between distant parties through a combination of measurements, shedding new light on entanglement distribution in quantum networks.","Additionally, we showcase through illustrative examples how successive measurements enhance entanglement compared to single measurements, underscoring the practical benefits of our approach in enhancing entanglement.","Moreover, our protocol extends beyond bipartite qubit states to higher-dimensional maximally entangled states, emphasizing its versatility and applicability."],"url":"http://arxiv.org/abs/2403.06518v1","category":"quant-ph"}
{"created":"2024-03-11 08:45:31","title":"Active Generation for Image Classification","abstract":"Recently, the growing capabilities of deep generative models have underscored their potential in enhancing image classification accuracy. However, existing methods often demand the generation of a disproportionately large number of images compared to the original dataset, while having only marginal improvements in accuracy. This computationally expensive and time-consuming process hampers the practicality of such approaches. In this paper, we propose to address the efficiency of image generation by focusing on the specific needs and characteristics of the model. With a central tenet of active learning, our method, named ActGen, takes a training-aware approach to image generation. It aims to create images akin to the challenging or misclassified samples encountered by the current model and incorporates these generated images into the training set to augment model performance. ActGen introduces an attentive image guidance technique, using real images as guides during the denoising process of a diffusion model. The model's attention on class prompt is leveraged to ensure the preservation of similar foreground object while diversifying the background. Furthermore, we introduce a gradient-based generation guidance method, which employs two losses to generate more challenging samples and prevent the generated images from being too similar to previously generated ones. Experimental results on the CIFAR and ImageNet datasets demonstrate that our method achieves better performance with a significantly reduced number of generated images.","sentences":["Recently, the growing capabilities of deep generative models have underscored their potential in enhancing image classification accuracy.","However, existing methods often demand the generation of a disproportionately large number of images compared to the original dataset, while having only marginal improvements in accuracy.","This computationally expensive and time-consuming process hampers the practicality of such approaches.","In this paper, we propose to address the efficiency of image generation by focusing on the specific needs and characteristics of the model.","With a central tenet of active learning, our method, named ActGen, takes a training-aware approach to image generation.","It aims to create images akin to the challenging or misclassified samples encountered by the current model and incorporates these generated images into the training set to augment model performance.","ActGen introduces an attentive image guidance technique, using real images as guides during the denoising process of a diffusion model.","The model's attention on class prompt is leveraged to ensure the preservation of similar foreground object while diversifying the background.","Furthermore, we introduce a gradient-based generation guidance method, which employs two losses to generate more challenging samples and prevent the generated images from being too similar to previously generated ones.","Experimental results on the CIFAR and ImageNet datasets demonstrate that our method achieves better performance with a significantly reduced number of generated images."],"url":"http://arxiv.org/abs/2403.06517v1","category":"cs.CV"}
{"created":"2024-03-11 08:43:57","title":"Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning","abstract":"Recent advances in text-conditioned image generation diffusion models have begun paving the way for new opportunities in modern medical domain, in particular, generating Chest X-rays (CXRs) from diagnostic reports. Nonetheless, to further drive the diffusion models to generate CXRs that faithfully reflect the complexity and diversity of real data, it has become evident that a nontrivial learning approach is needed. In light of this, we propose CXRL, a framework motivated by the potential of reinforcement learning (RL). Specifically, we integrate a policy gradient RL approach with well-designed multiple distinctive CXR-domain specific reward models. This approach guides the diffusion denoising trajectory, achieving precise CXR posture and pathological details. Here, considering the complex medical image environment, we present \"RL with Comparative Feedback\" (RLCF) for the reward mechanism, a human-like comparative evaluation that is known to be more effective and reliable in complex scenarios compared to direct evaluation. Our CXRL framework includes jointly optimizing learnable adaptive condition embeddings (ACE) and the image generator, enabling the model to produce more accurate and higher perceptual CXR quality. Our extensive evaluation of the MIMIC-CXR-JPG dataset demonstrates the effectiveness of our RL-based tuning approach. Consequently, our CXRL generates pathologically realistic CXRs, establishing a new standard for generating CXRs with high fidelity to real-world clinical scenarios.","sentences":["Recent advances in text-conditioned image generation diffusion models have begun paving the way for new opportunities in modern medical domain, in particular, generating Chest X-rays (CXRs) from diagnostic reports.","Nonetheless, to further drive the diffusion models to generate CXRs that faithfully reflect the complexity and diversity of real data, it has become evident that a nontrivial learning approach is needed.","In light of this, we propose CXRL, a framework motivated by the potential of reinforcement learning (RL).","Specifically, we integrate a policy gradient RL approach with well-designed multiple distinctive CXR-domain specific reward models.","This approach guides the diffusion denoising trajectory, achieving precise CXR posture and pathological details.","Here, considering the complex medical image environment, we present \"RL with Comparative Feedback\" (RLCF) for the reward mechanism, a human-like comparative evaluation that is known to be more effective and reliable in complex scenarios compared to direct evaluation.","Our CXRL framework includes jointly optimizing learnable adaptive condition embeddings (ACE) and the image generator, enabling the model to produce more accurate and higher perceptual CXR quality.","Our extensive evaluation of the MIMIC-CXR-JPG dataset demonstrates the effectiveness of our RL-based tuning approach.","Consequently, our CXRL generates pathologically realistic CXRs, establishing a new standard for generating CXRs with high fidelity to real-world clinical scenarios."],"url":"http://arxiv.org/abs/2403.06516v1","category":"cs.CV"}
{"created":"2024-03-11 17:57:41","title":"Memory-based Adapters for Online 3D Scene Perception","abstract":"In this paper, we propose a new framework for online 3D scene perception. Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene geometry as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos. To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information. To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability. Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features. Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame. We further propose 3D-to-2D adapter to enhance image features with strong global context. Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply finetuning existing offline models, without any model and task-specific designs. \\href{https://xuxw98.github.io/Online3D/}{Project page}.","sentences":["In this paper, we propose a new framework for online 3D scene perception.","Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene geometry as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos.","To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information.","To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability.","Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features.","Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame.","We further propose 3D-to-2D adapter to enhance image features with strong global context.","Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks.","Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply finetuning existing offline models, without any model and task-specific designs.","\\href{https://xuxw98.github.io/Online3D/}{Project page}."],"url":"http://arxiv.org/abs/2403.06974v1","category":"cs.CV"}
{"created":"2024-03-11 17:47:47","title":"Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena","abstract":"Argument Structure Constructions (ASCs) are one of the most well-studied construction groups, providing a unique opportunity to demonstrate the usefulness of Construction Grammar (CxG). For example, the caused-motion construction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates that constructions must carry meaning, otherwise the fact that ``sneeze'' in this context causes movement cannot be explained. We form the hypothesis that this remains challenging even for state-of-the-art Large Language Models (LLMs), for which we devise a test based on substituting the verb with a prototypical motion verb. To be able to perform this test at statistically significant scale, in the absence of adequate CxG corpora, we develop a novel pipeline of NLP-assisted collection of linguistically annotated text. We show how dependency parsing and GPT-3.5 can be used to significantly reduce annotation cost and thus enable the annotation of rare phenomena at scale. We then evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of the CMC using the newly collected corpus. We find that all models struggle with understanding the motion component that the CMC adds to a sentence.","sentences":["Argument Structure Constructions (ASCs) are one of the most well-studied construction groups, providing a unique opportunity to demonstrate the usefulness of Construction Grammar (CxG).","For example, the caused-motion construction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates that constructions must carry meaning, otherwise the fact that ``sneeze'' in this context causes movement cannot be explained.","We form the hypothesis that this remains challenging even for state-of-the-art Large Language Models (LLMs), for which we devise a test based on substituting the verb with a prototypical motion verb.","To be able to perform this test at statistically significant scale, in the absence of adequate CxG corpora, we develop a novel pipeline of NLP-assisted collection of linguistically annotated text.","We show how dependency parsing and GPT-3.5 can be used to significantly reduce annotation cost and thus enable the annotation of rare phenomena at scale.","We then evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of the CMC using the newly collected corpus.","We find that all models struggle with understanding the motion component that the CMC adds to a sentence."],"url":"http://arxiv.org/abs/2403.06965v1","category":"cs.CL"}
{"created":"2024-03-11 17:25:01","title":"TCAM-SSD: A Framework for Search-Based Computing in Solid-State Drives","abstract":"As the amount of data produced in society continues to grow at an exponential rate, modern applications are incurring significant performance and energy penalties due to high data movement between the CPU and memory/storage. While processing in main memory can alleviate these penalties, it is becoming increasingly difficult to keep large datasets entirely in main memory. This has led to a recent push for in-storage computation, where processing is performed inside the storage device.   We propose TCAM-SSD, a new framework for search-based computation inside the NAND flash memory arrays of a conventional solid-state drive (SSD), which requires lightweight modifications to only the array periphery and firmware. TCAM-SSD introduces a search manager and link table, which can logically partition the NAND flash memory's contents into search-enabled regions and standard storage regions. Together, these light firmware changes enable TCAM-SSD to seamlessly handle block I/O operations, in addition to new search operations, thereby reducing end-to-end execution time and total data movement. We provide an NVMe-compatible interface that provides programmers with the ability to dynamically allocate data on and make use of TCAM-SSD, allowing the system to be leveraged by a wide variety of applications. We evaluate three example use cases of TCAM-SSD to demonstrate its benefits. For transactional databases, TCAM-SSD can mitigate the performance penalties for applications with large datasets, achieving a 60.9% speedup over a conventional system that retrieves data from the SSD and computes using the CPU. For database analytics, TCAM-SSD provides an average speedup of 17.7x over a conventional system for a collection of analytical queries. For graph analytics, we combine TCAM-SSD's associative search with a sparse data structure, speeding up graph computing for larger-than-memory datasets by 14.5%.","sentences":["As the amount of data produced in society continues to grow at an exponential rate, modern applications are incurring significant performance and energy penalties due to high data movement between the CPU and memory/storage.","While processing in main memory can alleviate these penalties, it is becoming increasingly difficult to keep large datasets entirely in main memory.","This has led to a recent push for in-storage computation, where processing is performed inside the storage device.   ","We propose TCAM-SSD, a new framework for search-based computation inside the NAND flash memory arrays of a conventional solid-state drive (SSD), which requires lightweight modifications to only the array periphery and firmware.","TCAM-SSD introduces a search manager and link table, which can logically partition the NAND flash memory's contents into search-enabled regions and standard storage regions.","Together, these light firmware changes enable TCAM-SSD to seamlessly handle block I/O operations, in addition to new search operations, thereby reducing end-to-end execution time and total data movement.","We provide an NVMe-compatible interface that provides programmers with the ability to dynamically allocate data on and make use of TCAM-SSD, allowing the system to be leveraged by a wide variety of applications.","We evaluate three example use cases of TCAM-SSD to demonstrate its benefits.","For transactional databases, TCAM-SSD can mitigate the performance penalties for applications with large datasets, achieving a 60.9% speedup over a conventional system that retrieves data from the SSD and computes using the CPU.","For database analytics, TCAM-SSD provides an average speedup of 17.7x over a conventional system for a collection of analytical queries.","For graph analytics, we combine TCAM-SSD's associative search with a sparse data structure, speeding up graph computing for larger-than-memory datasets by 14.5%."],"url":"http://arxiv.org/abs/2403.06938v1","category":"cs.AR"}
{"created":"2024-03-11 16:54:23","title":"Dynamic Client Clustering, Bandwidth Allocation, and Workload Optimization for Semi-synchronous Federated Learning","abstract":"Federated Learning (FL) revolutionizes collaborative machine learning among Internet of Things (IoT) devices by enabling them to train models collectively while preserving data privacy. FL algorithms fall into two primary categories: synchronous and asynchronous. While synchronous FL efficiently handles straggler devices, it can compromise convergence speed and model accuracy. In contrast, asynchronous FL allows all devices to participate but incurs high communication overhead and potential model staleness. To overcome these limitations, the semi-synchronous FL framework introduces client tiering based on computing and communication latencies. Clients in different tiers upload their local models at distinct frequencies, striking a balance between straggler mitigation and communication costs. Enter the DecantFed algorithm (Dynamic client clustering, bandwidth allocation, and local training for semi-synchronous Federated learning), a dynamic solution that optimizes client clustering, bandwidth allocation, and local training workloads to maximize data sample processing rates. Additionally, DecantFed adapts client learning rates according to their tiers, addressing the model staleness problem. The algorithm's performance shines in extensive simulations using benchmark datasets, including MNIST and CIFAR-10, under independent and identically distributed (IID) and non-IID scenarios. DecantFed outpaces FedAvg and FedProx in terms of convergence speed and delivers a remarkable minimum 28% boost in model accuracy compared to FedProx.","sentences":["Federated Learning (FL) revolutionizes collaborative machine learning among Internet of Things (IoT) devices by enabling them to train models collectively while preserving data privacy.","FL algorithms fall into two primary categories: synchronous and asynchronous.","While synchronous FL efficiently handles straggler devices, it can compromise convergence speed and model accuracy.","In contrast, asynchronous FL allows all devices to participate but incurs high communication overhead and potential model staleness.","To overcome these limitations, the semi-synchronous FL framework introduces client tiering based on computing and communication latencies.","Clients in different tiers upload their local models at distinct frequencies, striking a balance between straggler mitigation and communication costs.","Enter the DecantFed algorithm (Dynamic client clustering, bandwidth allocation, and local training for semi-synchronous Federated learning), a dynamic solution that optimizes client clustering, bandwidth allocation, and local training workloads to maximize data sample processing rates.","Additionally, DecantFed adapts client learning rates according to their tiers, addressing the model staleness problem.","The algorithm's performance shines in extensive simulations using benchmark datasets, including MNIST and CIFAR-10, under independent and identically distributed (IID) and non-IID scenarios.","DecantFed outpaces FedAvg and FedProx in terms of convergence speed and delivers a remarkable minimum 28% boost in model accuracy compared to FedProx."],"url":"http://arxiv.org/abs/2403.06900v1","category":"cs.DC"}
{"created":"2024-03-11 16:31:48","title":"Anderson-Higgs amplitude mode in Josephson junctions","abstract":"The Anderson-Higgs mode in a superconductor corresponds to a collective and coherent oscillation of the order parameter amplitude. We propose to detect this mode in a tunnel Josephson junction between two singlet s-wave diffusive superconductors. We find a strong enhancement of the tunneling current when the junction is pumped at the equilibrium gap frequency, corresponding to the activation of the Anderson-Higgs mode. By solving the Keldysh-Usadel equations, we obtain current peaks at specific bias voltages that can serve as signatures of the Anderson-Higgs mode.","sentences":["The Anderson-Higgs mode in a superconductor corresponds to a collective and coherent oscillation of the order parameter amplitude.","We propose to detect this mode in a tunnel Josephson junction between two singlet s-wave diffusive superconductors.","We find a strong enhancement of the tunneling current when the junction is pumped at the equilibrium gap frequency, corresponding to the activation of the Anderson-Higgs mode.","By solving the Keldysh-Usadel equations, we obtain current peaks at specific bias voltages that can serve as signatures of the Anderson-Higgs mode."],"url":"http://arxiv.org/abs/2403.06878v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 16:13:58","title":"Quantum thermodynamics of driven-dissipative condensates","abstract":"Polariton condensates occur away from thermal equilibrium, in an open system where heat and particles are continually exchanged with reservoirs. These phenomena have been extensively analyzed in terms of kinetic equations. Based on the collection of knowledge about polariton kinetics provided by these simulations and by experimental works, we constructed a few-level model that captures the main processes involved in the buildup of a ground-state population of polaritons. This allows condensation to be understood as the output of a heat engine and exposes the thermodynamic constraints on its occurrence. The model consists of a three-level system interacting with a field and connected to a hot and a cold thermal reservoir that represent a non-resonant pump and the lattice phonons. This subsystem can drive a condensate, through polariton-polariton scattering, which produces work in the form of coherent light emission from the microcavity. We obtain a phase diagram as a function of the temperatures of the two baths and investigate the possible types of phase transition that lead to the condensate phase.","sentences":["Polariton condensates occur away from thermal equilibrium, in an open system where heat and particles are continually exchanged with reservoirs.","These phenomena have been extensively analyzed in terms of kinetic equations.","Based on the collection of knowledge about polariton kinetics provided by these simulations and by experimental works, we constructed a few-level model that captures the main processes involved in the buildup of a ground-state population of polaritons.","This allows condensation to be understood as the output of a heat engine and exposes the thermodynamic constraints on its occurrence.","The model consists of a three-level system interacting with a field and connected to a hot and a cold thermal reservoir that represent a non-resonant pump and the lattice phonons.","This subsystem can drive a condensate, through polariton-polariton scattering, which produces work in the form of coherent light emission from the microcavity.","We obtain a phase diagram as a function of the temperatures of the two baths and investigate the possible types of phase transition that lead to the condensate phase."],"url":"http://arxiv.org/abs/2403.06861v1","category":"quant-ph"}
{"created":"2024-03-11 15:39:52","title":"A quasilinear Keller-Segel model with saturated discontinuous advection","abstract":"We consider the singular limit of a chemotaxis model of bacterial collective motion recently introduced in arXiv:2009.11048 [math.AP]. The equation models aggregation-diffusion phenomena with advection that is discontinuous and depends sharply on the gradient of the density itself. The quasi-linearity of the problem poses major challenges in the construction of the solution and complications arise in the proof of regularity. Our method overcomes these obstacle by relying solely on entropy inequalities and the theory of monotone operators. We provide existence, uniqueness and smoothing estimates in any dimensional space.","sentences":["We consider the singular limit of a chemotaxis model of bacterial collective motion recently introduced in arXiv:2009.11048","[math.AP].","The equation models aggregation-diffusion phenomena with advection that is discontinuous and depends sharply on the gradient of the density itself.","The quasi-linearity of the problem poses major challenges in the construction of the solution and complications arise in the proof of regularity.","Our method overcomes these obstacle by relying solely on entropy inequalities and the theory of monotone operators.","We provide existence, uniqueness and smoothing estimates in any dimensional space."],"url":"http://arxiv.org/abs/2403.06820v1","category":"math.AP"}
{"created":"2024-03-11 14:36:43","title":"Determination of the number of $\u03c8(3686)$ events taken at BESIII","abstract":"The number of $\\psi(3686)$ events collected by the BESIII detector during the 2021 run period is determined to be $(2259.3\\pm 11.1)\\times 10^6$ by counting inclusive $\\psi(3686)$ hadronic events. The uncertainty is systematic and the statistical uncertainty is negligible. Meanwhile, the numbers of $\\psi(3686)$ events collected during the 2009 and 2012 run periods are updated to be $(107.7\\pm0.6)\\times 10^6$ and $(345.4\\pm 2.6)\\times 10^6$, respectively. Both numbers are consistent with the previous measurements within one standard deviation. The total number of $\\psi(3686)$ events in the three data samples is $(2712.4\\pm14.3)\\times10^6$.","sentences":["The number of $\\psi(3686)$ events collected by the BESIII detector during the 2021 run period is determined to be $(2259.3\\pm 11.1)\\times 10^6$ by counting inclusive $\\psi(3686)$ hadronic events.","The uncertainty is systematic and the statistical uncertainty is negligible.","Meanwhile, the numbers of $\\psi(3686)$ events collected during the 2009 and 2012 run periods are updated to be $(107.7\\pm0.6)\\times 10^6$ and $(345.4\\pm 2.6)\\times 10^6$, respectively.","Both numbers are consistent with the previous measurements within one standard deviation.","The total number of $\\psi(3686)$ events in the three data samples is $(2712.4\\pm14.3)\\times10^6$."],"url":"http://arxiv.org/abs/2403.06766v1","category":"hep-ex"}
{"created":"2024-03-11 14:28:02","title":"Complementing cell taxonomies with a multicellular functional analysis of tissues","abstract":"The application of single-cell molecular profiling coupled with spatial technologies has enabled charting cellular heterogeneity in reference tissues and in disease. This new wave of molecular data has highlighted the expected diversity of single-cell dynamics upon shared external queues and spatial organizations. However, little is known about the relationship between single cell heterogeneity and the emergence and maintenance of robust multicellular processes in developed tissues and its role in (patho)physiology. Here, we present emerging computational modeling strategies that use increasingly available large-scale cross-condition single cell and spatial datasets, to study multicellular organization in tissues and complement cell taxonomies. This perspective should enable us to better understand how cells within tissues collectively process information and adapt synchronized responses in disease contexts and to bridge the gap between structural changes and functions in tissues.","sentences":["The application of single-cell molecular profiling coupled with spatial technologies has enabled charting cellular heterogeneity in reference tissues and in disease.","This new wave of molecular data has highlighted the expected diversity of single-cell dynamics upon shared external queues and spatial organizations.","However, little is known about the relationship between single cell heterogeneity and the emergence and maintenance of robust multicellular processes in developed tissues and its role in (patho)physiology.","Here, we present emerging computational modeling strategies that use increasingly available large-scale cross-condition single cell and spatial datasets, to study multicellular organization in tissues and complement cell taxonomies.","This perspective should enable us to better understand how cells within tissues collectively process information and adapt synchronized responses in disease contexts and to bridge the gap between structural changes and functions in tissues."],"url":"http://arxiv.org/abs/2403.06753v1","category":"q-bio.TO"}
{"created":"2024-03-11 14:10:08","title":"PolyominoIdeals: a package for Macaulay2 to work with the inner $2$-minor ideals of collections of cells","abstract":"Let $\\mathcal{P}$ be a collection of cells and $I_\\mathcal{P}$ be the associated ideal of inner $2$-minors as defined by A. A. Qureshi in 2012. In this paper, we provide a description of the package $\\texttt{PolyominoIdeals}$ for the computer algebra software $\\texttt{Macaulay2}$. More precisely, this package provides some functions that allow to define the ideal $I_{\\mathcal{P}}$ in $\\texttt{Macaulay2}$ and to compute its algebraic invariants or verifying its algebraic properties. We explain the usage of these functions and also give some examples.","sentences":["Let $\\mathcal{P}$ be a collection of cells and $I_\\mathcal{P}$ be the associated ideal of inner $2$-minors as defined by A. A. Qureshi in 2012.","In this paper, we provide a description of the package $\\texttt{PolyominoIdeals}$ for the computer algebra software $\\texttt{Macaulay2}$. More precisely, this package provides some functions that allow to define the ideal $I_{\\mathcal{P}}$ in $\\texttt{Macaulay2}$ and to compute its algebraic invariants or verifying its algebraic properties.","We explain the usage of these functions and also give some examples."],"url":"http://arxiv.org/abs/2403.06743v1","category":"math.AC"}
{"created":"2024-03-11 14:07:21","title":"A Two-Field-Scan Harmonic Hall Voltage Analysis For Fast, Accurate Quantification Of Spin-Orbit Torques In Magnetic Heterostructures","abstract":"The efficiencies of the spin-orbit torques (SOTs) play a key role in the determination of the power consumption, integration density, and endurance of SOT-driven devices. Accurate and time-efficient determination of the SOT efficiencies is of great importance not only for evaluating the practical potential of SOT devices but also for developing new mechanisms for enhancing the SOT efficiencies. Here, we develop a \"two-field-scan\" harmonic Hall voltage (HHV) analysis that collects the second HHV as a function of a swept in-plane magnetic field at 45{\\deg} and 0{\\deg} relative to the excitation current. We demonstrate that this two-field-scan analysis is as accurate as the well-established but time-consuming angle-scan HHV analysis even in the presence of considerable thermoelectric effects but takes more than a factor of 7 less measurement time. We also show that the 3-parameter fit of the HHV data from a single field scan at 0{\\deg}, which is commonly employed in the literature, is not reliable because the employment of too many free parameters in the fitting of the very slowly varying HHV signal allows unrealistic pseudo-solution and thus erroneous conclusion about the SOT efficiencies.","sentences":["The efficiencies of the spin-orbit torques (SOTs) play a key role in the determination of the power consumption, integration density, and endurance of SOT-driven devices.","Accurate and time-efficient determination of the SOT efficiencies is of great importance not only for evaluating the practical potential of SOT devices but also for developing new mechanisms for enhancing the SOT efficiencies.","Here, we develop a \"two-field-scan\" harmonic Hall voltage (HHV) analysis that collects the second HHV as a function of a swept in-plane magnetic field at 45{\\deg} and 0{\\deg} relative to the excitation current.","We demonstrate that this two-field-scan analysis is as accurate as the well-established but time-consuming angle-scan HHV analysis even in the presence of considerable thermoelectric effects but takes more than a factor of 7 less measurement time.","We also show that the 3-parameter fit of the HHV data from a single field scan at 0{\\deg}, which is commonly employed in the literature, is not reliable because the employment of too many free parameters in the fitting of the very slowly varying HHV signal allows unrealistic pseudo-solution and thus erroneous conclusion about the SOT efficiencies."],"url":"http://arxiv.org/abs/2403.06740v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 14:03:57","title":"Energy loss of a heavy fermion in a collisional QED plasma","abstract":"We compute the energy loss of heavy fermions moving in a plasma, taking into account the modification of the photon collective modes induced by collisions using a Bhatnagar-Gross-Krook collisional kernel. We include contributions from both hard and soft scatterings of the heavy fermion using a collisionally modified hard-thermal-loop resummed propagator. Using this method, one does not need to introduce a separation scale between hard- and soft-momentum exchanges. To place our calculation in context, we review other theoretical approaches to computing the collisional energy loss of fermions and discuss the systematics and results obtained in each approach compared to using a resummed propagator for both hard and soft momentum exchanges. Our final results indicate that self-consistently including the effect of collisions in the self-energies of the resummed propagator results in an increased energy loss compared to using collisionless hard-thermal-loop propagators. The effect becomes larger as the magnitude of the coupling constant and the velocity of the fermion increase.","sentences":["We compute the energy loss of heavy fermions moving in a plasma, taking into account the modification of the photon collective modes induced by collisions using a Bhatnagar-Gross-Krook collisional kernel.","We include contributions from both hard and soft scatterings of the heavy fermion using a collisionally modified hard-thermal-loop resummed propagator.","Using this method, one does not need to introduce a separation scale between hard- and soft-momentum exchanges.","To place our calculation in context, we review other theoretical approaches to computing the collisional energy loss of fermions and discuss the systematics and results obtained in each approach compared to using a resummed propagator for both hard and soft momentum exchanges.","Our final results indicate that self-consistently including the effect of collisions in the self-energies of the resummed propagator results in an increased energy loss compared to using collisionless hard-thermal-loop propagators.","The effect becomes larger as the magnitude of the coupling constant and the velocity of the fermion increase."],"url":"http://arxiv.org/abs/2403.06739v1","category":"hep-ph"}
{"created":"2024-03-11 13:44:41","title":"A note on the Segal conjecture for large objects","abstract":"The Segal conjecture for $C_p$ (as proved by Lin and Gunawardena) asserts that the canonical map from the $p$-complete sphere spectrum to the Tate construction for the trivial action of $C_p$ on the $p$-complete sphere spectrum is an isomorphism. In this article we extend the collection of spectra for which the canonical map $X \\to X^{tC_p}$ is known to be an isomorphism to include any $p$-complete, bounded below spectrum whose mod $p$ homology, viewed a module over the Steenrod algebra, is complete with respect to the maximal ideal $I \\subseteq \\mathcal{A}$.","sentences":["The Segal conjecture for $C_p$ (as proved by Lin and Gunawardena) asserts that the canonical map from the $p$-complete sphere spectrum to the Tate construction for the trivial action of $C_p$ on the $p$-complete sphere spectrum is an isomorphism.","In this article we extend the collection of spectra for which the canonical map $X \\to X^{tC_p}$ is known to be an isomorphism to include any $p$-complete, bounded below spectrum whose mod $p$ homology, viewed a module over the Steenrod algebra, is complete with respect to the maximal ideal $I \\subseteq \\mathcal{A}$."],"url":"http://arxiv.org/abs/2403.06724v1","category":"math.AT"}
{"created":"2024-03-11 13:40:46","title":"Emergency Response Inference Mapping (ERIMap): A Bayesian Network-based Method for Dynamic Observation Processing in Spatially Distributed Emergencies","abstract":"In emergencies, high stake decisions often have to be made under time pressure and strain. In order to support such decisions, information from various sources needs to be collected and processed rapidly. The information available tends to be temporally and spatially variable, uncertain, and sometimes conflicting, leading to potential biases in decisions. Currently, there is a lack of systematic approaches for information processing and situation assessment which meet the particular demands of emergency situations. To address this gap, we present a Bayesian network-based method called ERIMap that is tailored to the complex information-scape during emergencies. The method enables the systematic and rapid processing of heterogeneous and potentially uncertain observations and draws inferences about key variables of an emergency. It thereby reduces complexity and cognitive load for decision makers. The output of the ERIMap method is a dynamically evolving and spatially resolved map of beliefs about key variables of an emergency that is updated each time a new observation becomes available. The method is illustrated in a case study in which an emergency response is triggered by an accident causing a gas leakage on a chemical plant site.","sentences":["In emergencies, high stake decisions often have to be made under time pressure and strain.","In order to support such decisions, information from various sources needs to be collected and processed rapidly.","The information available tends to be temporally and spatially variable, uncertain, and sometimes conflicting, leading to potential biases in decisions.","Currently, there is a lack of systematic approaches for information processing and situation assessment which meet the particular demands of emergency situations.","To address this gap, we present a Bayesian network-based method called ERIMap that is tailored to the complex information-scape during emergencies.","The method enables the systematic and rapid processing of heterogeneous and potentially uncertain observations and draws inferences about key variables of an emergency.","It thereby reduces complexity and cognitive load for decision makers.","The output of the ERIMap method is a dynamically evolving and spatially resolved map of beliefs about key variables of an emergency that is updated each time a new observation becomes available.","The method is illustrated in a case study in which an emergency response is triggered by an accident causing a gas leakage on a chemical plant site."],"url":"http://arxiv.org/abs/2403.06716v1","category":"cs.IR"}
{"created":"2024-03-11 09:41:54","title":"Integrated SWIPT Receiver with Memory Effects: Circuit Analysis and Information Detection","abstract":"Wireless power transfer has been proposed as a key technology for the foreseen machine type networks. A main challenge in the research community lies in acquiring a simple yet accurate model to capture the energy harvesting performance. In this work, we focus on a half-wave rectifier and based on circuit analysis we provide the actual output of the circuit which accounts for the memory introduced by the capacitor. The provided expressions are also validated through circuit simulations on ADS. Then, the half-wave rectifier is used as an integrated simultaneous wireless information and power transfer receiver where the circuit's output is used for decoding information based on amplitude modulation. We investigate the bit error rate performance based on two detection schemes: (i) symbol-by-symbol maximum likelihood (ML); and (ii) ML sequence detection (MLSD). We show that the symbol period is critical due to the intersymbol interference induced by circuit. Our results reveal that MLSD is necessary towards improving the error probability and achieving higher data rates.","sentences":["Wireless power transfer has been proposed as a key technology for the foreseen machine type networks.","A main challenge in the research community lies in acquiring a simple yet accurate model to capture the energy harvesting performance.","In this work, we focus on a half-wave rectifier and based on circuit analysis we provide the actual output of the circuit which accounts for the memory introduced by the capacitor.","The provided expressions are also validated through circuit simulations on ADS.","Then, the half-wave rectifier is used as an integrated simultaneous wireless information and power transfer receiver where the circuit's output is used for decoding information based on amplitude modulation.","We investigate the bit error rate performance based on two detection schemes: (i) symbol-by-symbol maximum likelihood (ML); and (ii) ML sequence detection (MLSD).","We show that the symbol period is critical due to the intersymbol interference induced by circuit.","Our results reveal that MLSD is necessary towards improving the error probability and achieving higher data rates."],"url":"http://arxiv.org/abs/2403.06544v1","category":"eess.SP"}
{"created":"2024-03-11 08:40:37","title":"Structure Your Data: Towards Semantic Graph Counterfactuals","abstract":"Counterfactual explanations (CEs) based on concepts are explanations that consider alternative scenarios to understand which high-level semantic features contributed to particular model predictions. In this work, we propose CEs based on the semantic graphs accompanying input data to achieve more descriptive, accurate, and human-aligned explanations. Building upon state-of-the-art (SoTA) conceptual attempts, we adopt a model-agnostic edit-based approach and introduce leveraging GNNs for efficient Graph Edit Distance (GED) computation. With a focus on the visual domain, we represent images as scene graphs and obtain their GNN embeddings to bypass solving the NP-hard graph similarity problem for all input pairs, an integral part of the CE computation process. We apply our method to benchmark and real-world datasets with varying difficulty and availability of semantic annotations. Testing on diverse classifiers, we find that our CEs outperform previous SoTA explanation models based on semantics, including both white and black-box as well as conceptual and pixel-level approaches. Their superiority is proven quantitatively and qualitatively, as validated by human subjects, highlighting the significance of leveraging semantic edges in the presence of intricate relationships. Our model-agnostic graph-based approach is widely applicable and easily extensible, producing actionable explanations across different contexts.","sentences":["Counterfactual explanations (CEs) based on concepts are explanations that consider alternative scenarios to understand which high-level semantic features contributed to particular model predictions.","In this work, we propose CEs based on the semantic graphs accompanying input data to achieve more descriptive, accurate, and human-aligned explanations.","Building upon state-of-the-art (SoTA) conceptual attempts, we adopt a model-agnostic edit-based approach and introduce leveraging GNNs for efficient Graph Edit Distance (GED) computation.","With a focus on the visual domain, we represent images as scene graphs and obtain their GNN embeddings to bypass solving the NP-hard graph similarity problem for all input pairs, an integral part of the CE computation process.","We apply our method to benchmark and real-world datasets with varying difficulty and availability of semantic annotations.","Testing on diverse classifiers, we find that our CEs outperform previous SoTA explanation models based on semantics, including both white and black-box as well as conceptual and pixel-level approaches.","Their superiority is proven quantitatively and qualitatively, as validated by human subjects, highlighting the significance of leveraging semantic edges in the presence of intricate relationships.","Our model-agnostic graph-based approach is widely applicable and easily extensible, producing actionable explanations across different contexts."],"url":"http://arxiv.org/abs/2403.06514v1","category":"cs.CV"}
{"created":"2024-03-11 08:40:01","title":"Asset-driven Threat Modeling for AI-based Systems","abstract":"Threat modeling is a popular method to securely develop systems by achieving awareness of potential areas of future damage caused by adversaries. The benefit of threat modeling lies in its ability to indicate areas of concern, paving the way to consider mitigation during the design stage. However, threat modeling for systems relying on Artificial Intelligence is still not well explored. While conventional threat modeling methods and tools did not address AI-related threats, research on this amalgamation still lacks solutions capable of guiding and automating the process, as well as providing evidence that the methods hold up in practice. To evaluate that the work at hand is able to guide and automatically identify AI-related threats during the architecture definition stage, several experts were tasked to create a threat model of an AI system designed in the healthcare domain. The usability of the solution was well-perceived, and the results indicate that it is effective for threat identification.","sentences":["Threat modeling is a popular method to securely develop systems by achieving awareness of potential areas of future damage caused by adversaries.","The benefit of threat modeling lies in its ability to indicate areas of concern, paving the way to consider mitigation during the design stage.","However, threat modeling for systems relying on Artificial Intelligence is still not well explored.","While conventional threat modeling methods and tools did not address AI-related threats, research on this amalgamation still lacks solutions capable of guiding and automating the process, as well as providing evidence that the methods hold up in practice.","To evaluate that the work at hand is able to guide and automatically identify AI-related threats during the architecture definition stage, several experts were tasked to create a threat model of an AI system designed in the healthcare domain.","The usability of the solution was well-perceived, and the results indicate that it is effective for threat identification."],"url":"http://arxiv.org/abs/2403.06512v1","category":"cs.CR"}
{"created":"2024-03-11 08:35:04","title":"Collective nuclear excitation dynamics in mono-modal x-ray waveguides","abstract":"Ensembles of identical atoms exhibit peculiar collective properties in their interaction with radiation depending on geometry and environment where they are embedded in. A remarkably clean and versatile platform to study collective effects in resonant light scattering are M\\\"ossbauer nuclei placed in planar x-ray waveguides. Here we conceive and demonstrate experimentally distinct temporal emission characteristics in these systems, ranging from a tunable accelerated exponential decay all the way to a pronounced oscillatory emission pattern, depending on the waveguide geometry and mode of excitation. The observed temporal and spatial emission characteristics of the collectively excited nuclear state in the waveguide -- the nuclear exciton -- are well reproduced by a unified theoretical model. Our findings pave the way for applications ranging from fundamental studies of cooperative emission at hard x-ray frequencies up to new methods of narrowband x-ray control via the engineering of collective radiation patterns.","sentences":["Ensembles of identical atoms exhibit peculiar collective properties in their interaction with radiation depending on geometry and environment where they are embedded in.","A remarkably clean and versatile platform to study collective effects in resonant light scattering are M\\\"ossbauer nuclei placed in planar x-ray waveguides.","Here we conceive and demonstrate experimentally distinct temporal emission characteristics in these systems, ranging from a tunable accelerated exponential decay all the way to a pronounced oscillatory emission pattern, depending on the waveguide geometry and mode of excitation.","The observed temporal and spatial emission characteristics of the collectively excited nuclear state in the waveguide -- the nuclear exciton -- are well reproduced by a unified theoretical model.","Our findings pave the way for applications ranging from fundamental studies of cooperative emission at hard x-ray frequencies up to new methods of narrowband x-ray control via the engineering of collective radiation patterns."],"url":"http://arxiv.org/abs/2403.06508v1","category":"quant-ph"}
{"created":"2024-03-11 08:25:52","title":"Automatic Generation of Python Programs Using Context-Free Grammars","abstract":"In recent years, data has emerged as the new gold, serving as a powerful tool for creating intelligent systems. However, procuring high-quality data remains challenging, especially for code. To address this, we developed TinyPy Generator, a tool that generates random Python programs using a context-free grammar. The generated programs are guaranteed to be correct by construction. Our system uses custom production rules (in the Backus-Naur Form (BNF) format) to recursively generate code. This allows us to generate code with different levels of complexity, ranging from code containing only assignments to more complex code containing conditionals and loops. Our proposed tool enables effortless large-scale Python code generation, beneficial for a wide range of applications. TinyPy Generator is particularly useful in the field of machine learning, where it can generate substantial amounts of Python code for training Python language models. Additionally, researchers who are studying programming languages can utilize this tool to create datasets for their experiments, which can help validate the robustness of code interpreters or compilers. Unlike existing research, we have open-sourced our implementation. This allows customization according to user needs and extends potential usage to other languages.","sentences":["In recent years, data has emerged as the new gold, serving as a powerful tool for creating intelligent systems.","However, procuring high-quality data remains challenging, especially for code.","To address this, we developed TinyPy","Generator, a tool that generates random Python programs using a context-free grammar.","The generated programs are guaranteed to be correct by construction.","Our system uses custom production rules (in the Backus-Naur Form (BNF) format) to recursively generate code.","This allows us to generate code with different levels of complexity, ranging from code containing only assignments to more complex code containing conditionals and loops.","Our proposed tool enables effortless large-scale Python code generation, beneficial for a wide range of applications.","TinyPy","Generator is particularly useful in the field of machine learning, where it can generate substantial amounts of Python code for training Python language models.","Additionally, researchers who are studying programming languages can utilize this tool to create datasets for their experiments, which can help validate the robustness of code interpreters or compilers.","Unlike existing research, we have open-sourced our implementation.","This allows customization according to user needs and extends potential usage to other languages."],"url":"http://arxiv.org/abs/2403.06503v1","category":"cs.PL"}
{"created":"2024-03-11 07:48:35","title":"Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach","abstract":"Due to the scale and complexity of cloud systems, a system failure would trigger an \"alert storm\", i.e., massive correlated alerts. Although these alerts can be traced back to a few root causes, the overwhelming number makes it infeasible for manual handling. Alert aggregation is thus critical to help engineers concentrate on the root cause and facilitate failure resolution. Existing methods typically utilize semantic similarity-based methods or statistical methods to aggregate alerts. However, semantic similarity-based methods overlook the causal rationale of alerts, while statistical methods can hardly handle infrequent alerts.   To tackle these limitations, we introduce leveraging external knowledge, i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation. The correlation mining module effectively captures the temporal and spatial relations between alerts, measuring their correlations in an efficient manner. Subsequently, only uncertain pairs with low confidence are forwarded to the LLM reasoning module for detailed analysis. This hybrid design harnesses both statistical evidence for frequent alerts and the reasoning capabilities of computationally intensive LLMs, ensuring the overall efficiency of COLA in handling large volumes of alerts in practical scenarios. We evaluate COLA on three datasets collected from the production environment of a large-scale cloud platform. The experimental results show COLA achieves F1-scores from 0.901 to 0.930, outperforming state-of-the-art methods and achieving comparable efficiency. We also share our experience in deploying COLA in our real-world cloud system, Cloud X.","sentences":["Due to the scale and complexity of cloud systems, a system failure would trigger an \"alert storm\", i.e., massive correlated alerts.","Although these alerts can be traced back to a few root causes, the overwhelming number makes it infeasible for manual handling.","Alert aggregation is thus critical to help engineers concentrate on the root cause and facilitate failure resolution.","Existing methods typically utilize semantic similarity-based methods or statistical methods to aggregate alerts.","However, semantic similarity-based methods overlook the causal rationale of alerts, while statistical methods can hardly handle infrequent alerts.   ","To tackle these limitations, we introduce leveraging external knowledge, i.e., Standard Operation Procedure (SOP) of alerts as a supplement.","We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation.","The correlation mining module effectively captures the temporal and spatial relations between alerts, measuring their correlations in an efficient manner.","Subsequently, only uncertain pairs with low confidence are forwarded to the LLM reasoning module for detailed analysis.","This hybrid design harnesses both statistical evidence for frequent alerts and the reasoning capabilities of computationally intensive LLMs, ensuring the overall efficiency of COLA in handling large volumes of alerts in practical scenarios.","We evaluate COLA on three datasets collected from the production environment of a large-scale cloud platform.","The experimental results show COLA achieves F1-scores from 0.901 to 0.930, outperforming state-of-the-art methods and achieving comparable efficiency.","We also share our experience in deploying COLA in our real-world cloud system, Cloud X."],"url":"http://arxiv.org/abs/2403.06485v1","category":"cs.SE"}
{"created":"2024-03-11 07:44:59","title":"The negation of permutation mass function","abstract":"Negation is a important perspective of knowledge representation. Existing negation methods are mainly applied in probability theory, evidence theory and complex evidence theory. As a generalization of evidence theory, random permutation sets theory may represent information more precisely. However, how to apply the concept of negation to random permutation sets theory has not been studied. In this paper, the negation of permutation mass function is proposed. Moreover, in the negation process, the convergence of proposed negation method is verified. The trends of uncertainty and dissimilarity after each negation operation are investigated. Numerical examples are used to demonstrate the rationality of the proposed method.","sentences":["Negation is a important perspective of knowledge representation.","Existing negation methods are mainly applied in probability theory, evidence theory and complex evidence theory.","As a generalization of evidence theory, random permutation sets theory may represent information more precisely.","However, how to apply the concept of negation to random permutation sets theory has not been studied.","In this paper, the negation of permutation mass function is proposed.","Moreover, in the negation process, the convergence of proposed negation method is verified.","The trends of uncertainty and dissimilarity after each negation operation are investigated.","Numerical examples are used to demonstrate the rationality of the proposed method."],"url":"http://arxiv.org/abs/2403.06483v1","category":"cs.AI"}
{"created":"2024-03-11 07:42:40","title":"Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template Matching","abstract":"Soft tissue tracking is crucial for computer-assisted interventions. Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches. However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery. To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template. Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames. To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates. Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-term dynamics modeling by introducing global temporal compensation. We evaluate our approach on the public SurgT benchmark, which is generated from Hamlyn, SCARED, and Kidney boundary datasets. The experimental results show that Ada-Tracker achieves superior accuracy and performs more robustly against prior works. Code is available at https://github.com/wrld/Ada-Tracker.","sentences":["Soft tissue tracking is crucial for computer-assisted interventions.","Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches.","However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery.","To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template.","Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames.","To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates.","Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-term dynamics modeling by introducing global temporal compensation.","We evaluate our approach on the public SurgT benchmark, which is generated from Hamlyn, SCARED, and Kidney boundary datasets.","The experimental results show that Ada-Tracker achieves superior accuracy and performs more robustly against prior works.","Code is available at https://github.com/wrld/Ada-Tracker."],"url":"http://arxiv.org/abs/2403.06479v1","category":"cs.CV"}
{"created":"2024-03-11 07:19:29","title":"Toward Robust Canine Cardiac Diagnosis: Deep Prototype Alignment Network-Based Few-Shot Segmentation in Veterinary Medicine","abstract":"In the cutting-edge domain of medical artificial intelligence (AI), remarkable advances have been achieved in areas such as diagnosis, prediction, and therapeutic interventions. Despite these advances, the technology for image segmentation faces the significant barrier of having to produce extensively annotated datasets. To address this challenge, few-shot segmentation (FSS) has been recognized as one of the innovative solutions. Although most of the FSS research has focused on human health care, its application in veterinary medicine, particularly for pet care, remains largely limited. This study has focused on accurate segmentation of the heart and left atrial enlargement on canine chest radiographs using the proposed deep prototype alignment network (DPANet). The PANet architecture is adopted as the backbone model, and experiments are conducted using various encoders based on VGG-19, ResNet-18, and ResNet-50 to extract features. Experimental results demonstrate that the proposed DPANet achieves the highest performance. In the 2way-1shot scenario, it achieves the highest intersection over union (IoU) value of 0.6966, and in the 2way-5shot scenario, it achieves the highest IoU value of 0.797. The DPANet not only signifies a performance improvement, but also shows an improved training speed in the 2way-5shot scenario. These results highlight our model's exceptional capability as a trailblazing solution for segmenting the heart and left atrial enlargement in veterinary applications through FSS, setting a new benchmark in veterinary AI research, and demonstrating its superior potential to veterinary medicine advances.","sentences":["In the cutting-edge domain of medical artificial intelligence (AI), remarkable advances have been achieved in areas such as diagnosis, prediction, and therapeutic interventions.","Despite these advances, the technology for image segmentation faces the significant barrier of having to produce extensively annotated datasets.","To address this challenge, few-shot segmentation (FSS) has been recognized as one of the innovative solutions.","Although most of the FSS research has focused on human health care, its application in veterinary medicine, particularly for pet care, remains largely limited.","This study has focused on accurate segmentation of the heart and left atrial enlargement on canine chest radiographs using the proposed deep prototype alignment network (DPANet).","The PANet architecture is adopted as the backbone model, and experiments are conducted using various encoders based on VGG-19, ResNet-18, and ResNet-50 to extract features.","Experimental results demonstrate that the proposed DPANet achieves the highest performance.","In the 2way-1shot scenario, it achieves the highest intersection over union (IoU) value of 0.6966, and in the 2way-5shot scenario, it achieves the highest IoU value of 0.797.","The DPANet not only signifies a performance improvement, but also shows an improved training speed in the 2way-5shot scenario.","These results highlight our model's exceptional capability as a trailblazing solution for segmenting the heart and left atrial enlargement in veterinary applications through FSS, setting a new benchmark in veterinary AI research, and demonstrating its superior potential to veterinary medicine advances."],"url":"http://arxiv.org/abs/2403.06471v1","category":"cs.CV"}
{"created":"2024-03-11 07:07:05","title":"RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling Approach","abstract":"Multiple Line Bus Scheduling Problem (MLBSP) is vital to save operational cost of bus company and guarantee service quality for passengers. Existing approaches typically generate a bus scheduling scheme in an offline manner and then schedule buses according to the scheme. In practice, uncertain events such as traffic congestion occur frequently, which may make the pre-determined bus scheduling scheme infeasible. In this paper, MLBSP is modeled as a Markov Decision Process (MDP). A Reinforcement Learning-based Multi-line bus Scheduling Approach (RL-MSA) is proposed for bus scheduling at both the offline and online phases. At the offline phase, deadhead decision is integrated into bus selection decision for the first time to simplify the learning problem. At the online phase, deadhead decision is made through a time window mechanism based on the policy learned at the offline phase. We develop several new and useful state features including the features for control points, bus lines and buses. A bus priority screening mechanism is invented to construct bus-related features. Considering the interests of both the bus company and passengers, a reward function combining the final reward and the step-wise reward is devised. Experiments at the offline phase demonstrate that the number of buses used of RL-MSA is decreased compared with offline optimization approaches. At the online phase, RL-MSA can cover all departure times in a timetable (i.e., service quality) without increasing the number of buses used (i.e., operational cost).","sentences":["Multiple Line Bus Scheduling Problem (MLBSP) is vital to save operational cost of bus company and guarantee service quality for passengers.","Existing approaches typically generate a bus scheduling scheme in an offline manner and then schedule buses according to the scheme.","In practice, uncertain events such as traffic congestion occur frequently, which may make the pre-determined bus scheduling scheme infeasible.","In this paper, MLBSP is modeled as a Markov Decision Process (MDP).","A Reinforcement Learning-based Multi-line bus Scheduling Approach (RL-MSA) is proposed for bus scheduling at both the offline and online phases.","At the offline phase, deadhead decision is integrated into bus selection decision for the first time to simplify the learning problem.","At the online phase, deadhead decision is made through a time window mechanism based on the policy learned at the offline phase.","We develop several new and useful state features including the features for control points, bus lines and buses.","A bus priority screening mechanism is invented to construct bus-related features.","Considering the interests of both the bus company and passengers, a reward function combining the final reward and the step-wise reward is devised.","Experiments at the offline phase demonstrate that the number of buses used of RL-MSA is decreased compared with offline optimization approaches.","At the online phase, RL-MSA can cover all departure times in a timetable (i.e., service quality) without increasing the number of buses used (i.e., operational cost)."],"url":"http://arxiv.org/abs/2403.06466v1","category":"cs.LG"}
{"created":"2024-03-11 07:07:02","title":"RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems","abstract":"This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at \\url{https://github.com/microsoft/RecAI}.","sentences":["This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs).","RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives.","The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences.","We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems.","The source code of RecAI is available at \\url{https://github.com/microsoft/RecAI}."],"url":"http://arxiv.org/abs/2403.06465v1","category":"cs.IR"}
{"created":"2024-03-11 06:54:35","title":"RIS-Enabled Joint Near-Field 3D Localization and Synchronization in SISO Multipath Environments","abstract":"Reconfigurable Intelligent Surfaces (RIS) show great promise in the realm of 6th generation (6G) wireless systems, particularly in the areas of localization and communication. Their cost-effectiveness and energy efficiency enable the integration of numerous passive and reflective elements, enabling near-field propagation. In this paper, we tackle the challenges of RIS-aided 3D localization and synchronization in multipath environments, focusing on the near-field of mmWave systems. Specifically, our approach involves formulating a maximum likelihood (ML) estimation problem for the channel parameters. To initiate this process, we leverage a combination of canonical polyadic decomposition (CPD) and orthogonal matching pursuit (OMP) to obtain coarse estimates of the time of arrival (ToA) and angle of departure (AoD) under the far-field approximation. Subsequently, distances are estimated using $l_{1}$-regularization based on a near-field model. Additionally, we introduce a refinement phase employing the spatial alternating generalized expectation maximization (SAGE) algorithm. Finally, a weighted least squares approach is applied to convert channel parameters into position and clock offset estimates. To extend the estimation algorithm to ultra-large (UL) RIS-assisted localization scenarios, it is further enhanced to reduce errors associated with far-field approximations, especially in the presence of significant near-field effects, achieved by narrowing the RIS aperture. Moreover, the Cram\\'er-Rao Bound (CRB) is derived and the RIS phase shifts are optimized to improve the positioning accuracy. Numerical results affirm the efficacy of the proposed estimation algorithm.","sentences":["Reconfigurable Intelligent Surfaces (RIS) show great promise in the realm of 6th generation (6G) wireless systems, particularly in the areas of localization and communication.","Their cost-effectiveness and energy efficiency enable the integration of numerous passive and reflective elements, enabling near-field propagation.","In this paper, we tackle the challenges of RIS-aided 3D localization and synchronization in multipath environments, focusing on the near-field of mmWave systems.","Specifically, our approach involves formulating a maximum likelihood (ML) estimation problem for the channel parameters.","To initiate this process, we leverage a combination of canonical polyadic decomposition (CPD) and orthogonal matching pursuit (OMP) to obtain coarse estimates of the time of arrival (ToA) and angle of departure (AoD) under the far-field approximation.","Subsequently, distances are estimated using $l_{1}$-regularization based on a near-field model.","Additionally, we introduce a refinement phase employing the spatial alternating generalized expectation maximization (SAGE) algorithm.","Finally, a weighted least squares approach is applied to convert channel parameters into position and clock offset estimates.","To extend the estimation algorithm to ultra-large (UL) RIS-assisted localization scenarios, it is further enhanced to reduce errors associated with far-field approximations, especially in the presence of significant near-field effects, achieved by narrowing the RIS aperture.","Moreover, the Cram\\'er-Rao Bound (CRB) is derived and the RIS phase shifts are optimized to improve the positioning accuracy.","Numerical results affirm the efficacy of the proposed estimation algorithm."],"url":"http://arxiv.org/abs/2403.06460v1","category":"eess.SP"}
{"created":"2024-03-11 06:36:33","title":"Prediction of Wort Density with LSTM Network","abstract":"Many physical target values in technical processes are error-prone, cumbersome, or expensive to measure automatically. One example of a physical target value is the wort density, which is an important value needed for beer production. This article introduces a system that helps the brewer measure wort density through sensors in order to reduce errors in manual data collection. Instead of a direct measurement of wort density, a method is developed that calculates the density from measured values acquired by inexpensive standard sensors such as pressure or temperature. The model behind the calculation is a neural network, known as LSTM.","sentences":["Many physical target values in technical processes are error-prone, cumbersome, or expensive to measure automatically.","One example of a physical target value is the wort density, which is an important value needed for beer production.","This article introduces a system that helps the brewer measure wort density through sensors in order to reduce errors in manual data collection.","Instead of a direct measurement of wort density, a method is developed that calculates the density from measured values acquired by inexpensive standard sensors such as pressure or temperature.","The model behind the calculation is a neural network, known as LSTM."],"url":"http://arxiv.org/abs/2403.06458v1","category":"cs.LG"}
{"created":"2024-03-11 05:51:03","title":"Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models","abstract":"Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.","sentences":["Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate.","This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs.","Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process.","To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations.","Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process.","Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection."],"url":"http://arxiv.org/abs/2403.06448v1","category":"cs.CL"}
{"created":"2024-03-11 05:49:34","title":"CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation","abstract":"The long-tail recommendation is a challenging task for traditional recommender systems, due to data sparsity and data imbalance issues. The recent development of large language models (LLMs) has shown their abilities in complex reasoning, which can help to deduce users' preferences based on very few previous interactions. However, since most LLM-based systems rely on items' semantic meaning as the sole evidence for reasoning, the collaborative information of user-item interactions is neglected, which can cause the LLM's reasoning to be misaligned with task-specific collaborative information of the dataset. To further align LLMs' reasoning to task-specific user-item interaction knowledge, we introduce collaborative retrieval-augmented LLMs, CoRAL, which directly incorporate collaborative evidence into the prompts. Based on the retrieved user-item interactions, the LLM can analyze shared and distinct preferences among users, and summarize the patterns indicating which types of users would be attracted by certain items. The retrieved collaborative evidence prompts the LLM to align its reasoning with the user-item interaction patterns in the dataset. However, since the capacity of the input prompt is limited, finding the minimally-sufficient collaborative information for recommendation tasks can be challenging. We propose to find the optimal interaction set through a sequential decision-making process and develop a retrieval policy learned through a reinforcement learning (RL) framework, CoRAL. Our experimental results show that CoRAL can significantly improve LLMs' reasoning abilities on specific recommendation tasks. Our analysis also reveals that CoRAL can more efficiently explore collaborative information through reinforcement learning.","sentences":["The long-tail recommendation is a challenging task for traditional recommender systems, due to data sparsity and data imbalance issues.","The recent development of large language models (LLMs) has shown their abilities in complex reasoning, which can help to deduce users' preferences based on very few previous interactions.","However, since most LLM-based systems rely on items' semantic meaning as the sole evidence for reasoning, the collaborative information of user-item interactions is neglected, which can cause the LLM's reasoning to be misaligned with task-specific collaborative information of the dataset.","To further align LLMs' reasoning to task-specific user-item interaction knowledge, we introduce collaborative retrieval-augmented LLMs, CoRAL, which directly incorporate collaborative evidence into the prompts.","Based on the retrieved user-item interactions, the LLM can analyze shared and distinct preferences among users, and summarize the patterns indicating which types of users would be attracted by certain items.","The retrieved collaborative evidence prompts the LLM to align its reasoning with the user-item interaction patterns in the dataset.","However, since the capacity of the input prompt is limited, finding the minimally-sufficient collaborative information for recommendation tasks can be challenging.","We propose to find the optimal interaction set through a sequential decision-making process and develop a retrieval policy learned through a reinforcement learning (RL) framework, CoRAL.","Our experimental results show that CoRAL can significantly improve LLMs' reasoning abilities on specific recommendation tasks.","Our analysis also reveals that CoRAL can more efficiently explore collaborative information through reinforcement learning."],"url":"http://arxiv.org/abs/2403.06447v1","category":"cs.IR"}
{"created":"2024-03-11 05:29:46","title":"Temporal-Mapping Photography for Event Cameras","abstract":"Event cameras, or Dynamic Vision Sensors (DVS) are novel neuromorphic sensors that capture brightness changes as a continuous stream of ``events'' rather than traditional intensity frames. Converting sparse events to dense intensity frames faithfully has long been an ill-posed problem. Previous methods have primarily focused on converting events to video in dynamic scenes or with a moving camera. In this paper, for the first time, we realize events to dense intensity image conversion using a stationary event camera in static scenes. Different from traditional methods that mainly rely on event integration, the proposed Event-Based Temporal Mapping Photography (EvTemMap) measures the time of event emitting for each pixel. Then, the resulting Temporal Matrix is converted to an intensity frame with a temporal mapping neural network. At the hardware level, the proposed EvTemMap is implemented by combining a transmittance adjustment device with a DVS, named Adjustable Transmittance Dynamic Vision Sensor. Additionally, we collected TemMat dataset under various conditions including low-light and high dynamic range scenes. The experimental results showcase the high dynamic range, fine-grained details, and high-grayscale-resolution of the proposed EvTemMap, as well as the enhanced performance on downstream computer vision tasks compared to other methods. The code and TemMat dataset will be made publicly available.","sentences":["Event cameras, or Dynamic Vision Sensors (DVS) are novel neuromorphic sensors that capture brightness changes as a continuous stream of ``events'' rather than traditional intensity frames.","Converting sparse events to dense intensity frames faithfully has long been an ill-posed problem.","Previous methods have primarily focused on converting events to video in dynamic scenes or with a moving camera.","In this paper, for the first time, we realize events to dense intensity image conversion using a stationary event camera in static scenes.","Different from traditional methods that mainly rely on event integration, the proposed Event-Based Temporal Mapping Photography (EvTemMap) measures the time of event emitting for each pixel.","Then, the resulting Temporal Matrix is converted to an intensity frame with a temporal mapping neural network.","At the hardware level, the proposed EvTemMap is implemented by combining a transmittance adjustment device with a DVS, named Adjustable Transmittance Dynamic Vision Sensor.","Additionally, we collected TemMat dataset under various conditions including low-light and high dynamic range scenes.","The experimental results showcase the high dynamic range, fine-grained details, and high-grayscale-resolution of the proposed EvTemMap, as well as the enhanced performance on downstream computer vision tasks compared to other methods.","The code and TemMat dataset will be made publicly available."],"url":"http://arxiv.org/abs/2403.06443v1","category":"cs.CV"}
{"created":"2024-03-11 04:58:36","title":"Fine-Grained Pillar Feature Encoding Via Spatio-Temporal Virtual Grid for 3D Object Detection","abstract":"Developing high-performance, real-time architectures for LiDAR-based 3D object detectors is essential for the successful commercialization of autonomous vehicles. Pillar-based methods stand out as a practical choice for onboard deployment due to their computational efficiency. However, despite their efficiency, these methods can sometimes underperform compared to alternative point encoding techniques such as Voxel-encoding or PointNet++. We argue that current pillar-based methods have not sufficiently captured the fine-grained distributions of LiDAR points within each pillar structure. Consequently, there exists considerable room for improvement in pillar feature encoding. In this paper, we introduce a novel pillar encoding architecture referred to as Fine-Grained Pillar Feature Encoding (FG-PFE). FG-PFE utilizes Spatio-Temporal Virtual (STV) grids to capture the distribution of point clouds within each pillar across vertical, temporal, and horizontal dimensions. Through STV grids, points within each pillar are individually encoded using Vertical PFE (V-PFE), Temporal PFE (T-PFE), and Horizontal PFE (H-PFE). These encoded features are then aggregated through an Attentive Pillar Aggregation method. Our experiments conducted on the nuScenes dataset demonstrate that FG-PFE achieves significant performance improvements over baseline models such as PointPillar, CenterPoint-Pillar, and PillarNet, with only a minor increase in computational overhead.","sentences":["Developing high-performance, real-time architectures for LiDAR-based 3D object detectors is essential for the successful commercialization of autonomous vehicles.","Pillar-based methods stand out as a practical choice for onboard deployment due to their computational efficiency.","However, despite their efficiency, these methods can sometimes underperform compared to alternative point encoding techniques such as Voxel-encoding or PointNet++.","We argue that current pillar-based methods have not sufficiently captured the fine-grained distributions of LiDAR points within each pillar structure.","Consequently, there exists considerable room for improvement in pillar feature encoding.","In this paper, we introduce a novel pillar encoding architecture referred to as Fine-Grained Pillar Feature Encoding (FG-PFE).","FG-PFE utilizes Spatio-Temporal Virtual (STV) grids to capture the distribution of point clouds within each pillar across vertical, temporal, and horizontal dimensions.","Through STV grids, points within each pillar are individually encoded using Vertical PFE (V-PFE), Temporal PFE (T-PFE), and Horizontal PFE (H-PFE).","These encoded features are then aggregated through an Attentive Pillar Aggregation method.","Our experiments conducted on the nuScenes dataset demonstrate that FG-PFE achieves significant performance improvements over baseline models such as PointPillar, CenterPoint-Pillar, and PillarNet, with only a minor increase in computational overhead."],"url":"http://arxiv.org/abs/2403.06433v1","category":"cs.CV"}
{"created":"2024-03-11 04:26:18","title":"A Differential Geometric View and Explainability of GNN on Evolving Graphs","abstract":"Graphs are ubiquitous in social networks and biochemistry, where Graph Neural Networks (GNN) are the state-of-the-art models for prediction. Graphs can be evolving and it is vital to formally model and understand how a trained GNN responds to graph evolution. We propose a smooth parameterization of the GNN predicted distributions using axiomatic attribution, where the distributions are on a low-dimensional manifold within a high-dimensional embedding space. We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold. We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation. Extensive experiments on node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better sparsity, faithfulness, and intuitiveness of the proposed method over the state-of-the-art methods.","sentences":["Graphs are ubiquitous in social networks and biochemistry, where Graph Neural Networks (GNN) are the state-of-the-art models for prediction.","Graphs can be evolving and it is vital to formally model and understand how a trained GNN responds to graph evolution.","We propose a smooth parameterization of the GNN predicted distributions using axiomatic attribution, where the distributions are on a low-dimensional manifold within a high-dimensional embedding space.","We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold.","We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation.","Extensive experiments on node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better sparsity, faithfulness, and intuitiveness of the proposed method over the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.06425v1","category":"cs.LG"}
{"created":"2024-03-11 04:16:57","title":"Bounding the Photon Mass with Ultra-Wide Bandwidth Pulsar Timing Data and De-dispersed Pulses of Fast Radio Bursts","abstract":"Exploring the concept of a massive photon has been an important area in astronomy and physics. If photons have mass, their propagation in non-vacuum space would be affected by both the non-zero mass $m_{\\gamma}$ and the presence of a plasma medium. This would lead to a delay time proportional to $m_{\\gamma}^2\\nu^{-4}$, which deviates from the classical dispersion relation (proportional to $\\nu^{-2}$). For the first time, we have derived the dispersion relation of a photon with a non-zero mass propagating in plasma. To reduce the impact of variations in the dispersion measure (DM), we employed the high-precision timing data to constrain the upper bound of the photon mass. Specifically, the DM/time of arrival (TOA) uncertainties derived from ultra-wide bandwidth (UWB) observations conducted by the Parkes Pulsar Timing Array (PPTA) are used. The de-dispersed pulses from fast radio bursts (FRBs) with minimal scattering effects are also used to constrain the upper bound of photon mass. The stringent limit on the photon mass is determined by uncertainties of the TOA of pulsars, with an optimum value of $9.52\\times 10^{-46} \\, \\rm kg \\,\\,(5.34 \\times 10^{-10}\\, \\rm eV/c^2$). In the future, it is essential to investigate the photon mass, as pulsar timing data are collected by PTA and UWB receivers, or FRBs with wide-band spectra are detected by UWB receivers.","sentences":["Exploring the concept of a massive photon has been an important area in astronomy and physics.","If photons have mass, their propagation in non-vacuum space would be affected by both the non-zero mass $m_{\\gamma}$ and the presence of a plasma medium.","This would lead to a delay time proportional to $m_{\\gamma}^2\\nu^{-4}$, which deviates from the classical dispersion relation (proportional to $\\nu^{-2}$).","For the first time, we have derived the dispersion relation of a photon with a non-zero mass propagating in plasma.","To reduce the impact of variations in the dispersion measure (DM), we employed the high-precision timing data to constrain the upper bound of the photon mass.","Specifically, the DM/time of arrival (TOA) uncertainties derived from ultra-wide bandwidth (UWB) observations conducted by the Parkes Pulsar Timing Array (PPTA) are used.","The de-dispersed pulses from fast radio bursts (FRBs) with minimal scattering effects are also used to constrain the upper bound of photon mass.","The stringent limit on the photon mass is determined by uncertainties of the TOA of pulsars, with an optimum value of $9.52\\times 10^{-46} \\, \\rm kg \\,\\,(5.34 \\times 10^{-10}\\, \\rm eV/c^2$).","In the future, it is essential to investigate the photon mass, as pulsar timing data are collected by PTA and UWB receivers, or FRBs with wide-band spectra are detected by UWB receivers."],"url":"http://arxiv.org/abs/2403.06422v1","category":"astro-ph.HE"}
{"created":"2024-03-11 04:13:38","title":"A Comparative Study of Perceptual Quality Metrics for Audio-driven Talking Head Videos","abstract":"The rapid advancement of Artificial Intelligence Generated Content (AIGC) technology has propelled audio-driven talking head generation, gaining considerable research attention for practical applications. However, performance evaluation research lags behind the development of talking head generation techniques. Existing literature relies on heuristic quantitative metrics without human validation, hindering accurate progress assessment. To address this gap, we collect talking head videos generated from four generative methods and conduct controlled psychophysical experiments on visual quality, lip-audio synchronization, and head movement naturalness. Our experiments validate consistency between model predictions and human annotations, identifying metrics that align better with human opinions than widely-used measures. We believe our work will facilitate performance evaluation and model development, providing insights into AIGC in a broader context. Code and data will be made available at https://github.com/zwx8981/ADTH-QA.","sentences":["The rapid advancement of Artificial Intelligence Generated Content (AIGC) technology has propelled audio-driven talking head generation, gaining considerable research attention for practical applications.","However, performance evaluation research lags behind the development of talking head generation techniques.","Existing literature relies on heuristic quantitative metrics without human validation, hindering accurate progress assessment.","To address this gap, we collect talking head videos generated from four generative methods and conduct controlled psychophysical experiments on visual quality, lip-audio synchronization, and head movement naturalness.","Our experiments validate consistency between model predictions and human annotations, identifying metrics that align better with human opinions than widely-used measures.","We believe our work will facilitate performance evaluation and model development, providing insights into AIGC in a broader context.","Code and data will be made available at https://github.com/zwx8981/ADTH-QA."],"url":"http://arxiv.org/abs/2403.06421v1","category":"cs.CV"}
{"created":"2024-03-11 04:13:26","title":"RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models","abstract":"Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations. To this end, we first present how to extract the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated. Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency. We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the LLM-generated controller. RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL. We demonstrated that RLingua can significantly reduce the sample complexity of TD3 in the robot tasks of panda_gym and achieve high success rates in sparsely rewarded robot tasks in RLBench, where the standard TD3 fails. Additionally, We validated RLingua's effectiveness in real-world robot experiments through Sim2Real, demonstrating that the learned policies are effectively transferable to real robot tasks. Further details and videos about our work are available at our project website https://rlingua.github.io.","sentences":["Reinforcement learning (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency.","In this paper, we propose RLingua, a framework that can leverage the internal knowledge of large language models (LLMs) to reduce the sample complexity of RL in robotic manipulations.","To this end, we first present how to extract the prior knowledge of LLMs by prompt engineering so that a preliminary rule-based robot controller for a specific task can be generated.","Despite being imperfect, the LLM-generated robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL's sample efficiency.","We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the LLM-generated controller.","RLingua also provides a novel method of improving the imperfect LLM-generated robot controllers by RL.","We demonstrated that RLingua can significantly reduce the sample complexity of TD3 in the robot tasks of panda_gym and achieve high success rates in sparsely rewarded robot tasks in RLBench, where the standard TD3 fails.","Additionally, We validated RLingua's effectiveness in real-world robot experiments through Sim2Real, demonstrating that the learned policies are effectively transferable to real robot tasks.","Further details and videos about our work are available at our project website https://rlingua.github.io."],"url":"http://arxiv.org/abs/2403.06420v1","category":"cs.RO"}
{"created":"2024-03-11 03:54:33","title":"CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean","abstract":"Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge is required to answer the question correctly. Using CLIcK, we test 13 language models to assess their performance. Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension. CLIcK offers the first large-scale comprehensive Korean-centric analysis of LLMs' proficiency in Korean culture and language.","sentences":["Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge.","Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts.","For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and hate speech detection are offered.","To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.","CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture.","For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge is required to answer the question correctly.","Using CLIcK, we test 13 language models to assess their performance.","Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension.","CLIcK offers the first large-scale comprehensive Korean-centric analysis of LLMs' proficiency in Korean culture and language."],"url":"http://arxiv.org/abs/2403.06412v1","category":"cs.CL"}
{"created":"2024-03-11 03:45:09","title":"A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation","abstract":"Generating coherent and credible explanations remains a significant challenge in the field of AI. In recent years, researchers have delved into the utilization of entailment trees to depict explanations, which exhibit a reasoning process of how a hypothesis is deduced from the supporting facts. However, existing models often overlook the importance of generating intermediate conclusions with logical consistency from the given facts, leading to inaccurate conclusions and undermining the overall credibility of entailment trees. To address this limitation, we propose the logical pattern memory pre-trained model (LMPM). LMPM incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions. Furthermore, to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data, we introduce an entity abstraction approach to construct the dataset for pre-training LMPM. The experimental results highlight the effectiveness of our approach in improving the quality of entailment tree generation. By leveraging logical entailment patterns, our model produces more coherent and reasonable conclusions that closely align with the underlying premises. Code and Data are released at https://github.com/YuanLi95/T5-LMPM","sentences":["Generating coherent and credible explanations remains a significant challenge in the field of AI.","In recent years, researchers have delved into the utilization of entailment trees to depict explanations, which exhibit a reasoning process of how a hypothesis is deduced from the supporting facts.","However, existing models often overlook the importance of generating intermediate conclusions with logical consistency from the given facts, leading to inaccurate conclusions and undermining the overall credibility of entailment trees.","To address this limitation, we propose the logical pattern memory pre-trained model (LMPM).","LMPM incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions.","Furthermore, to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data, we introduce an entity abstraction approach to construct the dataset for pre-training LMPM.","The experimental results highlight the effectiveness of our approach in improving the quality of entailment tree generation.","By leveraging logical entailment patterns, our model produces more coherent and reasonable conclusions that closely align with the underlying premises.","Code and Data are released at https://github.com/YuanLi95/T5-LMPM"],"url":"http://arxiv.org/abs/2403.06410v1","category":"cs.CL"}
{"created":"2024-03-11 03:42:51","title":"What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation","abstract":"Quantization has emerged as a promising technique for improving the memory and computational efficiency of large language models (LLMs). Though the trade-off between performance and efficiency is well-known, there is still much to be learned about the relationship between quantization and LLM performance. To shed light on this relationship, we propose a new perspective on quantization, viewing it as perturbations added to the weights and activations of LLMs. We call this approach \"the lens of perturbation\". Using this lens, we conduct experiments with various artificial perturbations to explore their impact on LLM performance. Our findings reveal several connections between the properties of perturbations and LLM performance, providing insights into the failure cases of uniform quantization and suggesting potential solutions to improve the robustness of LLM quantization. To demonstrate the significance of our findings, we implement a simple non-uniform quantization approach based on our insights. Our experiments show that this approach achieves minimal performance degradation on both 4-bit weight quantization and 8-bit quantization for weights and activations. These results validate the correctness of our approach and highlight its potential to improve the efficiency of LLMs without sacrificing performance.","sentences":["Quantization has emerged as a promising technique for improving the memory and computational efficiency of large language models (LLMs).","Though the trade-off between performance and efficiency is well-known, there is still much to be learned about the relationship between quantization and LLM performance.","To shed light on this relationship, we propose a new perspective on quantization, viewing it as perturbations added to the weights and activations of LLMs.","We call this approach \"the lens of perturbation\".","Using this lens, we conduct experiments with various artificial perturbations to explore their impact on LLM performance.","Our findings reveal several connections between the properties of perturbations and LLM performance, providing insights into the failure cases of uniform quantization and suggesting potential solutions to improve the robustness of LLM quantization.","To demonstrate the significance of our findings, we implement a simple non-uniform quantization approach based on our insights.","Our experiments show that this approach achieves minimal performance degradation on both 4-bit weight quantization and 8-bit quantization for weights and activations.","These results validate the correctness of our approach and highlight its potential to improve the efficiency of LLMs without sacrificing performance."],"url":"http://arxiv.org/abs/2403.06408v1","category":"cs.LG"}
{"created":"2024-03-11 03:19:45","title":"On the Diminishing Returns of Width for Continual Learning","abstract":"While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \\emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.","sentences":["While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \\emph{catastrophic forgetting} when trained on new tasks in sequence.","Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning.","We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN).","Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns.","We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory."],"url":"http://arxiv.org/abs/2403.06398v1","category":"cs.LG"}
{"created":"2024-03-11 03:17:33","title":"DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning","abstract":"Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints. Some recent work has integrated control theory with multi-agent reinforcement learning to address the challenge of ensuring safety. However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments. To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics. Our method applies MARL principles to search for optimal solutions. Through the employment of MPC, the actions of agents can be restricted within safe states concurrently. We demonstrate the effectiveness of our approach using the Safe Multi-agent MuJoCo environment, showcasing significant advancements in addressing safety concerns in MARL.","sentences":["Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints.","Some recent work has integrated control theory with multi-agent reinforcement learning to address the challenge of ensuring safety.","However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments.","To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (DeepSafeMPC).","The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics.","Our method applies MARL principles to search for optimal solutions.","Through the employment of MPC, the actions of agents can be restricted within safe states concurrently.","We demonstrate the effectiveness of our approach using the Safe Multi-agent MuJoCo environment, showcasing significant advancements in addressing safety concerns in MARL."],"url":"http://arxiv.org/abs/2403.06397v1","category":"cs.LG"}
{"created":"2024-03-11 02:45:06","title":"Towards Decoupling Frontend Enhancement and Backend Recognition in Monaural Robust ASR","abstract":"It has been shown that the intelligibility of noisy speech can be improved by speech enhancement (SE) algorithms. However, monaural SE has not been established as an effective frontend for automatic speech recognition (ASR) in noisy conditions compared to an ASR model trained on noisy speech directly. The divide between SE and ASR impedes the progress of robust ASR systems, especially as SE has made major advances in recent years. This paper focuses on eliminating this divide with an ARN (attentive recurrent network) time-domain and a CrossNet time-frequency domain enhancement models. The proposed systems fully decouple frontend enhancement and backend ASR trained only on clean speech. Results on the WSJ, CHiME-2, LibriSpeech, and CHiME-4 corpora demonstrate that ARN and CrossNet enhanced speech both translate to improved ASR results in noisy and reverberant environments, and generalize well to real acoustic scenarios. The proposed system outperforms the baselines trained on corrupted speech directly. Furthermore, it cuts the previous best word error rate (WER) on CHiME-2 by $28.4\\%$ relatively with a $5.57\\%$ WER, and achieves $3.32/4.44\\%$ WER on single-channel CHiME-4 simulated/real test data without training on CHiME-4.","sentences":["It has been shown that the intelligibility of noisy speech can be improved by speech enhancement (SE) algorithms.","However, monaural SE has not been established as an effective frontend for automatic speech recognition (ASR) in noisy conditions compared to an ASR model trained on noisy speech directly.","The divide between SE and ASR impedes the progress of robust ASR systems, especially as SE has made major advances in recent years.","This paper focuses on eliminating this divide with an ARN (attentive recurrent network) time-domain and a CrossNet time-frequency domain enhancement models.","The proposed systems fully decouple frontend enhancement and backend ASR trained only on clean speech.","Results on the WSJ, CHiME-2, LibriSpeech, and CHiME-4 corpora demonstrate that ARN and CrossNet enhanced speech both translate to improved ASR results in noisy and reverberant environments, and generalize well to real acoustic scenarios.","The proposed system outperforms the baselines trained on corrupted speech directly.","Furthermore, it cuts the previous best word error rate (WER) on CHiME-2 by $28.4\\%$ relatively with a $5.57\\%$ WER, and achieves $3.32/4.44\\%$ WER on single-channel CHiME-4 simulated/real test data without training on CHiME-4."],"url":"http://arxiv.org/abs/2403.06387v1","category":"cs.SD"}
{"created":"2024-03-11 02:24:32","title":"Pre-Trained Model Recommendation for Downstream Fine-tuning","abstract":"As a fundamental problem in transfer learning, model selection aims to rank off-the-shelf pre-trained models and select the most suitable one for the new target task. Existing model selection techniques are often constrained in their scope and tend to overlook the nuanced relationships between models and tasks. In this paper, we present a pragmatic framework \\textbf{Fennec}, delving into a diverse, large-scale model repository while meticulously considering the intricate connections between tasks and models. The key insight is to map all models and historical tasks into a transfer-related subspace, where the distance between model vectors and task vectors represents the magnitude of transferability. A large vision model, as a proxy, infers a new task's representation in the transfer space, thereby circumventing the computational burden of extensive forward passes. We also investigate the impact of the inherent inductive bias of models on transfer results and propose a novel method called \\textbf{archi2vec} to encode the intricate structures of models. The transfer score is computed through straightforward vector arithmetic with a time complexity of $\\mathcal{O}(1)$. Finally, we make a substantial contribution to the field by releasing a comprehensive benchmark. We validate the effectiveness of our framework through rigorous testing on two benchmarks. The benchmark and the code will be publicly available in the near future.","sentences":["As a fundamental problem in transfer learning, model selection aims to rank off-the-shelf pre-trained models and select the most suitable one for the new target task.","Existing model selection techniques are often constrained in their scope and tend to overlook the nuanced relationships between models and tasks.","In this paper, we present a pragmatic framework \\textbf{Fennec}, delving into a diverse, large-scale model repository while meticulously considering the intricate connections between tasks and models.","The key insight is to map all models and historical tasks into a transfer-related subspace, where the distance between model vectors and task vectors represents the magnitude of transferability.","A large vision model, as a proxy, infers a new task's representation in the transfer space, thereby circumventing the computational burden of extensive forward passes.","We also investigate the impact of the inherent inductive bias of models on transfer results and propose a novel method called \\textbf{archi2vec} to encode the intricate structures of models.","The transfer score is computed through straightforward vector arithmetic with a time complexity of $\\mathcal{O}(1)$. Finally, we make a substantial contribution to the field by releasing a comprehensive benchmark.","We validate the effectiveness of our framework through rigorous testing on two benchmarks.","The benchmark and the code will be publicly available in the near future."],"url":"http://arxiv.org/abs/2403.06382v1","category":"cs.CV"}
{"created":"2024-03-11 01:18:00","title":"Human and Automatic Interpretation of Romanian Noun Compounds","abstract":"Determining the intended, context-dependent meanings of noun compounds like \"shoe sale\" and \"fire sale\" remains a challenge for NLP. Previous work has relied on inventories of semantic relations that capture the different meanings between compound members. Focusing on Romanian compounds, whose morphosyntax differs from that of their English counterparts, we propose a new set of relations and test it with human annotators and a neural net classifier. Results show an alignment of the network's predictions and human judgments, even where the human agreement rate is low. Agreement tracks with the frequency of the selected relations, regardless of structural differences. However, the most frequently selected relation was none of the sixteen labeled semantic relations, indicating the need for a better relation inventory.","sentences":["Determining the intended, context-dependent meanings of noun compounds like \"shoe sale\" and \"fire sale\" remains a challenge for NLP.","Previous work has relied on inventories of semantic relations that capture the different meanings between compound members.","Focusing on Romanian compounds, whose morphosyntax differs from that of their English counterparts, we propose a new set of relations and test it with human annotators and a neural net classifier.","Results show an alignment of the network's predictions and human judgments, even where the human agreement rate is low.","Agreement tracks with the frequency of the selected relations, regardless of structural differences.","However, the most frequently selected relation was none of the sixteen labeled semantic relations, indicating the need for a better relation inventory."],"url":"http://arxiv.org/abs/2403.06360v1","category":"cs.CL"}
{"created":"2024-03-11 01:11:28","title":"Video Generation with Consistency Tuning","abstract":"Currently, various studies have been exploring generation of long videos. However, the generated frames in these videos often exhibit jitter and noise. Therefore, in order to generate the videos without these noise, we propose a novel framework composed of four modules: separate tuning module, average fusion module, combined tuning module, and inter-frame consistency module. By applying our newly proposed modules subsequently, the consistency of the background and foreground in each video frames is optimized. Besides, the experimental results demonstrate that videos generated by our method exhibit a high quality in comparison of the state-of-the-art methods.","sentences":["Currently, various studies have been exploring generation of long videos.","However, the generated frames in these videos often exhibit jitter and noise.","Therefore, in order to generate the videos without these noise, we propose a novel framework composed of four modules: separate tuning module, average fusion module, combined tuning module, and inter-frame consistency module.","By applying our newly proposed modules subsequently, the consistency of the background and foreground in each video frames is optimized.","Besides, the experimental results demonstrate that videos generated by our method exhibit a high quality in comparison of the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.06356v1","category":"cs.CV"}
{"created":"2024-03-11 01:00:00","title":"Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos","abstract":"We investigate exocentric-to-egocentric cross-view translation, which aims to generate a first-person (egocentric) view of an actor based on a video recording that captures the actor from a third-person (exocentric) perspective. To this end, we propose a generative framework called Exo2Ego that decouples the translation process into two stages: high-level structure transformation, which explicitly encourages cross-view correspondence between exocentric and egocentric views, and a diffusion-based pixel-level hallucination, which incorporates a hand layout prior to enhance the fidelity of the generated egocentric view. To pave the way for future advancements in this field, we curate a comprehensive exo-to-ego cross-view translation benchmark. It consists of a diverse collection of synchronized ego-exo tabletop activity video pairs sourced from three public datasets: H2O, Aria Pilot, and Assembly101. The experimental results validate that Exo2Ego delivers photorealistic video results with clear hand manipulation details and outperforms several baselines in terms of both synthesis quality and generalization ability to new actions.","sentences":["We investigate exocentric-to-egocentric cross-view translation, which aims to generate a first-person (egocentric) view of an actor based on a video recording that captures the actor from a third-person (exocentric) perspective.","To this end, we propose a generative framework called Exo2Ego that decouples the translation process into two stages: high-level structure transformation, which explicitly encourages cross-view correspondence between exocentric and egocentric views, and a diffusion-based pixel-level hallucination, which incorporates a hand layout prior to enhance the fidelity of the generated egocentric view.","To pave the way for future advancements in this field, we curate a comprehensive exo-to-ego cross-view translation benchmark.","It consists of a diverse collection of synchronized ego-exo tabletop activity video pairs sourced from three public datasets: H2O, Aria Pilot, and Assembly101.","The experimental results validate that Exo2Ego delivers photorealistic video results with clear hand manipulation details and outperforms several baselines in terms of both synthesis quality and generalization ability to new actions."],"url":"http://arxiv.org/abs/2403.06351v1","category":"cs.CV"}
{"created":"2024-03-11 00:33:28","title":"MOAB: Multi-Modal Outer Arithmetic Block For Fusion Of Histopathological Images And Genetic Data For Brain Tumor Grading","abstract":"Brain tumors are an abnormal growth of cells in the brain. They can be classified into distinct grades based on their growth. Often grading is performed based on a histological image and is one of the most significant predictors of a patients prognosis, the higher the grade, the more aggressive the tumor. Correct diagnosis of a tumor grade remains challenging. Though histopathological grading has been shown to be prognostic, results are subject to interobserver variability, even among experienced pathologists. Recently, the World Health Organization reported that advances in molecular genetics have led to improvements in tumor classification. This paper seeks to integrate histological images and genetic data for improved computer-aided diagnosis. We propose a novel Multi-modal Outer Arithmetic Block (MOAB) based on arithmetic operations to combine latent representations of the different modalities for predicting the tumor grade (Grade \\rom{2}, \\rom{3} and \\rom{4}). Extensive experiments evaluate the effectiveness of our approach. By applying MOAB to The Cancer Genome Atlas (TCGA) glioma dataset, we show that it can improve separation between similar classes (Grade \\rom{2} and \\rom{3}) and outperform prior state-of-the-art grade classification techniques.","sentences":["Brain tumors are an abnormal growth of cells in the brain.","They can be classified into distinct grades based on their growth.","Often grading is performed based on a histological image and is one of the most significant predictors of a patients prognosis, the higher the grade, the more aggressive the tumor.","Correct diagnosis of a tumor grade remains challenging.","Though histopathological grading has been shown to be prognostic, results are subject to interobserver variability, even among experienced pathologists.","Recently, the World Health Organization reported that advances in molecular genetics have led to improvements in tumor classification.","This paper seeks to integrate histological images and genetic data for improved computer-aided diagnosis.","We propose a novel Multi-modal Outer Arithmetic Block (MOAB) based on arithmetic operations to combine latent representations of the different modalities for predicting the tumor grade (Grade \\rom{2}, \\rom{3} and \\rom{4}).","Extensive experiments evaluate the effectiveness of our approach.","By applying MOAB to The Cancer Genome Atlas (TCGA) glioma dataset, we show that it can improve separation between similar classes (Grade \\rom{2} and \\rom{3}) and outperform prior state-of-the-art grade classification techniques."],"url":"http://arxiv.org/abs/2403.06349v1","category":"cs.CV"}
{"created":"2024-03-10 23:09:42","title":"Sparse Spatial Smoothing: Reduced Complexity and Improved Beamforming Gain via Sparse Sub-Arrays","abstract":"This paper addresses the problem of single snapshot Direction-of-Arrival (DOA) estimation, which is of great importance in a wide-range of applications including automotive radar. A popular approach to achieving high angular resolution when only one temporal snapshot is available is via subspace methods using spatial smoothing. This involves leveraging spatial shift-invariance in the antenna array geometry, typically a uniform linear array (ULA), to rearrange the single snapshot measurement vector into a spatially smoothed matrix that reveals the signal subspace of interest. However, conventional approaches using spatially shifted ULA sub-arrays can lead to a prohibitively high computational complexity due to the large dimensions of the resulting spatially smoothed matrix. Hence, we propose to instead employ judiciously designed sparse sub-arrays, such as nested arrays, to reduce the computational complexity of spatial smoothing while retaining the aperture and identifiability of conventional ULA-based approaches. Interestingly, this idea also suggests a novel beamforming method which linearly combines multiple spatially smoothed matrices corresponding to different sets of shifts of the sparse (nested) sub-array. This so-called shift-domain beamforming method is demonstrated to boost the effective SNR, and thereby resolution, in a desired angular region of interest, enabling single snapshot low-complexity DOA estimation with identifiability guarantees.","sentences":["This paper addresses the problem of single snapshot Direction-of-Arrival (DOA) estimation, which is of great importance in a wide-range of applications including automotive radar.","A popular approach to achieving high angular resolution when only one temporal snapshot is available is via subspace methods using spatial smoothing.","This involves leveraging spatial shift-invariance in the antenna array geometry, typically a uniform linear array (ULA), to rearrange the single snapshot measurement vector into a spatially smoothed matrix that reveals the signal subspace of interest.","However, conventional approaches using spatially shifted ULA sub-arrays can lead to a prohibitively high computational complexity due to the large dimensions of the resulting spatially smoothed matrix.","Hence, we propose to instead employ judiciously designed sparse sub-arrays, such as nested arrays, to reduce the computational complexity of spatial smoothing while retaining the aperture and identifiability of conventional ULA-based approaches.","Interestingly, this idea also suggests a novel beamforming method which linearly combines multiple spatially smoothed matrices corresponding to different sets of shifts of the sparse (nested) sub-array.","This so-called shift-domain beamforming method is demonstrated to boost the effective SNR, and thereby resolution, in a desired angular region of interest, enabling single snapshot low-complexity DOA estimation with identifiability guarantees."],"url":"http://arxiv.org/abs/2403.06337v1","category":"eess.SP"}
{"created":"2024-03-10 22:40:07","title":"Exploiting the Margin: How Capitalism Fuels AI at the Expense of Minoritized Groups","abstract":"This article investigates the complex nexus of capitalism, racial oppression, and artificial intelligence (AI), revealing how these elements coalesce to deepen social inequities. By tracing the historical exploitation of marginalized communities through capitalist practices, the study demonstrates how AI technologies not only reflect but also amplify societal biases, particularly in exacerbating racial disparities. Through a focused analysis, the paper presents how AI's development and application exploit marginalized groups via mechanisms such as gig economy labor abuses, biased facial recognition technologies, and the disproportionate mental health burdens placed on these communities. These examples underscore the critical role of AI in reinforcing and intensifying existing inequalities. Concluding that unregulated AI significantly threatens to compound current oppressions, the article calls for a concerted effort towards responsible AI development. This entails adopting a holistic approach that rectifies systemic flaws and champions the empowerment of marginalized individuals, ensuring that technological advancement contributes to societal healing rather than perpetuating cycles of exploitation.","sentences":["This article investigates the complex nexus of capitalism, racial oppression, and artificial intelligence (AI), revealing how these elements coalesce to deepen social inequities.","By tracing the historical exploitation of marginalized communities through capitalist practices, the study demonstrates how AI technologies not only reflect but also amplify societal biases, particularly in exacerbating racial disparities.","Through a focused analysis, the paper presents how AI's development and application exploit marginalized groups via mechanisms such as gig economy labor abuses, biased facial recognition technologies, and the disproportionate mental health burdens placed on these communities.","These examples underscore the critical role of AI in reinforcing and intensifying existing inequalities.","Concluding that unregulated AI significantly threatens to compound current oppressions, the article calls for a concerted effort towards responsible AI development.","This entails adopting a holistic approach that rectifies systemic flaws and champions the empowerment of marginalized individuals, ensuring that technological advancement contributes to societal healing rather than perpetuating cycles of exploitation."],"url":"http://arxiv.org/abs/2403.06332v1","category":"cs.CY"}
{"created":"2024-03-10 22:27:21","title":"Transferable Reinforcement Learning via Generalized Occupancy Models","abstract":"Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without having to redo policy optimization. By directly modeling long-term outcomes, GOMs avoid compounding error while retaining generality across arbitrary reward functions. We provide a practical instantiation of GOMs using diffusion models and show its efficacy as a new class of transferable models, both theoretically and empirically across a variety of simulated robotics problems. Videos and code at https://weirdlabuw.github.io/gom/.","sentences":["Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks.","Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards.","However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces.","In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error.","The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state.","These models can then quickly be used to select the optimal action for arbitrary new tasks, without having to redo policy optimization.","By directly modeling long-term outcomes, GOMs avoid compounding error while retaining generality across arbitrary reward functions.","We provide a practical instantiation of GOMs using diffusion models and show its efficacy as a new class of transferable models, both theoretically and empirically across a variety of simulated robotics problems.","Videos and code at https://weirdlabuw.github.io/gom/."],"url":"http://arxiv.org/abs/2403.06328v1","category":"cs.LG"}
{"created":"2024-03-10 22:14:54","title":"From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification","abstract":"User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints. We observe that user instructions typically contain constraints. While assessing response quality in terms of the whole instruction is often costly, efficiently evaluating the satisfaction rate of constraints is feasible. We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints. Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response. It samples multiple responses for each prompt and collect preference labels based on their CSR automatically. Subsequently, ACT adapts the LM to the target task through a ranking-based learning process. Experiments on fine-grained entity typing, abstractive summarization, and temporal question answering show that ACT is able to enhance LMs' capability to adhere to different classes of constraints, thereby improving task performance. Further experiments show that the constraint-following capabilities are transferable.","sentences":["User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints.","We observe that user instructions typically contain constraints.","While assessing response quality in terms of the whole instruction is often costly, efficiently evaluating the satisfaction rate of constraints is feasible.","We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints.","Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response.","It samples multiple responses for each prompt and collect preference labels based on their CSR automatically.","Subsequently, ACT adapts the LM to the target task through a ranking-based learning process.","Experiments on fine-grained entity typing, abstractive summarization, and temporal question answering show that ACT is able to enhance LMs' capability to adhere to different classes of constraints, thereby improving task performance.","Further experiments show that the constraint-following capabilities are transferable."],"url":"http://arxiv.org/abs/2403.06326v1","category":"cs.CL"}
{"created":"2024-03-10 21:43:47","title":"Leveraging Computer Vision in the Intensive Care Unit (ICU) for Examining Visitation and Mobility","abstract":"Despite the importance of closely monitoring patients in the Intensive Care Unit (ICU), many aspects are still assessed in a limited manner due to the time constraints imposed on healthcare providers. For example, although excessive visitations during rest hours can potentially exacerbate the risk of circadian rhythm disruption and delirium, it is not captured in the ICU. Likewise, while mobility can be an important indicator of recovery or deterioration in ICU patients, it is only captured sporadically or not captured at all. In the past few years, the computer vision field has found application in many domains by reducing the human burden. Using computer vision systems in the ICU can also potentially enable non-existing assessments or enhance the frequency and accuracy of existing assessments while reducing the staff workload. In this study, we leverage a state-of-the-art noninvasive computer vision system based on depth imaging to characterize ICU visitations and patients' mobility. We then examine the relationship between visitation and several patient outcomes, such as pain, acuity, and delirium. We found an association between deteriorating patient acuity and the incidence of delirium with increased visitations. In contrast, self-reported pain, reported using the Defense and Veteran Pain Rating Scale (DVPRS), was correlated with decreased visitations. Our findings highlight the feasibility and potential of using noninvasive autonomous systems to monitor ICU patients.","sentences":["Despite the importance of closely monitoring patients in the Intensive Care Unit (ICU), many aspects are still assessed in a limited manner due to the time constraints imposed on healthcare providers.","For example, although excessive visitations during rest hours can potentially exacerbate the risk of circadian rhythm disruption and delirium, it is not captured in the ICU.","Likewise, while mobility can be an important indicator of recovery or deterioration in ICU patients, it is only captured sporadically or not captured at all.","In the past few years, the computer vision field has found application in many domains by reducing the human burden.","Using computer vision systems in the ICU can also potentially enable non-existing assessments or enhance the frequency and accuracy of existing assessments while reducing the staff workload.","In this study, we leverage a state-of-the-art noninvasive computer vision system based on depth imaging to characterize ICU visitations and patients' mobility.","We then examine the relationship between visitation and several patient outcomes, such as pain, acuity, and delirium.","We found an association between deteriorating patient acuity and the incidence of delirium with increased visitations.","In contrast, self-reported pain, reported using the Defense and Veteran Pain Rating Scale (DVPRS), was correlated with decreased visitations.","Our findings highlight the feasibility and potential of using noninvasive autonomous systems to monitor ICU patients."],"url":"http://arxiv.org/abs/2403.06322v1","category":"cs.CV"}
{"created":"2024-03-10 21:33:53","title":"An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation","abstract":"Generative modelling for shapes is a prerequisite for In-Silico Clinical Trials (ISCTs), which aim to cost-effectively validate medical device interventions using synthetic anatomical shapes, often represented as 3D surface meshes. However, constructing AI models to generate shapes closely resembling the real mesh samples is challenging due to variable vertex counts, connectivities, and the lack of dense vertex-wise correspondences across the training data. Employing graph representations for meshes, we develop a novel unsupervised geometric deep-learning model to establish refinable shape correspondences in a latent space, construct a population-derived atlas and generate realistic synthetic shapes. We additionally extend our proposed base model to a joint shape generative-clustering multi-atlas framework to incorporate further variability and preserve more details in the generated shapes. Experimental results using liver and left-ventricular models demonstrate the approach's applicability to computational medicine, highlighting its suitability for ISCTs through a comparative analysis.","sentences":["Generative modelling for shapes is a prerequisite for In-Silico Clinical Trials (ISCTs), which aim to cost-effectively validate medical device interventions using synthetic anatomical shapes, often represented as 3D surface meshes.","However, constructing AI models to generate shapes closely resembling the real mesh samples is challenging due to variable vertex counts, connectivities, and the lack of dense vertex-wise correspondences across the training data.","Employing graph representations for meshes, we develop a novel unsupervised geometric deep-learning model to establish refinable shape correspondences in a latent space, construct a population-derived atlas and generate realistic synthetic shapes.","We additionally extend our proposed base model to a joint shape generative-clustering multi-atlas framework to incorporate further variability and preserve more details in the generated shapes.","Experimental results using liver and left-ventricular models demonstrate the approach's applicability to computational medicine, highlighting its suitability for ISCTs through a comparative analysis."],"url":"http://arxiv.org/abs/2403.06317v1","category":"cs.CV"}
{"created":"2024-03-10 21:30:22","title":"A Study on Domain Generalization for Failure Detection through Human Reactions in HRI","abstract":"Machine learning models are commonly tested in-distribution (same dataset); performance almost always drops in out-of-distribution settings. For HRI research, the goal is often to develop generalized models. This makes domain generalization - retaining performance in different settings - a critical issue. In this study, we present a concise analysis of domain generalization in failure detection models trained on human facial expressions. Using two distinct datasets of humans reacting to videos where error occurs, one from a controlled lab setting and another collected online, we trained deep learning models on each dataset. When testing these models on the alternate dataset, we observed a significant performance drop. We reflect on the causes for the observed model behavior and leave recommendations. This work emphasizes the need for HRI research focusing on improving model robustness and real-life applicability.","sentences":["Machine learning models are commonly tested in-distribution (same dataset); performance almost always drops in out-of-distribution settings.","For HRI research, the goal is often to develop generalized models.","This makes domain generalization - retaining performance in different settings - a critical issue.","In this study, we present a concise analysis of domain generalization in failure detection models trained on human facial expressions.","Using two distinct datasets of humans reacting to videos where error occurs, one from a controlled lab setting and another collected online, we trained deep learning models on each dataset.","When testing these models on the alternate dataset, we observed a significant performance drop.","We reflect on the causes for the observed model behavior and leave recommendations.","This work emphasizes the need for HRI research focusing on improving model robustness and real-life applicability."],"url":"http://arxiv.org/abs/2403.06315v1","category":"cs.RO"}
{"created":"2024-03-10 21:18:54","title":"Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning","abstract":"Deep reinforcement learning(DRL) has shown significant promise in a wide range of applications including computer games and robotics. Yet, training DRL policies consume extraordinary computing resources resulting in dense policies which are prone to overfitting. Moreover, inference with dense DRL policies limit their practical applications, especially in edge computing. Techniques such as pruning and singular value decomposition have been used with deep learning models to achieve sparsification and model compression to limit overfitting and reduce memory consumption. However, these techniques resulted in sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$ regularization techniques have been proposed for neural network sparsification and sparse auto-encoder development, but their implementation in DRL environments has not been apparent. We propose a novel $L_0$-norm-regularization technique using an optimal sparsity map to sparsify DRL policies and promote their decomposition to a lower rank without decay in rewards. We evaluated our $L_0$-norm-regularization technique across five different environments (Cartpole-v1, Acrobat-v1, LunarLander-v2, SuperMarioBros-7.1.v0 and Surgical Robot Learning) using several on-policy and off-policy algorithms. We demonstrated that the $L_0$-norm-regularized DRL policy in the SuperMarioBros environment achieved 93% sparsity and gained 70% compression when subjected to low-rank decomposition, while significantly outperforming the dense policy. Additionally, the $L_0$-norm-regularized DRL policy in the Surgical Robot Learning environment achieved a 36% sparsification and gained 46% compression when decomposed to a lower rank, while being performant. The results suggest that our custom $L_0$-norm-regularization technique for sparsification of DRL policies is a promising avenue to reduce computational resources and limit overfitting.","sentences":["Deep reinforcement learning(DRL) has shown significant promise in a wide range of applications including computer games and robotics.","Yet, training DRL policies consume extraordinary computing resources resulting in dense policies which are prone to overfitting.","Moreover, inference with dense DRL policies limit their practical applications, especially in edge computing.","Techniques such as pruning and singular value decomposition have been used with deep learning models to achieve sparsification and model compression to limit overfitting and reduce memory consumption.","However, these techniques resulted in sub-optimal performance with notable decay in rewards.","$L_1$ and $L_2$ regularization techniques have been proposed for neural network sparsification and sparse auto-encoder development, but their implementation in DRL environments has not been apparent.","We propose a novel $L_0$-norm-regularization technique using an optimal sparsity map to sparsify DRL policies and promote their decomposition to a lower rank without decay in rewards.","We evaluated our $L_0$-norm-regularization technique across five different environments (Cartpole-v1, Acrobat-v1, LunarLander-v2, SuperMarioBros-7.1.v0 and Surgical Robot Learning) using several on-policy and off-policy algorithms.","We demonstrated that the $L_0$-norm-regularized DRL policy in the SuperMarioBros environment achieved 93% sparsity and gained 70% compression when subjected to low-rank decomposition, while significantly outperforming the dense policy.","Additionally, the $L_0$-norm-regularized DRL policy in the Surgical Robot Learning environment achieved a 36% sparsification and gained 46% compression when decomposed to a lower rank, while being performant.","The results suggest that our custom $L_0$-norm-regularization technique for sparsification of DRL policies is a promising avenue to reduce computational resources and limit overfitting."],"url":"http://arxiv.org/abs/2403.06313v1","category":"cs.LG"}
{"created":"2024-03-10 20:51:36","title":"Spectral Diffusion Posterior Sampling for Synergistic Reconstruction in Spectral Computed Tomography","abstract":"Using recent advances in generative artificial intelligence (AI) brought by diffusion models, this paper introduces a new synergistic method for spectral computed tomography (CT) reconstruction. Diffusion models define a neural network to approximate the gradient of the log-density of the training data, which is then used to generate new images similar to the training ones. Following the inverse problem paradigm, we propose to adapt this generative process to synergistically reconstruct multiple images at different energy bins from multiple measurements. The experiments suggest that using multiple energy bins simultaneously improves the reconstruction by inverse diffusion and outperforms state-of-the-art synergistic reconstruction techniques.","sentences":["Using recent advances in generative artificial intelligence (AI) brought by diffusion models, this paper introduces a new synergistic method for spectral computed tomography (CT) reconstruction.","Diffusion models define a neural network to approximate the gradient of the log-density of the training data, which is then used to generate new images similar to the training ones.","Following the inverse problem paradigm, we propose to adapt this generative process to synergistically reconstruct multiple images at different energy bins from multiple measurements.","The experiments suggest that using multiple energy bins simultaneously improves the reconstruction by inverse diffusion and outperforms state-of-the-art synergistic reconstruction techniques."],"url":"http://arxiv.org/abs/2403.06308v1","category":"physics.med-ph"}
{"created":"2024-03-10 19:47:00","title":"ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes","abstract":"There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Decision (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, Reasoner(a symbolic solver) identify a series of rational and coherent arguments to support decision. ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner. The setup experiments show that ArgMed-Agents not only improves accuracy in complex clinical decision reasoning problems compared to other prompt methods, but more importantly, it provides users with decision explanations that increase their confidence.","sentences":["There are two main barriers to using large language models (LLMs) in clinical reasoning.","Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations.","Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes.","This leads to user distrust.","In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction.","ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Decision (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships.","Ultimately, Reasoner(a symbolic solver) identify a series of rational and coherent arguments to support decision.","ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner.","The setup experiments show that ArgMed-Agents not only improves accuracy in complex clinical decision reasoning problems compared to other prompt methods, but more importantly, it provides users with decision explanations that increase their confidence."],"url":"http://arxiv.org/abs/2403.06294v1","category":"cs.AI"}
{"created":"2024-03-10 19:05:12","title":"Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning","abstract":"Human-annotated vision datasets inevitably contain a fraction of human mislabelled examples. While the detrimental effects of such mislabelling on supervised learning are well-researched, their influence on Supervised Contrastive Learning (SCL) remains largely unexplored. In this paper, we show that human-labelling errors not only differ significantly from synthetic label errors, but also pose unique challenges in SCL, different to those in traditional supervised learning methods. Specifically, our results indicate they adversely impact the learning process in the ~99% of cases when they occur as false positive samples. Existing noise-mitigating methods primarily focus on synthetic label errors and tackle the unrealistic setting of very high synthetic noise rates (40-80%), but they often underperform on common image datasets due to overfitting. To address this issue, we introduce a novel SCL objective with robustness to human-labelling errors, SCL-RHE. SCL-RHE is designed to mitigate the effects of real-world mislabelled examples, typically characterized by much lower noise rates (<5%). We demonstrate that SCL-RHE consistently outperforms state-of-the-art representation learning and noise-mitigating methods across various vision benchmarks, by offering improved resilience against human-labelling errors.","sentences":["Human-annotated vision datasets inevitably contain a fraction of human mislabelled examples.","While the detrimental effects of such mislabelling on supervised learning are well-researched, their influence on Supervised Contrastive Learning (SCL) remains largely unexplored.","In this paper, we show that human-labelling errors not only differ significantly from synthetic label errors, but also pose unique challenges in SCL, different to those in traditional supervised learning methods.","Specifically, our results indicate they adversely impact the learning process in the ~99% of cases when they occur as false positive samples.","Existing noise-mitigating methods primarily focus on synthetic label errors and tackle the unrealistic setting of very high synthetic noise rates (40-80%), but they often underperform on common image datasets due to overfitting.","To address this issue, we introduce a novel SCL objective with robustness to human-labelling errors, SCL-RHE.","SCL-RHE is designed to mitigate the effects of real-world mislabelled examples, typically characterized by much lower noise rates (<5%).","We demonstrate that SCL-RHE consistently outperforms state-of-the-art representation learning and noise-mitigating methods across various vision benchmarks, by offering improved resilience against human-labelling errors."],"url":"http://arxiv.org/abs/2403.06289v1","category":"cs.CV"}
{"created":"2024-03-10 18:39:49","title":"Developing an AI-Based Psychometric System for Assessing Learning Difficulties and Adaptive System to Overcome: A Qualitative and Conceptual Framework","abstract":"Learning difficulties pose significant challenges for students, impacting their academic performance and overall educational experience. These difficulties could sometimes put students into a downward spiral that lack of educational resources for personalized support consistently led to under-accommodation of students special needs, and the student lose opportunities in the longer term academic and work development. This research aims to propose a conceptual framework for an adaptive AI-based virtual tutor system that incorporates psychometric assessment to support students with learning difficulties. This process involves the careful selection and integration of validated current mature psychometric scales that assess key dimensions of learning, such as cognitive abilities, learning styles, and academic skills. By incorporating scales that specifically assess these difficulties, the psychometric test will provide a comprehensive understanding of each students unique learning profile and inform targeted interventions within the adaptive tutoring system. The paper also proposes using autoencoders to identify the latent patterns to generate the students profile vector for collection of psychometric data, defining state space and action space representing the students desired combination of images, sound and text engagements, employing extended Bayesian knowledge tracing and hierarchical model and Metropolis-Hastings to continuously estimate and monitor the students performance in various psychometric constructs. The proposed system will leverage the capabilities of LLMs, visual generation models, and psychometric assessments to provide personalized instruction and support tailored to each students unique learning characteristics and needs.","sentences":["Learning difficulties pose significant challenges for students, impacting their academic performance and overall educational experience.","These difficulties could sometimes put students into a downward spiral that lack of educational resources for personalized support consistently led to under-accommodation of students special needs, and the student lose opportunities in the longer term academic and work development.","This research aims to propose a conceptual framework for an adaptive AI-based virtual tutor system that incorporates psychometric assessment to support students with learning difficulties.","This process involves the careful selection and integration of validated current mature psychometric scales that assess key dimensions of learning, such as cognitive abilities, learning styles, and academic skills.","By incorporating scales that specifically assess these difficulties, the psychometric test will provide a comprehensive understanding of each students unique learning profile and inform targeted interventions within the adaptive tutoring system.","The paper also proposes using autoencoders to identify the latent patterns to generate the students profile vector for collection of psychometric data, defining state space and action space representing the students desired combination of images, sound and text engagements, employing extended Bayesian knowledge tracing and hierarchical model and Metropolis-Hastings to continuously estimate and monitor the students performance in various psychometric constructs.","The proposed system will leverage the capabilities of LLMs, visual generation models, and psychometric assessments to provide personalized instruction and support tailored to each students unique learning characteristics and needs."],"url":"http://arxiv.org/abs/2403.06284v1","category":"cs.HC"}
{"created":"2024-03-10 18:19:40","title":"Refinement of MMIO Models for Improving the Coverage of Firmware Fuzzing","abstract":"Embedded systems (ESes) are now ubiquitous, collecting sensitive user data and helping the users make safety-critical decisions. Their vulnerability may thus pose a grave threat to the security and privacy of billions of ES users. Grey-box fuzzing is widely used for testing ES firmware. It usually runs the firmware in a fully emulated environment for efficient testing. In such a setting, the fuzzer cannot access peripheral hardware and hence must model the firmware's interactions with peripherals to achieve decent code coverage. The state-of-the-art (SOTA) firmware fuzzers focus on modeling the memory-mapped I/O (MMIO) of peripherals.   We find that SOTA MMIO models for firmware fuzzing do not describe the MMIO reads well for retrieving a data chunk, leaving ample room for improvement of code coverage. Thus, we propose ES-Fuzz that boosts the code coverage by refining the MMIO models in use. ES-Fuzz uses a given firmware fuzzer to generate stateless and fixed MMIO models besides test cases after testing an ES firmware. ES-Fuzz then instruments a given test harness, runs it with the highest-coverage test case, and gets the execution trace. The trace guides ES-Fuzz to build stateful and adaptable MMIO models. The given fuzzer thereafter tests the firmware with the newly-built models. The alternation between the fuzzer and ES-Fuzz iteratively enhances the coverage of fuzz-testing. We have implemented ES-Fuzz upon Fuzzware and evaluated it with 21 popular ES firmware. ES-Fuzz boosts Fuzzware's coverage by up to $160\\%$ in some of these firmware without lowering the coverage in the others much.","sentences":["Embedded systems (ESes) are now ubiquitous, collecting sensitive user data and helping the users make safety-critical decisions.","Their vulnerability may thus pose a grave threat to the security and privacy of billions of ES users.","Grey-box fuzzing is widely used for testing ES firmware.","It usually runs the firmware in a fully emulated environment for efficient testing.","In such a setting, the fuzzer cannot access peripheral hardware and hence must model the firmware's interactions with peripherals to achieve decent code coverage.","The state-of-the-art (SOTA) firmware fuzzers focus on modeling the memory-mapped I/O (MMIO) of peripherals.   ","We find that SOTA MMIO models for firmware fuzzing do not describe the MMIO reads well for retrieving a data chunk, leaving ample room for improvement of code coverage.","Thus, we propose ES-Fuzz that boosts the code coverage by refining the MMIO models in use.","ES-Fuzz uses a given firmware fuzzer to generate stateless and fixed MMIO models besides test cases after testing an ES firmware.","ES-Fuzz then instruments a given test harness, runs it with the highest-coverage test case, and gets the execution trace.","The trace guides ES-Fuzz to build stateful and adaptable MMIO models.","The given fuzzer thereafter tests the firmware with the newly-built models.","The alternation between the fuzzer and ES-Fuzz iteratively enhances the coverage of fuzz-testing.","We have implemented ES-Fuzz upon Fuzzware and evaluated it with 21 popular ES firmware.","ES-Fuzz boosts Fuzzware's coverage by up to $160\\%$ in some of these firmware without lowering the coverage in the others much."],"url":"http://arxiv.org/abs/2403.06281v1","category":"cs.CR"}
{"created":"2024-03-10 18:05:41","title":"UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation","abstract":"Nakagami imaging holds promise for visualizing and quantifying tissue scattering in ultrasound waves, with potential applications in tumor diagnosis and fat fraction estimation which are challenging to discern by conventional ultrasound B-mode images. Existing methods struggle with optimal window size selection and suffer from estimator instability, leading to degraded resolution images. To address this, here we propose a novel method called UNICORN (Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an accurate, closed-form estimator for Nakagami parameter estimation in terms of the score function of ultrasonic envelope. Extensive experiments using simulation and real ultrasound RF data demonstrate UNICORN's superiority over conventional approaches in accuracy and resolution quality.","sentences":["Nakagami imaging holds promise for visualizing and quantifying tissue scattering in ultrasound waves, with potential applications in tumor diagnosis and fat fraction estimation which are challenging to discern by conventional ultrasound B-mode images.","Existing methods struggle with optimal window size selection and suffer from estimator instability, leading to degraded resolution images.","To address this, here we propose a novel method called UNICORN (Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an accurate, closed-form estimator for Nakagami parameter estimation in terms of the score function of ultrasonic envelope.","Extensive experiments using simulation and real ultrasound RF data demonstrate UNICORN's superiority over conventional approaches in accuracy and resolution quality."],"url":"http://arxiv.org/abs/2403.06275v1","category":"cs.CV"}
{"created":"2024-03-10 17:13:01","title":"Dimension-free matricial Nullstellens\u00e4tze for noncommutative polynomials","abstract":"Hilbert's Nullstellensatz is one of the most fundamental correspondences between algebra and geometry, and has inspired a plethora of noncommutative analogs. In last two decades, there has been an increased interest in understanding vanishing sets of polynomials in several matrix variables without restricting the matrix size, prompted by developments in noncommutative function theory, control systems, operator algebras, and quantum information theory. The emerging results vary according to the interpretation of what vanishing means. For example, given a collection of noncommutative polynomials, one can consider all matrix tuples at which the values of these polynomials are all zero, singular, have common kernel, or have zero trace. This survey reviews Nullstellens\\\"atze for the above types of vanishing sets, and identifies their structural counterparts in the free algebra.","sentences":["Hilbert's Nullstellensatz is one of the most fundamental correspondences between algebra and geometry, and has inspired a plethora of noncommutative analogs.","In last two decades, there has been an increased interest in understanding vanishing sets of polynomials in several matrix variables without restricting the matrix size, prompted by developments in noncommutative function theory, control systems, operator algebras, and quantum information theory.","The emerging results vary according to the interpretation of what vanishing means.","For example, given a collection of noncommutative polynomials, one can consider all matrix tuples at which the values of these polynomials are all zero, singular, have common kernel, or have zero trace.","This survey reviews Nullstellens\\\"atze for the above types of vanishing sets, and identifies their structural counterparts in the free algebra."],"url":"http://arxiv.org/abs/2403.06270v1","category":"math.RA"}
{"created":"2024-03-10 17:07:28","title":"Physics-Guided Abnormal Trajectory Gap Detection","abstract":"Given trajectories with gaps (i.e., missing data), we investigate algorithms to identify abnormal gaps in trajectories which occur when a given moving object did not report its location, but other moving objects in the same geographic region periodically did. The problem is important due to its societal applications, such as improving maritime safety and regulatory enforcement for global security concerns such as illegal fishing, illegal oil transfers, and trans-shipments. The problem is challenging due to the difficulty of bounding the possible locations of the moving object during a trajectory gap, and the very high computational cost of detecting gaps in such a large volume of location data. The current literature on anomalous trajectory detection assumes linear interpolation within gaps, which may not be able to detect abnormal gaps since objects within a given region may have traveled away from their shortest path. In preliminary work, we introduced an abnormal gap measure that uses a classical space-time prism model to bound an object's possible movement during the trajectory gap and provided a scalable memoized gap detection algorithm (Memo-AGD). In this paper, we propose a Space Time-Aware Gap Detection (STAGD) approach to leverage space-time indexing and merging of trajectory gaps. We also incorporate a Dynamic Region Merge-based (DRM) approach to efficiently compute gap abnormality scores. We provide theoretical proofs that both algorithms are correct and complete and also provide analysis of asymptotic time complexity. Experimental results on synthetic and real-world maritime trajectory data show that the proposed approach substantially improves computation time over the baseline technique.","sentences":["Given trajectories with gaps (i.e., missing data), we investigate algorithms to identify abnormal gaps in trajectories which occur when a given moving object did not report its location, but other moving objects in the same geographic region periodically did.","The problem is important due to its societal applications, such as improving maritime safety and regulatory enforcement for global security concerns such as illegal fishing, illegal oil transfers, and trans-shipments.","The problem is challenging due to the difficulty of bounding the possible locations of the moving object during a trajectory gap, and the very high computational cost of detecting gaps in such a large volume of location data.","The current literature on anomalous trajectory detection assumes linear interpolation within gaps, which may not be able to detect abnormal gaps since objects within a given region may have traveled away from their shortest path.","In preliminary work, we introduced an abnormal gap measure that uses a classical space-time prism model to bound an object's possible movement during the trajectory gap and provided a scalable memoized gap detection algorithm (Memo-AGD).","In this paper, we propose a Space Time-Aware Gap Detection (STAGD) approach to leverage space-time indexing and merging of trajectory gaps.","We also incorporate a Dynamic Region Merge-based (DRM) approach to efficiently compute gap abnormality scores.","We provide theoretical proofs that both algorithms are correct and complete and also provide analysis of asymptotic time complexity.","Experimental results on synthetic and real-world maritime trajectory data show that the proposed approach substantially improves computation time over the baseline technique."],"url":"http://arxiv.org/abs/2403.06268v1","category":"cs.CV"}
{"created":"2024-03-10 17:07:20","title":"FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System to Assist Human Labelers' Preference Elicitation","abstract":"Preference-based learning aims to align robot task objectives with human values. One of the most common methods to infer human preferences is by pairwise comparisons of robot task trajectories. Traditional comparison-based preference labeling systems seldom support labelers to digest and identify critical differences between complex trajectories recorded in videos. Our formative study (N = 12) suggests that individuals may overlook non-salient task features and establish biased preference criteria during their preference elicitation process because of partial observations. In addition, they may experience mental fatigue when given many pairs to compare, causing their label quality to deteriorate. To mitigate these issues, we propose FARPLS, a Feature-Augmented Robot trajectory Preference Labeling System. FARPLS highlights potential outliers in a wide variety of task features that matter to humans and extracts the corresponding video keyframes for easy review and comparison. It also dynamically adjusts the labeling order according to users' familiarities, difficulties of the trajectory pair, and level of disagreements. At the same time, the system monitors labelers' consistency and provides feedback on labeling progress to keep labelers engaged. A between-subjects study (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows that FARPLS can help users establish preference criteria more easily and notice more relevant details in the presented trajectories than the conventional interface. FARPLS also improves labeling consistency and engagement, mitigating challenges in preference elicitation without raising cognitive loads significantly","sentences":["Preference-based learning aims to align robot task objectives with human values.","One of the most common methods to infer human preferences is by pairwise comparisons of robot task trajectories.","Traditional comparison-based preference labeling systems seldom support labelers to digest and identify critical differences between complex trajectories recorded in videos.","Our formative study (N = 12) suggests that individuals may overlook non-salient task features and establish biased preference criteria during their preference elicitation process because of partial observations.","In addition, they may experience mental fatigue when given many pairs to compare, causing their label quality to deteriorate.","To mitigate these issues, we propose FARPLS, a Feature-Augmented Robot trajectory Preference Labeling System.","FARPLS highlights potential outliers in a wide variety of task features that matter to humans and extracts the corresponding video keyframes for easy review and comparison.","It also dynamically adjusts the labeling order according to users' familiarities, difficulties of the trajectory pair, and level of disagreements.","At the same time, the system monitors labelers' consistency and provides feedback on labeling progress to keep labelers engaged.","A between-subjects study (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows that FARPLS can help users establish preference criteria more easily and notice more relevant details in the presented trajectories than the conventional interface.","FARPLS also improves labeling consistency and engagement, mitigating challenges in preference elicitation without raising cognitive loads significantly"],"url":"http://arxiv.org/abs/2403.06267v1","category":"cs.HC"}
{"created":"2024-03-10 17:02:53","title":"Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance","abstract":"Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokenization quality. These correlations are more pronounced for generation tasks (over classification) or for smaller models (over large ones). We replicated a representative part of our experiments on Turkish and found similar results, confirming that our results hold for languages with typological characteristics dissimilar to English. We conclude that building better compressing tokenizers is a fruitful avenue for further research and for improving overall model performance.","sentences":["Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear.","In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens.","We also demonstrate the empirical importance of compression for downstream success of pre-trained language models.","We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all.","We then pre-train English language models based on those tokenizers and fine-tune them over several tasks.","We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokenization quality.","These correlations are more pronounced for generation tasks (over classification) or for smaller models (over large ones).","We replicated a representative part of our experiments on Turkish and found similar results, confirming that our results hold for languages with typological characteristics dissimilar to English.","We conclude that building better compressing tokenizers is a fruitful avenue for further research and for improving overall model performance."],"url":"http://arxiv.org/abs/2403.06265v1","category":"cs.CL"}
{"created":"2024-03-10 16:57:10","title":"Editing Conceptual Knowledge for Large Language Models","abstract":"Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.","sentences":["Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs).","Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear.","This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation.","The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance.","We anticipate this can inspire further progress in better understanding LLMs.","Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit."],"url":"http://arxiv.org/abs/2403.06259v1","category":"cs.CL"}
{"created":"2024-03-10 16:36:43","title":"Demystifying Tacit Knowledge in Graphic Design: Characteristics, Instances, Approaches, and Guidelines","abstract":"Despite the growing demand for professional graphic design knowledge, the tacit nature of design inhibits knowledge sharing. However, there is a limited understanding on the characteristics and instances of tacit knowledge in graphic design. In this work, we build a comprehensive set of tacit knowledge characteristics through a literature review. Through interviews with 10 professional graphic designers, we collected 123 tacit knowledge instances and labeled their characteristics. By qualitatively coding the instances, we identified the prominent elements, actions, and purposes of tacit knowledge. To identify which instances have been addressed the least, we conducted a systematic literature review of prior system support to graphic design. By understanding the reasons for the lack of support on these instances based on their characteristics, we propose design guidelines for capturing and applying tacit knowledge in design tools. This work takes a step towards understanding tacit knowledge, and how this knowledge can be communicated.","sentences":["Despite the growing demand for professional graphic design knowledge, the tacit nature of design inhibits knowledge sharing.","However, there is a limited understanding on the characteristics and instances of tacit knowledge in graphic design.","In this work, we build a comprehensive set of tacit knowledge characteristics through a literature review.","Through interviews with 10 professional graphic designers, we collected 123 tacit knowledge instances and labeled their characteristics.","By qualitatively coding the instances, we identified the prominent elements, actions, and purposes of tacit knowledge.","To identify which instances have been addressed the least, we conducted a systematic literature review of prior system support to graphic design.","By understanding the reasons for the lack of support on these instances based on their characteristics, we propose design guidelines for capturing and applying tacit knowledge in design tools.","This work takes a step towards understanding tacit knowledge, and how this knowledge can be communicated."],"url":"http://arxiv.org/abs/2403.06252v1","category":"cs.HC"}
{"created":"2024-03-10 16:11:17","title":"Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation","abstract":"We propose a text-guided variational image generation method to address the challenge of getting clean data for anomaly detection in industrial manufacturing. Our method utilizes text information about the target object, learned from extensive text library documents, to generate non-defective data images resembling the input image. The proposed framework ensures that the generated non-defective images align with anticipated distributions derived from textual and image-based knowledge, ensuring stability and generality. Experimental results demonstrate the effectiveness of our approach, surpassing previous methods even with limited non-defective data. Our approach is validated through generalization tests across four baseline models and three distinct datasets. We present an additional analysis to enhance the effectiveness of anomaly detection models by utilizing the generated images.","sentences":["We propose a text-guided variational image generation method to address the challenge of getting clean data for anomaly detection in industrial manufacturing.","Our method utilizes text information about the target object, learned from extensive text library documents, to generate non-defective data images resembling the input image.","The proposed framework ensures that the generated non-defective images align with anticipated distributions derived from textual and image-based knowledge, ensuring stability and generality.","Experimental results demonstrate the effectiveness of our approach, surpassing previous methods even with limited non-defective data.","Our approach is validated through generalization tests across four baseline models and three distinct datasets.","We present an additional analysis to enhance the effectiveness of anomaly detection models by utilizing the generated images."],"url":"http://arxiv.org/abs/2403.06247v1","category":"cs.CV"}
{"created":"2024-03-10 16:08:35","title":"Estimating Factor-Based Spot Volatility Matrices with Noisy and Asynchronous High-Frequency Data","abstract":"We propose a new estimator of high-dimensional spot volatility matrices satisfying a low-rank plus sparse structure from noisy and asynchronous high-frequency data collected for an ultra-large number of assets. The noise processes are allowed to be temporally correlated, heteroskedastic, asymptotically vanishing and dependent on the efficient prices. We define a kernel-weighted pre-averaging method to jointly tackle the microstructure noise and asynchronicity issues, and we obtain uniformly consistent estimates for latent prices. We impose a continuous-time factor model with time-varying factor loadings on the price processes, and estimate the common factors and loadings via a local principal component analysis. Assuming a uniform sparsity condition on the idiosyncratic volatility structure, we combine the POET and kernel-smoothing techniques to estimate the spot volatility matrices for both the latent prices and idiosyncratic errors. Under some mild restrictions, the estimated spot volatility matrices are shown to be uniformly consistent under various matrix norms. We provide Monte-Carlo simulation and empirical studies to examine the numerical performance of the developed estimation methodology.","sentences":["We propose a new estimator of high-dimensional spot volatility matrices satisfying a low-rank plus sparse structure from noisy and asynchronous high-frequency data collected for an ultra-large number of assets.","The noise processes are allowed to be temporally correlated, heteroskedastic, asymptotically vanishing and dependent on the efficient prices.","We define a kernel-weighted pre-averaging method to jointly tackle the microstructure noise and asynchronicity issues, and we obtain uniformly consistent estimates for latent prices.","We impose a continuous-time factor model with time-varying factor loadings on the price processes, and estimate the common factors and loadings via a local principal component analysis.","Assuming a uniform sparsity condition on the idiosyncratic volatility structure, we combine the POET and kernel-smoothing techniques to estimate the spot volatility matrices for both the latent prices and idiosyncratic errors.","Under some mild restrictions, the estimated spot volatility matrices are shown to be uniformly consistent under various matrix norms.","We provide Monte-Carlo simulation and empirical studies to examine the numerical performance of the developed estimation methodology."],"url":"http://arxiv.org/abs/2403.06246v1","category":"econ.EM"}
{"created":"2024-03-10 16:01:26","title":"Strong Lensing by Galaxy Clusters","abstract":"Galaxy clusters as gravitational lenses play a unique role in astrophysics and cosmology: they permit mapping the dark matter distribution on a range of scales; they reveal the properties of high and intermediate redshift background galaxies that would otherwise be unreachable with telescopes; they constrain the particle nature of dark matter and are a powerful probe of global cosmological parameters, like the Hubble constant. In this review we summarize the current status of cluster lensing observations and the insights they provide, and offer a glimpse into the capabilities that ongoing, and the upcoming next generation of telescopes and surveys will deliver. While many open questions remain, cluster lensing promises to remain at the forefront of discoveries in astrophysics and cosmology.","sentences":["Galaxy clusters as gravitational lenses play a unique role in astrophysics and cosmology: they permit mapping the dark matter distribution on a range of scales; they reveal the properties of high and intermediate redshift background galaxies that would otherwise be unreachable with telescopes; they constrain the particle nature of dark matter and are a powerful probe of global cosmological parameters, like the Hubble constant.","In this review we summarize the current status of cluster lensing observations and the insights they provide, and offer a glimpse into the capabilities that ongoing, and the upcoming next generation of telescopes and surveys will deliver.","While many open questions remain, cluster lensing promises to remain at the forefront of discoveries in astrophysics and cosmology."],"url":"http://arxiv.org/abs/2403.06245v1","category":"astro-ph.CO"}
{"created":"2024-03-10 15:38:20","title":"Cooperative Classification and Rationalization for Graph Generalization","abstract":"Graph Neural Networks (GNNs) have achieved impressive results in graph classification tasks, but they struggle to generalize effectively when faced with out-of-distribution (OOD) data. Several approaches have been proposed to address this problem. Among them, one solution is to diversify training distributions in vanilla classification by modifying the data environment, yet accessing the environment information is complex. Besides, another promising approach involves rationalization, extracting invariant rationales for predictions. However, extracting rationales is difficult due to limited learning signals, resulting in less accurate rationales and diminished predictions. To address these challenges, in this paper, we propose a Cooperative Classification and Rationalization (C2R) method, consisting of the classification and the rationalization module. Specifically, we first assume that multiple environments are available in the classification module. Then, we introduce diverse training distributions using an environment-conditional generative network, enabling robust graph representations. Meanwhile, the rationalization module employs a separator to identify relevant rationale subgraphs while the remaining non-rationale subgraphs are de-correlated with labels. Next, we align graph representations from the classification module with rationale subgraph representations using the knowledge distillation methods, enhancing the learning signal for rationales. Finally, we infer multiple environments by gathering non-rationale representations and incorporate them into the classification module for cooperative learning. Extensive experimental results on both benchmarks and synthetic datasets demonstrate the effectiveness of C2R. Code is available at https://github.com/yuelinan/Codes-of-C2R.","sentences":["Graph Neural Networks (GNNs) have achieved impressive results in graph classification tasks, but they struggle to generalize effectively when faced with out-of-distribution (OOD) data.","Several approaches have been proposed to address this problem.","Among them, one solution is to diversify training distributions in vanilla classification by modifying the data environment, yet accessing the environment information is complex.","Besides, another promising approach involves rationalization, extracting invariant rationales for predictions.","However, extracting rationales is difficult due to limited learning signals, resulting in less accurate rationales and diminished predictions.","To address these challenges, in this paper, we propose a Cooperative Classification and Rationalization (C2R) method, consisting of the classification and the rationalization module.","Specifically, we first assume that multiple environments are available in the classification module.","Then, we introduce diverse training distributions using an environment-conditional generative network, enabling robust graph representations.","Meanwhile, the rationalization module employs a separator to identify relevant rationale subgraphs while the remaining non-rationale subgraphs are de-correlated with labels.","Next, we align graph representations from the classification module with rationale subgraph representations using the knowledge distillation methods, enhancing the learning signal for rationales.","Finally, we infer multiple environments by gathering non-rationale representations and incorporate them into the classification module for cooperative learning.","Extensive experimental results on both benchmarks and synthetic datasets demonstrate the effectiveness of C2R. Code is available at https://github.com/yuelinan/Codes-of-C2R."],"url":"http://arxiv.org/abs/2403.06239v1","category":"cs.LG"}
{"created":"2024-03-10 15:25:49","title":"Probabilistic Neural Circuits","abstract":"Probabilistic circuits (PCs) have gained prominence in recent years as a versatile framework for discussing probabilistic models that support tractable queries and are yet expressive enough to model complex probability distributions. Nevertheless, tractability comes at a cost: PCs are less expressive than neural networks. In this paper we introduce probabilistic neural circuits (PNCs), which strike a balance between PCs and neural nets in terms of tractability and expressive power. Theoretically, we show that PNCs can be interpreted as deep mixtures of Bayesian networks. Experimentally, we demonstrate that PNCs constitute powerful function approximators.","sentences":["Probabilistic circuits (PCs) have gained prominence in recent years as a versatile framework for discussing probabilistic models that support tractable queries and are yet expressive enough to model complex probability distributions.","Nevertheless, tractability comes at a cost: PCs are less expressive than neural networks.","In this paper we introduce probabilistic neural circuits (PNCs), which strike a balance between PCs and neural nets in terms of tractability and expressive power.","Theoretically, we show that PNCs can be interpreted as deep mixtures of Bayesian networks.","Experimentally, we demonstrate that PNCs constitute powerful function approximators."],"url":"http://arxiv.org/abs/2403.06235v1","category":"cs.LG"}
{"created":"2024-03-10 14:11:25","title":"MoST: Motion Style Transformer between Diverse Action Contents","abstract":"While existing motion style transfer methods are effective between two motions with identical content, their performance significantly diminishes when transferring style between motions with different contents. This challenge lies in the lack of clear separation between content and style of a motion. To tackle this challenge, we propose a novel motion style transformer that effectively disentangles style from content and generates a plausible motion with transferred style from a source motion. Our distinctive approach to achieving the goal of disentanglement is twofold: (1) a new architecture for motion style transformer with 'part-attentive style modulator across body parts' and 'Siamese encoders that encode style and content features separately'; (2) style disentanglement loss. Our method outperforms existing methods and demonstrates exceptionally high quality, particularly in motion pairs with different contents, without the need for heuristic post-processing. Codes are available at https://github.com/Boeun-Kim/MoST.","sentences":["While existing motion style transfer methods are effective between two motions with identical content, their performance significantly diminishes when transferring style between motions with different contents.","This challenge lies in the lack of clear separation between content and style of a motion.","To tackle this challenge, we propose a novel motion style transformer that effectively disentangles style from content and generates a plausible motion with transferred style from a source motion.","Our distinctive approach to achieving the goal of disentanglement is twofold: (1) a new architecture for motion style transformer with 'part-attentive style modulator across body parts' and 'Siamese encoders that encode style and content features separately'; (2) style disentanglement loss.","Our method outperforms existing methods and demonstrates exceptionally high quality, particularly in motion pairs with different contents, without the need for heuristic post-processing.","Codes are available at https://github.com/Boeun-Kim/MoST."],"url":"http://arxiv.org/abs/2403.06225v1","category":"cs.CV"}
{"created":"2024-03-10 14:07:46","title":"IDEAS: Information-Driven EV Admission in Charging Station Considering User Impatience to Improve QoS and Station Utilization","abstract":"Our work delves into user behaviour at Electric Vehicle(EV) charging stations during peak times, particularly focusing on how impatience drives balking (not joining queues) and reneging (leaving queues prematurely). We introduce an Agent-based simulation framework that incorporates user optimism levels (pessimistic, standard, and optimistic) in the queue dynamics. Unlike previous work, this framework highlights the crucial role of human behaviour in shaping station efficiency for peak demand. The simulation reveals a key issue: balking often occurs due to a lack of queue insights, creating user dilemmas. To address this, we propose real-time sharing of wait time metrics with arriving EV users at the station. This ensures better Quality of Service (QoS) with user-informed queue joining and demonstrates significant reductions in reneging (up to 94%) improving the charging operation. Further analysis shows that charging speed decreases significantly beyond 80%, but most users prioritize full charges due to range anxiety, leading to a longer queue. To address this, we propose a two-mode, two-port charger design with power-sharing options. This allows users to fast-charge to 80% and automatically switch to slow charging, enabling fast charging on the second port. Thus, increasing fast charger availability and throughput by up to 5%. As the mobility sector transitions towards intelligent traffic, our modelling framework, which integrates human decision-making within automated planning, provides valuable insights for optimizing charging station efficiency and improving the user experience. This approach is particularly relevant during the introduction phase of new stations, when historical data might be limited.","sentences":["Our work delves into user behaviour at Electric Vehicle(EV) charging stations during peak times, particularly focusing on how impatience drives balking (not joining queues) and reneging (leaving queues prematurely).","We introduce an Agent-based simulation framework that incorporates user optimism levels (pessimistic, standard, and optimistic) in the queue dynamics.","Unlike previous work, this framework highlights the crucial role of human behaviour in shaping station efficiency for peak demand.","The simulation reveals a key issue: balking often occurs due to a lack of queue insights, creating user dilemmas.","To address this, we propose real-time sharing of wait time metrics with arriving EV users at the station.","This ensures better Quality of Service (QoS) with user-informed queue joining and demonstrates significant reductions in reneging (up to 94%) improving the charging operation.","Further analysis shows that charging speed decreases significantly beyond 80%, but most users prioritize full charges due to range anxiety, leading to a longer queue.","To address this, we propose a two-mode, two-port charger design with power-sharing options.","This allows users to fast-charge to 80% and automatically switch to slow charging, enabling fast charging on the second port.","Thus, increasing fast charger availability and throughput by up to 5%.","As the mobility sector transitions towards intelligent traffic, our modelling framework, which integrates human decision-making within automated planning, provides valuable insights for optimizing charging station efficiency and improving the user experience.","This approach is particularly relevant during the introduction phase of new stations, when historical data might be limited."],"url":"http://arxiv.org/abs/2403.06223v1","category":"cs.MA"}
{"created":"2024-03-10 13:58:38","title":"TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision","abstract":"Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration selection via thought matching, leading to more helpful demonstrations and less irrelevant input noise. Then, TRAD introduces Aligned Decision, complementing retrieved demonstration steps with their previous or subsequent steps, which enables tolerance for imperfect thought and provides a choice for balance between more context and less noise. Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not only outperforms state-of-the-art models but also effectively helps in reducing noise and promoting generalization. Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and improves the success rate of robotic process automation.","sentences":["Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability.","Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples.","Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks.","However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context.","In this paper, we propose a novel framework (TRAD) to address these issues.","TRAD first conducts Thought Retrieval, achieving step-level demonstration selection via thought matching, leading to more helpful demonstrations and less irrelevant input noise.","Then, TRAD introduces Aligned Decision, complementing retrieved demonstration steps with their previous or subsequent steps, which enables tolerance for imperfect thought and provides a choice for balance between more context and less noise.","Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not only outperforms state-of-the-art models but also effectively helps in reducing noise and promoting generalization.","Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and improves the success rate of robotic process automation."],"url":"http://arxiv.org/abs/2403.06221v1","category":"cs.AI"}
{"created":"2024-03-10 13:26:24","title":"$V_kD:$ Improving Knowledge Distillation using Orthogonal Projections","abstract":"Knowledge distillation is an effective method for training small and efficient deep learning models. However, the efficacy of a single method can degenerate when transferring to other tasks, modalities, or even other architectures. To address this limitation, we propose a novel constrained feature distillation method. This method is derived from a small set of core principles, which results in two emerging components: an orthogonal projection and a task-specific normalisation. Equipped with both of these components, our transformer models can outperform all previous methods on ImageNet and reach up to a 4.4% relative improvement over the previous state-of-the-art methods. To further demonstrate the generality of our method, we apply it to object detection and image generation, whereby we obtain consistent and substantial performance improvements over state-of-the-art. Code and models are publicly available: https://github.com/roymiles/vkd","sentences":["Knowledge distillation is an effective method for training small and efficient deep learning models.","However, the efficacy of a single method can degenerate when transferring to other tasks, modalities, or even other architectures.","To address this limitation, we propose a novel constrained feature distillation method.","This method is derived from a small set of core principles, which results in two emerging components: an orthogonal projection and a task-specific normalisation.","Equipped with both of these components, our transformer models can outperform all previous methods on ImageNet and reach up to a 4.4% relative improvement over the previous state-of-the-art methods.","To further demonstrate the generality of our method, we apply it to object detection and image generation, whereby we obtain consistent and substantial performance improvements over state-of-the-art.","Code and models are publicly available: https://github.com/roymiles/vkd"],"url":"http://arxiv.org/abs/2403.06213v1","category":"cs.CV"}
{"created":"2024-03-10 13:04:09","title":"Limit of the Maximum Random Permutation Set Entropy","abstract":"The Random Permutation Set (RPS) is a new type of set proposed recently, which can be regarded as the generalization of evidence theory. To measure the uncertainty of RPS, the entropy of RPS and its corresponding maximum entropy have been proposed. Exploring the maximum entropy provides a possible way of understanding the physical meaning of RPS. In this paper, a new concept, the envelope of entropy function, is defined. In addition, the limit of the envelope of RPS entropy is derived and proved. Compared with the existing method, the computational complexity of the proposed method to calculate the envelope of RPS entropy decreases greatly. The result shows that when $N \\to \\infty$, the limit form of the envelope of the entropy of RPS converges to $e \\times (N!)^2$, which is highly connected to the constant $e$ and factorial. Finally, numerical examples validate the efficiency and conciseness of the proposed envelope, which provides a new insight into the maximum entropy function.","sentences":["The Random Permutation Set (RPS) is a new type of set proposed recently, which can be regarded as the generalization of evidence theory.","To measure the uncertainty of RPS, the entropy of RPS and its corresponding maximum entropy have been proposed.","Exploring the maximum entropy provides a possible way of understanding the physical meaning of RPS.","In this paper, a new concept, the envelope of entropy function, is defined.","In addition, the limit of the envelope of RPS entropy is derived and proved.","Compared with the existing method, the computational complexity of the proposed method to calculate the envelope of RPS entropy decreases greatly.","The result shows that when $N \\to \\infty$, the limit form of the envelope of the entropy of RPS converges to $e \\times (N!)^2$, which is highly connected to the constant $e$ and factorial.","Finally, numerical examples validate the efficiency and conciseness of the proposed envelope, which provides a new insight into the maximum entropy function."],"url":"http://arxiv.org/abs/2403.06206v1","category":"cs.IT"}
{"created":"2024-03-10 13:04:01","title":"S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes","abstract":"Current 3D stylization methods often assume static scenes, which violates the dynamic nature of our real world. To address this limitation, we present S-DyRF, a reference-based spatio-temporal stylization method for dynamic neural radiance fields. However, stylizing dynamic 3D scenes is inherently challenging due to the limited availability of stylized reference images along the temporal axis. Our key insight lies in introducing additional temporal cues besides the provided reference. To this end, we generate temporal pseudo-references from the given stylized reference. These pseudo-references facilitate the propagation of style information from the reference to the entire dynamic 3D scene. For coarse style transfer, we enforce novel views and times to mimic the style details present in pseudo-references at the feature level. To preserve high-frequency details, we create a collection of stylized temporal pseudo-rays from temporal pseudo-references. These pseudo-rays serve as detailed and explicit stylization guidance for achieving fine style transfer. Experiments on both synthetic and real-world datasets demonstrate that our method yields plausible stylized results of space-time view synthesis on dynamic 3D scenes.","sentences":["Current 3D stylization methods often assume static scenes, which violates the dynamic nature of our real world.","To address this limitation, we present S-DyRF, a reference-based spatio-temporal stylization method for dynamic neural radiance fields.","However, stylizing dynamic 3D scenes is inherently challenging due to the limited availability of stylized reference images along the temporal axis.","Our key insight lies in introducing additional temporal cues besides the provided reference.","To this end, we generate temporal pseudo-references from the given stylized reference.","These pseudo-references facilitate the propagation of style information from the reference to the entire dynamic 3D scene.","For coarse style transfer, we enforce novel views and times to mimic the style details present in pseudo-references at the feature level.","To preserve high-frequency details, we create a collection of stylized temporal pseudo-rays from temporal pseudo-references.","These pseudo-rays serve as detailed and explicit stylization guidance for achieving fine style transfer.","Experiments on both synthetic and real-world datasets demonstrate that our method yields plausible stylized results of space-time view synthesis on dynamic 3D scenes."],"url":"http://arxiv.org/abs/2403.06205v1","category":"cs.CV"}
{"created":"2024-03-10 12:50:35","title":"Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!","abstract":"There is a burgeoning discussion around the capabilities of Large Language Models (LLMs) in acting as fundamental components that can be seamlessly incorporated into Artificial Intelligence of Things (AIoT) to interpret complex trajectories. This study introduces LLMTrack, a model that illustrates how LLMs can be leveraged for Zero-Shot Trajectory Recognition by employing a novel single-prompt technique that combines role-play and think step-by-step methodologies with unprocessed Inertial Measurement Unit (IMU) data. We evaluate the model using real-world datasets designed to challenge it with distinct trajectories characterized by indoor and outdoor scenarios. In both test scenarios, LLMTrack not only meets but exceeds the performance benchmarks set by traditional machine learning approaches and even contemporary state-of-the-art deep learning models, all without the requirement of training on specialized datasets. The results of our research suggest that, with strategically designed prompts, LLMs can tap into their extensive knowledge base and are well-equipped to analyze raw sensor data with remarkable effectiveness.","sentences":["There is a burgeoning discussion around the capabilities of Large Language Models (LLMs) in acting as fundamental components that can be seamlessly incorporated into Artificial Intelligence of Things (AIoT) to interpret complex trajectories.","This study introduces LLMTrack, a model that illustrates how LLMs can be leveraged for Zero-Shot Trajectory Recognition by employing a novel single-prompt technique that combines role-play and think step-by-step methodologies with unprocessed Inertial Measurement Unit (IMU) data.","We evaluate the model using real-world datasets designed to challenge it with distinct trajectories characterized by indoor and outdoor scenarios.","In both test scenarios, LLMTrack not only meets but exceeds the performance benchmarks set by traditional machine learning approaches and even contemporary state-of-the-art deep learning models, all without the requirement of training on specialized datasets.","The results of our research suggest that, with strategically designed prompts, LLMs can tap into their extensive knowledge base and are well-equipped to analyze raw sensor data with remarkable effectiveness."],"url":"http://arxiv.org/abs/2403.06201v1","category":"cs.CL"}
{"created":"2024-03-10 10:59:22","title":"Domain Adversarial Active Learning for Domain Generalization Classification","abstract":"Domain generalization models aim to learn cross-domain knowledge from source domain data, to improve performance on unknown target domains. Recent research has demonstrated that diverse and rich source domain samples can enhance domain generalization capability. This paper argues that the impact of each sample on the model's generalization ability varies. Despite its small scale, a high-quality dataset can still attain a certain level of generalization ability. Motivated by this, we propose a domain-adversarial active learning (DAAL) algorithm for classification tasks in domain generalization. First, we analyze that the objective of tasks is to maximize the inter-class distance within the same domain and minimize the intra-class distance across different domains. To achieve this objective, we design a domain adversarial selection method that prioritizes challenging samples. Second, we posit that even in a converged model, there are subsets of features that lack discriminatory power within each domain. We attempt to identify these feature subsets and optimize them by a constraint loss. We validate and analyze our DAAL algorithm on multiple domain generalization datasets, comparing it with various domain generalization algorithms and active learning algorithms. Our results demonstrate that the DAAL algorithm can achieve strong generalization ability with fewer data resources, thereby reducing data annotation costs in domain generalization tasks.","sentences":["Domain generalization models aim to learn cross-domain knowledge from source domain data, to improve performance on unknown target domains.","Recent research has demonstrated that diverse and rich source domain samples can enhance domain generalization capability.","This paper argues that the impact of each sample on the model's generalization ability varies.","Despite its small scale, a high-quality dataset can still attain a certain level of generalization ability.","Motivated by this, we propose a domain-adversarial active learning (DAAL) algorithm for classification tasks in domain generalization.","First, we analyze that the objective of tasks is to maximize the inter-class distance within the same domain and minimize the intra-class distance across different domains.","To achieve this objective, we design a domain adversarial selection method that prioritizes challenging samples.","Second, we posit that even in a converged model, there are subsets of features that lack discriminatory power within each domain.","We attempt to identify these feature subsets and optimize them by a constraint loss.","We validate and analyze our DAAL algorithm on multiple domain generalization datasets, comparing it with various domain generalization algorithms and active learning algorithms.","Our results demonstrate that the DAAL algorithm can achieve strong generalization ability with fewer data resources, thereby reducing data annotation costs in domain generalization tasks."],"url":"http://arxiv.org/abs/2403.06174v1","category":"cs.LG"}
{"created":"2024-03-10 10:39:32","title":"DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation","abstract":"Due to the difficulty and labor-consuming nature of getting highly accurate or matting annotations, there only exists a limited amount of highly accurate labels available to the public. To tackle this challenge, we propose a DiffuMatting which inherits the strong Everything generation ability of diffusion and endows the power of \"matting anything\". Our DiffuMatting can 1). act as an anything matting factory with high accurate annotations 2). be well-compatible with community LoRAs or various conditional control approaches to achieve the community-friendly art design and controllable generation. Specifically, inspired by green-screen-matting, we aim to teach the diffusion model to paint on a fixed green screen canvas. To this end, a large-scale greenscreen dataset (Green100K) is collected as a training dataset for DiffuMatting. Secondly, a green background control loss is proposed to keep the drawing board as a pure green color to distinguish the foreground and background. To ensure the synthesized object has more edge details, a detailed-enhancement of transition boundary loss is proposed as a guideline to generate objects with more complicated edge structures. Aiming to simultaneously generate the object and its matting annotation, we build a matting head to make a green color removal in the latent space of the VAE decoder. Our DiffuMatting shows several potential applications (e.g., matting-data generator, community-friendly art design and controllable generation). As a matting-data generator, DiffuMatting synthesizes general object and portrait matting sets, effectively reducing the relative MSE error by 15.4% in General Object Matting and 11.4% in Portrait Matting tasks.","sentences":["Due to the difficulty and labor-consuming nature of getting highly accurate or matting annotations, there only exists a limited amount of highly accurate labels available to the public.","To tackle this challenge, we propose a DiffuMatting which inherits the strong Everything generation ability of diffusion and endows the power of \"matting anything\".","Our DiffuMatting can 1).","act as an anything matting factory with high accurate annotations 2).","be well-compatible with community LoRAs or various conditional control approaches to achieve the community-friendly art design and controllable generation.","Specifically, inspired by green-screen-matting, we aim to teach the diffusion model to paint on a fixed green screen canvas.","To this end, a large-scale greenscreen dataset (Green100K) is collected as a training dataset for DiffuMatting.","Secondly, a green background control loss is proposed to keep the drawing board as a pure green color to distinguish the foreground and background.","To ensure the synthesized object has more edge details, a detailed-enhancement of transition boundary loss is proposed as a guideline to generate objects with more complicated edge structures.","Aiming to simultaneously generate the object and its matting annotation, we build a matting head to make a green color removal in the latent space of the VAE decoder.","Our DiffuMatting shows several potential applications (e.g., matting-data generator, community-friendly art design and controllable generation).","As a matting-data generator, DiffuMatting synthesizes general object and portrait matting sets, effectively reducing the relative MSE error by 15.4% in General Object Matting and 11.4% in Portrait Matting tasks."],"url":"http://arxiv.org/abs/2403.06168v1","category":"cs.CV"}
{"created":"2024-03-10 09:39:00","title":"Can Large Language Models Automatically Score Proficiency of Written Essays?","abstract":"Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness. Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays. We experimented with two popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential to this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right prompt depends highly on the model and nature of the task. Second, the two LLMs exhibited comparable average performance in AES, with a slight advantage for ChatGPT. Finally, despite the performance gap between the two LLMs and SOTA models in terms of predictions, they provide feedback to enhance the quality of the essays, which can potentially help both teachers and students.","sentences":["Although several methods were proposed to address the problem of automated essay scoring (AES) in the last 50 years, there is still much to desire in terms of effectiveness.","Large Language Models (LLMs) are transformer-based models that demonstrate extraordinary capabilities on various tasks.","In this paper, we test the ability of LLMs, given their powerful linguistic knowledge, to analyze and effectively score written essays.","We experimented with two popular LLMs, namely ChatGPT and Llama.","We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait.","We utilized prompt-engineering tactics in designing four different prompts to bring their maximum potential to this task.","Our experiments conducted on the ASAP dataset revealed several interesting observations.","First, choosing the right prompt depends highly on the model and nature of the task.","Second, the two LLMs exhibited comparable average performance in AES, with a slight advantage for ChatGPT.","Finally, despite the performance gap between the two LLMs and SOTA models in terms of predictions, they provide feedback to enhance the quality of the essays, which can potentially help both teachers and students."],"url":"http://arxiv.org/abs/2403.06149v1","category":"cs.CL"}
{"created":"2024-03-10 09:24:53","title":"All-in-one platform for AI R&D in medical imaging, encompassing data collection, selection, annotation, and pre-processing","abstract":"Deep Learning is advancing medical imaging Research and Development (R&D), leading to the frequent clinical use of Artificial Intelligence/Machine Learning (AI/ML)-based medical devices. However, to advance AI R&D, two challenges arise: 1) significant data imbalance, with most data from Europe/America and under 10% from Asia, despite its 60% global population share; and 2) hefty time and investment needed to curate proprietary datasets for commercial use. In response, we established the first commercial medical imaging platform, encompassing steps like: 1) data collection, 2) data selection, 3) annotation, and 4) pre-processing. Moreover, we focus on harnessing under-represented data from Japan and broader Asia, including Computed Tomography, Magnetic Resonance Imaging, and Whole Slide Imaging scans. Using the collected data, we are preparing/providing ready-to-use datasets for medical AI R&D by 1) offering these datasets to AI firms, biopharma, and medical device makers and 2) using them as training/test data to develop tailored AI solutions for such entities. We also aim to merge Blockchain for data security and plan to synthesize rare disease data via generative AI. DataHub Website: https://medical-datahub.ai/","sentences":["Deep Learning is advancing medical imaging Research and Development (R&D), leading to the frequent clinical use of Artificial Intelligence/Machine Learning (AI/ML)-based medical devices.","However, to advance AI R&D, two challenges arise: 1) significant data imbalance, with most data from Europe/America and under 10% from Asia, despite its 60% global population share; and 2) hefty time and investment needed to curate proprietary datasets for commercial use.","In response, we established the first commercial medical imaging platform, encompassing steps like: 1) data collection, 2) data selection, 3) annotation, and 4) pre-processing.","Moreover, we focus on harnessing under-represented data from Japan and broader Asia, including Computed Tomography, Magnetic Resonance Imaging, and Whole Slide Imaging scans.","Using the collected data, we are preparing/providing ready-to-use datasets for medical AI R&D by 1) offering these datasets to AI firms, biopharma, and medical device makers and 2) using them as training/test data to develop tailored AI solutions for such entities.","We also aim to merge Blockchain for data security and plan to synthesize rare disease data via generative AI.","DataHub Website: https://medical-datahub.ai/"],"url":"http://arxiv.org/abs/2403.06145v1","category":"cs.CV"}
{"created":"2024-03-10 09:11:57","title":"Fluent: Round-efficient Secure Aggregation for Private Federated Learning","abstract":"Federated learning (FL) facilitates collaborative training of machine learning models among a large number of clients while safeguarding the privacy of their local datasets. However, FL remains susceptible to vulnerabilities such as privacy inference and inversion attacks. Single-server secure aggregation schemes were proposed to address these threats. Nonetheless, they encounter practical constraints due to their round and communication complexities. This work introduces Fluent, a round and communication-efficient secure aggregation scheme for private FL. Fluent has several improvements compared to state-of-the-art solutions like Bell et al. (CCS 2020) and Ma et al. (SP 2023): (1) it eliminates frequent handshakes and secret sharing operations by efficiently reusing the shares across multiple training iterations without leaking any private information; (2) it accomplishes both the consistency check and gradient unmasking in one logical step, thereby reducing another round of communication. With these innovations, Fluent achieves the fewest communication rounds (i.e., two in the collection phase) in the malicious server setting, in contrast to at least three rounds in existing schemes. This significantly minimizes the latency for geographically distributed clients; (3) Fluent also introduces Fluent-Dynamic with a participant selection algorithm and an alternative secret sharing scheme. This can facilitate dynamic client joining and enhance the system flexibility and scalability. We implemented Fluent and compared it with existing solutions. Experimental results show that Fluent improves the computational cost by at least 75% and communication overhead by at least 25% for normal clients. Fluent also reduces the communication overhead for the server at the expense of a marginal increase in computational cost.","sentences":["Federated learning (FL) facilitates collaborative training of machine learning models among a large number of clients while safeguarding the privacy of their local datasets.","However, FL remains susceptible to vulnerabilities such as privacy inference and inversion attacks.","Single-server secure aggregation schemes were proposed to address these threats.","Nonetheless, they encounter practical constraints due to their round and communication complexities.","This work introduces Fluent, a round and communication-efficient secure aggregation scheme for private FL.","Fluent has several improvements compared to state-of-the-art solutions like Bell et al.","(CCS 2020) and Ma et al.","(SP 2023): (1) it eliminates frequent handshakes and secret sharing operations by efficiently reusing the shares across multiple training iterations without leaking any private information; (2) it accomplishes both the consistency check and gradient unmasking in one logical step, thereby reducing another round of communication.","With these innovations, Fluent achieves the fewest communication rounds (i.e., two in the collection phase) in the malicious server setting, in contrast to at least three rounds in existing schemes.","This significantly minimizes the latency for geographically distributed clients; (3) Fluent also introduces Fluent-Dynamic with a participant selection algorithm and an alternative secret sharing scheme.","This can facilitate dynamic client joining and enhance the system flexibility and scalability.","We implemented Fluent and compared it with existing solutions.","Experimental results show that Fluent improves the computational cost by at least 75% and communication overhead by at least 25% for normal clients.","Fluent also reduces the communication overhead for the server at the expense of a marginal increase in computational cost."],"url":"http://arxiv.org/abs/2403.06143v1","category":"cs.CR"}
{"created":"2024-03-10 08:59:04","title":"Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity","abstract":"Due to the sparsity of user data, sentiment analysis on user reviews in e-commerce platforms often suffers from poor performance, especially when faced with extremely sparse user data or long-tail labels. Recently, the emergence of LLMs has introduced new solutions to such problems by leveraging graph structures to generate supplementary user profiles. However, previous approaches have not fully utilized the graph understanding capabilities of LLMs and have struggled to adapt to complex streaming data environments. In this work, we propose a fine-grained streaming data synthesis framework that categorizes sparse users into three categories: Mid-tail, Long-tail, and Extreme. Specifically, we design LLMs to comprehensively understand three key graph elements in streaming data, including Local-global Graph Understanding, Second-Order Relationship Extraction, and Product Attribute Understanding, which enables the generation of high-quality synthetic data to effectively address sparsity across different categories. Experimental results on three real datasets demonstrate significant performance improvements, with synthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%, respectively.","sentences":["Due to the sparsity of user data, sentiment analysis on user reviews in e-commerce platforms often suffers from poor performance, especially when faced with extremely sparse user data or long-tail labels.","Recently, the emergence of LLMs has introduced new solutions to such problems by leveraging graph structures to generate supplementary user profiles.","However, previous approaches have not fully utilized the graph understanding capabilities of LLMs and have struggled to adapt to complex streaming data environments.","In this work, we propose a fine-grained streaming data synthesis framework that categorizes sparse users into three categories: Mid-tail, Long-tail, and Extreme.","Specifically, we design LLMs to comprehensively understand three key graph elements in streaming data, including Local-global Graph Understanding, Second-Order Relationship Extraction, and Product Attribute Understanding, which enables the generation of high-quality synthetic data to effectively address sparsity across different categories.","Experimental results on three real datasets demonstrate significant performance improvements, with synthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%, respectively."],"url":"http://arxiv.org/abs/2403.06139v1","category":"cs.CL"}
{"created":"2024-03-10 08:50:56","title":"MACE: Mass Concept Erasure in Diffusion Models","abstract":"The rapid expansion of large-scale text-to-image diffusion models has raised growing concerns regarding their potential misuse in creating harmful or misleading content. In this paper, we introduce MACE, a finetuning framework for the task of mass concept erasure. This task aims to prevent models from generating images that embody unwanted concepts when prompted. Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity). In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity. This is achieved by leveraging closed-form cross-attention refinement along with LoRA finetuning, collectively eliminating the information of undesirable concepts. Furthermore, MACE integrates multiple LoRAs without mutual interference. We conduct extensive evaluations of MACE against prior methods across four different tasks: object erasure, celebrity erasure, explicit content erasure, and artistic style erasure. Our results reveal that MACE surpasses prior methods in all evaluated tasks. Code is available at https://github.com/Shilin-LU/MACE.","sentences":["The rapid expansion of large-scale text-to-image diffusion models has raised growing concerns regarding their potential misuse in creating harmful or misleading content.","In this paper, we introduce MACE, a finetuning framework for the task of mass concept erasure.","This task aims to prevent models from generating images that embody unwanted concepts when prompted.","Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity).","In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity.","This is achieved by leveraging closed-form cross-attention refinement along with LoRA finetuning, collectively eliminating the information of undesirable concepts.","Furthermore, MACE integrates multiple LoRAs without mutual interference.","We conduct extensive evaluations of MACE against prior methods across four different tasks: object erasure, celebrity erasure, explicit content erasure, and artistic style erasure.","Our results reveal that MACE surpasses prior methods in all evaluated tasks.","Code is available at https://github.com/Shilin-LU/MACE."],"url":"http://arxiv.org/abs/2403.06135v1","category":"cs.CV"}
{"created":"2024-03-10 08:41:22","title":"FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning","abstract":"Instruction tuning has proven essential for enhancing the performance of large language models (LLMs) in generating human-aligned responses. However, collecting diverse, high-quality instruction data for tuning poses challenges, particularly in privacy-sensitive domains. Federated instruction tuning (FedIT) has emerged as a solution, leveraging federated learning from multiple data owners while preserving privacy. Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks. To address these issues, we propose a novel federated algorithm, FedPIT, which utilizes LLMs' in-context learning capability to self-generate task-specific synthetic data for training autonomously. Our method employs parameter-isolated training to maintain global parameters trained on synthetic data and local parameters trained on augmented local data, effectively thwarting data extraction attacks. Extensive experiments on real-world medical data demonstrate the effectiveness of FedPIT in improving federated few-shot performance while preserving privacy and robustness against data heterogeneity.","sentences":["Instruction tuning has proven essential for enhancing the performance of large language models (LLMs) in generating human-aligned responses.","However, collecting diverse, high-quality instruction data for tuning poses challenges, particularly in privacy-sensitive domains.","Federated instruction tuning (FedIT) has emerged as a solution, leveraging federated learning from multiple data owners while preserving privacy.","Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks.","To address these issues, we propose a novel federated algorithm, FedPIT, which utilizes LLMs' in-context learning capability to self-generate task-specific synthetic data for training autonomously.","Our method employs parameter-isolated training to maintain global parameters trained on synthetic data and local parameters trained on augmented local data, effectively thwarting data extraction attacks.","Extensive experiments on real-world medical data demonstrate the effectiveness of FedPIT in improving federated few-shot performance while preserving privacy and robustness against data heterogeneity."],"url":"http://arxiv.org/abs/2403.06131v1","category":"cs.CR"}
{"created":"2024-03-10 08:27:38","title":"Blockchain-Enabled Variational Information Bottleneck for IoT Networks","abstract":"In Internet of Things (IoT) networks, the amount of data sensed by user devices may be huge, resulting in the serious network congestion. To solve this problem, intelligent data compression is critical. The variational information bottleneck (VIB) approach, combined with machine learning, can be employed to train the encoder and decoder, so that the required transmission data size can be reduced significantly. However, VIB suffers from the computing burden and network insecurity. In this paper, we propose a blockchain-enabled VIB (BVIB) approach to relieve the computing burden while guaranteeing network security. Extensive simulations conducted by Python and C++ demonstrate that BVIB outperforms VIB by 36%, 22% and 57% in terms of time and CPU cycles cost, mutual information, and accuracy under attack, respectively.","sentences":["In Internet of Things (IoT) networks, the amount of data sensed by user devices may be huge, resulting in the serious network congestion.","To solve this problem, intelligent data compression is critical.","The variational information bottleneck (VIB) approach, combined with machine learning, can be employed to train the encoder and decoder, so that the required transmission data size can be reduced significantly.","However, VIB suffers from the computing burden and network insecurity.","In this paper, we propose a blockchain-enabled VIB (BVIB) approach to relieve the computing burden while guaranteeing network security.","Extensive simulations conducted by Python and C++ demonstrate that BVIB outperforms VIB by 36%, 22% and 57% in terms of time and CPU cycles cost, mutual information, and accuracy under attack, respectively."],"url":"http://arxiv.org/abs/2403.06129v1","category":"eess.SP"}
{"created":"2024-03-10 07:21:31","title":"FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language","abstract":"The effectiveness of central bank communication is a crucial aspect of monetary policy transmission. While recent research has examined the influence of policy communication by the chairs of the Federal Reserve on various financial variables, much of the literature relies on rule-based or dictionary-based methods in parsing the language of the chairs, leaving nuanced information about policy stance contained in nonverbal emotion out of the analysis. In the current study, we propose the Fine-Grained Monetary Policy Analysis Framework (FMPAF), a novel approach that integrates large language models (LLMs) with regression analysis to provide a comprehensive analysis of the impact of the press-conference communications of chairs of the Federal Reserve on financial markets. We conduct extensive comparisons of model performance under different levels of granularity, modalities, and communication scenarios. Based on our preferred specification, a one-unit increase in the sentiment score is associated with an increase of the price of S\\&P 500 Exchange-Traded Fund by approximately 500 basis points, a 15-basis-point decrease in the policy interest rate, while not leading to a significant response in exchange rates.","sentences":["The effectiveness of central bank communication is a crucial aspect of monetary policy transmission.","While recent research has examined the influence of policy communication by the chairs of the Federal Reserve on various financial variables, much of the literature relies on rule-based or dictionary-based methods in parsing the language of the chairs, leaving nuanced information about policy stance contained in nonverbal emotion out of the analysis.","In the current study, we propose the Fine-Grained Monetary Policy Analysis Framework (FMPAF), a novel approach that integrates large language models (LLMs) with regression analysis to provide a comprehensive analysis of the impact of the press-conference communications of chairs of the Federal Reserve on financial markets.","We conduct extensive comparisons of model performance under different levels of granularity, modalities, and communication scenarios.","Based on our preferred specification, a one-unit increase in the sentiment score is associated with an increase of the price of S\\&P 500 Exchange-Traded Fund by approximately 500 basis points, a 15-basis-point decrease in the policy interest rate, while not leading to a significant response in exchange rates."],"url":"http://arxiv.org/abs/2403.06115v1","category":"cs.CL"}
{"created":"2024-03-10 07:13:34","title":"Expected Performance of Cosmic Muon Veto Detector","abstract":"The India-Based Neutrino Observatory (INO) collaboration houses the miniICAL detector, at the transit campus of IICHEP, Madurai, India, which serves as a prototype-detector of the larger Iron-Calorimeter detector (ICAL). The purpose of miniICAL lies in unraveling the intricate engineering challenges inherent in constructing a substantial ICAL-type detector. To explore the feasibility of building a large-scale neutrino experiment at shallow depths the collaboration has embarked upon the construction of a Cosmic Muon Veto Detector (CMVD) around the miniICAL detector. The primary objective of this endeavor revolves around attaining a veto efficiency surpassing $99.99\\%$, while simultaneously maintaining a false-positive rate lower than $10^{-5}$.   The CMVD system is based on extruded plastic scintillators (EPS) and utilizes wavelength-shifting fibers to collect scintillation photons and uses silicon photomultipliers (SiPMs) as photo-transducers. The expected performance of the CMVD is estimated using simulated muon tracks in the miniICAL stack taking into account efficiency, multiplicity of RPC detectors from the miniICAL data as well as the noise of SiPM, observed SiPM spectra and time resolution due to cosmic muon along the whole length of EPS etc.   The CMVD experiment is a crucial step in the larger context of neutrino research, by increasing the veto efficiency of cosmic muons, the CMVD experiment helps to pave the way for future large-scale shallow-depth neutrino experiments, providing valuable insights into the study of neutrinos and their properties.","sentences":["The India-Based Neutrino Observatory (INO) collaboration houses the miniICAL detector, at the transit campus of IICHEP, Madurai, India, which serves as a prototype-detector of the larger Iron-Calorimeter detector (ICAL).","The purpose of miniICAL lies in unraveling the intricate engineering challenges inherent in constructing a substantial ICAL-type detector.","To explore the feasibility of building a large-scale neutrino experiment at shallow depths the collaboration has embarked upon the construction of a Cosmic Muon Veto Detector (CMVD) around the miniICAL detector.","The primary objective of this endeavor revolves around attaining a veto efficiency surpassing $99.99\\%$, while simultaneously maintaining a false-positive rate lower than $10^{-5}$.   The CMVD system is based on extruded plastic scintillators (EPS) and utilizes wavelength-shifting fibers to collect scintillation photons and uses silicon photomultipliers (SiPMs) as photo-transducers.","The expected performance of the CMVD is estimated using simulated muon tracks in the miniICAL stack taking into account efficiency, multiplicity of RPC detectors from the miniICAL data as well as the noise of SiPM, observed SiPM spectra and time resolution due to cosmic muon along the whole length of EPS etc.   ","The CMVD experiment is a crucial step in the larger context of neutrino research, by increasing the veto efficiency of cosmic muons, the CMVD experiment helps to pave the way for future large-scale shallow-depth neutrino experiments, providing valuable insights into the study of neutrinos and their properties."],"url":"http://arxiv.org/abs/2403.06114v1","category":"hep-ex"}
{"created":"2024-03-10 07:05:50","title":"Decentralized P2P Trading based on Blockchain for Retail Electricity Markets","abstract":"This paper introduces peer to peer (P2P) trading mechanisms based on decentralized Blockchain to facilitate retail electricity market for ever-increasing distributed energy resources (DERs). The Blockchain network supports fast and secure retail trading among DERs and facilitates a sustainable local P2P trading platform. In this decentralized Blockchain architecture no single entity or organization has control over the entire system rather all users collectively maintain control. The effectiveness of the proposed automated market design and optimization is simulated using different use case scenarios in an open source Blockchain Simulator and MATLAB. The results show the efficacy of the trading mechanism in achieving demand response through strategies such as peak load shaving, load shifting, and integration of DERs.","sentences":["This paper introduces peer to peer (P2P) trading mechanisms based on decentralized Blockchain to facilitate retail electricity market for ever-increasing distributed energy resources (DERs).","The Blockchain network supports fast and secure retail trading among DERs and facilitates a sustainable local P2P trading platform.","In this decentralized Blockchain architecture no single entity or organization has control over the entire system rather all users collectively maintain control.","The effectiveness of the proposed automated market design and optimization is simulated using different use case scenarios in an open source Blockchain Simulator and MATLAB.","The results show the efficacy of the trading mechanism in achieving demand response through strategies such as peak load shaving, load shifting, and integration of DERs."],"url":"http://arxiv.org/abs/2403.06112v1","category":"eess.SY"}
{"created":"2024-03-10 06:30:54","title":"Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning","abstract":"This paper delves into enhancing the classification performance on the GoEmotions dataset, a large, manually annotated dataset for emotion detection in text. The primary goal of this paper is to address the challenges of detecting subtle emotions in text, a complex issue in Natural Language Processing (NLP) with significant practical applications. The findings offer valuable insights into addressing the challenges of emotion detection in text and suggest directions for future research, including the potential for a survey paper that synthesizes methods and performances across various datasets in this domain.","sentences":["This paper delves into enhancing the classification performance on the GoEmotions dataset, a large, manually annotated dataset for emotion detection in text.","The primary goal of this paper is to address the challenges of detecting subtle emotions in text, a complex issue in Natural Language Processing (NLP) with significant practical applications.","The findings offer valuable insights into addressing the challenges of emotion detection in text and suggest directions for future research, including the potential for a survey paper that synthesizes methods and performances across various datasets in this domain."],"url":"http://arxiv.org/abs/2403.06108v1","category":"cs.CL"}
{"created":"2024-03-10 05:40:12","title":"VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models","abstract":"The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, as well as other text-to-video diffusion models, highly relies on the prompts, and there is no publicly available dataset featuring a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 million unique text-to-video prompts from real users. Additionally, the dataset includes 6.69 million videos generated by four state-of-the-art diffusion models and some related data. We initially demonstrate the curation of this large-scale dataset, which is a time-consuming and costly process. Subsequently, we show how the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Based on the analysis of these prompts, we identify the necessity for a new prompt dataset specifically designed for text-to-video generation and gain insights into the preferences of real users when creating videos. Our large-scale and diverse dataset also inspires many exciting new research areas. For instance, to develop better, more efficient, and safer text-to-video diffusion models, we suggest exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models. We make the collected dataset VidProM publicly available at GitHub and Hugging Face under the CC-BY- NC 4.0 License.","sentences":["The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications.","However, Sora, as well as other text-to-video diffusion models, highly relies on the prompts, and there is no publicly available dataset featuring a study of text-to-video prompts.","In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 million unique text-to-video prompts from real users.","Additionally, the dataset includes 6.69 million videos generated by four state-of-the-art diffusion models and some related data.","We initially demonstrate the curation of this large-scale dataset, which is a time-consuming and costly process.","Subsequently, we show how the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation.","Based on the analysis of these prompts, we identify the necessity for a new prompt dataset specifically designed for text-to-video generation and gain insights into the preferences of real users when creating videos.","Our large-scale and diverse dataset also inspires many exciting new research areas.","For instance, to develop better, more efficient, and safer text-to-video diffusion models, we suggest exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models.","We make the collected dataset VidProM publicly available at GitHub and Hugging Face under the CC-BY- NC 4.0 License."],"url":"http://arxiv.org/abs/2403.06098v1","category":"cs.CV"}
{"created":"2024-03-10 05:12:16","title":"Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery","abstract":"We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame \\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task of address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage \\textbf{M}odel annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \\url{https://github.com/zhhvvv/CNER-UAV}.","sentences":["We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame \\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task of address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle delivery systems.","The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models.","To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity.","The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage \\textbf{M}odel annotation.","We evaluated classical NER models on our dataset and provided in-depth analysis.","The dataset and models are publicly available at \\url{https://github.com/zhhvvv/CNER-UAV}."],"url":"http://arxiv.org/abs/2403.06097v1","category":"cs.CL"}
{"created":"2024-03-10 05:10:34","title":"RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion","abstract":"Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present RepoHyper, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHyper is the Repo-level Semantic Graph (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that RepoHyper markedly outperforms existing techniques in repository-level code completion, showcasing enhanced accuracy across various datasets when compared to several strong baselines.","sentences":["Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks.","However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions.","To overcome these limitations, we present RepoHyper, a multifaceted framework designed to address the complex challenges associated with repository-level code completion.","Central to RepoHyper is the Repo-level Semantic Graph (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories.","Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets.","Our evaluations show that RepoHyper markedly outperforms existing techniques in repository-level code completion, showcasing enhanced accuracy across various datasets when compared to several strong baselines."],"url":"http://arxiv.org/abs/2403.06095v1","category":"cs.SE"}
{"created":"2024-03-10 04:17:54","title":"Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models","abstract":"In the burgeoning field of intelligent transportation systems, enhancing vehicle-driver interaction through facial attribute recognition, such as facial expression, eye gaze, age, etc., is of paramount importance for safety, personalization, and overall user experience. However, the scarcity of comprehensive large-scale, real-world datasets poses a significant challenge for training robust multi-task models. Existing literature often overlooks the potential of synthetic datasets and the comparative efficacy of state-of-the-art vision foundation models in such constrained settings. This paper addresses these gaps by investigating the utility of synthetic datasets for training complex multi-task models that recognize facial attributes of passengers of a vehicle, such as gaze plane, age, and facial expression. Utilizing transfer learning techniques with both pre-trained Vision Transformer (ViT) and Residual Network (ResNet) models, we explore various training and adaptation methods to optimize performance, particularly when data availability is limited. We provide extensive post-evaluation analysis, investigating the effects of synthetic data distributions on model performance in in-distribution data and out-of-distribution inference. Our study unveils counter-intuitive findings, notably the superior performance of ResNet over ViTs in our specific multi-task context, which is attributed to the mismatch in model complexity relative to task complexity. Our results highlight the challenges and opportunities for enhancing the use of synthetic data and vision foundation models in practical applications.","sentences":["In the burgeoning field of intelligent transportation systems, enhancing vehicle-driver interaction through facial attribute recognition, such as facial expression, eye gaze, age, etc., is of paramount importance for safety, personalization, and overall user experience.","However, the scarcity of comprehensive large-scale, real-world datasets poses a significant challenge for training robust multi-task models.","Existing literature often overlooks the potential of synthetic datasets and the comparative efficacy of state-of-the-art vision foundation models in such constrained settings.","This paper addresses these gaps by investigating the utility of synthetic datasets for training complex multi-task models that recognize facial attributes of passengers of a vehicle, such as gaze plane, age, and facial expression.","Utilizing transfer learning techniques with both pre-trained Vision Transformer (ViT) and Residual Network (ResNet) models, we explore various training and adaptation methods to optimize performance, particularly when data availability is limited.","We provide extensive post-evaluation analysis, investigating the effects of synthetic data distributions on model performance in in-distribution data and out-of-distribution inference.","Our study unveils counter-intuitive findings, notably the superior performance of ResNet over ViTs in our specific multi-task context, which is attributed to the mismatch in model complexity relative to task complexity.","Our results highlight the challenges and opportunities for enhancing the use of synthetic data and vision foundation models in practical applications."],"url":"http://arxiv.org/abs/2403.06088v1","category":"cs.CV"}
{"created":"2024-03-10 04:16:04","title":"Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach","abstract":"Estimating the potential behavior of the surrounding human-driven vehicles is crucial for the safety of autonomous vehicles in a mixed traffic flow. Recent state-of-the-art achieved accurate prediction using deep neural networks. However, these end-to-end models are usually black boxes with weak interpretability and generalizability. This paper proposes the Goal-based Neural Variational Agent (GNeVA), an interpretable generative model for motion prediction with robust generalizability to out-of-distribution cases. For interpretability, the model achieves target-driven motion prediction by estimating the spatial distribution of long-term destinations with a variational mixture of Gaussians. We identify a causal structure among maps and agents' histories and derive a variational posterior to enhance generalizability. Experiments on motion prediction datasets validate that the fitted model can be interpretable and generalizable and can achieve comparable performance to state-of-the-art results.","sentences":["Estimating the potential behavior of the surrounding human-driven vehicles is crucial for the safety of autonomous vehicles in a mixed traffic flow.","Recent state-of-the-art achieved accurate prediction using deep neural networks.","However, these end-to-end models are usually black boxes with weak interpretability and generalizability.","This paper proposes the Goal-based Neural Variational Agent (GNeVA), an interpretable generative model for motion prediction with robust generalizability to out-of-distribution cases.","For interpretability, the model achieves target-driven motion prediction by estimating the spatial distribution of long-term destinations with a variational mixture of Gaussians.","We identify a causal structure among maps and agents' histories and derive a variational posterior to enhance generalizability.","Experiments on motion prediction datasets validate that the fitted model can be interpretable and generalizable and can achieve comparable performance to state-of-the-art results."],"url":"http://arxiv.org/abs/2403.06086v1","category":"cs.AI"}
{"created":"2024-03-10 03:46:13","title":"Steering a Fleet: Adaptation for Large-Scale, Workflow-Based Experiments","abstract":"Experimental science is increasingly driven by instruments that produce vast volumes of data and thus a need to manage, compute, describe, and index this data. High performance and distributed computing provide the means of addressing the computing needs; however, in practice, the variety of actions required and the distributed set of resources involved, requires sophisticated \"flows\" defining the steps to be performed on data. As each scan or measurement is performed by an instrument, a new instance of the flow is initiated resulting in a \"fleet\" of concurrently running flows, with the overall goal to process all the data collected during a potentially long-running experiment. During the course of the experiment, each flow may need to adapt its execution due to changes in the environment, such as computational or storage resource availability, or based on the progress of the fleet as a whole such as completion or discovery of an intermediate result leading to a change in subsequent flow's behavior. We introduce a cloud-based decision engine, Braid, which flows consult during execution to query their run-time environment and coordinate with other flows within their fleet. Braid accepts streams of measurements taken from the run-time environment or from within flow runs which can then be statistically aggregated and compared to other streams to determine a strategy to guide flow execution. For example, queue lengths in execution environments can be used to direct a flow to run computations in one environment or another, or experiment progress as measured by individual flows can be aggregated to determine the progress and subsequent direction of the flows within a fleet. We describe Braid, its interface, implementation and performance characteristics. We further show through examples and experience modifying an existing scientific flow how Braid is used to make adaptable flows.","sentences":["Experimental science is increasingly driven by instruments that produce vast volumes of data and thus a need to manage, compute, describe, and index this data.","High performance and distributed computing provide the means of addressing the computing needs; however, in practice, the variety of actions required and the distributed set of resources involved, requires sophisticated \"flows\" defining the steps to be performed on data.","As each scan or measurement is performed by an instrument, a new instance of the flow is initiated resulting in a \"fleet\" of concurrently running flows, with the overall goal to process all the data collected during a potentially long-running experiment.","During the course of the experiment, each flow may need to adapt its execution due to changes in the environment, such as computational or storage resource availability, or based on the progress of the fleet as a whole such as completion or discovery of an intermediate result leading to a change in subsequent flow's behavior.","We introduce a cloud-based decision engine, Braid, which flows consult during execution to query their run-time environment and coordinate with other flows within their fleet.","Braid accepts streams of measurements taken from the run-time environment or from within flow runs which can then be statistically aggregated and compared to other streams to determine a strategy to guide flow execution.","For example, queue lengths in execution environments can be used to direct a flow to run computations in one environment or another, or experiment progress as measured by individual flows can be aggregated to determine the progress and subsequent direction of the flows within a fleet.","We describe Braid, its interface, implementation and performance characteristics.","We further show through examples and experience modifying an existing scientific flow how Braid is used to make adaptable flows."],"url":"http://arxiv.org/abs/2403.06077v1","category":"cs.DC"}
{"created":"2024-03-10 03:36:59","title":"Stochastic Geometry Analysis for Distributed RISs-Assisted mmWave Communications","abstract":"Millimeter wave (mmWave) has attracted considerable attention due to its wide bandwidth and high frequency. However, it is highly susceptible to blockages, resulting in significant degradation of the coverage and the sum rate. A promising approach is deploying distributed reconfigurable intelligent surfaces (RISs), which can establish extra communication links. In this paper, we investigate the impact of distributed RISs on the coverage probability and the sum rate in mmWave wireless communication systems. Specifically, we first introduce the system model, which includes the blockage, the RIS and the user distribution models, leveraging the Poisson point process. Then, we define the association criterion and derive the conditional coverage probabilities for the two cases of direct association and reflective association through RISs. Finally, we combine the two cases using Campbell's theorem and the total probability theorem to obtain the closed-form expressions for the ergodic coverage probability and the sum rate. Simulation results validate the effectiveness of the proposed analytical approach, demonstrating that the deployment of distributed RISs significantly improves the ergodic coverage probability by 45.4% and the sum rate by over 1.5 times.","sentences":["Millimeter wave (mmWave) has attracted considerable attention due to its wide bandwidth and high frequency.","However, it is highly susceptible to blockages, resulting in significant degradation of the coverage and the sum rate.","A promising approach is deploying distributed reconfigurable intelligent surfaces (RISs), which can establish extra communication links.","In this paper, we investigate the impact of distributed RISs on the coverage probability and the sum rate in mmWave wireless communication systems.","Specifically, we first introduce the system model, which includes the blockage, the RIS and the user distribution models, leveraging the Poisson point process.","Then, we define the association criterion and derive the conditional coverage probabilities for the two cases of direct association and reflective association through RISs.","Finally, we combine the two cases using Campbell's theorem and the total probability theorem to obtain the closed-form expressions for the ergodic coverage probability and the sum rate.","Simulation results validate the effectiveness of the proposed analytical approach, demonstrating that the deployment of distributed RISs significantly improves the ergodic coverage probability by 45.4% and the sum rate by over 1.5 times."],"url":"http://arxiv.org/abs/2403.06073v1","category":"cs.IT"}
{"created":"2024-03-10 02:16:13","title":"L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification","abstract":"Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\\%$ on Citeseer and 81.3$\\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitude faster than other nonlinear GCN models on PubMed dataset. Our code is publicly available at https://github.com/llqy123/LLGC-master.","sentences":["Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data.","However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs.","In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN.","Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data.","Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\\%$ on Citeseer and 81.3$\\%$ on PubMed datasets.","Furthermore, we observe that our approach can be trained up to two orders of magnitude faster than other nonlinear GCN models on PubMed dataset.","Our code is publicly available at https://github.com/llqy123/LLGC-master."],"url":"http://arxiv.org/abs/2403.06064v1","category":"cs.LG"}
{"created":"2024-03-10 02:14:24","title":"Target-constrained Bidirectional Planning for Generation of Target-oriented Proactive Dialogue","abstract":"Target-oriented proactive dialogue systems aim to lead conversations from a dialogue context toward a pre-determined target, such as making recommendations on designated items or introducing new specific topics. To this end, it is critical for such dialogue systems to plan reasonable actions to drive the conversation proactively, and meanwhile, to plan appropriate topics to move the conversation forward to the target topic smoothly. In this work, we mainly focus on effective dialogue planning for target-oriented dialogue generation. Inspired by decision-making theories in cognitive science, we propose a novel target-constrained bidirectional planning (TRIP) approach, which plans an appropriate dialogue path by looking ahead and looking back. By formulating the planning as a generation task, our TRIP bidirectionally generates a dialogue path consisting of a sequence of <action, topic> pairs using two Transformer decoders. They are expected to supervise each other and converge on consistent actions and topics by minimizing the decision gap and contrastive generation of targets. Moreover, we propose a target-constrained decoding algorithm with a bidirectional agreement to better control the planning process. Subsequently, we adopt the planned dialogue paths to guide dialogue generation in a pipeline manner, where we explore two variants: prompt-based generation and plan-controlled generation. Extensive experiments are conducted on two challenging dialogue datasets, which are re-purposed for exploring target-oriented dialogue. Our automatic and human evaluations demonstrate that the proposed methods significantly outperform various baseline models.","sentences":["Target-oriented proactive dialogue systems aim to lead conversations from a dialogue context toward a pre-determined target, such as making recommendations on designated items or introducing new specific topics.","To this end, it is critical for such dialogue systems to plan reasonable actions to drive the conversation proactively, and meanwhile, to plan appropriate topics to move the conversation forward to the target topic smoothly.","In this work, we mainly focus on effective dialogue planning for target-oriented dialogue generation.","Inspired by decision-making theories in cognitive science, we propose a novel target-constrained bidirectional planning (TRIP) approach, which plans an appropriate dialogue path by looking ahead and looking back.","By formulating the planning as a generation task, our TRIP bidirectionally generates a dialogue path consisting of a sequence of <action, topic> pairs using two Transformer decoders.","They are expected to supervise each other and converge on consistent actions and topics by minimizing the decision gap and contrastive generation of targets.","Moreover, we propose a target-constrained decoding algorithm with a bidirectional agreement to better control the planning process.","Subsequently, we adopt the planned dialogue paths to guide dialogue generation in a pipeline manner, where we explore two variants: prompt-based generation and plan-controlled generation.","Extensive experiments are conducted on two challenging dialogue datasets, which are re-purposed for exploring target-oriented dialogue.","Our automatic and human evaluations demonstrate that the proposed methods significantly outperform various baseline models."],"url":"http://arxiv.org/abs/2403.06063v1","category":"cs.CL"}
{"created":"2024-03-10 00:47:05","title":"Decoupled Data Consistency with Diffusion Purification for Image Restoration","abstract":"Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involves alternating between a reconstruction phase to maintain data consistency and a refinement phase that enforces the prior via diffusion purification. Our approach demonstrates versatility, making it highly adaptable for efficient problem-solving in latent space. Additionally, it reduces the necessity for numerous sampling steps through the integration of consistency models. The efficacy of our approach is validated through comprehensive experiments across various image restoration tasks, including image denoising, deblurring, inpainting, and super-resolution.","sentences":["Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions.","To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models.","However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time.","They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps.","In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps.","Our method involves alternating between a reconstruction phase to maintain data consistency and a refinement phase that enforces the prior via diffusion purification.","Our approach demonstrates versatility, making it highly adaptable for efficient problem-solving in latent space.","Additionally, it reduces the necessity for numerous sampling steps through the integration of consistency models.","The efficacy of our approach is validated through comprehensive experiments across various image restoration tasks, including image denoising, deblurring, inpainting, and super-resolution."],"url":"http://arxiv.org/abs/2403.06054v1","category":"eess.IV"}
{"created":"2024-03-09 23:28:54","title":"MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts","abstract":"Data-driven methods have great advantages in modeling complicated human behavioral dynamics and dealing with many human-robot interaction applications. However, collecting massive and annotated real-world human datasets has been a laborious task, especially for highly interactive scenarios. On the other hand, algorithmic data generation methods are usually limited by their model capacities, making them unable to offer realistic and diverse data needed by various application users. In this work, we study trajectory-level data generation for multi-human or human-robot interaction scenarios and propose a learning-based automatic trajectory generation model, which we call Multi-Agent TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of generating interactive human behaviors in realistic diverse contexts. We achieve this goal by modeling the explicit and interpretable objectives so that MATRIX can generate human motions based on diverse destinations and heterogeneous behaviors. We carried out extensive comparison and ablation studies to illustrate the effectiveness of our approach across various metrics. We also presented experiments that demonstrate the capability of MATRIX to serve as data augmentation for imitation-based motion planning.","sentences":["Data-driven methods have great advantages in modeling complicated human behavioral dynamics and dealing with many human-robot interaction applications.","However, collecting massive and annotated real-world human datasets has been a laborious task, especially for highly interactive scenarios.","On the other hand, algorithmic data generation methods are usually limited by their model capacities, making them unable to offer realistic and diverse data needed by various application users.","In this work, we study trajectory-level data generation for multi-human or human-robot interaction scenarios and propose a learning-based automatic trajectory generation model, which we call Multi-Agent TRajectory generation with dIverse conteXts (MATRIX).","MATRIX is capable of generating interactive human behaviors in realistic diverse contexts.","We achieve this goal by modeling the explicit and interpretable objectives so that MATRIX can generate human motions based on diverse destinations and heterogeneous behaviors.","We carried out extensive comparison and ablation studies to illustrate the effectiveness of our approach across various metrics.","We also presented experiments that demonstrate the capability of MATRIX to serve as data augmentation for imitation-based motion planning."],"url":"http://arxiv.org/abs/2403.06041v1","category":"cs.RO"}
{"created":"2024-03-09 23:22:56","title":"A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation","abstract":"Content creators increasingly utilize generative artificial intelligence (Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging sites to produce imaginative images, AI-generated videos, and articles using Large Language Models (LLMs). Despite its growing popularity, there remains an underexplored area concerning the specific domains where AI-generated content is being applied, and the methodologies content creators employ with Gen-AI tools during the creation process. This study initially explores this emerging area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI usage. Our research focuses on identifying the content domains, the variety of tools used, the activities performed, and the nature of the final products generated by Gen-AI in the context of user-generated content.","sentences":["Content creators increasingly utilize generative artificial intelligence (Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging sites to produce imaginative images, AI-generated videos, and articles using Large Language Models (LLMs).","Despite its growing popularity, there remains an underexplored area concerning the specific domains where AI-generated content is being applied, and the methodologies content creators employ with Gen-AI tools during the creation process.","This study initially explores this emerging area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI usage.","Our research focuses on identifying the content domains, the variety of tools used, the activities performed, and the nature of the final products generated by Gen-AI in the context of user-generated content."],"url":"http://arxiv.org/abs/2403.06039v1","category":"cs.HC"}
{"created":"2024-03-09 22:58:29","title":"Deciphering Crypto Twitter","abstract":"Cryptocurrency is a fast-moving space, with a continuous influx of new projects every year. However, an increasing number of incidents in the space, such as hacks and security breaches, threaten the growth of the community and the development of technology. This dynamic and often tumultuous landscape is vividly mirrored and shaped by discussions within Crypto Twitter, a key digital arena where investors, enthusiasts, and skeptics converge, revealing real-time sentiments and trends through social media interactions. We present our analysis on a Twitter dataset collected during a formative period of the cryptocurrency landscape. We collected 40 million tweets using cryptocurrency-related keywords and performed a nuanced analysis that involved grouping the tweets by semantic similarity and constructing a tweet and user network. We used sentence-level embeddings and autoencoders to create K-means clusters of tweets and identified six groups of tweets and their topics to examine different cryptocurrency-related interests and the change in sentiment over time. Moreover, we discovered sentiment indicators that point to real-life incidents in the crypto world, such as the FTX incident of November 2022. We also constructed and analyzed different networks of tweets and users in our dataset by considering the reply and quote relationships and analyzed the largest components of each network. Our networks reveal a structure of bot activity in Crypto Twitter and suggest that they can be detected and handled using a network-based approach. Our work sheds light on the potential of social media signals to detect and understand crypto events, benefiting investors, regulators, and curious observers alike, as well as the potential for bot detection in Crypto Twitter using a network-based approach.","sentences":["Cryptocurrency is a fast-moving space, with a continuous influx of new projects every year.","However, an increasing number of incidents in the space, such as hacks and security breaches, threaten the growth of the community and the development of technology.","This dynamic and often tumultuous landscape is vividly mirrored and shaped by discussions within Crypto Twitter, a key digital arena where investors, enthusiasts, and skeptics converge, revealing real-time sentiments and trends through social media interactions.","We present our analysis on a Twitter dataset collected during a formative period of the cryptocurrency landscape.","We collected 40 million tweets using cryptocurrency-related keywords and performed a nuanced analysis that involved grouping the tweets by semantic similarity and constructing a tweet and user network.","We used sentence-level embeddings and autoencoders to create K-means clusters of tweets and identified six groups of tweets and their topics to examine different cryptocurrency-related interests and the change in sentiment over time.","Moreover, we discovered sentiment indicators that point to real-life incidents in the crypto world, such as the FTX incident of November 2022.","We also constructed and analyzed different networks of tweets and users in our dataset by considering the reply and quote relationships and analyzed the largest components of each network.","Our networks reveal a structure of bot activity in Crypto Twitter and suggest that they can be detected and handled using a network-based approach.","Our work sheds light on the potential of social media signals to detect and understand crypto events, benefiting investors, regulators, and curious observers alike, as well as the potential for bot detection in Crypto Twitter using a network-based approach."],"url":"http://arxiv.org/abs/2403.06036v1","category":"cs.CE"}
{"created":"2024-03-09 22:41:33","title":"FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition","abstract":"Machine learning requires defining one's target variable for predictions or decisions, a process that can have profound implications on fairness: biases are often encoded in target variable definition itself, before any data collection or training. We present an interactive simulator, FairTargetSim (FTS), that illustrates how target variable definition impacts fairness. FTS is a valuable tool for algorithm developers, researchers, and non-technical stakeholders. FTS uses a case study of algorithmic hiring, using real-world data and user-defined target variables. FTS is open-source and available at: http://tinyurl.com/ftsinterface. The video accompanying this paper is here: http://tinyurl.com/ijcaifts.","sentences":["Machine learning requires defining one's target variable for predictions or decisions, a process that can have profound implications on fairness: biases are often encoded in target variable definition itself, before any data collection or training.","We present an interactive simulator, FairTargetSim (FTS), that illustrates how target variable definition impacts fairness.","FTS is a valuable tool for algorithm developers, researchers, and non-technical stakeholders.","FTS uses a case study of algorithmic hiring, using real-world data and user-defined target variables.","FTS is open-source and available at: http://tinyurl.com/ftsinterface.","The video accompanying this paper is here: http://tinyurl.com/ijcaifts."],"url":"http://arxiv.org/abs/2403.06031v1","category":"cs.LG"}
{"created":"2024-03-09 22:28:46","title":"Towards a Generic Representation of Cominatorial Problems for Learning-Based Approaches","abstract":"In recent years, there has been a growing interest in using learning-based approaches for solving combinatorial problems, either in an end-to-end manner or in conjunction with traditional optimization algorithms. In both scenarios, the challenge lies in encoding the targeted combinatorial problems into a structure compatible with the learning algorithm. Many existing works have proposed problem-specific representations, often in the form of a graph, to leverage the advantages of \\textit{graph neural networks}. However, these approaches lack generality, as the representation cannot be easily transferred from one combinatorial problem to another one. While some attempts have been made to bridge this gap, they still offer a partial generality only. In response to this challenge, this paper advocates for progress toward a fully generic representation of combinatorial problems for learning-based approaches. The approach we propose involves constructing a graph by breaking down any constraint of a combinatorial problem into an abstract syntax tree and expressing relationships (e.g., a variable involved in a constraint) through the edges. Furthermore, we introduce a graph neural network architecture capable of efficiently learning from this representation. The tool provided operates on combinatorial problems expressed in the XCSP3 format, handling all the constraints available in the 2023 mini-track competition. Experimental results on four combinatorial problems demonstrate that our architecture achieves performance comparable to dedicated architectures while maintaining generality. Our code and trained models are publicly available at \\url{https://github.com/corail-research/learning-generic-csp}.","sentences":["In recent years, there has been a growing interest in using learning-based approaches for solving combinatorial problems, either in an end-to-end manner or in conjunction with traditional optimization algorithms.","In both scenarios, the challenge lies in encoding the targeted combinatorial problems into a structure compatible with the learning algorithm.","Many existing works have proposed problem-specific representations, often in the form of a graph, to leverage the advantages of \\textit{graph neural networks}.","However, these approaches lack generality, as the representation cannot be easily transferred from one combinatorial problem to another one.","While some attempts have been made to bridge this gap, they still offer a partial generality only.","In response to this challenge, this paper advocates for progress toward a fully generic representation of combinatorial problems for learning-based approaches.","The approach we propose involves constructing a graph by breaking down any constraint of a combinatorial problem into an abstract syntax tree and expressing relationships (e.g., a variable involved in a constraint) through the edges.","Furthermore, we introduce a graph neural network architecture capable of efficiently learning from this representation.","The tool provided operates on combinatorial problems expressed in the XCSP3 format, handling all the constraints available in the 2023 mini-track competition.","Experimental results on four combinatorial problems demonstrate that our architecture achieves performance comparable to dedicated architectures while maintaining generality.","Our code and trained models are publicly available at \\url{https://github.com/corail-research/learning-generic-csp}."],"url":"http://arxiv.org/abs/2403.06026v1","category":"cs.LG"}
{"created":"2024-03-09 22:25:14","title":"CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming","abstract":"We introduce a new approach using computer vision to predict the land surface displacement from subsurface geometry images for Carbon Capture and Sequestration (CCS). CCS has been proved to be a key component for a carbon neutral society. However, scientists see there are challenges along the way including the high computational cost due to the large model scale and limitations to generalize a pre-trained model with complex physics. We tackle those challenges by training models directly from the subsurface geometry images. The goal is to understand the respons of land surface displacement due to carbon injection and utilize our trained models to inform decision making in CCS projects.   We implement multiple models (CNN, ResNet, and ResNetUNet) for static mechanics problem, which is a image prediction problem. Next, we use the LSTM and transformer for transient mechanics scenario, which is a video prediction problem. It shows ResNetUNet outperforms the others thanks to its architecture in static mechanics problem, and LSTM shows comparable performance to transformer in transient problem. This report proceeds by outlining our dataset in detail followed by model descriptions in method section. Result and discussion state the key learning, observations, and conclusion with future work rounds out the paper.","sentences":["We introduce a new approach using computer vision to predict the land surface displacement from subsurface geometry images for Carbon Capture and Sequestration (CCS).","CCS has been proved to be a key component for a carbon neutral society.","However, scientists see there are challenges along the way including the high computational cost due to the large model scale and limitations to generalize a pre-trained model with complex physics.","We tackle those challenges by training models directly from the subsurface geometry images.","The goal is to understand the respons of land surface displacement due to carbon injection and utilize our trained models to inform decision making in CCS projects.   ","We implement multiple models (CNN, ResNet, and ResNetUNet) for static mechanics problem, which is a image prediction problem.","Next, we use the LSTM and transformer for transient mechanics scenario, which is a video prediction problem.","It shows ResNetUNet outperforms the others thanks to its architecture in static mechanics problem, and LSTM shows comparable performance to transformer in transient problem.","This report proceeds by outlining our dataset in detail followed by model descriptions in method section.","Result and discussion state the key learning, observations, and conclusion with future work rounds out the paper."],"url":"http://arxiv.org/abs/2403.06025v1","category":"cs.CV"}
{"created":"2024-03-09 21:55:55","title":"Hierarchical Query Classification in E-commerce Search","abstract":"E-commerce platforms typically store and structure product information and search data in a hierarchy. Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification.   To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss to discern fine-grained instance relationships within the hierarchy, called ''instance hierarchy'', and (ii) a nuanced hierarchical classification loss that attends to the intrinsic label taxonomy, named ''label hierarchy''. Additionally, based on our observation that certain unlabeled queries share typographical similarities with labeled queries, we propose a neighborhood-aware sampling technique to intelligently select these unlabeled queries to boost the classification performance. Extensive experiments demonstrate that our proposed method is better than state-of-the-art (SOTA) on the proprietary Amazon dataset, and comparable to SOTA on the public datasets of Web of Science and RCV1-V2. These results underscore the efficacy of our proposed solution, and pave the path toward the next generation of hierarchy-aware query classification systems.","sentences":["E-commerce platforms typically store and structure product information and search data in a hierarchy.","Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research.","The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts.","The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification.   ","To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss to discern fine-grained instance relationships within the hierarchy, called ''instance hierarchy'', and (ii) a nuanced hierarchical classification loss that attends to the intrinsic label taxonomy, named ''label hierarchy''.","Additionally, based on our observation that certain unlabeled queries share typographical similarities with labeled queries, we propose a neighborhood-aware sampling technique to intelligently select these unlabeled queries to boost the classification performance.","Extensive experiments demonstrate that our proposed method is better than state-of-the-art (SOTA) on the proprietary Amazon dataset, and comparable to SOTA on the public datasets of Web of Science and RCV1-V2.","These results underscore the efficacy of our proposed solution, and pave the path toward the next generation of hierarchy-aware query classification systems."],"url":"http://arxiv.org/abs/2403.06021v1","category":"cs.IR"}
{"created":"2024-03-09 21:36:13","title":"Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages","abstract":"Large pre-trained language models (PLMs) are at the forefront of advances in Natural Language Processing. One widespread use case of PLMs is \"prompting\" - or in-context learning - where a user provides a description of a task and some completed examples of the task to a PLM as context before prompting the PLM to perform the task on a new example. Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind. The data limitations in most languages preclude the training of language-specific PLMs capable of prompting. Albeit the surge in work of prompting settings, it is still unclear how PLMs should be adapted cross-lingually specifically for prompting. We evaluate the possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, namely for Kinyarwanda, Hausa, and Luganda. We consider three methods: few-shot prompting (prompt), language-adaptive fine-tuning (LAFT), and neural machine translation (translate), and evaluate on abstractive summarization, multi-class topic classification, and named-entity recognition. Although LAFT carries the greatest compute cost and intuitively should lead to the best results, our experiments exhibit that LAFT is only occasionally the optimal choice for adapting PLMs for prompting. Rather, the translate and prompt settings are a compute-efficient and cost-effective method of few-shot prompting for the selected low-resource languages. We find that the results are task and language dependent but find that the prompting method is the best on average across all tasks and languages. Results show that the prompt setting performs better than both translating and LAFT with statistical significance for all shots when aggregated across all tasks and languages.","sentences":["Large pre-trained language models (PLMs) are at the forefront of advances in Natural Language Processing.","One widespread use case of PLMs is \"prompting\" - or in-context learning - where a user provides a description of a task and some completed examples of the task to a PLM as context before prompting the PLM to perform the task on a new example.","Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind.","The data limitations in most languages preclude the training of language-specific PLMs capable of prompting.","Albeit the surge in work of prompting settings, it is still unclear how PLMs should be adapted cross-lingually specifically for prompting.","We evaluate the possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, namely for Kinyarwanda, Hausa, and Luganda.","We consider three methods: few-shot prompting (prompt), language-adaptive fine-tuning (LAFT), and neural machine translation (translate), and evaluate on abstractive summarization, multi-class topic classification, and named-entity recognition.","Although LAFT carries the greatest compute cost and intuitively should lead to the best results, our experiments exhibit that LAFT is only occasionally the optimal choice for adapting PLMs for prompting.","Rather, the translate and prompt settings are a compute-efficient and cost-effective method of few-shot prompting for the selected low-resource languages.","We find that the results are task and language dependent but find that the prompting method is the best on average across all tasks and languages.","Results show that the prompt setting performs better than both translating and LAFT with statistical significance for all shots when aggregated across all tasks and languages."],"url":"http://arxiv.org/abs/2403.06018v1","category":"cs.CL"}
{"created":"2024-03-09 21:33:26","title":"Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark","abstract":"Fair graph learning plays a pivotal role in numerous practical applications. Recently, many fair graph learning methods have been proposed; however, their evaluation often relies on poorly constructed semi-synthetic datasets or substandard real-world datasets. In such cases, even a basic Multilayer Perceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utility and fairness. In this work, we illustrate that many datasets fail to provide meaningful information in the edges, which may challenge the necessity of using graph structures in these problems. To address these issues, we develop and introduce a collection of synthetic, semi-synthetic, and real-world datasets that fulfill a broad spectrum of requirements. These datasets are thoughtfully designed to include relevant graph structures and bias information crucial for the fair evaluation of models. The proposed synthetic and semi-synthetic datasets offer the flexibility to create data with controllable bias parameters, thereby enabling the generation of desired datasets with user-defined bias values with ease. Moreover, we conduct systematic evaluations of these proposed datasets and establish a unified evaluation approach for fair graph learning models. Our extensive experimental results with fair graph learning methods across our datasets demonstrate their effectiveness in benchmarking the performance of these methods. Our datasets and the code for reproducing our experiments are available at https://github.com/XweiQ/Benchmark-GraphFairness.","sentences":["Fair graph learning plays a pivotal role in numerous practical applications.","Recently, many fair graph learning methods have been proposed; however, their evaluation often relies on poorly constructed semi-synthetic datasets or substandard real-world datasets.","In such cases, even a basic Multilayer Perceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utility and fairness.","In this work, we illustrate that many datasets fail to provide meaningful information in the edges, which may challenge the necessity of using graph structures in these problems.","To address these issues, we develop and introduce a collection of synthetic, semi-synthetic, and real-world datasets that fulfill a broad spectrum of requirements.","These datasets are thoughtfully designed to include relevant graph structures and bias information crucial for the fair evaluation of models.","The proposed synthetic and semi-synthetic datasets offer the flexibility to create data with controllable bias parameters, thereby enabling the generation of desired datasets with user-defined bias values with ease.","Moreover, we conduct systematic evaluations of these proposed datasets and establish a unified evaluation approach for fair graph learning models.","Our extensive experimental results with fair graph learning methods across our datasets demonstrate their effectiveness in benchmarking the performance of these methods.","Our datasets and the code for reproducing our experiments are available at https://github.com/XweiQ/Benchmark-GraphFairness."],"url":"http://arxiv.org/abs/2403.06017v1","category":"cs.LG"}
{"created":"2024-03-09 21:26:22","title":"Hard-label based Small Query Black-box Adversarial Attack","abstract":"We consider the hard label based black box adversarial attack setting which solely observes predicted classes from the target model. Most of the attack methods in this setting suffer from impractical number of queries required to achieve a successful attack. One approach to tackle this drawback is utilising the adversarial transferability between white box surrogate models and black box target model. However, the majority of the methods adopting this approach are soft label based to take the full advantage of zeroth order optimisation. Unlike mainstream methods, we propose a new practical setting of hard label based attack with an optimisation process guided by a pretrained surrogate model. Experiments show the proposed method significantly improves the query efficiency of the hard label based black-box attack across various target model architectures. We find the proposed method achieves approximately 5 times higher attack success rate compared to the benchmarks, especially at the small query budgets as 100 and 250.","sentences":["We consider the hard label based black box adversarial attack setting which solely observes predicted classes from the target model.","Most of the attack methods in this setting suffer from impractical number of queries required to achieve a successful attack.","One approach to tackle this drawback is utilising the adversarial transferability between white box surrogate models and black box target model.","However, the majority of the methods adopting this approach are soft label based to take the full advantage of zeroth order optimisation.","Unlike mainstream methods, we propose a new practical setting of hard label based attack with an optimisation process guided by a pretrained surrogate model.","Experiments show the proposed method significantly improves the query efficiency of the hard label based black-box attack across various target model architectures.","We find the proposed method achieves approximately 5 times higher attack success rate compared to the benchmarks, especially at the small query budgets as 100 and 250."],"url":"http://arxiv.org/abs/2403.06014v1","category":"cs.LG"}
{"created":"2024-03-09 20:32:17","title":"A Generalized Acquisition Function for Preference-based Reward Learning","abstract":"Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic environment, an assistive robotics environment with domain transfer, and a natural language processing problem with real datasets demonstrate the superior performance of our querying method over the state-of-the-art information gain method.","sentences":["Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task.","Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency.","The information gain criterion focuses on precisely identifying all parameters of the reward function.","This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks.","Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar.","We introduce a tractable framework that can capture such definitions of similarity.","Our experiments in a synthetic environment, an assistive robotics environment with domain transfer, and a natural language processing problem with real datasets demonstrate the superior performance of our querying method over the state-of-the-art information gain method."],"url":"http://arxiv.org/abs/2403.06003v1","category":"cs.RO"}
{"created":"2024-03-09 20:05:12","title":"Time-dependent droplet detachment behaviour from wettability-engineered fibers during fog harvesting","abstract":"Water collection from natural and industrial fogs has recently been viewed as a viable freshwater source. An interesting outgrowth of the relevant research as focused on arresting of the drift losses (un-evaporated and re-condensed water droplets present in the exhaust plume from industrial cooling towers. Such exploits in fog collection have implemented metal and polyester meshes as fog water collectors (FWC). Fog droplets impinge and deposit on mesh fibers. They coalesce with previously deposited liquid to evolve as larger drops before detaching from the fibers under their own weight, an event largely dependent on the mesh fiber wettability, diameter and its arrangement relative to the fog flow. To better estimate drainage and hence collection from these fibers, the study, focuses on droplet detachment from differently wetted horizontally positioned cylindrical fibers of various diameters, placed orthogonally in the path of an oncoming fog. Droplet detachment volume is found to increase with fiber diameter and fiber surface wettability. Interestingly, in a typical fogging condition, the detachment volume is also found to exhibit a time-dependent behaviour, altering the droplet detachment criteria otherwise predicted from emulation. Our current study sheds light on this unexplored phenomenon.","sentences":["Water collection from natural and industrial fogs has recently been viewed as a viable freshwater source.","An interesting outgrowth of the relevant research as focused on arresting of the drift losses (un-evaporated and re-condensed water droplets present in the exhaust plume from industrial cooling towers.","Such exploits in fog collection have implemented metal and polyester meshes as fog water collectors (FWC).","Fog droplets impinge and deposit on mesh fibers.","They coalesce with previously deposited liquid to evolve as larger drops before detaching from the fibers under their own weight, an event largely dependent on the mesh fiber wettability, diameter and its arrangement relative to the fog flow.","To better estimate drainage and hence collection from these fibers, the study, focuses on droplet detachment from differently wetted horizontally positioned cylindrical fibers of various diameters, placed orthogonally in the path of an oncoming fog.","Droplet detachment volume is found to increase with fiber diameter and fiber surface wettability.","Interestingly, in a typical fogging condition, the detachment volume is also found to exhibit a time-dependent behaviour, altering the droplet detachment criteria otherwise predicted from emulation.","Our current study sheds light on this unexplored phenomenon."],"url":"http://arxiv.org/abs/2403.05997v1","category":"physics.flu-dyn"}
{"created":"2024-03-09 19:56:40","title":"Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence","abstract":"We show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we dissect the phenomena underlying the primacy bias. We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios, show its efficacy on the widely used dm_control suite, and obtain strong performance on the challenging dog tasks, competitive with model-based approaches. Our results question, in parts, the prior explanation for sub-optimal learning due to overfitting on early data.","sentences":["We show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples.","Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn.","In this work, we dissect the phenomena underlying the primacy bias.","We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation.","Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum.","We employ a simple unit-ball normalization that enables learning under large update ratios, show its efficacy on the widely used dm_control suite, and obtain strong performance on the challenging dog tasks, competitive with model-based approaches.","Our results question, in parts, the prior explanation for sub-optimal learning due to overfitting on early data."],"url":"http://arxiv.org/abs/2403.05996v1","category":"cs.LG"}
{"created":"2024-03-09 18:12:24","title":"Deep Reinforcement Learning Enhanced Rate-Splitting Multiple Access for Interference Mitigation","abstract":"This study explores the application of the rate-splitting multiple access (RSMA) technique, vital for interference mitigation in modern communication systems. It investigates the use of precoding methods in RSMA, especially in complex multiple-antenna interference channels, employing deep reinforcement learning. The aim is to optimize precoders and power allocation for common and private data streams involving multiple decision-makers. A multi-agent deep deterministic policy gradient (MADDPG) framework is employed to address this complexity, where decentralized agents collectively learn to optimize actions in a continuous policy space. We also explore the challenges posed by imperfect channel side information at the transmitter. Additionally, decoding order estimation is addressed to determine the optimal decoding sequence for common and private data sequences. Simulation results demonstrate the effectiveness of the proposed RSMA method based on MADDPG, achieving the upper bound in single-antenna scenarios and closely approaching theoretical limits in multi-antenna scenarios. Comparative analysis shows superiority over other techniques such as MADDPG without rate-splitting, maximal ratio transmission (MRT), zero-forcing (ZF), and leakage-based precoding methods. These findings highlight the potential of deep reinforcement learning-driven RSMA in reducing interference and enhancing system performance in communication systems.","sentences":["This study explores the application of the rate-splitting multiple access (RSMA) technique, vital for interference mitigation in modern communication systems.","It investigates the use of precoding methods in RSMA, especially in complex multiple-antenna interference channels, employing deep reinforcement learning.","The aim is to optimize precoders and power allocation for common and private data streams involving multiple decision-makers.","A multi-agent deep deterministic policy gradient (MADDPG) framework is employed to address this complexity, where decentralized agents collectively learn to optimize actions in a continuous policy space.","We also explore the challenges posed by imperfect channel side information at the transmitter.","Additionally, decoding order estimation is addressed to determine the optimal decoding sequence for common and private data sequences.","Simulation results demonstrate the effectiveness of the proposed RSMA method based on MADDPG, achieving the upper bound in single-antenna scenarios and closely approaching theoretical limits in multi-antenna scenarios.","Comparative analysis shows superiority over other techniques such as MADDPG without rate-splitting, maximal ratio transmission (MRT), zero-forcing (ZF), and leakage-based precoding methods.","These findings highlight the potential of deep reinforcement learning-driven RSMA in reducing interference and enhancing system performance in communication systems."],"url":"http://arxiv.org/abs/2403.05974v1","category":"cs.IT"}
{"created":"2024-03-09 17:46:24","title":"Calibrating Large Language Models Using Their Generations Only","abstract":"As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important. However, finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge. We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively in terms of calibration error for white-box and black-box LLMs on closed-book question-answering to detect incorrect LLM answers.","sentences":["As large language models (LLMs) are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model's confidence in its prediction becomes even more important.","However, finding effective ways to calibrate LLMs - especially when the only interface to the models is their generated text - remains a challenge.","We propose APRICOT (auxiliary prediction of confidence targets):","A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone.","This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the language generation, and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence.","We show how our approach performs competitively in terms of calibration error for white-box and black-box LLMs on closed-book question-answering to detect incorrect LLM answers."],"url":"http://arxiv.org/abs/2403.05973v1","category":"cs.CL"}
{"created":"2024-03-09 17:24:36","title":"Sample Size Selection under an Infill Asymptotic Domain","abstract":"Experimental studies often fail to appropriately account for the number of collected samples within a fixed time interval for functional responses. Data of this nature appropriately falls under an Infill Asymptotic domain that is constrained by time and not considered infinite. Therefore, the sample size should account for this infill asymptotic domain. This paper provides general guidance on selecting an appropriate size for an experimental study for various simple linear regression models and tuning parameter values of the covariance structure used under an asymptotic domain, an Ornstein-Uhlenbeck process. Selecting an appropriate sample size is determined based on the percent of total variation that is captured at any given sample size for each parameter. Additionally, guidance on the selection of the tuning parameter is given by linking this value to the signal-to-noise ratio utilized for power calculations under design of experiments.","sentences":["Experimental studies often fail to appropriately account for the number of collected samples within a fixed time interval for functional responses.","Data of this nature appropriately falls under an Infill Asymptotic domain that is constrained by time and not considered infinite.","Therefore, the sample size should account for this infill asymptotic domain.","This paper provides general guidance on selecting an appropriate size for an experimental study for various simple linear regression models and tuning parameter values of the covariance structure used under an asymptotic domain, an Ornstein-Uhlenbeck process.","Selecting an appropriate sample size is determined based on the percent of total variation that is captured at any given sample size for each parameter.","Additionally, guidance on the selection of the tuning parameter is given by linking this value to the signal-to-noise ratio utilized for power calculations under design of experiments."],"url":"http://arxiv.org/abs/2403.05969v1","category":"stat.ME"}
{"created":"2024-03-09 17:17:56","title":"On various notions of distance between subalgebras of operator algebras","abstract":"Given any irreducible inclusion $\\mB \\subset \\mA$ of unital $C^*$-algebras with a finite-index conditional expectation $E: \\mA \\to \\mB$, we show that the set of $E$-compatible intermediate $C^*$-subalgebras is finite, thereby generalizing a finiteness result of Ino and Watatani (from \\cite{IW}). A finiteness result for a certain collection of intermediate $C^*$-subalgebras of a non-irreducible inclusion of simple unital $C^*$-algebras is also obtained, which provides a $C^*$-version of a finiteness result of Khoshkam and Mashood (from \\cite{KM}).   Apart from these finiteness results, comparisons between various notions of distance between subalgebras of operator algebras by Kadison-Kastler, Christensen and Mashood-Taylor are made. Further, these comparisons are used satisfactorily to provide some concrete calculations of distance between operator algebras associated to two distinct subgroups of a given discrete group.","sentences":["Given any irreducible inclusion $\\mB \\subset \\mA$ of unital $C^*$-algebras with a finite-index conditional expectation $E: \\mA \\to \\mB$, we show that the set of $E$-compatible intermediate $C^*$-subalgebras is finite, thereby generalizing a finiteness result of Ino and Watatani (from \\cite{IW}).","A finiteness result for a certain collection of intermediate $C^*$-subalgebras of a non-irreducible inclusion of simple unital $C^*$-algebras is also obtained, which provides a $C^*$-version of a finiteness result of Khoshkam and Mashood (from \\cite{KM}).   ","Apart from these finiteness results, comparisons between various notions of distance between subalgebras of operator algebras by Kadison-Kastler, Christensen and Mashood-Taylor are made.","Further, these comparisons are used satisfactorily to provide some concrete calculations of distance between operator algebras associated to two distinct subgroups of a given discrete group."],"url":"http://arxiv.org/abs/2403.05967v1","category":"math.OA"}
{"created":"2024-03-09 17:09:53","title":"RadCloud: Real-Time High-Resolution Point Cloud Generation Using Low-Cost Radars for Aerial and Ground Vehicles","abstract":"In this work, we present RadCloud, a novel real time framework for directly obtaining higher-resolution lidar-like 2D point clouds from low-resolution radar frames on resource-constrained platforms commonly used in unmanned aerial and ground vehicles (UAVs and UGVs, respectively); such point clouds can then be used for accurate environmental mapping, navigating unknown environments, and other robotics tasks. While high-resolution sensing using radar data has been previously reported, existing methods cannot be used on most UAVs, which have limited computational power and energy; thus, existing demonstrations focus on offline radar processing. RadCloud overcomes these challenges by using a radar configuration with 1/4th of the range resolution and employing a deep learning model with 2.25x fewer parameters. Additionally, RadCloud utilizes a novel chirp-based approach that makes obtained point clouds resilient to rapid movements (e.g., aggressive turns or spins), which commonly occur during UAV flights. In real-world experiments, we demonstrate the accuracy and applicability of RadCloud on commercially available UAVs and UGVs, with off-the-shelf radar platforms on-board.","sentences":["In this work, we present RadCloud, a novel real time framework for directly obtaining higher-resolution lidar-like 2D point clouds from low-resolution radar frames on resource-constrained platforms commonly used in unmanned aerial and ground vehicles (UAVs and UGVs, respectively); such point clouds can then be used for accurate environmental mapping, navigating unknown environments, and other robotics tasks.","While high-resolution sensing using radar data has been previously reported, existing methods cannot be used on most UAVs, which have limited computational power and energy; thus, existing demonstrations focus on offline radar processing.","RadCloud overcomes these challenges by using a radar configuration with 1/4th of the range resolution and employing a deep learning model with 2.25x fewer parameters.","Additionally, RadCloud utilizes a novel chirp-based approach that makes obtained point clouds resilient to rapid movements (e.g., aggressive turns or spins), which commonly occur during UAV flights.","In real-world experiments, we demonstrate the accuracy and applicability of RadCloud on commercially available UAVs and UGVs, with off-the-shelf radar platforms on-board."],"url":"http://arxiv.org/abs/2403.05964v1","category":"cs.RO"}
{"created":"2024-03-09 16:39:37","title":"What Motivates People to Trust 'AI' Systems?","abstract":"Companies, organizations, and governments across the world are eager to employ so-called 'AI' (artificial intelligence) technology in a broad range of different products and systems. The promise of this cause c\\'el\\`ebre is that the technologies offer increased automation, efficiency, and productivity - meanwhile, critics sound warnings of illusions of objectivity, pollution of our information ecosystems, and reproduction of biases and discriminatory outcomes. This paper explores patterns of motivation in the general population for trusting (or distrusting) 'AI' systems. Based on a survey with more than 450 respondents from more than 30 different countries (and about 3000 open text answers), this paper presents a qualitative analysis of current opinions and thoughts about 'AI' technology, focusing on reasons for trusting such systems. The different reasons are synthesized into four rationales (lines of reasoning): the Human favoritism rationale, the Black box rationale, the OPSEC rationale, and the 'Wicked world, tame computers' rationale. These rationales provide insights into human motivation for trusting 'AI' which could be relevant for developers and designers of such systems, as well as for scholars developing measures of trust in technological systems.","sentences":["Companies, organizations, and governments across the world are eager to employ so-called 'AI' (artificial intelligence) technology in a broad range of different products and systems.","The promise of this cause c\\'el\\`ebre is that the technologies offer increased automation, efficiency, and productivity - meanwhile, critics sound warnings of illusions of objectivity, pollution of our information ecosystems, and reproduction of biases and discriminatory outcomes.","This paper explores patterns of motivation in the general population for trusting (or distrusting) 'AI' systems.","Based on a survey with more than 450 respondents from more than 30 different countries (and about 3000 open text answers), this paper presents a qualitative analysis of current opinions and thoughts about 'AI' technology, focusing on reasons for trusting such systems.","The different reasons are synthesized into four rationales (lines of reasoning): the Human favoritism rationale, the Black box rationale, the OPSEC rationale, and the 'Wicked world, tame computers' rationale.","These rationales provide insights into human motivation for trusting 'AI' which could be relevant for developers and designers of such systems, as well as for scholars developing measures of trust in technological systems."],"url":"http://arxiv.org/abs/2403.05957v1","category":"cs.HC"}
{"created":"2024-03-09 16:05:31","title":"Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A GRU LSTM Hybrid Approach","abstract":"Accurate classification of objects in 3D point clouds is a significant problem in several applications, such as autonomous navigation and augmented/virtual reality scenarios, which has become a research hot spot. In this paper, we presented a deep learning strategy for 3D object classification in augmented reality. The proposed approach is a combination of the GRU and LSTM. LSTM networks learn longer dependencies well, but due to the number of gates, it takes longer to train; on the other hand, GRU networks have a weaker performance than LSTM, but their training speed is much higher than GRU, which is The speed is due to its fewer gates. The proposed approach used the combination of speed and accuracy of these two networks. The proposed approach achieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes eight classes (unlabeled, man-made terrain, natural terrain, high vegetation, low vegetation, buildings, hardscape, scanning artifacts, cars). Meanwhile, the traditional machine learning approaches could achieve a maximum accuracy of 0.9489 in the best case. Keywords: Point Cloud Classification, Virtual Reality, Hybrid Model, GRULSTM, GRU, LSTM","sentences":["Accurate classification of objects in 3D point clouds is a significant problem in several applications, such as autonomous navigation and augmented/virtual reality scenarios, which has become a research hot spot.","In this paper, we presented a deep learning strategy for 3D object classification in augmented reality.","The proposed approach is a combination of the GRU and LSTM.","LSTM networks learn longer dependencies well, but due to the number of gates, it takes longer to train; on the other hand, GRU networks have a weaker performance than LSTM, but their training speed is much higher than GRU, which is The speed is due to its fewer gates.","The proposed approach used the combination of speed and accuracy of these two networks.","The proposed approach achieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes eight classes (unlabeled, man-made terrain, natural terrain, high vegetation, low vegetation, buildings, hardscape, scanning artifacts, cars).","Meanwhile, the traditional machine learning approaches could achieve a maximum accuracy of 0.9489 in the best case.","Keywords: Point Cloud Classification, Virtual Reality, Hybrid Model, GRULSTM, GRU, LSTM"],"url":"http://arxiv.org/abs/2403.05950v1","category":"cs.CV"}
{"created":"2024-03-09 14:57:03","title":"Learned 3D volumetric recovery of clouds and its uncertainty for climate analysis","abstract":"Significant uncertainty in climate prediction and cloud physics is tied to observational gaps relating to shallow scattered clouds. Addressing these challenges requires remote sensing of their three-dimensional (3D) heterogeneous volumetric scattering content. This calls for passive scattering computed tomography (CT). We design a learning-based model (ProbCT) to achieve CT of such clouds, based on noisy multi-view spaceborne images. ProbCT infers - for the first time - the posterior probability distribution of the heterogeneous extinction coefficient, per 3D location. This yields arbitrary valuable statistics, e.g., the 3D field of the most probable extinction and its uncertainty. ProbCT uses a neural-field representation, making essentially real-time inference. ProbCT undergoes supervised training by a new labeled multi-class database of physics-based volumetric fields of clouds and their corresponding images. To improve out-of-distribution inference, we incorporate self-supervised learning through differential rendering. We demonstrate the approach in simulations and on real-world data, and indicate the relevance of 3D recovery and uncertainty to precipitation and renewable energy.","sentences":["Significant uncertainty in climate prediction and cloud physics is tied to observational gaps relating to shallow scattered clouds.","Addressing these challenges requires remote sensing of their three-dimensional (3D) heterogeneous volumetric scattering content.","This calls for passive scattering computed tomography (CT).","We design a learning-based model (ProbCT) to achieve CT of such clouds, based on noisy multi-view spaceborne images.","ProbCT infers - for the first time - the posterior probability distribution of the heterogeneous extinction coefficient, per 3D location.","This yields arbitrary valuable statistics, e.g., the 3D field of the most probable extinction and its uncertainty.","ProbCT uses a neural-field representation, making essentially real-time inference.","ProbCT undergoes supervised training by a new labeled multi-class database of physics-based volumetric fields of clouds and their corresponding images.","To improve out-of-distribution inference, we incorporate self-supervised learning through differential rendering.","We demonstrate the approach in simulations and on real-world data, and indicate the relevance of 3D recovery and uncertainty to precipitation and renewable energy."],"url":"http://arxiv.org/abs/2403.05932v1","category":"cs.CV"}
{"created":"2024-03-09 14:04:06","title":"OntoChat: a Framework for Conversational Ontology Engineering using Language Models","abstract":"Ontology engineering (OE) in large projects poses a number of challenges arising from the heterogeneous backgrounds of the various stakeholders, domain experts, and their complex interactions with ontology designers. This multi-party interaction often creates systematic ambiguities and biases from the elicitation of ontology requirements, which directly affect the design, evaluation and may jeopardise the target reuse. Meanwhile, current OE methodologies strongly rely on manual activities (e.g., interviews, discussion pages). After collecting evidence on the most crucial OE activities, we introduce OntoChat, a framework for conversational ontology engineering that supports requirement elicitation, analysis, and testing. By interacting with a conversational agent, users can steer the creation of user stories and the extraction of competency questions, while receiving computational support to analyse the overall requirements and test early versions of the resulting ontologies. We evaluate OntoChat by replicating the engineering of the Music Meta Ontology, and collecting preliminary metrics on the effectiveness of each component from users. We release all code at https://github.com/King-s-Knowledge-Graph-Lab/OntoChat.","sentences":["Ontology engineering (OE) in large projects poses a number of challenges arising from the heterogeneous backgrounds of the various stakeholders, domain experts, and their complex interactions with ontology designers.","This multi-party interaction often creates systematic ambiguities and biases from the elicitation of ontology requirements, which directly affect the design, evaluation and may jeopardise the target reuse.","Meanwhile, current OE methodologies strongly rely on manual activities (e.g., interviews, discussion pages).","After collecting evidence on the most crucial OE activities, we introduce OntoChat, a framework for conversational ontology engineering that supports requirement elicitation, analysis, and testing.","By interacting with a conversational agent, users can steer the creation of user stories and the extraction of competency questions, while receiving computational support to analyse the overall requirements and test early versions of the resulting ontologies.","We evaluate OntoChat by replicating the engineering of the Music Meta Ontology, and collecting preliminary metrics on the effectiveness of each component from users.","We release all code at https://github.com/King-s-Knowledge-Graph-Lab/OntoChat."],"url":"http://arxiv.org/abs/2403.05921v1","category":"cs.AI"}
{"created":"2024-03-09 14:02:59","title":"High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models","abstract":"Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology. The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods. Over the past thirty years, progress toward making high throughput phenotyping feasible. In this study, we demonstrate that a large language model and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy. Large language models will likely emerge as the preferred method for high throughput deep phenotyping of physician notes.","sentences":["Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology.","The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods.","Over the past thirty years, progress toward making high throughput phenotyping feasible.","In this study, we demonstrate that a large language model and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy.","Large language models will likely emerge as the preferred method for high throughput deep phenotyping of physician notes."],"url":"http://arxiv.org/abs/2403.05920v1","category":"cs.CL"}
{"created":"2024-03-09 14:01:04","title":"SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data","abstract":"In the field of data mining and machine learning, commonly used classification models cannot effectively learn in unbalanced data. In order to balance the data distribution before model training,oversamplingmethods are often used to generate data for a small number of classes to solve the problem of classifying unbalanced data. Most of the classical oversampling methods are based on theSMOTE technique, which only focuses on the local information of the data, and therefore the generated data may have the problem of not being realistic enough. In the current oversampling methods based on generative networks, the methods based on GANs can capture the true distribution of data, but there is the problem of pattern collapse and training instability in training; in the oversampling methods based on denoising diffusion probability models, the neural network of the inverse diffusion process using the U-Net is not applicable to tabular data, and although the MLP can be used to replace the U-Net, the problem exists due to the simplicity of the structure and the poor effect of removing noise. problem of poor noise removal. In order to overcome the above problems, we propose a novel oversampling method SEMRes-DDPM.In the SEMRes?DDPM backward diffusion process, a new neural network structure SEMST-ResNet is used, which is suitable for tabular data and has good noise removal effect, and it can generate tabular data with higher quality. Experiments show that the SEMResNet network removes noise better than MLP; SEMRes?DDPM generates data distributions that are closer to the real data distributions than TabDDPM with CWGAN-GP; on 20 real unbalanced tabular datasets with 9 classification models, SEMRes-DDPM improves the quality of the generated tabular data in terms of three evaluation metrics (F1, G-mean, AUC) with better classification performance than other SOTA oversampling methods.","sentences":["In the field of data mining and machine learning, commonly used classification models cannot effectively learn in unbalanced data.","In order to balance the data distribution before model training,oversamplingmethods are often used to generate data for a small number of classes to solve the problem of classifying unbalanced data.","Most of the classical oversampling methods are based on theSMOTE technique, which only focuses on the local information of the data, and therefore the generated data may have the problem of not being realistic enough.","In the current oversampling methods based on generative networks, the methods based on GANs can capture the true distribution of data, but there is the problem of pattern collapse and training instability in training; in the oversampling methods based on denoising diffusion probability models, the neural network of the inverse diffusion process using the U-Net is not applicable to tabular data, and although the MLP can be used to replace the U-Net, the problem exists due to the simplicity of the structure and the poor effect of removing noise.","problem of poor noise removal.","In order to overcome the above problems, we propose a novel oversampling method SEMRes-DDPM.In the SEMRes?DDPM backward diffusion process, a new neural network structure SEMST-ResNet is used, which is suitable for tabular data and has good noise removal effect, and it can generate tabular data with higher quality.","Experiments show that the SEMResNet network removes noise better than MLP; SEMRes?DDPM generates data distributions that are closer to the real data distributions than TabDDPM with CWGAN-GP; on 20 real unbalanced tabular datasets with 9 classification models, SEMRes-DDPM improves the quality of the generated tabular data in terms of three evaluation metrics (F1, G-mean, AUC) with better classification performance than other SOTA oversampling methods."],"url":"http://arxiv.org/abs/2403.05918v1","category":"cs.LG"}
{"created":"2024-03-09 13:56:25","title":"GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing","abstract":"Multimodal language models (MLMs) are designed to process and integrate information from multiple sources, such as text, speech, images, and videos. Despite its success in language understanding, it is critical to evaluate the performance of downstream tasks for better human-centric applications. This paper assesses the application of MLMs with 5 crucial abilities for affective computing, spanning from visual affective tasks and reasoning tasks. The results show that GPT4 has high accuracy in facial action unit recognition and micro-expression detection while its general facial expression recognition performance is not accurate. We also highlight the challenges of achieving fine-grained micro-expression recognition and the potential for further study and demonstrate the versatility and potential of GPT4 for handling advanced tasks in emotion recognition and related fields by integrating with task-related agents for more complex tasks, such as heart rate estimation through signal processing. In conclusion, this paper provides valuable insights into the potential applications and challenges of MLMs in human-centric computing. The interesting samples are available at \\url{https://github.com/LuPaoPao/GPT4Affectivity}.","sentences":["Multimodal language models (MLMs) are designed to process and integrate information from multiple sources, such as text, speech, images, and videos.","Despite its success in language understanding, it is critical to evaluate the performance of downstream tasks for better human-centric applications.","This paper assesses the application of MLMs with 5 crucial abilities for affective computing, spanning from visual affective tasks and reasoning tasks.","The results show that GPT4 has high accuracy in facial action unit recognition and micro-expression detection while its general facial expression recognition performance is not accurate.","We also highlight the challenges of achieving fine-grained micro-expression recognition and the potential for further study and demonstrate the versatility and potential of GPT4 for handling advanced tasks in emotion recognition and related fields by integrating with task-related agents for more complex tasks, such as heart rate estimation through signal processing.","In conclusion, this paper provides valuable insights into the potential applications and challenges of MLMs in human-centric computing.","The interesting samples are available at \\url{https://github.com/LuPaoPao/GPT4Affectivity}."],"url":"http://arxiv.org/abs/2403.05916v1","category":"cs.CV"}
{"created":"2024-03-09 13:30:00","title":"Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning","abstract":"As AI assistance is increasingly infused into decision-making processes, we may seek to optimize human-centric objectives beyond decision accuracy, such as skill improvement or task enjoyment of individuals interacting with these systems. With this aspiration in mind, we propose offline reinforcement learning (RL) as a general approach for modeling human-AI decision-making to optimize such human-centric objectives. Our approach seeks to optimize different objectives by adaptively providing decision support to humans -- the right type of assistance, to the right person, at the right time. We instantiate our approach with two objectives: human-AI accuracy on the decision-making task and human learning about the task, and learn policies that optimize these two objectives from previous human-AI interaction data. We compare the optimized policies against various baselines in AI-assisted decision-making. Across two experiments (N = 316 and N = 964), our results consistently demonstrate that people interacting with policies optimized for accuracy achieve significantly better accuracy -- and even human-AI complementarity -- compared to those interacting with any other type of AI support. Our results further indicate that human learning is more difficult to optimize than accuracy, with participants who interacted with learning-optimized policies showing significant learning improvement only at times. Our research (1) demonstrates offline RL to be a promising approach to model dynamics of human-AI decision-making, leading to policies that may optimize various human-centric objectives and provide novel insights about the AI-assisted decision-making space, and (2) emphasizes the importance of considering human-centric objectives beyond decision accuracy in AI-assisted decision-making, while also opening up the novel research challenge of optimizing such objectives.","sentences":["As AI assistance is increasingly infused into decision-making processes, we may seek to optimize human-centric objectives beyond decision accuracy, such as skill improvement or task enjoyment of individuals interacting with these systems.","With this aspiration in mind, we propose offline reinforcement learning (RL) as a general approach for modeling human-AI decision-making to optimize such human-centric objectives.","Our approach seeks to optimize different objectives by adaptively providing decision support to humans -- the right type of assistance, to the right person, at the right time.","We instantiate our approach with two objectives: human-AI accuracy on the decision-making task and human learning about the task, and learn policies that optimize these two objectives from previous human-AI interaction data.","We compare the optimized policies against various baselines in AI-assisted decision-making.","Across two experiments (N = 316 and N = 964), our results consistently demonstrate that people interacting with policies optimized for accuracy achieve significantly better accuracy -- and even human-AI complementarity -- compared to those interacting with any other type of AI support.","Our results further indicate that human learning is more difficult to optimize than accuracy, with participants who interacted with learning-optimized policies showing significant learning improvement only at times.","Our research (1) demonstrates offline RL to be a promising approach to model dynamics of human-AI decision-making, leading to policies that may optimize various human-centric objectives and provide novel insights about the AI-assisted decision-making space, and (2) emphasizes the importance of considering human-centric objectives beyond decision accuracy in AI-assisted decision-making, while also opening up the novel research challenge of optimizing such objectives."],"url":"http://arxiv.org/abs/2403.05911v1","category":"cs.HC"}
{"created":"2024-03-09 12:10:50","title":"Stacked Intelligent Metasurface Enabled LEO Satellite Communications Relying on Statistical CSI","abstract":"Low earth orbit (LEO) satellite communication systems have gained increasing attention as a crucial supplement to terrestrial wireless networks due to their extensive coverage area. This letter presents a novel system design for LEO satellite systems by leveraging stacked intelligent metasurface (SIM) technology. Specifically, the lightweight and energy-efficient SIM is mounted on a satellite to achieve multiuser beamforming directly in the electromagnetic wave domain, which substantially reduces the processing delay and computational load of the satellite compared to the traditional digital beamforming scheme. To overcome the challenges of obtaining instantaneous channel state information (CSI) at the transmitter and maximize the system's performance, a joint power allocation and SIM phase shift optimization problem for maximizing the ergodic sum rate is formulated based on statistical CSI, and an alternating optimization (AO) algorithm is customized to solve it efficiently. Additionally, a user grouping method based on channel correlation and an antenna selection algorithm are proposed to further improve the system performance. Simulation results demonstrate the effectiveness of the proposed SIM-based LEO satellite system design and statistical CSI-based AO algorithm.","sentences":["Low earth orbit (LEO) satellite communication systems have gained increasing attention as a crucial supplement to terrestrial wireless networks due to their extensive coverage area.","This letter presents a novel system design for LEO satellite systems by leveraging stacked intelligent metasurface (SIM) technology.","Specifically, the lightweight and energy-efficient SIM is mounted on a satellite to achieve multiuser beamforming directly in the electromagnetic wave domain, which substantially reduces the processing delay and computational load of the satellite compared to the traditional digital beamforming scheme.","To overcome the challenges of obtaining instantaneous channel state information (CSI) at the transmitter and maximize the system's performance, a joint power allocation and SIM phase shift optimization problem for maximizing the ergodic sum rate is formulated based on statistical CSI, and an alternating optimization (AO) algorithm is customized to solve it efficiently.","Additionally, a user grouping method based on channel correlation and an antenna selection algorithm are proposed to further improve the system performance.","Simulation results demonstrate the effectiveness of the proposed SIM-based LEO satellite system design and statistical CSI-based AO algorithm."],"url":"http://arxiv.org/abs/2403.05892v1","category":"cs.IT"}
{"created":"2024-03-09 10:44:07","title":"Channel Estimation for Stacked Intelligent Metasurface-Assisted Wireless Networks","abstract":"Emerging technologies, such as holographic multiple-input multiple-output (HMIMO) and stacked intelligent metasurface (SIM), are driving the development of wireless communication systems. Specifically, the SIM is physically constructed by stacking multiple layers of metasurfaces and has an architecture similar to an artificial neural network (ANN), which can flexibly manipulate the electromagnetic waves that propagate through it at the speed of light. This architecture enables the SIM to achieve HMIMO precoding and combining in the wave domain, thus significantly reducing the hardware cost and energy consumption. In this letter, we investigate the channel estimation problem in SIM-assisted multi-user HMIMO communication systems. Since the number of antennas at the base station (BS) is much smaller than the number of meta-atoms per layer of the SIM, it is challenging to acquire the channel state information (CSI) in SIM-assisted multi-user systems. To address this issue, we collect multiple copies of the uplink pilot signals that propagate through the SIM. Furthermore, we leverage the array geometry to identify the subspace that spans arbitrary spatial correlation matrices. Based on partial CSI about the channel statistics, a pair of subspace-based channel estimators are proposed. Additionally, we compute the mean square error (MSE) of the proposed channel estimators and optimize the phase shifts of the SIM to minimize the MSE. Numerical results are illustrated to analyze the effectiveness of the proposed channel estimation schemes.","sentences":["Emerging technologies, such as holographic multiple-input multiple-output (HMIMO) and stacked intelligent metasurface (SIM), are driving the development of wireless communication systems.","Specifically, the SIM is physically constructed by stacking multiple layers of metasurfaces and has an architecture similar to an artificial neural network (ANN), which can flexibly manipulate the electromagnetic waves that propagate through it at the speed of light.","This architecture enables the SIM to achieve HMIMO precoding and combining in the wave domain, thus significantly reducing the hardware cost and energy consumption.","In this letter, we investigate the channel estimation problem in SIM-assisted multi-user HMIMO communication systems.","Since the number of antennas at the base station (BS) is much smaller than the number of meta-atoms per layer of the SIM, it is challenging to acquire the channel state information (CSI) in SIM-assisted multi-user systems.","To address this issue, we collect multiple copies of the uplink pilot signals that propagate through the SIM.","Furthermore, we leverage the array geometry to identify the subspace that spans arbitrary spatial correlation matrices.","Based on partial CSI about the channel statistics, a pair of subspace-based channel estimators are proposed.","Additionally, we compute the mean square error (MSE) of the proposed channel estimators and optimize the phase shifts of the SIM to minimize the MSE.","Numerical results are illustrated to analyze the effectiveness of the proposed channel estimation schemes."],"url":"http://arxiv.org/abs/2403.05870v1","category":"cs.IT"}
{"created":"2024-03-09 10:17:14","title":"DeepVM: Integrating Spot and On-Demand VMs for Cost-Efficient Deep Learning Clusters in the Cloud","abstract":"Distributed Deep Learning (DDL), as a paradigm, dictates the use of GPU-based clusters as the optimal infrastructure for training large-scale Deep Neural Networks (DNNs). However, the high cost of such resources makes them inaccessible to many users. Public cloud services, particularly Spot Virtual Machines (VMs), offer a cost-effective alternative, but their unpredictable availability poses a significant challenge to the crucial checkpointing process in DDL. To address this, we introduce DeepVM, a novel solution that recommends cost-effective cluster configurations by intelligently balancing the use of Spot and On-Demand VMs. DeepVM leverages a four-stage process that analyzes instance performance using the FLOPP (FLoating-point Operations Per Price) metric, performs architecture-level analysis with linear programming, and identifies the optimal configuration for the user-specific needs. Extensive simulations and real-world deployments in the AWS environment demonstrate that DeepVM consistently outperforms other policies, reducing training costs and overall makespan. By enabling cost-effective checkpointing with Spot VMs, DeepVM opens up DDL to a wider range of users and facilitates a more efficient training of complex DNNs.","sentences":["Distributed Deep Learning (DDL), as a paradigm, dictates the use of GPU-based clusters as the optimal infrastructure for training large-scale Deep Neural Networks (DNNs).","However, the high cost of such resources makes them inaccessible to many users.","Public cloud services, particularly Spot Virtual Machines (VMs), offer a cost-effective alternative, but their unpredictable availability poses a significant challenge to the crucial checkpointing process in DDL.","To address this, we introduce DeepVM, a novel solution that recommends cost-effective cluster configurations by intelligently balancing the use of Spot and On-Demand VMs.","DeepVM leverages a four-stage process that analyzes instance performance using the FLOPP (FLoating-point Operations Per Price) metric, performs architecture-level analysis with linear programming, and identifies the optimal configuration for the user-specific needs.","Extensive simulations and real-world deployments in the AWS environment demonstrate that DeepVM consistently outperforms other policies, reducing training costs and overall makespan.","By enabling cost-effective checkpointing with Spot VMs, DeepVM opens up DDL to a wider range of users and facilitates a more efficient training of complex DNNs."],"url":"http://arxiv.org/abs/2403.05861v1","category":"cs.DC"}
{"created":"2024-03-09 09:04:53","title":"Reverse That Number! Decoding Order Matters in Arithmetic Learning","abstract":"Recent advancements in pretraining have demonstrated that modern Large Language Models (LLMs) possess the capability to effectively learn arithmetic operations. However, despite acknowledging the significance of digit order in arithmetic computation, current methodologies predominantly rely on sequential, step-by-step approaches for teaching LLMs arithmetic, resulting in a conclusion where obtaining better performance involves fine-grained step-by-step. Diverging from this conventional path, our work introduces a novel strategy that not only reevaluates the digit order by prioritizing output from the least significant digit but also incorporates a step-by-step methodology to substantially reduce complexity. We have developed and applied this method in a comprehensive set of experiments. Compared to the previous state-of-the-art (SOTA) method, our findings reveal an overall improvement of in accuracy while requiring only a third of the tokens typically used during training. For the purpose of facilitating replication and further research, we have made our code and dataset publicly available at \\url{https://anonymous.4open.science/r/RAIT-9FB7/}.","sentences":["Recent advancements in pretraining have demonstrated that modern Large Language Models (LLMs) possess the capability to effectively learn arithmetic operations.","However, despite acknowledging the significance of digit order in arithmetic computation, current methodologies predominantly rely on sequential, step-by-step approaches for teaching LLMs arithmetic, resulting in a conclusion where obtaining better performance involves fine-grained step-by-step.","Diverging from this conventional path, our work introduces a novel strategy that not only reevaluates the digit order by prioritizing output from the least significant digit but also incorporates a step-by-step methodology to substantially reduce complexity.","We have developed and applied this method in a comprehensive set of experiments.","Compared to the previous state-of-the-art (SOTA) method, our findings reveal an overall improvement of in accuracy while requiring only a third of the tokens typically used during training.","For the purpose of facilitating replication and further research, we have made our code and dataset publicly available at \\url{https://anonymous.4open.science/r/RAIT-9FB7/}."],"url":"http://arxiv.org/abs/2403.05845v1","category":"cs.CL"}
{"created":"2024-03-09 08:54:52","title":"Hufu: A Modality-Agnositc Watermarking System for Pre-Trained Transformers via Permutation Equivariance","abstract":"With the blossom of deep learning models and services, it has become an imperative concern to safeguard the valuable model parameters from being stolen. Watermarking is considered an important tool for ownership verification. However, current watermarking schemes are customized for different models and tasks, hard to be integrated as an integrated intellectual protection service. We propose Hufu, a modality-agnostic watermarking system for pre-trained Transformer-based models, relying on the permutation equivariance property of Transformers. Hufu embeds watermark by fine-tuning the pre-trained model on a set of data samples specifically permuted, and the embedded model essentially contains two sets of weights -- one for normal use and the other for watermark extraction which is triggered on permuted inputs. The permutation equivariance ensures minimal interference between these two sets of model weights and thus high fidelity on downstream tasks. Since our method only depends on the model itself, it is naturally modality-agnostic, task-independent, and trigger-sample-free. Extensive experiments on the state-of-the-art vision Transformers, BERT, and GPT2 have demonstrated Hufu's superiority in meeting watermarking requirements including effectiveness, efficiency, fidelity, and robustness, showing its great potential to be deployed as a uniform ownership verification service for various Transformers.","sentences":["With the blossom of deep learning models and services, it has become an imperative concern to safeguard the valuable model parameters from being stolen.","Watermarking is considered an important tool for ownership verification.","However, current watermarking schemes are customized for different models and tasks, hard to be integrated as an integrated intellectual protection service.","We propose Hufu, a modality-agnostic watermarking system for pre-trained Transformer-based models, relying on the permutation equivariance property of Transformers.","Hufu embeds watermark by fine-tuning the pre-trained model on a set of data samples specifically permuted, and the embedded model essentially contains two sets of weights -- one for normal use and the other for watermark extraction which is triggered on permuted inputs.","The permutation equivariance ensures minimal interference between these two sets of model weights and thus high fidelity on downstream tasks.","Since our method only depends on the model itself, it is naturally modality-agnostic, task-independent, and trigger-sample-free.","Extensive experiments on the state-of-the-art vision Transformers, BERT, and GPT2 have demonstrated Hufu's superiority in meeting watermarking requirements including effectiveness, efficiency, fidelity, and robustness, showing its great potential to be deployed as a uniform ownership verification service for various Transformers."],"url":"http://arxiv.org/abs/2403.05842v1","category":"cs.CR"}
{"created":"2024-03-09 08:49:50","title":"Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline","abstract":"Current event-/frame-event based trackers undergo evaluation on short-term tracking datasets, however, the tracking of real-world scenarios involves long-term tracking, and the performance of existing tracking algorithms in these scenarios remains unclear. In this paper, we first propose a new long-term and large-scale frame-event single object tracking dataset, termed FELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs and has become the largest frame-event tracking dataset to date. We re-train and evaluate 15 baseline trackers on our dataset for future works to compare. More importantly, we find that the RGB frames and event streams are naturally incomplete due to the influence of challenging factors and spatially sparse event flow. In response to this, we propose a novel associative memory Transformer network as a unified backbone by introducing modern Hopfield layers into multi-head self-attention blocks to fuse both RGB and event data. Extensive experiments on both FELT and RGB-T tracking dataset LasHeR fully validated the effectiveness of our model. The dataset and source code can be found at \\url{https://github.com/Event-AHU/FELT_SOT_Benchmark}.","sentences":["Current event-/frame-event based trackers undergo evaluation on short-term tracking datasets, however, the tracking of real-world scenarios involves long-term tracking, and the performance of existing tracking algorithms in these scenarios remains unclear.","In this paper, we first propose a new long-term and large-scale frame-event single object tracking dataset, termed FELT.","It contains 742 videos and 1,594,474 RGB frames and event stream pairs and has become the largest frame-event tracking dataset to date.","We re-train and evaluate 15 baseline trackers on our dataset for future works to compare.","More importantly, we find that the RGB frames and event streams are naturally incomplete due to the influence of challenging factors and spatially sparse event flow.","In response to this, we propose a novel associative memory Transformer network as a unified backbone by introducing modern Hopfield layers into multi-head self-attention blocks to fuse both RGB and event data.","Extensive experiments on both FELT and RGB-T tracking dataset LasHeR fully validated the effectiveness of our model.","The dataset and source code can be found at \\url{https://github.com/Event-AHU/FELT_SOT_Benchmark}."],"url":"http://arxiv.org/abs/2403.05839v1","category":"cs.CV"}
{"created":"2024-03-09 08:48:54","title":"LEO- and RIS-Empowered User Tracking: A Riemannian Manifold Approach","abstract":"Low Earth orbit (LEO) satellites and reconfigurable intelligent surfaces (RISs) have recently drawn significant attention as two transformative technologies, and the synergy between them emerges as a promising paradigm for providing cross-environment communication and positioning services. This paper investigates an integrated terrestrial and non-terrestrial wireless network that leverages LEO satellites and RISs to achieve simultaneous tracking of the 3D position, 3D velocity, and 3D orientation of user equipment (UE). To address inherent challenges including nonlinear observation function, constrained UE state, and unknown observation statistics, we develop a Riemannian manifold-based unscented Kalman filter (UKF) method. This method propagates statistics over nonlinear functions using generated sigma points and maintains state constraints through projection onto the defined manifold space. Additionally, by employing Fisher information matrices (FIMs) of the sigma points, a belief assignment principle is proposed to approximate the unknown observation covariance matrix, thereby ensuring accurate measurement updates in the UKF procedure. Numerical results demonstrate a substantial enhancement in tracking accuracy facilitated by RIS integration, despite urban signal reception challenges from LEO satellites. In addition, extensive simulations underscore the superior performance of the proposed tracking method and FIM-based belief assignment over the adopted benchmarks. Furthermore, the robustness of the proposed UKF is verified across various uncertainty levels.","sentences":["Low Earth orbit (LEO) satellites and reconfigurable intelligent surfaces (RISs) have recently drawn significant attention as two transformative technologies, and the synergy between them emerges as a promising paradigm for providing cross-environment communication and positioning services.","This paper investigates an integrated terrestrial and non-terrestrial wireless network that leverages LEO satellites and RISs to achieve simultaneous tracking of the 3D position, 3D velocity, and 3D orientation of user equipment (UE).","To address inherent challenges including nonlinear observation function, constrained UE state, and unknown observation statistics, we develop a Riemannian manifold-based unscented Kalman filter (UKF) method.","This method propagates statistics over nonlinear functions using generated sigma points and maintains state constraints through projection onto the defined manifold space.","Additionally, by employing Fisher information matrices (FIMs) of the sigma points, a belief assignment principle is proposed to approximate the unknown observation covariance matrix, thereby ensuring accurate measurement updates in the UKF procedure.","Numerical results demonstrate a substantial enhancement in tracking accuracy facilitated by RIS integration, despite urban signal reception challenges from LEO satellites.","In addition, extensive simulations underscore the superior performance of the proposed tracking method and FIM-based belief assignment over the adopted benchmarks.","Furthermore, the robustness of the proposed UKF is verified across various uncertainty levels."],"url":"http://arxiv.org/abs/2403.05838v1","category":"eess.SP"}
{"created":"2024-03-09 08:29:29","title":"Research progress on intelligent optimization techniques for energy-efficient design of ship hull forms","abstract":"The design optimization of ship hull form based on hydrodynamics theory and simulation-based design (SBD) technologies generally considers ship performance and energy efficiency performance as the design objective, which plays an important role in smart design and manufacturing of green ship. An optimal design of sustainable energy system requires multidisciplinary tools to build ships with the least resistance and energy consumption. Through a systematic approach, this paper presents the research progress of energy-efficient design of ship hull forms based on intelligent optimization techniques. We discuss different methods involved in the optimization procedure, especially the latest developments of intelligent optimization algorithms and surrogate models. Moreover, current development trends and technical challenges of multidisciplinary design optimization and surrogate-assisted evolutionary algorithms for ship design are further analyzed. We explore the gaps and potential future directions, so as to paving the way towards the design of the next generation of more energy-efficient ship hull form.","sentences":["The design optimization of ship hull form based on hydrodynamics theory and simulation-based design (SBD) technologies generally considers ship performance and energy efficiency performance as the design objective, which plays an important role in smart design and manufacturing of green ship.","An optimal design of sustainable energy system requires multidisciplinary tools to build ships with the least resistance and energy consumption.","Through a systematic approach, this paper presents the research progress of energy-efficient design of ship hull forms based on intelligent optimization techniques.","We discuss different methods involved in the optimization procedure, especially the latest developments of intelligent optimization algorithms and surrogate models.","Moreover, current development trends and technical challenges of multidisciplinary design optimization and surrogate-assisted evolutionary algorithms for ship design are further analyzed.","We explore the gaps and potential future directions, so as to paving the way towards the design of the next generation of more energy-efficient ship hull form."],"url":"http://arxiv.org/abs/2403.05832v1","category":"cs.CE"}
{"created":"2024-03-11 17:59:58","title":"Imaging of I Zw 18 by JWST: I. Strategy and First Results of Dusty Stellar Populations","abstract":"We present a James Webb Space Telescope (JWST) imaging survey of I Zw 18, the archetypal extremely metal-poor, star-forming, blue compact dwarf galaxy. With an oxygen abundance of only $\\sim$3% $Z_{\\odot}$, it is among the lowest-metallicity systems known in the local universe, and is, therefore, an excellent accessible analog for the galactic building blocks which existed at early epochs of ionization and star formation. These JWST data provide a comprehensive infrared (IR) view of I Zw 18 with eight filters utilizing both NIRCam (F115W, F200W, F356W, and F444W) and MIRI (F770W, F1000W, F1500W, and F1800W) photometry, which we have used to identify key stellar populations that are bright in the near- and mid-IR. These data allow for a better understanding of the origins of dust and dust-production mechanisms in metal-poor environments by characterizing the population of massive, evolved stars in the red supergiant (RSG) and asymptotic giant branch (AGB) phases. In addition, it enables the identification of the brightest dust-enshrouded young stellar objects (YSOs), which provide insight into the formation of massive stars at extremely low metallicities typical of the very early universe. This paper provides an overview of the observational strategy and data processing, and presents first science results, including identifications of dusty AGB star, RSG, and bright YSO candidates. These first results assess the scientific quality of JWST data and provide a guide for obtaining and interpreting future observations of the dusty and evolved stars inhabiting compact dwarf star-forming galaxies in the local universe.","sentences":["We present a James Webb Space Telescope (JWST) imaging survey of I","Zw 18, the archetypal extremely metal-poor, star-forming, blue compact dwarf galaxy.","With an oxygen abundance of only $\\sim$3% $Z_{\\odot}$, it is among the lowest-metallicity systems known in the local universe, and is, therefore, an excellent accessible analog for the galactic building blocks which existed at early epochs of ionization and star formation.","These JWST data provide a comprehensive infrared (IR) view of I Zw 18 with eight filters utilizing both NIRCam (F115W, F200W, F356W, and F444W) and MIRI (F770W, F1000W, F1500W, and F1800W) photometry, which we have used to identify key stellar populations that are bright in the near- and mid-IR.","These data allow for a better understanding of the origins of dust and dust-production mechanisms in metal-poor environments by characterizing the population of massive, evolved stars in the red supergiant (RSG) and asymptotic giant branch (AGB) phases.","In addition, it enables the identification of the brightest dust-enshrouded young stellar objects (YSOs), which provide insight into the formation of massive stars at extremely low metallicities typical of the very early universe.","This paper provides an overview of the observational strategy and data processing, and presents first science results, including identifications of dusty AGB star, RSG, and bright YSO candidates.","These first results assess the scientific quality of JWST data and provide a guide for obtaining and interpreting future observations of the dusty and evolved stars inhabiting compact dwarf star-forming galaxies in the local universe."],"url":"http://arxiv.org/abs/2403.06980v1","category":"astro-ph.GA"}
{"created":"2024-03-11 17:59:41","title":"Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling","abstract":"In this paper, we introduce Attention Prompt Tuning (APT) - a computationally efficient variant of prompt tuning for video-based applications such as action recognition. Prompt tuning approaches involve injecting a set of learnable prompts along with data tokens during fine-tuning while keeping the backbone frozen. This approach greatly reduces the number of learnable parameters compared to full tuning. For image-based downstream tasks, normally a couple of learnable prompts achieve results close to those of full tuning. However, videos, which contain more complex spatiotemporal information, require hundreds of tunable prompts to achieve reasonably good results. This reduces the parameter efficiency observed in images and significantly increases latency and the number of floating-point operations (FLOPs) during inference. To tackle these issues, we directly inject the prompts into the keys and values of the non-local attention mechanism within the transformer block. Additionally, we introduce a novel prompt reparameterization technique to make APT more robust against hyperparameter selection. The proposed APT approach greatly reduces the number of FLOPs and latency while achieving a significant performance boost over the existing parameter-efficient tuning methods on UCF101, HMDB51, and SSv2 datasets for action recognition. The code and pre-trained models are available at https://github.com/wgcban/apt","sentences":["In this paper, we introduce Attention Prompt Tuning (APT) - a computationally efficient variant of prompt tuning for video-based applications such as action recognition.","Prompt tuning approaches involve injecting a set of learnable prompts along with data tokens during fine-tuning while keeping the backbone frozen.","This approach greatly reduces the number of learnable parameters compared to full tuning.","For image-based downstream tasks, normally a couple of learnable prompts achieve results close to those of full tuning.","However, videos, which contain more complex spatiotemporal information, require hundreds of tunable prompts to achieve reasonably good results.","This reduces the parameter efficiency observed in images and significantly increases latency and the number of floating-point operations (FLOPs) during inference.","To tackle these issues, we directly inject the prompts into the keys and values of the non-local attention mechanism within the transformer block.","Additionally, we introduce a novel prompt reparameterization technique to make APT more robust against hyperparameter selection.","The proposed APT approach greatly reduces the number of FLOPs and latency while achieving a significant performance boost over the existing parameter-efficient tuning methods on UCF101, HMDB51, and SSv2 datasets for action recognition.","The code and pre-trained models are available at https://github.com/wgcban/apt"],"url":"http://arxiv.org/abs/2403.06978v1","category":"cs.CV"}
{"created":"2024-03-11 17:59:34","title":"VideoMamba: State Space Model for Efficient Video Understanding","abstract":"Addressing the dual challenges of local redundancy and global dependencies in video understanding, this work innovatively adapts the Mamba to the video domain. The proposed VideoMamba overcomes the limitations of existing 3D convolution neural networks and video transformers. Its linear-complexity operator enables efficient long-term modeling, which is crucial for high-resolution long video understanding. Extensive evaluations reveal VideoMamba's four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel self-distillation technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in multi-modal contexts. Through these distinct advantages, VideoMamba sets a new benchmark for video understanding, offering a scalable and efficient solution for comprehensive video understanding. All the code and models are available at https://github.com/OpenGVLab/VideoMamba.","sentences":["Addressing the dual challenges of local redundancy and global dependencies in video understanding, this work innovatively adapts the Mamba to the video domain.","The proposed VideoMamba overcomes the limitations of existing 3D convolution neural networks and video transformers.","Its linear-complexity operator enables efficient long-term modeling, which is crucial for high-resolution long video understanding.","Extensive evaluations reveal VideoMamba's four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel self-distillation technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in multi-modal contexts.","Through these distinct advantages, VideoMamba sets a new benchmark for video understanding, offering a scalable and efficient solution for comprehensive video understanding.","All the code and models are available at https://github.com/OpenGVLab/VideoMamba."],"url":"http://arxiv.org/abs/2403.06977v1","category":"cs.CV"}
{"created":"2024-03-11 17:54:33","title":"MRL Parsing Without Tears: The Case of Hebrew","abstract":"Syntactic parsing remains a critical tool for relation extraction and information extraction, especially in resource-scarce languages where LLMs are lacking. Yet in morphologically rich languages (MRLs), where parsers need to identify multiple lexical units in each token, existing systems suffer in latency and setup complexity. Some use a pipeline to peel away the layers: first segmentation, then morphology tagging, and then syntax parsing; however, errors in earlier layers are then propagated forward. Others use a joint architecture to evaluate all permutations at once; while this improves accuracy, it is notoriously slow. In contrast, and taking Hebrew as a test case, we present a new \"flipped pipeline\": decisions are made directly on the whole-token units by expert classifiers, each one dedicated to one specific task. The classifiers are independent of one another, and only at the end do we synthesize their predictions. This blazingly fast approach sets a new SOTA in Hebrew POS tagging and dependency parsing, while also reaching near-SOTA performance on other Hebrew NLP tasks. Because our architecture does not rely on any language-specific resources, it can serve as a model to develop similar parsers for other MRLs.","sentences":["Syntactic parsing remains a critical tool for relation extraction and information extraction, especially in resource-scarce languages where LLMs are lacking.","Yet in morphologically rich languages (MRLs), where parsers need to identify multiple lexical units in each token, existing systems suffer in latency and setup complexity.","Some use a pipeline to peel away the layers: first segmentation, then morphology tagging, and then syntax parsing; however, errors in earlier layers are then propagated forward.","Others use a joint architecture to evaluate all permutations at once; while this improves accuracy, it is notoriously slow.","In contrast, and taking Hebrew as a test case, we present a new \"flipped pipeline\": decisions are made directly on the whole-token units by expert classifiers, each one dedicated to one specific task.","The classifiers are independent of one another, and only at the end do we synthesize their predictions.","This blazingly fast approach sets a new SOTA in Hebrew POS tagging and dependency parsing, while also reaching near-SOTA performance on other Hebrew NLP tasks.","Because our architecture does not rely on any language-specific resources, it can serve as a model to develop similar parsers for other MRLs."],"url":"http://arxiv.org/abs/2403.06970v1","category":"cs.CL"}
{"created":"2024-03-11 17:45:31","title":"The MODEST catalog of depth-dependent spatially coupled inversions of sunspots observed by Hinode/SOT-SP","abstract":"We present a catalog that we named MODEST containing depth-dependent information on the atmospheric conditions inside sunspot groups of all types. The catalog is currently composed of 942 observations of 117 individual active regions with sunspots that cover all types of features observed in the solar photosphere. We use the SPINOR-2D code to perform spatially coupled inversions of the Stokes profiles observed by Hinode/SOT-SP at high spatial resolution. SPINOR-2D accounts for the unavoidable degradation of the spatial information due to the point spread function of the telescope. The sunspot sample focuses on complex sunspot groups, but simple sunspots are also part of the catalog for completeness. Sunspots were observed from 2006 to 2019, covering parts of solar cycles 23 and 24. The catalog is a living resource, as with time, more sunspot groups will be included.","sentences":["We present a catalog that we named MODEST containing depth-dependent information on the atmospheric conditions inside sunspot groups of all types.","The catalog is currently composed of 942 observations of 117 individual active regions with sunspots that cover all types of features observed in the solar photosphere.","We use the SPINOR-2D code to perform spatially coupled inversions of the Stokes profiles observed by Hinode/SOT-SP at high spatial resolution.","SPINOR-2D accounts for the unavoidable degradation of the spatial information due to the point spread function of the telescope.","The sunspot sample focuses on complex sunspot groups, but simple sunspots are also part of the catalog for completeness.","Sunspots were observed from 2006 to 2019, covering parts of solar cycles 23 and 24.","The catalog is a living resource, as with time, more sunspot groups will be included."],"url":"http://arxiv.org/abs/2403.06960v1","category":"astro-ph.SR"}
{"created":"2024-03-11 17:36:44","title":"Quadruped-Frog: Rapid Online Optimization of Continuous Quadruped Jumping","abstract":"Legged robots are becoming increasingly agile in exhibiting dynamic behaviors such as running and jumping. Usually, such behaviors are either optimized and engineered offline (i.e. the behavior is designed for before it is needed), either through model-based trajectory optimization, or through deep learning-based methods involving millions of timesteps of simulation interactions. Notably, such offline-designed locomotion controllers cannot perfectly model the true dynamics of the system, such as the motor dynamics. In contrast, in this paper, we consider a quadruped jumping task that we rapidly optimize online. We design foot force profiles parameterized by only a few parameters which we optimize for directly on hardware with Bayesian Optimization. The force profiles are tracked at the joint level, and added to Cartesian PD impedance control and Virtual Model Control to stabilize the jumping motions. After optimization, which takes only a handful of jumps, we show that this control architecture is capable of diverse and omnidirectional jumps including forward, lateral, and twist (turning) jumps, even on uneven terrain, enabling the Unitree Go1 quadruped to jump 0.5 m high, 0.5 m forward, and jump-turn over 2 rad. Video results can be found at https://youtu.be/SvfVNQ90k_w.","sentences":["Legged robots are becoming increasingly agile in exhibiting dynamic behaviors such as running and jumping.","Usually, such behaviors are either optimized and engineered offline (i.e. the behavior is designed for before it is needed), either through model-based trajectory optimization, or through deep learning-based methods involving millions of timesteps of simulation interactions.","Notably, such offline-designed locomotion controllers cannot perfectly model the true dynamics of the system, such as the motor dynamics.","In contrast, in this paper, we consider a quadruped jumping task that we rapidly optimize online.","We design foot force profiles parameterized by only a few parameters which we optimize for directly on hardware with Bayesian Optimization.","The force profiles are tracked at the joint level, and added to Cartesian PD impedance control and Virtual Model Control to stabilize the jumping motions.","After optimization, which takes only a handful of jumps, we show that this control architecture is capable of diverse and omnidirectional jumps including forward, lateral, and twist (turning) jumps, even on uneven terrain, enabling the Unitree Go1 quadruped to jump 0.5 m high, 0.5 m forward, and jump-turn over 2 rad.","Video results can be found at https://youtu.be/SvfVNQ90k_w."],"url":"http://arxiv.org/abs/2403.06954v1","category":"cs.RO"}
{"created":"2024-03-11 17:26:51","title":"Comparison of Static Analysis Architecture Recovery Tools for Microservice Applications","abstract":"Architecture recovery tools help software engineers obtain an overview of their software systems during all phases of the software development lifecycle. This is especially important for microservice applications because their distributed nature makes it more challenging to oversee the architecture. Various tools and techniques for this task are presented in academic and grey literature sources. Practitioners and researchers can benefit from a comprehensive overview of these tools and their abilities. However, no such overview exists that is based on executing the identified tools and assessing their outputs regarding effectiveness. With the study described in this paper, we plan to first identify static analysis architecture recovery tools for microservice applications via a multi-vocal literature review, and then execute them on a common dataset and compare the measured effectiveness in architecture recovery. We will focus on static approaches because they are also suitable for integration into fast-paced CI/CD pipelines.","sentences":["Architecture recovery tools help software engineers obtain an overview of their software systems during all phases of the software development lifecycle.","This is especially important for microservice applications because their distributed nature makes it more challenging to oversee the architecture.","Various tools and techniques for this task are presented in academic and grey literature sources.","Practitioners and researchers can benefit from a comprehensive overview of these tools and their abilities.","However, no such overview exists that is based on executing the identified tools and assessing their outputs regarding effectiveness.","With the study described in this paper, we plan to first identify static analysis architecture recovery tools for microservice applications via a multi-vocal literature review, and then execute them on a common dataset and compare the measured effectiveness in architecture recovery.","We will focus on static approaches because they are also suitable for integration into fast-paced CI/CD pipelines."],"url":"http://arxiv.org/abs/2403.06941v1","category":"cs.SE"}
{"created":"2024-03-11 17:24:47","title":"Distributed computing quantum unitary evolution","abstract":"A distributed computing approach to solve the curse of dimensionality, caused by the complex quantum system modeling, is discussed. With the help of Cannon's algorithm, the distributed computing transformation of numerical method for simulating quantum unitary evolution is achieved. Based on the Tavis-Cummings model, a large number of atoms are added into the optical cavity to obtain a high-dimensional quantum closed system, implemented on the supercomputer platform. The comparison of time cost and speedup of different distributed computing strategies is discussed.","sentences":["A distributed computing approach to solve the curse of dimensionality, caused by the complex quantum system modeling, is discussed.","With the help of Cannon's algorithm, the distributed computing transformation of numerical method for simulating quantum unitary evolution is achieved.","Based on the Tavis-Cummings model, a large number of atoms are added into the optical cavity to obtain a high-dimensional quantum closed system, implemented on the supercomputer platform.","The comparison of time cost and speedup of different distributed computing strategies is discussed."],"url":"http://arxiv.org/abs/2403.06937v1","category":"quant-ph"}
{"created":"2024-03-11 17:19:50","title":"Impact of spin torques and spin pumping phenomena on magnon-plasmon polaritons in antiferromagnetic insulator-semiconductor heterostructures","abstract":"We investigate the impact of spin torque and spin pumping on the surface magnon polariton dispersion in a antiferromagnetic insulator-semiconductor heterostructure. In the bilayer system, the surface magnon polaritons conventionally couple to the plasma-oscillations in the semiconductor via electromagnetic fields. Additionally, magnons in the antiferromagnetic insulator layer may interact with the semiconductor layer via spin torques and their reciprocal phenomena of spin pumping. Due to the spin-to-charge conversion from the spin Hall and inverse spin Hall effects in the semiconductor layer with a strong spin-orbit coupling, this can couple the magnons to the plasmons in the semiconductor layer. Our research reveals that modifications in the mode frequency and the hybridization gap induced by these phenomena depend on the thickness of the antiferromagnetic layer. In thick layers, the spin-pumping contribution to the frequency shift and damping is inversely proportional to the wavelength, while in thin layers it is inversely proportional to the thickness. Furthermore, hybridization of the surface magnon polariton and dispersive magnons in the antiferromagnet is shown to depend on both the thickness and wavelength of the modes.","sentences":["We investigate the impact of spin torque and spin pumping on the surface magnon polariton dispersion in a antiferromagnetic insulator-semiconductor heterostructure.","In the bilayer system, the surface magnon polaritons conventionally couple to the plasma-oscillations in the semiconductor via electromagnetic fields.","Additionally, magnons in the antiferromagnetic insulator layer may interact with the semiconductor layer via spin torques and their reciprocal phenomena of spin pumping.","Due to the spin-to-charge conversion from the spin Hall and inverse spin Hall effects in the semiconductor layer with a strong spin-orbit coupling, this can couple the magnons to the plasmons in the semiconductor layer.","Our research reveals that modifications in the mode frequency and the hybridization gap induced by these phenomena depend on the thickness of the antiferromagnetic layer.","In thick layers, the spin-pumping contribution to the frequency shift and damping is inversely proportional to the wavelength, while in thin layers it is inversely proportional to the thickness.","Furthermore, hybridization of the surface magnon polariton and dispersive magnons in the antiferromagnet is shown to depend on both the thickness and wavelength of the modes."],"url":"http://arxiv.org/abs/2403.06934v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 17:18:53","title":"ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis","abstract":"Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs.","sentences":["Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks.","However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities.","These challenges arise from the presence of implicit relationships that demand multi-step reasoning.","In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT).","Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\\% on GPT3.5 compared to previous SOTA baselines.","Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs."],"url":"http://arxiv.org/abs/2403.06932v1","category":"cs.CL"}
{"created":"2024-03-11 17:17:18","title":"Heavy Ball Momentum for Non-Strongly Convex Optimization","abstract":"When considering the minimization of a quadratic or strongly convex function, it is well known that first-order methods involving an inertial term weighted by a constant-in-time parameter are particularly efficient (see Polyak [32], Nesterov [28], and references therein). By setting the inertial parameter according to the condition number of the objective function, these methods guarantee a fast exponential decay of the error. We prove that this type of schemes (which are later called Heavy Ball schemes) is relevant in a relaxed setting, i.e. for composite functions satisfying a quadratic growth condition. In particular, we adapt V-FISTA, introduced by Beck in [10] for strongly convex functions, to this broader class of functions. To the authors' knowledge, the resulting worst-case convergence rates are faster than any other in the literature, including those of FISTA restart schemes. No assumption on the set of minimizers is required and guarantees are also given in the non-optimal case, i.e. when the condition number is not exactly known. This analysis follows the study of the corresponding continuous-time dynamical system (Heavy Ball with friction system), for which new convergence results of the trajectory are shown.","sentences":["When considering the minimization of a quadratic or strongly convex function, it is well known that first-order methods involving an inertial term weighted by a constant-in-time parameter are particularly efficient (see","Polyak","[32], Nesterov","[28], and references therein).","By setting the inertial parameter according to the condition number of the objective function, these methods guarantee a fast exponential decay of the error.","We prove that this type of schemes (which are later called Heavy Ball schemes) is relevant in a relaxed setting, i.e. for composite functions satisfying a quadratic growth condition.","In particular, we adapt V-FISTA, introduced by Beck in [10] for strongly convex functions, to this broader class of functions.","To the authors' knowledge, the resulting worst-case convergence rates are faster than any other in the literature, including those of FISTA restart schemes.","No assumption on the set of minimizers is required and guarantees are also given in the non-optimal case, i.e. when the condition number is not exactly known.","This analysis follows the study of the corresponding continuous-time dynamical system (Heavy Ball with friction system), for which new convergence results of the trajectory are shown."],"url":"http://arxiv.org/abs/2403.06930v1","category":"math.OC"}
{"created":"2024-03-11 17:11:44","title":"A method for accelerating low precision operations by sparse matrix multiplication","abstract":"In recent years, the fervent demand for computational power across various domains has prompted hardware manufacturers to introduce specialized computing hardware aimed at enhancing computational capabilities. Particularly, the utilization of tensor hardware supporting low precision has gained increasing prominence in scientific research. However, the use of low-precision tensor hardware for computational acceleration often introduces errors, posing a fundamental challenge of simultaneously achieving effective acceleration while maintaining computational accuracy.   This paper proposes improvements in the methodology by incorporating low-precision quantization and employing a residual matrix for error correction and combines vector-wise quantization method.. The key innovation lies in the use of sparse matrices instead of dense matrices when compensating for errors with a residual matrix. By focusing solely on values that may significantly impact relative errors under a specified threshold, this approach aims to control quantization errors while reducing computational complexity. Experimental results demonstrate that this method can effectively control the quantization error while maintaining high acceleration effect.The improved algorithm on the CPU can achieve up to 15\\% accuracy improvement while 1.46 times speed improvement.","sentences":["In recent years, the fervent demand for computational power across various domains has prompted hardware manufacturers to introduce specialized computing hardware aimed at enhancing computational capabilities.","Particularly, the utilization of tensor hardware supporting low precision has gained increasing prominence in scientific research.","However, the use of low-precision tensor hardware for computational acceleration often introduces errors, posing a fundamental challenge of simultaneously achieving effective acceleration while maintaining computational accuracy.   ","This paper proposes improvements in the methodology by incorporating low-precision quantization and employing a residual matrix for error correction and combines vector-wise quantization method..","The key innovation lies in the use of sparse matrices instead of dense matrices when compensating for errors with a residual matrix.","By focusing solely on values that may significantly impact relative errors under a specified threshold, this approach aims to control quantization errors while reducing computational complexity.","Experimental results demonstrate that this method can effectively control the quantization error while maintaining high acceleration effect.","The improved algorithm on the CPU can achieve up to 15\\% accuracy improvement while 1.46 times speed improvement."],"url":"http://arxiv.org/abs/2403.06924v1","category":"math.NA"}
{"created":"2024-03-11 17:08:15","title":"Distributed Average Consensus via Noisy and Non-Coherent Over-the-Air Aggregation","abstract":"Over-the-air aggregation has attracted widespread attention for its potential advantages in task-oriented applications, such as distributed sensing, learning, and consensus. In this paper, we develop a communication-efficient distributed average consensus protocol by utilizing over-the-air aggregation, which exploits the superposition property of wireless channels rather than combat it. Noisy channels and non-coherent transmission are taken into account, and only half-duplex transceivers are required. We prove that the system can achieve average consensus in mean square and even almost surely under the proposed protocol. Furthermore, we extend the analysis to the scenarios with time-varying topology. Numerical simulation shows the effectiveness of the proposed protocol.","sentences":["Over-the-air aggregation has attracted widespread attention for its potential advantages in task-oriented applications, such as distributed sensing, learning, and consensus.","In this paper, we develop a communication-efficient distributed average consensus protocol by utilizing over-the-air aggregation, which exploits the superposition property of wireless channels rather than combat it.","Noisy channels and non-coherent transmission are taken into account, and only half-duplex transceivers are required.","We prove that the system can achieve average consensus in mean square and even almost surely under the proposed protocol.","Furthermore, we extend the analysis to the scenarios with time-varying topology.","Numerical simulation shows the effectiveness of the proposed protocol."],"url":"http://arxiv.org/abs/2403.06920v1","category":"eess.SP"}
{"created":"2024-03-11 17:06:22","title":"Memory in cyclically crumpled sheets","abstract":"We investigate the crumpling of a sheet as it is repeatedly crushed onto itself by rolling it into a cylinder and twisting it axially while allowing the end-to-end length to evolve freely. As deduced from its plastic deformations, the sheet creases and collapses into structures which repeat and sharpen over hundreds of cycles to a remarkable degree before forming new configurations. The observed metastablilty increases with applied cycles leading to recurrent structures over a significant range of loading, but reconfigurations can continue to occur for large enough loading as the creases develop tears. The evolution of the sheet structure as measured by the mean curvature and the total crease length is found to increase logarithmically with cycle number with a rate which increases with degree of compression. We explain the overall extent of creasing using flat folding models, and show the logarithmic growth as being a consequence of individual creases becoming sharper with number of folding cycles, and due to the bifurcation in the curvature field leading to the formation of new creases and folding pathways. Thus, we show that elastoplastic sheets can follow complex folding pathways to form convergent structures after a sufficiently large number of training cycles provided material fatigue remains unimportant.","sentences":["We investigate the crumpling of a sheet as it is repeatedly crushed onto itself by rolling it into a cylinder and twisting it axially while allowing the end-to-end length to evolve freely.","As deduced from its plastic deformations, the sheet creases and collapses into structures which repeat and sharpen over hundreds of cycles to a remarkable degree before forming new configurations.","The observed metastablilty increases with applied cycles leading to recurrent structures over a significant range of loading, but reconfigurations can continue to occur for large enough loading as the creases develop tears.","The evolution of the sheet structure as measured by the mean curvature and the total crease length is found to increase logarithmically with cycle number with a rate which increases with degree of compression.","We explain the overall extent of creasing using flat folding models, and show the logarithmic growth as being a consequence of individual creases becoming sharper with number of folding cycles, and due to the bifurcation in the curvature field leading to the formation of new creases and folding pathways.","Thus, we show that elastoplastic sheets can follow complex folding pathways to form convergent structures after a sufficiently large number of training cycles provided material fatigue remains unimportant."],"url":"http://arxiv.org/abs/2403.06918v1","category":"cond-mat.soft"}
{"created":"2024-03-11 17:04:04","title":"Monitoring the Venice Lagoon: an IoT Cloud-Based Sensor Nerwork Approach","abstract":"Monitoring the coastal area of the Venice Lagoon is of significant importance. While the impact of global warming is felt worldwide, coastal and littoral regions bear the brunt more prominently. These areas not only face the threat of rising sea levels but also contend with the escalating occurrence of seaquakes and floods. Additionally, the intricate ecosystems of rivers, seas, and lakes undergo profound transformations due to climate change and pollutants.   Employing devices like the SENSWICH floating wireless sensor presented in this article and similar measurement instruments proves invaluable to automate environmental monitoring, hence eliminating the need for manual sampling campaigns. The utilization of wireless measurement devices offers cost-effectiveness, real-time analysis, and a reduction in human resource requirements. Storing data in cloud services further enhances the ability to monitor parameter changes over extended time intervals.   In this article, we present an enhanced sensing device aimed at automating water quality assessment, while considering power consumption and reducing circuit complexity. Specifically, we will introduce the new schematic and circuit of SENSWICH which had changes in circuit and electronic aspects. Furthermore, we outline the methodology for aggregating data in a cloud service environment, such as Amazon Web Service (AWS), and using Grafana for visualization.","sentences":["Monitoring the coastal area of the Venice Lagoon is of significant importance.","While the impact of global warming is felt worldwide, coastal and littoral regions bear the brunt more prominently.","These areas not only face the threat of rising sea levels but also contend with the escalating occurrence of seaquakes and floods.","Additionally, the intricate ecosystems of rivers, seas, and lakes undergo profound transformations due to climate change and pollutants.   ","Employing devices like the SENSWICH floating wireless sensor presented in this article and similar measurement instruments proves invaluable to automate environmental monitoring, hence eliminating the need for manual sampling campaigns.","The utilization of wireless measurement devices offers cost-effectiveness, real-time analysis, and a reduction in human resource requirements.","Storing data in cloud services further enhances the ability to monitor parameter changes over extended time intervals.   ","In this article, we present an enhanced sensing device aimed at automating water quality assessment, while considering power consumption and reducing circuit complexity.","Specifically, we will introduce the new schematic and circuit of SENSWICH which had changes in circuit and electronic aspects.","Furthermore, we outline the methodology for aggregating data in a cloud service environment, such as Amazon Web Service (AWS), and using Grafana for visualization."],"url":"http://arxiv.org/abs/2403.06915v1","category":"cs.NI"}
{"created":"2024-03-11 16:47:09","title":"Application of Quantum Tensor Networks for Protein Classification","abstract":"We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems. We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences. We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results. We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve the binary classification task mentioned above. Our top-performing quantum model has achieved a 94% accuracy rate, which is comparable to the performance of a classical model that uses the ESM2 protein language model embeddings. It's noteworthy that the ESM2 model is extremely large, containing 8 million parameters in its smallest configuration, whereas our best quantum model requires only around 800 parameters. We demonstrate that these hybrid models exhibit promising performance, showcasing their potential to compete with classical models of similar complexity.","sentences":["We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems.","We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms.","Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences.","We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results.","We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve the binary classification task mentioned above.","Our top-performing quantum model has achieved a 94% accuracy rate, which is comparable to the performance of a classical model that uses the ESM2 protein language model embeddings.","It's noteworthy that the ESM2 model is extremely large, containing 8 million parameters in its smallest configuration, whereas our best quantum model requires only around 800 parameters.","We demonstrate that these hybrid models exhibit promising performance, showcasing their potential to compete with classical models of similar complexity."],"url":"http://arxiv.org/abs/2403.06890v1","category":"quant-ph"}
{"created":"2024-03-11 16:42:46","title":"Model Predictive Control Strategies for Electric Endurance Race Cars Accounting for Competitors Interactions","abstract":"This paper presents model predictive control strategies for battery electric endurance race cars accounting for interactions with the competitors. In particular, we devise an optimization framework capturing the impact of the actions of the ego vehicle when interacting with competitors in a probabilistic fashion, jointly accounting for the optimal pit stop decision making, the charge times and the driving style in the course of the race. We showcase our method for a simulated 1h endurance race at the Zandvoort circuit, using real-life data of internal combustion engine race cars from a previous event. Our results show that optimizing both the race strategy as well as the decision making during the race is very important, resulting in a significant 21s advantage over an always overtake approach, whilst revealing the competitiveness of e-race cars w.r.t. conventional ones.","sentences":["This paper presents model predictive control strategies for battery electric endurance race cars accounting for interactions with the competitors.","In particular, we devise an optimization framework capturing the impact of the actions of the ego vehicle when interacting with competitors in a probabilistic fashion, jointly accounting for the optimal pit stop decision making, the charge times and the driving style in the course of the race.","We showcase our method for a simulated 1h endurance race at the Zandvoort circuit, using real-life data of internal combustion engine race cars from a previous event.","Our results show that optimizing both the race strategy as well as the decision making during the race is very important, resulting in a significant 21s advantage over an always overtake approach, whilst revealing the competitiveness of e-race cars w.r.t. conventional ones."],"url":"http://arxiv.org/abs/2403.06885v1","category":"eess.SY"}
{"created":"2024-03-11 16:42:28","title":"Rates of convergence for holomorphic semigroups of finite shift","abstract":"We study parabolic semigroups of finite shift in the unit disk with regard to the rate of convergence of their orbits to the Denjoy--Wolff point. We examine this rate in terms of Euclidean distance, hyperbolic distance and harmonic measure. In each case, we provide explicit examples to display the sharpness of the results. We further discuss the corresponding rates of convergence for parabolic semigroups of positive hyperbolic step and infinite shift.","sentences":["We study parabolic semigroups of finite shift in the unit disk with regard to the rate of convergence of their orbits to the Denjoy--Wolff point.","We examine this rate in terms of Euclidean distance, hyperbolic distance and harmonic measure.","In each case, we provide explicit examples to display the sharpness of the results.","We further discuss the corresponding rates of convergence for parabolic semigroups of positive hyperbolic step and infinite shift."],"url":"http://arxiv.org/abs/2403.06883v1","category":"math.CV"}
{"created":"2024-03-11 16:38:01","title":"Algebraic Bethe ansatz approach to the correlation functions of the one-dimensional bosons with attraction","abstract":"We consider a model of a one-dimensional Bose gas with attraction. We study ground state equal-time correlation functions in this model using the algebraic Bethe ansatz. In cases of strong interaction or/and large-volume systems, we obtain very simple explicit formulas for correlations.","sentences":["We consider a model of a one-dimensional Bose gas with attraction.","We study ground state equal-time correlation functions in this model using the algebraic Bethe ansatz.","In cases of strong interaction or/and large-volume systems, we obtain very simple explicit formulas for correlations."],"url":"http://arxiv.org/abs/2403.06882v1","category":"math-ph"}
{"created":"2024-03-11 16:30:58","title":"Hierarchical Cutting of Complex Networks Performed by Random Walks","abstract":"Several interesting approaches have been reported in the literature on complex networks, random walks, and hierarchy of graphs. While many of these works perform random walks on stable, fixed networks, in the present work we address the situation in which the connections traversed by each step of a uniformly random walks are progressively removed, yielding a successively less interconnected structure that may break into two components, therefore establishing a respective hierarchy. The sizes of each of these pairs of sliced networks, as well as the permanence of each connected component, are studied in the present work. Several interesting results are reported, including the tendency of geometrical networks sometimes to be broken into two components with comparable large sizes.","sentences":["Several interesting approaches have been reported in the literature on complex networks, random walks, and hierarchy of graphs.","While many of these works perform random walks on stable, fixed networks, in the present work we address the situation in which the connections traversed by each step of a uniformly random walks are progressively removed, yielding a successively less interconnected structure that may break into two components, therefore establishing a respective hierarchy.","The sizes of each of these pairs of sliced networks, as well as the permanence of each connected component, are studied in the present work.","Several interesting results are reported, including the tendency of geometrical networks sometimes to be broken into two components with comparable large sizes."],"url":"http://arxiv.org/abs/2403.06876v1","category":"cs.SI"}
{"created":"2024-03-11 16:26:53","title":"Ising model with non-reciprocal interactions","abstract":"Effective interactions that violate Newton's third law of action-reaction symmetry are common in systems where interactions are mediated by a non-equilibrium environment. Extensive Monte Carlo simulations are carried out on a two-dimensional Ising model, where the interactions are modified non-reciprocally. We demonstrate that the critical temperature decreases as the non-reciprocity increases and this decrease depends only on the magnitude of non-reciprocity. Further, travelling spin waves due to the local fluctuations in magnetisation are observed and these spin waves travel opposite to the non-reciprocity vector.","sentences":["Effective interactions that violate Newton's third law of action-reaction symmetry are common in systems where interactions are mediated by a non-equilibrium environment.","Extensive Monte Carlo simulations are carried out on a two-dimensional Ising model, where the interactions are modified non-reciprocally.","We demonstrate that the critical temperature decreases as the non-reciprocity increases and this decrease depends only on the magnitude of non-reciprocity.","Further, travelling spin waves due to the local fluctuations in magnetisation are observed and these spin waves travel opposite to the non-reciprocity vector."],"url":"http://arxiv.org/abs/2403.06875v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-11 16:16:55","title":"Rigid Poisson suspensions without roots","abstract":"Examples of rigid Poisson suspensions without roots are presented. The discrete rational component in spectrum of an ergodic automorphism S prevents some roots from existing. If S is tensorly multiplied by an ergodic automorphism of the space with a sigma-finite measure, discrete spectrum disappears in this product, but the memory of it can remain in the form of the absence of roots. In additional conditions, this effect is inherited by the Poisson suspension over the product.","sentences":["Examples of rigid Poisson suspensions without roots are presented.","The discrete rational component in spectrum of an ergodic automorphism S prevents some roots from existing.","If S is tensorly multiplied by an ergodic automorphism of the space with a sigma-finite measure, discrete spectrum disappears in this product, but the memory of it can remain in the form of the absence of roots.","In additional conditions, this effect is inherited by the Poisson suspension over the product."],"url":"http://arxiv.org/abs/2403.06864v1","category":"math.DS"}
{"created":"2024-03-11 16:16:48","title":"Single sided multiplier Hopf algebras","abstract":"Let $A$ be a non-degenerate algebra over the complex numbers and $\\Delta$ a homomorphism from $A$ to the multiplier algebra $M(A\\otimes A)$. Consider the linear maps $T_1$ and $T_2$ from $A\\otimes A$ to $M(A\\otimes A)$ defined by \\begin{equation*} T_1(a\\otimes b)=\\Delta(a)(1\\otimes b) \\qquad\\text{and}\\qquad T_2(c\\otimes a)=(c\\otimes 1)\\Delta(a). \\end{equation*} The pair $(A,\\Delta)$ is a multiplier Hopf algebra if these two maps have range in $A\\otimes A$ and are bijections from $A\\otimes A$ to itself. In our recent paper on the Larson-Sweedler theorem, single sided multiplier Hopf algebras emerge in a natural way. For this case, instead of requiring the above for the maps $T_1$ and $T_2$, we now have this property for the maps $T_1$ and $T_4$ or for $T_2$ and $T_3$ where \\begin{equation*} T_3(a\\otimes b)=(1\\otimes b)\\Delta(a) \\qquad\\text{and}\\qquad T_4(c\\otimes a)=\\Delta(a)(c\\otimes 1). \\end{equation*} As it turns out, also for these single sided multiplier Hopf algebras, the existence of a unique counit and antipode can be proven. In fact, rather surprisingly, using the properties of the antipode, one can actually show that for a single sided multiplier Hopf algebra all four canonical maps are bijections from $A\\otimes A$ to itself. In other words, $(A,\\Delta)$ is automatically a regular multiplier Hopf algebra. We take the advantage of this approach to reconsider some of the known results for a regular multiplier Hopf algebra.","sentences":["Let $A$ be a non-degenerate algebra over the complex numbers and $\\Delta$ a homomorphism from $A$ to the multiplier algebra $M(A\\otimes A)$. Consider the linear maps $T_1$ and $T_2$ from $A\\otimes A$ to $M(A\\otimes A)$ defined by \\begin{equation*} T_1(a\\otimes b)=\\Delta(a)(1\\otimes b) \\qquad\\text{and}\\qquad T_2(c\\otimes a)=(c\\otimes 1)\\Delta(a).","\\end{equation*} The pair $(A,\\Delta)$ is a multiplier Hopf algebra if these two maps have range in $A\\otimes A$ and are bijections from $A\\otimes A$ to itself.","In our recent paper on the Larson-Sweedler theorem, single sided multiplier Hopf algebras emerge in a natural way.","For this case, instead of requiring the above for the maps $T_1$ and $T_2$, we now have this property for the maps $T_1$ and $T_4$ or for $T_2$ and $T_3$ where \\begin{equation*} T_3(a\\otimes b)=(1\\otimes b)\\Delta(a) \\qquad\\text{and}\\qquad T_4(c\\otimes a)=\\Delta(a)(c\\otimes 1).","\\end{equation*} As it turns out, also for these single sided multiplier Hopf algebras, the existence of a unique counit and antipode can be proven.","In fact, rather surprisingly, using the properties of the antipode, one can actually show that for a single sided multiplier Hopf algebra all four canonical maps are bijections from $A\\otimes A$ to itself.","In other words, $(A,\\Delta)$ is automatically a regular multiplier Hopf algebra.","We take the advantage of this approach to reconsider some of the known results for a regular multiplier Hopf algebra."],"url":"http://arxiv.org/abs/2403.06863v1","category":"math.RA"}
{"created":"2024-03-11 16:13:58","title":"A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa","abstract":"Desert locust swarms present a major threat to agriculture and food security. Addressing this challenge, our study develops an operationally-ready model for predicting locust breeding grounds, which has the potential to enhance early warning systems and targeted control measures. We curated a dataset from the United Nations Food and Agriculture Organization's (UN-FAO) locust observation records and analyzed it using two types of spatio-temporal input features: remotely-sensed environmental and climate data as well as multi-spectral earth observation images. Our approach employed custom deep learning models (three-dimensional and LSTM-based recurrent convolutional networks), along with the geospatial foundational model Prithvi recently released by Jakubik et al., 2023. These models notably outperformed existing baselines, with the Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding from our research is that multi-spectral earth observation images alone are sufficient for effective locust breeding ground prediction without the need to explicitly incorporate climatic or environmental features.","sentences":["Desert locust swarms present a major threat to agriculture and food security.","Addressing this challenge, our study develops an operationally-ready model for predicting locust breeding grounds, which has the potential to enhance early warning systems and targeted control measures.","We curated a dataset from the United Nations Food and Agriculture Organization's (UN-FAO) locust observation records and analyzed it using two types of spatio-temporal input features: remotely-sensed environmental and climate data as well as multi-spectral earth observation images.","Our approach employed custom deep learning models (three-dimensional and LSTM-based recurrent convolutional networks), along with the geospatial foundational model Prithvi recently released by Jakubik et al., 2023.","These models notably outperformed existing baselines, with the Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively).","A significant finding from our research is that multi-spectral earth observation images alone are sufficient for effective locust breeding ground prediction without the need to explicitly incorporate climatic or environmental features."],"url":"http://arxiv.org/abs/2403.06860v1","category":"cs.LG"}
{"created":"2024-03-11 16:09:39","title":"Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification","abstract":"Inverse reinforcement learning (IRL) aims to infer an agent's preferences (represented as a reward function $R$) from their behaviour (represented as a policy $\\pi$). To do this, we need a behavioural model of how $\\pi$ relates to $R$. In the current literature, the most common behavioural models are optimality, Boltzmann-rationality, and causal entropy maximisation. However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models. This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data. In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model. Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error above a given threshold. In addition to this, we also characterise the conditions under which a behavioural model is robust to small perturbations of the observed policy, and we analyse how robust many behavioural models are to misspecification of their parameter values (such as e.g.\\ the discount rate). Our analysis suggests that the IRL problem is highly sensitive to misspecification, in the sense that very mild misspecification can lead to very large errors in the inferred reward function.","sentences":["Inverse reinforcement learning (IRL) aims to infer an agent's preferences (represented as a reward function $R$) from their behaviour (represented as a policy $\\pi$).","To do this, we need a behavioural model of how $\\pi$ relates to $R$. In the current literature, the most common behavioural models are optimality, Boltzmann-rationality, and causal entropy maximisation.","However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models.","This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data.","In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model.","Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error above a given threshold.","In addition to this, we also characterise the conditions under which a behavioural model is robust to small perturbations of the observed policy, and we analyse how robust many behavioural models are to misspecification of their parameter values (such as e.g.\\ the discount rate).","Our analysis suggests that the IRL problem is highly sensitive to misspecification, in the sense that very mild misspecification can lead to very large errors in the inferred reward function."],"url":"http://arxiv.org/abs/2403.06854v1","category":"cs.LG"}
{"created":"2024-03-11 16:04:18","title":"SonoTraceLab - A Raytracing-Based Acoustic Modelling System for Simulating Echolocation Behavior of Bats","abstract":"Echolocation is the prime sensing modality for many species of bats, who show the intricate ability to perform a plethora of tasks in complex and unstructured environments. Understanding this exceptional feat of sensorimotor interaction is a key aspect into building more robust and performant man-made sonar sensors. In order to better understand the underlying perception mechanisms it is important to get a good insight into the nature of the reflected signals that the bat perceives. While ensonification experiments are in important way to better understand the nature of these signals, they are as time-consuming to perform as they are informative. In this paper we present SonoTraceLab, an open-source software package for simulating both technical as well as biological sonar systems in complex scenes. Using simulation approaches can drastically increase insights into the nature of biological echolocation systems, while reducing the time- and material complexity of performing them.","sentences":["Echolocation is the prime sensing modality for many species of bats, who show the intricate ability to perform a plethora of tasks in complex and unstructured environments.","Understanding this exceptional feat of sensorimotor interaction is a key aspect into building more robust and performant man-made sonar sensors.","In order to better understand the underlying perception mechanisms it is important to get a good insight into the nature of the reflected signals that the bat perceives.","While ensonification experiments are in important way to better understand the nature of these signals, they are as time-consuming to perform as they are informative.","In this paper we present SonoTraceLab, an open-source software package for simulating both technical as well as biological sonar systems in complex scenes.","Using simulation approaches can drastically increase insights into the nature of biological echolocation systems, while reducing the time- and material complexity of performing them."],"url":"http://arxiv.org/abs/2403.06847v1","category":"eess.AS"}
{"created":"2024-03-11 16:02:01","title":"Hybrid optimal control with mixed-integer Lagrangian methods","abstract":"Models involving hybrid systems are versatile in their application, but difficult to handle and optimize efficiently due to their combinatorial nature. This work presents a method to cope with hybrid optimal control problems which, in contrast to decomposition techniques, does not require relaxing the integrality constraints. Based on the discretize-then-optimize approach, our scheme addresses mixed-integer nonlinear problems under mild assumptions. The proposed numerical algorithm builds upon the augmented Lagrangian framework, whose subproblems are handled using successive mixed-integer linearizations with trust regions. We validate the performance of the numerical routine with extensive investigations using several hybrid optimal control problems from different fields of application. Promising preliminary results are presented for a motion planning task with hysteresis and drag, a Lotka-Volterra fishing problem, and a facility location design problem.","sentences":["Models involving hybrid systems are versatile in their application, but difficult to handle and optimize efficiently due to their combinatorial nature.","This work presents a method to cope with hybrid optimal control problems which, in contrast to decomposition techniques, does not require relaxing the integrality constraints.","Based on the discretize-then-optimize approach, our scheme addresses mixed-integer nonlinear problems under mild assumptions.","The proposed numerical algorithm builds upon the augmented Lagrangian framework, whose subproblems are handled using successive mixed-integer linearizations with trust regions.","We validate the performance of the numerical routine with extensive investigations using several hybrid optimal control problems from different fields of application.","Promising preliminary results are presented for a motion planning task with hysteresis and drag, a Lotka-Volterra fishing problem, and a facility location design problem."],"url":"http://arxiv.org/abs/2403.06842v1","category":"math.OC"}
{"created":"2024-03-11 16:00:58","title":"Advanced Channel Coding Designs for Index-Modulated Fluid Antenna Systems","abstract":"The concept of fluid antennas (FAs) has emerged as a promising solution to enhance the spectral efficiency of wireless networks, achieved by introducing additional degrees of freedom, including reconfigurability and flexibility. In this paper, we investigate the use of index-modulated (IM) transmissions within the framework of FA systems, where an FA position is activated during each transmission interval. This approach is motivated by the common characteristics exhibited by FAs and IM transmissions, which entails the use of a single radio-frequency chain. From this perspective, we derive a closed-form expression for the bit error rate of IM-FAs considering spatial correlation, and demonstrating superior performance compared to conventional IM systems. To enhance the performance of IM-FAs in correlated conditions, channel coding techniques are applied. We first analyze a set partition coding (SPC) scheme for IM-FAs to spatially separate the FA ports, and provide a tight performance bound over correlated channels. Furthermore, the spatial SPC scheme is extended to turbo-coded modulation where the performance is analyzed for low and high signal-to-noise ratios. Our results reveal that through the implementation of channel coding techniques designed for FAs and IM transmission, the performance of coded IM-FAs exhibits notable enhancements, particularly in high correlation scenarios.","sentences":["The concept of fluid antennas (FAs) has emerged as a promising solution to enhance the spectral efficiency of wireless networks, achieved by introducing additional degrees of freedom, including reconfigurability and flexibility.","In this paper, we investigate the use of index-modulated (IM) transmissions within the framework of FA systems, where an FA position is activated during each transmission interval.","This approach is motivated by the common characteristics exhibited by FAs and IM transmissions, which entails the use of a single radio-frequency chain.","From this perspective, we derive a closed-form expression for the bit error rate of IM-FAs considering spatial correlation, and demonstrating superior performance compared to conventional IM systems.","To enhance the performance of IM-FAs in correlated conditions, channel coding techniques are applied.","We first analyze a set partition coding (SPC) scheme for IM-FAs to spatially separate the FA ports, and provide a tight performance bound over correlated channels.","Furthermore, the spatial SPC scheme is extended to turbo-coded modulation where the performance is analyzed for low and high signal-to-noise ratios.","Our results reveal that through the implementation of channel coding techniques designed for FAs and IM transmission, the performance of coded IM-FAs exhibits notable enhancements, particularly in high correlation scenarios."],"url":"http://arxiv.org/abs/2403.06839v1","category":"cs.IT"}
{"created":"2024-03-11 15:27:29","title":"Data-driven sparse modeling of oscillations in plasma space propulsion","abstract":"An algorithm to obtain data-driven models of oscillatory phenomena in plasma space propulsion systems is presented, based on sparse regression (SINDy) and Pareto front analysis. The algorithm can incorporate physical constraints, use data bootstrapping for additional robustness, and fine-tuning to different metrics. Standard, weak and integral SINDy formulations are discussed and compared. The scheme is benchmarked in the case of breathing-mode oscillations in Hall effect thrusters, using PIC/fluid simulation data. Models of varying complexity are obtained for the average plasma properties, and shown to have a clear physical interpretability and agreement with existing 0D models in the literature. Lastly, the algorithm applied is also shown to enable the identification of physical subdomains with qualitatively different plasma dynamics, providing valuable information for more advanced modeling approaches.","sentences":["An algorithm to obtain data-driven models of oscillatory phenomena in plasma space propulsion systems is presented, based on sparse regression (SINDy) and Pareto front analysis.","The algorithm can incorporate physical constraints, use data bootstrapping for additional robustness, and fine-tuning to different metrics.","Standard, weak and integral SINDy formulations are discussed and compared.","The scheme is benchmarked in the case of breathing-mode oscillations in Hall effect thrusters, using PIC/fluid simulation data.","Models of varying complexity are obtained for the average plasma properties, and shown to have a clear physical interpretability and agreement with existing 0D models in the literature.","Lastly, the algorithm applied is also shown to enable the identification of physical subdomains with qualitatively different plasma dynamics, providing valuable information for more advanced modeling approaches."],"url":"http://arxiv.org/abs/2403.06809v1","category":"physics.plasm-ph"}
{"created":"2024-03-11 15:25:03","title":"On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes","abstract":"We present the first finite time global convergence analysis of policy gradient in the context of infinite horizon average reward Markov decision processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite state and action spaces. Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O\\left({\\frac{1}{T}}\\right),$ which translates to $O\\left({\\log(T)}\\right)$ regret, where $T$ represents the number of iterations. Prior work on performance bounds for discounted reward MDPs cannot be extended to average reward MDPs because the bounds grow proportional to the fifth power of the effective horizon. Thus, our primary contribution is in proving that the policy gradient algorithm converges for average-reward MDPs and in obtaining finite-time performance guarantees. In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit dependence on constants that capture the complexity of the underlying MDP. Motivated by this observation, we reexamine and improve the existing performance bounds for discounted reward MDPs. We also present simulations to empirically evaluate the performance of average reward policy gradient algorithm.","sentences":["We present the first finite time global convergence analysis of policy gradient in the context of infinite horizon average reward Markov decision processes (MDPs).","Specifically, we focus on ergodic tabular MDPs with finite state and action spaces.","Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O\\left({\\frac{1}{T}}\\right),$ which translates to $O\\left({\\log(T)}\\right)$ regret, where $T$ represents the number of iterations.","Prior work on performance bounds for discounted reward MDPs cannot be extended to average reward MDPs because the bounds grow proportional to the fifth power of the effective horizon.","Thus, our primary contribution is in proving that the policy gradient algorithm converges for average-reward MDPs and in obtaining finite-time performance guarantees.","In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit dependence on constants that capture the complexity of the underlying MDP.","Motivated by this observation, we reexamine and improve the existing performance bounds for discounted reward MDPs.","We also present simulations to empirically evaluate the performance of average reward policy gradient algorithm."],"url":"http://arxiv.org/abs/2403.06806v1","category":"cs.LG"}
{"created":"2024-03-11 15:17:25","title":"MambaMIL: Enhancing Long Sequence Modeling with Sequence Reordering in Computational Pathology","abstract":"Multiple Instance Learning (MIL) has emerged as a dominant paradigm to extract discriminative feature representations within Whole Slide Images (WSIs) in computational pathology. Despite driving notable progress, existing MIL approaches suffer from limitations in facilitating comprehensive and efficient interactions among instances, as well as challenges related to time-consuming computations and overfitting. In this paper, we incorporate the Selective Scan Space State Sequential Model (Mamba) in Multiple Instance Learning (MIL) for long sequence modeling with linear complexity, termed as MambaMIL. By inheriting the capability of vanilla Mamba, MambaMIL demonstrates the ability to comprehensively understand and perceive long sequences of instances. Furthermore, we propose the Sequence Reordering Mamba (SR-Mamba) aware of the order and distribution of instances, which exploits the inherent valuable information embedded within the long sequences. With the SR-Mamba as the core component, MambaMIL can effectively capture more discriminative features and mitigate the challenges associated with overfitting and high computational overhead. Extensive experiments on two public challenging tasks across nine diverse datasets demonstrate that our proposed framework performs favorably against state-of-the-art MIL methods. The code is released at https://github.com/isyangshu/MambaMIL.","sentences":["Multiple Instance Learning (MIL) has emerged as a dominant paradigm to extract discriminative feature representations within Whole Slide Images (WSIs) in computational pathology.","Despite driving notable progress, existing MIL approaches suffer from limitations in facilitating comprehensive and efficient interactions among instances, as well as challenges related to time-consuming computations and overfitting.","In this paper, we incorporate the Selective Scan Space State Sequential Model (Mamba) in Multiple Instance Learning (MIL) for long sequence modeling with linear complexity, termed as MambaMIL.","By inheriting the capability of vanilla Mamba, MambaMIL demonstrates the ability to comprehensively understand and perceive long sequences of instances.","Furthermore, we propose the Sequence Reordering Mamba (SR-Mamba) aware of the order and distribution of instances, which exploits the inherent valuable information embedded within the long sequences.","With the SR-Mamba as the core component, MambaMIL can effectively capture more discriminative features and mitigate the challenges associated with overfitting and high computational overhead.","Extensive experiments on two public challenging tasks across nine diverse datasets demonstrate that our proposed framework performs favorably against state-of-the-art MIL methods.","The code is released at https://github.com/isyangshu/MambaMIL."],"url":"http://arxiv.org/abs/2403.06800v1","category":"cs.CV"}
{"created":"2024-03-11 15:17:10","title":"A perspective on active glassy dynamics in biological systems","abstract":"Dynamics is central to living systems. In the last two decades, experiments have revealed that the dynamics in diverse biological systems - from intracellular cytoplasm to cellular and organismal aggregates - are remarkably similar to that in dense systems of inanimate particles in equilibrium. They show a glass transition from a solid-like jammed state to a fluid-like flowing state, where a moderate change in control parameter leads to an enormous variation in relaxation time. However, biological systems have crucial differences from the equilibrium systems: the former have activity that drives them out of equilibrium, novel control parameters, and enormous levels of complexity. These active systems showing glassy dynamics are known as active glasses. The field is at the interface of physics and biology, freely borrowing tools from both disciplines and promising novel, fascinating discoveries. We review the experiments that started this field, simulations that have been instrumental for insights, and theories that have helped unify diverse phenomena, reveal correlations, and make novel quantitative predictions. We discuss the primary characteristics that define a glassy system. For most concepts, we first discuss the known equilibrium scenario and then present the key aspects when activity is introduced. We end the article with a discussion of the challenges in the field and possible future directions.","sentences":["Dynamics is central to living systems.","In the last two decades, experiments have revealed that the dynamics in diverse biological systems - from intracellular cytoplasm to cellular and organismal aggregates - are remarkably similar to that in dense systems of inanimate particles in equilibrium.","They show a glass transition from a solid-like jammed state to a fluid-like flowing state, where a moderate change in control parameter leads to an enormous variation in relaxation time.","However, biological systems have crucial differences from the equilibrium systems: the former have activity that drives them out of equilibrium, novel control parameters, and enormous levels of complexity.","These active systems showing glassy dynamics are known as active glasses.","The field is at the interface of physics and biology, freely borrowing tools from both disciplines and promising novel, fascinating discoveries.","We review the experiments that started this field, simulations that have been instrumental for insights, and theories that have helped unify diverse phenomena, reveal correlations, and make novel quantitative predictions.","We discuss the primary characteristics that define a glassy system.","For most concepts, we first discuss the known equilibrium scenario and then present the key aspects when activity is introduced.","We end the article with a discussion of the challenges in the field and possible future directions."],"url":"http://arxiv.org/abs/2403.06799v1","category":"cond-mat.soft"}
{"created":"2024-03-11 15:13:37","title":"Jeffery Orbits with Noise Revisited","abstract":"The behavior of non-spherical particles in a shear-flow is of significant practical and theoretical interest. These systems have been the object of numerous investigations since the pioneering work of Jeffery a century ago. His eponymous orbits describe the deterministic motion of an isolated, rod-like particle in a shear flow. Subsequently, the effect of adding noise was investigated. The theory has been applied to colloidal particles, macromolecules, anisometric granular particles and most recently to microswimmers, for example bacteria. We study the Jeffery orbits of elongated particles subject to noise using Langevin simulations and a Fokker-Planck equation. We extend the analytical solution for infinitely thin needles ($\\beta=1$) obtained by Doi and Edwards to particles with arbitrary shape factor ($0\\le \\beta\\le 1$) and validate the theory by comparing it with simulations. We examine the rotation of the particle around the vorticity axis and study the orientational order matrix. We use the latter to obtain scalar order parameters $s$ and $r$ describing nematic ordering and biaxiality from the orientational distribution function. The value of $s$ (nematic ordering) increases monotonically with increasing P\\'eclet number, while $r$ (measure of biaxiality) displays a maximum value. From perturbation theory we obtain simple expressions that provide accurate descriptions at low noise (or large P\\'eclet numbers). We also examine the orientational distribution in the v-grad v plane and in the perpendicular direction. Finally we present the solution of the Fokker-Planck equation for a strictly two-dimensional (2D) system. For the same noise amplitude the average rotation speed of the particle in 3D is larger than in 2D.","sentences":["The behavior of non-spherical particles in a shear-flow is of significant practical and theoretical interest.","These systems have been the object of numerous investigations since the pioneering work of Jeffery a century ago.","His eponymous orbits describe the deterministic motion of an isolated, rod-like particle in a shear flow.","Subsequently, the effect of adding noise was investigated.","The theory has been applied to colloidal particles, macromolecules, anisometric granular particles and most recently to microswimmers, for example bacteria.","We study the Jeffery orbits of elongated particles subject to noise using Langevin simulations and a Fokker-Planck equation.","We extend the analytical solution for infinitely thin needles ($\\beta=1$) obtained by Doi and Edwards to particles with arbitrary shape factor ($0\\le \\beta\\le 1$) and validate the theory by comparing it with simulations.","We examine the rotation of the particle around the vorticity axis and study the orientational order matrix.","We use the latter to obtain scalar order parameters $s$ and $r$ describing nematic ordering and biaxiality from the orientational distribution function.","The value of $s$ (nematic ordering) increases monotonically with increasing P\\'eclet number, while $r$ (measure of biaxiality) displays a maximum value.","From perturbation theory we obtain simple expressions that provide accurate descriptions at low noise (or large P\\'eclet numbers).","We also examine the orientational distribution in the v-grad v plane and in the perpendicular direction.","Finally we present the solution of the Fokker-Planck equation for a strictly two-dimensional (2D) system.","For the same noise amplitude the average rotation speed of the particle in 3D is larger than in 2D."],"url":"http://arxiv.org/abs/2403.06795v1","category":"cond-mat.soft"}
{"created":"2024-03-11 15:08:11","title":"Next4: Snapshots in Ext4 File System","abstract":"The growing value of data as a strategic asset has given rise to the necessity of implementing reliable backup and recovery solutions in the most efficient and cost-effective manner. The data backup methods available today on linux are not effective enough, because while running, most of them block I/Os to guarantee data integrity. We propose and implement Next4 - file system based snapshot feature in Ext4 which creates an instant image of the file system, to provide incremental versions of data, enabling reliable backup and data recovery. In our design, the snapshot feature is implemented by efficiently infusing the copy-on-write strategy in the write-in-place, extent based Ext4 file system, without affecting its basic structure. Each snapshot is an incremental backup of the data within the system. What distinguishes Next4 is the way that the data is backed up, improving both space utilization as well as performance.","sentences":["The growing value of data as a strategic asset has given rise to the necessity of implementing reliable backup and recovery solutions in the most efficient and cost-effective manner.","The data backup methods available today on linux are not effective enough, because while running, most of them block I/Os to guarantee data integrity.","We propose and implement Next4 - file system based snapshot feature in Ext4 which creates an instant image of the file system, to provide incremental versions of data, enabling reliable backup and data recovery.","In our design, the snapshot feature is implemented by efficiently infusing the copy-on-write strategy in the write-in-place, extent based Ext4 file system, without affecting its basic structure.","Each snapshot is an incremental backup of the data within the system.","What distinguishes Next4 is the way that the data is backed up, improving both space utilization as well as performance."],"url":"http://arxiv.org/abs/2403.06790v1","category":"cs.OS"}
{"created":"2024-03-11 14:51:34","title":"Domain-Independent Dynamic Programming and Constraint Programming Approaches for Assembly Line Balancing Problems with Setups","abstract":"We propose domain-independent dynamic programming (DIDP) and constraint programming (CP) models to exactly solve type-1 and type-2 assembly line balancing problem with sequence-dependent setup times (SUALBP). The goal is to assign tasks to assembly stations and to sequence these tasks within each station, while satisfying precedence relations specified between a subset of task pairs. Each task has a given processing time and a setup time dependent on the previous task on the station to which the task is assigned. The sum of the processing and setup times of tasks assigned to each station constitute the station time and the maximum station time is called the cycle time. For type-1 SUALBP, the objective is to minimize the number of stations, given a maximum cycle time. For type-2 SUALBP, the objective is to minimize the cycle time, given the number of stations. On a set of diverse SUALBP instances, experimental results show that our approaches significantly outperform the state-of-the-art mixed integer programming models for SUALBP-1. For SUALBP-2, the DIDP model outperforms the state-of-the-art exact approach based on logic-based Benders decomposition. By closing 76 open instances for SUALBP-2, our results demonstrate the promise of DIDP for solving complex planning and scheduling problems.","sentences":["We propose domain-independent dynamic programming (DIDP) and constraint programming (CP) models to exactly solve type-1 and type-2 assembly line balancing problem with sequence-dependent setup times (SUALBP).","The goal is to assign tasks to assembly stations and to sequence these tasks within each station, while satisfying precedence relations specified between a subset of task pairs.","Each task has a given processing time and a setup time dependent on the previous task on the station to which the task is assigned.","The sum of the processing and setup times of tasks assigned to each station constitute the station time and the maximum station time is called the cycle time.","For type-1 SUALBP, the objective is to minimize the number of stations, given a maximum cycle time.","For type-2 SUALBP, the objective is to minimize the cycle time, given the number of stations.","On a set of diverse SUALBP instances, experimental results show that our approaches significantly outperform the state-of-the-art mixed integer programming models for SUALBP-1.","For SUALBP-2, the DIDP model outperforms the state-of-the-art exact approach based on logic-based Benders decomposition.","By closing 76 open instances for SUALBP-2, our results demonstrate the promise of DIDP for solving complex planning and scheduling problems."],"url":"http://arxiv.org/abs/2403.06780v1","category":"math.OC"}
{"created":"2024-03-11 14:48:57","title":"From Factor Models to Deep Learning: Machine Learning in Reshaping Empirical Asset Pricing","abstract":"This paper comprehensively reviews the application of machine learning (ML) and AI in finance, specifically in the context of asset pricing. It starts by summarizing the traditional asset pricing models and examining their limitations in capturing the complexities of financial markets. It explores how 1) ML models, including supervised, unsupervised, semi-supervised, and reinforcement learning, provide versatile frameworks to address these complexities, and 2) the incorporation of advanced ML algorithms into traditional financial models enhances return prediction and portfolio optimization. These methods can adapt to changing market dynamics by modeling structural changes and incorporating heterogeneous data sources, such as text and images. In addition, this paper explores challenges in applying ML in asset pricing, addressing the growing demand for explainability in decision-making and mitigating overfitting in complex models. This paper aims to provide insights into novel methodologies showcasing the potential of ML to reshape the future of quantitative finance.","sentences":["This paper comprehensively reviews the application of machine learning (ML) and AI in finance, specifically in the context of asset pricing.","It starts by summarizing the traditional asset pricing models and examining their limitations in capturing the complexities of financial markets.","It explores how 1) ML models, including supervised, unsupervised, semi-supervised, and reinforcement learning, provide versatile frameworks to address these complexities, and 2) the incorporation of advanced ML algorithms into traditional financial models enhances return prediction and portfolio optimization.","These methods can adapt to changing market dynamics by modeling structural changes and incorporating heterogeneous data sources, such as text and images.","In addition, this paper explores challenges in applying ML in asset pricing, addressing the growing demand for explainability in decision-making and mitigating overfitting in complex models.","This paper aims to provide insights into novel methodologies showcasing the potential of ML to reshape the future of quantitative finance."],"url":"http://arxiv.org/abs/2403.06779v1","category":"q-fin.ST"}
{"created":"2024-03-11 14:39:24","title":"Redefining Event Types and Group Evolution in Temporal Data","abstract":"Groups -- such as clusters of points or communities of nodes -- are fundamental when addressing various data mining tasks. In temporal data, the predominant approach for characterizing group evolution has been through the identification of ``events\". However, the events usually described in the literature, e.g., shrinks/growths, splits/merges, are often arbitrarily defined, creating a gap between such theoretical/predefined types and real-data group observations. Moving beyond existing taxonomies, we think of events as ``archetypes\" characterized by a unique combination of quantitative dimensions that we call ``facets\". Group dynamics are defined by their position within the facet space, where archetypal events occupy extremities. Thus, rather than enforcing strict event types, our approach can allow for hybrid descriptions of dynamics involving group proximity to multiple archetypes. We apply our framework to evolving groups from several face-to-face interaction datasets, showing it enables richer, more reliable characterization of group dynamics with respect to state-of-the-art methods, especially when the groups are subject to complex relationships. Our approach also offers intuitive solutions to common tasks related to dynamic group analysis, such as choosing an appropriate aggregation scale, quantifying partition stability, and evaluating event quality.","sentences":["Groups -- such as clusters of points or communities of nodes -- are fundamental when addressing various data mining tasks.","In temporal data, the predominant approach for characterizing group evolution has been through the identification of ``events\".","However, the events usually described in the literature, e.g., shrinks/growths, splits/merges, are often arbitrarily defined, creating a gap between such theoretical/predefined types and real-data group observations.","Moving beyond existing taxonomies, we think of events as ``archetypes\" characterized by a unique combination of quantitative dimensions that we call ``facets\".","Group dynamics are defined by their position within the facet space, where archetypal events occupy extremities.","Thus, rather than enforcing strict event types, our approach can allow for hybrid descriptions of dynamics involving group proximity to multiple archetypes.","We apply our framework to evolving groups from several face-to-face interaction datasets, showing it enables richer, more reliable characterization of group dynamics with respect to state-of-the-art methods, especially when the groups are subject to complex relationships.","Our approach also offers intuitive solutions to common tasks related to dynamic group analysis, such as choosing an appropriate aggregation scale, quantifying partition stability, and evaluating event quality."],"url":"http://arxiv.org/abs/2403.06771v1","category":"cs.LG"}
{"created":"2024-03-11 14:33:12","title":"A dark-field setup for the measurement of light-by-light scattering with high-intensity lasers","abstract":"We put forward a concrete experimental setup allowing to measure light-by-light scattering in the collision of two optical high-intensity laser beams at state-of-the-art high-field facilities operating petawatt class laser systems. Our setup uses the same focusing optics for both laser beams to be collided and employs a dark-field approach for the detection of the single-photon-level nonlinear quantum vacuum response in the presence of a large background. Based on an advanced modeling of the colliding laser fields, we in particular provide reliable estimates for the prospective numbers of signal photons scattered into the dark-field for various laser polarizations.","sentences":["We put forward a concrete experimental setup allowing to measure light-by-light scattering in the collision of two optical high-intensity laser beams at state-of-the-art high-field facilities operating petawatt class laser systems.","Our setup uses the same focusing optics for both laser beams to be collided and employs a dark-field approach for the detection of the single-photon-level nonlinear quantum vacuum response in the presence of a large background.","Based on an advanced modeling of the colliding laser fields, we in particular provide reliable estimates for the prospective numbers of signal photons scattered into the dark-field for various laser polarizations."],"url":"http://arxiv.org/abs/2403.06762v1","category":"physics.optics"}
{"created":"2024-03-11 14:31:48","title":"Magnetic billiards and the Hofer-Zehnder capacity of disk tangent bundles of lens spaces","abstract":"We compute the Hofer-Zehnder capacity of disk tangent bundles of certain lens spaces with respect to the round metric. Interestingly we find that the Hofer-Zehnder capacity does not see the covering, i.e. the capacity of the disk tangent bundle of the lens space coincides with the capacity of the disk tangent bundle of the 3-sphere covering it. In particular, this gives a first example, where Gromov width and Hofer-Zehnder capacity of a disk tangent bundle disagree. Techniques we use include for the lower bound magnetic billiards and for the upper bound Gromov-Witten invariants.","sentences":["We compute the Hofer-Zehnder capacity of disk tangent bundles of certain lens spaces with respect to the round metric.","Interestingly we find that the Hofer-Zehnder capacity does not see the covering, i.e. the capacity of the disk tangent bundle of the lens space coincides with the capacity of the disk tangent bundle of the 3-sphere covering it.","In particular, this gives a first example, where Gromov width and Hofer-Zehnder capacity of a disk tangent bundle disagree.","Techniques we use include for the lower bound magnetic billiards and for the upper bound Gromov-Witten invariants."],"url":"http://arxiv.org/abs/2403.06761v1","category":"math.SG"}
{"created":"2024-03-11 14:31:21","title":"Performance of SK-Gd's Upgraded Real-time Supernova Monitoring System","abstract":"Among multi-messenger observations of the next galactic core-collapse supernova, Super-Kamiokande (SK) plays a critical role in detecting the emitted supernova neutrinos, determining the direction to the supernova (SN), and notifying the astronomical community of these observations in advance of the optical signal. On 2022, SK has increased the gadolinium dissolved in its water target (SK-Gd) and has achieved a Gd concentration of 0.033%, resulting in enhanced neutron detection capability, which in turn enables more accurate determination of the supernova direction. Accordingly, SK-Gd's real-time supernova monitoring system (Abe te al. 2016b) has been upgraded. SK_SN Notice, a warning system that works together with this monitoring system, was released on December 13, 2021, and is available through GCN Notices (Barthelmy et al. 2000). When the monitoring system detects an SN-like burst of events, SK_SN Notice will automatically distribute an alarm with the reconstructed direction to the supernova candidate within a few minutes. In this paper, we present a systematic study of SK-Gd's response to a simulated galactic SN. Assuming a supernova situated at 10 kpc, neutrino fluxes from six supernova models are used to characterize SK-Gd's pointing accuracy using the same tools as the online monitoring system. The pointing accuracy is found to vary from 3-7$^\\circ$ depending on the models. However, if the supernova is closer than 10 kpc, SK_SN Notice can issue an alarm with three-degree accuracy, which will benefit follow-up observations by optical telescopes with large fields of view.","sentences":["Among multi-messenger observations of the next galactic core-collapse supernova, Super-Kamiokande (SK) plays a critical role in detecting the emitted supernova neutrinos, determining the direction to the supernova (SN), and notifying the astronomical community of these observations in advance of the optical signal.","On 2022, SK has increased the gadolinium dissolved in its water target (SK-Gd) and has achieved a Gd concentration of 0.033%, resulting in enhanced neutron detection capability, which in turn enables more accurate determination of the supernova direction.","Accordingly, SK-Gd's real-time supernova monitoring system (Abe te al. 2016b) has been upgraded.","SK_SN Notice, a warning system that works together with this monitoring system, was released on December 13, 2021, and is available through GCN Notices (Barthelmy et al. 2000).","When the monitoring system detects an SN-like burst of events, SK_SN Notice will automatically distribute an alarm with the reconstructed direction to the supernova candidate within a few minutes.","In this paper, we present a systematic study of SK-Gd's response to a simulated galactic SN.","Assuming a supernova situated at 10 kpc, neutrino fluxes from six supernova models are used to characterize SK-Gd's pointing accuracy using the same tools as the online monitoring system.","The pointing accuracy is found to vary from 3-7$^\\circ$ depending on the models.","However, if the supernova is closer than 10 kpc, SK_SN Notice can issue an alarm with three-degree accuracy, which will benefit follow-up observations by optical telescopes with large fields of view."],"url":"http://arxiv.org/abs/2403.06760v1","category":"astro-ph.HE"}
{"created":"2024-03-11 14:29:56","title":"Koopman Ensembles for Probabilistic Time Series Forecasting","abstract":"In the context of an increasing popularity of data-driven models to represent dynamical systems, many machine learning-based implementations of the Koopman operator have recently been proposed. However, the vast majority of those works are limited to deterministic predictions, while the knowledge of uncertainty is critical in fields like meteorology and climatology. In this work, we investigate the training of ensembles of models to produce stochastic outputs. We show through experiments on real remote sensing image time series that ensembles of independently trained models are highly overconfident and that using a training criterion that explicitly encourages the members to produce predictions with high inter-model variances greatly improves the uncertainty quantification of the ensembles.","sentences":["In the context of an increasing popularity of data-driven models to represent dynamical systems, many machine learning-based implementations of the Koopman operator have recently been proposed.","However, the vast majority of those works are limited to deterministic predictions, while the knowledge of uncertainty is critical in fields like meteorology and climatology.","In this work, we investigate the training of ensembles of models to produce stochastic outputs.","We show through experiments on real remote sensing image time series that ensembles of independently trained models are highly overconfident and that using a training criterion that explicitly encourages the members to produce predictions with high inter-model variances greatly improves the uncertainty quantification of the ensembles."],"url":"http://arxiv.org/abs/2403.06757v1","category":"cs.LG"}
{"created":"2024-03-11 14:29:29","title":"One-Bit Target Detection in Collocated MIMO Radar with Colored Background Noise","abstract":"One-bit sampling has emerged as a promising technique in multiple-input multiple-output (MIMO) radar systems due to its ability to significantly reduce data volume and processing requirements. Nevertheless, current detection methods have not adequately addressed the impact of colored noise, which is frequently encountered in real scenarios. In this paper, we present a novel detection method that accounts for colored noise in MIMO radar systems. Specifically, we derive Rao's test by computing the derivative of the likelihood function with respect to the target reflectivity parameter and the Fisher information matrix, resulting in a detector that takes the form of a weighted matched filter. To ensure the constant false alarm rate (CFAR) property, we also consider noise covariance uncertainty and examine its effect on the probability of false alarm. The detection probability is also studied analytically. Simulation results demonstrate that the proposed detector provides considerable performance gains in the presence of colored noise.","sentences":["One-bit sampling has emerged as a promising technique in multiple-input multiple-output (MIMO) radar systems due to its ability to significantly reduce data volume and processing requirements.","Nevertheless, current detection methods have not adequately addressed the impact of colored noise, which is frequently encountered in real scenarios.","In this paper, we present a novel detection method that accounts for colored noise in MIMO radar systems.","Specifically, we derive Rao's test by computing the derivative of the likelihood function with respect to the target reflectivity parameter and the Fisher information matrix, resulting in a detector that takes the form of a weighted matched filter.","To ensure the constant false alarm rate (CFAR) property, we also consider noise covariance uncertainty and examine its effect on the probability of false alarm.","The detection probability is also studied analytically.","Simulation results demonstrate that the proposed detector provides considerable performance gains in the presence of colored noise."],"url":"http://arxiv.org/abs/2403.06756v1","category":"eess.SP"}
{"created":"2024-03-11 14:20:13","title":"Generalising Multi-Agent Cooperation through Task-Agnostic Communication","abstract":"Existing communication methods for multi-agent reinforcement learning (MARL) in cooperative multi-robot problems are almost exclusively task-specific, training new communication strategies for each unique task. We address this inefficiency by introducing a communication strategy applicable to any task within a given environment. We pre-train the communication strategy without task-specific reward guidance in a self-supervised manner using a set autoencoder. Our objective is to learn a fixed-size latent Markov state from a variable number of agent observations. Under mild assumptions, we prove that policies using our latent representations are guaranteed to converge, and upper bound the value error introduced by our Markov state approximation. Our method enables seamless adaptation to novel tasks without fine-tuning the communication strategy, gracefully supports scaling to more agents than present during training, and detects out-of-distribution events in an environment. Empirical results on diverse MARL scenarios validate the effectiveness of our approach, surpassing task-specific communication strategies in unseen tasks. Our implementation of this work is available at https://github.com/proroklab/task-agnostic-comms.","sentences":["Existing communication methods for multi-agent reinforcement learning (MARL) in cooperative multi-robot problems are almost exclusively task-specific, training new communication strategies for each unique task.","We address this inefficiency by introducing a communication strategy applicable to any task within a given environment.","We pre-train the communication strategy without task-specific reward guidance in a self-supervised manner using a set autoencoder.","Our objective is to learn a fixed-size latent Markov state from a variable number of agent observations.","Under mild assumptions, we prove that policies using our latent representations are guaranteed to converge, and upper bound the value error introduced by our Markov state approximation.","Our method enables seamless adaptation to novel tasks without fine-tuning the communication strategy, gracefully supports scaling to more agents than present during training, and detects out-of-distribution events in an environment.","Empirical results on diverse MARL scenarios validate the effectiveness of our approach, surpassing task-specific communication strategies in unseen tasks.","Our implementation of this work is available at https://github.com/proroklab/task-agnostic-comms."],"url":"http://arxiv.org/abs/2403.06750v1","category":"cs.MA"}
{"created":"2024-03-11 14:13:41","title":"MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation","abstract":"Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSN) to split user history sequence regarding to the volume of stock for each product, and adopt differentiated modeling approaches for different sequences. As for the limited-stock products, a meta-learning approach is applied to address the problem of inconvergence, which is achieved by designing meta scaling and shifting networks with ID and side information. In addition, traditional approach can hardly update item embedding once the product is consumed. Thereby, we propose an auxiliary loss that makes the parameters updatable even when the product is no longer in distribution. To the best of our knowledge, this is the first solution addressing the recommendation of limited-stock product. Experimental results on the production dataset and online A/B testing demonstrate the effectiveness of our proposed method.","sentences":["Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system.","This poses several unique challenges for click-through rate (CTR) prediction.","Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge.","This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks.","Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output.","To this end, we propose the Meta-Split Network (MSN) to split user history sequence regarding to the volume of stock for each product, and adopt differentiated modeling approaches for different sequences.","As for the limited-stock products, a meta-learning approach is applied to address the problem of inconvergence, which is achieved by designing meta scaling and shifting networks with ID and side information.","In addition, traditional approach can hardly update item embedding once the product is consumed.","Thereby, we propose an auxiliary loss that makes the parameters updatable even when the product is no longer in distribution.","To the best of our knowledge, this is the first solution addressing the recommendation of limited-stock product.","Experimental results on the production dataset and online A/B testing demonstrate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2403.06747v1","category":"cs.IR"}
{"created":"2024-03-11 14:12:21","title":"Integration of Physics-Derived Memristor Models with Machine Learning Frameworks","abstract":"Simulation frameworks such MemTorch, DNN+NeuroSim, and aihwkit are commonly used to facilitate the end-to-end co-design of memristive machine learning (ML) accelerators. These simulators can take device nonidealities into account and are integrated with modern ML frameworks. However, memristors in these simulators are modeled with either lookup tables or simple analytic models with basic nonlinearities. These simple models are unable to capture certain performance-critical aspects of device nonidealities. For example, they ignore the physical cause of switching, which induces errors in switching timings and thus incorrect estimations of conductance states. This work aims at bringing physical dynamics into consideration to model nonidealities while being compatible with GPU accelerators. We focus on Valence Change Memory (VCM) cells, where the switching nonlinearity and SET/RESET asymmetry relate tightly with the thermal resistance, ion mobility, Schottky barrier height, parasitic resistance, and other effects. The resulting dynamics require solving an ODE that captures changes in oxygen vacancies. We modified a physics-derived SPICE-level VCM model, integrated it with the aihwkit simulator and tested the performance with the MNIST dataset. Results show that noise that disrupts the SET/RESET matching affects network performance the most. This work serves as a tool for evaluating how physical dynamics in memristive devices affect neural network accuracy and can be used to guide the development of future integrated devices.","sentences":["Simulation frameworks such MemTorch, DNN+NeuroSim, and aihwkit are commonly used to facilitate the end-to-end co-design of memristive machine learning (ML) accelerators.","These simulators can take device nonidealities into account and are integrated with modern ML frameworks.","However, memristors in these simulators are modeled with either lookup tables or simple analytic models with basic nonlinearities.","These simple models are unable to capture certain performance-critical aspects of device nonidealities.","For example, they ignore the physical cause of switching, which induces errors in switching timings and thus incorrect estimations of conductance states.","This work aims at bringing physical dynamics into consideration to model nonidealities while being compatible with GPU accelerators.","We focus on Valence Change Memory (VCM) cells, where the switching nonlinearity and SET/RESET asymmetry relate tightly with the thermal resistance, ion mobility, Schottky barrier height, parasitic resistance, and other effects.","The resulting dynamics require solving an ODE that captures changes in oxygen vacancies.","We modified a physics-derived SPICE-level VCM model, integrated it with the aihwkit simulator and tested the performance with the MNIST dataset.","Results show that noise that disrupts the SET/RESET matching affects network performance the most.","This work serves as a tool for evaluating how physical dynamics in memristive devices affect neural network accuracy and can be used to guide the development of future integrated devices."],"url":"http://arxiv.org/abs/2403.06746v1","category":"cs.ET"}
{"created":"2024-03-11 14:10:35","title":"Design and Performance Comparison of FuzzyPID and Non-linear Model Predictive Controller for 4-Wheel Omni-drive Robot","abstract":"Trajectory tracking for an Omni-drive robot presents a challenging task that demands an efficient controller design. To address the limitations of manual tuning, we introduce a self-optimizing controller named fuzzyPID, leveraging the analysis of responses from various dynamic and static systems. The rule-based controller design is implemented using Matlab/Simulink, and trajectory tracking simulations are conducted within the CoppeliaSim environment. Similarly, a non-linear model predictive controller(NMPC) is proposed to compare tracking performance with fuzzyPID. We also assess the impact of tunable parameters of NMPC on its tracking accuracy. Simulation results validate the precision and effectiveness of NMPC over fuzzyPID controller while trading computational complexity.","sentences":["Trajectory tracking for an Omni-drive robot presents a challenging task that demands an efficient controller design.","To address the limitations of manual tuning, we introduce a self-optimizing controller named fuzzyPID, leveraging the analysis of responses from various dynamic and static systems.","The rule-based controller design is implemented using Matlab/Simulink, and trajectory tracking simulations are conducted within the CoppeliaSim environment.","Similarly, a non-linear model predictive controller(NMPC) is proposed to compare tracking performance with fuzzyPID.","We also assess the impact of tunable parameters of NMPC on its tracking accuracy.","Simulation results validate the precision and effectiveness of NMPC over fuzzyPID controller while trading computational complexity."],"url":"http://arxiv.org/abs/2403.06744v1","category":"cs.RO"}
{"created":"2024-03-11 14:02:24","title":"Post-Training Attribute Unlearning in Recommender Systems","abstract":"With the growing privacy concerns in recommender systems, recommendation unlearning is getting increasing attention. Existing studies predominantly use training data, i.e., model inputs, as unlearning target. However, attackers can extract private information from the model even if it has not been explicitly encountered during training. We name this unseen information as \\textit{attribute} and treat it as unlearning target. To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to make target attributes indistinguishable. In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed. To address the PoT-AU problem in recommender systems, we propose a two-component loss function. The first component is distinguishability loss, where we design a distribution-based measurement to make attribute labels indistinguishable from attackers. We further extend this measurement to handle multi-class attribute cases with efficient computational overhead. The second component is regularization loss, where we explore a function-space measurement that effectively maintains recommendation performance compared to parameter-space regularization. We use stochastic gradient descent algorithm to optimize our proposed loss. Extensive experiments on four real-world datasets demonstrate the effectiveness of our proposed methods.","sentences":["With the growing privacy concerns in recommender systems, recommendation unlearning is getting increasing attention.","Existing studies predominantly use training data, i.e., model inputs, as unlearning target.","However, attackers can extract private information from the model even if it has not been explicitly encountered during training.","We name this unseen information as \\textit{attribute} and treat it as unlearning target.","To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to make target attributes indistinguishable.","In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed.","To address the PoT-AU problem in recommender systems, we propose a two-component loss function.","The first component is distinguishability loss, where we design a distribution-based measurement to make attribute labels indistinguishable from attackers.","We further extend this measurement to handle multi-class attribute cases with efficient computational overhead.","The second component is regularization loss, where we explore a function-space measurement that effectively maintains recommendation performance compared to parameter-space regularization.","We use stochastic gradient descent algorithm to optimize our proposed loss.","Extensive experiments on four real-world datasets demonstrate the effectiveness of our proposed methods."],"url":"http://arxiv.org/abs/2403.06737v1","category":"cs.IR"}
{"created":"2024-03-11 13:44:38","title":"A SysML Profile for the Standardized Description of Processes during System Development","abstract":"A key aspect in creating models of production systems with the use of model-based systems engineering (MBSE) lies in the description of system functions. These functions shouldbe described in a clear and standardized manner.The VDI/VDE 3682 standard for Formalised Process De-scription (FPD) provides a simple and easily understandable representation of processes. These processes can be conceptualized as functions within the system model, making the FPD particularly well-suited for the standardized representation ofthe required functions. Hence, this contribution focuses on thedevelopment of a Domain-Specific Modeling Language(DSML) that facilitates the integration of VDI/VDE 3682 into the Systems Modeling Language (SysML). The presented approach not onlyextends classical SysML with domain-specific requirements but also facilitates model verification through constraints modeled in Object Constraint Language (OCL). Additionally, it enables automatic serialization of process descriptions into the Extensible Markup Language (XML) using the Velocity Template Language (VTL). This serialization enables the use of process modeling in applications outside of MBSE. The approach was validated using an collar screwing use case in the major component assembly in aircraft production.","sentences":["A key aspect in creating models of production systems with the use of model-based systems engineering (MBSE) lies in the description of system functions.","These functions shouldbe described in a clear and standardized manner.","The VDI/VDE 3682 standard for Formalised Process De-scription (FPD) provides a simple and easily understandable representation of processes.","These processes can be conceptualized as functions within the system model, making the FPD particularly well-suited for the standardized representation ofthe required functions.","Hence, this contribution focuses on thedevelopment of a Domain-Specific Modeling Language(DSML) that facilitates the integration of VDI/VDE 3682 into the Systems Modeling Language (SysML).","The presented approach not onlyextends classical SysML with domain-specific requirements but also facilitates model verification through constraints modeled in Object Constraint Language (OCL).","Additionally, it enables automatic serialization of process descriptions into the Extensible Markup Language (XML) using the Velocity Template Language (VTL).","This serialization enables the use of process modeling in applications outside of MBSE.","The approach was validated using an collar screwing use case in the major component assembly in aircraft production."],"url":"http://arxiv.org/abs/2403.06723v1","category":"cs.SE"}
{"created":"2024-03-11 13:43:40","title":"Asymptotics of the finite-temperature sine kernel determinant","abstract":"In the present paper, we study the asymptotics of the Fredholm determinant $D(x,s)$ of the finite-temperature deformation of the sine kernel, which represents the probability that there is no particles on the interval $(-x/\\pi,x/\\pi)$ in the bulk scaling limit of the finite-temperature fermion system. The variable $s$ in $D(x,s)$ is related to the temperature. The determinant also corresponds to the finite-temperature correlation function of one dimensional Bose gas. We derive the asymptotics of $D(x,s)$ in several different regimes in the $(x,s)$-plane. A third-order phase transition is observed in the asymptotic expansions as both $x$ and $s$ tend to positive infinity at certain related speed. The phase transition is then shown to be described by the integral involving the Hastings-McLeod solution of the second Painlev\\'e equation.","sentences":["In the present paper, we study the asymptotics of the Fredholm determinant $D(x,s)$ of the finite-temperature deformation of the sine kernel, which represents the probability that there is no particles on the interval $(-x/\\pi,x/\\pi)$ in the bulk scaling limit of the finite-temperature fermion system.","The variable $s$ in $D(x,s)$ is related to the temperature.","The determinant also corresponds to the finite-temperature correlation function of one dimensional Bose gas.","We derive the asymptotics of $D(x,s)$ in several different regimes in the $(x,s)$-plane.","A third-order phase transition is observed in the asymptotic expansions as both $x$ and $s$ tend to positive infinity at certain related speed.","The phase transition is then shown to be described by the integral involving the Hastings-McLeod solution of the second Painlev\\'e equation."],"url":"http://arxiv.org/abs/2403.06722v1","category":"math-ph"}
{"created":"2024-03-11 13:42:17","title":"Injection spectra of different species of cosmic rays from AMS-02, ACE-CRIS and Voyager-1","abstract":"Precise measurements of energy spectra of different cosmic ray species were obtained in recent years, by particularly the AMS-02 experiment on the International Space Station. It has been shown that apparent differences exist in different groups of the primary cosmic rays. However, it is not straightforward to conclude that the source spectra of different particle groups are different since they will experience different propagation processes (e.g., energy losses and fragmentations) either. In this work, we study the injection spectra of different nuclear species using the measurements from Voyager-1 outside the solar system, and ACR-CRIS and AMS-02 on top of atmosphere, in a physical framework of cosmic ray transportation. Two types of injection spectra are assumed, the broken power-law and the non-parametric spline interpolation form. The non-parametric form fits the data better than the broken power-law form, implying that potential structures beyond the constrained spectral shape of broken power-law may exist. For different nuclei the injection spectra are overall similar in shape but do show some differences among each other. For the non-parametric spectral form, the helium injection spectrum is the softest at low energies and the hardest at high energies. For both spectral shapes, the low-energy injection spectrum of neon is the hardest among all these species, and the carbon and oxygen spectra have more prominent bumps in 1-10 GV in the R2dN/dR presentation. Such differences suggest the existence of differences in the sources or acceleration processes of various nuclei of cosmic rays.","sentences":["Precise measurements of energy spectra of different cosmic ray species were obtained in recent years, by particularly the AMS-02 experiment on the International Space Station.","It has been shown that apparent differences exist in different groups of the primary cosmic rays.","However, it is not straightforward to conclude that the source spectra of different particle groups are different since they will experience different propagation processes (e.g., energy losses and fragmentations) either.","In this work, we study the injection spectra of different nuclear species using the measurements from Voyager-1 outside the solar system, and ACR-CRIS and AMS-02 on top of atmosphere, in a physical framework of cosmic ray transportation.","Two types of injection spectra are assumed, the broken power-law and the non-parametric spline interpolation form.","The non-parametric form fits the data better than the broken power-law form, implying that potential structures beyond the constrained spectral shape of broken power-law may exist.","For different nuclei the injection spectra are overall similar in shape but do show some differences among each other.","For the non-parametric spectral form, the helium injection spectrum is the softest at low energies and the hardest at high energies.","For both spectral shapes, the low-energy injection spectrum of neon is the hardest among all these species, and the carbon and oxygen spectra have more prominent bumps in 1-10 GV in the R2dN/dR presentation.","Such differences suggest the existence of differences in the sources or acceleration processes of various nuclei of cosmic rays."],"url":"http://arxiv.org/abs/2403.06719v1","category":"astro-ph.HE"}
{"created":"2024-03-11 13:42:05","title":"Unprotected 4G/5G Control Procedures at Low Layers Considered Dangerous","abstract":"Over the years, several security vulnerabilities in the 3GPP cellular systems have been demonstrated in the literature. Most studies focus on higher layers of the cellular radio stack, such as the RRC and NAS, which are cryptographically protected. However, lower layers of the stack, such as PHY and MAC, are not as thoroughly studied, even though they are neither encrypted nor integrity protected. Furthermore, the latest releases of 5G significantly increased the number of low-layer control messages and procedures. The complexity of the cellular standards and the high degree of cross-layer operations, makes reasoning about security non-trivial, and requires a systematic analysis. We study the control procedures carried by each physical channel, and find that current cellular systems are susceptible to several new passive attacks due to information leakage, and active attacks by injecting MAC and PHY messages. For instance, we find that beamforming information leakage enables fingerprinting-based localization and tracking of users. We identify active attacks that reduce the users' throughput by disabling RF front ends at the UE, disrupt user communications by tricking other connected UEs into acting as jammers, or stealthily disconnect an active user. We evaluate our attacks against COTS UEs in various scenarios and demonstrate their practicality by measuring current operators' configurations across three countries. Our results show that an attacker can, among other things, localize users with an accuracy of 20 meters 96% of the time, track users' moving paths with a probability of 90%, reduce throughput by more than 95% within 2 seconds (by spoofing a 39 bits DCI), and disconnect users.","sentences":["Over the years, several security vulnerabilities in the 3GPP cellular systems have been demonstrated in the literature.","Most studies focus on higher layers of the cellular radio stack, such as the RRC and NAS, which are cryptographically protected.","However, lower layers of the stack, such as PHY and MAC, are not as thoroughly studied, even though they are neither encrypted nor integrity protected.","Furthermore, the latest releases of 5G significantly increased the number of low-layer control messages and procedures.","The complexity of the cellular standards and the high degree of cross-layer operations, makes reasoning about security non-trivial, and requires a systematic analysis.","We study the control procedures carried by each physical channel, and find that current cellular systems are susceptible to several new passive attacks due to information leakage, and active attacks by injecting MAC and PHY messages.","For instance, we find that beamforming information leakage enables fingerprinting-based localization and tracking of users.","We identify active attacks that reduce the users' throughput by disabling RF front ends at the UE, disrupt user communications by tricking other connected UEs into acting as jammers, or stealthily disconnect an active user.","We evaluate our attacks against COTS UEs in various scenarios and demonstrate their practicality by measuring current operators' configurations across three countries.","Our results show that an attacker can, among other things, localize users with an accuracy of 20 meters 96% of the time, track users' moving paths with a probability of 90%, reduce throughput by more than 95% within 2 seconds (by spoofing a 39 bits DCI), and disconnect users."],"url":"http://arxiv.org/abs/2403.06717v1","category":"cs.CR"}
{"created":"2024-03-11 13:34:24","title":"Validation of the GreenX library time-frequency component for efficient GW and RPA calculations","abstract":"Electronic structure calculations based on many-body perturbation theory (e.g. \\textit{GW} or the random-phase approximation (RPA)) require function evaluations in the complex time and frequency domain, for example inhomogeneous Fourier transforms or analytic continuation from the imaginary axis to the real axis. For inhomogeneous Fourier transforms, the time-frequency component of the GreenX library provides time-frequency grids that can be utilized in low-scaling RPA and \\textit{GW} implementations. In addition, the adoption of the compact frequency grids provided by our library also reduces the computational overhead in RPA implementations with conventional scaling. In this work, we present low-scaling \\textit{GW} and conventional RPA benchmark calculations using the GreenX grids with different codes (FHI-aims, CP2K and ABINIT) for molecules, two-dimensional materials and solids. Very small integration errors are observed when using 30 time-frequency points for our test cases, namely $<10^{-8}$\\,eV/electron for the RPA correlation energies, and $\\le 10$~meV for the $GW$ quasiparticle energies.","sentences":["Electronic structure calculations based on many-body perturbation theory (e.g. \\textit{GW} or the random-phase approximation (RPA)) require function evaluations in the complex time and frequency domain, for example inhomogeneous Fourier transforms or analytic continuation from the imaginary axis to the real axis.","For inhomogeneous Fourier transforms, the time-frequency component of the GreenX library provides time-frequency grids that can be utilized in low-scaling RPA and \\textit{GW} implementations.","In addition, the adoption of the compact frequency grids provided by our library also reduces the computational overhead in RPA implementations with conventional scaling.","In this work, we present low-scaling \\textit{GW} and conventional RPA benchmark calculations using the GreenX grids with different codes (FHI-aims, CP2K and ABINIT) for molecules, two-dimensional materials and solids.","Very small integration errors are observed when using 30 time-frequency points for our test cases, namely $<10^{-8}$\\,eV/electron for the RPA correlation energies, and $\\le 10$~meV for the $GW$ quasiparticle energies."],"url":"http://arxiv.org/abs/2403.06709v1","category":"physics.comp-ph"}
{"created":"2024-03-11 13:33:31","title":"Tikhonov Regularization for Stochastic Non-Smooth Convex Optimization in Hilbert Spaces","abstract":"To solve non-smooth convex optimization problems with a noisy gradient input, we analyze the global behavior of subgradient-like flows under stochastic errors. The objective function is composite, being equal to the sum of two convex functions, one being differentiable and the other potentially non-smooth. We then use stochastic differential inclusions where the drift term is minus the subgradient of the objective function, and the diffusion term is either bounded or square-integrable. In this context, under Lipschitz's continuity of the differentiable term and a growth condition of the non-smooth term, our first main result shows almost sure weak convergence of the trajectory process towards a minimizer of the objective function. Then, using Tikhonov regularization with a properly tuned vanishing parameter, we can obtain almost sure strong convergence of the trajectory towards the minimum norm solution. We find an explicit tuning of this parameter when our objective function satisfies a local error-bound inequality. We also provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex and strongly convex case.","sentences":["To solve non-smooth convex optimization problems with a noisy gradient input, we analyze the global behavior of subgradient-like flows under stochastic errors.","The objective function is composite, being equal to the sum of two convex functions, one being differentiable and the other potentially non-smooth.","We then use stochastic differential inclusions where the drift term is minus the subgradient of the objective function, and the diffusion term is either bounded or square-integrable.","In this context, under Lipschitz's continuity of the differentiable term and a growth condition of the non-smooth term, our first main result shows almost sure weak convergence of the trajectory process towards a minimizer of the objective function.","Then, using Tikhonov regularization with a properly tuned vanishing parameter, we can obtain almost sure strong convergence of the trajectory towards the minimum norm solution.","We find an explicit tuning of this parameter when our objective function satisfies a local error-bound inequality.","We also provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex and strongly convex case."],"url":"http://arxiv.org/abs/2403.06708v1","category":"math.OC"}
{"created":"2024-03-11 13:20:23","title":"Proof complexity of universal algebra in a CSP dichotomy proof","abstract":"The constraint satisfaction problem (CSP) can be formulated as a homomorphism problem between relational structures: given a structure $\\mathcal{A}$, for any structure $\\mathcal{X}$, whether there exists a homomorphism from $\\mathcal{X}$ to $\\mathcal{A}$. For years, it has been conjectured that all problems of this type are divided into polynomial-time and NP-complete problems, and the conjecture was proved in 2017 separately by Zhuk (2017) and Bulatov (2017). Zhuk's algorithm solves tractable CSPs in polynomial time. The algorithm is partly based on universal algebra theorems: informally, they state that after reducing some domain of an instance to its strong subuniverses, a satisfiable instance maintains a solution.   In this paper, we present the formalization of the proofs of these theorems in the bounded arithmetic $W^1_1$ introduced by Skelley (2004). The formalization, together with our previous results (2022), shows that $W_1^1$ proves the soundness of Zhuk's algorithm, where by soundness we mean that any rejection of the algorithm is correct. From the known relation of the theory to propositional calculus $G$, it follows that tautologies, expressing the non-existence of a solution for unsatisfiable instances, have short proofs in $G$.","sentences":["The constraint satisfaction problem (CSP) can be formulated as a homomorphism problem between relational structures: given a structure $\\mathcal{A}$, for any structure $\\mathcal{X}$, whether there exists a homomorphism from $\\mathcal{X}$ to $\\mathcal{A}$. For years, it has been conjectured that all problems of this type are divided into polynomial-time and NP-complete problems, and the conjecture was proved in 2017 separately by Zhuk (2017) and Bulatov (2017).","Zhuk's algorithm solves tractable CSPs in polynomial time.","The algorithm is partly based on universal algebra theorems: informally, they state that after reducing some domain of an instance to its strong subuniverses, a satisfiable instance maintains a solution.   ","In this paper, we present the formalization of the proofs of these theorems in the bounded arithmetic $W^1_1$ introduced by Skelley (2004).","The formalization, together with our previous results (2022), shows that $W_1^1$ proves the soundness of Zhuk's algorithm, where by soundness we mean that any rejection of the algorithm is correct.","From the known relation of the theory to propositional calculus $G$, it follows that tautologies, expressing the non-existence of a solution for unsatisfiable instances, have short proofs in $G$."],"url":"http://arxiv.org/abs/2403.06704v1","category":"math.LO"}
{"created":"2024-03-11 13:19:29","title":"A viscoelastic model for skin via homogenization theory","abstract":"We carry out the homogenization of a fluid-structure interaction problem consisting in the periodic inclusions of a viscous fluid in an elastic body. We get a macrostructure model where the body behaves as a viscoelastic material with a long-range memory term. Our aim is not only to get this limit problem but also to study its main properties. Using the micro-structure variables it is simple to check that it satisfies an energy conservation law assuring in particular the existence and uniqueness of solution. The difficulty is to characterize these properties using only the macroscopic system. We prove that the nonlocal term is given through a convolution kernel which exponentially decreases to zero and satisfies some positive conditions which we write in terms of the Laplace transform. These conditions can be used to directly prove the existence and uniqueness of solution. The results apply to modeling the mechanical behavior of skin as an indicator of what kind of models we should use. In a naive interpretation the fluid inclusions represent the cells and the elastic medium the extracellular matrix.","sentences":["We carry out the homogenization of a fluid-structure interaction problem consisting in the periodic inclusions of a viscous fluid in an elastic body.","We get a macrostructure model where the body behaves as a viscoelastic material with a long-range memory term.","Our aim is not only to get this limit problem but also to study its main properties.","Using the micro-structure variables it is simple to check that it satisfies an energy conservation law assuring in particular the existence and uniqueness of solution.","The difficulty is to characterize these properties using only the macroscopic system.","We prove that the nonlocal term is given through a convolution kernel which exponentially decreases to zero and satisfies some positive conditions which we write in terms of the Laplace transform.","These conditions can be used to directly prove the existence and uniqueness of solution.","The results apply to modeling the mechanical behavior of skin as an indicator of what kind of models we should use.","In a naive interpretation the fluid inclusions represent the cells and the elastic medium the extracellular matrix."],"url":"http://arxiv.org/abs/2403.06703v1","category":"math.AP"}
{"created":"2024-03-11 13:15:53","title":"Solving Distributed Flexible Job Shop Scheduling Problems in the Wool Textile Industry with Quantum Annealing","abstract":"Many modern manufacturing companies have evolved from a single production site to a multi-factory production environment that must handle both geographically dispersed production orders and their multi-site production steps. The availability of a range of machines in different locations capable of performing the same operation and shipping times between factories have transformed planning systems from the classic Job Shop Scheduling Problem (JSSP) to Distributed Flexible Job Shop Scheduling Problem (DFJSP). As a result, the complexity of production planning has increased significantly. In our work, we use Quantum Annealing (QA) to solve the DFJSP. In addition to the assignment of production orders to production sites, the assignment of production steps to production sites also takes place. This requirement is based on a real use case of a wool textile manufacturer. To investigate the applicability of this method to large problem instances, problems ranging from 50 variables up to 250 variables, the largest problem that could be embedded into a D-Wave quantum annealer Quantum Processing Unit (QPU), are formulated and solved. Special attention is dedicated to the determination of the Lagrange parameters of the Quadratic Unconstrained Binary Optimization (QUBO) model and the QPU configuration parameters, as these factors can significantly impact solution quality. The obtained solutions are compared to solutions obtained by Simulated Annealing (SA), both in terms of solution quality and calculation time. The results demonstrate that QA has the potential to solve large problem instances specific to the industry.","sentences":["Many modern manufacturing companies have evolved from a single production site to a multi-factory production environment that must handle both geographically dispersed production orders and their multi-site production steps.","The availability of a range of machines in different locations capable of performing the same operation and shipping times between factories have transformed planning systems from the classic Job Shop Scheduling Problem (JSSP) to Distributed Flexible Job Shop Scheduling Problem (DFJSP).","As a result, the complexity of production planning has increased significantly.","In our work, we use Quantum Annealing (QA) to solve the DFJSP.","In addition to the assignment of production orders to production sites, the assignment of production steps to production sites also takes place.","This requirement is based on a real use case of a wool textile manufacturer.","To investigate the applicability of this method to large problem instances, problems ranging from 50 variables up to 250 variables, the largest problem that could be embedded into a D-Wave quantum annealer Quantum Processing Unit (QPU), are formulated and solved.","Special attention is dedicated to the determination of the Lagrange parameters of the Quadratic Unconstrained Binary Optimization (QUBO) model and the QPU configuration parameters, as these factors can significantly impact solution quality.","The obtained solutions are compared to solutions obtained by Simulated Annealing (SA), both in terms of solution quality and calculation time.","The results demonstrate that QA has the potential to solve large problem instances specific to the industry."],"url":"http://arxiv.org/abs/2403.06699v1","category":"quant-ph"}
{"created":"2024-03-11 13:11:30","title":"On the smallness of mean oscillations on metric-measure spaces and applications","abstract":"It will be established that the mean oscillation of a function on a metric-measure space $X\\times Y$ will be small if its mean oscillation on $X$ is small and some simple information on its (partial $Y$) upper-gradient is given. Applications to the regularity and global existence of bounded solutions to strongly coupled elliptic/parabolic systems on thin domains are also considered.","sentences":["It will be established that the mean oscillation of a function on a metric-measure space $X\\times Y$ will be small if its mean oscillation on $X$ is small and some simple information on its (partial $Y$) upper-gradient is given.","Applications to the regularity and global existence of bounded solutions to strongly coupled elliptic/parabolic systems on thin domains are also considered."],"url":"http://arxiv.org/abs/2403.06696v1","category":"math.AP"}
{"created":"2024-03-11 13:04:21","title":"Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data","abstract":"Graph neural networks (GNNs) have proven effective in capturing relationships among nodes in a graph. This study introduces a novel perspective by considering a graph as a simplicial complex, encompassing nodes, edges, triangles, and $k$-simplices, enabling the definition of graph-structured data on any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous graph attention network (HL-HGAT), designed to learn heterogeneous signal representations across $k$-simplices. The HL-HGAT incorporates three key components: HL convolutional filters (HL-filters), simplicial projection (SP), and simplicial attention pooling (SAP) operators, applied to $k$-simplices. HL-filters leverage the unique topology of $k$-simplices encoded by the Hodge-Laplacian (HL) operator, operating within the spectral domain of the $k$-th HL operator. To address computation challenges, we introduce a polynomial approximation for HL-filters, exhibiting spatial localization properties. Additionally, we propose a pooling operator to coarsen $k$-simplices, combining features through simplicial attention mechanisms of self-attention and cross-attention via transformers and SP operators, capturing topological interconnections across multiple dimensions of simplices. The HL-HGAT is comprehensively evaluated across diverse graph applications, including NP-hard problems, graph multi-label and classification challenges, and graph regression tasks in logistics, computer vision, biology, chemistry, and neuroscience. The results demonstrate the model's efficacy and versatility in handling a wide range of graph-based scenarios.","sentences":["Graph neural networks (GNNs) have proven effective in capturing relationships among nodes in a graph.","This study introduces a novel perspective by considering a graph as a simplicial complex, encompassing nodes, edges, triangles, and $k$-simplices, enabling the definition of graph-structured data on any $k$-simplices.","Our contribution is the Hodge-Laplacian heterogeneous graph attention network (HL-HGAT), designed to learn heterogeneous signal representations across $k$-simplices.","The HL-HGAT incorporates three key components: HL convolutional filters (HL-filters), simplicial projection (SP), and simplicial attention pooling (SAP) operators, applied to $k$-simplices.","HL-filters leverage the unique topology of $k$-simplices encoded by the Hodge-Laplacian (HL) operator, operating within the spectral domain of the $k$-th HL operator.","To address computation challenges, we introduce a polynomial approximation for HL-filters, exhibiting spatial localization properties.","Additionally, we propose a pooling operator to coarsen $k$-simplices, combining features through simplicial attention mechanisms of self-attention and cross-attention via transformers and SP operators, capturing topological interconnections across multiple dimensions of simplices.","The HL-HGAT is comprehensively evaluated across diverse graph applications, including NP-hard problems, graph multi-label and classification challenges, and graph regression tasks in logistics, computer vision, biology, chemistry, and neuroscience.","The results demonstrate the model's efficacy and versatility in handling a wide range of graph-based scenarios."],"url":"http://arxiv.org/abs/2403.06687v1","category":"cs.LG"}
{"created":"2024-03-11 12:55:08","title":"Identifikation ausl\u00f6sender Umst\u00e4nde von SOTIF-Gef\u00e4hrdungen durch systemtheoretische Prozessanalyse","abstract":"Developers have to obtain a sound understanding of existing risk potentials already in the concept phase of driverless vehicles. Deductive as well as inductive SOTIF analyses of potential triggering conditions for hazardous behavior help to achieve this goal. In this regard, ISO 21448 suggests conducting a System-Theoretic Process Analysis (STPA). In this article, we introduce German terminology for SOTIF considerations and critically discuss STPA theory in the course of an example application, while also proposing methodological additions. -- --   Um bereits in der Konzeptphase autonomer Fahrzeuge einen fundierten Eindruck bestehender Risikopotenziale zu erhalten, werden im Zuge von deduktiven und induktiven SOTIF-Analysen m\\\"ogliche ausl\\\"osende Umst\\\"ande f\\\"ur gef\\\"ahrliches Verhalten untersucht. In diesem Zusammenhang wird in der ISO 21448 die Durchf\\\"uhrung einer systemtheoretischen Prozessanalyse (STPA) vorgeschlagen. In diesem Beitrag f\\\"uhren wir deutsche Terminologie f\\\"ur SOTIF-Betrachtungen ein und setzen uns im Zuge einer Anwendung kritisch mit der STPA-Theorie auseinander, wobei wir begleitend methodische Erg\\\"anzungen anregen.","sentences":["Developers have to obtain a sound understanding of existing risk potentials already in the concept phase of driverless vehicles.","Deductive as well as inductive SOTIF analyses of potential triggering conditions for hazardous behavior help to achieve this goal.","In this regard, ISO 21448 suggests conducting a System-Theoretic Process Analysis (STPA).","In this article, we introduce German terminology for SOTIF considerations and critically discuss STPA theory in the course of an example application, while also proposing methodological additions.","-- --   ","Um bereits in der Konzeptphase autonomer Fahrzeuge einen fundierten Eindruck bestehender Risikopotenziale zu erhalten, werden im Zuge von deduktiven und induktiven SOTIF-Analysen m\\\"ogliche ausl\\\"osende Umst\\\"ande f\\\"ur gef\\\"ahrliches Verhalten untersucht.","In diesem Zusammenhang wird in der ISO 21448 die Durchf\\\"uhrung einer systemtheoretischen Prozessanalyse (STPA) vorgeschlagen.","In diesem Beitrag f\\\"uhren wir deutsche","Terminologie f\\\"ur SOTIF-Betrachtungen ein und setzen uns im","Zuge einer Anwendung kritisch mit der STPA-Theorie auseinander, wobei wir begleitend methodische Erg\\\"anzungen anregen."],"url":"http://arxiv.org/abs/2403.06680v1","category":"eess.SY"}
{"created":"2024-03-11 12:48:22","title":"CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object Localization Perspective","abstract":"Recently, convolutional neural networks (CNNs) with large size kernels have attracted much attention in the computer vision field, following the success of the Vision Transformers. Large kernel CNNs have been reported to perform well in downstream vision tasks as well as in classification performance. The reason for the high-performance of large kernel CNNs in downstream tasks has been attributed to the large effective receptive field (ERF) produced by large size kernels, but this view has not been fully tested. We therefore revisit the performance of large kernel CNNs in downstream task, focusing on the weakly supervised object localization (WSOL) task. WSOL, a difficult downstream task that is not fully supervised, provides a new angle to explore the capabilities of the large kernel CNNs. Our study compares the modern large kernel CNNs ConvNeXt, RepLKNet, and SLaK to test the validity of the naive expectation that ERF size is important for improving downstream task performance. Our analysis of the factors contributing to high performance provides a different perspective, in which the main factor is feature map improvement. Furthermore, we find that modern CNNs are robust to the CAM problems of local regions of objects being activated, which has long been discussed in WSOL. CAM is the most classic WSOL method, but because of the above-mentioned problems, it is often used as a baseline method for comparison. However, experiments on the CUB-200-2011 dataset show that simply combining a large kernel CNN, CAM, and simple data augmentation methods can achieve performance (90.99% MaxBoxAcc) comparable to the latest WSOL method, which is CNN-based and requires special training or complex post-processing. The code is available at https://github.com/snskysk/CAM-Back-Again.","sentences":["Recently, convolutional neural networks (CNNs) with large size kernels have attracted much attention in the computer vision field, following the success of the Vision Transformers.","Large kernel CNNs have been reported to perform well in downstream vision tasks as well as in classification performance.","The reason for the high-performance of large kernel CNNs in downstream tasks has been attributed to the large effective receptive field (ERF) produced by large size kernels, but this view has not been fully tested.","We therefore revisit the performance of large kernel CNNs in downstream task, focusing on the weakly supervised object localization (WSOL) task.","WSOL, a difficult downstream task that is not fully supervised, provides a new angle to explore the capabilities of the large kernel CNNs.","Our study compares the modern large kernel CNNs ConvNeXt, RepLKNet, and SLaK to test the validity of the naive expectation that ERF size is important for improving downstream task performance.","Our analysis of the factors contributing to high performance provides a different perspective, in which the main factor is feature map improvement.","Furthermore, we find that modern CNNs are robust to the CAM problems of local regions of objects being activated, which has long been discussed in WSOL.","CAM is the most classic WSOL method, but because of the above-mentioned problems, it is often used as a baseline method for comparison.","However, experiments on the CUB-200-2011 dataset show that simply combining a large kernel CNN, CAM, and simple data augmentation methods can achieve performance (90.99% MaxBoxAcc) comparable to the latest WSOL method, which is CNN-based and requires special training or complex post-processing.","The code is available at https://github.com/snskysk/CAM-Back-Again."],"url":"http://arxiv.org/abs/2403.06676v1","category":"cs.CV"}
{"created":"2024-03-11 12:45:59","title":"Ensemble inequivalence in long-range quantum systems","abstract":"Ensemble inequivalence, i.e. the possibility of observing different thermodynamic properties depending on the statistical ensemble which describes the system, is one of the hallmarks of long-range physics, which has been demonstrated in numerous classical systems. Here, an example of ensemble inequivalence of a long-range quantum ferromagnet is presented. While the $T=0$ microcanonical quantum phase-diagram coincides with that in the canonical ensemble, the phase-diagrams of the two ensembles are different at finite temperature. This is in contrast with the common lore of statistical mechanics of systems with short-range interactions where thermodynamic properties are bound to coincide for macroscopic systems described by different ensembles. The consequences of these findings in the context of atomic, molecular and optical (AMO) setups are delineated.","sentences":["Ensemble inequivalence, i.e. the possibility of observing different thermodynamic properties depending on the statistical ensemble which describes the system, is one of the hallmarks of long-range physics, which has been demonstrated in numerous classical systems.","Here, an example of ensemble inequivalence of a long-range quantum ferromagnet is presented.","While the $T=0$ microcanonical quantum phase-diagram coincides with that in the canonical ensemble, the phase-diagrams of the two ensembles are different at finite temperature.","This is in contrast with the common lore of statistical mechanics of systems with short-range interactions where thermodynamic properties are bound to coincide for macroscopic systems described by different ensembles.","The consequences of these findings in the context of atomic, molecular and optical (AMO) setups are delineated."],"url":"http://arxiv.org/abs/2403.06673v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-11 12:35:21","title":"Optimal Bounds for Distinct Quartics","abstract":"A fundamental concept related to strings is that of repetitions. It has been extensively studied in many versions, from both purely combinatorial and algorithmic angles. One of the most basic questions is how many distinct squares, i.e., distinct strings of the form $UU$, a string of length $n$ can contain as fragments. It turns out that this is always $\\mathcal{O}(n)$, and the bound cannot be improved to sublinear in $n$ [Fraenkel and Simpson, JCTA 1998].   Several similar questions about repetitions in strings have been considered, and by now we seem to have a good understanding of their repetitive structure. For higher-dimensional strings, the basic concept of periodicity has been successfully extended and applied to design efficient algorithms -- it is inherently more complex than for regular strings. Extending the notion of repetitions and understanding the repetitive structure of higher-dimensional strings is however far from complete.   Quartics were introduced by Apostolico and Brimkov [TCS 2000] as analogues of squares in two dimensions. Charalampopoulos, Radoszewski, Rytter, Wale\\'n, and Zuba [ESA 2020] proved that the number of distinct quartics in an $n\\times n$ 2D string is $\\mathcal{O}(n^2 \\log^2 n)$ and that they can be computed in $\\mathcal{O}(n^2 \\log^2 n)$ time. Gawrychowski, Ghazawi, and Landau [SPIRE 2021] constructed an infinite family of $n \\times n$ 2D strings with $\\Omega(n^2 \\log n)$ distinct quartics. This brings the challenge of determining asymptotically tight bounds. Here, we settle both the combinatorial and the algorithmic aspects of this question: the number of distinct quartics in an $n\\times n$ 2D string is $\\mathcal{O}(n^2 \\log n)$ and they can be computed in the worst-case optimal $\\mathcal{O}(n^2 \\log n)$ time.","sentences":["A fundamental concept related to strings is that of repetitions.","It has been extensively studied in many versions, from both purely combinatorial and algorithmic angles.","One of the most basic questions is how many distinct squares, i.e., distinct strings of the form $UU$, a string of length $n$ can contain as fragments.","It turns out that this is always $\\mathcal{O}(n)$, and the bound cannot be improved to sublinear in $n$ [Fraenkel and Simpson, JCTA 1998].   ","Several similar questions about repetitions in strings have been considered, and by now we seem to have a good understanding of their repetitive structure.","For higher-dimensional strings, the basic concept of periodicity has been successfully extended and applied to design efficient algorithms -- it is inherently more complex than for regular strings.","Extending the notion of repetitions and understanding the repetitive structure of higher-dimensional strings is however far from complete.   ","Quartics were introduced by Apostolico and Brimkov [TCS 2000] as analogues of squares in two dimensions.","Charalampopoulos, Radoszewski, Rytter, Wale\\'n, and Zuba","[ESA 2020] proved that the number of distinct quartics in an $n\\times n$ 2D string is $\\mathcal{O}(n^2 \\log^2 n)$ and that they can be computed in $\\mathcal{O}(n^2 \\log^2 n)$ time.","Gawrychowski, Ghazawi, and Landau","[SPIRE 2021] constructed an infinite family of $n \\times n$ 2D strings with $\\Omega(n^2 \\log n)$ distinct quartics.","This brings the challenge of determining asymptotically tight bounds.","Here, we settle both the combinatorial and the algorithmic aspects of this question: the number of distinct quartics in an $n\\times n$ 2D string is $\\mathcal{O}(n^2 \\log n)$","and they can be computed in the worst-case optimal $\\mathcal{O}(n^2 \\log n)$ time."],"url":"http://arxiv.org/abs/2403.06667v1","category":"cs.DS"}
{"created":"2024-03-11 12:32:51","title":"Beyond Pairwise: Higher-order physical interactions affect phase separation in multi-component liquids","abstract":"Phase separation, crucial for spatially segregating biomolecules in cells, is well-understood in the simple case of a few components with pairwise interactions. Yet, biological cells challenge the simple picture in at least two ways: First, biomolecules, like proteins and nucleic acids, exhibit complex, higher-order interactions, where a single molecule may interact with multiple others simultaneously. Second, cells comprise a myriad of different components that form various droplets. Such multicomponent phase separation has been studied in the simple case of pairwise interactions, but an analysis of higher-order interactions is lacking. We propose such a theory and study the corresponding phase diagrams numerically. We find that interactions between three components are similar to pairwise interactions, whereas composition-dependent higher-order interactions between two components can oppose phase separation. This surprising result can only be revealed from the equilibrium phase diagrams, implying that the often-used stability analysis of homogeneous states is inadequate to study these systems. We thus show that higher-order interactions could play a crucial role in forming droplets in cells, and their manipulation could offer novel approaches to controlling multicomponent phase separation.","sentences":["Phase separation, crucial for spatially segregating biomolecules in cells, is well-understood in the simple case of a few components with pairwise interactions.","Yet, biological cells challenge the simple picture in at least two ways: First, biomolecules, like proteins and nucleic acids, exhibit complex, higher-order interactions, where a single molecule may interact with multiple others simultaneously.","Second, cells comprise a myriad of different components that form various droplets.","Such multicomponent phase separation has been studied in the simple case of pairwise interactions, but an analysis of higher-order interactions is lacking.","We propose such a theory and study the corresponding phase diagrams numerically.","We find that interactions between three components are similar to pairwise interactions, whereas composition-dependent higher-order interactions between two components can oppose phase separation.","This surprising result can only be revealed from the equilibrium phase diagrams, implying that the often-used stability analysis of homogeneous states is inadequate to study these systems.","We thus show that higher-order interactions could play a crucial role in forming droplets in cells, and their manipulation could offer novel approaches to controlling multicomponent phase separation."],"url":"http://arxiv.org/abs/2403.06666v1","category":"cond-mat.soft"}
{"created":"2024-03-11 12:32:14","title":"Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System","abstract":"The recent huge advance of Large Language Models (LLMs) is mainly driven by the increase in the number of parameters. This has led to substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity. One popular solution to this is storage-offloaded training, which uses host memory and storage as an extended memory hierarchy. However, this obviously comes at the cost of storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck of storage-offloaded LLM training using near-storage processing devices on a real system. The main component of Smart-Infinity is SmartUpdate, which performs parameter updates on custom near-storage accelerators. We identify that moving parameter updates to the storage side removes most of the storage traffic. In addition, we propose an efficient data transfer handler structure to address the system integration issues for Smart-Infinity. The handler allows overlapping data transfers with fixed memory consumption by reusing the device buffer. Lastly, we propose accelerator-assisted gradient compression/decompression to enhance the scalability of Smart-Infinity. When scaling to multiple near-storage processing devices, the write traffic on the shared channel becomes the bottleneck. To alleviate this, we compress the gradients on the GPU and decompress them on the accelerators. It provides further acceleration from reduced traffic. As a result, Smart-Infinity achieves a significant speedup compared to the baseline. Notably, Smart-Infinity is a ready-to-use approach that is fully integrated into PyTorch on a real system. We will open-source Smart-Infinity to facilitate its use.","sentences":["The recent huge advance of Large Language Models (LLMs) is mainly driven by the increase in the number of parameters.","This has led to substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity.","One popular solution to this is storage-offloaded training, which uses host memory and storage as an extended memory hierarchy.","However, this obviously comes at the cost of storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories.","Our work, Smart-Infinity, addresses the storage bandwidth bottleneck of storage-offloaded LLM training using near-storage processing devices on a real system.","The main component of Smart-Infinity is SmartUpdate, which performs parameter updates on custom near-storage accelerators.","We identify that moving parameter updates to the storage side removes most of the storage traffic.","In addition, we propose an efficient data transfer handler structure to address the system integration issues for Smart-Infinity.","The handler allows overlapping data transfers with fixed memory consumption by reusing the device buffer.","Lastly, we propose accelerator-assisted gradient compression/decompression to enhance the scalability of Smart-Infinity.","When scaling to multiple near-storage processing devices, the write traffic on the shared channel becomes the bottleneck.","To alleviate this, we compress the gradients on the GPU and decompress them on the accelerators.","It provides further acceleration from reduced traffic.","As a result, Smart-Infinity achieves a significant speedup compared to the baseline.","Notably, Smart-Infinity is a ready-to-use approach that is fully integrated into PyTorch on a real system.","We will open-source Smart-Infinity to facilitate its use."],"url":"http://arxiv.org/abs/2403.06664v1","category":"cs.AR"}
{"created":"2024-03-11 12:19:49","title":"Thermalization in Krylov Basis","abstract":"We study thermalization for closed non-integrable quantum systems in the Krylov basis. Following the idea of the eigenstate thermalization hypothesis, one may introduce Krylov basis thermalization hypothesis which imposes a condition on the matrix elements of local operators in the Krylov basis. In this context, the nature of thermalization may be probed by the infinite time average of the Krylov complexity. We also compute the variance of Lanczos coefficients which may provide another quantity to examine the nature of thermalization. We will see that there is a direct relation between the behavior of the infinite time average of complexity and that of the inverse participation ratio of initial states.","sentences":["We study thermalization for closed non-integrable quantum systems in the Krylov basis.","Following the idea of the eigenstate thermalization hypothesis, one may introduce Krylov basis thermalization hypothesis which imposes a condition on the matrix elements of local operators in the Krylov basis.","In this context, the nature of thermalization may be probed by the infinite time average of the Krylov complexity.","We also compute the variance of Lanczos coefficients which may provide another quantity to examine the nature of thermalization.","We will see that there is a direct relation between the behavior of the infinite time average of complexity and that of the inverse participation ratio of initial states."],"url":"http://arxiv.org/abs/2403.06655v1","category":"quant-ph"}
{"created":"2024-03-11 12:16:14","title":"SoniWeight Shoes: Investigating Effects and Personalization of a Wearable Sound Device for Altering Body Perception and Behavior","abstract":"Changes in body perception influence behavior and emotion and can be induced through multisensory feedback. Auditory feedback to one's actions can trigger such alterations; however, it is unclear which individual factors modulate these effects. We employ and evaluate SoniWeight Shoes, a wearable device based on literature for altering one's weight perception through manipulated footstep sounds. In a healthy population sample across a spectrum of individuals (n=84) with varying degrees of eating disorder symptomatology, physical activity levels, body concerns, and mental imagery capacities, we explore the effects of three sound conditions (low-frequency, high-frequency and control) on extensive body perception measures (demographic, behavioral, physiological, psychological, and subjective). Analyses revealed an impact of individual differences in each of these dimensions. Besides replicating previous findings, we reveal and highlight the role of individual differences in body perception, offering avenues for personalized sonification strategies. Datasets, technical refinements, and novel body map quantification tools are provided.","sentences":["Changes in body perception influence behavior and emotion and can be induced through multisensory feedback.","Auditory feedback to one's actions can trigger such alterations; however, it is unclear which individual factors modulate these effects.","We employ and evaluate SoniWeight Shoes, a wearable device based on literature for altering one's weight perception through manipulated footstep sounds.","In a healthy population sample across a spectrum of individuals (n=84) with varying degrees of eating disorder symptomatology, physical activity levels, body concerns, and mental imagery capacities, we explore the effects of three sound conditions (low-frequency, high-frequency and control) on extensive body perception measures (demographic, behavioral, physiological, psychological, and subjective).","Analyses revealed an impact of individual differences in each of these dimensions.","Besides replicating previous findings, we reveal and highlight the role of individual differences in body perception, offering avenues for personalized sonification strategies.","Datasets, technical refinements, and novel body map quantification tools are provided."],"url":"http://arxiv.org/abs/2403.06651v1","category":"cs.HC"}
{"created":"2024-03-11 12:15:18","title":"Bond-Order Density Wave Phases in Dimerized Extended Bose-Hubbard Models","abstract":"The Bose-Hubbard model (BHM) has been widely explored to develop a profound understanding of the strongly correlated behavior of interacting bosons. Quantum simulators not only allow the exploration of the BHM but also extend it to models with interesting phenomena such as gapped phases with multiple orders and topological phases. In this work, an extended Bose-Hubbard model involving a dimerized one-dimensional model of long-range interacting hard-core bosons is studied. Bond-order density wave phases (BODW) are characterized in terms of their symmetry breaking and topological properties. At certain fillings, interactions combined with dimerized hoppings give rise to an emergent symmetry-breaking leading to BODW phases, which differs from the case of non-interacting models that require an explicit breaking of the symmetry. Specifically, the BODW phase at filling $\\rho=1/3$ possesses no analogue in the non-interacting model in terms of its symmetry-breaking properties and the unit cell structure. Upon changing the dimerization pattern, the system realizes topologically trivial BODW phases. At filling $\\rho=1/4$, on-site density modulations are shown to stabilize the topological BODW phase. Our work provides the bridge between interacting and non-interacting BODW phases and highlights the significance of long-range interactions in a dimerized lattice by showing unique BODW phases that do not exist in the non-interacting model.","sentences":["The Bose-Hubbard model (BHM) has been widely explored to develop a profound understanding of the strongly correlated behavior of interacting bosons.","Quantum simulators not only allow the exploration of the BHM but also extend it to models with interesting phenomena such as gapped phases with multiple orders and topological phases.","In this work, an extended Bose-Hubbard model involving a dimerized one-dimensional model of long-range interacting hard-core bosons is studied.","Bond-order density wave phases (BODW) are characterized in terms of their symmetry breaking and topological properties.","At certain fillings, interactions combined with dimerized hoppings give rise to an emergent symmetry-breaking leading to BODW phases, which differs from the case of non-interacting models that require an explicit breaking of the symmetry.","Specifically, the BODW phase at filling $\\rho=1/3$ possesses no analogue in the non-interacting model in terms of its symmetry-breaking properties and the unit cell structure.","Upon changing the dimerization pattern, the system realizes topologically trivial BODW phases.","At filling $\\rho=1/4$, on-site density modulations are shown to stabilize the topological BODW phase.","Our work provides the bridge between interacting and non-interacting BODW phases and highlights the significance of long-range interactions in a dimerized lattice by showing unique BODW phases that do not exist in the non-interacting model."],"url":"http://arxiv.org/abs/2403.06649v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-11 12:02:30","title":"Socio-spatial segregation and human mobility: A review of empirical evidence","abstract":"Social segregation, the spatial and social separation between individuals from different backgrounds, can affect sustainable urban development and social cohesion. The literature has traditionally focused on residential segregation, examining how individuals' residential locations are distributed differently across neighborhoods based on income, ethnicity, and education. However, this approach overlooks the complexity of spatial segregation because daily activities often extend far beyond residential areas. Over the past one to two decades, emerging mobility data sources have enabled a new understanding of socio-spatial segregation by considering daily activities such as work, school, shopping, and leisure visits. From traditional surveys to GPS trajectories, diverse data sources reveal that day-to-day movements can impact segregation by reducing or amplifying segregation levels obtained when considering residential aspects alone. This literature review focuses on three critical questions: (a) to what extent do individual mobility patterns contribute to segregation? (b) Which factors explain the role played by mobility in segregation? and (c) What insights are gained by incorporating extensive mobility data into segregation research? Our literature review contributes to an improved understanding of socio-spatial segregation at the individual level and offers actionable insights into reducing segregation and addressing research gaps in the field.","sentences":["Social segregation, the spatial and social separation between individuals from different backgrounds, can affect sustainable urban development and social cohesion.","The literature has traditionally focused on residential segregation, examining how individuals' residential locations are distributed differently across neighborhoods based on income, ethnicity, and education.","However, this approach overlooks the complexity of spatial segregation because daily activities often extend far beyond residential areas.","Over the past one to two decades, emerging mobility data sources have enabled a new understanding of socio-spatial segregation by considering daily activities such as work, school, shopping, and leisure visits.","From traditional surveys to GPS trajectories, diverse data sources reveal that day-to-day movements can impact segregation by reducing or amplifying segregation levels obtained when considering residential aspects alone.","This literature review focuses on three critical questions: (a) to what extent do individual mobility patterns contribute to segregation?","(b) Which factors explain the role played by mobility in segregation?","and (c) What insights are gained by incorporating extensive mobility data into segregation research?","Our literature review contributes to an improved understanding of socio-spatial segregation at the individual level and offers actionable insights into reducing segregation and addressing research gaps in the field."],"url":"http://arxiv.org/abs/2403.06641v1","category":"cs.SI"}
{"created":"2024-03-11 11:59:24","title":"Passive iFIR filters for data-driven control","abstract":"We consider the design of a new class of passive iFIR controllers given by the parallel action of an integrator and a finite impulse response filter. iFIRs are more expressive than PID controllers but retain their features and simplicity. The paper provides a model-free data-driven design for passive iFIR controllers based on virtual reference feedback tuning. Passivity is enforced through constrained optimization (three different formulations are discussed). The proposed design does not rely on large datasets or accurate plant models.","sentences":["We consider the design of a new class of passive iFIR controllers given by the parallel action of an integrator and a finite impulse response filter.","iFIRs are more expressive than PID controllers but retain their features and simplicity.","The paper provides a model-free data-driven design for passive iFIR controllers based on virtual reference feedback tuning.","Passivity is enforced through constrained optimization (three different formulations are discussed).","The proposed design does not rely on large datasets or accurate plant models."],"url":"http://arxiv.org/abs/2403.06640v1","category":"eess.SY"}
{"created":"2024-03-11 11:58:27","title":"Robust and fast backbone tracking via phase-locked loops","abstract":"Phase-locked loops are commonly used for shaker-based backbone tracking of nonlinear structures. The state of the art is to tune the control parameters by trial and error. In the present work, an approach is proposed to make backbone tracking much more robust and faster. A simple PI controller is proposed, and closed-form expressions for the gains are provided that lead to an optimal settling of the phase transient. The required input parameters are obtained from a conventional shaker-based linear modal test, and an open-loop sine test at a single frequency and level. For phase detection, an adaptive filter based on the LMS algorithm is used, which is shown to be superior to the synchronous demodulation commonly used. Once the phase has locked, one can directly take the next step along the backbone, eliminating the hold times. The latter are currently used for recording the steady state, and to estimate Fourier coefficients in the post-process, which becomes unnecessary since the adaptive filter yields a highly accurate estimation at runtime. The excellent performance of the proposed approach is demonstrated for a doubly clamped beam undergoing bending-stretching coupling leading to a 20 percent shift of the lowest modal frequency. Even for fixed control parameters, designed for the linear regime, only about 100 periods are needed per backbone point, also in the nonlinear regime. This is much faster than what has been reported in the literature so far. In fact, the nonlinear backbone test becomes faster than the linear modal test, shifting the established paradigm.","sentences":["Phase-locked loops are commonly used for shaker-based backbone tracking of nonlinear structures.","The state of the art is to tune the control parameters by trial and error.","In the present work, an approach is proposed to make backbone tracking much more robust and faster.","A simple PI controller is proposed, and closed-form expressions for the gains are provided that lead to an optimal settling of the phase transient.","The required input parameters are obtained from a conventional shaker-based linear modal test, and an open-loop sine test at a single frequency and level.","For phase detection, an adaptive filter based on the LMS algorithm is used, which is shown to be superior to the synchronous demodulation commonly used.","Once the phase has locked, one can directly take the next step along the backbone, eliminating the hold times.","The latter are currently used for recording the steady state, and to estimate Fourier coefficients in the post-process, which becomes unnecessary since the adaptive filter yields a highly accurate estimation at runtime.","The excellent performance of the proposed approach is demonstrated for a doubly clamped beam undergoing bending-stretching coupling leading to a 20 percent shift of the lowest modal frequency.","Even for fixed control parameters, designed for the linear regime, only about 100 periods are needed per backbone point, also in the nonlinear regime.","This is much faster than what has been reported in the literature so far.","In fact, the nonlinear backbone test becomes faster than the linear modal test, shifting the established paradigm."],"url":"http://arxiv.org/abs/2403.06639v1","category":"eess.SY"}
{"created":"2024-03-11 11:47:07","title":"Aggregated distribution grid flexibilities in subtransmission grid operational management","abstract":"Aggregated flexibilities or PQ-capabilities (active and reactive power capabilities) are termed in literature as Feasible Operating Regions (FORs). The FORs from underlying active distribution grids can effectively contribute to the operational management at the HV grid level. The HV buses are allocated aggregated FORs from the underlying MV grids, which are inherently nonlinear and non-convex. Therefore, two approaches are proposed in the paper to apply the FOR constraints in the HV grid operational management. First, a mixed integer linear programming (MILP) based optimization approach for alleviating the HV grid constraint violations is proposed, which addresses the non-convexity of the FOR using piecewise segmentation. Furthermore, the MILP method is enhanced to consider the influence of the HV bus voltage on the underlying MV grid flexibilities resulting in a three dimensional PQ(V)-FOR. Second, a convexification approach is proposed, which uses a convex approximation of the non-convex 3D PQ(V)-FOR shape for implementation in a linear optimization method. Results reveal a robust utilization of the distribution flexibilities to maintain grid security and reliability at the HV grid level. Comparisons present increased computation times for the MILP method which are significantly improved using the convexification based approach.","sentences":["Aggregated flexibilities or PQ-capabilities (active and reactive power capabilities) are termed in literature as Feasible Operating Regions (FORs).","The FORs from underlying active distribution grids can effectively contribute to the operational management at the HV grid level.","The HV buses are allocated aggregated FORs from the underlying MV grids, which are inherently nonlinear and non-convex.","Therefore, two approaches are proposed in the paper to apply the FOR constraints in the HV grid operational management.","First, a mixed integer linear programming (MILP) based optimization approach for alleviating the HV grid constraint violations is proposed, which addresses the non-convexity of the FOR using piecewise segmentation.","Furthermore, the MILP method is enhanced to consider the influence of the HV bus voltage on the underlying MV grid flexibilities resulting in a three dimensional PQ(V)-FOR.","Second, a convexification approach is proposed, which uses a convex approximation of the non-convex 3D PQ(V)-FOR shape for implementation in a linear optimization method.","Results reveal a robust utilization of the distribution flexibilities to maintain grid security and reliability at the HV grid level.","Comparisons present increased computation times for the MILP method which are significantly improved using the convexification based approach."],"url":"http://arxiv.org/abs/2403.06635v1","category":"eess.SY"}
{"created":"2024-03-11 11:43:40","title":"Self-Sovereign Identity for Electric Vehicle Charging","abstract":"Electric Vehicles (EVs) are more and more charged at public Charge Points (CPs) using Plug-and-Charge (PnC) protocols such as the ISO 15118 standard which eliminates user interaction for authentication and authorization. Currently, this requires a rather complex Public Key Infrastructure (PKI) and enables driver tracking via the included unique identifiers. In this paper, we propose an approach for using Self-Sovereign Identities (SSIs) as trusted credentials for EV charging authentication and authorization which overcomes the privacy problems and the issues of a complex centralized PKI. Our implementation shows the feasibility of our approach with ISO 15118. The security and privacy of the proposed approach is shown in a formal analysis using the Tamarin prover.","sentences":["Electric Vehicles (EVs) are more and more charged at public Charge Points (CPs) using Plug-and-Charge (PnC) protocols such as the ISO 15118 standard which eliminates user interaction for authentication and authorization.","Currently, this requires a rather complex Public Key Infrastructure (PKI) and enables driver tracking via the included unique identifiers.","In this paper, we propose an approach for using Self-Sovereign Identities (SSIs) as trusted credentials for EV charging authentication and authorization which overcomes the privacy problems and the issues of a complex centralized PKI.","Our implementation shows the feasibility of our approach with ISO 15118.","The security and privacy of the proposed approach is shown in a formal analysis using the Tamarin prover."],"url":"http://arxiv.org/abs/2403.06632v1","category":"cs.CR"}
{"created":"2024-03-11 11:27:53","title":"SmartML: Towards a Modeling Language for Smart Contracts","abstract":"Smart contracts codify real-world transactions and automatically execute the terms of the contract when predefined conditions are met. This paper proposes SmartML, a modeling language for smart contracts that is platform independent and easy to comprehend. We detail the formal semantics and the type system, focusing on its role in addressing security vulnerabilities and attacks. Through case studies we show how SmartML contributes to the prevention of reentrancy attacks, illustrating its efficacy in reinforcing the reliability and security of smart contracts within decentralized systems.","sentences":["Smart contracts codify real-world transactions and automatically execute the terms of the contract when predefined conditions are met.","This paper proposes SmartML, a modeling language for smart contracts that is platform independent and easy to comprehend.","We detail the formal semantics and the type system, focusing on its role in addressing security vulnerabilities and attacks.","Through case studies we show how SmartML contributes to the prevention of reentrancy attacks, illustrating its efficacy in reinforcing the reliability and security of smart contracts within decentralized systems."],"url":"http://arxiv.org/abs/2403.06622v1","category":"cs.SE"}
{"created":"2024-03-11 11:21:25","title":"On l-th roots and division by l","abstract":"We give a characterization of the codomain [l]E(k) of the multiplication-by-l map [l] in the case of elliptic curves over a field k of characteristic different from 2 and 3, with l-torsion E[l]=<W1,W2> fully defined over k, for primes l different from the characteristic. We show that a point Q in E(k) lies in [l]E(k) if and only if hW1(-Q) and hW2(-Q) are l-powers of k, where hW1 and hW2 are functions on E with divisor div(hWi)= l Wi- l P_infty. Our characterization leads to an effective procedure to find pre-images of [l] by solving an order l system of linear equations and computing a polynomial gcd.","sentences":["We give a characterization of the codomain [l]E(k) of the multiplication-by-l map [l] in the case of elliptic curves over a field k of characteristic different from 2 and 3, with l-torsion E[l]=<W1,W2> fully defined over k, for primes l different from the characteristic.","We show that a point Q in E(k) lies in [l]E(k) if and only if hW1(-Q) and hW2(-Q) are l-powers of k, where hW1 and hW2 are functions on E with divisor div(hWi)= l","Wi-","l P_infty.","Our characterization leads to an effective procedure to find pre-images of [l] by solving an order l system of linear equations and computing a polynomial gcd."],"url":"http://arxiv.org/abs/2403.06619v1","category":"math.NT"}
{"created":"2024-03-11 11:06:41","title":"Density-Guided Label Smoothing for Temporal Localization of Driving Actions","abstract":"Temporal localization of driving actions plays a crucial role in advanced driver-assistance systems and naturalistic driving studies. However, this is a challenging task due to strict requirements for robustness, reliability and accurate localization. In this work, we focus on improving the overall performance by efficiently utilizing video action recognition networks and adapting these to the problem of action localization. To this end, we first develop a density-guided label smoothing technique based on label probability distributions to facilitate better learning from boundary video-segments that typically include multiple labels. Second, we design a post-processing step to efficiently fuse information from video-segments and multiple camera views into scene-level predictions, which facilitates elimination of false positives. Our methodology yields a competitive performance on the A2 test set of the naturalistic driving action recognition track of the 2022 NVIDIA AI City Challenge with an F1 score of 0.271.","sentences":["Temporal localization of driving actions plays a crucial role in advanced driver-assistance systems and naturalistic driving studies.","However, this is a challenging task due to strict requirements for robustness, reliability and accurate localization.","In this work, we focus on improving the overall performance by efficiently utilizing video action recognition networks and adapting these to the problem of action localization.","To this end, we first develop a density-guided label smoothing technique based on label probability distributions to facilitate better learning from boundary video-segments that typically include multiple labels.","Second, we design a post-processing step to efficiently fuse information from video-segments and multiple camera views into scene-level predictions, which facilitates elimination of false positives.","Our methodology yields a competitive performance on the A2 test set of the naturalistic driving action recognition track of the 2022 NVIDIA AI City Challenge with an F1 score of 0.271."],"url":"http://arxiv.org/abs/2403.06616v1","category":"cs.CV"}
{"created":"2024-03-11 10:52:22","title":"Balanced Substructures in Bicolored Graphs","abstract":"An edge-colored graph is said to be balanced if it has an equal number of edges of each color. Given a graph $G$ whose edges are colored using two colors and a positive integer $k$, the objective in the Edge Balanced Connected Subgraph problem is to determine if $G$ has a balanced connected subgraph containing at least $k$ edges. We first show that this problem is NP-complete and remains so even if the solution is required to be a tree or a path. Then, we focus on the parameterized complexity of Edge Balanced Connected Subgraph and its variants (where the balanced subgraph is required to be a path/tree) with respect to $k$ as the parameter. Towards this, we show that if a graph has a balanced connected subgraph/tree/path of size at least $k$, then it has one of size at least $k$ and at most $f(k)$ where $f$ is a linear function. We use this result combined with dynamic programming algorithms based on color coding and representative sets to show that Edge Balanced Connected Subgraph and its variants are FPT. Further, using polynomial-time reductions to the Multilinear Monomial Detection problem, we give faster randomized FPT algorithms for the problems. In order to describe these reductions, we define a combinatorial object called relaxed-subgraph. We define this object in such a way that balanced connected subgraphs, trees and paths are relaxed-subgraphs with certain properties. This object is defined in the spirit of branching walks known for the Steiner Tree problem and may be of independent interest.","sentences":["An edge-colored graph is said to be balanced if it has an equal number of edges of each color.","Given a graph $G$ whose edges are colored using two colors and a positive integer $k$, the objective in the Edge Balanced Connected Subgraph problem is to determine if $G$ has a balanced connected subgraph containing at least $k$ edges.","We first show that this problem is NP-complete and remains so even if the solution is required to be a tree or a path.","Then, we focus on the parameterized complexity of Edge Balanced Connected Subgraph and its variants (where the balanced subgraph is required to be a path/tree) with respect to $k$ as the parameter.","Towards this, we show that if a graph has a balanced connected subgraph/tree/path of size at least $k$, then it has one of size at least $k$ and at most $f(k)$ where $f$ is a linear function.","We use this result combined with dynamic programming algorithms based on color coding and representative sets to show that Edge Balanced Connected Subgraph and its variants are FPT.","Further, using polynomial-time reductions to the Multilinear Monomial Detection problem, we give faster randomized FPT algorithms for the problems.","In order to describe these reductions, we define a combinatorial object called relaxed-subgraph.","We define this object in such a way that balanced connected subgraphs, trees and paths are relaxed-subgraphs with certain properties.","This object is defined in the spirit of branching walks known for the Steiner Tree problem and may be of independent interest."],"url":"http://arxiv.org/abs/2403.06608v1","category":"cs.DS"}
{"created":"2024-03-11 10:51:39","title":"Decoding Complexity: Exploring Human-AI Concordance in Qualitative Coding","abstract":"Qualitative data analysis provides insight into the underlying perceptions and experiences within unstructured data. However, the time-consuming nature of the coding process, especially for larger datasets, calls for innovative approaches, such as the integration of Large Language Models (LLMs). This short paper presents initial findings from a study investigating the integration of LLMs for coding tasks of varying complexity in a real-world dataset. Our results highlight the challenges inherent in coding with extensive codebooks and contexts, both for human coders and LLMs, and suggest that the integration of LLMs into the coding process requires a task-by-task evaluation. We examine factors influencing the complexity of coding tasks and initiate a discussion on the usefulness and limitations of incorporating LLMs in qualitative research.","sentences":["Qualitative data analysis provides insight into the underlying perceptions and experiences within unstructured data.","However, the time-consuming nature of the coding process, especially for larger datasets, calls for innovative approaches, such as the integration of Large Language Models (LLMs).","This short paper presents initial findings from a study investigating the integration of LLMs for coding tasks of varying complexity in a real-world dataset.","Our results highlight the challenges inherent in coding with extensive codebooks and contexts, both for human coders and LLMs, and suggest that the integration of LLMs into the coding process requires a task-by-task evaluation.","We examine factors influencing the complexity of coding tasks and initiate a discussion on the usefulness and limitations of incorporating LLMs in qualitative research."],"url":"http://arxiv.org/abs/2403.06607v1","category":"cs.HC"}
{"created":"2024-03-11 10:49:44","title":"Thermoelectric transport of the coexistence topological semimetal in the quantum limit","abstract":"We explore the thermoelectric transport properties of a coexistence topological semimetal, characterized by the presence of both a pair of Weyl points and a nodal ring in the quantum limit. This system gives rise to complex Landau bands when subjected to a magnetic field aligned with the direction connecting two Weyl points. In the longitudinal configuration, where the magnetic field is parallel to the electric field or the temperature gradient, the thermoelectric conductivity indicates a plateau independent of the magnetic field and the Fermi energy at $\\delta$-form short-range scattering. This platform structure should also exist in pure two-node Weyl semimetals. However, the thermoelectric conductivity and the Seebeck coefficient are significantly influenced by the parameters of long-ranged Gaussian or screened Coulomb scattering potentials for both fixed carrier density and Fermi energy scenarios. In the transverse configuration, both Gaussian and screened Coulomb scatterings yield substantial positive magnetoresistance and thermoelectric conductance. Since the Hall conductivity is larger than the longitudinal one, the Seebeck coefficient, exhibiting a quadratic increase with the magnetic field, is close to the dissipationless limit irrespective of scatterings, while the Nernst response is notably dependent on the scattering mechanism. Additionally, the model parameter, distinct from the two-node Weyl model, influences the thermoelectric transport properties. The magnetic field response of the thermoelectric coefficients to different scattering potentials can be used as a basis for distinguishing scattering mechanisms in materials.","sentences":["We explore the thermoelectric transport properties of a coexistence topological semimetal, characterized by the presence of both a pair of Weyl points and a nodal ring in the quantum limit.","This system gives rise to complex Landau bands when subjected to a magnetic field aligned with the direction connecting two Weyl points.","In the longitudinal configuration, where the magnetic field is parallel to the electric field or the temperature gradient, the thermoelectric conductivity indicates a plateau independent of the magnetic field and the Fermi energy at $\\delta$-form short-range scattering.","This platform structure should also exist in pure two-node Weyl semimetals.","However, the thermoelectric conductivity and the Seebeck coefficient are significantly influenced by the parameters of long-ranged Gaussian or screened Coulomb scattering potentials for both fixed carrier density and Fermi energy scenarios.","In the transverse configuration, both Gaussian and screened Coulomb scatterings yield substantial positive magnetoresistance and thermoelectric conductance.","Since the Hall conductivity is larger than the longitudinal one, the Seebeck coefficient, exhibiting a quadratic increase with the magnetic field, is close to the dissipationless limit irrespective of scatterings, while the Nernst response is notably dependent on the scattering mechanism.","Additionally, the model parameter, distinct from the two-node Weyl model, influences the thermoelectric transport properties.","The magnetic field response of the thermoelectric coefficients to different scattering potentials can be used as a basis for distinguishing scattering mechanisms in materials."],"url":"http://arxiv.org/abs/2403.06603v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 10:35:33","title":"HDA-LVIO: A High-Precision LiDAR-Visual-Inertial Odometry in Urban Environments with Hybrid Data Association","abstract":"To enhance localization accuracy in urban environments, an innovative LiDAR-Visual-Inertial odometry, named HDA-LVIO, is proposed by employing hybrid data association. The proposed HDA_LVIO system can be divided into two subsystems: the LiDAR-Inertial subsystem (LIS) and the Visual-Inertial subsystem (VIS). In the LIS, the LiDAR pointcloud is utilized to calculate the Iterative Closest Point (ICP) error, serving as the measurement value of Error State Iterated Kalman Filter (ESIKF) to construct the global map. In the VIS, an incremental method is firstly employed to adaptively extract planes from the global map. And the centroids of these planes are projected onto the image to obtain projection points. Then, feature points are extracted from the image and tracked along with projection points using Lucas-Kanade (LK) optical flow. Next, leveraging the vehicle states from previous intervals, sliding window optimization is performed to estimate the depth of feature points. Concurrently, a method based on epipolar geometric constraints is proposed to address tracking failures for feature points, which can improve the accuracy of depth estimation for feature points by ensuring sufficient parallax within the sliding window. Subsequently, the feature points and projection points are hybridly associated to construct reprojection error, serving as the measurement value of ESIKF to estimate vehicle states. Finally, the localization accuracy of the proposed HDA-LVIO is validated using public datasets and data from our equipment. The results demonstrate that the proposed algorithm achieves obviously improvement in localization accuracy compared to various existing algorithms.","sentences":["To enhance localization accuracy in urban environments, an innovative LiDAR-Visual-Inertial odometry, named HDA-LVIO, is proposed by employing hybrid data association.","The proposed HDA_LVIO system can be divided into two subsystems: the LiDAR-Inertial subsystem (LIS) and the Visual-Inertial subsystem (VIS).","In the LIS, the LiDAR pointcloud is utilized to calculate the Iterative Closest Point (ICP) error, serving as the measurement value of Error State Iterated Kalman Filter (ESIKF) to construct the global map.","In the VIS, an incremental method is firstly employed to adaptively extract planes from the global map.","And the centroids of these planes are projected onto the image to obtain projection points.","Then, feature points are extracted from the image and tracked along with projection points using Lucas-Kanade (LK) optical flow.","Next, leveraging the vehicle states from previous intervals, sliding window optimization is performed to estimate the depth of feature points.","Concurrently, a method based on epipolar geometric constraints is proposed to address tracking failures for feature points, which can improve the accuracy of depth estimation for feature points by ensuring sufficient parallax within the sliding window.","Subsequently, the feature points and projection points are hybridly associated to construct reprojection error, serving as the measurement value of ESIKF to estimate vehicle states.","Finally, the localization accuracy of the proposed HDA-LVIO is validated using public datasets and data from our equipment.","The results demonstrate that the proposed algorithm achieves obviously improvement in localization accuracy compared to various existing algorithms."],"url":"http://arxiv.org/abs/2403.06590v1","category":"cs.RO"}
{"created":"2024-03-11 10:29:50","title":"Data Poisoning Attacks in Gossip Learning","abstract":"Traditional machine learning systems were designed in a centralized manner. In such designs, the central entity maintains both the machine learning model and the data used to adjust the model's parameters. As data centralization yields privacy issues, Federated Learning was introduced to reduce data sharing and have a central server coordinate the learning of multiple devices. While Federated Learning is more decentralized, it still relies on a central entity that may fail or be subject to attacks, provoking the failure of the whole system. Then, Decentralized Federated Learning removes the need for a central server entirely, letting participating processes handle the coordination of the model construction. This distributed control urges studying the possibility of malicious attacks by the participants themselves. While poisoning attacks on Federated Learning have been extensively studied, their effects in Decentralized Federated Learning did not get the same level of attention. Our work is the first to propose a methodology to assess poisoning attacks in Decentralized Federated Learning in both churn free and churn prone scenarios. Furthermore, in order to evaluate our methodology on a case study representative for gossip learning we extended the gossipy simulator with an attack injector module.","sentences":["Traditional machine learning systems were designed in a centralized manner.","In such designs, the central entity maintains both the machine learning model and the data used to adjust the model's parameters.","As data centralization yields privacy issues, Federated Learning was introduced to reduce data sharing and have a central server coordinate the learning of multiple devices.","While Federated Learning is more decentralized, it still relies on a central entity that may fail or be subject to attacks, provoking the failure of the whole system.","Then, Decentralized Federated Learning removes the need for a central server entirely, letting participating processes handle the coordination of the model construction.","This distributed control urges studying the possibility of malicious attacks by the participants themselves.","While poisoning attacks on Federated Learning have been extensively studied, their effects in Decentralized Federated Learning did not get the same level of attention.","Our work is the first to propose a methodology to assess poisoning attacks in Decentralized Federated Learning in both churn free and churn prone scenarios.","Furthermore, in order to evaluate our methodology on a case study representative for gossip learning we extended the gossipy simulator with an attack injector module."],"url":"http://arxiv.org/abs/2403.06583v1","category":"cs.DC"}
{"created":"2024-03-11 10:27:28","title":"Edge Information Hub: Orchestrating Satellites, UAVs, MEC, Sensing and Communications for 6G Closed-Loop Controls","abstract":"An increasing number of field robots would be used for mission-critical tasks in remote or post-disaster areas. Due to usually-limited individual abilities, these robots require an edge information hub (EIH), which is capable of not only communications but also sensing and computing. Such EIH could be deployed on a flexibly-dispatched unmanned aerial vehicle (UAV). Different from traditional aerial base stations or mobile edge computing (MEC), the EIH would direct the operations of robots via sensing-communication-computing-control ($\\textbf{SC}^3$) closed-loop orchestration. This paper aims to optimize the closed-loop control performance of multiple $\\textbf{SC}^3$ loops, under the constraints of satellite-backhaul rate, computing capability, and on-board energy. Specifically, the linear quadratic regulator (LQR) control cost is used to measure the closed-loop utility, and a sum LQR cost minimization problem is formulated to jointly optimize the splitting of sensor data and allocation of communication and computing resources. We first derive the optimal splitting ratio of sensor data, and then recast the problem to a more tractable form. An iterative algorithm is finally proposed to provide a sub-optimal solution. Simulation results demonstrate the superiority of the proposed algorithm. We also uncover the influence of $\\textbf{SC}^3$ parameters on closed-loop controls, highlighting more systematic understanding.","sentences":["An increasing number of field robots would be used for mission-critical tasks in remote or post-disaster areas.","Due to usually-limited individual abilities, these robots require an edge information hub (EIH), which is capable of not only communications but also sensing and computing.","Such EIH could be deployed on a flexibly-dispatched unmanned aerial vehicle (UAV).","Different from traditional aerial base stations or mobile edge computing (MEC), the EIH would direct the operations of robots via sensing-communication-computing-control ($\\textbf{SC}^3$) closed-loop orchestration.","This paper aims to optimize the closed-loop control performance of multiple $\\textbf{SC}^3$ loops, under the constraints of satellite-backhaul rate, computing capability, and on-board energy.","Specifically, the linear quadratic regulator (LQR) control cost is used to measure the closed-loop utility, and a sum LQR cost minimization problem is formulated to jointly optimize the splitting of sensor data and allocation of communication and computing resources.","We first derive the optimal splitting ratio of sensor data, and then recast the problem to a more tractable form.","An iterative algorithm is finally proposed to provide a sub-optimal solution.","Simulation results demonstrate the superiority of the proposed algorithm.","We also uncover the influence of $\\textbf{SC}^3$ parameters on closed-loop controls, highlighting more systematic understanding."],"url":"http://arxiv.org/abs/2403.06579v1","category":"eess.SY"}
{"created":"2024-03-11 10:20:44","title":"Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings","abstract":"Mastering autonomous drone landing on dynamic platforms presents formidable challenges due to unpredictable velocities and external disturbances caused by the wind, ground effect, turbines or propellers of the docking platform. This study introduces an advanced Deep Reinforcement Learning (DRL) agent, Lander.AI, designed to navigate and land on platforms in the presence of windy conditions, thereby enhancing drone autonomy and safety. Lander.AI is rigorously trained within the gym-pybullet-drone simulation, an environment that mirrors real-world complexities, including wind turbulence, to ensure the agent's robustness and adaptability.   The agent's capabilities were empirically validated with Crazyflie 2.1 drones across various test scenarios, encompassing both simulated environments and real-world conditions. The experimental results showcased Lander.AI's high-precision landing and its ability to adapt to moving platforms, even under wind-induced disturbances. Furthermore, the system performance was benchmarked against a baseline PID controller augmented with an Extended Kalman Filter, illustrating significant improvements in landing precision and error recovery. Lander.AI leverages bio-inspired learning to adapt to external forces like birds, enhancing drone adaptability without knowing force magnitudes.This research not only advances drone landing technologies, essential for inspection and emergency applications, but also highlights the potential of DRL in addressing intricate aerodynamic challenges.","sentences":["Mastering autonomous drone landing on dynamic platforms presents formidable challenges due to unpredictable velocities and external disturbances caused by the wind, ground effect, turbines or propellers of the docking platform.","This study introduces an advanced Deep Reinforcement Learning (DRL) agent, Lander.AI, designed to navigate and land on platforms in the presence of windy conditions, thereby enhancing drone autonomy and safety.","Lander.","AI is rigorously trained within the gym-pybullet-drone simulation, an environment that mirrors real-world complexities, including wind turbulence, to ensure the agent's robustness and adaptability.   ","The agent's capabilities were empirically validated with Crazyflie 2.1 drones across various test scenarios, encompassing both simulated environments and real-world conditions.","The experimental results showcased Lander.","AI's high-precision landing and its ability to adapt to moving platforms, even under wind-induced disturbances.","Furthermore, the system performance was benchmarked against a baseline PID controller augmented with an Extended Kalman Filter, illustrating significant improvements in landing precision and error recovery.","Lander.","AI leverages bio-inspired learning to adapt to external forces like birds, enhancing drone adaptability without knowing force magnitudes.","This research not only advances drone landing technologies, essential for inspection and emergency applications, but also highlights the potential of DRL in addressing intricate aerodynamic challenges."],"url":"http://arxiv.org/abs/2403.06572v1","category":"cs.RO"}
{"created":"2024-03-11 10:11:29","title":"Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting Applications","abstract":"Past studies on end-to-end meeting transcription have focused on model architecture and have mostly been evaluated on simulated meeting data. We present a novel study aiming to optimize the use of a Speaker-Attributed ASR (SA-ASR) system in real-life scenarios, such as the AMI meeting corpus, for improved speaker assignment of speech segments. First, we propose a pipeline tailored to real-life applications involving Voice Activity Detection (VAD), Speaker Diarization (SD), and SA-ASR. Second, we advocate using VAD output segments to fine-tune the SA-ASR model, considering that it is also applied to VAD segments during test, and show that this results in a relative reduction of Speaker Error Rate (SER) up to 28%. Finally, we explore strategies to enhance the extraction of the speaker embedding templates used as inputs by the SA-ASR system. We show that extracting them from SD output rather than annotated speaker segments results in a relative SER reduction up to 20%.","sentences":["Past studies on end-to-end meeting transcription have focused on model architecture and have mostly been evaluated on simulated meeting data.","We present a novel study aiming to optimize the use of a Speaker-Attributed ASR (SA-ASR) system in real-life scenarios, such as the AMI meeting corpus, for improved speaker assignment of speech segments.","First, we propose a pipeline tailored to real-life applications involving Voice Activity Detection (VAD), Speaker Diarization (SD), and SA-ASR.","Second, we advocate using VAD output segments to fine-tune the SA-ASR model, considering that it is also applied to VAD segments during test, and show that this results in a relative reduction of Speaker Error Rate (SER) up to 28%.","Finally, we explore strategies to enhance the extraction of the speaker embedding templates used as inputs by the SA-ASR system.","We show that extracting them from SD output rather than annotated speaker segments results in a relative SER reduction up to 20%."],"url":"http://arxiv.org/abs/2403.06570v1","category":"cs.CL"}
{"created":"2024-03-11 10:06:38","title":"Ride-pooling Electric Autonomous Mobility-on-Demand: Joint Optimization of Operations and Fleet and Infrastructure Design","abstract":"This paper presents a modeling and design optimization framework for an Electric Autonomous Mobility-on-Demand system that allows for ride-pooling, i.e., multiple users can be transported at the same time towards a similar direction to decrease vehicle hours traveled by the fleet at the cost of additional waiting time and delays caused by detours. In particular, we first devise a multi-layer time-invariant network flow model that jointly captures the position and state of charge of the vehicles. Second, we frame the time-optimal operational problem of the fleet, including charging and ride-pooling decisions as a mixed-integer linear program, whereby we jointly optimize the placement of the charging infrastructure. Finally, we perform a case-study using Manhattan taxi-data. Our results indicate that jointly optimizing the charging infrastructure placement allows to decrease overall energy consumption of the fleet and vehicle hours traveled by approximately 1% compared to an heuristic placement. Most significantly, ride-pooling can decrease such costs considerably more, and up to 45%. Finally, we investigate the impact of the vehicle choice on the energy consumption of the fleet, comparing a lightweight two-seater with a heavier four-seater, whereby our results show that the former and latter designs are most convenient for low- and high-demand areas, respectively.","sentences":["This paper presents a modeling and design optimization framework for an Electric Autonomous Mobility-on-Demand system that allows for ride-pooling, i.e., multiple users can be transported at the same time towards a similar direction to decrease vehicle hours traveled by the fleet at the cost of additional waiting time and delays caused by detours.","In particular, we first devise a multi-layer time-invariant network flow model that jointly captures the position and state of charge of the vehicles.","Second, we frame the time-optimal operational problem of the fleet, including charging and ride-pooling decisions as a mixed-integer linear program, whereby we jointly optimize the placement of the charging infrastructure.","Finally, we perform a case-study using Manhattan taxi-data.","Our results indicate that jointly optimizing the charging infrastructure placement allows to decrease overall energy consumption of the fleet and vehicle hours traveled by approximately 1% compared to an heuristic placement.","Most significantly, ride-pooling can decrease such costs considerably more, and up to 45%.","Finally, we investigate the impact of the vehicle choice on the energy consumption of the fleet, comparing a lightweight two-seater with a heavier four-seater, whereby our results show that the former and latter designs are most convenient for low- and high-demand areas, respectively."],"url":"http://arxiv.org/abs/2403.06566v1","category":"eess.SY"}
{"created":"2024-03-11 10:00:55","title":"Waiting times for sea level variations in Port of Trieste: from scale-free to exponential-like distributions","abstract":"We report here a series of detailed statistical analyses on the sea level variations in the Port of Trieste using one of the largest existing catalogues that covers more than a century of measurements. We show that the distribution of waiting times, which are defined here akin to econophysics, namely the series of shortest time spans between a given sea level L and the next sea level of at least L + {\\delta} in the catalogue, exhibits a distinct scale-free character for small values of {\\delta}, while for larger values of {\\delta} the distribution is very similar to the exponential distribution. The distribution of waiting times for small values of {\\delta} is typical for complex systems exhibiting criticality and is reported abundantly in the literature, while the exponential-like distribution observed for large values of {\\delta} has been observed in contexts as diverse as magnetic systems and light sleep duration.","sentences":["We report here a series of detailed statistical analyses on the sea level variations in the Port of Trieste using one of the largest existing catalogues that covers more than a century of measurements.","We show that the distribution of waiting times, which are defined here akin to econophysics, namely the series of shortest time spans between a given sea level L and the next sea level of at least L + {\\delta} in the catalogue, exhibits a distinct scale-free character for small values of {\\delta}, while for larger values of {\\delta} the distribution is very similar to the exponential distribution.","The distribution of waiting times for small values of {\\delta} is typical for complex systems exhibiting criticality and is reported abundantly in the literature, while the exponential-like distribution observed for large values of {\\delta} has been observed in contexts as diverse as magnetic systems and light sleep duration."],"url":"http://arxiv.org/abs/2403.06559v1","category":"physics.comp-ph"}
{"created":"2024-03-11 10:00:42","title":"Nonlinear spatial evolution of degenerate quartets of water waves","abstract":"In this manuscript we investigate the Benjamin-Feir (or modulation) instability for the spatial evolution of water waves from the perspective of the discrete, spatial Zakharov equation, which captures cubically nonlinear and resonant wave interactions in deep water without restrictions on spectral bandwidth. Spatial evolution, with measurements at discrete locations, is pertinent for laboratory hydrodynamic experiments, such as in wave flumes, which rely on time-series measurements at a series of fixed gauges installed along the facility. This setting is likewise appropriate for experiments in electromagnetic and plasma waves. Through a reformulation of the problem for a degenerate quartet, we bring to bear techniques of phase-plane analysis which elucidate the full dynamics without recourse to linear stability analysis. In particular we find hitherto unexplored breather solutions and discuss the optimal transfer of energy from carrier to sidebands. Finally, we discuss the observability of such discrete solutions in light of numerical simulations.","sentences":["In this manuscript we investigate the Benjamin-Feir (or modulation) instability for the spatial evolution of water waves from the perspective of the discrete, spatial Zakharov equation, which captures cubically nonlinear and resonant wave interactions in deep water without restrictions on spectral bandwidth.","Spatial evolution, with measurements at discrete locations, is pertinent for laboratory hydrodynamic experiments, such as in wave flumes, which rely on time-series measurements at a series of fixed gauges installed along the facility.","This setting is likewise appropriate for experiments in electromagnetic and plasma waves.","Through a reformulation of the problem for a degenerate quartet, we bring to bear techniques of phase-plane analysis which elucidate the full dynamics without recourse to linear stability analysis.","In particular we find hitherto unexplored breather solutions and discuss the optimal transfer of energy from carrier to sidebands.","Finally, we discuss the observability of such discrete solutions in light of numerical simulations."],"url":"http://arxiv.org/abs/2403.06558v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 09:59:41","title":"Modeling iPSC-derived Endothelial Cell Transition in Tumor Angiogenesis using Petri Nets","abstract":"Tumor angiogenesis concerns the development of new blood vessels supplying the necessary nutrients for the further development of existing tumor cells. The entire process is complex, involving the production and consumption of chemicals, endothelial cell transitions as well as cell interactions, divisions, and migrations. Microfluidic cell culture platform has been used to study angiogenesis of endothelial cells derived from human induced pluripotent stem cells (iPSC-ECs) for a physiological relevant micro-environment. In this paper, we elaborate on how to define a pipeline for simulating the transformation and process that an iPSC-derived endothelial cell goes through in this biological scenario. We leverage the robustness and simplicity of Petri nets for modeling the cell transformation and associated constraints. The environmental and spacial factors are added using custom 2-dimensional grids. Although the pipeline does not capture the entire complexity of tumor angiogenesis, we are able to capture the essence of endothelial cell transitions in tumor angiogenesis using this conceptually simplified solution.","sentences":["Tumor angiogenesis concerns the development of new blood vessels supplying the necessary nutrients for the further development of existing tumor cells.","The entire process is complex, involving the production and consumption of chemicals, endothelial cell transitions as well as cell interactions, divisions, and migrations.","Microfluidic cell culture platform has been used to study angiogenesis of endothelial cells derived from human induced pluripotent stem cells (iPSC-ECs) for a physiological relevant micro-environment.","In this paper, we elaborate on how to define a pipeline for simulating the transformation and process that an iPSC-derived endothelial cell goes through in this biological scenario.","We leverage the robustness and simplicity of Petri nets for modeling the cell transformation and associated constraints.","The environmental and spacial factors are added using custom 2-dimensional grids.","Although the pipeline does not capture the entire complexity of tumor angiogenesis, we are able to capture the essence of endothelial cell transitions in tumor angiogenesis using this conceptually simplified solution."],"url":"http://arxiv.org/abs/2403.06555v1","category":"q-bio.CB"}
{"created":"2024-03-11 09:38:17","title":"Forward completeness implies bounded reachable sets for time-delay systems on the state space of essentially bounded measurable functions","abstract":"We consider time-delay systems with a finite number of delays in the state space $L^\\infty\\times\\R^n$. In this framework, we show that forward completeness implies the bounded reachability sets property, while this implication was recently shown by J.L. Mancilla-Aguilar and H. Haimovich to fail in the state space of continuous functions. As a consequence, we show that global asymptotic stability is always uniform in the state space $L^\\infty\\times\\R^n$.","sentences":["We consider time-delay systems with a finite number of delays in the state space $L^\\infty\\times\\R^n$. In this framework, we show that forward completeness implies the bounded reachability sets property, while this implication was recently shown by J.L. Mancilla-Aguilar and H. Haimovich to fail in the state space of continuous functions.","As a consequence, we show that global asymptotic stability is always uniform in the state space $L^\\infty\\times\\R^n$."],"url":"http://arxiv.org/abs/2403.06543v1","category":"math.OC"}
{"created":"2024-03-11 09:31:42","title":"Elasticity affects the shock-induced aerobreakup of a polymeric droplet","abstract":"Boger fluids are viscoelastic liquids having constant viscosity for a broad range of shear rates. They are commonly used to separate the effects of liquid elasticity from viscosity in any experiment. We present an experimental study on the shock-induced aerobreakup of a Boger fluid droplet in the Shear-induced entrainment (SIE) and catastrophic breakup regime (Weber number ranging from ~ 800 to 5000). The results are compared with the aerobreakup of a Newtonian droplet having similar viscosity, and with shear-thinning droplets. The study aims to identify the role of liquid elasticity without the added complexity of simultaneous shear-thinning behavior. It is observed that at the early stages of droplet breakup, liquid elasticity plays an insignificant role, and all the fluids show similar behavior. However, during the late stages, the impact of liquid elasticity becomes dominant, which results in a markedly different morphology of the fragmenting liquid mass compared to a Newtonian droplet.","sentences":["Boger fluids are viscoelastic liquids having constant viscosity for a broad range of shear rates.","They are commonly used to separate the effects of liquid elasticity from viscosity in any experiment.","We present an experimental study on the shock-induced aerobreakup of a Boger fluid droplet in the Shear-induced entrainment (SIE) and catastrophic breakup regime (Weber number ranging from ~ 800 to 5000).","The results are compared with the aerobreakup of a Newtonian droplet having similar viscosity, and with shear-thinning droplets.","The study aims to identify the role of liquid elasticity without the added complexity of simultaneous shear-thinning behavior.","It is observed that at the early stages of droplet breakup, liquid elasticity plays an insignificant role, and all the fluids show similar behavior.","However, during the late stages, the impact of liquid elasticity becomes dominant, which results in a markedly different morphology of the fragmenting liquid mass compared to a Newtonian droplet."],"url":"http://arxiv.org/abs/2403.06539v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 09:29:44","title":"3DRef: 3D Dataset and Benchmark for Reflection Detection in RGB and Lidar Data","abstract":"Reflective surfaces present a persistent challenge for reliable 3D mapping and perception in robotics and autonomous systems. However, existing reflection datasets and benchmarks remain limited to sparse 2D data. This paper introduces the first large-scale 3D reflection detection dataset containing more than 50,000 aligned samples of multi-return Lidar, RGB images, and 2D/3D semantic labels across diverse indoor environments with various reflections. Textured 3D ground truth meshes enable automatic point cloud labeling to provide precise ground truth annotations. Detailed benchmarks evaluate three Lidar point cloud segmentation methods, as well as current state-of-the-art image segmentation networks for glass and mirror detection. The proposed dataset advances reflection detection by providing a comprehensive testbed with precise global alignment, multi-modal data, and diverse reflective objects and materials. It will drive future research towards reliable reflection detection. The dataset is publicly available at http://3dref.github.io","sentences":["Reflective surfaces present a persistent challenge for reliable 3D mapping and perception in robotics and autonomous systems.","However, existing reflection datasets and benchmarks remain limited to sparse 2D data.","This paper introduces the first large-scale 3D reflection detection dataset containing more than 50,000 aligned samples of multi-return Lidar, RGB images, and 2D/3D semantic labels across diverse indoor environments with various reflections.","Textured 3D ground truth meshes enable automatic point cloud labeling to provide precise ground truth annotations.","Detailed benchmarks evaluate three Lidar point cloud segmentation methods, as well as current state-of-the-art image segmentation networks for glass and mirror detection.","The proposed dataset advances reflection detection by providing a comprehensive testbed with precise global alignment, multi-modal data, and diverse reflective objects and materials.","It will drive future research towards reliable reflection detection.","The dataset is publicly available at http://3dref.github.io"],"url":"http://arxiv.org/abs/2403.06538v1","category":"cs.RO"}
{"created":"2024-03-11 09:19:16","title":"Autonomous Overhead Powerline Recharging for Uninterrupted Drone Operations","abstract":"We present a fully autonomous self-recharging drone system capable of long-duration sustained operations near powerlines. The drone is equipped with a robust onboard perception and navigation system that enables it to locate powerlines and approach them for landing. A passively actuated gripping mechanism grasps the powerline cable during landing after which a control circuit regulates the magnetic field inside a split-core current transformer to provide sufficient holding force as well as battery recharging. The system is evaluated in an active outdoor three-phase powerline environment. We demonstrate multiple contiguous hours of fully autonomous uninterrupted drone operations composed of several cycles of flying, landing, recharging, and takeoff, validating the capability of extended, essentially unlimited, operational endurance.","sentences":["We present a fully autonomous self-recharging drone system capable of long-duration sustained operations near powerlines.","The drone is equipped with a robust onboard perception and navigation system that enables it to locate powerlines and approach them for landing.","A passively actuated gripping mechanism grasps the powerline cable during landing after which a control circuit regulates the magnetic field inside a split-core current transformer to provide sufficient holding force as well as battery recharging.","The system is evaluated in an active outdoor three-phase powerline environment.","We demonstrate multiple contiguous hours of fully autonomous uninterrupted drone operations composed of several cycles of flying, landing, recharging, and takeoff, validating the capability of extended, essentially unlimited, operational endurance."],"url":"http://arxiv.org/abs/2403.06533v1","category":"cs.RO"}
{"created":"2024-03-11 08:57:10","title":"Quantum Entanglement in a Diluted Magnetic Semiconductor Quantum Dot","abstract":"We investigated the entanglement in a diluted magnetic semiconductor quantum dot, crucial for quantum technologies. Despite their potential, these systems exhibit low extraction rates. We explore self-assembled InGaAs quantum dots, focusing on entanglement between them based on spin states. Our analysis involves defining wavefunctions, employing density matrix operators, and measuring entanglement entropy. Numerical assessments reveal few promising pairs among various quantum dot combinations that exhibit significant entanglement. Additionally, this work discusses theoretical developments and statistical evaluations of entanglement in diluted magnetic semiconductor quantum dots, providing insights into their potential for quantum applications.","sentences":["We investigated the entanglement in a diluted magnetic semiconductor quantum dot, crucial for quantum technologies.","Despite their potential, these systems exhibit low extraction rates.","We explore self-assembled InGaAs quantum dots, focusing on entanglement between them based on spin states.","Our analysis involves defining wavefunctions, employing density matrix operators, and measuring entanglement entropy.","Numerical assessments reveal few promising pairs among various quantum dot combinations that exhibit significant entanglement.","Additionally, this work discusses theoretical developments and statistical evaluations of entanglement in diluted magnetic semiconductor quantum dots, providing insights into their potential for quantum applications."],"url":"http://arxiv.org/abs/2403.06522v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 08:43:42","title":"Multiple Reachability in Linear Dynamical Systems","abstract":"We consider reachability decision problems for linear dynamical systems: Given a linear map on $\\mathbb{R}^d$ , together with source and target sets, determine whether there is a point in the source set whose orbit, obtained by repeatedly applying the linear map, enters the target set. When the source and target sets are semialgebraic, this problem can be reduced to a point-to-polytope reachability question. The latter is generally believed not to be substantially harder than the well-known Skolem and Positivity Problems. The situation is markedly different for multiple reachability, i.e. the question of whether the orbit visits the target set at least m times, for some given positive integer m. In this paper, we prove that when the source set is semialgebraic and the target set consists of a hyperplane, multiple reachability is undecidable; in fact we already obtain undecidability in ambient dimension d = 10 and with fixed m = 9. Moreover, as we observe that procedures for dimensions 3 up to 9 would imply strong results pertaining to effective solutions of Diophantine equations, we mainly focus on the affine plane ($\\mathbb{R}^2$). We obtain two main positive results. We show that multiple reachability is decidable for halfplane targets, and that it is also decidable for general semialgebraic targets, provided the linear map is a rotation. The latter result involves a new method, based on intersections of algebraic subgroups with subvarieties, due to Bombieri and Zannier.","sentences":["We consider reachability decision problems for linear dynamical systems: Given a linear map on $\\mathbb{R}^d$ , together with source and target sets, determine whether there is a point in the source set whose orbit, obtained by repeatedly applying the linear map, enters the target set.","When the source and target sets are semialgebraic, this problem can be reduced to a point-to-polytope reachability question.","The latter is generally believed not to be substantially harder than the well-known Skolem and Positivity Problems.","The situation is markedly different for multiple reachability, i.e. the question of whether the orbit visits the target set at least m times, for some given positive integer m. In this paper, we prove that when the source set is semialgebraic and the target set consists of a hyperplane, multiple reachability is undecidable; in fact we already obtain undecidability in ambient dimension d","= 10 and with fixed m = 9.","Moreover, as we observe that procedures for dimensions 3 up to 9 would imply strong results pertaining to effective solutions of Diophantine equations, we mainly focus on the affine plane ($\\mathbb{R}^2$).","We obtain two main positive results.","We show that multiple reachability is decidable for halfplane targets, and that it is also decidable for general semialgebraic targets, provided the linear map is a rotation.","The latter result involves a new method, based on intersections of algebraic subgroups with subvarieties, due to Bombieri and Zannier."],"url":"http://arxiv.org/abs/2403.06515v1","category":"cs.LO"}
{"created":"2024-03-11 08:36:03","title":"Giant graviton expansion of Schur index and quasimodular forms","abstract":"The flavored superconformal Schur index of $\\mathcal N=4$ $U(N)$ SYM has finite $N$ corrections encoded in its giant graviton expansion in terms of D3 branes wrapped in $AdS_{5}\\times S^{5}$. The key element of this decomposition is the non-trivial index of the theory living on the wrapped brane system. A remarkable feature of the Schur limit is that the brane index is an analytic continuation of the flavored index of $\\mathcal N=4$ $U(n)$ SYM, where $n$ is the total brane wrapping number. We exploit recent exact results about the Schur index of $\\mathcal N=4$ $U(N)$ SYM to evaluate the closed form of the the brane indices appearing in the giant graviton expansion. Away from the unflavored limit, they are characterized by quasimodular forms providing exact information at all orders in the index universal fugacity. As an application of these results, we present novel exact expressions for the giant graviton expansion of the unflavored Schur index in a class of four dimensional $\\mathcal N=2$ theories with equal central charges $a=c$, i.e. the non-Lagrangian theories $\\widehat{\\Gamma}(SU(N))$ with $\\Gamma = E_{6}, E_{7}, E_{8}$.","sentences":["The flavored superconformal Schur index of $\\mathcal N=4$ $U(N)$ SYM has finite $N$ corrections encoded in its giant graviton expansion in terms of D3 branes wrapped in $AdS_{5}\\times S^{5}$. The key element of this decomposition is the non-trivial index of the theory living on the wrapped brane system.","A remarkable feature of the Schur limit is that the brane index is an analytic continuation of the flavored index of $\\mathcal N=4$ $U(n)$ SYM, where $n$ is the total brane wrapping number.","We exploit recent exact results about the Schur index of $\\mathcal N=4$ $U(N)$ SYM to evaluate the closed form of the the brane indices appearing in the giant graviton expansion.","Away from the unflavored limit, they are characterized by quasimodular forms providing exact information at all orders in the index universal fugacity.","As an application of these results, we present novel exact expressions for the giant graviton expansion of the unflavored Schur index in a class of four dimensional $\\mathcal N=2$ theories with equal central charges $a=c$, i.e. the non-Lagrangian theories $\\widehat{\\Gamma}(SU(N))$ with $\\Gamma = E_{6}, E_{7}, E_{8}$."],"url":"http://arxiv.org/abs/2403.06509v1","category":"hep-th"}
{"created":"2024-03-11 08:23:37","title":"Scalable Distributed Optimization Despite Byzantine Adversaries","abstract":"The problem of distributed optimization requires a group of networked agents to compute a parameter that minimizes the average of their local cost functions. While there are a variety of distributed optimization algorithms that can solve this problem, they are typically vulnerable to ``Byzantine'' agents that do not follow the algorithm. Recent attempts to address this issue focus on single dimensional functions, or assume certain statistical properties of the functions at the agents. In this paper, we provide two resilient, scalable, distributed optimization algorithms for multi-dimensional functions. Our schemes involve two filters, (1) a distance-based filter and (2) a min-max filter, which each remove neighborhood states that are extreme (defined precisely in our algorithms) at each iteration. We show that these algorithms can mitigate the impact of up to $F$ (unknown) Byzantine agents in the neighborhood of each regular agent. In particular, we show that if the network topology satisfies certain conditions, all of the regular agents' states are guaranteed to converge to a bounded region that contains the minimizer of the average of the regular agents' functions.","sentences":["The problem of distributed optimization requires a group of networked agents to compute a parameter that minimizes the average of their local cost functions.","While there are a variety of distributed optimization algorithms that can solve this problem, they are typically vulnerable to ``Byzantine'' agents that do not follow the algorithm.","Recent attempts to address this issue focus on single dimensional functions, or assume certain statistical properties of the functions at the agents.","In this paper, we provide two resilient, scalable, distributed optimization algorithms for multi-dimensional functions.","Our schemes involve two filters, (1) a distance-based filter and (2) a min-max filter, which each remove neighborhood states that are extreme (defined precisely in our algorithms) at each iteration.","We show that these algorithms can mitigate the impact of up to $F$ (unknown) Byzantine agents in the neighborhood of each regular agent.","In particular, we show that if the network topology satisfies certain conditions, all of the regular agents' states are guaranteed to converge to a bounded region that contains the minimizer of the average of the regular agents' functions."],"url":"http://arxiv.org/abs/2403.06502v1","category":"cs.MA"}
{"created":"2024-03-11 08:11:46","title":"Incorporating Improved Sinusoidal Threshold-based Semi-supervised Method and Diffusion Models for Osteoporosis Diagnosis","abstract":"Osteoporosis is a common skeletal disease that seriously affects patients' quality of life. Traditional osteoporosis diagnosis methods are expensive and complex. The semi-supervised model based on diffusion model and class threshold sinusoidal decay proposed in this paper can automatically diagnose osteoporosis based on patient's imaging data, which has the advantages of convenience, accuracy, and low cost. Unlike previous semi-supervised models, all the unlabeled data used in this paper are generated by the diffusion model. Compared with real unlabeled data, synthetic data generated by the diffusion model show better performance. In addition, this paper proposes a novel pseudo-label threshold adjustment mechanism, Sinusoidal Threshold Decay, which can make the semi-supervised model converge more quickly and improve its performance. Specifically, the method is tested on a dataset including 749 dental panoramic images, and its achieved leading detect performance and produces a 80.10% accuracy.","sentences":["Osteoporosis is a common skeletal disease that seriously affects patients' quality of life.","Traditional osteoporosis diagnosis methods are expensive and complex.","The semi-supervised model based on diffusion model and class threshold sinusoidal decay proposed in this paper can automatically diagnose osteoporosis based on patient's imaging data, which has the advantages of convenience, accuracy, and low cost.","Unlike previous semi-supervised models, all the unlabeled data used in this paper are generated by the diffusion model.","Compared with real unlabeled data, synthetic data generated by the diffusion model show better performance.","In addition, this paper proposes a novel pseudo-label threshold adjustment mechanism, Sinusoidal Threshold Decay, which can make the semi-supervised model converge more quickly and improve its performance.","Specifically, the method is tested on a dataset including 749 dental panoramic images, and its achieved leading detect performance and produces a 80.10% accuracy."],"url":"http://arxiv.org/abs/2403.06498v1","category":"eess.IV"}
{"created":"2024-03-11 08:09:30","title":"QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning","abstract":"Transformer-based models have gained widespread popularity in both the computer vision (CV) and natural language processing (NLP) fields. However, significant challenges arise during post-training linear quantization, leading to noticeable reductions in inference accuracy. Our study focuses on uncovering the underlying causes of these accuracy drops and proposing a quantization-friendly fine-tuning method, \\textbf{QuantTune}. Firstly, our analysis revealed that, on average, 65\\% of quantization errors result from the precision loss incurred by the dynamic range amplification effect of outliers across the target Transformer-based models. Secondly, \\textbf{QuantTune} adjusts weights based on the deviation of outlier activations and effectively constrains the dynamic ranges of the problematic activations. As a result, it successfully mitigates the negative impact of outliers on the inference accuracy of quantized models. Lastly, \\textbf{QuantTune} can be seamlessly integrated into the back-propagation pass in the fine-tuning process without requiring extra complexity in inference software and hardware design. Our approach showcases significant improvements in post-training quantization across a range of Transformer-based models, including ViT, Bert-base, and OPT. QuantTune reduces accuracy drops by 12.09\\% at 8-bit quantization and 33.8\\% at 7-bit compared to top calibration methods, outperforming state-of-the-art solutions by over 18.84\\% across ViT models.","sentences":["Transformer-based models have gained widespread popularity in both the computer vision (CV) and natural language processing (NLP) fields.","However, significant challenges arise during post-training linear quantization, leading to noticeable reductions in inference accuracy.","Our study focuses on uncovering the underlying causes of these accuracy drops and proposing a quantization-friendly fine-tuning method, \\textbf{QuantTune}.","Firstly, our analysis revealed that, on average, 65\\% of quantization errors result from the precision loss incurred by the dynamic range amplification effect of outliers across the target Transformer-based models.","Secondly, \\textbf{QuantTune} adjusts weights based on the deviation of outlier activations and effectively constrains the dynamic ranges of the problematic activations.","As a result, it successfully mitigates the negative impact of outliers on the inference accuracy of quantized models.","Lastly, \\textbf{QuantTune} can be seamlessly integrated into the back-propagation pass in the fine-tuning process without requiring extra complexity in inference software and hardware design.","Our approach showcases significant improvements in post-training quantization across a range of Transformer-based models, including ViT, Bert-base, and OPT.","QuantTune reduces accuracy drops by 12.09\\% at 8-bit quantization and 33.8\\% at 7-bit compared to top calibration methods, outperforming state-of-the-art solutions by over 18.84\\% across ViT models."],"url":"http://arxiv.org/abs/2403.06497v1","category":"cs.CV"}
{"created":"2024-03-11 08:07:11","title":"An Efficient Solution to the 2D Visibility Problem in Cartesian Grid Maps and its Application in Heuristic Path Planning","abstract":"This paper introduces a novel, lightweight method to solve the visibility problem for 2D grids. The proposed method evaluates the existence of lines-of-sight from a source point to all other grid cells in a single pass with no preprocessing and independently of the number and shape of obstacles. It has a compute and memory complexity of $\\mathcal{O}(n)$, where $n = n_{x}\\times{} n_{y}$ is the size of the grid, and requires at most ten arithmetic operations per grid cell. In the proposed approach, we use a linear first-order hyperbolic partial differential equation to transport the visibility quantity in all directions. In order to accomplish that, we use an entropy-satisfying upwind scheme that converges to the true visibility polygon as the step size goes to zero. This dynamic-programming approach allows the evaluation of visibility for an entire grid orders of magnitude faster than typical ray-casting algorithms. We provide a practical application of our proposed algorithm by posing the visibility quantity as a heuristic and implementing a deterministic, local-minima-free path planner, setting apart the proposed planner from traditional methods. Lastly, we provide necessary algorithms and an open-source implementation of the proposed methods.","sentences":["This paper introduces a novel, lightweight method to solve the visibility problem for 2D grids.","The proposed method evaluates the existence of lines-of-sight from a source point to all other grid cells in a single pass with no preprocessing and independently of the number and shape of obstacles.","It has a compute and memory complexity of $\\mathcal{O}(n)$, where $n = n_{x}\\times{} n_{y}$ is the size of the grid, and requires at most ten arithmetic operations per grid cell.","In the proposed approach, we use a linear first-order hyperbolic partial differential equation to transport the visibility quantity in all directions.","In order to accomplish that, we use an entropy-satisfying upwind scheme that converges to the true visibility polygon as the step size goes to zero.","This dynamic-programming approach allows the evaluation of visibility for an entire grid orders of magnitude faster than typical ray-casting algorithms.","We provide a practical application of our proposed algorithm by posing the visibility quantity as a heuristic and implementing a deterministic, local-minima-free path planner, setting apart the proposed planner from traditional methods.","Lastly, we provide necessary algorithms and an open-source implementation of the proposed methods."],"url":"http://arxiv.org/abs/2403.06494v1","category":"cs.CG"}
{"created":"2024-03-11 07:57:48","title":"On Asymptotically Almost Periodic Solutions of the parabolic-elliptic Keller-Segel system on real hyperbolic Manifolds","abstract":"In this article we investigate the existence, uniqueness and exponential decay of asymptotically almost periodic solutions of the parabolic-elliptic Keller-Segel system on a real hyperbolic manifold. We prove the existence and uniqueness of such solutions in the linear equation case by using the dispersive and smoothing estimates of the heat semigroup. Then we pass to the well-posedness of semi-linear equation case by using the results of linear equation and fixed point arguments. The exponential decay is proven by using Gronwall's inequality.","sentences":["In this article we investigate the existence, uniqueness and exponential decay of asymptotically almost periodic solutions of the parabolic-elliptic Keller-Segel system on a real hyperbolic manifold.","We prove the existence and uniqueness of such solutions in the linear equation case by using the dispersive and smoothing estimates of the heat semigroup.","Then we pass to the well-posedness of semi-linear equation case by using the results of linear equation and fixed point arguments.","The exponential decay is proven by using Gronwall's inequality."],"url":"http://arxiv.org/abs/2403.06492v1","category":"math.AP"}
{"created":"2024-03-11 07:55:51","title":"Real eternal PDE solutions are not complex entire: a quadratic parabolic example","abstract":"In parabolic or hyperbolic PDEs, solutions which remain uniformly bounded for all real times $t=r\\in\\mathbb{R}$ are often called PDE entire or eternal. For example, consider the quadratic parabolic PDE \\begin{equation*} \\label{*} w_t=w_{xx}+6w^2-\\lambda, \\tag{*} \\end{equation*} for $0<x<\\tfrac{1}{2}$, under Neumann boundary conditions. By its gradient-like structure, all real eternal non-equilibrium orbits $\\Gamma(r)$ of \\eqref{*} are heteroclinic among equilibria $w=W_n(x)$. All nontrivial real $W_n$ are rescaled and properly translated real-valued Weierstrass elliptic functions $W_n(x)=-n^2 \\wp(nx+\\tfrac{1}{2}\\tau),\\ n=1,2,3,\\ldots$, with lattice periods 1 and the purely imaginary modular parameter $\\tau$, and with Morse index $i(W_n)=n$ at $\\lambda=\\tfrac{1}{2}n^4 g_2(\\tau)$.   We show that the complex time extensions $\\Gamma(r+\\mathrm{i}s)$, of the real analytic heteroclinic orbits towards $W_0=-\\sqrt{\\lambda/6}$, are not complex entire. For any fixed real $r_0$, for example, consider the time-reversible complex-valued solution $\\psi(s)$ of the nonlinear and nonconservative quadratic Schr\\\"odinger equation \\begin{equation*} \\label{**} \\mathrm{i}\\psi_s=\\psi_{xx}+6\\psi^2-\\lambda \\tag{**} \\end{equation*} with real initial condition $\\psi_0=\\Gamma(r_0)$. Then there exist $r_0$ such that $\\psi(s)$ blows up at some finite real times $\\pm s^*$.   Abstractly, our results are formulated in the setting of analytic semigroups. They are based on Poincar\\'e non-resonance of unstable eigenvalues at equilibria $W_n$, near pitchfork bifurcation. Technically, we have to except a discrete set of $\\lambda>0$, and are currently limited to unstable dimensions $n\\leq22$, or to fast unstable manifolds of dimensions $d<1+\\tfrac{1}{\\sqrt{2}}n$.","sentences":["In parabolic or hyperbolic PDEs, solutions which remain uniformly bounded for all real times $t=r\\in\\mathbb{R}$ are often called PDE entire or eternal.","For example, consider the quadratic parabolic PDE \\begin{equation*} \\label{*} w_t=w_{xx}+6w^2-\\lambda, \\tag{*} \\end{equation*} for $0<x<\\tfrac{1}{2}$, under Neumann boundary conditions.","By its gradient-like structure, all real eternal non-equilibrium orbits $\\Gamma(r)$ of \\eqref{*} are heteroclinic among equilibria $w=W_n(x)$. All nontrivial real $W_n$ are rescaled and properly translated real-valued Weierstrass elliptic functions $W_n(x)=-n^2 \\wp(nx+\\tfrac{1}{2}\\tau),\\ n=1,2,3,\\ldots$, with lattice periods 1 and the purely imaginary modular parameter $\\tau$, and with Morse index $i(W_n)=n$ at $\\lambda=\\tfrac{1}{2}n^4 g_2(\\tau)$.   We show that the complex time extensions $\\Gamma(r+\\mathrm{i}s)$, of the real analytic heteroclinic orbits towards $W_0=-\\sqrt{\\lambda/6}$, are not complex entire.","For any fixed real $r_0$, for example, consider the time-reversible complex-valued solution $\\psi(s)$ of the nonlinear and nonconservative quadratic Schr\\\"odinger equation \\begin{equation*} \\label{**} \\mathrm{i}\\psi_s=\\psi_{xx}+6\\psi^2-\\lambda \\tag{**} \\end{equation*} with real initial condition $\\psi_0=\\Gamma(r_0)$. Then there exist $r_0$ such that $\\psi(s)$ blows up at some finite real times $\\pm s^*$.   Abstractly, our results are formulated in the setting of analytic semigroups.","They are based on Poincar\\'e non-resonance of unstable eigenvalues at equilibria $W_n$, near pitchfork bifurcation.","Technically, we have to except a discrete set of $\\lambda>0$, and are currently limited to unstable dimensions $n\\leq22$, or to fast unstable manifolds of dimensions $d<1+\\tfrac{1}{\\sqrt{2}}n$."],"url":"http://arxiv.org/abs/2403.06490v1","category":"math.AP"}
{"created":"2024-03-11 07:51:27","title":"Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling","abstract":"Uplift modeling aims to measure the incremental effect, which we call uplift, of a strategy or action on the users from randomized experiments or observational data. Most existing uplift methods only use individual data, which are usually not informative enough to capture the unobserved and complex hidden factors regarding the uplift. Furthermore, uplift modeling scenario usually has scarce labeled data, especially for the treatment group, which also poses a great challenge for model training. Considering that the neighbors' features and the social relationships are very informative to characterize a user's uplift, we propose a graph neural network-based framework with two uplift estimators, called GNUM, to learn from the social graph for uplift estimation. Specifically, we design the first estimator based on a class-transformed target. The estimator is general for all types of outcomes, and is able to comprehensively model the treatment and control group data together to approach the uplift. When the outcome is discrete, we further design the other uplift estimator based on our defined partial labels, which is able to utilize more labeled data from both the treatment and control groups, to further alleviate the label scarcity problem. Comprehensive experiments on a public dataset and two industrial datasets show a superior performance of our proposed framework over state-of-the-art methods under various evaluation metrics. The proposed algorithms have been deployed online to serve real-world uplift estimation scenarios.","sentences":["Uplift modeling aims to measure the incremental effect, which we call uplift, of a strategy or action on the users from randomized experiments or observational data.","Most existing uplift methods only use individual data, which are usually not informative enough to capture the unobserved and complex hidden factors regarding the uplift.","Furthermore, uplift modeling scenario usually has scarce labeled data, especially for the treatment group, which also poses a great challenge for model training.","Considering that the neighbors' features and the social relationships are very informative to characterize a user's uplift, we propose a graph neural network-based framework with two uplift estimators, called GNUM, to learn from the social graph for uplift estimation.","Specifically, we design the first estimator based on a class-transformed target.","The estimator is general for all types of outcomes, and is able to comprehensively model the treatment and control group data together to approach the uplift.","When the outcome is discrete, we further design the other uplift estimator based on our defined partial labels, which is able to utilize more labeled data from both the treatment and control groups, to further alleviate the label scarcity problem.","Comprehensive experiments on a public dataset and two industrial datasets show a superior performance of our proposed framework over state-of-the-art methods under various evaluation metrics.","The proposed algorithms have been deployed online to serve real-world uplift estimation scenarios."],"url":"http://arxiv.org/abs/2403.06489v1","category":"cs.LG"}
{"created":"2024-03-11 07:49:16","title":"Unified theory for frequency combs in ring and Fabry-Perot quantum cascade lasers: an order-parameter equation approach","abstract":"We present a unified model to describe the dynamics of optical frequency combs (OFCs) in quantum cascade lasers (QCLs), incorporating both ring and Fabry-P\\'erot (FP) cavity configurations. The model derives a modified complex Ginzburg-Landau equation (CGLE), leveraging an order parameter approach and is capable of capturing the dynamics of both configurations, thus enabling a comparative analysis. In the modified CGLE, a nonlinear integral term appears which is associated with the coupling between counterpropagating fields in the FP cavity and whose suppression yields the ring model, which is known to be properly described by a conventional CGLE. We show that this crucial term holds a key role in inhibiting the formation of harmonic frequency combs (HFCs), associated to multi-peaked localized structures, due to its anti-patterning effect. We provide in support a comprehensive campaign of numerical simulations, in which we observe a higher occurrence of HFCs in the ring configuration compared to the FP case. Furthermore, the simulations demonstrate the model's capability to reproduce experimental observations, including the coexistence of amplitude and frequency modulation, linear chirp, and typical dynamic scenarios observed in QCLs. Finally, we perform a linear stability analysis of the single-mode solution for the ring case, confirming its consistency with numerical simulations and highlighting its predictive power regarding the formation of harmonic combs.","sentences":["We present a unified model to describe the dynamics of optical frequency combs (OFCs) in quantum cascade lasers (QCLs), incorporating both ring and Fabry-P\\'erot (FP) cavity configurations.","The model derives a modified complex Ginzburg-Landau equation (CGLE), leveraging an order parameter approach and is capable of capturing the dynamics of both configurations, thus enabling a comparative analysis.","In the modified CGLE, a nonlinear integral term appears which is associated with the coupling between counterpropagating fields in the FP cavity and whose suppression yields the ring model, which is known to be properly described by a conventional CGLE.","We show that this crucial term holds a key role in inhibiting the formation of harmonic frequency combs (HFCs), associated to multi-peaked localized structures, due to its anti-patterning effect.","We provide in support a comprehensive campaign of numerical simulations, in which we observe a higher occurrence of HFCs in the ring configuration compared to the FP case.","Furthermore, the simulations demonstrate the model's capability to reproduce experimental observations, including the coexistence of amplitude and frequency modulation, linear chirp, and typical dynamic scenarios observed in QCLs.","Finally, we perform a linear stability analysis of the single-mode solution for the ring case, confirming its consistency with numerical simulations and highlighting its predictive power regarding the formation of harmonic combs."],"url":"http://arxiv.org/abs/2403.06486v1","category":"physics.optics"}
{"created":"2024-03-11 07:48:35","title":"Technical Debt Management: The Road Ahead for Successful Software Delivery","abstract":"Technical Debt, considered by many to be the 'silent killer' of software projects, has undeniably become part of the everyday vocabulary of software engineers. We know it compromises the internal quality of a system, either deliberately or inadvertently. We understand Technical Debt is not all derogatory, often serving the purpose of expediency. But, it is associated with a clear risk, especially for large and complex systems with extended service life: if we do not properly manage Technical Debt, it threatens to \"bankrupt\" those systems. Software engineers and organizations that develop software-intensive systems are facing an increasingly more dire future state of those systems if they do not start incorporating Technical Debt management into their day to day practice. But how? What have the wins and losses of the past decade of research and practice in managing Technical Debt taught us and where should we focus next? In this paper, we examine the state of the art in both industry and research communities in managing Technical Debt; we subsequently distill the gaps in industrial practice and the research shortcomings, and synthesize them to define and articulate a vision for what Technical Debt management looks like five years hence.","sentences":["Technical Debt, considered by many to be the 'silent killer' of software projects, has undeniably become part of the everyday vocabulary of software engineers.","We know it compromises the internal quality of a system, either deliberately or inadvertently.","We understand Technical Debt is not all derogatory, often serving the purpose of expediency.","But, it is associated with a clear risk, especially for large and complex systems with extended service life: if we do not properly manage Technical Debt, it threatens to \"bankrupt\" those systems.","Software engineers and organizations that develop software-intensive systems are facing an increasingly more dire future state of those systems if they do not start incorporating Technical Debt management into their day to day practice.","But how?","What have the wins and losses of the past decade of research and practice in managing Technical Debt taught us and where should we focus next?","In this paper, we examine the state of the art in both industry and research communities in managing Technical Debt; we subsequently distill the gaps in industrial practice and the research shortcomings, and synthesize them to define and articulate a vision for what Technical Debt management looks like five years hence."],"url":"http://arxiv.org/abs/2403.06484v1","category":"cs.SE"}
{"created":"2024-03-11 07:43:39","title":"SFT covers for actions of the first Grigorchuk group","abstract":"We study symbolic dynamical representations of actions of the first Grigorchuk group $G$, namely its action on the boundary of the infinite rooted binary tree, its representation in the topological full group of a minimal substitutive $\\mathbb{Z}$-shift, and its representation as a minimal system of Schreier graphs. We show that the first system admits an SFT cover, and the latter two systems are conjugate to sofic subshifts on $G$, but are not of finite type.","sentences":["We study symbolic dynamical representations of actions of the first Grigorchuk group $G$, namely its action on the boundary of the infinite rooted binary tree, its representation in the topological full group of a minimal substitutive $\\mathbb{Z}$-shift, and its representation as a minimal system of Schreier graphs.","We show that the first system admits an SFT cover, and the latter two systems are conjugate to sofic subshifts on $G$, but are not of finite type."],"url":"http://arxiv.org/abs/2403.06480v1","category":"math.DS"}
{"created":"2024-03-11 07:29:27","title":"Relative velocities between $^{13}$CO structures within $^{12}$CO Molecular clouds","abstract":"Velocity fields of molecular clouds (MCs) can provide crucial information on the merger and split between clouds, as well as their internal kinematics and maintenance, energy injection and redistribution, even star formation within clouds. Using the CO spectral lines data from the Milky Way Imaging Scroll Painting (MWISP) survey, we measure the relative velocities along the line of sight ($\\Delta$V$_{\\rm LOS}$) between $^{13}$CO structures within $^{12}$CO MCs. Emphasizing MCs with double and triple $^{13}$CO structures, we find that approximately 70$\\%$ of $\\Delta$V$_{\\rm LOS}$ values are less than $\\sim$ 1 km s$^{-1}$, and roughly 10$\\%$ of values exceed 2 km s$^{-1}$, with a maximum of $\\sim$ 5 km s$^{-1}$. Additionally, we compare $\\Delta$V$_{\\rm LOS}$ with the internal velocity dispersion of $^{13}$CO structures ($\\sigma_{\\rm ^{13}CO,in}$) and find that about 40$\\%$ of samples in either double or triple regime display distinct velocity discontinuities, i.e. the relative velocities between $^{13}$CO structures are larger than the internal linewidths of $^{13}$CO structures. Among these 40$\\%$ samples in the triple regime, 33$\\%$ exhibit signatures of combinations through the two-body motion, whereas the remaining 7$\\%$ show features of configurations through the multiple-body motion. The $\\Delta$V$_{\\rm LOS}$ distributions for MCs with double and triple $^{13}$CO structures are similar, as well as their $\\Delta$V$_{\\rm LOS}$/$\\sigma_{\\rm ^{13}CO,in}$ distributions. This suggests that relative motions of $^{13}$CO structures within MCs are random and independent of cloud complexities and scales.","sentences":["Velocity fields of molecular clouds (MCs) can provide crucial information on the merger and split between clouds, as well as their internal kinematics and maintenance, energy injection and redistribution, even star formation within clouds.","Using the CO spectral lines data from the Milky Way Imaging Scroll Painting (MWISP) survey, we measure the relative velocities along the line of sight ($\\Delta$V$_{\\rm LOS}$) between $^{13}$CO structures within $^{12}$CO MCs.","Emphasizing MCs with double and triple $^{13}$CO structures, we find that approximately 70$\\%$ of $\\Delta$V$_{\\rm LOS}$ values are less than $\\sim$ 1 km s$^{-1}$, and roughly 10$\\%$ of values exceed 2 km s$^{-1}$,","with a maximum of $\\sim$ 5 km s$^{-1}$. Additionally, we compare $\\Delta$V$_{\\rm LOS}$ with the internal velocity dispersion of $^{13}$CO structures ($\\sigma_{\\rm ^{13}CO,in}$) and find that about 40$\\%$ of samples in either double or triple regime display distinct velocity discontinuities, i.e. the relative velocities between $^{13}$CO structures are larger than the internal linewidths of $^{13}$CO structures.","Among these 40$\\%$ samples in the triple regime, 33$\\%$ exhibit signatures of combinations through the two-body motion, whereas the remaining 7$\\%$ show features of configurations through the multiple-body motion.","The $\\Delta$V$_{\\rm LOS}$ distributions for MCs with double and triple $^{13}$CO structures are similar, as well as their $\\Delta$V$_{\\rm LOS}$/$\\sigma_{\\rm ^{13}CO,in}$ distributions.","This suggests that relative motions of $^{13}$CO structures within MCs are random and independent of cloud complexities and scales."],"url":"http://arxiv.org/abs/2403.06475v1","category":"astro-ph.GA"}
{"created":"2024-03-11 07:23:55","title":"Spectroscopic Searches for Evolutionary Orbital Period Changes in WR+OB Binaries: the Case of WR 127 (Hen 3-1772)","abstract":"We aim at searching for secular evolution of the orbital period in the short-period binary system WR 127 (WN3b+O9.5V, $P = 9.555^d$). We performed new low-resolution spectroscopic observations of WR 127 on 2.5-m CMO SAI telescope to construct the radial velocity curves of the components suggesting the component masses $M_\\mathrm{WR}\\sin^3(i) = 11.8\\pm1.4$ $M_{\\odot}$, $M_\\mathrm{O}\\sin^3(i)=17.2\\pm1.4$ $M_{\\odot}$. The comparison with archival radial velocity curves enabled us to calculate the $(O-C)$ plot with accuracy sufficient to search for the orbital period change in WR 127. We report on the reliable detection of a secular increase in the orbital period of WR 127 at a rate of $\\dot{P} = 0.83\\pm0.14~\\mbox{s~yr}^{-1}$ corresponding to the dynamical mass-loss rate from the WR star $\\dot{M}_\\mathrm{WR} = (2.6\\pm0.5)\\times 10^{-5}$ $M_{\\odot}$ yr$^{-1}$. The mass-loss rate from WR stars in three WR+OB binaries (WR 127, CX Cep and V444 Cyg) as inferred from spectroscopic and photometric measurements suggests a preliminary empirical correlation between the WR star mass and the dynamical mass-loss rate $\\dot M_\\mathrm{WR}\\sim M_\\mathrm{WR}^{1.8}$. This relation is important for the understanding of the evolution of massive close binaries with WR stars -- precursors of gravitational-wave binary merging events with neutron stars and black holes.","sentences":["We aim at searching for secular evolution of the orbital period in the short-period binary system WR 127 (WN3b+O9.5V, $P = 9.555^d$).","We performed new low-resolution spectroscopic observations of WR 127 on 2.5-m CMO SAI telescope to construct the radial velocity curves of the components suggesting the component masses $M_\\mathrm{WR}\\sin^3(i) = 11.8\\pm1.4$ $M_{\\odot}$, $M_\\mathrm{O}\\sin^3(i)=17.2\\pm1.4$ $M_{\\odot}$. The comparison with archival radial velocity curves enabled us to calculate the $(O-C)$ plot with accuracy sufficient to search for the orbital period change in WR 127.","We report on the reliable detection of a secular increase in the orbital period of WR 127 at a rate of $\\dot{P} = 0.83\\pm0.14~\\mbox{s~yr}^{-1}$ corresponding to the dynamical mass-loss rate from the WR star $\\dot{M}_\\mathrm{WR} = (2.6\\pm0.5)\\times 10^{-5}$ $M_{\\odot}$ yr$^{-1}$.","The mass-loss rate from WR stars in three WR+OB binaries (WR 127, CX Cep and V444 Cyg) as inferred from spectroscopic and photometric measurements suggests a preliminary empirical correlation between the WR star mass and the dynamical mass-loss rate $\\dot M_\\mathrm{WR}\\sim M_\\mathrm{WR}^{1.8}$.","This relation is important for the understanding of the evolution of massive close binaries with WR stars -- precursors of gravitational-wave binary merging events with neutron stars and black holes."],"url":"http://arxiv.org/abs/2403.06473v1","category":"astro-ph.SR"}
{"created":"2024-03-11 07:08:17","title":"Generators for the Algebra of Symmetric Functions","abstract":"The algebra of symmetric functions contains several interesting families of symmetric functions indexed by integer partitions or skew partitions. Given a sequence $\\{u_n\\}$ of symmetric functions taken from one of these families such that $u_n$ is homogeneous of degree $n$, we provide necessary and sufficient conditions for the sequence to form a system of algebraically independent generators for the algebra of symmetric functions.","sentences":["The algebra of symmetric functions contains several interesting families of symmetric functions indexed by integer partitions or skew partitions.","Given a sequence $\\{u_n\\}$ of symmetric functions taken from one of these families such that $u_n$ is homogeneous of degree $n$, we provide necessary and sufficient conditions for the sequence to form a system of algebraically independent generators for the algebra of symmetric functions."],"url":"http://arxiv.org/abs/2403.06468v1","category":"math.CO"}
{"created":"2024-03-11 07:07:39","title":"Point Mamba: A Novel Point Cloud Backbone Based on State Space Model with Octree-Based Ordering Strategy","abstract":"Recently, state space model (SSM) has gained great attention due to its promising performance, linear complexity, and long sequence modeling ability in both language and image domains. However, it is non-trivial to extend SSM to the point cloud field, because of the causality requirement of SSM and the disorder and irregularity nature of point clouds. In this paper, we propose a novel SSM-based point cloud processing backbone, named Point Mamba, with a causality-aware ordering mechanism. To construct the causal dependency relationship, we design an octree-based ordering strategy on raw irregular points, globally sorting points in a z-order sequence and also retaining their spatial proximity. Our method achieves state-of-the-art performance compared with transformer-based counterparts, with 93.4% accuracy and 75.7 mIOU respectively on the ModelNet40 classification dataset and ScanNet semantic segmentation dataset. Furthermore, our Point Mamba has linear complexity, which is more efficient than transformer-based methods. Our method demonstrates the great potential that SSM can serve as a generic backbone in point cloud understanding. Codes are released at https://github.com/IRMVLab/Point-Mamba.","sentences":["Recently, state space model (SSM) has gained great attention due to its promising performance, linear complexity, and long sequence modeling ability in both language and image domains.","However, it is non-trivial to extend SSM to the point cloud field, because of the causality requirement of SSM and the disorder and irregularity nature of point clouds.","In this paper, we propose a novel SSM-based point cloud processing backbone, named Point Mamba, with a causality-aware ordering mechanism.","To construct the causal dependency relationship, we design an octree-based ordering strategy on raw irregular points, globally sorting points in a z-order sequence and also retaining their spatial proximity.","Our method achieves state-of-the-art performance compared with transformer-based counterparts, with 93.4% accuracy and 75.7 mIOU respectively on the ModelNet40 classification dataset and ScanNet semantic segmentation dataset.","Furthermore, our Point Mamba has linear complexity, which is more efficient than transformer-based methods.","Our method demonstrates the great potential that SSM can serve as a generic backbone in point cloud understanding.","Codes are released at https://github.com/IRMVLab/Point-Mamba."],"url":"http://arxiv.org/abs/2403.06467v1","category":"cs.CV"}
{"created":"2024-03-11 07:05:08","title":"Revisiting Cross-Diffusion for Overcrowding Dispersal in Interacting Species System","abstract":"This work introduces and study a novel class of reaction-diffusion systems to model the evolution of two interacting species that disperse to avoid overcrowding. Our approach employs the principle of proximal minimization energy, implemented through a minimum flow proximal algorithm. This framework offers a potential generalization of existing systems commonly used in the theory of segregation for interacting species dynamics. Beyond existence and uniqueness, this approach allows us to capture the dynamic interplay between diffusion rates and concentration gradients of each species, and has the potential to significantly advance our understanding of how cross-diffusion shapes the spatial distribution, coexistence, and pattern formation of multiple species within a system.","sentences":["This work introduces and study a novel class of reaction-diffusion systems to model the evolution of two interacting species that disperse to avoid overcrowding.","Our approach employs the principle of proximal minimization energy, implemented through a minimum flow proximal algorithm.","This framework offers a potential generalization of existing systems commonly used in the theory of segregation for interacting species dynamics.","Beyond existence and uniqueness, this approach allows us to capture the dynamic interplay between diffusion rates and concentration gradients of each species, and has the potential to significantly advance our understanding of how cross-diffusion shapes the spatial distribution, coexistence, and pattern formation of multiple species within a system."],"url":"http://arxiv.org/abs/2403.06464v1","category":"math.AP"}
{"created":"2024-03-11 07:03:15","title":"A prediction-based forward-looking vehicle dispatching strategy for dynamic ride-pooling","abstract":"For on-demand dynamic ride-pooling services, e.g., Uber Pool and Didi Pinche, a well-designed vehicle dispatching strategy is crucial for platform profitability and passenger experience. Most existing dispatching strategies overlook incoming pairing opportunities, therefore suffer from short-sighted limitations. In this paper, we propose a forward-looking vehicle dispatching strategy, which first predicts the expected distance saving that could be brought about by future orders and then solves a bipartite matching problem based on the prediction to match passengers with partially occupied or vacant vehicles or keep passengers waiting for next rounds of matching. To demonstrate the performance of the proposed strategy, a number of simulation experiments and comparisons are conducted based on the real-world road network and historical trip data from Haikou, China. Results show that the proposed strategy outperform the baseline strategies by generating approximately 31\\% more distance saving and 18\\% less average passenger detour distance. It indicates the significant benefits of considering future pairing opportunities in dispatching, and highlights the effectiveness of our innovative forward-looking vehicle dispatching strategy in improving system efficiency and user experience for dynamic ride-pooling services.","sentences":["For on-demand dynamic ride-pooling services, e.g., Uber Pool and Didi Pinche, a well-designed vehicle dispatching strategy is crucial for platform profitability and passenger experience.","Most existing dispatching strategies overlook incoming pairing opportunities, therefore suffer from short-sighted limitations.","In this paper, we propose a forward-looking vehicle dispatching strategy, which first predicts the expected distance saving that could be brought about by future orders and then solves a bipartite matching problem based on the prediction to match passengers with partially occupied or vacant vehicles or keep passengers waiting for next rounds of matching.","To demonstrate the performance of the proposed strategy, a number of simulation experiments and comparisons are conducted based on the real-world road network and historical trip data from Haikou, China.","Results show that the proposed strategy outperform the baseline strategies by generating approximately 31\\% more distance saving and 18\\% less average passenger detour distance.","It indicates the significant benefits of considering future pairing opportunities in dispatching, and highlights the effectiveness of our innovative forward-looking vehicle dispatching strategy in improving system efficiency and user experience for dynamic ride-pooling services."],"url":"http://arxiv.org/abs/2403.06463v1","category":"eess.SY"}
{"created":"2024-03-11 06:34:05","title":"Ensemble Quadratic Assignment Network for Graph Matching","abstract":"Graph matching is a commonly used technique in computer vision and pattern recognition. Recent data-driven approaches have improved the graph matching accuracy remarkably, whereas some traditional algorithm-based methods are more robust to feature noises, outlier nodes, and global transformation (e.g.~rotation). In this paper, we propose a graph neural network (GNN) based approach to combine the advantages of data-driven and traditional methods. In the GNN framework, we transform traditional graph-matching solvers as single-channel GNNs on the association graph and extend the single-channel architecture to the multi-channel network. The proposed model can be seen as an ensemble method that fuses multiple algorithms at every iteration. Instead of averaging the estimates at the end of the ensemble, in our approach, the independent iterations of the ensembled algorithms exchange their information after each iteration via a 1x1 channel-wise convolution layer. Experiments show that our model improves the performance of traditional algorithms significantly. In addition, we propose a random sampling strategy to reduce the computational complexity and GPU memory usage, so the model applies to matching graphs with thousands of nodes. We evaluate the performance of our method on three tasks: geometric graph matching, semantic feature matching, and few-shot 3D shape classification. The proposed model performs comparably or outperforms the best existing GNN-based methods.","sentences":["Graph matching is a commonly used technique in computer vision and pattern recognition.","Recent data-driven approaches have improved the graph matching accuracy remarkably, whereas some traditional algorithm-based methods are more robust to feature noises, outlier nodes, and global transformation (e.g.~rotation).","In this paper, we propose a graph neural network (GNN) based approach to combine the advantages of data-driven and traditional methods.","In the GNN framework, we transform traditional graph-matching solvers as single-channel GNNs on the association graph and extend the single-channel architecture to the multi-channel network.","The proposed model can be seen as an ensemble method that fuses multiple algorithms at every iteration.","Instead of averaging the estimates at the end of the ensemble, in our approach, the independent iterations of the ensembled algorithms exchange their information after each iteration via a 1x1 channel-wise convolution layer.","Experiments show that our model improves the performance of traditional algorithms significantly.","In addition, we propose a random sampling strategy to reduce the computational complexity and GPU memory usage, so the model applies to matching graphs with thousands of nodes.","We evaluate the performance of our method on three tasks: geometric graph matching, semantic feature matching, and few-shot 3D shape classification.","The proposed model performs comparably or outperforms the best existing GNN-based methods."],"url":"http://arxiv.org/abs/2403.06457v1","category":"cs.CV"}
{"created":"2024-03-11 05:56:58","title":"Quantum double structure in cold atom superfluids","abstract":"The theory of topological quantum computation is underpinned by two important classes of models. One is based on non-abelian Chern-Simons theory, which yields the so-called $\\rm{SU}(2)_k$ anyon models that often appear in the context of electrically charged quantum fluids. The physics of the other is captured by symmetry broken Yang-Mills theory in the absence of a Chern-Simons term, and results in the so-called quantum double models. Extensive resources have been invested into the search for $\\rm{SU}(2)_k$ anyon quasi-particles; in particular the so-called Ising anyons ($k=2$) of which Majorana zero modes are believed to be an incarnation. In contrast to the $\\rm{SU}(2)_k$ models, quantum doubles have attracted little attention in experiments despite their pivotal role in the theory of error correction. Beyond topological error correcting codes, the appearance of quantum doubles has been limited to contexts primarily within mathematical physics, and as such, they are of seemingly little relevance for the study of experimentally tangible systems. However, recent works suggest that quantum double anyons may be found in spinor Bose-Einstein condensates. In light of this, the core purpose of this article is to provide a self-contained exposition of the quantum double structure, framed in the context of spinor condensates, by constructing explicitly the quantum doubles for various ground state symmetry groups and discuss their experimental realisability. We also derive analytically an equation for the quantum double Clebsch-Gordan coefficients from which the relevant braid matrices can be worked out. Finally, the existence of a particle-vortex duality is exposed and illuminated upon in this context.","sentences":["The theory of topological quantum computation is underpinned by two important classes of models.","One is based on non-abelian Chern-Simons theory, which yields the so-called $\\rm{SU}(2)_k$ anyon models that often appear in the context of electrically charged quantum fluids.","The physics of the other is captured by symmetry broken Yang-Mills theory in the absence of a Chern-Simons term, and results in the so-called quantum double models.","Extensive resources have been invested into the search for $\\rm{SU}(2)_k$ anyon quasi-particles; in particular the so-called Ising anyons ($k=2$) of which Majorana zero modes are believed to be an incarnation.","In contrast to the $\\rm{SU}(2)_k$ models, quantum doubles have attracted little attention in experiments despite their pivotal role in the theory of error correction.","Beyond topological error correcting codes, the appearance of quantum doubles has been limited to contexts primarily within mathematical physics, and as such, they are of seemingly little relevance for the study of experimentally tangible systems.","However, recent works suggest that quantum double anyons may be found in spinor Bose-Einstein condensates.","In light of this, the core purpose of this article is to provide a self-contained exposition of the quantum double structure, framed in the context of spinor condensates, by constructing explicitly the quantum doubles for various ground state symmetry groups and discuss their experimental realisability.","We also derive analytically an equation for the quantum double Clebsch-Gordan coefficients from which the relevant braid matrices can be worked out.","Finally, the existence of a particle-vortex duality is exposed and illuminated upon in this context."],"url":"http://arxiv.org/abs/2403.06451v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-11 05:24:52","title":"Symmetry Hierarchy and Thermalization Frustration in Graphene Nanoresonators","abstract":"As the essential cause of the intrinsic dissipation that limits the quality of graphene nanoresonators, intermodal energy transfer is also a key issue in thermalization dynamics. Typically systems with larger initial energy demand shorter time to be thermalized. However, we find quantitatively that instead of becoming shorter, the equipartition time of the graphene nanoresonator can increase abruptly by one order of magnitude. This thermalization frustration emerges due to the partition of the normal modes based on the hierarchical symmetry, and a sensitive on-off switching of the energy flow channels between symmetry classes controlled by Mathieu instabilities. The results uncover the decisive roles of symmetry in the thermalization at the nanoscale, and may also lead to strategies for improving the performance of graphene nanoresonators.","sentences":["As the essential cause of the intrinsic dissipation that limits the quality of graphene nanoresonators, intermodal energy transfer is also a key issue in thermalization dynamics.","Typically systems with larger initial energy demand shorter time to be thermalized.","However, we find quantitatively that instead of becoming shorter, the equipartition time of the graphene nanoresonator can increase abruptly by one order of magnitude.","This thermalization frustration emerges due to the partition of the normal modes based on the hierarchical symmetry, and a sensitive on-off switching of the energy flow channels between symmetry classes controlled by Mathieu instabilities.","The results uncover the decisive roles of symmetry in the thermalization at the nanoscale, and may also lead to strategies for improving the performance of graphene nanoresonators."],"url":"http://arxiv.org/abs/2403.06442v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-11 05:20:59","title":"The energy spectrum of a quantum vortex loop moving in a long pipe","abstract":"In this study we consider the closed vortex filament that moves in a pipe of length $L >> R_1$, where $R_1$ means inner radius of the pipe. The vortex filament is described in the Local Induction Approximation. We quantize this dynamical system and calculate the spectrum both circulation and energy. In the study, we focus on the case $L \\to \\infty$.","sentences":["In this study we consider the closed vortex filament that moves in a pipe of length $L >> R_1$, where $R_1$ means inner radius of the pipe.","The vortex filament is described in the Local Induction Approximation.","We quantize this dynamical system and calculate the spectrum both circulation and energy.","In the study, we focus on the case $L \\to \\infty$."],"url":"http://arxiv.org/abs/2403.06441v1","category":"math-ph"}
{"created":"2024-03-11 05:15:19","title":"Wide-Field, High-Resolution Reconstruction in Computational Multi-Aperture Miniscope Using a Fourier Neural Network","abstract":"Traditional fluorescence microscopy is constrained by inherent trade-offs among resolution, field-of-view, and system complexity. To navigate these challenges, we introduce a simple and low-cost computational multi-aperture miniature microscope, utilizing a microlens array for single-shot wide-field, high-resolution imaging. Addressing the challenges posed by extensive view multiplexing and non-local, shift-variant aberrations in this device, we present SV-FourierNet, a novel multi-channel Fourier neural network. SV-FourierNet facilitates high-resolution image reconstruction across the entire imaging field through its learned global receptive field. We establish a close relationship between the physical spatially-varying point-spread functions and the network's learned effective receptive field. This ensures that SV-FourierNet has effectively encapsulated the spatially-varying aberrations in our system, and learned a physically meaningful function for image reconstruction. Training of SV-FourierNet is conducted entirely on a physics-based simulator. We showcase wide-field, high-resolution video reconstructions on colonies of freely moving C. elegans and imaging of a mouse brain section. Our computational multi-aperture miniature microscope, augmented with SV-FourierNet, represents a major advancement in computational microscopy and may find broad applications in biomedical research and other fields requiring compact microscopy solutions.","sentences":["Traditional fluorescence microscopy is constrained by inherent trade-offs among resolution, field-of-view, and system complexity.","To navigate these challenges, we introduce a simple and low-cost computational multi-aperture miniature microscope, utilizing a microlens array for single-shot wide-field, high-resolution imaging.","Addressing the challenges posed by extensive view multiplexing and non-local, shift-variant aberrations in this device, we present SV-FourierNet, a novel multi-channel Fourier neural network.","SV-FourierNet facilitates high-resolution image reconstruction across the entire imaging field through its learned global receptive field.","We establish a close relationship between the physical spatially-varying point-spread functions and the network's learned effective receptive field.","This ensures that SV-FourierNet has effectively encapsulated the spatially-varying aberrations in our system, and learned a physically meaningful function for image reconstruction.","Training of SV-FourierNet is conducted entirely on a physics-based simulator.","We showcase wide-field, high-resolution video reconstructions on colonies of freely moving C. elegans and imaging of a mouse brain section.","Our computational multi-aperture miniature microscope, augmented with SV-FourierNet, represents a major advancement in computational microscopy and may find broad applications in biomedical research and other fields requiring compact microscopy solutions."],"url":"http://arxiv.org/abs/2403.06439v1","category":"physics.optics"}
{"created":"2024-03-11 05:00:56","title":"BoostER: Leveraging Large Language Models for Enhancing Entity Resolution","abstract":"Entity resolution, which involves identifying and merging records that refer to the same real-world entity, is a crucial task in areas like Web data integration. This importance is underscored by the presence of numerous duplicated and multi-version data resources on the Web. However, achieving high-quality entity resolution typically demands significant effort. The advent of Large Language Models (LLMs) like GPT-4 has demonstrated advanced linguistic capabilities, which can be a new paradigm for this task. In this paper, we propose a demonstration system named BoostER that examines the possibility of leveraging LLMs in the entity resolution process, revealing advantages in both easy deployment and low cost. Our approach optimally selects a set of matching questions and poses them to LLMs for verification, then refines the distribution of entity resolution results with the response of LLMs. This offers promising prospects to achieve a high-quality entity resolution result for real-world applications, especially to individuals or small companies without the need for extensive model training or significant financial investment.","sentences":["Entity resolution, which involves identifying and merging records that refer to the same real-world entity, is a crucial task in areas like Web data integration.","This importance is underscored by the presence of numerous duplicated and multi-version data resources on the Web.","However, achieving high-quality entity resolution typically demands significant effort.","The advent of Large Language Models (LLMs) like GPT-4 has demonstrated advanced linguistic capabilities, which can be a new paradigm for this task.","In this paper, we propose a demonstration system named BoostER that examines the possibility of leveraging LLMs in the entity resolution process, revealing advantages in both easy deployment and low cost.","Our approach optimally selects a set of matching questions and poses them to LLMs for verification, then refines the distribution of entity resolution results with the response of LLMs.","This offers promising prospects to achieve a high-quality entity resolution result for real-world applications, especially to individuals or small companies without the need for extensive model training or significant financial investment."],"url":"http://arxiv.org/abs/2403.06434v1","category":"cs.DB"}
{"created":"2024-03-11 04:44:34","title":"From Fitting Participation to Forging Relationships: The Art of Participatory ML","abstract":"Participatory machine learning (ML) encourages the inclusion of end users and people affected by ML systems in design and development processes. We interviewed 18 participation brokers -- individuals who facilitate such inclusion and transform the products of participants' labour into inputs for an ML artefact or system -- across a range of organisational settings and project locations. Our findings demonstrate the inherent challenges of integrating messy contextual information generated through participation with the structured data formats required by ML workflows and the uneven power dynamics in project contexts. We advocate for evolution in the role of brokers to more equitably balance value generated in Participatory ML projects for design and development teams with value created for participants. To move beyond `fitting' participation to existing processes and empower participants to envision alternative futures through ML, brokers must become educators and advocates for end users, while attending to frustration and dissent from indirect stakeholders.","sentences":["Participatory machine learning (ML) encourages the inclusion of end users and people affected by ML systems in design and development processes.","We interviewed 18 participation brokers -- individuals who facilitate such inclusion and transform the products of participants' labour into inputs for an ML artefact or system -- across a range of organisational settings and project locations.","Our findings demonstrate the inherent challenges of integrating messy contextual information generated through participation with the structured data formats required by ML workflows and the uneven power dynamics in project contexts.","We advocate for evolution in the role of brokers to more equitably balance value generated in Participatory ML projects for design and development teams with value created for participants.","To move beyond `fitting' participation to existing processes and empower participants to envision alternative futures through ML, brokers must become educators and advocates for end users, while attending to frustration and dissent from indirect stakeholders."],"url":"http://arxiv.org/abs/2403.06431v1","category":"cs.HC"}
{"created":"2024-03-11 04:40:24","title":"Designing for Projection-based Communication between Autonomous Vehicles and Pedestrians","abstract":"Recent studies have investigated new approaches for communicating an autonomous vehicle's (AV) intent and awareness to pedestrians. This paper adds to this body of work by presenting the design and evaluation of in-situ projections on the road. Our design combines common traffic light patterns with aesthetic visual elements. We describe the iterative design process and the prototyping methods used in each stage. The final design concept was represented as a virtual reality simulation and evaluated with 18 participants in four different street crossing scenarios, which included three scenarios that simulated various degrees of system errors. We found that different design elements were able to support participants' confidence in their decision even when the AV failed to correctly detect their presence. We also identified elements in our design that needed to be more clearly communicated. Based on these findings, the paper presents a series of design recommendations for projection-based communication between AVs and pedestrians.","sentences":["Recent studies have investigated new approaches for communicating an autonomous vehicle's (AV) intent and awareness to pedestrians.","This paper adds to this body of work by presenting the design and evaluation of in-situ projections on the road.","Our design combines common traffic light patterns with aesthetic visual elements.","We describe the iterative design process and the prototyping methods used in each stage.","The final design concept was represented as a virtual reality simulation and evaluated with 18 participants in four different street crossing scenarios, which included three scenarios that simulated various degrees of system errors.","We found that different design elements were able to support participants' confidence in their decision even when the AV failed to correctly detect their presence.","We also identified elements in our design that needed to be more clearly communicated.","Based on these findings, the paper presents a series of design recommendations for projection-based communication between AVs and pedestrians."],"url":"http://arxiv.org/abs/2403.06429v1","category":"cs.HC"}
{"created":"2024-03-11 04:17:41","title":"LiDAR Point Cloud-based Multiple Vehicle Tracking with Probabilistic Measurement-Region Association","abstract":"Multiple extended target tracking (ETT) has attracted increasing interest due to the development of high-precision LiDAR and radar sensors in automotive applications. For LiDAR point cloud-based vehicle tracking, this paper presents a probabilistic measurement-region association (PMRA) ETT model, which can depict the complex measurement distribution by dividing the target extent into different regions. The PMRA model overcomes the drawbacks of previous data-region association (DRA) models by eliminating the approximation error of constrained estimation and using continuous integrals to more reliably calculate the association probabilities. Furthermore, the PMRA model is integrated with the Poisson multi-Bernoulli mixture (PMBM) filter for tracking multiple vehicles. Simulation results illustrate the superior estimation accuracy of the proposed PMRA-PMBM filter in terms of both positions and extents of the vehicles comparing with PMBM filters using the gamma Gaussian inverse Wishart and DRA implementations.","sentences":["Multiple extended target tracking (ETT) has attracted increasing interest due to the development of high-precision LiDAR and radar sensors in automotive applications.","For LiDAR point cloud-based vehicle tracking, this paper presents a probabilistic measurement-region association (PMRA) ETT model, which can depict the complex measurement distribution by dividing the target extent into different regions.","The PMRA model overcomes the drawbacks of previous data-region association (DRA) models by eliminating the approximation error of constrained estimation and using continuous integrals to more reliably calculate the association probabilities.","Furthermore, the PMRA model is integrated with the Poisson multi-Bernoulli mixture (PMBM) filter for tracking multiple vehicles.","Simulation results illustrate the superior estimation accuracy of the proposed PMRA-PMBM filter in terms of both positions and extents of the vehicles comparing with PMBM filters using the gamma Gaussian inverse Wishart and DRA implementations."],"url":"http://arxiv.org/abs/2403.06423v1","category":"eess.SP"}
{"created":"2024-03-11 17:53:37","title":"Non-Abelian R-symmetries in $\\mathcal{N}=1$ supersymmetry","abstract":"We investigate non-Abelian R-symmetries in $\\mathcal{N}=1$ supersymmetric theory, where fields may transform under the R-symmetry in representations with dimension higher than one. While a continuous non-Abelian R-symmetry can always be decomposed to a $U(1)$ R-symmetry and non-R symmetries, there are non-trivial discrete non-Abelian R-symmetries that do not admit such a decomposition, and effective R-charges cannot be defined in such models. Previous results on sufficient conditions for R-symmetric supersymmetric vacua in Wess-Zumino models still hold, and do not depend on fields in representations of dimension greater than one. However, fields in higher-dimensional representations enter the sufficient conditions for supersymmetric vacua that break R-symmetry, but it is difficult to identify the independent variables which can be used to solve the F-flatness equation in this case, unless other conditions are fulfilled. We present examples with discrete non-Abelian R-symmetries of the lowest order in this case.","sentences":["We investigate non-Abelian R-symmetries in $\\mathcal{N}=1$ supersymmetric theory, where fields may transform under the R-symmetry in representations with dimension higher than one.","While a continuous non-Abelian R-symmetry can always be decomposed to a $U(1)$ R-symmetry and non-R symmetries, there are non-trivial discrete non-Abelian R-symmetries that do not admit such a decomposition, and effective R-charges cannot be defined in such models.","Previous results on sufficient conditions for R-symmetric supersymmetric vacua in Wess-Zumino models still hold, and do not depend on fields in representations of dimension greater than one.","However, fields in higher-dimensional representations enter the sufficient conditions for supersymmetric vacua that break R-symmetry, but it is difficult to identify the independent variables which can be used to solve the F-flatness equation in this case, unless other conditions are fulfilled.","We present examples with discrete non-Abelian R-symmetries of the lowest order in this case."],"url":"http://arxiv.org/abs/2403.06969v1","category":"hep-th"}
{"created":"2024-03-11 17:20:12","title":"Naming, Describing, and Quantifying Visual Objects in Humans and LLMs","abstract":"While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision \\& Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, humans show clear production preferences for highly context-sensitive expressions, such as the quantifiers `few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the ability of VLLMs to capture human naming preferences, with all models failing in tasks that require high-level reasoning such as assigning quantifiers.","sentences":["While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision \\& Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question.","This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy.","Furthermore, humans show clear production preferences for highly context-sensitive expressions, such as the quantifiers `few' or `most'.","In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work.","Our results reveal mixed evidence on the ability of VLLMs to capture human naming preferences, with all models failing in tasks that require high-level reasoning such as assigning quantifiers."],"url":"http://arxiv.org/abs/2403.06935v1","category":"cs.CL"}
{"created":"2024-03-11 17:02:11","title":"DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization","abstract":"Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$ faster rendering speed.","sentences":["Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed.","This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs.","Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease.","In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint.","Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance.","To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes.","Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$ faster rendering speed."],"url":"http://arxiv.org/abs/2403.06912v1","category":"cs.CV"}
{"created":"2024-03-11 17:00:47","title":"Heat transport in the quantum Rabi model: Universality and ultrastrong coupling effects","abstract":"Heat transport in the quantum Rabi model at weak interaction with the heat baths is controlled by the qubit-oscillator coupling. Universality of the linear conductance versus the temperature is found for $T\\lesssim T_K$, with $T_K$ a coupling-dependent Kondo-like temperature. At low temperature, coherent heat transfer via virtual processes yields a $\\sim T^3$ behavior with destructive interference in the presence of quasi-degeneracies in the spectrum. As the temperature increases, incoherent emission and absorption dominate and a maximum is reached at $T\\sim T_K/2$. In the presence of a bias on the qubit, the conductance makes a transition from a resonant to a broad, zero-bias peak regime. Parallels and differences are found compared to the spin-boson model in [K. Saito and T. Kato, Phys. Rev. Lett. \\textbf{111}, 214301 (2013)], where the qubit-bath coupling instead of the internal qubit-oscillator coupling rules thermal transport.","sentences":["Heat transport in the quantum Rabi model at weak interaction with the heat baths is controlled by the qubit-oscillator coupling.","Universality of the linear conductance versus the temperature is found for $T\\lesssim T_K$, with $T_K$ a coupling-dependent Kondo-like temperature.","At low temperature, coherent heat transfer via virtual processes yields a $\\sim T^3$ behavior with destructive interference in the presence of quasi-degeneracies in the spectrum.","As the temperature increases, incoherent emission and absorption dominate and a maximum is reached at $T\\sim T_K/2$. In the presence of a bias on the qubit, the conductance makes a transition from a resonant to a broad, zero-bias peak regime.","Parallels and differences are found compared to the spin-boson model in [K. Saito and T. Kato, Phys.","Rev. Lett.","\\textbf{111}, 214301 (2013)], where the qubit-bath coupling instead of the internal qubit-oscillator coupling rules thermal transport."],"url":"http://arxiv.org/abs/2403.06909v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 16:53:47","title":"Multiobject Tracking for Thresholded Cell Measurements","abstract":"In many multiobject tracking applications, including radar and sonar tracking, after prefiltering the received signal, measurement data is typically structured in cells. The cells, e.g., represent different range and bearing values. However, conventional multiobject tracking methods use so-called point measurements. Point measurements are provided by a preprocessing stage that applies a threshold or detector and breaks up the cell's structure by converting cell indexes into, e.g., range and bearing measurements. We here propose a Bayesian multiobject tracking method that processes measurements that have been thresholded but are still cell-structured. We first derive a likelihood function that systematically incorporates an adjustable detection threshold which makes it possible to control the number of cell measurements. We then propose a Poisson Multi-Bernoulli (PMB) filter based on the likelihood function for cell measurements. Furthermore, we establish a link to the conventional point measurement model by deriving the likelihood function for point measurements with amplitude information (AM) and discuss the PMB filter that uses point measurements with AM. Our numerical results demonstrate the advantages of the proposed method that relies on thresholded cell measurements compared to the conventional multiobject tracking based on point measurements with and without AM.","sentences":["In many multiobject tracking applications, including radar and sonar tracking, after prefiltering the received signal, measurement data is typically structured in cells.","The cells, e.g., represent different range and bearing values.","However, conventional multiobject tracking methods use so-called point measurements.","Point measurements are provided by a preprocessing stage that applies a threshold or detector and breaks up the cell's structure by converting cell indexes into, e.g., range and bearing measurements.","We here propose a Bayesian multiobject tracking method that processes measurements that have been thresholded but are still cell-structured.","We first derive a likelihood function that systematically incorporates an adjustable detection threshold which makes it possible to control the number of cell measurements.","We then propose a Poisson Multi-Bernoulli (PMB) filter based on the likelihood function for cell measurements.","Furthermore, we establish a link to the conventional point measurement model by deriving the likelihood function for point measurements with amplitude information (AM) and discuss the PMB filter that uses point measurements with AM.","Our numerical results demonstrate the advantages of the proposed method that relies on thresholded cell measurements compared to the conventional multiobject tracking based on point measurements with and without AM."],"url":"http://arxiv.org/abs/2403.06899v1","category":"eess.SP"}
{"created":"2024-03-11 16:45:53","title":"Numerical simulation of individual coil placement - A proof-of-concept study for the prediction of recurrence after aneurysm coiling","abstract":"Rupture of intracranial aneurysms results in severe subarachnoidal hemorrhage, which is associated with high morbidity and mortality. Neurointerventional occlusion of the aneurysm through coiling has evolved to a therapeutical standard. The choice of the specific coil has an important influence on secondary regrowth requiring retreatment. Aneurysm occlusion was simulated either through virtual implantation of a preshaped 3D coil or with a porous media approach. In this study, we used a recently developed numerical approach to simulate aneurysm shapes in specific challenging aneurysm anatomies and correlated these with aneurysm recurrence 6 months after treatment. The simulation showed a great variety of coil shapes depending on the variability in possible microcatheter positions. Aneurysms with a later recurrence showed a tendency for more successful coiling attempts. Results revealed further trends suggesting lower simulated packing densities in aneurysms with reoccurrence. Simulated packing densities did not correlate with those calculated by conventional software, indicating the potential for our approach to offer additional predictive value. Our study, therefore, pioneers a comprehensive numerical model for simulating aneurysm coiling, providing insights into individualized treatment strategies and outcome prediction. Future directions involve expanding the model's capabilities to simulate intraprocedural outcomes and long-term predictions, aiming to refine occlusion quality criteria and validate prediction parameters in larger patient cohorts. This simulation framework holds promise for enhancing clinical decision-making and optimizing patient outcomes in endovascular aneurysm treatment.","sentences":["Rupture of intracranial aneurysms results in severe subarachnoidal hemorrhage, which is associated with high morbidity and mortality.","Neurointerventional occlusion of the aneurysm through coiling has evolved to a therapeutical standard.","The choice of the specific coil has an important influence on secondary regrowth requiring retreatment.","Aneurysm occlusion was simulated either through virtual implantation of a preshaped 3D coil or with a porous media approach.","In this study, we used a recently developed numerical approach to simulate aneurysm shapes in specific challenging aneurysm anatomies and correlated these with aneurysm recurrence 6 months after treatment.","The simulation showed a great variety of coil shapes depending on the variability in possible microcatheter positions.","Aneurysms with a later recurrence showed a tendency for more successful coiling attempts.","Results revealed further trends suggesting lower simulated packing densities in aneurysms with reoccurrence.","Simulated packing densities did not correlate with those calculated by conventional software, indicating the potential for our approach to offer additional predictive value.","Our study, therefore, pioneers a comprehensive numerical model for simulating aneurysm coiling, providing insights into individualized treatment strategies and outcome prediction.","Future directions involve expanding the model's capabilities to simulate intraprocedural outcomes and long-term predictions, aiming to refine occlusion quality criteria and validate prediction parameters in larger patient cohorts.","This simulation framework holds promise for enhancing clinical decision-making and optimizing patient outcomes in endovascular aneurysm treatment."],"url":"http://arxiv.org/abs/2403.06889v1","category":"cs.CE"}
{"created":"2024-03-11 16:42:29","title":"A Holistic Framework Towards Vision-based Traffic Signal Control with Microscopic Simulation","abstract":"Traffic signal control (TSC) is crucial for reducing traffic congestion that leads to smoother traffic flow, reduced idling time, and mitigated CO2 emissions. In this study, we explore the computer vision approach for TSC that modulates on-road traffic flows through visual observation. Unlike traditional feature-based approaches, vision-based methods depend much less on heuristics and predefined features, bringing promising potentials for end-to-end learning and optimization of traffic signals. Thus, we introduce a holistic traffic simulation framework called TrafficDojo towards vision-based TSC and its benchmarking by integrating the microscopic traffic flow provided in SUMO into the driving simulator MetaDrive. This proposed framework offers a versatile traffic environment for in-depth analysis and comprehensive evaluation of traffic signal controllers across diverse traffic conditions and scenarios. We establish and compare baseline algorithms including both traditional and Reinforecment Learning (RL) approaches. This work sheds insights into the design and development of vision-based TSC approaches and open up new research opportunities. All the code and baselines will be made publicly available.","sentences":["Traffic signal control (TSC) is crucial for reducing traffic congestion that leads to smoother traffic flow, reduced idling time, and mitigated CO2 emissions.","In this study, we explore the computer vision approach for TSC that modulates on-road traffic flows through visual observation.","Unlike traditional feature-based approaches, vision-based methods depend much less on heuristics and predefined features, bringing promising potentials for end-to-end learning and optimization of traffic signals.","Thus, we introduce a holistic traffic simulation framework called TrafficDojo towards vision-based TSC and its benchmarking by integrating the microscopic traffic flow provided in SUMO into the driving simulator MetaDrive.","This proposed framework offers a versatile traffic environment for in-depth analysis and comprehensive evaluation of traffic signal controllers across diverse traffic conditions and scenarios.","We establish and compare baseline algorithms including both traditional and Reinforecment Learning (RL) approaches.","This work sheds insights into the design and development of vision-based TSC approaches and open up new research opportunities.","All the code and baselines will be made publicly available."],"url":"http://arxiv.org/abs/2403.06884v1","category":"cs.CV"}
{"created":"2024-03-11 16:23:38","title":"Semantic Residual Prompts for Continual Learning","abstract":"Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a second pool. The retrieved prompts serve to adapt a pre-trained ViT, granting plasticity. In doing so, we also propose a novel residual mechanism to transfer CLIP semantics to the ViT layers. Through extensive analysis on established CL benchmarks, we show that our method significantly outperforms both state-of-the-art CL approaches and the zero-shot CLIP test. Notably, our findings hold true even for datasets with a substantial domain gap w.r.t. the pre-training knowledge of the backbone model, as showcased by experiments on satellite imagery and medical datasets.","sentences":["Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts.","Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values).","However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches.","For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts.","To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism.","Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes.","The second level, instead, uses these prototypes along with the query image as keys to index a second pool.","The retrieved prompts serve to adapt a pre-trained ViT, granting plasticity.","In doing so, we also propose a novel residual mechanism to transfer CLIP semantics to the ViT layers.","Through extensive analysis on established CL benchmarks, we show that our method significantly outperforms both state-of-the-art CL approaches and the zero-shot CLIP test.","Notably, our findings hold true even for datasets with a substantial domain gap w.r.t.","the pre-training knowledge of the backbone model, as showcased by experiments on satellite imagery and medical datasets."],"url":"http://arxiv.org/abs/2403.06870v1","category":"cs.LG"}
{"created":"2024-03-11 16:08:52","title":"Suppressing Correlated Noise in Quantum Computers via Context-Aware Compiling","abstract":"Coherent errors, and especially those that occur in correlation among a set of qubits, are detrimental for large-scale quantum computing. Correlations in noise can occur as a result of spatial and temporal configurations of instructions executing on the quantum processor. In this paper, we perform a detailed experimental characterization of many of these error sources, and theoretically connect them to the physics of superconducting qubits and gate operations. Equipped with this knowledge, we devise compiler strategies to suppress these errors using dynamical decoupling or error compensation into the rest of the circuit. Importantly, these strategies are successful when the context at each layer of computation is taken into account: how qubits are connected, what crosstalk terms exist on the device, and what gates or idle periods occur in that layer. Our context-aware compiler thus suppresses some dominant sources of error, making further error mitigation or error correction substantially less expensive. For example, our experiments show an increase of 18.5\\% in layer fidelity for a candidate 10-qubit circuit layer compared to context-unaware suppression. Owing to the exponential nature of error mitigation, these improvements due to error suppression translate to several orders of magnitude reduction of sampling overhead for a circuit consisting of a moderate number of layers.","sentences":["Coherent errors, and especially those that occur in correlation among a set of qubits, are detrimental for large-scale quantum computing.","Correlations in noise can occur as a result of spatial and temporal configurations of instructions executing on the quantum processor.","In this paper, we perform a detailed experimental characterization of many of these error sources, and theoretically connect them to the physics of superconducting qubits and gate operations.","Equipped with this knowledge, we devise compiler strategies to suppress these errors using dynamical decoupling or error compensation into the rest of the circuit.","Importantly, these strategies are successful when the context at each layer of computation is taken into account: how qubits are connected, what crosstalk terms exist on the device, and what gates or idle periods occur in that layer.","Our context-aware compiler thus suppresses some dominant sources of error, making further error mitigation or error correction substantially less expensive.","For example, our experiments show an increase of 18.5\\% in layer fidelity for a candidate 10-qubit circuit layer compared to context-unaware suppression.","Owing to the exponential nature of error mitigation, these improvements due to error suppression translate to several orders of magnitude reduction of sampling overhead for a circuit consisting of a moderate number of layers."],"url":"http://arxiv.org/abs/2403.06852v1","category":"quant-ph"}
{"created":"2024-03-11 15:14:04","title":"Defaults: a double-edged sword in governing common resources","abstract":"Extracting from shared resources requires making choices to balance personal profit and sustainability. We present the results of a behavioural experiment wherein we manipulate the default extraction from a finite resource. Participants were exposed to two treatments -- pro-social or self-serving extraction defaults -- and a control without defaults. We examined the persistence of these nudges by removing the default after five rounds. Results reveal that a self-serving default increased the average extraction while present, whereas a pro-social default only decreased extraction for the first two rounds. Notably, the influence of defaults depended on individual inclinations, with cooperative individuals extracting more under a self-serving default, and selfish individuals less under a pro-social default. After the removal of the default, we observed no significant differences with the control treatment. Our research highlights the potential of defaults as cost-effective tools for promoting sustainability, while also advocating for a careful use to avoid adverse effects.","sentences":["Extracting from shared resources requires making choices to balance personal profit and sustainability.","We present the results of a behavioural experiment wherein we manipulate the default extraction from a finite resource.","Participants were exposed to two treatments -- pro-social or self-serving extraction defaults -- and a control without defaults.","We examined the persistence of these nudges by removing the default after five rounds.","Results reveal that a self-serving default increased the average extraction while present, whereas a pro-social default only decreased extraction for the first two rounds.","Notably, the influence of defaults depended on individual inclinations, with cooperative individuals extracting more under a self-serving default, and selfish individuals less under a pro-social default.","After the removal of the default, we observed no significant differences with the control treatment.","Our research highlights the potential of defaults as cost-effective tools for promoting sustainability, while also advocating for a careful use to avoid adverse effects."],"url":"http://arxiv.org/abs/2403.06796v1","category":"cs.GT"}
{"created":"2024-03-11 15:12:03","title":"Closed-loop control of gamma oscillations in the brain connections through the transcranial stimulations","abstract":"The reconstruction of brain neural network connections occurs not only during the infancy and early childhood stages of brain development, but also in patients with cognitive impairment in middle and old age under the therapy with stimulated external interference, such as the non-invasive repetitive transcranial magnetic stimulation (rTMS) and the transcranial direct current stimulation(tDCS). However, until now, it is not clear how brain stimulation triggers and controls the reconstruction of neural network connections in the brain. This paper combines the EEG data analysis and the cortical neuronal network modeling methods. On one hand, an E-I balanced cortical neural network model was constructed under a long-lasting external stimulation of sinusoidal-exponential form TMS or square-wave tDCS was introduced into the network model for simulate the treatment process for the brain connections. On the other hand, by combining Butterworth filter and functional connectivity algorithm, the paper analyzes the relations between the attentional gamma oscillation responses and the brain connection based on the publicly available EEGs during the pre-tDCS and post-tDCS treatment phases. Firstly, the simulation results indicate that, during long-lasting external stimulations of tDCS/rTMS, The sustained gamma oscillation was found to trigger more release of BDNF from astrocytes to participate in the positively reshaping the excitatory neuronal network connection.","sentences":["The reconstruction of brain neural network connections occurs not only during the infancy and early childhood stages of brain development, but also in patients with cognitive impairment in middle and old age under the therapy with stimulated external interference, such as the non-invasive repetitive transcranial magnetic stimulation (rTMS) and the transcranial direct current stimulation(tDCS).","However, until now, it is not clear how brain stimulation triggers and controls the reconstruction of neural network connections in the brain.","This paper combines the EEG data analysis and the cortical neuronal network modeling methods.","On one hand, an E-I balanced cortical neural network model was constructed under a long-lasting external stimulation of sinusoidal-exponential form TMS or square-wave tDCS was introduced into the network model for simulate the treatment process for the brain connections.","On the other hand, by combining Butterworth filter and functional connectivity algorithm, the paper analyzes the relations between the attentional gamma oscillation responses and the brain connection based on the publicly available EEGs during the pre-tDCS and post-tDCS treatment phases.","Firstly, the simulation results indicate that, during long-lasting external stimulations of tDCS/rTMS, The sustained gamma oscillation was found to trigger more release of BDNF from astrocytes to participate in the positively reshaping the excitatory neuronal network connection."],"url":"http://arxiv.org/abs/2403.06794v1","category":"q-bio.NC"}
{"created":"2024-03-11 14:39:51","title":"Local Intuitionistic Modal Logics and Their Calculi","abstract":"We investigate intuitionistic modal logics with locally interpreted $\\square$ and $\\lozenge$. The basic logic LIK is stronger than constructive modal logic WK and incomparable with intuitionistic modal logic IK. We propose an axiomatization of LIK and some of its extensions. We propose bi-nested calculi for LIK and these extensions, thus providing both a decision procedure and a procedure of finite countermodel extraction.","sentences":["We investigate intuitionistic modal logics with locally interpreted $\\square$ and $\\lozenge$. The basic logic LIK is stronger than constructive modal logic WK and incomparable with intuitionistic modal logic IK.","We propose an axiomatization of LIK and some of its extensions.","We propose bi-nested calculi for LIK and these extensions, thus providing both a decision procedure and a procedure of finite countermodel extraction."],"url":"http://arxiv.org/abs/2403.06772v1","category":"cs.LO"}
{"created":"2024-03-11 13:47:55","title":"An invariance principle for the 2d weakly self-repelling Brownian polymer","abstract":"We investigate the large-scale behaviour of the Self-Repelling Brownian Polymer (SRBP) in the critical dimension $d=2$. The SRBP is a model of self-repelling motion, which is formally given by the solution a stochastic differential equation driven by a standard Brownian motion and with a drift given by the negative gradient of its own local time. As with its discrete counterpart, the \"true\" self-avoiding walk (TSAW) of [D.J. Amit, G. Parisi, & L. Peliti, Asymptotic behaviour of the \"true\" self-avoiding walk, Phys. Rev. B, 1983], it is conjectured to be logarithmically superdiffusive, i.e. to be such that its mean-square displacement grows as $t(\\log t)^\\beta$ for $t$ large and some currently unknown $\\beta\\in(0,1)$. The main result of the paper is an invariance principle for the SRBP under the weak coupling scaling, which corresponds to scaling the SRBP diffusively and simultaneously tuning down the strength of the self-interaction in a scale-dependent way. The diffusivity for the limiting Brownian motion is explicit and its expression provides compelling evidence that the $\\beta$ above should be $1/2$. Further, we derive the scaling limit of the so-called environment seen by the particle process, which formally solves a non-linear singular stochastic PDE of transport-type, and prove this is given by the solution of a stochastic linear transport equation with enhanced diffusivity.","sentences":["We investigate the large-scale behaviour of the Self-Repelling Brownian Polymer (SRBP) in the critical dimension $d=2$. The SRBP is a model of self-repelling motion, which is formally given by the solution a stochastic differential equation driven by a standard Brownian motion and with a drift given by the negative gradient of its own local time.","As with its discrete counterpart, the \"true\" self-avoiding walk (TSAW) of [D.J. Amit, G. Parisi, & L. Peliti, Asymptotic behaviour of the \"true\" self-avoiding walk, Phys. Rev. B, 1983], it is conjectured to be logarithmically superdiffusive, i.e. to be such that its mean-square displacement grows as $t(\\log t)^\\beta$ for $t$ large and some currently unknown $\\beta\\in(0,1)$. The main result of the paper is an invariance principle for the SRBP under the weak coupling scaling, which corresponds to scaling the SRBP diffusively and simultaneously tuning down the strength of the self-interaction in a scale-dependent way.","The diffusivity for the limiting Brownian motion is explicit and its expression provides compelling evidence that the $\\beta$ above should be $1/2$. Further, we derive the scaling limit of the so-called environment seen by the particle process, which formally solves a non-linear singular stochastic PDE of transport-type, and prove this is given by the solution of a stochastic linear transport equation with enhanced diffusivity."],"url":"http://arxiv.org/abs/2403.06730v1","category":"math.PR"}
{"created":"2024-03-11 13:42:12","title":"Bayesian prediction regions and density estimation with type-2 censored data","abstract":"For exponentially distributed lifetimes, we consider the prediction of future order statistics based on having observed the first $m$ order statistics. We focus on the previously less explored aspects of predicting: (i) an arbitrary pair of future order statistics such as the next and last ones, as well as (ii) the next $N$ future order statistics. We provide explicit and exact Bayesian credible regions associated with Gamma priors, and constructed by identifying a region with a given credibility $1-\\lambda$ under the Bayesian predictive density. For (ii), the HPD region is obtained, while a two-step algorithm is given for (i). The predictive distributions are represented as mixtures of bivariate Pareto distributions, as well as multivariate Pareto distributions. For the non-informative prior density choice, we demonstrate that a resulting Bayesian credible region has matching frequentist coverage probability, and that the resulting predictive density possesses the optimality properties of best invariance and minimaxity.","sentences":["For exponentially distributed lifetimes, we consider the prediction of future order statistics based on having observed the first $m$ order statistics.","We focus on the previously less explored aspects of predicting: (i) an arbitrary pair of future order statistics such as the next and last ones, as well as (ii) the next $N$ future order statistics.","We provide explicit and exact Bayesian credible regions associated with Gamma priors, and constructed by identifying a region with a given credibility $1-\\lambda$ under the Bayesian predictive density.","For (ii), the HPD region is obtained, while a two-step algorithm is given for (i).","The predictive distributions are represented as mixtures of bivariate Pareto distributions, as well as multivariate Pareto distributions.","For the non-informative prior density choice, we demonstrate that a resulting Bayesian credible region has matching frequentist coverage probability, and that the resulting predictive density possesses the optimality properties of best invariance and minimaxity."],"url":"http://arxiv.org/abs/2403.06718v1","category":"math.ST"}
{"created":"2024-03-11 13:39:46","title":"Societal and scientific impact of policy research: A large-scale empirical study of some explanatory factors using Altmetric and Overton","abstract":"This study investigates how scientific research influences policymaking by analyzing citations of research articles in policy documents (policy impact) for nearly 125,000 articles across 434 public policy journals. We reveal distinct citation patterns between policymakers and other stakeholders like researchers, journalists, and the public. News and blog mentions, social media engagement, and open access publications (excluding fully open access) significantly increase the likelihood of a research article being cited in policy documents. Conversely, articles locked behind paywalls and those published under the full open access model (based on Altmetric data) have a lower chance of being policy-cited. Publication year and policy type show no significant influence. Our findings emphasize the crucial role of science communication channels like news media and social media in bridging the gap between research and policy. Interestingly, academic citations hold a weaker influence on policy citations compared to news mentions, suggesting a potential disconnect between how researchers reference research and how policymakers utilize it. This highlights the need for improved communication strategies to ensure research informs policy decisions more effectively. This study provides valuable insights for researchers, policymakers, and science communicators. Researchers can tailor their dissemination efforts to reach policymakers through media channels. Policymakers can leverage these findings to identify research with higher policy relevance. Science communicators can play a critical role in translating research for policymakers and fostering dialogue between the scientific and policymaking communities.","sentences":["This study investigates how scientific research influences policymaking by analyzing citations of research articles in policy documents (policy impact) for nearly 125,000 articles across 434 public policy journals.","We reveal distinct citation patterns between policymakers and other stakeholders like researchers, journalists, and the public.","News and blog mentions, social media engagement, and open access publications (excluding fully open access) significantly increase the likelihood of a research article being cited in policy documents.","Conversely, articles locked behind paywalls and those published under the full open access model (based on Altmetric data) have a lower chance of being policy-cited.","Publication year and policy type show no significant influence.","Our findings emphasize the crucial role of science communication channels like news media and social media in bridging the gap between research and policy.","Interestingly, academic citations hold a weaker influence on policy citations compared to news mentions, suggesting a potential disconnect between how researchers reference research and how policymakers utilize it.","This highlights the need for improved communication strategies to ensure research informs policy decisions more effectively.","This study provides valuable insights for researchers, policymakers, and science communicators.","Researchers can tailor their dissemination efforts to reach policymakers through media channels.","Policymakers can leverage these findings to identify research with higher policy relevance.","Science communicators can play a critical role in translating research for policymakers and fostering dialogue between the scientific and policymaking communities."],"url":"http://arxiv.org/abs/2403.06714v1","category":"cs.DL"}
{"created":"2024-03-11 13:01:57","title":"NLP4RE Tools: Classification, Overview, and Management","abstract":"Tools constitute an essential contribution to natural language processing for requirements engineering (NLP4RE) research. They are executable instruments that make research usable and applicable in practice. In this chapter, we first introduce a systematic classification of NLP4RE tools to improve the understanding of their types and properties. Then, we extend an existing overview with a systematic summary of 126 NLP4RE tools published between April 2019 and June 2023 to ease reuse and evolution of existing tools. Finally, we provide instructions on how to create, maintain, and disseminate NLP4RE tools to support a more rigorous management and dissemination.","sentences":["Tools constitute an essential contribution to natural language processing for requirements engineering (NLP4RE) research.","They are executable instruments that make research usable and applicable in practice.","In this chapter, we first introduce a systematic classification of NLP4RE tools to improve the understanding of their types and properties.","Then, we extend an existing overview with a systematic summary of 126 NLP4RE tools published between April 2019 and June 2023 to ease reuse and evolution of existing tools.","Finally, we provide instructions on how to create, maintain, and disseminate NLP4RE tools to support a more rigorous management and dissemination."],"url":"http://arxiv.org/abs/2403.06685v1","category":"cs.SE"}
{"created":"2024-03-11 12:57:51","title":"Transferring Relative Monocular Depth to Surgical Vision with Temporal Consistency","abstract":"Relative monocular depth, inferring depth up to shift and scale from a single image, is an active research topic. Recent deep learning models, trained on large and varied meta-datasets, now provide excellent performance in the domain of natural images. However, few datasets exist which provide ground truth depth for endoscopic images, making training such models from scratch unfeasible. This work investigates the transfer of these models into the surgical domain, and presents an effective and simple way to improve on standard supervision through the use of temporal consistency self-supervision. We show temporal consistency significantly improves supervised training alone when transferring to the low-data regime of endoscopy, and outperforms the prevalent self-supervision technique for this task. In addition we show our method drastically outperforms the state-of-the-art method from within the domain of endoscopy. We also release our code, model and ensembled meta-dataset, Meta-MED, establishing a strong benchmark for future work.","sentences":["Relative monocular depth, inferring depth up to shift and scale from a single image, is an active research topic.","Recent deep learning models, trained on large and varied meta-datasets, now provide excellent performance in the domain of natural images.","However, few datasets exist which provide ground truth depth for endoscopic images, making training such models from scratch unfeasible.","This work investigates the transfer of these models into the surgical domain, and presents an effective and simple way to improve on standard supervision through the use of temporal consistency self-supervision.","We show temporal consistency significantly improves supervised training alone when transferring to the low-data regime of endoscopy, and outperforms the prevalent self-supervision technique for this task.","In addition we show our method drastically outperforms the state-of-the-art method from within the domain of endoscopy.","We also release our code, model and ensembled meta-dataset, Meta-MED, establishing a strong benchmark for future work."],"url":"http://arxiv.org/abs/2403.06683v1","category":"cs.CV"}
{"created":"2024-03-11 12:42:31","title":"Untangling Gaussian Mixtures","abstract":"Tangles were originally introduced as a concept to formalize regions of high connectivity in graphs. In recent years, they have also been discovered as a link between structural graph theory and data science: when interpreting similarity in data sets as connectivity between points, finding clusters in the data essentially amounts to finding tangles in the underlying graphs. This paper further explores the potential of tangles in data sets as a means for a formal study of clusters. Real-world data often follow a normal distribution. Accounting for this, we develop a quantitative theory of tangles in data sets drawn from Gaussian mixtures. To this end, we equip the data with a graph structure that models similarity between the points and allows us to apply tangle theory to the data. We provide explicit conditions under which tangles associated with the marginal Gaussian distributions exist asymptotically almost surely. This can be considered as a sufficient formal criterion for the separabability of clusters in the data.","sentences":["Tangles were originally introduced as a concept to formalize regions of high connectivity in graphs.","In recent years, they have also been discovered as a link between structural graph theory and data science: when interpreting similarity in data sets as connectivity between points, finding clusters in the data essentially amounts to finding tangles in the underlying graphs.","This paper further explores the potential of tangles in data sets as a means for a formal study of clusters.","Real-world data often follow a normal distribution.","Accounting for this, we develop a quantitative theory of tangles in data sets drawn from Gaussian mixtures.","To this end, we equip the data with a graph structure that models similarity between the points and allows us to apply tangle theory to the data.","We provide explicit conditions under which tangles associated with the marginal Gaussian distributions exist asymptotically almost surely.","This can be considered as a sufficient formal criterion for the separabability of clusters in the data."],"url":"http://arxiv.org/abs/2403.06671v1","category":"math.ST"}
{"created":"2024-03-11 12:36:14","title":"PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor","abstract":"Adversarial robustness of the neural network is a significant concern when it is applied to security-critical domains. In this situation, adversarial distillation is a promising option which aims to distill the robustness of the teacher network to improve the robustness of a small student network. Previous works pretrain the teacher network to make it robust to the adversarial examples aimed at itself. However, the adversarial examples are dependent on the parameters of the target network. The fixed teacher network inevitably degrades its robustness against the unseen transferred adversarial examples which targets the parameters of the student network in the adversarial distillation process. We propose PeerAiD to make a peer network learn the adversarial examples of the student network instead of adversarial examples aimed at itself. PeerAiD is an adversarial distillation that trains the peer network and the student network simultaneously in order to make the peer network specialized for defending the student network. We observe that such peer networks surpass the robustness of pretrained robust teacher network against student-attacked adversarial samples. With this peer network and adversarial distillation, PeerAiD achieves significantly higher robustness of the student network with AutoAttack (AA) accuracy up to 1.66%p and improves the natural accuracy of the student network up to 4.72%p with ResNet-18 and TinyImageNet dataset.","sentences":["Adversarial robustness of the neural network is a significant concern when it is applied to security-critical domains.","In this situation, adversarial distillation is a promising option which aims to distill the robustness of the teacher network to improve the robustness of a small student network.","Previous works pretrain the teacher network to make it robust to the adversarial examples aimed at itself.","However, the adversarial examples are dependent on the parameters of the target network.","The fixed teacher network inevitably degrades its robustness against the unseen transferred adversarial examples which targets the parameters of the student network in the adversarial distillation process.","We propose PeerAiD to make a peer network learn the adversarial examples of the student network instead of adversarial examples aimed at itself.","PeerAiD is an adversarial distillation that trains the peer network and the student network simultaneously in order to make the peer network specialized for defending the student network.","We observe that such peer networks surpass the robustness of pretrained robust teacher network against student-attacked adversarial samples.","With this peer network and adversarial distillation, PeerAiD achieves significantly higher robustness of the student network with AutoAttack (AA) accuracy up to 1.66%p and improves the natural accuracy of the student network up to 4.72%p with ResNet-18 and TinyImageNet dataset."],"url":"http://arxiv.org/abs/2403.06668v1","category":"cs.LG"}
{"created":"2024-03-11 12:30:52","title":"A PDE Framework of Consensus-Based Optimization for Objectives with Multiple Global Minimizers","abstract":"Consensus-based optimization (CBO) is an agent-based derivative-free method for non-smooth global optimization that has been introduced in 2017, leveraging a surprising interplay between stochastic exploration and Laplace principle. In addition to its versatility and effectiveness in handling high-dimensional, non-convex, and nonsmooth optimization problems, this approach lends itself well to theoretical analysis. Indeed, its dynamics is governed by a degenerate nonlinear Fokker-Planck equation, whose large-time behavior explains the convergence of the method. Recent results provide guarantees of convergence under the restrictive assumption of a unique global minimizer for the objective function. In this work, we proposed a novel and simple variation of CBO to tackle nonconvex optimizations with multiple minimizers. Despite the simplicity of this new model, its analysis is particularly challenging because of its nonlinearity and nonlocal nature. We prove the existence of solutions of the corresponding nonlinear Fokker-Planck equation and we show exponential concentration in time to the set of minimizers made of multiple smooth, convex, and compact components. Our proofs require combining several ingredients, such as delicate geometrical arguments, new variants of a quantitative Laplace principle, ad hoc regularizations and approximations, and regularity theory for parabolic equations. Ultimately, this result suggests that the corresponding CBO algorithm, formulated as an Euler-Maruyama discretization of the underlying empirical stochastic process, tends to converge to global minimizers.","sentences":["Consensus-based optimization (CBO) is an agent-based derivative-free method for non-smooth global optimization that has been introduced in 2017, leveraging a surprising interplay between stochastic exploration and Laplace principle.","In addition to its versatility and effectiveness in handling high-dimensional, non-convex, and nonsmooth optimization problems, this approach lends itself well to theoretical analysis.","Indeed, its dynamics is governed by a degenerate nonlinear Fokker-Planck equation, whose large-time behavior explains the convergence of the method.","Recent results provide guarantees of convergence under the restrictive assumption of a unique global minimizer for the objective function.","In this work, we proposed a novel and simple variation of CBO to tackle nonconvex optimizations with multiple minimizers.","Despite the simplicity of this new model, its analysis is particularly challenging because of its nonlinearity and nonlocal nature.","We prove the existence of solutions of the corresponding nonlinear Fokker-Planck equation and we show exponential concentration in time to the set of minimizers made of multiple smooth, convex, and compact components.","Our proofs require combining several ingredients, such as delicate geometrical arguments, new variants of a quantitative Laplace principle, ad hoc regularizations and approximations, and regularity theory for parabolic equations.","Ultimately, this result suggests that the corresponding CBO algorithm, formulated as an Euler-Maruyama discretization of the underlying empirical stochastic process, tends to converge to global minimizers."],"url":"http://arxiv.org/abs/2403.06662v1","category":"math.AP"}
{"created":"2024-03-11 12:22:23","title":"Investigation of the Thermal Structure in the Atmospheric Boundary Layer During Evening Transition and the Impact of Aerosols on Radiative Cooling","abstract":"We have explored the evening transition using data from eighty days of observations across two fog seasons at the Kempegowda International Airport, Bengaluru (KIAB). Through field experiments and simulations integrating aerosol interaction in a radiation-conduction model, we elucidate the impact of aerosols on longwave cooling of the Atmospheric Boundary Layer (ABL). Field observations indicate that under calm and clear-sky conditions, the evening transition typically results in a distinct vertical thermal structure called the Lifted Temperature Minimum (LTM). We observe that the prevailing profile near the surface post-sunset is the LTM-profile. Additionally, the occurrence of LTM is observed to increase with decreases in downward and upward longwave flux, soil sensible heat flux, wind speed, and turbulent kinetic energy measured at two meters above ground level (AGL). In such scenarios, the intensity of LTM-profiles is primarily governed by aerosol-induced longwave heating rate (LHR) within the surface layer. Furthermore, the presence of clouds leads to increased downward flux, causing the disappearance of LTM, whereas shallow fog can enhance LTM intensity, as observed in both field observations and simulations. Usually, prevailing radiation models underestimate aerosol-induced longwave heating rate (LHR) by an order, compared to actual field observations. We attribute this difference to aerosol-induced radiation divergence. We show that impact of aerosol-induced LHR extends hundreds of meters into the inversion layer, affecting temperature profiles and potentially influencing processes such as fog formation. As the fog layer develops, LHR strengthens at its upper boundary, however, we highlight the difficulty in detecting this cooling using remote instruments such as microwave radiometer.","sentences":["We have explored the evening transition using data from eighty days of observations across two fog seasons at the Kempegowda International Airport, Bengaluru (KIAB).","Through field experiments and simulations integrating aerosol interaction in a radiation-conduction model, we elucidate the impact of aerosols on longwave cooling of the Atmospheric Boundary Layer (ABL).","Field observations indicate that under calm and clear-sky conditions, the evening transition typically results in a distinct vertical thermal structure called the Lifted Temperature Minimum (LTM).","We observe that the prevailing profile near the surface post-sunset is the LTM-profile.","Additionally, the occurrence of LTM is observed to increase with decreases in downward and upward longwave flux, soil sensible heat flux, wind speed, and turbulent kinetic energy measured at two meters above ground level (AGL).","In such scenarios, the intensity of LTM-profiles is primarily governed by aerosol-induced longwave heating rate (LHR) within the surface layer.","Furthermore, the presence of clouds leads to increased downward flux, causing the disappearance of LTM, whereas shallow fog can enhance LTM intensity, as observed in both field observations and simulations.","Usually, prevailing radiation models underestimate aerosol-induced longwave heating rate (LHR) by an order, compared to actual field observations.","We attribute this difference to aerosol-induced radiation divergence.","We show that impact of aerosol-induced LHR extends hundreds of meters into the inversion layer, affecting temperature profiles and potentially influencing processes such as fog formation.","As the fog layer develops, LHR strengthens at its upper boundary, however, we highlight the difficulty in detecting this cooling using remote instruments such as microwave radiometer."],"url":"http://arxiv.org/abs/2403.06656v1","category":"physics.ao-ph"}
{"created":"2024-03-11 12:17:17","title":"Symmetry Resolved Measures in Quantum Field Theory: a Short Review","abstract":"In this short review we present the key definitions, ideas and techniques involved in the study of symmetry resolved entanglement measures, with a focus on the symmetry resolved entanglement entropy. In order to be able to define such entanglement measures, it is essential that the theory under study possess an internal symmetry. Then, symmetry resolved entanglement measures quantify the contribution to a particular entanglement measure that can be associated to a chosen symmetry sector. Our review focuses on conformal (gapless/massless/critical) and integrable (gapped/massive) quantum field theories, where the leading computational technique employs symmetry fields known as (composite) branch point twist fields.","sentences":["In this short review we present the key definitions, ideas and techniques involved in the study of symmetry resolved entanglement measures, with a focus on the symmetry resolved entanglement entropy.","In order to be able to define such entanglement measures, it is essential that the theory under study possess an internal symmetry.","Then, symmetry resolved entanglement measures quantify the contribution to a particular entanglement measure that can be associated to a chosen symmetry sector.","Our review focuses on conformal (gapless/massless/critical) and integrable (gapped/massive) quantum field theories, where the leading computational technique employs symmetry fields known as (composite) branch point twist fields."],"url":"http://arxiv.org/abs/2403.06652v1","category":"hep-th"}
{"created":"2024-03-11 12:07:41","title":"Unisolvence of random Kansa collocation by Thin-Plate Splines for the Poisson equation","abstract":"Existence of sufficient conditions for unisolvence of Kansa unsymmetric collocation for PDEs is still an open problem. In this paper we make a first step in this direction, proving that unsymmetric collocation matrices with Thin-Plate Splines for the 2D Poisson equation are almost surely nonsingular, when the discretization points are chosen randomly on domains with analytic boundary.","sentences":["Existence of sufficient conditions for unisolvence of Kansa unsymmetric collocation for PDEs is still an open problem.","In this paper we make a first step in this direction, proving that unsymmetric collocation matrices with Thin-Plate Splines for the 2D Poisson equation are almost surely nonsingular, when the discretization points are chosen randomly on domains with analytic boundary."],"url":"http://arxiv.org/abs/2403.06646v1","category":"math.NA"}
{"created":"2024-03-11 11:46:12","title":"Stealing Part of a Production Language Model","abstract":"We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under \\$2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.","sentences":["We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2.","Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access.","For under \\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models.","We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively.","We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under \\$2,000 in queries to recover the entire projection matrix.","We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack."],"url":"http://arxiv.org/abs/2403.06634v1","category":"cs.CR"}
{"created":"2024-03-11 11:35:12","title":"The Prime Geodesic Theorem for the Picard Manifold","abstract":"We prove the prime geodesic theorem for the Picard manifold whose error term shrinks as the hybrid subconvex exponent $\\theta$ for quadratic Dirichlet $L$-functions over Gaussian integers decreases. Our rate of decay is faster than that of Balkanova-Frolenkov (2022) and Kaneko (2022), and leads to the exponent $\\frac{202}{139}+\\varepsilon$ in the prime geodesic theorem under the generalised Lindel\\\"{o}f hypothesis $\\theta = 0$, improving upon the existing conditional results. This provides further solid evidence towards the validity of the conjectural exponent $1+\\varepsilon$.","sentences":["We prove the prime geodesic theorem for the Picard manifold whose error term shrinks as the hybrid subconvex exponent $\\theta$ for quadratic Dirichlet $L$-functions over Gaussian integers decreases.","Our rate of decay is faster than that of Balkanova-Frolenkov (2022) and Kaneko (2022), and leads to the exponent $\\frac{202}{139}+\\varepsilon$ in the prime geodesic theorem under the generalised Lindel\\\"{o}f hypothesis $\\theta = 0$, improving upon the existing conditional results.","This provides further solid evidence towards the validity of the conjectural exponent $1+\\varepsilon$."],"url":"http://arxiv.org/abs/2403.06626v1","category":"math.NT"}
{"created":"2024-03-11 10:45:26","title":"Study of dark matter scattering off ${}^2$H and ${}^4$He nuclei within chiral effective field theory","abstract":"We study dark matter, assumed to be composed by weak interacting massive particles (WIMPs), scattering off ${}^2$H and ${}^4$He nuclei. In order to parameterize the WIMP-nucleon interaction the chiral effective field theory approach is used. Considering only interactions invariant under parity, charge conjugation and time reversal, we examine five interaction types: scalar, pseudoscalar, vector, axial and tensor. Scattering amplitudes between two nucleons and a WIMP are determined up to second order of chiral perturbation theory. We apply this program to calculate the interaction rate as function of the WIMP mass and of the magnitude of the WIMP-quark coupling constants. From our study, we conclude that the scalar nuclear response functions result much greater than the others due to theirs large combination of low energy constants. We verify that the leading order contributions are dominant in this low energy processes. We also provide an estimate for the background due to atmospheric neutrinos.","sentences":["We study dark matter, assumed to be composed by weak interacting massive particles (WIMPs), scattering off ${}^2$H and ${}^4$He nuclei.","In order to parameterize the WIMP-nucleon interaction the chiral effective field theory approach is used.","Considering only interactions invariant under parity, charge conjugation and time reversal, we examine five interaction types: scalar, pseudoscalar, vector, axial and tensor.","Scattering amplitudes between two nucleons and a WIMP are determined up to second order of chiral perturbation theory.","We apply this program to calculate the interaction rate as function of the WIMP mass and of the magnitude of the WIMP-quark coupling constants.","From our study, we conclude that the scalar nuclear response functions result much greater than the others due to theirs large combination of low energy constants.","We verify that the leading order contributions are dominant in this low energy processes.","We also provide an estimate for the background due to atmospheric neutrinos."],"url":"http://arxiv.org/abs/2403.06599v1","category":"hep-ph"}
{"created":"2024-03-11 10:40:08","title":"Towards more accurate and useful data anonymity vulnerability measures","abstract":"The purpose of anonymizing structured data is to protect the privacy of individuals in the data while retaining the statistical properties of the data. There is a large body of work that examines anonymization vulnerabilities. Focusing on strong anonymization mechanisms, this paper examines a number of prominent attack papers and finds several problems, all of which lead to overstating risk. First, some papers fail to establish a correct statistical inference baseline (or any at all), leading to incorrect measures. Notably, the reconstruction attack from the US Census Bureau that led to a redesign of its disclosure method made this mistake. We propose the non-member framework, an improved method for how to compute a more accurate inference baseline, and give examples of its operation.   Second, some papers don't use a realistic membership base rate, leading to incorrect precision measures if precision is reported. Third, some papers unnecessarily report measures in such a way that it is difficult or impossible to assess risk. Virtually the entire literature on membership inference attacks, dozens of papers, make one or both of these errors. We propose that membership inference papers report precision/recall values using a representative range of base rates.","sentences":["The purpose of anonymizing structured data is to protect the privacy of individuals in the data while retaining the statistical properties of the data.","There is a large body of work that examines anonymization vulnerabilities.","Focusing on strong anonymization mechanisms, this paper examines a number of prominent attack papers and finds several problems, all of which lead to overstating risk.","First, some papers fail to establish a correct statistical inference baseline (or any at all), leading to incorrect measures.","Notably, the reconstruction attack from the US Census Bureau that led to a redesign of its disclosure method made this mistake.","We propose the non-member framework, an improved method for how to compute a more accurate inference baseline, and give examples of its operation.   ","Second, some papers don't use a realistic membership base rate, leading to incorrect precision measures if precision is reported.","Third, some papers unnecessarily report measures in such a way that it is difficult or impossible to assess risk.","Virtually the entire literature on membership inference attacks, dozens of papers, make one or both of these errors.","We propose that membership inference papers report precision/recall values using a representative range of base rates."],"url":"http://arxiv.org/abs/2403.06595v1","category":"cs.CR"}
{"created":"2024-03-11 10:33:45","title":"Tail Optimality and Performance Analysis of the Nudge-M Scheduling Algorithm","abstract":"Recently it was shown that the response time of First-Come-First-Served (FCFS) scheduling can be stochastically and asymptotically improved upon by the {\\it Nudge} scheduling algorithm in case of light-tailed job size distributions. Such improvements are feasible even when the jobs are partitioned into two types and the scheduler only has information about the type of incoming jobs (but not their size).   In this paper we introduce Nudge-$M$ scheduling, where basically any incoming type-1 job is allowed to pass any type-2 job that is still waiting in the queue given that it arrived as one of the last $M$ jobs. We prove that Nudge-$M$ has an asymptotically optimal response time within a large family of Nudge scheduling algorithms when job sizes are light-tailed. Simple explicit results for the asymptotic tail improvement ratio (ATIR) of Nudge-$M$ over FCFS are derived as well as explicit results for the optimal parameter $M$. An expression for the ATIR that only depends on the type-1 ad type-2 mean job sizes and the fraction of type-1 jobs is presented in the heavy traffic setting.   The paper further presents a numerical method to compute the response time distribution and mean response time of Nudge-$M$ scheduling provided that the job size distribution of both job types follows a phase-type distribution (by making use of the framework of Markov modulated fluid queues with jumps).","sentences":["Recently it was shown that the response time of First-Come-First-Served (FCFS) scheduling can be stochastically and asymptotically improved upon by the {\\it Nudge} scheduling algorithm in case of light-tailed job size distributions.","Such improvements are feasible even when the jobs are partitioned into two types and the scheduler only has information about the type of incoming jobs (but not their size).   ","In this paper we introduce Nudge-$M$ scheduling, where basically any incoming type-1 job is allowed to pass any type-2 job that is still waiting in the queue given that it arrived as one of the last $M$ jobs.","We prove that Nudge-$M$ has an asymptotically optimal response time within a large family of Nudge scheduling algorithms when job sizes are light-tailed.","Simple explicit results for the asymptotic tail improvement ratio (ATIR) of Nudge-$M$ over FCFS are derived as well as explicit results for the optimal parameter $M$. An expression for the ATIR that only depends on the type-1 ad type-2 mean job sizes and the fraction of type-1 jobs is presented in the heavy traffic setting.   ","The paper further presents a numerical method to compute the response time distribution and mean response time of Nudge-$M$ scheduling provided that the job size distribution of both job types follows a phase-type distribution (by making use of the framework of Markov modulated fluid queues with jumps)."],"url":"http://arxiv.org/abs/2403.06588v1","category":"cs.PF"}
{"created":"2024-03-11 10:28:21","title":"Nuclear magnetic shielding in heliumlike ions","abstract":"Ab initio QED calculations of the nuclear magnetic shielding constant in helium-like ions are presented. We combine the nonrelativistic QED approach based on an expansion in powers of the fine-structure constant $\\alpha$ and the so-called ``all-order'' QED approach which includes all orders in the parameter $Z\\alpha$ but uses a perturbation expansion in the parameter $1/Z$ (where $Z$ is the nuclear charge number). The combination of the two complementary methods makes our treatment applicable both to low-$Z$ and high-$Z$ ions. Our calculations confirm the presence of a rare antiscreening effect for the relativistic shielding correction and demonstrate the importance of the inclusion of the negative-energy part of the Dirac spectrum.","sentences":["Ab initio QED calculations of the nuclear magnetic shielding constant in helium-like ions are presented.","We combine the nonrelativistic QED approach based on an expansion in powers of the fine-structure constant $\\alpha$ and the so-called ``all-order'' QED approach which includes all orders in the parameter $Z\\alpha$ but uses a perturbation expansion in the parameter $1/Z$ (where $Z$ is the nuclear charge number).","The combination of the two complementary methods makes our treatment applicable both to low-$Z$ and high-$Z$ ions.","Our calculations confirm the presence of a rare antiscreening effect for the relativistic shielding correction and demonstrate the importance of the inclusion of the negative-energy part of the Dirac spectrum."],"url":"http://arxiv.org/abs/2403.06582v1","category":"physics.atom-ph"}
{"created":"2024-03-11 09:57:56","title":"Unconditional deep-water limit of the intermediate long wave equation in low-regularity","abstract":"In this paper, we establish the unconditional deep-water limit of the intermediate long wave equation (ILW) to the Benjamin-Ono equation (BO) in low-regularity Sobolev spaces on both the real line and the circle. Our main tool is new unconditional uniqueness results for ILW in $H^s$ when $s_0<s\\leq \\frac 14$ on the line and $s_0<s< \\frac 12$ on the circle, where $s_0 = 3-\\sqrt{33/4}\\approx 0.1277$. Here, we adapt the strategy of Mo\\c{s}incat-Pilod (2023) for BO to the setting of ILW by viewing ILW as a perturbation of BO and making use of the smoothing property of the perturbation term.","sentences":["In this paper, we establish the unconditional deep-water limit of the intermediate long wave equation (ILW) to the Benjamin-Ono equation (BO) in low-regularity Sobolev spaces on both the real line and the circle.","Our main tool is new unconditional uniqueness results for ILW in $H^s$ when $s_0<s\\leq","\\frac 14$ on the line and $s_0<s< \\frac 12$ on the circle, where $s_0 = 3-\\sqrt{33/4}\\approx 0.1277$. Here, we adapt the strategy of Mo\\c{s}incat-Pilod (2023) for BO to the setting of ILW by viewing ILW as a perturbation of BO and making use of the smoothing property of the perturbation term."],"url":"http://arxiv.org/abs/2403.06554v1","category":"math.AP"}
{"created":"2024-03-11 09:47:21","title":"Fun Maximizing Search, (Non) Instance Optimality, and Video Games for Parrots","abstract":"Computerized Adaptive Testing (CAT) measures an examinee's ability while adapting to their level. Both too many questions and too many hard questions can make a test frustrating. Are there some CAT algorithms which can be proven to be theoretically better than others, and in which framework? We show that slightly extending the traditional framework yields a partial order on CAT algorithms. For uni-dimensional knowledge domains, we analyze the theoretical performance of some old and new algorithms, and we prove that none of the algorithms presented are instance optimal, conjecturing that no instance optimal can exist for the CAT problem.","sentences":["Computerized Adaptive Testing (CAT) measures an examinee's ability while adapting to their level.","Both too many questions and too many hard questions can make a test frustrating.","Are there some CAT algorithms which can be proven to be theoretically better than others, and in which framework?","We show that slightly extending the traditional framework yields a partial order on CAT algorithms.","For uni-dimensional knowledge domains, we analyze the theoretical performance of some old and new algorithms, and we prove that none of the algorithms presented are instance optimal, conjecturing that no instance optimal can exist for the CAT problem."],"url":"http://arxiv.org/abs/2403.06547v1","category":"cs.DS"}
{"created":"2024-03-11 09:24:06","title":"On the Consideration of AI Openness: Can Good Intent Be Abused?","abstract":"Openness is critical for the advancement of science. In particular, recent rapid progress in AI has been made possible only by various open-source models, datasets, and libraries. However, this openness also means that technologies can be freely used for socially harmful purposes. Can open-source models or datasets be used for malicious purposes? If so, how easy is it to adapt technology for such goals? Here, we conduct a case study in the legal domain, a realm where individual decisions can have profound social consequences. To this end, we build EVE, a dataset consisting of 200 examples of questions and corresponding answers about criminal activities based on 200 Korean precedents. We found that a widely accepted open-source LLM, which initially refuses to answer unethical questions, can be easily tuned with EVE to provide unethical and informative answers about criminal activities. This implies that although open-source technologies contribute to scientific progress, some care must be taken to mitigate possible malicious use cases. Warning: This paper contains contents that some may find unethical.","sentences":["Openness is critical for the advancement of science.","In particular, recent rapid progress in AI has been made possible only by various open-source models, datasets, and libraries.","However, this openness also means that technologies can be freely used for socially harmful purposes.","Can open-source models or datasets be used for malicious purposes?","If so, how easy is it to adapt technology for such goals?","Here, we conduct a case study in the legal domain, a realm where individual decisions can have profound social consequences.","To this end, we build EVE, a dataset consisting of 200 examples of questions and corresponding answers about criminal activities based on 200 Korean precedents.","We found that a widely accepted open-source LLM, which initially refuses to answer unethical questions, can be easily tuned with EVE to provide unethical and informative answers about criminal activities.","This implies that although open-source technologies contribute to scientific progress, some care must be taken to mitigate possible malicious use cases.","Warning:","This paper contains contents that some may find unethical."],"url":"http://arxiv.org/abs/2403.06537v1","category":"cs.CL"}
{"created":"2024-03-11 08:32:00","title":"Extreme Point Pursuit -- Part I: A Framework for Constant Modulus Optimization","abstract":"This study develops a framework for a class of constant modulus (CM) optimization problems, which covers binary constraints, discrete phase constraints, semi-orthogonal matrix constraints, non-negative semi-orthogonal matrix constraints, and several types of binary assignment constraints. Capitalizing on the basic principles of concave minimization and error bounds, we study a convex-constrained penalized formulation for general CM problems. The advantage of such formulation is that it allows us to leverage non-convex optimization techniques, such as the simple projected gradient method, to build algorithms. As the first part of this study, we explore the theory of this framework. We study conditions under which the formulation provides exact penalization results. We also examine computational aspects relating to the use of the projected gradient method for each type of CM constraint. Our study suggests that the proposed framework has a broad scope of applicability.","sentences":["This study develops a framework for a class of constant modulus (CM) optimization problems, which covers binary constraints, discrete phase constraints, semi-orthogonal matrix constraints, non-negative semi-orthogonal matrix constraints, and several types of binary assignment constraints.","Capitalizing on the basic principles of concave minimization and error bounds, we study a convex-constrained penalized formulation for general CM problems.","The advantage of such formulation is that it allows us to leverage non-convex optimization techniques, such as the simple projected gradient method, to build algorithms.","As the first part of this study, we explore the theory of this framework.","We study conditions under which the formulation provides exact penalization results.","We also examine computational aspects relating to the use of the projected gradient method for each type of CM constraint.","Our study suggests that the proposed framework has a broad scope of applicability."],"url":"http://arxiv.org/abs/2403.06506v1","category":"eess.SP"}
{"created":"2024-03-11 08:09:13","title":"Are you sure? Modelling Drivers' Confidence Judgments in Left-Turn Gap Acceptance Decisions","abstract":"When a person makes a decision, it is automatically accompanied by a subjective probability judgment of the decision being correct, in other words, a confidence judgment. A better understanding of the mechanisms responsible for these confidence judgments could provide novel insights into human behavior. However, so far confidence judgments have been mostly studied in simplistic laboratory tasks while little is known about confidence in naturalistic dynamic tasks such as driving. In this study, we made a first attempt of connecting fundamental research on confidence with naturalistic driver behavior. We investigated the confidence of drivers in left-turn gap acceptance decisions in a driver simulator experiment (N=17). We found that confidence in these decisions depends on the size of the gap to the oncoming vehicle. Specifically, confidence increased with the gap size for trials in which the gap was accepted, and decreased with the gap size for rejected gaps. Similarly to more basic tasks, confidence was negatively related to the response times and correlated with action dynamics during decision execution. Finally, we found that confidence judgments can be captured with an extended dynamic drift-diffusion model. In the model, the drift rate of the evidence accumulator as well as the decision boundaries are functions of the gap size. Furthermore, we demonstrated that allowing for post-decision evidence accumulation in the model increases its ability to describe confidence judgments in rejected gap decisions. Overall, our study confirmed that principles known from fundamental confidence research extend to confidence judgments in dynamic decisions during a naturalistic task.","sentences":["When a person makes a decision, it is automatically accompanied by a subjective probability judgment of the decision being correct, in other words, a confidence judgment.","A better understanding of the mechanisms responsible for these confidence judgments could provide novel insights into human behavior.","However, so far confidence judgments have been mostly studied in simplistic laboratory tasks while little is known about confidence in naturalistic dynamic tasks such as driving.","In this study, we made a first attempt of connecting fundamental research on confidence with naturalistic driver behavior.","We investigated the confidence of drivers in left-turn gap acceptance decisions in a driver simulator experiment (N=17).","We found that confidence in these decisions depends on the size of the gap to the oncoming vehicle.","Specifically, confidence increased with the gap size for trials in which the gap was accepted, and decreased with the gap size for rejected gaps.","Similarly to more basic tasks, confidence was negatively related to the response times and correlated with action dynamics during decision execution.","Finally, we found that confidence judgments can be captured with an extended dynamic drift-diffusion model.","In the model, the drift rate of the evidence accumulator as well as the decision boundaries are functions of the gap size.","Furthermore, we demonstrated that allowing for post-decision evidence accumulation in the model increases its ability to describe confidence judgments in rejected gap decisions.","Overall, our study confirmed that principles known from fundamental confidence research extend to confidence judgments in dynamic decisions during a naturalistic task."],"url":"http://arxiv.org/abs/2403.06496v1","category":"q-bio.NC"}
{"created":"2024-03-11 08:07:46","title":"Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Sample Prompts","abstract":"This paper explores the problem of Generalist Anomaly Detection (GAD), aiming to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any further training on the target data. Some recent studies have shown that large pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization capabilities on detecting industrial defects from various datasets, but their methods rely heavily on handcrafted text prompts about defects, making them difficult to generalize to anomalies in other applications, e.g., medical image anomalies or semantic anomalies in natural images. In this work, we propose to train a GAD model with few-shot normal images as sample prompts for AD on diverse datasets on the fly. To this end, we introduce a novel approach that learns an in-context residual learning model for GAD, termed InCTRL. It is trained on an auxiliary dataset to discriminate anomalies from normal samples based on a holistic evaluation of the residuals between query images and few-shot normal sample prompts. Regardless of the datasets, per definition of anomaly, larger residuals are expected for anomalies than normal samples, thereby enabling InCTRL to generalize across different domains without further training.","sentences":["This paper explores the problem of Generalist Anomaly Detection (GAD), aiming to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any further training on the target data.","Some recent studies have shown that large pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization capabilities on detecting industrial defects from various datasets, but their methods rely heavily on handcrafted text prompts about defects, making them difficult to generalize to anomalies in other applications, e.g., medical image anomalies or semantic anomalies in natural images.","In this work, we propose to train a GAD model with few-shot normal images as sample prompts for AD on diverse datasets on the fly.","To this end, we introduce a novel approach that learns an in-context residual learning model for GAD, termed InCTRL.","It is trained on an auxiliary dataset to discriminate anomalies from normal samples based on a holistic evaluation of the residuals between query images and few-shot normal sample prompts.","Regardless of the datasets, per definition of anomaly, larger residuals are expected for anomalies than normal samples, thereby enabling InCTRL to generalize across different domains without further training."],"url":"http://arxiv.org/abs/2403.06495v1","category":"cs.CV"}
{"created":"2024-03-11 07:56:24","title":"Magnetic vortex polarity reversal induced gyrotropic motion spectrum splitting in a ferromagnetic disk","abstract":"We investigate the gyrotropic motion of the magnetic vortex core in a chain of a few micron-sized Permalloy disks by electrical resistance measurement with amplitude-modulated magnetic field. We observe a distinctive splitting of the resistance peak due to the resonant vortex-core motion under heightened radio frequency (RF) magnetic field excitation. Our micromagnetic simulation identifies the splitting of the resonant peak as an outcome of vortex polarity reversal under substantial RF amplitudes. This study enhances our understanding of nonlinear magnetic vortex dynamics amidst large RF amplitudes and proposes a potential pathway for spintronic neural computing thanks to their unique and controllable magnetization dynamics.","sentences":["We investigate the gyrotropic motion of the magnetic vortex core in a chain of a few micron-sized Permalloy disks by electrical resistance measurement with amplitude-modulated magnetic field.","We observe a distinctive splitting of the resistance peak due to the resonant vortex-core motion under heightened radio frequency (RF) magnetic field excitation.","Our micromagnetic simulation identifies the splitting of the resonant peak as an outcome of vortex polarity reversal under substantial RF amplitudes.","This study enhances our understanding of nonlinear magnetic vortex dynamics amidst large RF amplitudes and proposes a potential pathway for spintronic neural computing thanks to their unique and controllable magnetization dynamics."],"url":"http://arxiv.org/abs/2403.06491v1","category":"physics.app-ph"}
{"created":"2024-03-11 07:50:29","title":"Multilingual Turn-taking Prediction Using Voice Activity Projection","abstract":"This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrastive predictive coding (CPC) pre-trained on English, with a recent model based on multilingual wav2vec 2.0 (MMS).","sentences":["This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese.","The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants.","The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages.","However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages.","Further analyses show that the multilingual model has learned to discern the language of the input signal.","We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking.","Finally, we compare two different audio encoders, contrastive predictive coding (CPC) pre-trained on English, with a recent model based on multilingual wav2vec 2.0 (MMS)."],"url":"http://arxiv.org/abs/2403.06487v1","category":"cs.CL"}
{"created":"2024-03-11 07:44:56","title":"Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning","abstract":"User financial default prediction plays a critical role in credit risk forecasting and management. It aims at predicting the probability that the user will fail to make the repayments in the future. Previous methods mainly extract a set of user individual features regarding his own profiles and behaviors and build a binary-classification model to make default predictions. However, these methods cannot get satisfied results, especially for users with limited information. Although recent efforts suggest that default prediction can be improved by social relations, they fail to capture the higher-order topology structure at the level of small subgraph patterns. In this paper, we fill in this gap by proposing a motif-preserving Graph Neural Network with curriculum learning (MotifGNN) to jointly learn the lower-order structures from the original graph and higherorder structures from multi-view motif-based graphs for financial default prediction. Specifically, to solve the problem of weak connectivity in motif-based graphs, we design the motif-based gating mechanism. It utilizes the information learned from the original graph with good connectivity to strengthen the learning of the higher-order structure. And considering that the motif patterns of different samples are highly unbalanced, we propose a curriculum learning mechanism on the whole learning process to more focus on the samples with uncommon motif distributions. Extensive experiments on one public dataset and two industrial datasets all demonstrate the effectiveness of our proposed method.","sentences":["User financial default prediction plays a critical role in credit risk forecasting and management.","It aims at predicting the probability that the user will fail to make the repayments in the future.","Previous methods mainly extract a set of user individual features regarding his own profiles and behaviors and build a binary-classification model to make default predictions.","However, these methods cannot get satisfied results, especially for users with limited information.","Although recent efforts suggest that default prediction can be improved by social relations, they fail to capture the higher-order topology structure at the level of small subgraph patterns.","In this paper, we fill in this gap by proposing a motif-preserving Graph Neural Network with curriculum learning (MotifGNN) to jointly learn the lower-order structures from the original graph and higherorder structures from multi-view motif-based graphs for financial default prediction.","Specifically, to solve the problem of weak connectivity in motif-based graphs, we design the motif-based gating mechanism.","It utilizes the information learned from the original graph with good connectivity to strengthen the learning of the higher-order structure.","And considering that the motif patterns of different samples are highly unbalanced, we propose a curriculum learning mechanism on the whole learning process to more focus on the samples with uncommon motif distributions.","Extensive experiments on one public dataset and two industrial datasets all demonstrate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2403.06482v1","category":"q-fin.RM"}
{"created":"2024-03-11 07:28:37","title":"Non-Intrusive Load Monitoring in Smart Grids: A Comprehensive Review","abstract":"Non-Intrusive Load Monitoring (NILM) is pivotal in today's energy landscape, offering vital solutions for energy conservation and efficient management. Its growing importance in enhancing energy savings and understanding consumer behavior makes it a pivotal technology for addressing global energy challenges. This paper delivers an in-depth review of NILM, highlighting its critical role in smart homes and smart grids. The significant contributions of this study are threefold: Firstly, it compiles a comprehensive global dataset table, providing a valuable tool for researchers and engineers to select appropriate datasets for their NILM studies. Secondly, it categorizes NILM approaches, simplifying the understanding of various algorithms by focusing on technologies, label data requirements, feature usage, and monitoring states. Lastly, by identifying gaps in current NILM research, this work sets a clear direction for future studies, discussing potential areas of innovation.","sentences":["Non-Intrusive Load Monitoring (NILM) is pivotal in today's energy landscape, offering vital solutions for energy conservation and efficient management.","Its growing importance in enhancing energy savings and understanding consumer behavior makes it a pivotal technology for addressing global energy challenges.","This paper delivers an in-depth review of NILM, highlighting its critical role in smart homes and smart grids.","The significant contributions of this study are threefold:","Firstly, it compiles a comprehensive global dataset table, providing a valuable tool for researchers and engineers to select appropriate datasets for their NILM studies.","Secondly, it categorizes NILM approaches, simplifying the understanding of various algorithms by focusing on technologies, label data requirements, feature usage, and monitoring states.","Lastly, by identifying gaps in current NILM research, this work sets a clear direction for future studies, discussing potential areas of innovation."],"url":"http://arxiv.org/abs/2403.06474v1","category":"eess.SP"}
{"created":"2024-03-11 07:21:45","title":"What is a proper definition of spin current? -- Lessons from the Kane-Mele Model","abstract":"Spin current, a key concept in spintronics that carries spin angular momentum, has a non-unique definition due to the non-conservation of spins in solids. While two primary definitions exist -- conventional spin current and conserved spin current -- their validity has not been quantitatively examined. Here, we examine the validity of these definitions of spin current by comparing their spin Hall conductivities to the spin accumulation on edges of materials calculated in a real-time evolution simulation. Employing the Kane-Mele model with the Rashba term, which explicitly violates spin conservation, we reveal that the spin Hall conductivities calculated under both definitions fail to reproduce the simulated results of spin accumulation when the Rashba term is large. Our results suggest that the standard definitions of spin current and the associated spin Hall conductivity do not give an accurate quantitative estimate of spin accumulation. This conclusion indicates that real-time simulations are necessary to accurately estimate spin accumulation on edges/surfaces of materials.","sentences":["Spin current, a key concept in spintronics that carries spin angular momentum, has a non-unique definition due to the non-conservation of spins in solids.","While two primary definitions exist -- conventional spin current and conserved spin current -- their validity has not been quantitatively examined.","Here, we examine the validity of these definitions of spin current by comparing their spin Hall conductivities to the spin accumulation on edges of materials calculated in a real-time evolution simulation.","Employing the Kane-Mele model with the Rashba term, which explicitly violates spin conservation, we reveal that the spin Hall conductivities calculated under both definitions fail to reproduce the simulated results of spin accumulation when the Rashba term is large.","Our results suggest that the standard definitions of spin current and the associated spin Hall conductivity do not give an accurate quantitative estimate of spin accumulation.","This conclusion indicates that real-time simulations are necessary to accurately estimate spin accumulation on edges/surfaces of materials."],"url":"http://arxiv.org/abs/2403.06472v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 07:10:13","title":"Nonleptonic three-body charmed baryon weak decays with H(15)","abstract":"We study the nonleptonic three-body charmed baryon weak decays of $\\mathbf{B}_{c}\\rightarrow\\mathbf{B}_{n}PP^{\\prime}$ under the $SU(3)_{F}$ flavor symmetry, where $\\mathbf{B}_{c}$ denotes the anti-triplet charmed baryon, comprising $(\\Xi^{0}_{c},-\\Xi^{+}_{c},\\Lambda^{+}_{c})$, and $\\mathbf{B}_{n}$ and $P(P^{\\prime})$ represent octet baryon and pseudoscalar meson states, respectively. In addition to 12 parameters from the contributions of the color-antisymmetric part of the effective Hamiltonian, denoted as $H(\\bar{\\mathbf{6}})$, there are 4 parameters from the color-symmetric one, $H(\\mathbf{15})$, which were not included in the previous study. With 16 parameters in total and 28 experimental data points, we obtain the minimal $\\chi^2$ over degree of freedom of $\\chi^{2}/d.o.f=1.5$, which is a great improvement comparing to that without $H(\\mathbf{15})$. With the better fitting values, we evaluate the branching ratios and up-down asymmetries of $\\mathbf{B}_{c}\\rightarrow\\mathbf{B}_{n}PP^{\\prime}$, which present some interesting results such as $\\mathcal{B}\\,(\\Lambda^{+}_{c}\\rightarrow(\\Xi(1690)^{0}\\rightarrow\\Sigma^{+}K^{-})\\,K^{+})\\equiv(1.5\\pm0.4)\\times10^{-3}$ and potential $SU(3)$ breaking effects in $\\Xi^{+}_{c}\\rightarrow p\\pi^{+}K^{-}$ and $\\Lambda^{+}_{c}\\rightarrow \\Sigma^{+}\\pi^{-}K^{+}$ to be verified by the experiments at BESIII, Belle-II and LHCb.","sentences":["We study the nonleptonic three-body charmed","baryon weak decays of $\\mathbf{B}_{c}\\rightarrow\\mathbf{B}_{n}PP^{\\prime}$ under the $SU(3)_{F}$ flavor symmetry, where $\\mathbf{B}_{c}$ denotes the anti-triplet charmed baryon, comprising $(\\Xi^{0}_{c},-\\Xi^{+}_{c},\\Lambda^{+}_{c})$, and $\\mathbf{B}_{n}$ and $P(P^{\\prime})$ represent octet baryon and pseudoscalar meson states, respectively.","In addition to 12 parameters from the contributions of the color-antisymmetric part of the effective Hamiltonian, denoted as $H(\\bar{\\mathbf{6}})$, there are 4 parameters from the color-symmetric one, $H(\\mathbf{15})$, which were not included in the previous study.","With 16 parameters in total and 28 experimental data points, we obtain the minimal $\\chi^2$ over degree of freedom of $\\chi^{2}/d.o.f=1.5$, which is a great improvement comparing to that without $H(\\mathbf{15})$. With the better fitting values, we evaluate the branching ratios and up-down asymmetries of $\\mathbf{B}_{c}\\rightarrow\\mathbf{B}_{n}PP^{\\prime}$, which present some interesting results such as $\\mathcal{B}\\,(\\Lambda^{+}_{c}\\rightarrow(\\Xi(1690)^{0}\\rightarrow\\Sigma^{+}K^{-})\\,K^{+})\\equiv(1.5\\pm0.4)\\times10^{-3}$ and potential $SU(3)$ breaking effects in $\\Xi^{+}_{c}\\rightarrow p\\pi^{+}K^{-}$ and $\\Lambda^{+}_{c}\\rightarrow \\Sigma^{+}\\pi^{-}K^{+}$ to be verified by the experiments at BESIII, Belle-II and LHCb."],"url":"http://arxiv.org/abs/2403.06469v1","category":"hep-ph"}
{"created":"2024-03-11 06:59:05","title":"Towards the Uncharted: Density-Descending Feature Perturbation for Semi-supervised Semantic Segmentation","abstract":"Semi-supervised semantic segmentation allows model to mine effective supervision from unlabeled data to complement label-guided training. Recent research has primarily focused on consistency regularization techniques, exploring perturbation-invariant training at both the image and feature levels. In this work, we proposed a novel feature-level consistency learning framework named Density-Descending Feature Perturbation (DDFP). Inspired by the low-density separation assumption in semi-supervised learning, our key insight is that feature density can shed a light on the most promising direction for the segmentation classifier to explore, which is the regions with lower density. We propose to shift features with confident predictions towards lower-density regions by perturbation injection. The perturbed features are then supervised by the predictions on the original features, thereby compelling the classifier to explore less dense regions to effectively regularize the decision boundary. Central to our method is the estimation of feature density. To this end, we introduce a lightweight density estimator based on normalizing flow, allowing for efficient capture of the feature density distribution in an online manner. By extracting gradients from the density estimator, we can determine the direction towards less dense regions for each feature. The proposed DDFP outperforms other designs on feature-level perturbations and shows state of the art performances on both Pascal VOC and Cityscapes dataset under various partition protocols. The project is available at https://github.com/Gavinwxy/DDFP.","sentences":["Semi-supervised semantic segmentation allows model to mine effective supervision from unlabeled data to complement label-guided training.","Recent research has primarily focused on consistency regularization techniques, exploring perturbation-invariant training at both the image and feature levels.","In this work, we proposed a novel feature-level consistency learning framework named Density-Descending Feature Perturbation (DDFP).","Inspired by the low-density separation assumption in semi-supervised learning, our key insight is that feature density can shed a light on the most promising direction for the segmentation classifier to explore, which is the regions with lower density.","We propose to shift features with confident predictions towards lower-density regions by perturbation injection.","The perturbed features are then supervised by the predictions on the original features, thereby compelling the classifier to explore less dense regions to effectively regularize the decision boundary.","Central to our method is the estimation of feature density.","To this end, we introduce a lightweight density estimator based on normalizing flow, allowing for efficient capture of the feature density distribution in an online manner.","By extracting gradients from the density estimator, we can determine the direction towards less dense regions for each feature.","The proposed DDFP outperforms other designs on feature-level perturbations and shows state of the art performances on both Pascal VOC and Cityscapes dataset under various partition protocols.","The project is available at https://github.com/Gavinwxy/DDFP."],"url":"http://arxiv.org/abs/2403.06462v1","category":"cs.CV"}
{"created":"2024-03-11 05:09:44","title":"Designing a K-state P-bit Engine","abstract":"Probabilistic bit (p-bit)-based compute engines utilize the unique capability of a p-bit to probabilistically switch between two states to solve computationally challenging problems. However, when solving problems that require more than two states (e.g., problems such as Max-3-Cut, verifying if a graph is K-partite (K>2) etc.), additional pre-processing steps such as graph reduction are required to make the problem compatible with a two-state p-bit platform. Moreover, this not only increases the problem size by entailing the use of auxiliary variables but can also degrade the solution quality. In this work, we develop a unique framework for implementing a K-state (K>2) p-bit engine. Furthermore, from an implementation standpoint, we show that such a K-state p-bit engine can be implemented using N traditional (2-state) p-bits, and one multi-state p-bit -- a novel concept proposed here. Augmenting traditional p-bit platforms, our approach enables us to solve an archetypal combinatoric problem class requiring multiple states, namely Max-K-Cut (K=3, 4 shown here), without using any additional auxiliary variables. Thus, our work fundamentally advances the functional capability of p-bit engines, enabling them to solve a broader class of computationally challenging problems more efficiently.","sentences":["Probabilistic bit (p-bit)-based compute engines utilize the unique capability of a p-bit to probabilistically switch between two states to solve computationally challenging problems.","However, when solving problems that require more than two states (e.g., problems such as Max-3-Cut, verifying if a graph is K-partite (K>2) etc.), additional pre-processing steps such as graph reduction are required to make the problem compatible with a two-state p-bit platform.","Moreover, this not only increases the problem size by entailing the use of auxiliary variables but can also degrade the solution quality.","In this work, we develop a unique framework for implementing a K-state (K>2) p-bit engine.","Furthermore, from an implementation standpoint, we show that such a K-state p-bit engine can be implemented using N traditional (2-state) p-bits, and one multi-state p-bit -- a novel concept proposed here.","Augmenting traditional p-bit platforms, our approach enables us to solve an archetypal combinatoric problem class requiring multiple states, namely Max-K-Cut (K=3, 4 shown here), without using any additional auxiliary variables.","Thus, our work fundamentally advances the functional capability of p-bit engines, enabling them to solve a broader class of computationally challenging problems more efficiently."],"url":"http://arxiv.org/abs/2403.06436v1","category":"cs.ET"}
{"created":"2024-03-11 04:49:41","title":"Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain","abstract":"Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting. Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations. Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructing dynamic graphs, which enables the learning of higher-level semantic representations considering temporal perspectives, addressing the challenges in fMRI data representation learning. Utilizing the large-scale UK Biobank dataset for self-supervised learning, ST-JEMA shows exceptional representation learning performance on dynamic functional connectivity demonstrating superiority over previous methods in predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI datasets even with limited samples and effectiveness of temporal reconstruction on missing data scenarios. These findings highlight the potential of our approach as a robust representation learning method for leveraging label-scarce fMRI data.","sentences":["Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks.","However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult.","Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting.","Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations.","Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision.","ST-JEMA employs a JEPA-inspired strategy for reconstructing dynamic graphs, which enables the learning of higher-level semantic representations considering temporal perspectives, addressing the challenges in fMRI data representation learning.","Utilizing the large-scale UK Biobank dataset for self-supervised learning, ST-JEMA shows exceptional representation learning performance on dynamic functional connectivity demonstrating superiority over previous methods in predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI datasets even with limited samples and effectiveness of temporal reconstruction on missing data scenarios.","These findings highlight the potential of our approach as a robust representation learning method for leveraging label-scarce fMRI data."],"url":"http://arxiv.org/abs/2403.06432v1","category":"cs.LG"}
{"created":"2024-03-11 04:34:42","title":"Intra-Section Code Cave Injection for Adversarial Evasion Attacks on Windows PE Malware File","abstract":"Windows malware is predominantly available in cyberspace and is a prime target for deliberate adversarial evasion attacks. Although researchers have investigated the adversarial malware attack problem, a multitude of important questions remain unanswered, including (a) Are the existing techniques to inject adversarial perturbations in Windows Portable Executable (PE) malware files effective enough for evasion purposes?; (b) Does the attack process preserve the original behavior of malware?; (c) Are there unexplored approaches/locations that can be used to carry out adversarial evasion attacks on Windows PE malware?; and (d) What are the optimal locations and sizes of adversarial perturbations required to evade an ML-based malware detector without significant structural change in the PE file? To answer some of these questions, this work proposes a novel approach that injects a code cave within the section (i.e., intra-section) of Windows PE malware files to make space for adversarial perturbations. In addition, a code loader is also injected inside the PE file, which reverts adversarial malware to its original form during the execution, preserving the malware's functionality and executability. To understand the effectiveness of our approach, we injected adversarial perturbations inside the .text, .data and .rdata sections, generated using the gradient descent and Fast Gradient Sign Method (FGSM), to target the two popular CNN-based malware detectors, MalConv and MalConv2. Our experiments yielded notable results, achieving a 92.31% evasion rate with gradient descent and 96.26% with FGSM against MalConv, compared to the 16.17% evasion rate for append attacks. Similarly, when targeting MalConv2, our approach achieved a remarkable maximum evasion rate of 97.93% with gradient descent and 94.34% with FGSM, significantly surpassing the 4.01% evasion rate observed with append attacks.","sentences":["Windows malware is predominantly available in cyberspace and is a prime target for deliberate adversarial evasion attacks.","Although researchers have investigated the adversarial malware attack problem, a multitude of important questions remain unanswered, including (a) Are the existing techniques to inject adversarial perturbations in Windows Portable Executable (PE) malware files effective enough for evasion purposes?; (b) Does the attack process preserve the original behavior of malware?; (c) Are there unexplored approaches/locations that can be used to carry out adversarial evasion attacks on Windows PE malware?; and (d) What are the optimal locations and sizes of adversarial perturbations required to evade an ML-based malware detector without significant structural change in the PE file?","To answer some of these questions, this work proposes a novel approach that injects a code cave within the section (i.e., intra-section) of Windows PE malware files to make space for adversarial perturbations.","In addition, a code loader is also injected inside the PE file, which reverts adversarial malware to its original form during the execution, preserving the malware's functionality and executability.","To understand the effectiveness of our approach, we injected adversarial perturbations inside the .text, .data and .rdata sections, generated using the gradient descent and Fast Gradient Sign Method (FGSM), to target the two popular CNN-based malware detectors, MalConv and MalConv2.","Our experiments yielded notable results, achieving a 92.31% evasion rate with gradient descent and 96.26% with FGSM against MalConv, compared to the 16.17% evasion rate for append attacks.","Similarly, when targeting MalConv2, our approach achieved a remarkable maximum evasion rate of 97.93% with gradient descent and 94.34% with FGSM, significantly surpassing the 4.01% evasion rate observed with append attacks."],"url":"http://arxiv.org/abs/2403.06428v1","category":"cs.CR"}
{"created":"2024-03-11 04:26:55","title":"Formation of artificial Fermi surfaces with a triangular superlattice on a conventional two dimensional electron gas","abstract":"In nearly free electron theory the imposition of a periodic electrostatic potential on free electrons creates the bandstructure of a material, determined by the crystal lattice spacing and geometry. Imposing an artificially designed potential to the electrons confined in a GaAs quantum well makes it possible to engineer synthetic two-dimensional band structures, with electronic properties different from those in the host semiconductor. Here we report the fabrication and study of a tuneable triangular artificial lattice on a GaAs/AlGaAs heterostructure where it is possible to transform from the original GaAs bandstructure and Fermi surface to a new bandstructure with multiple artificial Fermi surfaces simply by altering a gate bias. For weak electrostatic potential modulation magnetotransport measurements reveal quantum oscillations from the GaAs two-dimensional Fermi surface, and classical oscillations due to these electrons scattering from the artificial lattice. Increasing the strength of the modulation reveals new quantum oscillations due to the formation of multiple artificial Fermi surfaces, and ultimately to new classical oscillations of the electrons from the artificial Fermi surface scattering from the superlattice modulation. These results show that low disorder gate-tuneable lateral superlattices can be used to form artificial two dimensional crystals with designer electronic properties.","sentences":["In nearly free electron theory the imposition of a periodic electrostatic potential on free electrons creates the bandstructure of a material, determined by the crystal lattice spacing and geometry.","Imposing an artificially designed potential to the electrons confined in a GaAs quantum well makes it possible to engineer synthetic two-dimensional band structures, with electronic properties different from those in the host semiconductor.","Here we report the fabrication and study of a tuneable triangular artificial lattice on a GaAs/AlGaAs heterostructure where it is possible to transform from the original GaAs bandstructure and Fermi surface to a new bandstructure with multiple artificial Fermi surfaces simply by altering a gate bias.","For weak electrostatic potential modulation magnetotransport measurements reveal quantum oscillations from the GaAs two-dimensional Fermi surface, and classical oscillations due to these electrons scattering from the artificial lattice.","Increasing the strength of the modulation reveals new quantum oscillations due to the formation of multiple artificial Fermi surfaces, and ultimately to new classical oscillations of the electrons from the artificial Fermi surface scattering from the superlattice modulation.","These results show that low disorder gate-tuneable lateral superlattices can be used to form artificial two dimensional crystals with designer electronic properties."],"url":"http://arxiv.org/abs/2403.06426v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 04:25:41","title":"Bridging Domains with Approximately Shared Features","abstract":"Multi-source domain adaptation aims to reduce performance degradation when applying machine learning models to unseen domains. A fundamental challenge is devising the optimal strategy for feature selection. Existing literature is somewhat paradoxical: some advocate for learning invariant features from source domains, while others favor more diverse features. To address the challenge, we propose a statistical framework that distinguishes the utilities of features based on the variance of their correlation to label $y$ across domains. Under our framework, we design and analyze a learning procedure consisting of learning approximately shared feature representation from source tasks and fine-tuning it on the target task. Our theoretical analysis necessitates the importance of learning approximately shared features instead of only the strictly invariant features and yields an improved population risk compared to previous results on both source and target tasks, thus partly resolving the paradox mentioned above. Inspired by our theory, we proposed a more practical way to isolate the content (invariant+approximately shared) from environmental features and further consolidate our theoretical findings.","sentences":["Multi-source domain adaptation aims to reduce performance degradation when applying machine learning models to unseen domains.","A fundamental challenge is devising the optimal strategy for feature selection.","Existing literature is somewhat paradoxical: some advocate for learning invariant features from source domains, while others favor more diverse features.","To address the challenge, we propose a statistical framework that distinguishes the utilities of features based on the variance of their correlation to label $y$ across domains.","Under our framework, we design and analyze a learning procedure consisting of learning approximately shared feature representation from source tasks and fine-tuning it on the target task.","Our theoretical analysis necessitates the importance of learning approximately shared features instead of only the strictly invariant features and yields an improved population risk compared to previous results on both source and target tasks, thus partly resolving the paradox mentioned above.","Inspired by our theory, we proposed a more practical way to isolate the content (invariant+approximately shared) from environmental features and further consolidate our theoretical findings."],"url":"http://arxiv.org/abs/2403.06424v1","category":"stat.ML"}
{"created":"2024-03-11 04:05:17","title":"Enhanced Sparsification via Stimulative Training","abstract":"Sparsification-based pruning has been an important category in model compression. Existing methods commonly set sparsity-inducing penalty terms to suppress the importance of dropped weights, which is regarded as the suppressed sparsification paradigm. However, this paradigm inactivates the dropped parts of networks causing capacity damage before pruning, thereby leading to performance degradation. To alleviate this issue, we first study and reveal the relative sparsity effect in emerging stimulative training and then propose a structured pruning framework, named STP, based on an enhanced sparsification paradigm which maintains the magnitude of dropped weights and enhances the expressivity of kept weights by self-distillation. Besides, to find an optimal architecture for the pruned network, we propose a multi-dimension architecture space and a knowledge distillation-guided exploration strategy. To reduce the huge capacity gap of distillation, we propose a subnet mutating expansion technique. Extensive experiments on various benchmarks indicate the effectiveness of STP. Specifically, without fine-tuning, our method consistently achieves superior performance at different budgets, especially under extremely aggressive pruning scenarios, e.g., remaining 95.11% Top-1 accuracy (72.43% in 76.15%) while reducing 85% FLOPs for ResNet-50 on ImageNet. Codes will be released soon.","sentences":["Sparsification-based pruning has been an important category in model compression.","Existing methods commonly set sparsity-inducing penalty terms to suppress the importance of dropped weights, which is regarded as the suppressed sparsification paradigm.","However, this paradigm inactivates the dropped parts of networks causing capacity damage before pruning, thereby leading to performance degradation.","To alleviate this issue, we first study and reveal the relative sparsity effect in emerging stimulative training and then propose a structured pruning framework, named STP, based on an enhanced sparsification paradigm which maintains the magnitude of dropped weights and enhances the expressivity of kept weights by self-distillation.","Besides, to find an optimal architecture for the pruned network, we propose a multi-dimension architecture space and a knowledge distillation-guided exploration strategy.","To reduce the huge capacity gap of distillation, we propose a subnet mutating expansion technique.","Extensive experiments on various benchmarks indicate the effectiveness of STP.","Specifically, without fine-tuning, our method consistently achieves superior performance at different budgets, especially under extremely aggressive pruning scenarios, e.g., remaining 95.11% Top-1 accuracy (72.43% in 76.15%) while reducing 85% FLOPs for ResNet-50 on ImageNet.","Codes will be released soon."],"url":"http://arxiv.org/abs/2403.06417v1","category":"cs.CV"}
{"created":"2024-03-11 03:31:35","title":"Cosine Scoring with Uncertainty for Neural Speaker Embedding","abstract":"Uncertainty modeling in speaker representation aims to learn the variability present in speech utterances. While the conventional cosine-scoring is computationally efficient and prevalent in speaker recognition, it lacks the capability to handle uncertainty. To address this challenge, this paper proposes an approach for estimating uncertainty at the speaker embedding front-end and propagating it to the cosine scoring back-end. Experiments conducted on the VoxCeleb and SITW datasets confirmed the efficacy of the proposed method in handling uncertainty arising from embedding estimation. It achieved improvement with 8.5% and 9.8% average reductions in EER and minDCF compared to the conventional cosine similarity. It is also computationally efficient in practice.","sentences":["Uncertainty modeling in speaker representation aims to learn the variability present in speech utterances.","While the conventional cosine-scoring is computationally efficient and prevalent in speaker recognition, it lacks the capability to handle uncertainty.","To address this challenge, this paper proposes an approach for estimating uncertainty at the speaker embedding front-end and propagating it to the cosine scoring back-end.","Experiments conducted on the VoxCeleb and SITW datasets confirmed the efficacy of the proposed method in handling uncertainty arising from embedding estimation.","It achieved improvement with 8.5% and 9.8% average reductions in EER and minDCF compared to the conventional cosine similarity.","It is also computationally efficient in practice."],"url":"http://arxiv.org/abs/2403.06404v1","category":"cs.SD"}
{"created":"2024-03-11 03:28:20","title":"PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models","abstract":"Recent success of vision foundation models have shown promising performance for the 2D perception tasks. However, it is difficult to train a 3D foundation network directly due to the limited dataset and it remains under explored whether existing foundation models can be lifted to 3D space seamlessly. In this paper, we present PointSeg, a novel training-free paradigm that leverages off-the-shelf vision foundation models to address 3D scene perception tasks. PointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to align their corresponding pixels across frames. Concretely, we design a two-branch prompts learning structure to construct the 3D point-box prompts pairs, combining with the bidirectional matching strategy for accurate point and proposal prompts generation. Then, we perform the iterative post-refinement adaptively when cooperated with different vision foundation models. Moreover, we design a affinity-aware merging algorithm to improve the final ensemble masks. PointSeg demonstrates impressive segmentation performance across various datasets, all without training. Specifically, our approach significantly surpasses the state-of-the-art specialist model by 13.4$\\%$, 11.3$\\%$, and 12$\\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets, respectively. On top of that, PointSeg can incorporate with various segmentation models and even surpasses the supervised methods.","sentences":["Recent success of vision foundation models have shown promising performance for the 2D perception tasks.","However, it is difficult to train a 3D foundation network directly due to the limited dataset and it remains under explored whether existing foundation models can be lifted to 3D space seamlessly.","In this paper, we present PointSeg, a novel training-free paradigm that leverages off-the-shelf vision foundation models to address 3D scene perception tasks.","PointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to align their corresponding pixels across frames.","Concretely, we design a two-branch prompts learning structure to construct the 3D point-box prompts pairs, combining with the bidirectional matching strategy for accurate point and proposal prompts generation.","Then, we perform the iterative post-refinement adaptively when cooperated with different vision foundation models.","Moreover, we design a affinity-aware merging algorithm to improve the final ensemble masks.","PointSeg demonstrates impressive segmentation performance across various datasets, all without training.","Specifically, our approach significantly surpasses the state-of-the-art specialist model by 13.4$\\%$, 11.3$\\%$, and 12$\\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets, respectively.","On top of that, PointSeg can incorporate with various segmentation models and even surpasses the supervised methods."],"url":"http://arxiv.org/abs/2403.06403v1","category":"cs.CV"}
{"created":"2024-03-11 03:05:05","title":"A Segmentation Foundation Model for Diverse-type Tumors","abstract":"Large pre-trained models with their numerous model parameters and extensive training datasets have shown excellent performance in various tasks. Many publicly available medical image datasets do not have a sufficient amount of data so there are few large-scale models in medical imaging. We propose a large-scale Tumor Segmentation Foundation Model (TSFM) with 1.6 billion parameters using Resblock-backbone and Transformer-bottleneck,which has good transfer ability for downstream tasks. To make TSFM exhibit good performance in tumor segmentation, we make full use of the strong spatial correlation between tumors and organs in the medical image, innovatively fuse 7 tumor datasets and 3 multi-organ datasets to build a 3D medical dataset pool, including 2779 cases with totally 300k medical images, whose size currently exceeds many other single publicly available datasets. TSFM is the pre-trained model for medical image segmentation, which also can be transferred to multiple downstream tasks for fine-tuning learning. The average performance of our pre-trained model is 2% higher than that of nnU-Net across various tumor types. In the transfer learning task, TSFM only needs 5% training epochs of nnU-Net to achieve similar performance and can surpass nnU-Net by 2% on average with 10% training epoch. Pre-trained TSFM and its code will be released soon.","sentences":["Large pre-trained models with their numerous model parameters and extensive training datasets have shown excellent performance in various tasks.","Many publicly available medical image datasets do not have a sufficient amount of data so there are few large-scale models in medical imaging.","We propose a large-scale Tumor Segmentation Foundation Model (TSFM) with 1.6 billion parameters using Resblock-backbone and Transformer-bottleneck,which has good transfer ability for downstream tasks.","To make TSFM exhibit good performance in tumor segmentation, we make full use of the strong spatial correlation between tumors and organs in the medical image, innovatively fuse 7 tumor datasets and 3 multi-organ datasets to build a 3D medical dataset pool, including 2779 cases with totally 300k medical images, whose size currently exceeds many other single publicly available datasets.","TSFM is the pre-trained model for medical image segmentation, which also can be transferred to multiple downstream tasks for fine-tuning learning.","The average performance of our pre-trained model is 2% higher than that of nnU-Net across various tumor types.","In the transfer learning task, TSFM only needs 5% training epochs of nnU-Net to achieve similar performance and can surpass nnU-Net by 2% on average with 10% training epoch.","Pre-trained TSFM and its code will be released soon."],"url":"http://arxiv.org/abs/2403.06396v1","category":"eess.IV"}
{"created":"2024-03-11 02:57:08","title":"Towards verifications of Krylov complexity","abstract":"Krylov complexity is considered to provide a measure of the growth of operators evolving under Hamiltonian dynamics. The main strategy is the analysis of the structure of Krylov subspace $\\mathcal{K}_M(\\mathcal{H},\\eta)$ spanned by the multiple applications of the Liouville operator $\\mathcal{L}$ defined by the commutator in terms of a Hamiltonian $\\mathcal{H}$, $\\mathcal{L}:=[\\mathcal{H},\\cdot]$ acting on an operator $\\eta$, $\\mathcal{K}_M(\\mathcal{H},\\eta)=\\text{span}\\{\\eta,\\mathcal{L}\\eta,\\ldots,\\mathcal{L}^{M-1}\\eta\\}$. For a given inner product $(\\cdot,\\cdot)$ of the operators, the orthonormal basis $\\{\\mathcal{O}_n\\}$ is constructed from $\\mathcal{O}_0=\\eta/\\sqrt{(\\eta,\\eta)}$ by Lanczos algorithm. The moments $\\mu_m=(\\mathcal{O}_0,\\mathcal{L}^m\\mathcal{O}_0)$ are closely related to the important data $\\{b_n\\}$ called Lanczos coefficients. I present the exact and explicit expressions of the moments $\\{\\mu_m\\}$ for 16 quantum mechanical systems which are {\\em exactly solvable both in the Schr\\\"odinger and Heisenberg pictures}. The operator $\\eta$ is the sinusoidal coordinate which is the variable of the eigenpolynomials $\\{P_n(\\eta)\\}$ of the exactly solvable Hamiltonian $\\mathcal{H}$.","sentences":["Krylov complexity is considered to provide a measure of the growth of operators evolving under Hamiltonian dynamics.","The main strategy is the analysis of the structure of Krylov subspace $\\mathcal{K}_M(\\mathcal{H},\\eta)$ spanned by the multiple applications of the Liouville operator $\\mathcal{L}$ defined by the commutator in terms of a Hamiltonian $\\mathcal{H}$, $\\mathcal{L}:=[\\mathcal{H},\\cdot]$ acting on an operator $\\eta$, $\\mathcal{K}_M(\\mathcal{H},\\eta)=\\text{span}\\{\\eta,\\mathcal{L}\\eta,\\ldots,\\mathcal{L}^{M-1}\\eta\\}$. For a given inner product $(\\cdot,\\cdot)$ of the operators, the orthonormal basis $\\{\\mathcal{O}_n\\}$ is constructed from $\\mathcal{O}_0=\\eta/\\sqrt{(\\eta,\\eta)}$ by Lanczos algorithm.","The moments $\\mu_m=(\\mathcal{O}_0,\\mathcal{L}^m\\mathcal{O}_0)$ are closely related to the important data $\\{b_n\\}$ called Lanczos coefficients.","I present the exact and explicit expressions of the moments $\\{\\mu_m\\}$ for 16 quantum mechanical systems which are {\\em exactly solvable both in the Schr\\\"odinger and Heisenberg pictures}.","The operator $\\eta$ is the sinusoidal coordinate which is the variable of the eigenpolynomials $\\{P_n(\\eta)\\}$ of the exactly solvable Hamiltonian $\\mathcal{H}$."],"url":"http://arxiv.org/abs/2403.06391v1","category":"quant-ph"}
{"created":"2024-03-11 02:09:53","title":"First-principles study of two-dimensional transition metal carbide M n+1 C n O 2(M=Nb,Ta)","abstract":"In the present work, the three stable MXenes M n+1 C n O 2 (M=Nb,Ta) are explored based onfirst-principles calculations. These materials are important derivatives of 2D materials and exhib-it distinctive properties, holding vast potential in nanodevices. All these M n+1 C n O 2 (M=Nb,Ta)materials exhibit outstanding superconducting performance, with corresponding superconductingtransition temperatures of 23.00K, 25.00K, and 29.00K. Analysis reveals that the high supercon-ducting transition temperatures of MXenes M n+1 C n O 2 (M=Nb,Ta) are closely associated with thehigh value of the logarithmic average of phonon frequencies, {\\omega} log , and the strong electron-phononcoupling (EPC), attributed to the crucial contribution of low-frequency phonons. Additionally, weapplied strain treatments of 2% and 4% to M n+1 C n O 2 (M=Nb,Ta), resulting in varying changes insuperconducting transition temperatures under different strains.","sentences":["In the present work, the three stable MXenes M n+1 C n","O 2 (M=Nb,Ta) are explored based onfirst-principles calculations.","These materials are important derivatives of 2D materials and exhib-it distinctive properties, holding vast potential in nanodevices.","All these M n+1 C n O 2 (M=Nb,Ta)materials exhibit outstanding superconducting performance, with corresponding superconductingtransition temperatures of 23.00K, 25.00K, and 29.00K. Analysis reveals that the high supercon-ducting transition temperatures of MXenes M n+1 C","n O 2 (M=Nb,Ta) are closely associated with thehigh value of the logarithmic average of phonon frequencies, {\\omega} log , and the strong electron-phononcoupling (EPC), attributed to the crucial contribution of low-frequency phonons.","Additionally, weapplied strain treatments of 2% and 4% to M n+1 C n O 2 (M=Nb,Ta), resulting in varying changes insuperconducting transition temperatures under different strains."],"url":"http://arxiv.org/abs/2403.06380v1","category":"cond-mat.supr-con"}
{"created":"2024-03-11 02:09:15","title":"First-principles Study of Carrier Mobility in MX (M=Sn, Pb; X=P, As) Monolayers","abstract":"Compounds from groups IV and V have been the focus of recent research due to their impressive physical characteristics and structural stability. In this study, the MX monolayers (M=Sn, Pb; N=P, As) are investigated with first-principles calculations based on Boltzmann transport theory. The results show that SnP, SnAs, and PbAs all exhibit indirect band gaps, whereas PbP is the only semiconductor with a direct band gap. One important finding is that intravalley scattering has a significant impact on electron-phonon coupling. Interestingly, changes in carrier concentration do not affect the electron mobility within these MX monolayers, with SnP exhibiting the highest electron mobility among them. Subsequently, the SnP under a 6% biaxial strain is further explored and the results demonstrated a considerable increase in electron mobility to 2,511.9 cm^2/Vs, which is attributable to decreased scattering. This suggests that MX monolayers, especially SnP, are promising options for 2D semiconductor materials in the future.","sentences":["Compounds from groups IV and V have been the focus of recent research due to their impressive physical characteristics and structural stability.","In this study, the MX monolayers (M=Sn, Pb; N=P, As) are investigated with first-principles calculations based on Boltzmann transport theory.","The results show that SnP, SnAs, and PbAs all exhibit indirect band gaps, whereas PbP is the only semiconductor with a direct band gap.","One important finding is that intravalley scattering has a significant impact on electron-phonon coupling.","Interestingly, changes in carrier concentration do not affect the electron mobility within these MX monolayers, with SnP exhibiting the highest electron mobility among them.","Subsequently, the SnP under a 6% biaxial strain is further explored and the results demonstrated a considerable increase in electron mobility to 2,511.9 cm^2/Vs, which is attributable to decreased scattering.","This suggests that MX monolayers, especially SnP, are promising options for 2D semiconductor materials in the future."],"url":"http://arxiv.org/abs/2403.06379v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-11 01:50:41","title":"Repeated Padding as Data Augmentation for Sequential Recommendation","abstract":"Sequential recommendation aims to provide users with personalized suggestions based on their historical interactions. When training sequential models, padding is a widely adopted technique for two main reasons: 1) The vast majority of models can only handle fixed-length sequences; 2) Batching-based training needs to ensure that the sequences in each batch have the same length. The special value \\emph{0} is usually used as the padding content, which does not contain the actual information and is ignored in the model calculations. This common-sense padding strategy leads us to a problem that has never been explored before: \\emph{Can we fully utilize this idle input space by padding other content to further improve model performance and training efficiency?}   In this paper, we propose a simple yet effective padding method called \\textbf{Rep}eated \\textbf{Pad}ding (\\textbf{RepPad}). Specifically, we use the original interaction sequences as the padding content and fill it to the padding positions during model training. This operation can be performed a finite number of times or repeated until the input sequences' length reaches the maximum limit. Our RepPad can be viewed as a sequence-level data augmentation strategy. Unlike most existing works, our method contains no trainable parameters or hyperparameters and is a plug-and-play data augmentation operation. Extensive experiments on various categories of sequential models and five real-world datasets demonstrate the effectiveness and efficiency of our approach. The average recommendation performance improvement is up to 60.3\\% on GRU4Rec and 24.3\\% on SASRec. We also provide in-depth analysis and explanation of what makes RepPad effective from multiple perspectives. The source code will be released to ensure the reproducibility of our experiments.","sentences":["Sequential recommendation aims to provide users with personalized suggestions based on their historical interactions.","When training sequential models, padding is a widely adopted technique for two main reasons: 1) The vast majority of models can only handle fixed-length sequences; 2) Batching-based training needs to ensure that the sequences in each batch have the same length.","The special value \\emph{0} is usually used as the padding content, which does not contain the actual information and is ignored in the model calculations.","This common-sense padding strategy leads us to a problem that has never been explored before: \\emph{Can we fully utilize this idle input space by padding other content to further improve model performance and training efficiency?}   ","In this paper, we propose a simple yet effective padding method called \\textbf{Rep}eated \\textbf{Pad}ding (\\textbf{RepPad}).","Specifically, we use the original interaction sequences as the padding content and fill it to the padding positions during model training.","This operation can be performed a finite number of times or repeated until the input sequences' length reaches the maximum limit.","Our RepPad can be viewed as a sequence-level data augmentation strategy.","Unlike most existing works, our method contains no trainable parameters or hyperparameters and is a plug-and-play data augmentation operation.","Extensive experiments on various categories of sequential models and five real-world datasets demonstrate the effectiveness and efficiency of our approach.","The average recommendation performance improvement is up to 60.3\\% on GRU4Rec and 24.3\\% on SASRec.","We also provide in-depth analysis and explanation of what makes RepPad effective from multiple perspectives.","The source code will be released to ensure the reproducibility of our experiments."],"url":"http://arxiv.org/abs/2403.06372v1","category":"cs.IR"}
{"created":"2024-03-11 01:44:14","title":"FeatAug: Automatic Feature Augmentation From One-to-Many Relationship Tables","abstract":"Feature augmentation from one-to-many relationship tables is a critical but challenging problem in ML model development. To augment good features, data scientists need to come up with SQL queries manually, which is time-consuming. Featuretools [1] is a widely used tool by the data science community to automatically augment the training data by extracting new features from relevant tables. It represents each feature as a group-by aggregation SQL query on relevant tables and can automatically generate these SQL queries. However, it does not include predicates in these queries, which significantly limits its application in many real-world scenarios. To overcome this limitation, we propose FEATAUG, a new feature augmentation framework that automatically extracts predicate-aware SQL queries from one-to-many relationship tables. This extension is not trivial because considering predicates will exponentially increase the number of candidate queries. As a result, the original Featuretools framework, which materializes all candidate queries, will not work and needs to be redesigned. We formally define the problem and model it as a hyperparameter optimization problem. We discuss how the Bayesian Optimization can be applied here and propose a novel warm-up strategy to optimize it. To make our algorithm more practical, we also study how to identify promising attribute combinations for predicates. We show that how the beam search idea can partially solve the problem and propose several techniques to further optimize it. Our experiments on four real-world datasets demonstrate that FeatAug extracts more effective features compared to Featuretools and other baselines. The code is open-sourced at https://github.com/sfu-db/FeatAug","sentences":["Feature augmentation from one-to-many relationship tables is a critical but challenging problem in ML model development.","To augment good features, data scientists need to come up with SQL queries manually, which is time-consuming.","Featuretools","[1] is a widely used tool by the data science community to automatically augment the training data by extracting new features from relevant tables.","It represents each feature as a group-by aggregation SQL query on relevant tables and can automatically generate these SQL queries.","However, it does not include predicates in these queries, which significantly limits its application in many real-world scenarios.","To overcome this limitation, we propose FEATAUG, a new feature augmentation framework that automatically extracts predicate-aware SQL queries from one-to-many relationship tables.","This extension is not trivial because considering predicates will exponentially increase the number of candidate queries.","As a result, the original Featuretools framework, which materializes all candidate queries, will not work and needs to be redesigned.","We formally define the problem and model it as a hyperparameter optimization problem.","We discuss how the Bayesian Optimization can be applied here and propose a novel warm-up strategy to optimize it.","To make our algorithm more practical, we also study how to identify promising attribute combinations for predicates.","We show that how the beam search idea can partially solve the problem and propose several techniques to further optimize it.","Our experiments on four real-world datasets demonstrate that FeatAug extracts more effective features compared to Featuretools and other baselines.","The code is open-sourced at https://github.com/sfu-db/FeatAug"],"url":"http://arxiv.org/abs/2403.06367v1","category":"cs.LG"}
{"created":"2024-03-11 01:36:37","title":"Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach","abstract":"Soft Q-learning is a variation of Q-learning designed to solve entropy regularized Markov decision problems where an agent aims to maximize the entropy regularized value function. Despite its empirical success, there have been limited theoretical studies of soft Q-learning to date. This paper aims to offer a novel and unified finite-time, control-theoretic analysis of soft Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one utilizing the log-sum-exp operator and the other employing the Boltzmann operator. By using dynamical switching system models, we derive novel finite-time error bounds for both soft Q-learning algorithms. We hope that our analysis will deepen the current understanding of soft Q-learning by establishing connections with switching system models and may even pave the way for new frameworks in the finite-time analysis of other reinforcement learning algorithms.","sentences":["Soft Q-learning is a variation of Q-learning designed to solve entropy regularized Markov decision problems where an agent aims to maximize the entropy regularized value function.","Despite its empirical success, there have been limited theoretical studies of soft Q-learning to date.","This paper aims to offer a novel and unified finite-time, control-theoretic analysis of soft Q-learning algorithms.","We focus on two types of soft Q-learning algorithms: one utilizing the log-sum-exp operator and the other employing the Boltzmann operator.","By using dynamical switching system models, we derive novel finite-time error bounds for both soft Q-learning algorithms.","We hope that our analysis will deepen the current understanding of soft Q-learning by establishing connections with switching system models and may even pave the way for new frameworks in the finite-time analysis of other reinforcement learning algorithms."],"url":"http://arxiv.org/abs/2403.06366v1","category":"cs.LG"}
{"created":"2024-03-11 01:12:25","title":"Inference for Median and a Generalization of HulC","abstract":"Constructing distribution-free confidence intervals for the median, a classic problem in statistics, has seen numerous solutions in the literature. While coverage validity has received ample attention, less has been explored about interval width. Our study breaks new ground by investigating the width of these intervals under non-standard assumptions. Surprisingly, we find that properly scaled, the interval width converges to a non-degenerate random variable, unlike traditional intervals. We also adapt our findings for constructing improved confidence intervals for general parameters, enhancing the existing HulC procedure. These advances provide practitioners with more robust tools for data analysis, reducing the need for strict distributional assumptions.","sentences":["Constructing distribution-free confidence intervals for the median, a classic problem in statistics, has seen numerous solutions in the literature.","While coverage validity has received ample attention, less has been explored about interval width.","Our study breaks new ground by investigating the width of these intervals under non-standard assumptions.","Surprisingly, we find that properly scaled, the interval width converges to a non-degenerate random variable, unlike traditional intervals.","We also adapt our findings for constructing improved confidence intervals for general parameters, enhancing the existing HulC procedure.","These advances provide practitioners with more robust tools for data analysis, reducing the need for strict distributional assumptions."],"url":"http://arxiv.org/abs/2403.06357v1","category":"math.ST"}
{"created":"2024-03-11 01:07:36","title":"Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment","abstract":"Multi-modal semantic understanding requires integrating information from different modalities to extract users' real intention behind words. Most previous work applies a dual-encoder structure to separately encode image and text, but fails to learn cross-modal feature alignment, making it hard to achieve cross-modal deep information interaction. This paper proposes a novel CLIP-guided contrastive-learning-based architecture to perform multi-modal feature alignment, which projects the features derived from different modalities into a unified deep space. On multi-modal sarcasm detection (MMSD) and multi-modal sentiment analysis (MMSA) tasks, the experimental results show that our proposed model significantly outperforms several baselines, and our feature alignment strategy brings obvious performance gain over models with different aggregating methods and models even enriched with knowledge. More importantly, our model is simple to implement without using task-specific external knowledge, and thus can easily migrate to other multi-modal tasks. Our source codes are available at https://github.com/ChangKe123/CLFA.","sentences":["Multi-modal semantic understanding requires integrating information from different modalities to extract users' real intention behind words.","Most previous work applies a dual-encoder structure to separately encode image and text, but fails to learn cross-modal feature alignment, making it hard to achieve cross-modal deep information interaction.","This paper proposes a novel CLIP-guided contrastive-learning-based architecture to perform multi-modal feature alignment, which projects the features derived from different modalities into a unified deep space.","On multi-modal sarcasm detection (MMSD) and multi-modal sentiment analysis (MMSA) tasks, the experimental results show that our proposed model significantly outperforms several baselines, and our feature alignment strategy brings obvious performance gain over models with different aggregating methods and models even enriched with knowledge.","More importantly, our model is simple to implement without using task-specific external knowledge, and thus can easily migrate to other multi-modal tasks.","Our source codes are available at https://github.com/ChangKe123/CLFA."],"url":"http://arxiv.org/abs/2403.06355v1","category":"cs.CL"}
{"created":"2024-03-11 00:30:25","title":"Accelerating Sparse Tensor Decomposition Using Adaptive Linearized Representation","abstract":"High-dimensional sparse data emerge in many critical application domains such as cybersecurity, healthcare, anomaly detection, and trend analysis. To quickly extract meaningful insights from massive volumes of these multi-dimensional data, scientists employ unsupervised analysis tools based on tensor decomposition (TD) methods. However, real-world sparse tensors exhibit highly irregular shapes, data distributions, and sparsity, which pose significant challenges for making efficient use of modern parallel architectures. This study breaks the prevailing assumption that compressing sparse tensors into coarse-grained structures (i.e., tensor slices or blocks) or along a particular dimension/mode (i.e., mode-specific) is more efficient than keeping them in a fine-grained, mode-agnostic form. Our novel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO), encodes tensors in a compact format that can be easily streamed from memory and is amenable to both caching and parallel execution. To demonstrate the efficacy of ALTO, we accelerate popular TD methods that compute the Canonical Polyadic Decomposition (CPD) model across a range of real-world sparse tensors. Additionally, we characterize the major execution bottlenecks of TD methods on multiple generations of the latest Intel Xeon Scalable processors, including Sapphire Rapids CPUs, and introduce dynamic adaptation heuristics to automatically select the best algorithm based on the sparse tensor characteristics. Across a diverse set of real-world data sets, ALTO outperforms the state-of-the-art approaches, achieving more than an order-of-magnitude speedup over the best mode-agnostic formats. Compared to the best mode-specific formats, which require multiple tensor copies, ALTO achieves more than 5.1x geometric mean speedup at a fraction (25%) of their storage.","sentences":["High-dimensional sparse data emerge in many critical application domains such as cybersecurity, healthcare, anomaly detection, and trend analysis.","To quickly extract meaningful insights from massive volumes of these multi-dimensional data, scientists employ unsupervised analysis tools based on tensor decomposition (TD) methods.","However, real-world sparse tensors exhibit highly irregular shapes, data distributions, and sparsity, which pose significant challenges for making efficient use of modern parallel architectures.","This study breaks the prevailing assumption that compressing sparse tensors into coarse-grained structures (i.e., tensor slices or blocks) or along a particular dimension/mode (i.e., mode-specific) is more efficient than keeping them in a fine-grained, mode-agnostic form.","Our novel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO), encodes tensors in a compact format that can be easily streamed from memory and is amenable to both caching and parallel execution.","To demonstrate the efficacy of ALTO, we accelerate popular TD methods that compute the Canonical Polyadic Decomposition (CPD) model across a range of real-world sparse tensors.","Additionally, we characterize the major execution bottlenecks of TD methods on multiple generations of the latest Intel Xeon Scalable processors, including Sapphire Rapids CPUs, and introduce dynamic adaptation heuristics to automatically select the best algorithm based on the sparse tensor characteristics.","Across a diverse set of real-world data sets, ALTO outperforms the state-of-the-art approaches, achieving more than an order-of-magnitude speedup over the best mode-agnostic formats.","Compared to the best mode-specific formats, which require multiple tensor copies, ALTO achieves more than 5.1x geometric mean speedup at a fraction (25%) of their storage."],"url":"http://arxiv.org/abs/2403.06348v1","category":"cs.DC"}
{"created":"2024-03-10 23:54:44","title":"Can One Hear the Shape of a Decision Problem?","abstract":"We explore the connection between an agent's decision problem and her ranking of information structures. We find that a finite amount of ordinal data on the agent's ranking of experiments is enough to identify her (finite) set of undominated actions (up to relabeling and duplication) and the beliefs rendering each such action optimal. An additional smattering of cardinal data, comparing the relative value to the agent of finitely many pairs of experiments, identifies her utility function up to an action-independent payoff.","sentences":["We explore the connection between an agent's decision problem and her ranking of information structures.","We find that a finite amount of ordinal data on the agent's ranking of experiments is enough to identify her (finite) set of undominated actions (up to relabeling and duplication) and the beliefs rendering each such action optimal.","An additional smattering of cardinal data, comparing the relative value to the agent of finitely many pairs of experiments, identifies her utility function up to an action-independent payoff."],"url":"http://arxiv.org/abs/2403.06344v1","category":"econ.TH"}
{"created":"2024-03-10 23:19:57","title":"Error-Mitigated Quantum Random Access Memory","abstract":"As an alternative to quantum error correction, quantum error mitigation methods, including Zero-Noise Extrapolation (ZNE), have been proposed to alleviate run-time errors in current noisy quantum devices. In this work, we propose a modified version of ZNE that provides for a significant performance enhancement on current noisy devices. Our modified ZNE method extrapolates to zero-noise data by evaluating groups of noisy data obtained from noise-scaled circuits and selecting extrapolation functions for each group with the assistance of estimated noisy simulation results. To quantify enhancement in a real-world quantum application, we embed our modified ZNE in Quantum Random Access Memory (QRAM) - a memory system important for future quantum networks and computers. Our new ZNE-enhanced QRAM designs are experimentally implemented on a 27-qubit noisy superconducting quantum device, the results of which demonstrate that with reasonable estimated simulation results, QRAM fidelity is improved significantly relative to traditional ZNE usage. Our results demonstrate the critical role the extrapolation function plays in ZNE - judicious choice of that function on a per-measurement basis can make the difference between a quantum application being functional or non-functional.","sentences":["As an alternative to quantum error correction, quantum error mitigation methods, including Zero-Noise Extrapolation (ZNE), have been proposed to alleviate run-time errors in current noisy quantum devices.","In this work, we propose a modified version of ZNE that provides for a significant performance enhancement on current noisy devices.","Our modified ZNE method extrapolates to zero-noise data by evaluating groups of noisy data obtained from noise-scaled circuits and selecting extrapolation functions for each group with the assistance of estimated noisy simulation results.","To quantify enhancement in a real-world quantum application, we embed our modified ZNE in Quantum Random Access Memory (QRAM) - a memory system important for future quantum networks and computers.","Our new ZNE-enhanced QRAM designs are experimentally implemented on a 27-qubit noisy superconducting quantum device, the results of which demonstrate that with reasonable estimated simulation results, QRAM fidelity is improved significantly relative to traditional ZNE usage.","Our results demonstrate the critical role the extrapolation function plays in ZNE - judicious choice of that function on a per-measurement basis can make the difference between a quantum application being functional or non-functional."],"url":"http://arxiv.org/abs/2403.06340v1","category":"quant-ph"}
{"created":"2024-03-10 23:11:05","title":"Disentangling shared and private latent factors in multimodal Variational Autoencoders","abstract":"Generative models for multimodal data permit the identification of latent factors that may be associated with important determinants of observed data heterogeneity. Common or shared factors could be important for explaining variation across modalities whereas other factors may be private and important only for the explanation of a single modality. Multimodal Variational Autoencoders, such as MVAE and MMVAE, are a natural choice for inferring those underlying latent factors and separating shared variation from private. In this work, we investigate their capability to reliably perform this disentanglement. In particular, we highlight a challenging problem setting where modality-specific variation dominates the shared signal. Taking a cross-modal prediction perspective, we demonstrate limitations of existing models, and propose a modification how to make them more robust to modality-specific variation. Our findings are supported by experiments on synthetic as well as various real-world multi-omics data sets.","sentences":["Generative models for multimodal data permit the identification of latent factors that may be associated with important determinants of observed data heterogeneity.","Common or shared factors could be important for explaining variation across modalities whereas other factors may be private and important only for the explanation of a single modality.","Multimodal Variational Autoencoders, such as MVAE and MMVAE, are a natural choice for inferring those underlying latent factors and separating shared variation from private.","In this work, we investigate their capability to reliably perform this disentanglement.","In particular, we highlight a challenging problem setting where modality-specific variation dominates the shared signal.","Taking a cross-modal prediction perspective, we demonstrate limitations of existing models, and propose a modification how to make them more robust to modality-specific variation.","Our findings are supported by experiments on synthetic as well as various real-world multi-omics data sets."],"url":"http://arxiv.org/abs/2403.06338v1","category":"stat.ML"}
{"created":"2024-03-10 22:00:39","title":"Odd-frequency superfluidity from a particle-number-conserving perspective","abstract":"We investigate odd-in-time (or odd-frequency) pairing of fermions in equilibrium systems within the particle-number-conserving framework of Penrose, Onsager and Yang, where superfluid order is defined by macrosocopic eigenvalues of reduced density matrices. We show that odd-frequency pair correlations are synonymous with even fermion-exchange symmetry in a time-dependent correlation function that generalises the two-body reduced density matrix. Macroscopic even-under-fermion-exchange pairing is found to emerge from conventional Penrose-Onsager-Yang condensation in two-body or higher-order reduced density matrices through the symmetry-mixing properties of the Hamiltonian. We identify and characterise a transformer matrix responsible for producing macroscopic even fermion-exchange correlations that coexist with a conventional Cooper-pair condensate, while a generator matrix is shown to be responsible for creating macroscopic even fermion-exchange correlations from hidden orders such as a multi-particle condensate. The transformer scenario is illustrated using the spin-imbalanced Fermi superfluid as an example. The generator scenario is demonstrated by the composite-boson condensate arising for itinerant electrons coupled to magnetic excitations. Structural analysis of the transformer and generator matrices is shown to provide general conditions for odd-frequency pairing order to arise in a given system.","sentences":["We investigate odd-in-time (or odd-frequency) pairing of fermions in equilibrium systems within the particle-number-conserving framework of Penrose, Onsager and Yang, where superfluid order is defined by macrosocopic eigenvalues of reduced density matrices.","We show that odd-frequency pair correlations are synonymous with even fermion-exchange symmetry in a time-dependent correlation function that generalises the two-body reduced density matrix.","Macroscopic even-under-fermion-exchange pairing is found to emerge from conventional Penrose-Onsager-Yang condensation in two-body or higher-order reduced density matrices through the symmetry-mixing properties of the Hamiltonian.","We identify and characterise a transformer matrix responsible for producing macroscopic even fermion-exchange correlations that coexist with a conventional Cooper-pair condensate, while a generator matrix is shown to be responsible for creating macroscopic even fermion-exchange correlations from hidden orders such as a multi-particle condensate.","The transformer scenario is illustrated using the spin-imbalanced Fermi superfluid as an example.","The generator scenario is demonstrated by the composite-boson condensate arising for itinerant electrons coupled to magnetic excitations.","Structural analysis of the transformer and generator matrices is shown to provide general conditions for odd-frequency pairing order to arise in a given system."],"url":"http://arxiv.org/abs/2403.06325v1","category":"cond-mat.supr-con"}
{"created":"2024-03-10 21:53:42","title":"ACM MMSys 2024 Bandwidth Estimation in Real Time Communications Challenge","abstract":"The quality of experience (QoE) delivered by video conferencing systems to end users depends in part on correctly estimating the capacity of the bottleneck link between the sender and the receiver over time. Bandwidth estimation for real-time communications (RTC) remains a significant challenge, primarily due to the continuously evolving heterogeneous network architectures and technologies. From the first bandwidth estimation challenge which was hosted at ACM MMSys 2021, we learnt that bandwidth estimation models trained with reinforcement learning (RL) in simulations to maximize network-based reward functions may not be optimal in reality due to the sim-to-real gap and the difficulty of aligning network-based rewards with user-perceived QoE. This grand challenge aims to advance bandwidth estimation model design by aligning reward maximization with user-perceived QoE optimization using offline RL and a real-world dataset with objective rewards which have high correlations with subjective user-perceived audio/video quality in Microsoft Teams. All models submitted to the grand challenge underwent initial evaluation on our emulation platform. For a comprehensive evaluation under diverse network conditions with temporal fluctuations, top models were further evaluated on our geographically distributed testbed by using each model to conduct 600 calls within a 12-day period. The winning model is shown to deliver comparable performance to the top behavior policy in the released dataset. By leveraging real-world data and integrating objective audio/video quality scores as rewards, offline RL can therefore facilitate the development of competitive bandwidth estimators for RTC.","sentences":["The quality of experience (QoE) delivered by video conferencing systems to end users depends in part on correctly estimating the capacity of the bottleneck link between the sender and the receiver over time.","Bandwidth estimation for real-time communications (RTC) remains a significant challenge, primarily due to the continuously evolving heterogeneous network architectures and technologies.","From the first bandwidth estimation challenge which was hosted at ACM MMSys 2021, we learnt that bandwidth estimation models trained with reinforcement learning (RL) in simulations to maximize network-based reward functions may not be optimal in reality due to the sim-to-real gap and the difficulty of aligning network-based rewards with user-perceived QoE.","This grand challenge aims to advance bandwidth estimation model design by aligning reward maximization with user-perceived QoE optimization using offline RL and a real-world dataset with objective rewards which have high correlations with subjective user-perceived audio/video quality in Microsoft Teams.","All models submitted to the grand challenge underwent initial evaluation on our emulation platform.","For a comprehensive evaluation under diverse network conditions with temporal fluctuations, top models were further evaluated on our geographically distributed testbed by using each model to conduct 600 calls within a 12-day period.","The winning model is shown to deliver comparable performance to the top behavior policy in the released dataset.","By leveraging real-world data and integrating objective audio/video quality scores as rewards, offline RL can therefore facilitate the development of competitive bandwidth estimators for RTC."],"url":"http://arxiv.org/abs/2403.06324v1","category":"cs.NI"}
{"created":"2024-03-10 21:45:12","title":"Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL","abstract":"We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk (CVaR), entropic risk and Markowitz's mean-variance. Using an augmented Markov Decision Process (MDP), we propose two general meta-algorithms via reductions to standard RL: one based on optimistic algorithms and another based on policy optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL theory with entropic risk or CVaR. Under discrete rewards, our optimistic theory also certifies the first RSRL regret bounds for MDPs with bounded coverability, e.g., exogenous block MDPs. Under discrete rewards, our policy optimization meta-algorithm enjoys both global convergence and local improvement guarantees in a novel metric that lower bounds the true OCE risk. Finally, we instantiate our framework with PPO, construct an MDP, and show that it learns the optimal risk-sensitive policy while prior algorithms provably fail.","sentences":["We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk (CVaR), entropic risk and Markowitz's mean-variance.","Using an augmented Markov Decision Process (MDP), we propose two general meta-algorithms via reductions to standard RL: one based on optimistic algorithms and another based on policy optimization.","Our optimistic meta-algorithm generalizes almost all prior RSRL theory with entropic risk or CVaR.","Under discrete rewards, our optimistic theory also certifies the first RSRL regret bounds for MDPs with bounded coverability, e.g., exogenous block MDPs.","Under discrete rewards, our policy optimization meta-algorithm enjoys both global convergence and local improvement guarantees in a novel metric that lower bounds the true OCE risk.","Finally, we instantiate our framework with PPO, construct an MDP, and show that it learns the optimal risk-sensitive policy while prior algorithms provably fail."],"url":"http://arxiv.org/abs/2403.06323v1","category":"cs.LG"}
{"created":"2024-03-10 21:37:21","title":"Fake or Compromised? Making Sense of Malicious Clients in Federated Learning","abstract":"Federated learning (FL) is a distributed machine learning paradigm that enables training models on decentralized data. The field of FL security against poisoning attacks is plagued with confusion due to the proliferation of research that makes different assumptions about the capabilities of adversaries and the adversary models they operate under. Our work aims to clarify this confusion by presenting a comprehensive analysis of the various poisoning attacks and defensive aggregation rules (AGRs) proposed in the literature, and connecting them under a common framework. To connect existing adversary models, we present a hybrid adversary model, which lies in the middle of the spectrum of adversaries, where the adversary compromises a few clients, trains a generative (e.g., DDPM) model with their compromised samples, and generates new synthetic data to solve an optimization for a stronger (e.g., cheaper, more practical) attack against different robust aggregation rules. By presenting the spectrum of FL adversaries, we aim to provide practitioners and researchers with a clear understanding of the different types of threats they need to consider when designing FL systems, and identify areas where further research is needed.","sentences":["Federated learning (FL) is a distributed machine learning paradigm that enables training models on decentralized data.","The field of FL security against poisoning attacks is plagued with confusion due to the proliferation of research that makes different assumptions about the capabilities of adversaries and the adversary models they operate under.","Our work aims to clarify this confusion by presenting a comprehensive analysis of the various poisoning attacks and defensive aggregation rules (AGRs) proposed in the literature, and connecting them under a common framework.","To connect existing adversary models, we present a hybrid adversary model, which lies in the middle of the spectrum of adversaries, where the adversary compromises a few clients, trains a generative (e.g., DDPM) model with their compromised samples, and generates new synthetic data to solve an optimization for a stronger (e.g., cheaper, more practical) attack against different robust aggregation rules.","By presenting the spectrum of FL adversaries, we aim to provide practitioners and researchers with a clear understanding of the different types of threats they need to consider when designing FL systems, and identify areas where further research is needed."],"url":"http://arxiv.org/abs/2403.06319v1","category":"cs.LG"}
{"created":"2024-03-10 21:30:58","title":"Background discrimination with a Micromegas detector prototype and veto system for BabyIAXO","abstract":"In this paper we present measurements performed with a Micromegas X-ray detector setup. The detector is a prototype in the context of the BabyIAXO helioscope, which is under construction to search for an emission of the hypothetical axion particle from the sun. An important component of such a helioscope is a low background X-ray detector with a high efficiency in the 1-10 keV energy range. The goal of the measurement was to study techniques for background discrimination. In addition to common techniques we used a multi-layer veto system designed to tag cosmogenic neutron background. Over an effective time of 52 days, a background level of $8.6 \\times 10^{-7}\\,\\text{counts keV}^{-1}\\,\\text{cm}^{-2} \\, \\text{s}^{-1}$ was reached in a laboratory at above ground level. This is the lowest background level achieved at surface level. In this paper we present the experimental setup, show simulations of the neutron-induced background, and demonstrate the process to identify background signals in the data. Finally, prospects to reach lower background levels down to $10^{-7} \\, \\text{counts keV}^{-1} \\, \\text{cm}^{-2} \\, \\text{s}^{-1}$ will be discussed.","sentences":["In this paper we present measurements performed with a Micromegas X-ray detector setup.","The detector is a prototype in the context of the BabyIAXO helioscope, which is under construction to search for an emission of the hypothetical axion particle from the sun.","An important component of such a helioscope is a low background X-ray detector with a high efficiency in the 1-10 keV energy range.","The goal of the measurement was to study techniques for background discrimination.","In addition to common techniques we used a multi-layer veto system designed to tag cosmogenic neutron background.","Over an effective time of 52 days, a background level of $8.6 \\times 10^{-7}\\,\\text{counts keV}^{-1}\\,\\text{cm}^{-2} \\, \\text{s}^{-1}$ was reached in a laboratory at above ground level.","This is the lowest background level achieved at surface level.","In this paper we present the experimental setup, show simulations of the neutron-induced background, and demonstrate the process to identify background signals in the data.","Finally, prospects to reach lower background levels down to $10^{-7} \\, \\text{counts keV}^{-1} \\, \\text{cm}^{-2} \\, \\text{s}^{-1}$ will be discussed."],"url":"http://arxiv.org/abs/2403.06316v1","category":"physics.ins-det"}
{"created":"2024-03-10 21:10:38","title":"Multi-gated perimeter flow control for monocentric cities: Efficiency and equity","abstract":"A control scheme for the multi-gated perimeter traffic flow control problem of cities is presented. The proposed scheme determines feasible and optimally distributed input flows for the various gates located at the periphery of a protected network. A parsimonious model is employed to describe the traffic dynamics of the protected network. To describe traffic dynamics outside of the protected area, the state-space model is augmented with additional state variables to account for vehicle queues at store-and-forward origin links at the periphery. The perimeter flow control problem is formulated as a convex optimisation problem with finite horizon, and constrained control and state variables. It aims to equalise the relative queues at origin links and to maintain the vehicle accumulation in the protected network around a desired set point, while the system's throughput is maximised. For real-time control, the optimal control problem is embedded in a rolling-horizon scheme using the current state of the system as the initial state as well as predicted demand flows at entrance links. Furthermore, practical flow allocation policies for single-region perimeter control without explicitly considering entrance link dynamics are presented. These policies allocate a global perimeter-ordered flow to candidate gates at the periphery of a protected network by taking into account the different geometric characteristics of origin links. The proposed flow allocation policies are then benchmarked against the multi-gated perimeter flow control. A study is carried out for a 2.5 square mile protected network area of San Francisco, CA, including fifteen gates of different geometric characteristics. The results have showed that the proposed scheme is able to manage excessive queues outside of the protected network and to optimally distribute the input flows, which confirms its efficiency and equity properties.","sentences":["A control scheme for the multi-gated perimeter traffic flow control problem of cities is presented.","The proposed scheme determines feasible and optimally distributed input flows for the various gates located at the periphery of a protected network.","A parsimonious model is employed to describe the traffic dynamics of the protected network.","To describe traffic dynamics outside of the protected area, the state-space model is augmented with additional state variables to account for vehicle queues at store-and-forward origin links at the periphery.","The perimeter flow control problem is formulated as a convex optimisation problem with finite horizon, and constrained control and state variables.","It aims to equalise the relative queues at origin links and to maintain the vehicle accumulation in the protected network around a desired set point, while the system's throughput is maximised.","For real-time control, the optimal control problem is embedded in a rolling-horizon scheme using the current state of the system as the initial state as well as predicted demand flows at entrance links.","Furthermore, practical flow allocation policies for single-region perimeter control without explicitly considering entrance link dynamics are presented.","These policies allocate a global perimeter-ordered flow to candidate gates at the periphery of a protected network by taking into account the different geometric characteristics of origin links.","The proposed flow allocation policies are then benchmarked against the multi-gated perimeter flow control.","A study is carried out for a 2.5 square mile protected network area of San Francisco, CA, including fifteen gates of different geometric characteristics.","The results have showed that the proposed scheme is able to manage excessive queues outside of the protected network and to optimally distribute the input flows, which confirms its efficiency and equity properties."],"url":"http://arxiv.org/abs/2403.06312v1","category":"eess.SY"}
{"created":"2024-03-10 20:44:39","title":"Preparation of Epitaxial Scandium Trifluoride Thin Films using Pulsed Laser Deposition","abstract":"Bulk Scandium trifluoride ($\\mathrm{ScF_3}$) is known for a pronounced negative thermal expansion (NTE) over a wide range of temperature, from $10~\\mathrm{K}~\\text{to}~ 1100~\\mathrm{K}$. The structure of $\\mathrm{ScF_3}$ can be described as an $\\mathrm{ABX_3}$ perovskite with an empty A-site and a space group of Pm-3m. Growing thin films of $\\mathrm{ScF_3}$ allows for tuning the lattice constant, the thermal expansion, and the construction of devices based upon differential thermal expansion. We have investigated the growth of $\\mathrm{ScF_3}$ films on oxide and fluoride substrates using pulsed laser deposition (PLD) This letter describes the successful growth recipe for producing high quality epitaxial $\\mathrm{ScF_3}$ thin films on positive thermal expansion (PTE) lithium fluoride ($\\mathrm{LiF}$) substrates, at substrate temperature, $350^{\\circ}\\mathrm{C}$ with a laser repetition rate of $1~\\mathrm{Hz}$, with an energy per pulse of $600~\\mathrm{mJ}$, under a vacuum of $1.5\\times 10^{-6}~ \\mathrm{torr}$, for a growth time of $6$ hours. However, even for films with excellent epitaxy and sharp peaks along the principal axes, diffraction peaks from certain crystallographic directions are extremely broad, with the example of ($104$) reflections, in this work. We attribute this broadening to disorder in the $\\mathrm{F_6}$ octahedral rotations that occur as an attempt to accommodate the large temperature-induced lattice mismatch that results in cooling from the growth temperature for this system of a NTE film mated to a PTE substrate.","sentences":["Bulk Scandium trifluoride ($\\mathrm{ScF_3}$) is known for a pronounced negative thermal expansion (NTE) over a wide range of temperature, from $10~\\mathrm{K}~\\text{to}~ 1100~\\mathrm{K}$.","The structure of $\\mathrm{ScF_3}$ can be described as an $\\mathrm{ABX_3}$ perovskite with an empty A-site and a space group of Pm-3m. Growing thin films of $\\mathrm{ScF_3}$ allows for tuning the lattice constant, the thermal expansion, and the construction of devices based upon differential thermal expansion.","We have investigated the growth of $\\mathrm{ScF_3}$ films on oxide and fluoride substrates using pulsed laser deposition (PLD)","This letter describes the successful growth recipe for producing high quality epitaxial $\\mathrm{ScF_3}$ thin films on positive thermal expansion (PTE) lithium fluoride ($\\mathrm{LiF}$) substrates, at substrate temperature, $350^{\\circ}\\mathrm{C}$ with a laser repetition rate of $1~\\mathrm{Hz}$, with an energy per pulse of $600~\\mathrm{mJ}$, under a vacuum of $1.5\\times 10^{-6}~ \\mathrm{torr}$, for a growth time of $6$ hours.","However, even for films with excellent epitaxy and sharp peaks along the principal axes, diffraction peaks from certain crystallographic directions are extremely broad, with the example of ($104$) reflections, in this work.","We attribute this broadening to disorder in the $\\mathrm{F_6}$ octahedral rotations that occur as an attempt to accommodate the large temperature-induced lattice mismatch that results in cooling from the growth temperature for this system of a NTE film mated to a PTE substrate."],"url":"http://arxiv.org/abs/2403.06305v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-10 20:39:01","title":"A Unifying Approach for the Pricing of Debt Securities","abstract":"We propose a unifying framework for the pricing of debt securities under general time-inhomogeneous short-rate diffusion processes. The pricing of bonds, bond options, callable/putable bonds, and convertible bonds (CBs) are covered. Using continuous-time Markov chain (CTMC) approximation, we obtain closed-form matrix expressions to approximate the price of bonds and bond options under general one-dimensional short-rate processes. A simple and efficient algorithm is also developed to price callable/putable debts. The availability of a closed-form expression for the price of zero-coupon bonds allows for the perfect fit of the approximated model to the current market term structure of interest rates, regardless of the complexity of the underlying diffusion process selected. We further consider the pricing of CBs under general bi-dimensional time-inhomogeneous diffusion processes to model equity and short-rate dynamics. Credit risk is also incorporated into the model using the approach of Tsiveriotis and Fernandes (1998). Based on a two-layer CTMC method, an efficient algorithm is developed to approximate the price of convertible bonds. When conversion is only allowed at maturity, a closed-form matrix expression is obtained. Numerical experiments show the accuracy and efficiency of the method across a wide range of model parameters and short-rate models.","sentences":["We propose a unifying framework for the pricing of debt securities under general time-inhomogeneous short-rate diffusion processes.","The pricing of bonds, bond options, callable/putable bonds, and convertible bonds (CBs) are covered.","Using continuous-time Markov chain (CTMC) approximation, we obtain closed-form matrix expressions to approximate the price of bonds and bond options under general one-dimensional short-rate processes.","A simple and efficient algorithm is also developed to price callable/putable debts.","The availability of a closed-form expression for the price of zero-coupon bonds allows for the perfect fit of the approximated model to the current market term structure of interest rates, regardless of the complexity of the underlying diffusion process selected.","We further consider the pricing of CBs under general bi-dimensional time-inhomogeneous diffusion processes to model equity and short-rate dynamics.","Credit risk is also incorporated into the model using the approach of Tsiveriotis and Fernandes (1998).","Based on a two-layer CTMC method, an efficient algorithm is developed to approximate the price of convertible bonds.","When conversion is only allowed at maturity, a closed-form matrix expression is obtained.","Numerical experiments show the accuracy and efficiency of the method across a wide range of model parameters and short-rate models."],"url":"http://arxiv.org/abs/2403.06303v1","category":"q-fin.PR"}
{"created":"2024-03-10 20:15:08","title":"Cross-ecosystem categorization: A manual-curation protocol for the categorization of Java Maven libraries along Python PyPI Topics","abstract":"Context: Software of different functional categories, such as text processing vs. networking, has different profiles in terms of metrics like security and updates. Using popularity to compare e.g. Java vs. Python libraries might give a skewed perspective, as the categories of the most popular software vary from one ecosystem to the next. How can one compare libraries datasets across software ecosystems, when not even the category names are uniform among them? Objective: We study how to generate a language-agnostic categorisation of software by functional purpose, that enables cross-ecosystem studies of libraries datasets. This provides the functional fingerprint information needed for software metrics comparisons. Method: We designed and implemented a human-guided protocol to categorise libraries from software ecosystems. Category names mirror PyPI Topic classifiers, but the protocol is generic and can be applied to any ecosystem. We demonstrate it by categorising 256 Java/Maven libraries with severe security vulnerabilities. Results: The protocol allows three or more people to categorise any number of libraries. The categorisation produced is functional-oriented and language-agnostic. The Java/Maven dataset demonstration resulted in a majority of Internet-oriented libraries, coherent with its selection by severe vulnerabilities. To allow replication and updates, we make the dataset and the protocol individual steps available as open data. Conclusions: Libraries categorisation by functional purpose is feasible with our protocol, which produced the fingerprint of a 256-libraries Java dataset. While this was labour intensive, humans excel in the required inference tasks, so full automation of the process is not envisioned. However, results can provide the ground truth needed for machine learning in large-scale cross-ecosystem empirical studies.","sentences":["Context: Software of different functional categories, such as text processing vs. networking, has different profiles in terms of metrics like security and updates.","Using popularity to compare e.g. Java vs. Python libraries might give a skewed perspective, as the categories of the most popular software vary from one ecosystem to the next.","How can one compare libraries datasets across software ecosystems, when not even the category names are uniform among them?","Objective: We study how to generate a language-agnostic categorisation of software by functional purpose, that enables cross-ecosystem studies of libraries datasets.","This provides the functional fingerprint information needed for software metrics comparisons.","Method: We designed and implemented a human-guided protocol to categorise libraries from software ecosystems.","Category names mirror PyPI Topic classifiers, but the protocol is generic and can be applied to any ecosystem.","We demonstrate it by categorising 256 Java/Maven libraries with severe security vulnerabilities.","Results:","The protocol allows three or more people to categorise any number of libraries.","The categorisation produced is functional-oriented and language-agnostic.","The Java/Maven dataset demonstration resulted in a majority of Internet-oriented libraries, coherent with its selection by severe vulnerabilities.","To allow replication and updates, we make the dataset and the protocol individual steps available as open data.","Conclusions: Libraries categorisation by functional purpose is feasible with our protocol, which produced the fingerprint of a 256-libraries Java dataset.","While this was labour intensive, humans excel in the required inference tasks, so full automation of the process is not envisioned.","However, results can provide the ground truth needed for machine learning in large-scale cross-ecosystem empirical studies."],"url":"http://arxiv.org/abs/2403.06300v1","category":"cs.SE"}
{"created":"2024-03-10 20:07:14","title":"Analysis of Total Variation Minimization for Clustered Federated Learning","abstract":"A key challenge in federated learning applications is the statistical heterogeneity of local datasets. Clustered federated learning addresses this challenge by identifying clusters of local datasets that are approximately homogeneous. One recent approach to clustered federated learning is generalized total variation minimization (GTVMin). This approach requires a similarity graph which can be obtained by domain expertise or in a data-driven fashion via graph learning techniques. Under a widely applicable clustering assumption, we derive an upper bound the deviation between GTVMin solutions and their cluster-wise averages. This bound provides valuable insights into the effectiveness and robustness of GTVMin in addressing statistical heterogeneity within federated learning environments.","sentences":["A key challenge in federated learning applications is the statistical heterogeneity of local datasets.","Clustered federated learning addresses this challenge by identifying clusters of local datasets that are approximately homogeneous.","One recent approach to clustered federated learning is generalized total variation minimization (GTVMin).","This approach requires a similarity graph which can be obtained by domain expertise or in a data-driven fashion via graph learning techniques.","Under a widely applicable clustering assumption, we derive an upper bound the deviation between GTVMin solutions and their cluster-wise averages.","This bound provides valuable insights into the effectiveness and robustness of GTVMin in addressing statistical heterogeneity within federated learning environments."],"url":"http://arxiv.org/abs/2403.06298v1","category":"cs.LG"}
{"created":"2024-03-10 20:05:42","title":"Entanglement of bosonic systems under monitored evolution","abstract":"The evolution of non-interacting bosons in the presence of repeated projective measurements is studied. Following the established approach, this monitored evolution is characterized by the first detected return and the first detected transition probabilities. We show that these quantities are directly related to the entanglement entropy and to the entanglement spectrum of a bipartite system. Calculations with specific values for the number of bosons, the number of measurements and the time step between measurements reveal a sensitive and often strongly fluctuating entanglement entropy. In particular, we demonstrate that in the vicinity of special values for the time steps the evolution of the entanglement entropy is either stationary or performs dynamical switching between two or more stationary values. In the entanglement spectrum, on the other hand, this complex behavior can be associated with level crossings, indicating that the dominant quantum states and their entanglement respond strongly to a change of the system parameters. We discuss briefly the role of time averaging to remove the fluctuations of the entanglement entropy.","sentences":["The evolution of non-interacting bosons in the presence of repeated projective measurements is studied.","Following the established approach, this monitored evolution is characterized by the first detected return and the first detected transition probabilities.","We show that these quantities are directly related to the entanglement entropy and to the entanglement spectrum of a bipartite system.","Calculations with specific values for the number of bosons, the number of measurements and the time step between measurements reveal a sensitive and often strongly fluctuating entanglement entropy.","In particular, we demonstrate that in the vicinity of special values for the time steps the evolution of the entanglement entropy is either stationary or performs dynamical switching between two or more stationary values.","In the entanglement spectrum, on the other hand, this complex behavior can be associated with level crossings, indicating that the dominant quantum states and their entanglement respond strongly to a change of the system parameters.","We discuss briefly the role of time averaging to remove the fluctuations of the entanglement entropy."],"url":"http://arxiv.org/abs/2403.06297v1","category":"quant-ph"}
{"created":"2024-03-10 18:47:47","title":"Constraining Cosmological Parameters with Viscous Modified Chaplygin Gas and Generalized Cosmic Chaplygin Gas Models in Horava-Lifshitz Gravity: Utilizing Late-time Datasets","abstract":"This study investigates accelerated cosmic expansion using the Viscous Modified Chaplygin Gas (VMMG) and Generalized Cosmic Chaplygin Gas (GCCM) within Horava-Lifshitz gravity. Our primary objective is to constrain essential cosmological parameters, such as the Hubble Parameter ($H_{0}$) and Sound Horizon ($r_{d}$). We incorporate recent datasets comprising 17 Baryon Acoustic Oscillation observations, 33 Cosmic Chronometer measurements, 40 Type Ia Supernovae data points, 24 quasar Hubble diagram data points, and 162 Gamma Ray Bursts data points. Additionally, we integrate the most recent determination of the Hubble constant (R22). Treating $r_{d}$ as a free parameter helps alleviate bias, enhance precision, and improve dataset compatibility. By simulating random correlations in the covariance matrix, errors are reduced. The obtained values of ($H_{0}$) and ($r_{d}$) are consistent with those from Planck and SDSS. Cosmography tests offer insights into the dynamics of both models, aiding our understanding of cosmological evolution. Statefinder diagnostics deepen our understanding of cosmic expansion dynamics and aid in distinguishing between different cosmological models. The $o_{m}$ Diagnostic test reveals that at late times, VMMG falls in the phantom region and GCCM falls in the phantom quintessence region. The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) provide support for all models under consideration, indicating that each model offers a plausible explanation. Notably, the $\\Lambda$CDM model emerges with the lowest AIC score, suggesting its relatively superior fit compared to others. Additionally, validation through the reduced $\\chi_{\\text{red}}^{2}$ statistic confirms satisfactory fits across all models, further reinforcing their credibility in explaining the observed data.","sentences":["This study investigates accelerated cosmic expansion using the Viscous Modified Chaplygin Gas (VMMG) and Generalized Cosmic Chaplygin Gas (GCCM) within Horava-Lifshitz gravity.","Our primary objective is to constrain essential cosmological parameters, such as the Hubble Parameter ($H_{0}$) and Sound Horizon ($r_{d}$).","We incorporate recent datasets comprising 17 Baryon Acoustic Oscillation observations, 33 Cosmic Chronometer measurements, 40 Type Ia Supernovae data points, 24 quasar Hubble diagram data points, and 162 Gamma Ray Bursts data points.","Additionally, we integrate the most recent determination of the Hubble constant (R22).","Treating $r_{d}$ as a free parameter helps alleviate bias, enhance precision, and improve dataset compatibility.","By simulating random correlations in the covariance matrix, errors are reduced.","The obtained values of ($H_{0}$) and ($r_{d}$) are consistent with those from Planck and SDSS.","Cosmography tests offer insights into the dynamics of both models, aiding our understanding of cosmological evolution.","Statefinder diagnostics deepen our understanding of cosmic expansion dynamics and aid in distinguishing between different cosmological models.","The $o_{m}$ Diagnostic test reveals that at late times, VMMG falls in the phantom region and","GCCM falls in the phantom quintessence region.","The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) provide support for all models under consideration, indicating that each model offers a plausible explanation.","Notably, the $\\Lambda$CDM model emerges with the lowest AIC score, suggesting its relatively superior fit compared to others.","Additionally, validation through the reduced $\\chi_{\\text{red}}^{2}$ statistic confirms satisfactory fits across all models, further reinforcing their credibility in explaining the observed data."],"url":"http://arxiv.org/abs/2403.06286v1","category":"astro-ph.CO"}
{"created":"2024-03-10 18:41:48","title":"A Tunable Universal Formula for Safety-Critical Control","abstract":"Sontag's universal formula is a widely-used technique for stabilizing control through control Lyapunov functions, and it has been extended to address safety-critical control in recent years by incorporating control barrier functions (CBFs). However, how to derive a universal formula that satisfies requirements on essential properties, including safety, robustness, and smoothness, is still an open problem. To address this challenge, this paper introduces a novel solution - a tunable universal formula - incorporating a (state-dependent) tunable scaling term into Sontag's universal formula. This tunable scaling term enables the regulation of safety control performances, allowing the attainment of desired properties through a proper selection. Furthermore, we extend this tunable universal formula to address safety-critical control problems with norm-bounded input constraints, showcasing its applicability across diverse control scenarios. Finally, we demonstrate the efficacy of our method through a collision avoidance example, investigating the essential properties including safety, robustness, and smoothness under various tunable scaling terms.","sentences":["Sontag's universal formula is a widely-used technique for stabilizing control through control Lyapunov functions, and it has been extended to address safety-critical control in recent years by incorporating control barrier functions (CBFs).","However, how to derive a universal formula that satisfies requirements on essential properties, including safety, robustness, and smoothness, is still an open problem.","To address this challenge, this paper introduces a novel solution - a tunable universal formula - incorporating a (state-dependent) tunable scaling term into Sontag's universal formula.","This tunable scaling term enables the regulation of safety control performances, allowing the attainment of desired properties through a proper selection.","Furthermore, we extend this tunable universal formula to address safety-critical control problems with norm-bounded input constraints, showcasing its applicability across diverse control scenarios.","Finally, we demonstrate the efficacy of our method through a collision avoidance example, investigating the essential properties including safety, robustness, and smoothness under various tunable scaling terms."],"url":"http://arxiv.org/abs/2403.06285v1","category":"eess.SY"}
{"created":"2024-03-10 17:12:01","title":"FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing","abstract":"Diffusion models have demonstrated remarkable capabilities in text-to-image and text-to-video generation, opening up possibilities for video editing based on textual input. However, the computational cost associated with sequential sampling in diffusion models poses challenges for efficient video editing. Existing approaches relying on image generation models for video editing suffer from time-consuming one-shot fine-tuning, additional condition extraction, or DDIM inversion, making real-time applications impractical. In this work, we propose FastVideoEdit, an efficient zero-shot video editing approach inspired by Consistency Models (CMs). By leveraging the self-consistency property of CMs, we eliminate the need for time-consuming inversion or additional condition extraction, reducing editing time. Our method enables direct mapping from source video to target video with strong preservation ability utilizing a special variance schedule. This results in improved speed advantages, as fewer sampling steps can be used while maintaining comparable generation quality. Experimental results validate the state-of-the-art performance and speed advantages of FastVideoEdit across evaluation metrics encompassing editing speed, temporal consistency, and text-video alignment.","sentences":["Diffusion models have demonstrated remarkable capabilities in text-to-image and text-to-video generation, opening up possibilities for video editing based on textual input.","However, the computational cost associated with sequential sampling in diffusion models poses challenges for efficient video editing.","Existing approaches relying on image generation models for video editing suffer from time-consuming one-shot fine-tuning, additional condition extraction, or DDIM inversion, making real-time applications impractical.","In this work, we propose FastVideoEdit, an efficient zero-shot video editing approach inspired by Consistency Models (CMs).","By leveraging the self-consistency property of CMs, we eliminate the need for time-consuming inversion or additional condition extraction, reducing editing time.","Our method enables direct mapping from source video to target video with strong preservation ability utilizing a special variance schedule.","This results in improved speed advantages, as fewer sampling steps can be used while maintaining comparable generation quality.","Experimental results validate the state-of-the-art performance and speed advantages of FastVideoEdit across evaluation metrics encompassing editing speed, temporal consistency, and text-video alignment."],"url":"http://arxiv.org/abs/2403.06269v1","category":"cs.CV"}
{"created":"2024-03-10 17:02:19","title":"Dynamics of Polarization Under Normative Institutions and Opinion Expression Stewarding","abstract":"Although there is mounting empirical evidence for the increase in affective polarization, few mechanistic models can explain its emergence at the population level. The question of how such a phenomenon can emerge from divergent opinions of a population on an ideological issue is still an open issue. In this paper, we establish that human normativity, that is, individual expression of normative opinions based on beliefs about the population, can lead to population-level polarization when ideological institutions distort beliefs in accordance with their objective of moving expressed opinion to one extreme. Using a game-theoretic model, we establish that individuals with more extreme opinions will have more extreme rhetoric and higher misperceptions about their outgroup members. Our model also shows that when social recommendation systems mediate institutional signals, we can observe the formation of different institutional communities, each with its unique community structure and characteristics. Using the model, we identify practical strategies platforms can implement, such as reducing exposure to signals from ideological institutions and a tailored approach to content moderation, both of which can rectify the affective polarization problem within its purview.","sentences":["Although there is mounting empirical evidence for the increase in affective polarization, few mechanistic models can explain its emergence at the population level.","The question of how such a phenomenon can emerge from divergent opinions of a population on an ideological issue is still an open issue.","In this paper, we establish that human normativity, that is, individual expression of normative opinions based on beliefs about the population, can lead to population-level polarization when ideological institutions distort beliefs in accordance with their objective of moving expressed opinion to one extreme.","Using a game-theoretic model, we establish that individuals with more extreme opinions will have more extreme rhetoric and higher misperceptions about their outgroup members.","Our model also shows that when social recommendation systems mediate institutional signals, we can observe the formation of different institutional communities, each with its unique community structure and characteristics.","Using the model, we identify practical strategies platforms can implement, such as reducing exposure to signals from ideological institutions and a tailored approach to content moderation, both of which can rectify the affective polarization problem within its purview."],"url":"http://arxiv.org/abs/2403.06264v1","category":"cs.MA"}
{"created":"2024-03-10 16:57:53","title":"ABC-Channel: An Advanced Blockchain-based Covert Channel","abstract":"Establishing efficient and robust covert channels is crucial for secure communication within insecure network environments. With its inherent benefits of decentralization and anonymization, blockchain has gained considerable attention in developing covert channels. To guarantee a highly secure covert channel, channel negotiation should be contactless before the communication, carrier transaction features must be indistinguishable from normal transactions during the communication, and communication identities must be untraceable after the communication. Such a full-lifecycle covert channel is indispensable to defend against a versatile adversary who intercepts two communicating parties comprehensively (e.g., on-chain and off-chain). Unfortunately, it has not been thoroughly investigated in the literature. We make the first effort to achieve a full-lifecycle covert channel, a novel blockchain-based covert channel named ABC-Channel. We tackle a series of challenges, such as off-chain contact dependency, increased masquerading difficulties as growing transaction volume, and time-evolving, communicable yet untraceable identities, to achieve contactless channel negotiation, indistinguishable transaction features, and untraceable communication identities, respectively. We develop a working prototype to validate ABC-Channel and conduct extensive tests on the Bitcoin testnet. The experimental results demonstrate that ABC-Channel achieves substantially secure covert capabilities. In comparison to existing methods, it also exhibits state-of-the-art transmission efficiency.","sentences":["Establishing efficient and robust covert channels is crucial for secure communication within insecure network environments.","With its inherent benefits of decentralization and anonymization, blockchain has gained considerable attention in developing covert channels.","To guarantee a highly secure covert channel, channel negotiation should be contactless before the communication, carrier transaction features must be indistinguishable from normal transactions during the communication, and communication identities must be untraceable after the communication.","Such a full-lifecycle covert channel is indispensable to defend against a versatile adversary who intercepts two communicating parties comprehensively (e.g., on-chain and off-chain).","Unfortunately, it has not been thoroughly investigated in the literature.","We make the first effort to achieve a full-lifecycle covert channel, a novel blockchain-based covert channel named ABC-Channel.","We tackle a series of challenges, such as off-chain contact dependency, increased masquerading difficulties as growing transaction volume, and time-evolving, communicable yet untraceable identities, to achieve contactless channel negotiation, indistinguishable transaction features, and untraceable communication identities, respectively.","We develop a working prototype to validate ABC-Channel and conduct extensive tests on the Bitcoin testnet.","The experimental results demonstrate that ABC-Channel achieves substantially secure covert capabilities.","In comparison to existing methods, it also exhibits state-of-the-art transmission efficiency."],"url":"http://arxiv.org/abs/2403.06261v1","category":"cs.CR"}
{"created":"2024-03-10 16:40:35","title":"mpbn: a simple tool for efficient edition and analysis of elementary properties of Boolean networks","abstract":"The tool mpbn offers a Python programming interface for an easy interactive editing of Boolean networks and the efficient computation of elementary properties of their dynamics, including fixed points, trap spaces, and reachability properties under the Most Permissive update mode. Relying on Answer-Set Programming logical framework, we show that mpbn is scalable to models with several thousands of nodes and is one of the best-performing tool for computing minimal and maximal trap spaces of Boolean networks, a key feature for understanding and controling their stable behaviors. The tool is available at https://github.com/bnediction/mpbn.","sentences":["The tool mpbn offers a Python programming interface for an easy interactive editing of Boolean networks and the efficient computation of elementary properties of their dynamics, including fixed points, trap spaces, and reachability properties under the Most Permissive update mode.","Relying on Answer-Set Programming logical framework, we show that mpbn is scalable to models with several thousands of nodes and is one of the best-performing tool for computing minimal and maximal trap spaces of Boolean networks, a key feature for understanding and controling their stable behaviors.","The tool is available at https://github.com/bnediction/mpbn."],"url":"http://arxiv.org/abs/2403.06255v1","category":"cs.LO"}
{"created":"2024-03-10 16:01:18","title":"Quotient Category of a Multiring Category","abstract":"The aim of this paper is to introduce a tensor structure for the quotient category of an abelian monoidal category with biexact tensor product in order to make the canonical funtor be a monoidal functor. In this tensor product, we prove that the quotient category of a multiring category (resp. a multitensor category) is still a multiring category (resp. a multitensor category).","sentences":["The aim of this paper is to introduce a tensor structure for the quotient category of an abelian monoidal category with biexact tensor product in order to make the canonical funtor be a monoidal functor.","In this tensor product, we prove that the quotient category of a multiring category (resp.","a multitensor category) is still a multiring category (resp.","a multitensor category)."],"url":"http://arxiv.org/abs/2403.06244v1","category":"math.CT"}
{"created":"2024-03-10 15:56:55","title":"BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video Deflickering","abstract":"Developing blind video deflickering (BVD) algorithms to enhance video temporal consistency, is gaining importance amid the flourish of image processing and video generation. However, the intricate nature of video data complicates the training of deep learning methods, leading to high resource consumption and instability, notably under severe lighting flicker. This underscores the critical need for a compact representation beyond pixel values to advance BVD research and applications. Inspired by the classic scale-time equalization (STE), our work introduces the histogram-assisted solution, called BlazeBVD, for high-fidelity and rapid BVD. Compared with STE, which directly corrects pixel values by temporally smoothing color histograms, BlazeBVD leverages smoothed illumination histograms within STE filtering to ease the challenge of learning temporal data using neural networks. In technique, BlazeBVD begins by condensing pixel values into illumination histograms that precisely capture flickering and local exposure variations. These histograms are then smoothed to produce singular frames set, filtered illumination maps, and exposure maps. Resorting to these deflickering priors, BlazeBVD utilizes a 2D network to restore faithful and consistent texture impacted by lighting changes or localized exposure issues. BlazeBVD also incorporates a lightweight 3D network to amend slight temporal inconsistencies, avoiding the resource consumption issue. Comprehensive experiments on synthetic, real-world and generated videos, showcase the superior qualitative and quantitative results of BlazeBVD, achieving inference speeds up to 10x faster than state-of-the-arts.","sentences":["Developing blind video deflickering (BVD) algorithms to enhance video temporal consistency, is gaining importance amid the flourish of image processing and video generation.","However, the intricate nature of video data complicates the training of deep learning methods, leading to high resource consumption and instability, notably under severe lighting flicker.","This underscores the critical need for a compact representation beyond pixel values to advance BVD research and applications.","Inspired by the classic scale-time equalization (STE), our work introduces the histogram-assisted solution, called BlazeBVD, for high-fidelity and rapid BVD.","Compared with STE, which directly corrects pixel values by temporally smoothing color histograms, BlazeBVD leverages smoothed illumination histograms within STE filtering to ease the challenge of learning temporal data using neural networks.","In technique, BlazeBVD begins by condensing pixel values into illumination histograms that precisely capture flickering and local exposure variations.","These histograms are then smoothed to produce singular frames set, filtered illumination maps, and exposure maps.","Resorting to these deflickering priors, BlazeBVD utilizes a 2D network to restore faithful and consistent texture impacted by lighting changes or localized exposure issues.","BlazeBVD also incorporates a lightweight 3D network to amend slight temporal inconsistencies, avoiding the resource consumption issue.","Comprehensive experiments on synthetic, real-world and generated videos, showcase the superior qualitative and quantitative results of BlazeBVD, achieving inference speeds up to 10x faster than state-of-the-arts."],"url":"http://arxiv.org/abs/2403.06243v1","category":"cs.CV"}
{"created":"2024-03-10 15:48:02","title":"TOI-1173 A $b$: The First Inflated Super-Neptune in a Wide Binary System","abstract":"Among Neptunian mass planets exoplanets ($20-50$ M$_\\oplus$), puffy hot Neptunes are extremely rare, and their unique combination of low mass and extended radii implies very low density ($\\rho < 0.3$ g cm$^{-3}$). Over the last decade, only a few puffy planets have been detected and precisely characterized with both transit and radial velocity observations, most notably including WASP-107 $b$, TOI-1420 $b$, and WASP-193 $b$. In this paper, we report the discovery of TOI-1173 A $b$, a low-density ($\\rho = 0.269_{-0.024}^{+0.028}$ g cm$^{-3}$) super-Neptune with $P = 7.06$ days in a nearly circular orbit around the primary G-dwarf star in the wide binary system TOI-1173 A/B. Using radial velocity observations with the MAROON-X spectrograph and transit photometry from TESS, we determined a planet mass of $M_{\\rm{p}} = 26.1\\pm1.9\\ M_{\\oplus}$ and radius of $R_{\\rm{p}} = 8.10\\pm0.17\\ R_{\\oplus}$. TOI-1173 A $b$ is the first puffy Super-Neptune planet detected in a wide binary system (separation $\\sim 11,400$ AU). We explored several mechanisms to understand the puffy nature of TOI-1173 A $b$, and showed that tidal heating is the most promising explanation. Furthermore, we demonstrate that TOI-1173 A $b$ likely has maintained its orbital stability over time and may have undergone von-Zeipel-Lidov-Kozai migration followed by tidal circularization given its present-day architecture, with important implications for planet migration theory and induced engulfment into the host star. Further investigation of the atmosphere of TOI-1173 A $b$ will shed light on the origin of close-in low-density Neptunian planets in field and binary systems, while spin-orbit analyses may elucidate the dynamical evolution of the system.","sentences":["Among Neptunian mass planets exoplanets ($20-50$ M$_\\oplus$), puffy hot Neptunes are extremely rare, and their unique combination of low mass and extended radii implies very low density ($\\rho < 0.3$ g cm$^{-3}$).","Over the last decade, only a few puffy planets have been detected and precisely characterized with both transit and radial velocity observations, most notably including WASP-107 $b$, TOI-1420 $b$, and WASP-193 $b$. In this paper, we report the discovery of TOI-1173 A $b$, a low-density ($\\rho = 0.269_{-0.024}^{+0.028}$ g cm$^{-3}$) super-Neptune with $P = 7.06$ days in a nearly circular orbit around the primary G-dwarf star in the wide binary system TOI-1173 A/B. Using radial velocity observations with the MAROON-X spectrograph and transit photometry from TESS, we determined a planet mass of $M_{\\rm{p}} = 26.1\\pm1.9\\ M_{\\oplus}$ and radius of $R_{\\rm{p}} = 8.10\\pm0.17\\ R_{\\oplus}$. TOI-1173","A $b$ is the first puffy Super-Neptune planet detected in a wide binary system (separation $\\sim 11,400$ AU).","We explored several mechanisms to understand the puffy nature of TOI-1173","A $b$, and showed that tidal heating is the most promising explanation.","Furthermore, we demonstrate that TOI-1173 A $b$ likely has maintained its orbital stability over time and may have undergone von-Zeipel-Lidov-Kozai migration followed by tidal circularization given its present-day architecture, with important implications for planet migration theory and induced engulfment into the host star.","Further investigation of the atmosphere of TOI-1173","A $b$ will shed light on the origin of close-in low-density Neptunian planets in field and binary systems, while spin-orbit analyses may elucidate the dynamical evolution of the system."],"url":"http://arxiv.org/abs/2403.06240v1","category":"astro-ph.EP"}
{"created":"2024-03-10 15:34:48","title":"Quantifying the Uncertainty of Imputed Demographic Disparity Estimates: The Dual-Bootstrap","abstract":"Measuring average differences in an outcome across racial or ethnic groups is a crucial first step for equity assessments, but researchers often lack access to data on individuals' races and ethnicities to calculate them. A common solution is to impute the missing race or ethnicity labels using proxies, then use those imputations to estimate the disparity. Conventional standard errors mischaracterize the resulting estimate's uncertainty because they treat the imputation model as given and fixed, instead of as an unknown object that must be estimated with uncertainty. We propose a dual-bootstrap approach that explicitly accounts for measurement uncertainty and thus enables more accurate statistical inference, which we demonstrate via simulation. In addition, we adapt our approach to the commonly used Bayesian Improved Surname Geocoding (BISG) imputation algorithm, where direct bootstrapping is infeasible because the underlying Census Bureau data are unavailable. In simulations, we find that measurement uncertainty is generally insignificant for BISG except in particular circumstances; bias, not variance, is likely the predominant source of error. We apply our method to quantify the uncertainty of prevalence estimates of common health conditions by race using data from the American Family Cohort.","sentences":["Measuring average differences in an outcome across racial or ethnic groups is a crucial first step for equity assessments, but researchers often lack access to data on individuals' races and ethnicities to calculate them.","A common solution is to impute the missing race or ethnicity labels using proxies, then use those imputations to estimate the disparity.","Conventional standard errors mischaracterize the resulting estimate's uncertainty because they treat the imputation model as given and fixed, instead of as an unknown object that must be estimated with uncertainty.","We propose a dual-bootstrap approach that explicitly accounts for measurement uncertainty and thus enables more accurate statistical inference, which we demonstrate via simulation.","In addition, we adapt our approach to the commonly used Bayesian Improved Surname Geocoding (BISG) imputation algorithm, where direct bootstrapping is infeasible because the underlying Census Bureau data are unavailable.","In simulations, we find that measurement uncertainty is generally insignificant for BISG except in particular circumstances; bias, not variance, is likely the predominant source of error.","We apply our method to quantify the uncertainty of prevalence estimates of common health conditions by race using data from the American Family Cohort."],"url":"http://arxiv.org/abs/2403.06238v1","category":"stat.ME"}
{"created":"2024-03-10 15:01:50","title":"LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding Linear Bandit Problem","abstract":"In this study, we delve into the Thresholding Linear Bandit (TLB) problem, a nuanced domain within stochastic Multi-Armed Bandit (MAB) problems, focusing on maximizing decision accuracy against a linearly defined threshold under resource constraints. We present LinearAPT, a novel algorithm designed for the fixed budget setting of TLB, providing an efficient solution to optimize sequential decision-making. This algorithm not only offers a theoretical upper bound for estimated loss but also showcases robust performance on both synthetic and real-world datasets. Our contributions highlight the adaptability, simplicity, and computational efficiency of LinearAPT, making it a valuable addition to the toolkit for addressing complex sequential decision-making challenges.","sentences":["In this study, we delve into the Thresholding Linear Bandit (TLB) problem, a nuanced domain within stochastic Multi-Armed Bandit (MAB) problems, focusing on maximizing decision accuracy against a linearly defined threshold under resource constraints.","We present LinearAPT, a novel algorithm designed for the fixed budget setting of TLB, providing an efficient solution to optimize sequential decision-making.","This algorithm not only offers a theoretical upper bound for estimated loss but also showcases robust performance on both synthetic and real-world datasets.","Our contributions highlight the adaptability, simplicity, and computational efficiency of LinearAPT, making it a valuable addition to the toolkit for addressing complex sequential decision-making challenges."],"url":"http://arxiv.org/abs/2403.06230v1","category":"cs.LG"}
{"created":"2024-03-10 14:07:52","title":"Imaginary gap-closed points and non-Hermitian dynamics in a class of dissipative systems","abstract":"We investigate imaginary gap-closed (IGC) points and their associated dynamics in dissipative systems. In a general non-Hermitian model, we derive the equation governing the IGC points of the energy spectrum, establishing that these points are only determined by the Hermitian part of the Hamiltonian. Focusing on a class of one-dimensional dissipative chains, we explore quantum walks across different scenarios and various parameters, showing that IGC points induce a power-law decay scaling in bulk loss probability and trigger a boundary phenomenon referred to as \"edge burst\". This observation underscores the crucial role of IGC points under periodic boundary conditions (PBCs) in shaping quantum walk dynamics. Finally, we demonstrate that the damping matrices of these dissipative chains under PBCs possess Liouvillian gapless points, implying an algebraic convergence towards the steady state in long-time dynamics.","sentences":["We investigate imaginary gap-closed (IGC) points and their associated dynamics in dissipative systems.","In a general non-Hermitian model, we derive the equation governing the IGC points of the energy spectrum, establishing that these points are only determined by the Hermitian part of the Hamiltonian.","Focusing on a class of one-dimensional dissipative chains, we explore quantum walks across different scenarios and various parameters, showing that IGC points induce a power-law decay scaling in bulk loss probability and trigger a boundary phenomenon referred to as \"edge burst\".","This observation underscores the crucial role of IGC points under periodic boundary conditions (PBCs) in shaping quantum walk dynamics.","Finally, we demonstrate that the damping matrices of these dissipative chains under PBCs possess Liouvillian gapless points, implying an algebraic convergence towards the steady state in long-time dynamics."],"url":"http://arxiv.org/abs/2403.06224v1","category":"quant-ph"}
{"created":"2024-03-10 13:59:18","title":"Robust Predictive Motion Planning by Learning Obstacle Uncertainty","abstract":"Safe motion planning for robotic systems in dynamic environments is nontrivial in the presence of uncertain obstacles, where estimation of obstacle uncertainties is crucial in predicting future motions of dynamic obstacles. The worst-case characterization gives a conservative uncertainty prediction and may result in infeasible motion planning for the ego robotic system. In this paper, an efficient, robust, and safe motion-planing algorithm is developed by learning the obstacle uncertainties online. More specifically, the unknown yet intended control set of obstacles is efficiently computed by solving a linear programming problem. The learned control set is used to compute forward reachable sets of obstacles that are less conservative than the worst-case prediction. Based on the forward prediction, a robust model predictive controller is designed to compute a safe reference trajectory for the ego robotic system that remains outside the reachable sets of obstacles over the prediction horizon. The method is applied to a car-like mobile robot in both simulations and hardware experiments to demonstrate its effectiveness.","sentences":["Safe motion planning for robotic systems in dynamic environments is nontrivial in the presence of uncertain obstacles, where estimation of obstacle uncertainties is crucial in predicting future motions of dynamic obstacles.","The worst-case characterization gives a conservative uncertainty prediction and may result in infeasible motion planning for the ego robotic system.","In this paper, an efficient, robust, and safe motion-planing algorithm is developed by learning the obstacle uncertainties online.","More specifically, the unknown yet intended control set of obstacles is efficiently computed by solving a linear programming problem.","The learned control set is used to compute forward reachable sets of obstacles that are less conservative than the worst-case prediction.","Based on the forward prediction, a robust model predictive controller is designed to compute a safe reference trajectory for the ego robotic system that remains outside the reachable sets of obstacles over the prediction horizon.","The method is applied to a car-like mobile robot in both simulations and hardware experiments to demonstrate its effectiveness."],"url":"http://arxiv.org/abs/2403.06222v1","category":"cs.RO"}
{"created":"2024-03-10 13:52:15","title":"Affine Semigroup Algebras And Their Fibered Sums","abstract":"We study affine semigroup rings as algebras over subsemigroup rings. From this relative viewpoint with respect to a given subsemigroup ring, the fibered sum of two affine semigroup algebras is constructed. Such a construction is compared to the tensor product and to the classical gluings of affine semigroup rings as defined in Rosales (1997).   While fibered sum can always be achieved, gluings of affine semigroup rings do not always exist. Therefore, we further investigate when the fibered sum of affine semigroup algebras gives rise to a gluing. A criterion is recovered in terms of the defining semigroups under which the gluing may take place.","sentences":["We study affine semigroup rings as algebras over subsemigroup rings.","From this relative viewpoint with respect to a given subsemigroup ring, the fibered sum of two affine semigroup algebras is constructed.","Such a construction is compared to the tensor product and to the classical gluings of affine semigroup rings as defined in Rosales (1997).   ","While fibered sum can always be achieved, gluings of affine semigroup rings do not always exist.","Therefore, we further investigate when the fibered sum of affine semigroup algebras gives rise to a gluing.","A criterion is recovered in terms of the defining semigroups under which the gluing may take place."],"url":"http://arxiv.org/abs/2403.06219v1","category":"math.AC"}
{"created":"2024-03-10 13:19:16","title":"AdaFold: Adapting Folding Trajectories of Cloths via Feedback-loop Manipulation","abstract":"We present AdaFold, a model-based feedback-loop framework for optimizing folding trajectories. AdaFold extracts a particle-based representation of cloth from RGB-D images and feeds back the representation to a model predictive control to re-plan folding trajectory at every time-step. A key component of AdaFold that enables feedback-loop manipulation is the use of semantic descriptors extracted from visual-language models. These descriptors enhance the particle representation of the cloth to distinguish between ambiguous point clouds of differently folded cloths. Our experiments demonstrate AdaFold's ability to adapt folding trajectories to cloths with varying physical properties and generalize from simulated training to real-world execution.","sentences":["We present AdaFold, a model-based feedback-loop framework for optimizing folding trajectories.","AdaFold extracts a particle-based representation of cloth from RGB-D images and feeds back the representation to a model predictive control to re-plan folding trajectory at every time-step.","A key component of AdaFold that enables feedback-loop manipulation is the use of semantic descriptors extracted from visual-language models.","These descriptors enhance the particle representation of the cloth to distinguish between ambiguous point clouds of differently folded cloths.","Our experiments demonstrate AdaFold's ability to adapt folding trajectories to cloths with varying physical properties and generalize from simulated training to real-world execution."],"url":"http://arxiv.org/abs/2403.06210v1","category":"cs.RO"}
{"created":"2024-03-10 12:57:42","title":"Constraining dark matter model using 21cm line intensity mapping","abstract":"We apply the Convolutional Neural Networks (CNNs) to the mock 21cm maps from the post-reionization epoch to show that the $\\Lambda$CDM and warm dark matter (WDM) model can be distinguished for WDM particle masses $m_{FD}<3$ keV, under the assumption of thermal production of WDM following the Fermi-Dirac (FD) distribution. We demonstrate that the CNN is a potent tool in distinguishing the dark matter masses, highlighting its sensitivity to the subtle differences in the 21cm maps produced by varying dark matter masses. Furthermore, we extend our analysis to encompass different WDM production mechanisms, recognizing that the dark matter production mechanism in the early Universe is among the sources of the most significant uncertainty for the dark matter model building.   In this work, given the mass of the dark matter, we discuss the feasibility of discriminating four different WDM models: Fermi-Dirac (FD) distribution model, neutrino Minimal Standard Model ($\\nu$MSM), Dodelson-Widrow (DW), and Shi-Fuller (SF) model. For instance, when the WDM mass is 2 keV, we show that one can differentiate between CDM, FD, $\\nu$MSM, and DW models while discerning between the DW and SF models turns out to be challenging. Our results reinforce the viability of the CNN as a robust analysis for 21cm maps and shed light on its potential to unravel the features associated with different dark matter production mechanisms.","sentences":["We apply the Convolutional Neural Networks (CNNs) to the mock 21cm maps from the post-reionization epoch to show that the $\\Lambda$CDM and warm dark matter (WDM) model can be distinguished for WDM particle masses $m_{FD}<3$ keV, under the assumption of thermal production of WDM following the Fermi-Dirac (FD) distribution.","We demonstrate that the CNN is a potent tool in distinguishing the dark matter masses, highlighting its sensitivity to the subtle differences in the 21cm maps produced by varying dark matter masses.","Furthermore, we extend our analysis to encompass different WDM production mechanisms, recognizing that the dark matter production mechanism in the early Universe is among the sources of the most significant uncertainty for the dark matter model building.   ","In this work, given the mass of the dark matter, we discuss the feasibility of discriminating four different WDM models: Fermi-Dirac (FD) distribution model, neutrino Minimal Standard Model ($\\nu$MSM), Dodelson-Widrow (DW), and Shi-Fuller (SF) model.","For instance, when the WDM mass is 2 keV, we show that one can differentiate between CDM, FD, $\\nu$MSM, and DW models while discerning between the DW and SF models turns out to be challenging.","Our results reinforce the viability of the CNN as a robust analysis for 21cm maps and shed light on its potential to unravel the features associated with different dark matter production mechanisms."],"url":"http://arxiv.org/abs/2403.06203v1","category":"astro-ph.CO"}
{"created":"2024-03-10 12:52:31","title":"Pursuit Winning Strategies for Reach-Avoid Games with Polygonal Obstacles","abstract":"This paper studies a multiplayer reach-avoid differential game in the presence of general polygonal obstacles that block the players' motions. The pursuers cooperate to protect a convex region from the evaders who try to reach the region. We propose a multiplayer onsite and close-to-goal (MOCG) pursuit strategy that can tell and achieve an increasing lower bound on the number of guaranteed defeated evaders. This pursuit strategy fuses the subgame outcomes for multiple pursuers against one evader with hierarchical optimal task allocation in the receding-horizon manner. To determine the qualitative subgame outcomes that who is the game winner, we construct three pursuit winning regions and strategies under which the pursuers guarantee to win against the evader, regardless of the unknown evader strategy. First, we utilize the expanded Apollonius circles and propose the onsite pursuit winning that achieves the capture in finite time. Second, we introduce convex goal-covering polygons (GCPs) and propose the close-to-goal pursuit winning for the pursuers whose visibility region contains the whole protected region, and the goal-visible property will be preserved afterwards. Third, we employ Euclidean shortest paths (ESPs) and construct a pursuit winning region and strategy for the non-goal-visible pursuers, where the pursuers are firstly steered to positions with goal visibility along ESPs. In each horizon, the hierarchical optimal task allocation maximizes the number of defeated evaders and consists of four sequential matchings: capture, enhanced, non-dominated and closest matchings. Numerical examples are presented to illustrate the results.","sentences":["This paper studies a multiplayer reach-avoid differential game in the presence of general polygonal obstacles that block the players' motions.","The pursuers cooperate to protect a convex region from the evaders who try to reach the region.","We propose a multiplayer onsite and close-to-goal (MOCG) pursuit strategy that can tell and achieve an increasing lower bound on the number of guaranteed defeated evaders.","This pursuit strategy fuses the subgame outcomes for multiple pursuers against one evader with hierarchical optimal task allocation in the receding-horizon manner.","To determine the qualitative subgame outcomes that who is the game winner, we construct three pursuit winning regions and strategies under which the pursuers guarantee to win against the evader, regardless of the unknown evader strategy.","First, we utilize the expanded Apollonius circles and propose the onsite pursuit winning that achieves the capture in finite time.","Second, we introduce convex goal-covering polygons (GCPs) and propose the close-to-goal pursuit winning for the pursuers whose visibility region contains the whole protected region, and the goal-visible property will be preserved afterwards.","Third, we employ Euclidean shortest paths (ESPs) and construct a pursuit winning region and strategy for the non-goal-visible pursuers, where the pursuers are firstly steered to positions with goal visibility along ESPs.","In each horizon, the hierarchical optimal task allocation maximizes the number of defeated evaders and consists of four sequential matchings: capture, enhanced, non-dominated and closest matchings.","Numerical examples are presented to illustrate the results."],"url":"http://arxiv.org/abs/2403.06202v1","category":"eess.SY"}
{"created":"2024-03-10 12:46:33","title":"SuPRA: Surgical Phase Recognition and Anticipation for Intra-Operative Planning","abstract":"Intra-operative recognition of surgical phases holds significant potential for enhancing real-time contextual awareness in the operating room. However, we argue that online recognition, while beneficial, primarily lends itself to post-operative video analysis due to its limited direct impact on the actual surgical decisions and actions during ongoing procedures. In contrast, we contend that the prediction and anticipation of surgical phases are inherently more valuable for intra-operative assistance, as they can meaningfully influence a surgeon's immediate and long-term planning by providing foresight into future steps. To address this gap, we propose a dual approach that simultaneously recognises the current surgical phase and predicts upcoming ones, thus offering comprehensive intra-operative assistance and guidance on the expected remaining workflow. Our novel method, Surgical Phase Recognition and Anticipation (SuPRA), leverages past and current information for accurate intra-operative phase recognition while using future segments for phase prediction. This unified approach challenges conventional frameworks that treat these objectives separately. We have validated SuPRA on two reputed datasets, Cholec80 and AutoLaparo21, where it demonstrated state-of-the-art performance with recognition accuracies of 91.8% and 79.3%, respectively. Additionally, we introduce and evaluate our model using new segment-level evaluation metrics, namely Edit and F1 Overlap scores, for a more temporal assessment of segment classification. In conclusion, SuPRA presents a new multi-task approach that paves the way for improved intra-operative assistance through surgical phase recognition and prediction of future events.","sentences":["Intra-operative recognition of surgical phases holds significant potential for enhancing real-time contextual awareness in the operating room.","However, we argue that online recognition, while beneficial, primarily lends itself to post-operative video analysis due to its limited direct impact on the actual surgical decisions and actions during ongoing procedures.","In contrast, we contend that the prediction and anticipation of surgical phases are inherently more valuable for intra-operative assistance, as they can meaningfully influence a surgeon's immediate and long-term planning by providing foresight into future steps.","To address this gap, we propose a dual approach that simultaneously recognises the current surgical phase and predicts upcoming ones, thus offering comprehensive intra-operative assistance and guidance on the expected remaining workflow.","Our novel method, Surgical Phase Recognition and Anticipation (SuPRA), leverages past and current information for accurate intra-operative phase recognition while using future segments for phase prediction.","This unified approach challenges conventional frameworks that treat these objectives separately.","We have validated SuPRA on two reputed datasets, Cholec80 and AutoLaparo21, where it demonstrated state-of-the-art performance with recognition accuracies of 91.8% and 79.3%, respectively.","Additionally, we introduce and evaluate our model using new segment-level evaluation metrics, namely Edit and F1 Overlap scores, for a more temporal assessment of segment classification.","In conclusion, SuPRA presents a new multi-task approach that paves the way for improved intra-operative assistance through surgical phase recognition and prediction of future events."],"url":"http://arxiv.org/abs/2403.06200v1","category":"cs.CV"}
{"created":"2024-03-10 12:41:34","title":"DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency","abstract":"The combination of electronic health records (EHR) and medical images is crucial for clinicians in making diagnoses and forecasting prognosis. Strategically fusing these two data modalities has great potential to improve the accuracy of machine learning models in clinical prediction tasks. However, the asynchronous and complementary nature of EHR and medical images presents unique challenges. Missing modalities due to clinical and administrative factors are inevitable in practice, and the significance of each data modality varies depending on the patient and the prediction target, resulting in inconsistent predictions and suboptimal model performance. To address these challenges, we propose DrFuse to achieve effective clinical multi-modal fusion. It tackles the missing modality issue by disentangling the features shared across modalities and those unique within each modality. Furthermore, we address the modal inconsistency issue via a disease-wise attention layer that produces the patient- and disease-wise weighting for each modality to make the final prediction. We validate the proposed method using real-world large-scale datasets, MIMIC-IV and MIMIC-CXR. Experimental results show that the proposed method significantly outperforms the state-of-the-art models. Our implementation is publicly available at https://github.com/dorothy-yao/drfuse.","sentences":["The combination of electronic health records (EHR) and medical images is crucial for clinicians in making diagnoses and forecasting prognosis.","Strategically fusing these two data modalities has great potential to improve the accuracy of machine learning models in clinical prediction tasks.","However, the asynchronous and complementary nature of EHR and medical images presents unique challenges.","Missing modalities due to clinical and administrative factors are inevitable in practice, and the significance of each data modality varies depending on the patient and the prediction target, resulting in inconsistent predictions and suboptimal model performance.","To address these challenges, we propose DrFuse to achieve effective clinical multi-modal fusion.","It tackles the missing modality issue by disentangling the features shared across modalities and those unique within each modality.","Furthermore, we address the modal inconsistency issue via a disease-wise attention layer that produces the patient- and disease-wise weighting for each modality to make the final prediction.","We validate the proposed method using real-world large-scale datasets, MIMIC-IV and MIMIC-CXR.","Experimental results show that the proposed method significantly outperforms the state-of-the-art models.","Our implementation is publicly available at https://github.com/dorothy-yao/drfuse."],"url":"http://arxiv.org/abs/2403.06197v1","category":"eess.IV"}
{"created":"2024-03-10 12:38:38","title":"F\u00edsica Estad\u00edstica: teor\u00eda, ejemplos trabajados, modelos y simulaciones","abstract":"In this book, we study Statistical Physics under conditions of thermodynamic equilibrium, starting from the definition of statistical ensembles. The book is divided into five chapters: First, a brief introduction to statistical methods. Second, the statistical description of isolated systems, corresponding to microcanonical ensembles. Third, the statistical description of systems in contact with a heat reservoir at a constant temperature T, known as the canonical ensemble. Fourth, the description of systems in contact with a heat reservoir at temperature T and with a constant chemical potential {\\mu}; the grand canonical ensemble. And finally, quantum statistics. In each chapter, a theoretical description, worked examples, links to simulations (see GitHub repository: https://github.com/davidalejandromiranda/StatisticalPhysics) and discussions on recent scientific reports will be presented so readers can complement their studies.","sentences":["In this book, we study Statistical Physics under conditions of thermodynamic equilibrium, starting from the definition of statistical ensembles.","The book is divided into five chapters:","First, a brief introduction to statistical methods.","Second, the statistical description of isolated systems, corresponding to microcanonical ensembles.","Third, the statistical description of systems in contact with a heat reservoir at a constant temperature T, known as the canonical ensemble.","Fourth, the description of systems in contact with a heat reservoir at temperature T and with a constant chemical potential {\\mu}; the grand canonical ensemble.","And finally, quantum statistics.","In each chapter, a theoretical description, worked examples, links to simulations (see GitHub repository: https://github.com/davidalejandromiranda/StatisticalPhysics) and discussions on recent scientific reports will be presented so readers can complement their studies."],"url":"http://arxiv.org/abs/2403.06195v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-10 12:33:12","title":"On depth prediction for autonomous driving using self-supervised learning","abstract":"Perception of the environment is a critical component for enabling autonomous driving. It provides the vehicle with the ability to comprehend its surroundings and make informed decisions. Depth prediction plays a pivotal role in this process, as it helps the understanding of the geometry and motion of the environment. This thesis focuses on the challenge of depth prediction using monocular self-supervised learning techniques. The problem is approached from a broader perspective first, exploring conditional generative adversarial networks (cGANs) as a potential technique to achieve better generalization was performed. In doing so, a fundamental contribution to the conditional GANs, the acontrario cGAN was proposed. The second contribution entails a single image-to-depth self-supervised method, proposing a solution for the rigid-scene assumption using a novel transformer-based method that outputs a pose for each dynamic object. The third significant aspect involves the introduction of a video-to-depth map forecasting approach. This method serves as an extension of self-supervised techniques to predict future depths. This involves the creation of a novel transformer model capable of predicting the future depth of a given scene. Moreover, the various limitations of the aforementioned methods were addressed and a video-to-video depth maps model was proposed. This model leverages the spatio-temporal consistency of the input and output sequence to predict a more accurate depth sequence output. These methods have significant applications in autonomous driving (AD) and advanced driver assistance systems (ADAS).","sentences":["Perception of the environment is a critical component for enabling autonomous driving.","It provides the vehicle with the ability to comprehend its surroundings and make informed decisions.","Depth prediction plays a pivotal role in this process, as it helps the understanding of the geometry and motion of the environment.","This thesis focuses on the challenge of depth prediction using monocular self-supervised learning techniques.","The problem is approached from a broader perspective first, exploring conditional generative adversarial networks (cGANs) as a potential technique to achieve better generalization was performed.","In doing so, a fundamental contribution to the conditional GANs, the acontrario cGAN was proposed.","The second contribution entails a single image-to-depth self-supervised method, proposing a solution for the rigid-scene assumption using a novel transformer-based method that outputs a pose for each dynamic object.","The third significant aspect involves the introduction of a video-to-depth map forecasting approach.","This method serves as an extension of self-supervised techniques to predict future depths.","This involves the creation of a novel transformer model capable of predicting the future depth of a given scene.","Moreover, the various limitations of the aforementioned methods were addressed and a video-to-video depth maps model was proposed.","This model leverages the spatio-temporal consistency of the input and output sequence to predict a more accurate depth sequence output.","These methods have significant applications in autonomous driving (AD) and advanced driver assistance systems (ADAS)."],"url":"http://arxiv.org/abs/2403.06194v1","category":"cs.CV"}
{"created":"2024-03-10 12:19:06","title":"Limit theorems for SDEs with irregular drifts","abstract":"In this paper, concerning SDEs with H\\\"older continuous drifts, which are merely dissipative at infinity, and SDEs with piecewise continuous drifts, we investigate the strong law of large numbers and the central limit theorem for underlying additive functionals and reveal the corresponding rates of convergence. To establish the limit theorems under consideration, the exponentially contractive property of solution processes under the (quasi-)Wasserstein distance plays an indispensable role. In order to achieve such contractive property, which is new and interesting in its own right for SDEs with H\\\"older continuous drifts or piecewise continuous drifts, the reflection coupling method is employed and meanwhile a sophisticated test function is built.","sentences":["In this paper, concerning SDEs with H\\\"older continuous drifts, which are merely dissipative at infinity, and SDEs with piecewise continuous drifts, we investigate the strong law of large numbers and the central limit theorem for underlying additive functionals and reveal the corresponding rates of convergence.","To establish the limit theorems under consideration, the exponentially contractive property of solution processes under the (quasi-)Wasserstein distance plays an indispensable role.","In order to achieve such contractive property, which is new and interesting in its own right for SDEs with H\\\"older continuous drifts or piecewise continuous drifts, the reflection coupling method is employed and meanwhile a sophisticated test function is built."],"url":"http://arxiv.org/abs/2403.06192v1","category":"math.PR"}
{"created":"2024-03-10 12:10:27","title":"On Geometrically Convex Risk Measures","abstract":"Geometrically convex functions constitute an interesting class of functions obtained by replacing the arithmetic mean with the geometric mean in the definition of convexity. As recently suggested, geometric convexity may be a sensible property for financial risk measures ([7,13,4]).   We introduce a notion of GG-convex conjugate, parallel to the classical notion of convex conjugate introduced by Fenchel, and we discuss its properties. We show how GG-convex conjugation can be axiomatized in the spirit of the notion of general duality transforms introduced in [2,3].   We then move to the study of GG-convex risk measures, which are defined as GG-convex functionals defined on suitable spaces of random variables. We derive a general dual representation that extends analogous expressions presented in [4] under the additional assumptions of monotonicity and positive homogeneity. As a prominent example, we study the family of Orlicz risk measures. Finally, we introduce multiplicative versions of the convex and of the increasing convex order and discuss related consistency properties of law-invariant GG-convex risk measures.","sentences":["Geometrically convex functions constitute an interesting class of functions obtained by replacing the arithmetic mean with the geometric mean in the definition of convexity.","As recently suggested, geometric convexity may be a sensible property for financial risk measures ([7,13,4]).   ","We introduce a notion of GG-convex conjugate, parallel to the classical notion of convex conjugate introduced by Fenchel, and we discuss its properties.","We show how GG-convex conjugation can be axiomatized in the spirit of the notion of general duality transforms introduced in [2,3].   ","We then move to the study of GG-convex risk measures, which are defined as GG-convex functionals defined on suitable spaces of random variables.","We derive a general dual representation that extends analogous expressions presented in [4] under the additional assumptions of monotonicity and positive homogeneity.","As a prominent example, we study the family of Orlicz risk measures.","Finally, we introduce multiplicative versions of the convex and of the increasing convex order and discuss related consistency properties of law-invariant GG-convex risk measures."],"url":"http://arxiv.org/abs/2403.06188v1","category":"q-fin.RM"}
{"created":"2024-03-10 12:05:50","title":"Quantized Constant-Envelope Waveform Design for Massive MIMO DFRC Systems","abstract":"Both dual-functional radar-communication (DFRC) and massive multiple-input multiple-output (MIMO) have been recognized as enabling technologies for 6G wireless networks. This paper considers the advanced waveform design for hardware-efficient massive MIMO DFRC systems. Specifically, the transmit waveform is imposed with the quantized constant-envelope (QCE) constraint, which facilitates the employment of low-resolution digital-to-analog converters (DACs) and power-efficient amplifiers. The waveform design problem is formulated as the minimization of the mean square error (MSE) between the designed and desired beampatterns subject to the constructive interference (CI)-based communication quality of service (QoS) constraints and the QCE constraint. To solve the formulated problem, we first utilize the penalty technique to transform the discrete problem into an equivalent continuous penalty model. Then, we propose an inexact augmented Lagrangian method (ALM) algorithm for solving the penalty model. In particular, the ALM subproblem at each iteration is solved by a custom-built block successive upper-bound minimization (BSUM) algorithm, which admits closed-form updates, making the proposed inexact ALM algorithm computationally efficient. Simulation results demonstrate the superiority of the proposed approach over existing state-of-the-art ones. In addition, extensive simulations are conducted to examine the impact of various system parameters on the trade-off between communication and radar performances.","sentences":["Both dual-functional radar-communication (DFRC) and massive multiple-input multiple-output (MIMO) have been recognized as enabling technologies for 6G wireless networks.","This paper considers the advanced waveform design for hardware-efficient massive MIMO DFRC systems.","Specifically, the transmit waveform is imposed with the quantized constant-envelope (QCE) constraint, which facilitates the employment of low-resolution digital-to-analog converters (DACs) and power-efficient amplifiers.","The waveform design problem is formulated as the minimization of the mean square error (MSE) between the designed and desired beampatterns subject to the constructive interference (CI)-based communication quality of service (QoS) constraints and the QCE constraint.","To solve the formulated problem, we first utilize the penalty technique to transform the discrete problem into an equivalent continuous penalty model.","Then, we propose an inexact augmented Lagrangian method (ALM) algorithm for solving the penalty model.","In particular, the ALM subproblem at each iteration is solved by a custom-built block successive upper-bound minimization (BSUM) algorithm, which admits closed-form updates, making the proposed inexact ALM algorithm computationally efficient.","Simulation results demonstrate the superiority of the proposed approach over existing state-of-the-art ones.","In addition, extensive simulations are conducted to examine the impact of various system parameters on the trade-off between communication and radar performances."],"url":"http://arxiv.org/abs/2403.06185v1","category":"cs.IT"}
{"created":"2024-03-10 11:49:49","title":"Hodge Theorem for Krein-Feller operators on compact Riemannian manifolds","abstract":"For an open set in a compact smooth oriented Riemannian n-manifold and a positive finite Borel measure with support contained in the closure of the open set, we define an associated Krein-Feller operator on k-forms by assuming the Poincare inequality. Krein-Feller operators on Euclidean space have been studied extensively in fractal geometry. Using results established by the authors, we obtain sufficient conditions for Krein-Feller operator to have compact resolvent. Under these conditions, we prove the Hodge theorem for forms, which states that there exists an orthonormal basis of L2 consisting of eigenforms of Krein-Feller operator, the eigenspaces are finite-dimensional, and the eigenvalues of Krein-Feller operator are real, countable, and increasing to infinity. One of these sufficient conditions is that the dimension of the measure is greater than n-2. Our result extends the classical Hodge theorem to Krein-Feller operators.","sentences":["For an open set in a compact smooth oriented Riemannian n-manifold and a positive finite Borel measure with support contained in the closure of the open set, we define an associated Krein-Feller operator on k-forms by assuming the Poincare inequality.","Krein-Feller operators on Euclidean space have been studied extensively in fractal geometry.","Using results established by the authors, we obtain sufficient conditions for Krein-Feller operator to have compact resolvent.","Under these conditions, we prove the Hodge theorem for forms, which states that there exists an orthonormal basis of L2 consisting of eigenforms of Krein-Feller operator, the eigenspaces are finite-dimensional, and the eigenvalues of Krein-Feller operator are real, countable, and increasing to infinity.","One of these sufficient conditions is that the dimension of the measure is greater than n-2.","Our result extends the classical Hodge theorem to Krein-Feller operators."],"url":"http://arxiv.org/abs/2403.06182v1","category":"math.FA"}
{"created":"2024-03-10 11:24:08","title":"Coalgebraic Modal Logic for Dynamic Systems with Uncertainty","abstract":"In this paper we define a class of polynomial functors suited for constructing coalgebras representing processes in which uncertainty plays an important role. In these polynomial functors we include upper and lower probability measures, finitely additive probability measures, plausibilty measures (and their duals, belief functions), and possibility measures. We give axioms and inference rules for the associated system of coalgebraic modal logic, and construct the canonical coalgebras to prove a completeness result.","sentences":["In this paper we define a class of polynomial functors suited for constructing coalgebras representing processes in which uncertainty plays an important role.","In these polynomial functors we include upper and lower probability measures, finitely additive probability measures, plausibilty measures (and their duals, belief functions), and possibility measures.","We give axioms and inference rules for the associated system of coalgebraic modal logic, and construct the canonical coalgebras to prove a completeness result."],"url":"http://arxiv.org/abs/2403.06177v1","category":"math.LO"}
{"created":"2024-03-10 10:54:44","title":"Understanding Parents' Perceptions and Practices Toward Children's Security and Privacy in Virtual Reality","abstract":"Recent years have seen a sharp increase in underage users of virtual reality (VR), where security and privacy (S\\&P) risks such as data surveillance and self-disclosure in social interaction have been increasingly prominent. Prior work shows children largely rely on parents to mitigate S\\&P risks in their technology use. Therefore, understanding parents' S\\&P knowledge, perceptions, and practices is critical for identifying the gaps for parents, technology designers, and policymakers to enhance children's S\\&P. While such empirical knowledge is substantial in other consumer technologies, it remains largely unknown in the context of VR. To address the gap, we conducted in-depth semi-structured interviews with 20 parents of children under the age of 18 who use VR at home. Our findings highlight parents generally lack S\\&P awareness due to the perception that VR is still in its infancy. To protect their children's interaction with VR, parents currently primarily rely on active strategies such as verbal education about S\\&P. Passive strategies such as parental controls in VR are not commonly used among our interviewees, mainly due to their perceived technical constraints. Parents also highlight that a multi-stakeholder ecosystem must be established towards more S\\&P support for children in VR. Based on the findings, we propose actionable S\\&P recommendations for critical stakeholders, including parents, educators, VR companies, and governments.","sentences":["Recent years have seen a sharp increase in underage users of virtual reality (VR), where security and privacy (S\\&P) risks such as data surveillance and self-disclosure in social interaction have been increasingly prominent.","Prior work shows children largely rely on parents to mitigate S\\&P risks in their technology use.","Therefore, understanding parents' S\\&P knowledge, perceptions, and practices is critical for identifying the gaps for parents, technology designers, and policymakers to enhance children's S\\&P. While such empirical knowledge is substantial in other consumer technologies, it remains largely unknown in the context of VR.","To address the gap, we conducted in-depth semi-structured interviews with 20 parents of children under the age of 18 who use VR at home.","Our findings highlight parents generally lack S\\&P awareness due to the perception that VR is still in its infancy.","To protect their children's interaction with VR, parents currently primarily rely on active strategies such as verbal education about S\\&P. Passive strategies such as parental controls in VR are not commonly used among our interviewees, mainly due to their perceived technical constraints.","Parents also highlight that a multi-stakeholder ecosystem must be established towards more S\\&P support for children in VR.","Based on the findings, we propose actionable S\\&P recommendations for critical stakeholders, including parents, educators, VR companies, and governments."],"url":"http://arxiv.org/abs/2403.06172v1","category":"cs.HC"}
{"created":"2024-03-10 10:36:39","title":"Direct Shooting Method for Numerical Optimal Control: A Modified Transcription Approach","abstract":"Direct shooting is an efficient method to solve numerical optimal control. It utilizes the Runge-Kutta scheme to discretize a continuous-time optimal control problem making the problem solvable by nonlinear programming solvers. However, conventional direct shooting raises a contradictory dynamics issue when using an augmented state to handle {high-order} systems. This paper fills the research gap by considering the direct shooting method for {high-order} systems. We derive the modified Euler and Runge-Kutta-4 methods to transcribe the system dynamics constraint directly. Additionally, we provide the global error upper bounds of our proposed methods. A set of benchmark optimal control problems shows that our methods provide more accurate solutions than existing approaches.","sentences":["Direct shooting is an efficient method to solve numerical optimal control.","It utilizes the Runge-Kutta scheme to discretize a continuous-time optimal control problem making the problem solvable by nonlinear programming solvers.","However, conventional direct shooting raises a contradictory dynamics issue when using an augmented state to handle {high-order} systems.","This paper fills the research gap by considering the direct shooting method for {high-order} systems.","We derive the modified Euler and Runge-Kutta-4 methods to transcribe the system dynamics constraint directly.","Additionally, we provide the global error upper bounds of our proposed methods.","A set of benchmark optimal control problems shows that our methods provide more accurate solutions than existing approaches."],"url":"http://arxiv.org/abs/2403.06167v1","category":"eess.SY"}
{"created":"2024-03-11 17:59:31","title":"BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion","abstract":"Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of diffusion models (DMs). Despite these advancements, current DM adaptations for inpainting, which involve modifications to the sampling strategy or the development of inpainting-specific DMs, frequently suffer from semantic inconsistencies and reduced image quality. Addressing these challenges, our work introduces a novel paradigm: the division of masked image features and noisy latent into separate branches. This division dramatically diminishes the model's learning load, facilitating a nuanced incorporation of essential masked image information in a hierarchical fashion. Herein, we present BrushNet, a novel plug-and-play dual-branch model engineered to embed pixel-level masked image features into any pre-trained DM, guaranteeing coherent and enhanced image inpainting outcomes. Additionally, we introduce BrushData and BrushBench to facilitate segmentation-based inpainting training and performance assessment. Our extensive experimental analysis demonstrates BrushNet's superior performance over existing models across seven key metrics, including image quality, mask region preservation, and textual coherence.","sentences":["Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of diffusion models (DMs).","Despite these advancements, current DM adaptations for inpainting, which involve modifications to the sampling strategy or the development of inpainting-specific DMs, frequently suffer from semantic inconsistencies and reduced image quality.","Addressing these challenges, our work introduces a novel paradigm: the division of masked image features and noisy latent into separate branches.","This division dramatically diminishes the model's learning load, facilitating a nuanced incorporation of essential masked image information in a hierarchical fashion.","Herein, we present BrushNet, a novel plug-and-play dual-branch model engineered to embed pixel-level masked image features into any pre-trained DM, guaranteeing coherent and enhanced image inpainting outcomes.","Additionally, we introduce BrushData and BrushBench to facilitate segmentation-based inpainting training and performance assessment.","Our extensive experimental analysis demonstrates BrushNet's superior performance over existing models across seven key metrics, including image quality, mask region preservation, and textual coherence."],"url":"http://arxiv.org/abs/2403.06976v1","category":"cs.CV"}
{"created":"2024-03-11 17:33:12","title":"Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation","abstract":"Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task. Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities. In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation. Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP's features into language-associated and vision-associated components. Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances. We align features across domains using a modality discriminator. Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS","sentences":["Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task.","Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities.","In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation.","Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP's features into language-associated and vision-associated components.","Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances.","We align features across domains using a modality discriminator.","Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs.","Code: https://github.com/TL-UESTC/UniMoS"],"url":"http://arxiv.org/abs/2403.06946v1","category":"cs.CV"}
{"created":"2024-03-11 17:05:16","title":"Energy dissipation in earthquakes","abstract":"Earthquakes are rupture-like processes that propagate along tectonic faults and cause seismic waves. The propagation speed and final area of the rupture, which determine an earthquake's potential impact, are directly related to the nature and quantity of the energy dissipation involved in the rupture process. Here we present the challenges associated with defining and measuring the energy dissipation in laboratory and natural earthquakes across many scales. We discuss the importance and implications of distinguishing between energy dissipation that occurs close to and far behind the rupture tip and we identify open scientific questions related to a consistent modeling framework for earthquake physics that extends beyond classical Linear Elastic Fracture Mechanics.","sentences":["Earthquakes are rupture-like processes that propagate along tectonic faults and cause seismic waves.","The propagation speed and final area of the rupture, which determine an earthquake's potential impact, are directly related to the nature and quantity of the energy dissipation involved in the rupture process.","Here we present the challenges associated with defining and measuring the energy dissipation in laboratory and natural earthquakes across many scales.","We discuss the importance and implications of distinguishing between energy dissipation that occurs close to and far behind the rupture tip and we identify open scientific questions related to a consistent modeling framework for earthquake physics that extends beyond classical Linear Elastic Fracture Mechanics."],"url":"http://arxiv.org/abs/2403.06916v1","category":"physics.geo-ph"}
{"created":"2024-03-11 16:08:22","title":"Human-Exoskeleton Interaction Portrait","abstract":"Human-robot physical interaction contains crucial information for optimizing user experience, enhancing robot performance, and objectively assessing user adaptation. This study introduces a new method to evaluate human-robot co-adaptation in lower limb exoskeletons by analyzing muscle activity and interaction torque as a two-dimensional random variable. We introduce the Interaction Portrait (IP), which visualizes this variable's distribution in polar coordinates. We applied this metric to compare a recent torque controller (HTC) based on kinematic state feedback and a novel feedforward controller (AMTC) with online learning, proposed herein, against a time-based controller (TBC) during treadmill walking at varying speeds. Compared to TBC, both HTC and AMTC significantly lower users' normalized oxygen uptake, suggesting enhanced user-exoskeleton coordination. IP analysis reveals this improvement stems from two distinct co-adaptation strategies, unidentifiable by traditional muscle activity or interaction torque analyses alone. HTC encourages users to yield control to the exoskeleton, decreasing muscular effort but increasing interaction torque, as the exoskeleton compensates for user dynamics. Conversely, AMTC promotes user engagement through increased muscular effort and reduced interaction torques, aligning it more closely with rehabilitation and gait training applications. IP phase evolution provides insight into each user's interaction strategy development, showcasing IP analysis's potential in comparing and designing novel controllers to optimize human-robot interaction in wearable robots.","sentences":["Human-robot physical interaction contains crucial information for optimizing user experience, enhancing robot performance, and objectively assessing user adaptation.","This study introduces a new method to evaluate human-robot co-adaptation in lower limb exoskeletons by analyzing muscle activity and interaction torque as a two-dimensional random variable.","We introduce the Interaction Portrait (IP), which visualizes this variable's distribution in polar coordinates.","We applied this metric to compare a recent torque controller (HTC) based on kinematic state feedback and a novel feedforward controller (AMTC) with online learning, proposed herein, against a time-based controller (TBC) during treadmill walking at varying speeds.","Compared to TBC, both HTC and AMTC significantly lower users' normalized oxygen uptake, suggesting enhanced user-exoskeleton coordination.","IP analysis reveals this improvement stems from two distinct co-adaptation strategies, unidentifiable by traditional muscle activity or interaction torque analyses alone.","HTC encourages users to yield control to the exoskeleton, decreasing muscular effort but increasing interaction torque, as the exoskeleton compensates for user dynamics.","Conversely, AMTC promotes user engagement through increased muscular effort and reduced interaction torques, aligning it more closely with rehabilitation and gait training applications.","IP phase evolution provides insight into each user's interaction strategy development, showcasing IP analysis's potential in comparing and designing novel controllers to optimize human-robot interaction in wearable robots."],"url":"http://arxiv.org/abs/2403.06851v1","category":"cs.RO"}
{"created":"2024-03-11 15:33:32","title":"LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations","abstract":"Contrastive instance discrimination outperforms supervised learning in downstream tasks like image classification and object detection. However, this approach heavily relies on data augmentation during representation learning, which may result in inferior results if not properly implemented. Random cropping followed by resizing is a common form of data augmentation used in contrastive learning, but it can lead to degraded representation learning if the two random crops contain distinct semantic content. To address this issue, this paper introduces LeOCLR (Leveraging Original Images for Contrastive Learning of Visual Representations), a framework that employs a new instance discrimination approach and an adapted loss function that ensures the shared region between positive pairs is semantically correct. The experimental results show that our approach consistently improves representation learning across different datasets compared to baseline models. For example, our approach outperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and several other methods on transfer learning tasks.","sentences":["Contrastive instance discrimination outperforms supervised learning in downstream tasks like image classification and object detection.","However, this approach heavily relies on data augmentation during representation learning, which may result in inferior results if not properly implemented.","Random cropping followed by resizing is a common form of data augmentation used in contrastive learning, but it can lead to degraded representation learning if the two random crops contain distinct semantic content.","To address this issue, this paper introduces LeOCLR (Leveraging Original Images for Contrastive Learning of Visual Representations), a framework that employs a new instance discrimination approach and an adapted loss function that ensures the shared region between positive pairs is semantically correct.","The experimental results show that our approach consistently improves representation learning across different datasets compared to baseline models.","For example, our approach outperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and several other methods on transfer learning tasks."],"url":"http://arxiv.org/abs/2403.06813v1","category":"cs.CV"}
{"created":"2024-03-11 13:36:00","title":"HILL: A Hallucination Identifier for Large Language Models","abstract":"Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\". First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILL's interface design by surveying 17 participants. Further, we investigated HILL's functionality to identify hallucinations based on an existing question-answering dataset and five user interviews. We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution. With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts.","sentences":["Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text.","Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors.","To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\".","First, we identified design features for HILL with a Wizard of Oz approach with nine participants.","Subsequently, we implemented HILL based on the identified design features and evaluated HILL's interface design by surveying 17 participants.","Further, we investigated HILL's functionality to identify hallucinations based on an existing question-answering dataset and five user interviews.","We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution.","With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts."],"url":"http://arxiv.org/abs/2403.06710v1","category":"cs.HC"}
{"created":"2024-03-11 11:51:42","title":"Design and Control of Delta: Deformable Multilinked Multirotor with Rolling Locomotion Ability in Terrestrial Domain","abstract":"In recent years, multiple types of locomotion methods for robots have been developed and enabled to adapt to multiple domains. In particular, aerial robots are useful for exploration in several situations, taking advantage of its three-dimensional mobility. Moreover, some aerial robots have achieved manipulation tasks in the air. However, energy consumption for flight is large and thus locomotion ability on the ground is also necessary for aerial robots to do tasks for long time. Therefore, in this work, we aim to develop deformable multirotor robot capable of rolling movement with its entire body and achieve motions on the ground and in the air. In this paper, we first describe the design methodology of a deformable multilinked air-ground hybrid multirotor. We also introduce its mechanical design and rotor configuration based on control stability. Then, thrust control method for locomotion in air and ground domains is described. Finally, we show the implemented prototype of the proposed robot and evaluate through experiments in air and terrestrial domains. To the best of our knowledge, this is the first time to achieve the rolling locomotion by multilink structured mutltrotor.","sentences":["In recent years, multiple types of locomotion methods for robots have been developed and enabled to adapt to multiple domains.","In particular, aerial robots are useful for exploration in several situations, taking advantage of its three-dimensional mobility.","Moreover, some aerial robots have achieved manipulation tasks in the air.","However, energy consumption for flight is large and thus locomotion ability on the ground is also necessary for aerial robots to do tasks for long time.","Therefore, in this work, we aim to develop deformable multirotor robot capable of rolling movement with its entire body and achieve motions on the ground and in the air.","In this paper, we first describe the design methodology of a deformable multilinked air-ground hybrid multirotor.","We also introduce its mechanical design and rotor configuration based on control stability.","Then, thrust control method for locomotion in air and ground domains is described.","Finally, we show the implemented prototype of the proposed robot and evaluate through experiments in air and terrestrial domains.","To the best of our knowledge, this is the first time to achieve the rolling locomotion by multilink structured mutltrotor."],"url":"http://arxiv.org/abs/2403.06636v1","category":"cs.RO"}
{"created":"2024-03-11 10:27:36","title":"DNNShield: Embedding Identifiers for Deep Neural Network Ownership Verification","abstract":"The surge in popularity of machine learning (ML) has driven significant investments in training Deep Neural Networks (DNNs). However, these models that require resource-intensive training are vulnerable to theft and unauthorized use. This paper addresses this challenge by introducing DNNShield, a novel approach for DNN protection that integrates seamlessly before training. DNNShield embeds unique identifiers within the model architecture using specialized protection layers. These layers enable secure training and deployment while offering high resilience against various attacks, including fine-tuning, pruning, and adaptive adversarial attacks. Notably, our approach achieves this security with minimal performance and computational overhead (less than 5\\% runtime increase). We validate the effectiveness and efficiency of DNNShield through extensive evaluations across three datasets and four model architectures. This practical solution empowers developers to protect their DNNs and intellectual property rights.","sentences":["The surge in popularity of machine learning (ML) has driven significant investments in training Deep Neural Networks (DNNs).","However, these models that require resource-intensive training are vulnerable to theft and unauthorized use.","This paper addresses this challenge by introducing DNNShield, a novel approach for DNN protection that integrates seamlessly before training.","DNNShield embeds unique identifiers within the model architecture using specialized protection layers.","These layers enable secure training and deployment while offering high resilience against various attacks, including fine-tuning, pruning, and adaptive adversarial attacks.","Notably, our approach achieves this security with minimal performance and computational overhead (less than 5\\% runtime increase).","We validate the effectiveness and efficiency of DNNShield through extensive evaluations across three datasets and four model architectures.","This practical solution empowers developers to protect their DNNs and intellectual property rights."],"url":"http://arxiv.org/abs/2403.06581v1","category":"cs.CR"}
{"created":"2024-03-11 10:10:45","title":"Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming","abstract":"Mobility impairment caused by limb loss is a significant challenge faced by millions of individuals worldwide. The development of advanced assistive technologies, such as prosthetic devices, has the potential to greatly improve the quality of life for amputee patients. A critical component in the design of such technologies is the accurate prediction of reference joint motion for the missing limb. However, this task is hindered by the scarcity of joint motion data available for amputee patients, in contrast to the substantial quantity of data from able-bodied subjects. To overcome this, we leverage deep learning's reprogramming property to repurpose well-trained models for a new goal without altering the model parameters. With only data-level manipulation, we adapt models originally designed for able-bodied people to forecast joint motion in amputees. The findings in this study have significant implications for advancing assistive tech and amputee mobility.","sentences":["Mobility impairment caused by limb loss is a significant challenge faced by millions of individuals worldwide.","The development of advanced assistive technologies, such as prosthetic devices, has the potential to greatly improve the quality of life for amputee patients.","A critical component in the design of such technologies is the accurate prediction of reference joint motion for the missing limb.","However, this task is hindered by the scarcity of joint motion data available for amputee patients, in contrast to the substantial quantity of data from able-bodied subjects.","To overcome this, we leverage deep learning's reprogramming property to repurpose well-trained models for a new goal without altering the model parameters.","With only data-level manipulation, we adapt models originally designed for able-bodied people to forecast joint motion in amputees.","The findings in this study have significant implications for advancing assistive tech and amputee mobility."],"url":"http://arxiv.org/abs/2403.06569v1","category":"cs.LG"}
{"created":"2024-03-11 09:51:02","title":"Fine boundary continuity for degenerate double-phase diffusion","abstract":"We study the boundary behavior of solutions to parabolic double-phase equations through the celebrated Wiener's sufficiency criterion. The analysis is conducted for cylindrical domains and the regularity up to the lateral boundary is shown in terms of either its $p$ or $q$ capacity, depending on whether the phase vanishes at the boundary or not. Eventually we obtain a fine boundary estimate that, when considering uniform geometric conditions as density or fatness, leads us to the boundary H\\\"older continuity of solutions. In particular, the double-phase elicits new questions on the definition of an adapted capacity.","sentences":["We study the boundary behavior of solutions to parabolic double-phase equations through the celebrated Wiener's sufficiency criterion.","The analysis is conducted for cylindrical domains and the regularity up to the lateral boundary is shown in terms of either its $p$ or $q$ capacity, depending on whether the phase vanishes at the boundary or not.","Eventually we obtain a fine boundary estimate that, when considering uniform geometric conditions as density or fatness, leads us to the boundary H\\\"older continuity of solutions.","In particular, the double-phase elicits new questions on the definition of an adapted capacity."],"url":"http://arxiv.org/abs/2403.06550v1","category":"math.AP"}
{"created":"2024-03-11 09:10:37","title":"Adaptive Federated Learning Over the Air","abstract":"We propose a federated version of adaptive gradient methods, particularly AdaGrad and Adam, within the framework of over-the-air model training. This approach capitalizes on the inherent superposition property of wireless channels, facilitating fast and scalable parameter aggregation. Meanwhile, it enhances the robustness of the model training process by dynamically adjusting the stepsize in accordance with the global gradient update. We derive the convergence rate of the training algorithms, encompassing the effects of channel fading and interference, for a broad spectrum of nonconvex loss functions. Our analysis shows that the AdaGrad-based algorithm converges to a stationary point at the rate of $\\mathcal{O}( \\ln{(T)} /{ T^{ 1 - \\frac{1}{\\alpha} } } )$, where $\\alpha$ represents the tail index of the electromagnetic interference. This result indicates that the level of heavy-tailedness in interference distribution plays a crucial role in the training efficiency: the heavier the tail, the slower the algorithm converges. In contrast, an Adam-like algorithm converges at the $\\mathcal{O}( 1/T )$ rate, demonstrating its advantage in expediting the model training process. We conduct extensive experiments that corroborate our theoretical findings and affirm the practical efficacy of our proposed federated adaptive gradient methods.","sentences":["We propose a federated version of adaptive gradient methods, particularly AdaGrad and Adam, within the framework of over-the-air model training.","This approach capitalizes on the inherent superposition property of wireless channels, facilitating fast and scalable parameter aggregation.","Meanwhile, it enhances the robustness of the model training process by dynamically adjusting the stepsize in accordance with the global gradient update.","We derive the convergence rate of the training algorithms, encompassing the effects of channel fading and interference, for a broad spectrum of nonconvex loss functions.","Our analysis shows that the AdaGrad-based algorithm converges to a stationary point at the rate of $\\mathcal{O}( \\ln{(T)} /{ T^{ 1 - \\frac{1}{\\alpha} } } )$, where $\\alpha$ represents the tail index of the electromagnetic interference.","This result indicates that the level of heavy-tailedness in interference distribution plays a crucial role in the training efficiency: the heavier the tail, the slower the algorithm converges.","In contrast, an Adam-like algorithm converges at the $\\mathcal{O}( 1/T )$ rate, demonstrating its advantage in expediting the model training process.","We conduct extensive experiments that corroborate our theoretical findings and affirm the practical efficacy of our proposed federated adaptive gradient methods."],"url":"http://arxiv.org/abs/2403.06528v1","category":"cs.LG"}
{"created":"2024-03-11 06:56:08","title":"Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation","abstract":"Multi-modal test-time adaptation (MM-TTA) is proposed to adapt models to an unlabeled target domain by leveraging the complementary multi-modal inputs in an online manner. Previous MM-TTA methods rely on predictions of cross-modal information in each input frame, while they ignore the fact that predictions of geometric neighborhoods within consecutive frames are highly correlated, leading to unstable predictions across time. To fulfill this gap, we propose ReLiable Spatial-temporal Voxels (Latte), an MM-TTA method that leverages reliable cross-modal spatial-temporal correspondences for multi-modal 3D segmentation. Motivated by the fact that reliable predictions should be consistent with their spatial-temporal correspondences, Latte aggregates consecutive frames in a slide window manner and constructs ST voxel to capture temporally local prediction consistency for each modality. After filtering out ST voxels with high ST entropy, Latte conducts cross-modal learning for each point and pixel by attending to those with reliable and consistent predictions among both spatial and temporal neighborhoods. Experimental results show that Latte achieves state-of-the-art performance on three different MM-TTA benchmarks compared to previous MM-TTA or TTA methods.","sentences":["Multi-modal test-time adaptation (MM-TTA) is proposed to adapt models to an unlabeled target domain by leveraging the complementary multi-modal inputs in an online manner.","Previous MM-TTA methods rely on predictions of cross-modal information in each input frame, while they ignore the fact that predictions of geometric neighborhoods within consecutive frames are highly correlated, leading to unstable predictions across time.","To fulfill this gap, we propose ReLiable Spatial-temporal Voxels (Latte), an MM-TTA method that leverages reliable cross-modal spatial-temporal correspondences for multi-modal 3D segmentation.","Motivated by the fact that reliable predictions should be consistent with their spatial-temporal correspondences, Latte aggregates consecutive frames in a slide window manner and constructs ST voxel to capture temporally local prediction consistency for each modality.","After filtering out ST voxels with high ST entropy, Latte conducts cross-modal learning for each point and pixel by attending to those with reliable and consistent predictions among both spatial and temporal neighborhoods.","Experimental results show that Latte achieves state-of-the-art performance on three different MM-TTA benchmarks compared to previous MM-TTA or TTA methods."],"url":"http://arxiv.org/abs/2403.06461v1","category":"cs.CV"}
{"created":"2024-03-11 06:08:16","title":"FontCLIP: A Semantic Typography Visual-Language Model for Multilingual Font Applications","abstract":"Acquiring the desired font for various design tasks can be challenging and requires professional typographic knowledge. While previous font retrieval or generation works have alleviated some of these difficulties, they often lack support for multiple languages and semantic attributes beyond the training data domains. To solve this problem, we present FontCLIP: a model that connects the semantic understanding of a large vision-language model with typographical knowledge. We integrate typography-specific knowledge into the comprehensive vision-language knowledge of a pretrained CLIP model through a novel finetuning approach. We propose to use a compound descriptive prompt that encapsulates adaptively sampled attributes from a font attribute dataset focusing on Roman alphabet characters. FontCLIP's semantic typographic latent space demonstrates two unprecedented generalization abilities. First, FontCLIP generalizes to different languages including Chinese, Japanese, and Korean (CJK), capturing the typographical features of fonts across different languages, even though it was only finetuned using fonts of Roman characters. Second, FontCLIP can recognize the semantic attributes that are not presented in the training data. FontCLIP's dual-modality and generalization abilities enable multilingual and cross-lingual font retrieval and letter shape optimization, reducing the burden of obtaining desired fonts.","sentences":["Acquiring the desired font for various design tasks can be challenging and requires professional typographic knowledge.","While previous font retrieval or generation works have alleviated some of these difficulties, they often lack support for multiple languages and semantic attributes beyond the training data domains.","To solve this problem, we present FontCLIP: a model that connects the semantic understanding of a large vision-language model with typographical knowledge.","We integrate typography-specific knowledge into the comprehensive vision-language knowledge of a pretrained CLIP model through a novel finetuning approach.","We propose to use a compound descriptive prompt that encapsulates adaptively sampled attributes from a font attribute dataset focusing on Roman alphabet characters.","FontCLIP's semantic typographic latent space demonstrates two unprecedented generalization abilities.","First, FontCLIP generalizes to different languages including Chinese, Japanese, and Korean (CJK), capturing the typographical features of fonts across different languages, even though it was only finetuned using fonts of Roman characters.","Second, FontCLIP can recognize the semantic attributes that are not presented in the training data.","FontCLIP's dual-modality and generalization abilities enable multilingual and cross-lingual font retrieval and letter shape optimization, reducing the burden of obtaining desired fonts."],"url":"http://arxiv.org/abs/2403.06453v1","category":"cs.CV"}
{"created":"2024-03-11 04:44:26","title":"AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration","abstract":"Deep learning-based face restoration models, increasingly prevalent in smart devices, have become targets for sophisticated backdoor attacks. These attacks, through subtle trigger injection into input face images, can lead to unexpected restoration outcomes. Unlike conventional methods focused on classification tasks, our approach introduces a unique degradation objective tailored for attacking restoration models. Moreover, we propose the Adaptive Selective Frequency Injection Backdoor Attack (AS-FIBA) framework, employing a neural network for input-specific trigger generation in the frequency domain, seamlessly blending triggers with benign images. This results in imperceptible yet effective attacks, guiding restoration predictions towards subtly degraded outputs rather than conspicuous targets. Extensive experiments demonstrate the efficacy of the degradation objective on state-of-the-art face restoration models. Additionally, it is notable that AS-FIBA can insert effective backdoors that are more imperceptible than existing backdoor attack methods, including WaNet, ISSBA, and FIBA.","sentences":["Deep learning-based face restoration models, increasingly prevalent in smart devices, have become targets for sophisticated backdoor attacks.","These attacks, through subtle trigger injection into input face images, can lead to unexpected restoration outcomes.","Unlike conventional methods focused on classification tasks, our approach introduces a unique degradation objective tailored for attacking restoration models.","Moreover, we propose the Adaptive Selective Frequency Injection Backdoor Attack (AS-FIBA) framework, employing a neural network for input-specific trigger generation in the frequency domain, seamlessly blending triggers with benign images.","This results in imperceptible yet effective attacks, guiding restoration predictions towards subtly degraded outputs rather than conspicuous targets.","Extensive experiments demonstrate the efficacy of the degradation objective on state-of-the-art face restoration models.","Additionally, it is notable that AS-FIBA can insert effective backdoors that are more imperceptible than existing backdoor attack methods, including WaNet, ISSBA, and FIBA."],"url":"http://arxiv.org/abs/2403.06430v1","category":"cs.CV"}
{"created":"2024-03-11 03:38:48","title":"Can LLMs' Tuning Methods Work in Medical Multimodal Domain?","abstract":"While large language models (LLMs) excel in world knowledge understanding, adapting them to specific subfields requires precise adjustments. Due to the model's vast scale, traditional global fine-tuning methods for large models can be computationally expensive and impact generalization. To address this challenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT) methods have emerged and achieved remarkable success in both LLMs and Large Vision-Language Models (LVLMs). In the medical domain, fine-tuning a medical Vision-Language Pretrained (VLP) model is essential for adapting it to specific tasks. Can the fine-tuning methods for large models be transferred to the medical field to enhance transfer learning efficiency? In this paper, we delve into the fine-tuning methods of LLMs and conduct extensive experiments to investigate the impact of fine-tuning methods for large models on existing multimodal models in the medical domain from the training data level and the model structure level. We show the different impacts of fine-tuning methods for large models on medical VLMs and develop the most efficient ways to fine-tune medical VLP models. We hope this research can guide medical domain researchers in optimizing VLMs' training costs, fostering the broader application of VLMs in healthcare fields. Code and dataset will be released upon acceptance.","sentences":["While large language models (LLMs) excel in world knowledge understanding, adapting them to specific subfields requires precise adjustments.","Due to the model's vast scale, traditional global fine-tuning methods for large models can be computationally expensive and impact generalization.","To address this challenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT) methods have emerged and achieved remarkable success in both LLMs and Large Vision-Language Models (LVLMs).","In the medical domain, fine-tuning a medical Vision-Language Pretrained (VLP) model is essential for adapting it to specific tasks.","Can the fine-tuning methods for large models be transferred to the medical field to enhance transfer learning efficiency?","In this paper, we delve into the fine-tuning methods of LLMs and conduct extensive experiments to investigate the impact of fine-tuning methods for large models on existing multimodal models in the medical domain from the training data level and the model structure level.","We show the different impacts of fine-tuning methods for large models on medical VLMs and develop the most efficient ways to fine-tune medical VLP models.","We hope this research can guide medical domain researchers in optimizing VLMs' training costs, fostering the broader application of VLMs in healthcare fields.","Code and dataset will be released upon acceptance."],"url":"http://arxiv.org/abs/2403.06407v1","category":"cs.CV"}
{"created":"2024-03-11 03:28:13","title":"'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification","abstract":"Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data. An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts). An important component of ICL is the use of a small number of labelled data instances as examples in the prompt. While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data. This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data instance is predicted by the Softmax posteriors of a classifier. The parameters of this classifier are fitted on the optimal number of examples in ICL required to correctly infer the label of each instance in the training set with the hypothesis that a test instance that is similar to a training instance should use the same (or a closely matching) number of few-shot examples. Our experiments show that our AICL method results in improvement in text classification task on several standard datasets.","sentences":["Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data.","An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts).","An important component of ICL is the use of a small number of labelled data instances as examples in the prompt.","While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data.","This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier.","In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data instance is predicted by the Softmax posteriors of a classifier.","The parameters of this classifier are fitted on the optimal number of examples in ICL required to correctly infer the label of each instance in the training set with the hypothesis that a test instance that is similar to a training instance should use the same (or a closely matching) number of few-shot examples.","Our experiments show that our AICL method results in improvement in text classification task on several standard datasets."],"url":"http://arxiv.org/abs/2403.06402v1","category":"cs.CL"}
{"created":"2024-03-11 02:59:30","title":"FSViewFusion: Few-Shots View Generation of Novel Objects","abstract":"Novel view synthesis has observed tremendous developments since the arrival of NeRFs. However, Nerf models overfit on a single scene, lacking generalization to out of distribution objects. Recently, diffusion models have exhibited remarkable performance on introducing generalization in view synthesis. Inspired by these advancements, we explore the capabilities of a pretrained stable diffusion model for view synthesis without explicit 3D priors. Specifically, we base our method on a personalized text to image model, Dreambooth, given its strong ability to adapt to specific novel objects with a few shots. Our research reveals two interesting findings. First, we observe that Dreambooth can learn the high level concept of a view, compared to arguably more complex strategies which involve finetuning diffusions on large amounts of multi-view data. Second, we establish that the concept of a view can be disentangled and transferred to a novel object irrespective of the original object's identify from which the views are learnt. Motivated by this, we introduce a learning strategy, FSViewFusion, which inherits a specific view through only one image sample of a single scene, and transfers the knowledge to a novel object, learnt from few shots, using low rank adapters. Through extensive experiments we demonstrate that our method, albeit simple, is efficient in generating reliable view samples for in the wild images. Code and models will be released.","sentences":["Novel view synthesis has observed tremendous developments since the arrival of NeRFs.","However, Nerf models overfit on a single scene, lacking generalization to out of distribution objects.","Recently, diffusion models have exhibited remarkable performance on introducing generalization in view synthesis.","Inspired by these advancements, we explore the capabilities of a pretrained stable diffusion model for view synthesis without explicit 3D priors.","Specifically, we base our method on a personalized text to image model, Dreambooth, given its strong ability to adapt to specific novel objects with a few shots.","Our research reveals two interesting findings.","First, we observe that Dreambooth can learn the high level concept of a view, compared to arguably more complex strategies which involve finetuning diffusions on large amounts of multi-view data.","Second, we establish that the concept of a view can be disentangled and transferred to a novel object irrespective of the original object's identify from which the views are learnt.","Motivated by this, we introduce a learning strategy, FSViewFusion, which inherits a specific view through only one image sample of a single scene, and transfers the knowledge to a novel object, learnt from few shots, using low rank adapters.","Through extensive experiments we demonstrate that our method, albeit simple, is efficient in generating reliable view samples for in the wild images.","Code and models will be released."],"url":"http://arxiv.org/abs/2403.06394v1","category":"cs.CV"}
{"created":"2024-03-11 02:00:15","title":"The Geometry of Cyclical Social Trends","abstract":"We investigate the emergence of periodic behavior in opinion dynamics and its underlying geometry. For this, we use a bounded-confidence model with contrarian agents in a convolution social network. This means that agents adapt their opinions by interacting with their neighbors in a time-varying social network. Being contrarian, the agents are kept from reaching consensus. This is the key feature that allows the emergence of cyclical trends. We show that the systems either converge to nonconsensual equilibrium or are attracted to periodic or quasi-periodic orbits. We bound the dimension of the attractors and the period of cyclical trends. We exhibit instances where each orbit is dense and uniformly distributed within its attractor. We also investigate the case of randomly changing social networks.","sentences":["We investigate the emergence of periodic behavior in opinion dynamics and its underlying geometry.","For this, we use a bounded-confidence model with contrarian agents in a convolution social network.","This means that agents adapt their opinions by interacting with their neighbors in a time-varying social network.","Being contrarian, the agents are kept from reaching consensus.","This is the key feature that allows the emergence of cyclical trends.","We show that the systems either converge to nonconsensual equilibrium or are attracted to periodic or quasi-periodic orbits.","We bound the dimension of the attractors and the period of cyclical trends.","We exhibit instances where each orbit is dense and uniformly distributed within its attractor.","We also investigate the case of randomly changing social networks."],"url":"http://arxiv.org/abs/2403.06376v1","category":"cs.MA"}
{"created":"2024-03-11 01:57:37","title":"Intrinsic polarization conversion and avoided-mode crossing in X-cut lithium niobate microrings","abstract":"Compared with well-developed free space polarization converters, polarization conversion between TE and TM modes in waveguide is generally considered to be caused by shape birefringence, like curvature, morphology of waveguide cross section and scattering. Here, we reveal a hidden polarization conversion mechanism in X-cut lithium niobate microrings, that is the conversion can be implemented by birefringence of waveguides, which will also introduce an unavoidable avoided-mode crossing. In the experiment, we find that this mode crossing results in severe suppression of one sideband in local nondegenerate four-wave mixing and disrupts the cascaded four-wave mixing on this side. Simultaneously, we proposed, for the first time to our best knowledge, one two-dimensional method to simulate the eigenmodes (TE and TM) in X-cut microrings, which avoids the obstacle from large computational effort in three-dimensional anisotropic microrings simulation, and the mode crossing point. This work will provide an entirely novel approach to the design of polarization converters and simulation for monolithic photonics integrated circuits, and may be helpful to the studies of missed temporal dissipative soliton formation in X-cut lithium niobate rings.","sentences":["Compared with well-developed free space polarization converters, polarization conversion between TE and TM modes in waveguide is generally considered to be caused by shape birefringence, like curvature, morphology of waveguide cross section and scattering.","Here, we reveal a hidden polarization conversion mechanism in X-cut lithium niobate microrings, that is the conversion can be implemented by birefringence of waveguides, which will also introduce an unavoidable avoided-mode crossing.","In the experiment, we find that this mode crossing results in severe suppression of one sideband in local nondegenerate four-wave mixing and disrupts the cascaded four-wave mixing on this side.","Simultaneously, we proposed, for the first time to our best knowledge, one two-dimensional method to simulate the eigenmodes (TE and TM) in X-cut microrings, which avoids the obstacle from large computational effort in three-dimensional anisotropic microrings simulation, and the mode crossing point.","This work will provide an entirely novel approach to the design of polarization converters and simulation for monolithic photonics integrated circuits, and may be helpful to the studies of missed temporal dissipative soliton formation in X-cut lithium niobate rings."],"url":"http://arxiv.org/abs/2403.06374v1","category":"physics.optics"}
{"created":"2024-03-11 01:20:03","title":"Say Anything with Any Style","abstract":"Generating stylized talking head with diverse head motions is crucial for achieving natural-looking videos but still remains challenging. Previous works either adopt a regressive method to capture the speaking style, resulting in a coarse style that is averaged across all training data, or employ a universal network to synthesize videos with different styles which causes suboptimal performance. To address these, we propose a novel dynamic-weight method, namely Say Anything withAny Style (SAAS), which queries the discrete style representation via a generative model with a learned style codebook. Specifically, we develop a multi-task VQ-VAE that incorporates three closely related tasks to learn a style codebook as a prior for style extraction. This discrete prior, along with the generative model, enhances the precision and robustness when extracting the speaking styles of the given style clips. By utilizing the extracted style, a residual architecture comprising a canonical branch and style-specific branch is employed to predict the mouth shapes conditioned on any driving audio while transferring the speaking style from the source to any desired one. To adapt to different speaking styles, we steer clear of employing a universal network by exploring an elaborate HyperStyle to produce the style-specific weights offset for the style branch. Furthermore, we construct a pose generator and a pose codebook to store the quantized pose representation, allowing us to sample diverse head motions aligned with the audio and the extracted style. Experiments demonstrate that our approach surpasses state-of-theart methods in terms of both lip-synchronization and stylized expression. Besides, we extend our SAAS to video-driven style editing field and achieve satisfactory performance.","sentences":["Generating stylized talking head with diverse head motions is crucial for achieving natural-looking videos but still remains challenging.","Previous works either adopt a regressive method to capture the speaking style, resulting in a coarse style that is averaged across all training data, or employ a universal network to synthesize videos with different styles which causes suboptimal performance.","To address these, we propose a novel dynamic-weight method, namely Say Anything withAny Style (SAAS), which queries the discrete style representation via a generative model with a learned style codebook.","Specifically, we develop a multi-task VQ-VAE that incorporates three closely related tasks to learn a style codebook as a prior for style extraction.","This discrete prior, along with the generative model, enhances the precision and robustness when extracting the speaking styles of the given style clips.","By utilizing the extracted style, a residual architecture comprising a canonical branch and style-specific branch is employed to predict the mouth shapes conditioned on any driving audio while transferring the speaking style from the source to any desired one.","To adapt to different speaking styles, we steer clear of employing a universal network by exploring an elaborate HyperStyle to produce the style-specific weights offset for the style branch.","Furthermore, we construct a pose generator and a pose codebook to store the quantized pose representation, allowing us to sample diverse head motions aligned with the audio and the extracted style.","Experiments demonstrate that our approach surpasses state-of-theart methods in terms of both lip-synchronization and stylized expression.","Besides, we extend our SAAS to video-driven style editing field and achieve satisfactory performance."],"url":"http://arxiv.org/abs/2403.06363v1","category":"cs.CV"}
{"created":"2024-03-11 01:18:49","title":"See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI","abstract":"Deciphering visual content from functional Magnetic Resonance Imaging (fMRI) helps illuminate the human vision system. However, the scarcity of fMRI data and noise hamper brain decoding model performance. Previous approaches primarily employ subject-specific models, sensitive to training sample size. In this paper, we explore a straightforward but overlooked solution to address data scarcity. We propose shallow subject-specific adapters to map cross-subject fMRI data into unified representations. Subsequently, a shared deeper decoding model decodes cross-subject features into the target feature space. During training, we leverage both visual and textual supervision for multi-modal brain decoding. Our model integrates a high-level perception decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level perceptions, simulating bottom-up and top-down processes in neuroscience. Empirical experiments demonstrate robust neural representation learning across subjects for both pipelines. Moreover, merging high-level and low-level information improves both low-level and high-level reconstruction metrics. Additionally, we successfully transfer learned general knowledge to new subjects by training new adapters with limited training data. Compared to previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis and fMRI-PTE), our approach achieves comparable or superior results across diverse tasks, showing promise as an alternative method for cross-subject fMRI data pre-training. Our code and pre-trained weights will be publicly released at https://github.com/YulongBonjour/See_Through_Their_Minds.","sentences":["Deciphering visual content from functional Magnetic Resonance Imaging (fMRI) helps illuminate the human vision system.","However, the scarcity of fMRI data and noise hamper brain decoding model performance.","Previous approaches primarily employ subject-specific models, sensitive to training sample size.","In this paper, we explore a straightforward but overlooked solution to address data scarcity.","We propose shallow subject-specific adapters to map cross-subject fMRI data into unified representations.","Subsequently, a shared deeper decoding model decodes cross-subject features into the target feature space.","During training, we leverage both visual and textual supervision for multi-modal brain decoding.","Our model integrates a high-level perception decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level perceptions, simulating bottom-up and top-down processes in neuroscience.","Empirical experiments demonstrate robust neural representation learning across subjects for both pipelines.","Moreover, merging high-level and low-level information improves both low-level and high-level reconstruction metrics.","Additionally, we successfully transfer learned general knowledge to new subjects by training new adapters with limited training data.","Compared to previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis and fMRI-PTE), our approach achieves comparable or superior results across diverse tasks, showing promise as an alternative method for cross-subject fMRI data pre-training.","Our code and pre-trained weights will be publicly released at https://github.com/YulongBonjour/See_Through_Their_Minds."],"url":"http://arxiv.org/abs/2403.06361v1","category":"cs.CV"}
{"created":"2024-03-11 01:02:01","title":"Exploring Hardware Friendly Bottleneck Architecture in CNN for Embedded Computing Systems","abstract":"In this paper, we explore how to design lightweight CNN architecture for embedded computing systems. We propose L-Mobilenet model for ZYNQ based hardware platform. L-Mobilenet can adapt well to the hardware computing and accelerating, and its network structure is inspired by the state-of-the-art work of Inception-ResnetV1 and MobilenetV2, which can effectively reduce parameters and delay while maintaining the accuracy of inference. We deploy our L-Mobilenet model to ZYNQ embedded platform for fully evaluating the performance of our design. By measuring in cifar10 and cifar100 datasets, L-Mobilenet model is able to gain 3x speed up and 3.7x fewer parameters than MobileNetV2 while maintaining a similar accuracy. It also can obtain 2x speed up and 1.5x fewer parameters than ShufflenetV2 while maintaining the same accuracy. Experiments show that our network model can obtain better performance because of the special considerations for hardware accelerating and software-hardware co-design strategies in our L-Mobilenet bottleneck architecture.","sentences":["In this paper, we explore how to design lightweight CNN architecture for embedded computing systems.","We propose L-Mobilenet model for ZYNQ based hardware platform.","L-Mobilenet can adapt well to the hardware computing and accelerating, and its network structure is inspired by the state-of-the-art work of Inception-ResnetV1 and MobilenetV2, which can effectively reduce parameters and delay while maintaining the accuracy of inference.","We deploy our L-Mobilenet model to ZYNQ embedded platform for fully evaluating the performance of our design.","By measuring in cifar10 and cifar100 datasets, L-Mobilenet model is able to gain 3x speed up and 3.7x fewer parameters than MobileNetV2 while maintaining a similar accuracy.","It also can obtain 2x speed up and 1.5x fewer parameters than ShufflenetV2 while maintaining the same accuracy.","Experiments show that our network model can obtain better performance because of the special considerations for hardware accelerating and software-hardware co-design strategies in our L-Mobilenet bottleneck architecture."],"url":"http://arxiv.org/abs/2403.06352v1","category":"cs.CV"}
{"created":"2024-03-11 00:23:17","title":"Practically adaptable CPABE based Health-Records sharing framework","abstract":"With recent elevated adaptation of cloud services in almost every major public sector, the health sector emerges as a vulnerable segment, particularly in data exchange of sensitive Health records, as determining the retention, exchange, and efficient use of patient records without jeopardizing patient privacy, particularly on mobile-applications remains an area to expand. In the existing scenarios of cloud-mobile services, several vulnerabilities can be found including trapping of data within a single cloud-service-provider and loss of resource control being the significant ones. In this study, we have suggested a CPABE and OAuth2.0 based framework for efficient access-control and authorization respectively to improve the practicality of EHR sharing across a single client-application. In addition to solving issues like practicality, data entrapment, and resource control loss, the suggested framework also aims to provide two significant functionalities simultaneously, the specific operation of client application itself, and straightforward access of data to institutions, governments, and organizations seeking delicate EHRs. Our implementation of the suggested framework along with its analytical comparison signifies its potential in terms of efficient performance and minimal latency as this study would have a considerable impact on the recent literature as it intends to bridge the pragmatic deficit in CPABE-based EHR services.","sentences":["With recent elevated adaptation of cloud services in almost every major public sector, the health sector emerges as a vulnerable segment, particularly in data exchange of sensitive Health records, as determining the retention, exchange, and efficient use of patient records without jeopardizing patient privacy, particularly on mobile-applications remains an area to expand.","In the existing scenarios of cloud-mobile services, several vulnerabilities can be found including trapping of data within a single cloud-service-provider and loss of resource control being the significant ones.","In this study, we have suggested a CPABE and OAuth2.0 based framework for efficient access-control and authorization respectively to improve the practicality of EHR sharing across a single client-application.","In addition to solving issues like practicality, data entrapment, and resource control loss, the suggested framework also aims to provide two significant functionalities simultaneously, the specific operation of client application itself, and straightforward access of data to institutions, governments, and organizations seeking delicate EHRs.","Our implementation of the suggested framework along with its analytical comparison signifies its potential in terms of efficient performance and minimal latency as this study would have a considerable impact on the recent literature as it intends to bridge the pragmatic deficit in CPABE-based EHR services."],"url":"http://arxiv.org/abs/2403.06347v1","category":"cs.CR"}
{"created":"2024-03-10 22:28:54","title":"Active Learning for Rapid Targeted Synthesis of Compositionally Complex Alloys","abstract":"The next generation of advanced materials is tending toward increasingly complex compositions. Synthesizing precise composition is time-consuming and becomes exponentially demanding with increasing compositional complexity. An experienced human operator does significantly better than a beginner but still struggles to consistently achieve precision when synthesis parameters are coupled. The time to optimize synthesis becomes a barrier to exploring scientifically and technologically exciting compositionally complex materials. This investigation demonstrates an Active Learning (AL) approach for optimizing physical vapor deposition synthesis of thin-film alloys with up to five principal elements. We compared AL based on Gaussian Process (GP) and Random Forest (RF) models. The best performing models were able to discover synthesis parameters for a target quinary alloy in 14 iterations. We also demonstrate the capability of these models to be used in transfer learning tasks. RF and GP models trained on lower dimensional systems (i.e. ternary, quarternary) show an immediate improvement in prediction accuracy compared to models trained only on quinary samples. Furthermore, samples that only share a few elements in common with the target composition can be used for model pre-training. We believe that such AL approaches can be widely adapted to significantly accelerate the exploration of compositionally complex materials.","sentences":["The next generation of advanced materials is tending toward increasingly complex compositions.","Synthesizing precise composition is time-consuming and becomes exponentially demanding with increasing compositional complexity.","An experienced human operator does significantly better than a beginner but still struggles to consistently achieve precision when synthesis parameters are coupled.","The time to optimize synthesis becomes a barrier to exploring scientifically and technologically exciting compositionally complex materials.","This investigation demonstrates an Active Learning (AL) approach for optimizing physical vapor deposition synthesis of thin-film alloys with up to five principal elements.","We compared AL based on Gaussian Process (GP) and Random Forest (RF) models.","The best performing models were able to discover synthesis parameters for a target quinary alloy in 14 iterations.","We also demonstrate the capability of these models to be used in transfer learning tasks.","RF and GP models trained on lower dimensional systems (i.e. ternary, quarternary) show an immediate improvement in prediction accuracy compared to models trained only on quinary samples.","Furthermore, samples that only share a few elements in common with the target composition can be used for model pre-training.","We believe that such AL approaches can be widely adapted to significantly accelerate the exploration of compositionally complex materials."],"url":"http://arxiv.org/abs/2403.06329v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-10 22:24:31","title":"Hybrid Soft Electrostatic Metamaterial Gripper for Multi-surface, Multi-object Adaptation","abstract":"One of the trendsetting themes in soft robotics has been the goal of developing the ultimate universal soft robotic gripper. One that is capable of manipulating items of various shapes, sizes, thicknesses, textures, and weights. All the while still being lightweight and scalable in order to adapt to use cases. In this work, we report a soft gripper that enables delicate and precise grasps of fragile, deformable, and flexible objects but also excels in lifting heavy objects of up to 1617x its own body weight. The principle behind the soft gripper is based on extending the capabilities of electroadhesion soft grippers through the enhancement principles found in metamaterial adhesion cut and patterning. This design amplifies the adhesion and grasping payload in one direction while reducing the adhesion capabilities in the other direction. This counteracts the residual forces during peeling (a common problem with electroadhesive grippers), thus increasing its speed of release. In essence, we are able to tune the maximum strength and peeling speed, beyond the capabilities of previous electroadhesive grippers. We study the capabilities of the system through a wide range of experiments with single and multiple-fingered peel tests. We also demonstrate its modular and adaptive capabilities in the real-world with a two-finger gripper, by performing grasping tests of up to $5$ different multi-surfaced objects.","sentences":["One of the trendsetting themes in soft robotics has been the goal of developing the ultimate universal soft robotic gripper.","One that is capable of manipulating items of various shapes, sizes, thicknesses, textures, and weights.","All the while still being lightweight and scalable in order to adapt to use cases.","In this work, we report a soft gripper that enables delicate and precise grasps of fragile, deformable, and flexible objects but also excels in lifting heavy objects of up to 1617x its own body weight.","The principle behind the soft gripper is based on extending the capabilities of electroadhesion soft grippers through the enhancement principles found in metamaterial adhesion cut and patterning.","This design amplifies the adhesion and grasping payload in one direction while reducing the adhesion capabilities in the other direction.","This counteracts the residual forces during peeling (a common problem with electroadhesive grippers), thus increasing its speed of release.","In essence, we are able to tune the maximum strength and peeling speed, beyond the capabilities of previous electroadhesive grippers.","We study the capabilities of the system through a wide range of experiments with single and multiple-fingered peel tests.","We also demonstrate its modular and adaptive capabilities in the real-world with a two-finger gripper, by performing grasping tests of up to $5$ different multi-surfaced objects."],"url":"http://arxiv.org/abs/2403.06327v1","category":"cs.RO"}
{"created":"2024-03-10 20:22:06","title":"Nonparametric Automatic Differentiation Variational Inference with Spline Approximation","abstract":"Automatic Differentiation Variational Inference (ADVI) is efficient in learning probabilistic models. Classic ADVI relies on the parametric approach to approximate the posterior. In this paper, we develop a spline-based nonparametric approximation approach that enables flexible posterior approximation for distributions with complicated structures, such as skewness, multimodality, and bounded support. Compared with widely-used nonparametric variational inference methods, the proposed method is easy to implement and adaptive to various data structures. By adopting the spline approximation, we derive a lower bound of the importance weighted autoencoder and establish the asymptotic consistency. Experiments demonstrate the efficiency of the proposed method in approximating complex posterior distributions and improving the performance of generative models with incomplete data.","sentences":["Automatic Differentiation Variational Inference (ADVI) is efficient in learning probabilistic models.","Classic ADVI relies on the parametric approach to approximate the posterior.","In this paper, we develop a spline-based nonparametric approximation approach that enables flexible posterior approximation for distributions with complicated structures, such as skewness, multimodality, and bounded support.","Compared with widely-used nonparametric variational inference methods, the proposed method is easy to implement and adaptive to various data structures.","By adopting the spline approximation, we derive a lower bound of the importance weighted autoencoder and establish the asymptotic consistency.","Experiments demonstrate the efficiency of the proposed method in approximating complex posterior distributions and improving the performance of generative models with incomplete data."],"url":"http://arxiv.org/abs/2403.06302v1","category":"stat.ML"}
{"created":"2024-03-10 18:24:10","title":"Modeling the Multi-wavelength Radiation Properties in Pulsar Dissipative Magnetospheres","abstract":"We explore the multiwavelength radiation properties of the light curves and energy spectra in the dissipative magnetospheres of pulsars. The dissipative magnetospheres are simulated by the pseudo-spectral method with the combined force-free and Aristotelian electrodynamics, which can produce self-consistent accelerating electric fields mainly distributed in the equatorial current sheet outside the light cylinder. The multiwavelength light curves and spectra are computed by using the multiple emission mechanisms of both the primary particles accelerated by the accelerating electric fields in the equatorial current sheet and the secondary pairs with an assumed distribution spectrum. We then compare the predicted multiwavelength light curves and spectra with the observed data from the Crab, Vela, and Geminga pulsars. Our modeling results can systematically well reproduce the observed trends of the multiwavelength light curves and the spectra for these three pulsars.","sentences":["We explore the multiwavelength radiation properties of the light curves and energy spectra in the dissipative magnetospheres of pulsars.","The dissipative magnetospheres are simulated by the pseudo-spectral method with the combined force-free and Aristotelian electrodynamics, which can produce self-consistent accelerating electric fields mainly distributed in the equatorial current sheet outside the light cylinder.","The multiwavelength light curves and spectra are computed by using the multiple emission mechanisms of both the primary particles accelerated by the accelerating electric fields in the equatorial current sheet and the secondary pairs with an assumed distribution spectrum.","We then compare the predicted multiwavelength light curves and spectra with the observed data from the Crab, Vela, and Geminga pulsars.","Our modeling results can systematically well reproduce the observed trends of the multiwavelength light curves and the spectra for these three pulsars."],"url":"http://arxiv.org/abs/2403.06282v1","category":"astro-ph.HE"}
{"created":"2024-03-10 17:06:17","title":"An adaptive mesh refinement strategy to ensure quasi-optimality of the conforming finite element method for the Helmholtz equation via T-coercivity","abstract":"It is well known that the quasi-optimality of the Galerkin finite element method for the Helmholtz equation is dependent on the mesh size and the wave-number. In literature, different criteria have been proposed to ensure quasi-optimality. Often these criteria are difficult to obtain and depend on wave-number explicit regularity estimates. In the present work, we focus on criteria based on T-coercivity and weak T-coercivity, which highlight mesh size dependence on the gap between the square of the wavenumber and Laplace eigenvalues. We also propose an adaptive scheme, coupled with a residual-based indicator, for optimal mesh generation with minimal degrees of freedom.","sentences":["It is well known that the quasi-optimality of the Galerkin finite element method for the Helmholtz equation is dependent on the mesh size and the wave-number.","In literature, different criteria have been proposed to ensure quasi-optimality.","Often these criteria are difficult to obtain and depend on wave-number explicit regularity estimates.","In the present work, we focus on criteria based on T-coercivity and weak T-coercivity, which highlight mesh size dependence on the gap between the square of the wavenumber and Laplace eigenvalues.","We also propose an adaptive scheme, coupled with a residual-based indicator, for optimal mesh generation with minimal degrees of freedom."],"url":"http://arxiv.org/abs/2403.06266v1","category":"math.NA"}
{"created":"2024-03-10 16:57:51","title":"SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations","abstract":"There is a growing interest in cost-effective self-supervised fine-tuning (SSFT) of self-supervised learning (SSL)-based speech models to obtain task-specific representations. These task-specific representations are used for robust performance on various downstream tasks by fine-tuning on the labelled data. This work presents a cost-effective SSFT method named Self-supervised Correspondence (SCORE) fine-tuning to adapt the SSL speech representations for content-related tasks. The proposed method uses a correspondence training strategy, aiming to learn similar representations from perturbed speech and original speech. Commonly used data augmentation techniques for content-related tasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT outperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of fine-tuning (< 5 hrs) on a single GPU for automatic speech recognition, phoneme recognition, and query-by-example tasks, with relative improvements of 1.09%, 3.58%, and 12.65%, respectively. SCORE provides competitive results with the recently proposed SSFT method SPIN, using only 1/3 of the processed speech compared to SPIN.","sentences":["There is a growing interest in cost-effective self-supervised fine-tuning (SSFT) of self-supervised learning (SSL)-based speech models to obtain task-specific representations.","These task-specific representations are used for robust performance on various downstream tasks by fine-tuning on the labelled data.","This work presents a cost-effective SSFT method named Self-supervised Correspondence (SCORE) fine-tuning to adapt the SSL speech representations for content-related tasks.","The proposed method uses a correspondence training strategy, aiming to learn similar representations from perturbed speech and original speech.","Commonly used data augmentation techniques for content-related tasks (ASR) are applied to obtain perturbed speech.","SCORE fine-tuned HuBERT outperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of fine-tuning (< 5 hrs) on a single GPU for automatic speech recognition, phoneme recognition, and query-by-example tasks, with relative improvements of 1.09%, 3.58%, and 12.65%, respectively.","SCORE provides competitive results with the recently proposed SSFT method SPIN, using only 1/3 of the processed speech compared to SPIN."],"url":"http://arxiv.org/abs/2403.06260v1","category":"cs.CL"}
{"created":"2024-03-10 15:49:42","title":"Quantifying spin contamination in algebraic diagrammatic construction theory of electronic excitations","abstract":"Algebraic diagrammatic construction (ADC) is a computationally efficient approach for simulating excited electronic states, absorption spectra, and electron correlation. Due to their origin in perturbation theory, the single-reference ADC methods may be susceptible to spin contamination when applied to molecules with unpaired electrons. In this work, we develop an approach to quantify spin contamination in the ADC calculations of electronic excitations and apply it to a variety of open-shell molecules starting with either the unrestricted (UHF) or restricted open-shell (ROHF) Hartree-Fock reference wavefunctions. Our results show that the accuracy of low-order ADC approximations (ADC(2), ADC(3)) significantly decreases when the UHF reference spin contamination exceeds 0.05 a.u. Such strongly spin-contaminated molecules exhibit severe excited-state spin symmetry breaking that contributes to decreasing the quality of computed excitation energies and oscillator strengths. In a case study of phenyl radical, we demonstrate that spin contamination can significantly affect the simulated UV/Vis spectra, altering the relative energies, intensities, and order of electronic transitions. The results presented here motivate the development of spin-adapted ADC methods for open-shell molecules.","sentences":["Algebraic diagrammatic construction (ADC) is a computationally efficient approach for simulating excited electronic states, absorption spectra, and electron correlation.","Due to their origin in perturbation theory, the single-reference ADC methods may be susceptible to spin contamination when applied to molecules with unpaired electrons.","In this work, we develop an approach to quantify spin contamination in the ADC calculations of electronic excitations and apply it to a variety of open-shell molecules starting with either the unrestricted (UHF) or restricted open-shell (ROHF) Hartree-Fock reference wavefunctions.","Our results show that the accuracy of low-order ADC approximations (ADC(2), ADC(3)) significantly decreases when the UHF reference spin contamination exceeds 0.05 a.u.","Such strongly spin-contaminated molecules exhibit severe excited-state spin symmetry breaking that contributes to decreasing the quality of computed excitation energies and oscillator strengths.","In a case study of phenyl radical, we demonstrate that spin contamination can significantly affect the simulated UV/Vis spectra, altering the relative energies, intensities, and order of electronic transitions.","The results presented here motivate the development of spin-adapted ADC methods for open-shell molecules."],"url":"http://arxiv.org/abs/2403.06241v1","category":"physics.chem-ph"}
{"created":"2024-03-10 14:22:33","title":"Drag on Cylinders Moving in Superfluid 3He-B as the Dimension Spans the Coherence Length","abstract":"Vibrating probes when immersed in a fluid can provide powerful tools for characterising the surrounding medium. In superfluid 3He-B, a condensate of Cooper pairs, the dissipation arising from the scattering of quasiparticle excitations from a mechanical oscillator provides the basis of extremely sensitive thermometry and bolometry at sub-millikelvin temperatures. The unique properties of the Andreev reflection process in this condensate also assist by providing a significantly enhanced dissipation. While existing models for such damping on an oscillating cylinder have been verified experimentally, they are valid only for flows with scales much greater than the coherence length of 3He, which is of the order of a hundred nanometres. With our increasing proficiency in fabricating nanosized oscillators which can be readily used in this superfluid there is a pressing need for the development of new models that account for the modification of the flow around these smaller oscillators. Here we report preliminary results on measurements of the damping in superfluid 3He-B of a range of cylindrical nano-sized oscillators with radii comparable to the coherence length, and outline a model for calculating the associated drag.","sentences":["Vibrating probes when immersed in a fluid can provide powerful tools for characterising the surrounding medium.","In superfluid 3He-B, a condensate of Cooper pairs, the dissipation arising from the scattering of quasiparticle excitations from a mechanical oscillator provides the basis of extremely sensitive thermometry and bolometry at sub-millikelvin temperatures.","The unique properties of the Andreev reflection process in this condensate also assist by providing a significantly enhanced dissipation.","While existing models for such damping on an oscillating cylinder have been verified experimentally, they are valid only for flows with scales much greater than the coherence length of 3He, which is of the order of a hundred nanometres.","With our increasing proficiency in fabricating nanosized oscillators which can be readily used in this superfluid there is a pressing need for the development of new models that account for the modification of the flow around these smaller oscillators.","Here we report preliminary results on measurements of the damping in superfluid 3He-B of a range of cylindrical nano-sized oscillators with radii comparable to the coherence length, and outline a model for calculating the associated drag."],"url":"http://arxiv.org/abs/2403.06226v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-10 13:21:54","title":"Solution-Hashing Search Based on Layout-Graph Transformation for Unequal Circle Packing","abstract":"The problem of packing unequal circles into a circular container stands as a classic and challenging optimization problem in computational geometry. This study introduces a suite of innovative and efficient methods to tackle this problem. Firstly, we present a novel layout-graph transformation method that represents configurations as graphs, together with an inexact hash method facilitating fast comparison of configurations for isomorphism or similarity. Leveraging these advancements, we propose an Iterative Solution-Hashing Search algorithm adept at circumventing redundant exploration through efficient configuration recording. Additionally, we introduce several enhancements to refine the optimization and search processes, including an adaptive adjacency maintenance method, an efficient vacancy detection technique, and a Voronoi-based locating method. Through comprehensive computational experiments across various benchmark instances, our algorithm demonstrates superior performance over existing state-of-the-art methods, showcasing remarkable applicability and versatility. Notably, our algorithm surpasses the best-known results for 56 out of 179 benchmark instances while achieving parity with the remaining instances.","sentences":["The problem of packing unequal circles into a circular container stands as a classic and challenging optimization problem in computational geometry.","This study introduces a suite of innovative and efficient methods to tackle this problem.","Firstly, we present a novel layout-graph transformation method that represents configurations as graphs, together with an inexact hash method facilitating fast comparison of configurations for isomorphism or similarity.","Leveraging these advancements, we propose an Iterative Solution-Hashing Search algorithm adept at circumventing redundant exploration through efficient configuration recording.","Additionally, we introduce several enhancements to refine the optimization and search processes, including an adaptive adjacency maintenance method, an efficient vacancy detection technique, and a Voronoi-based locating method.","Through comprehensive computational experiments across various benchmark instances, our algorithm demonstrates superior performance over existing state-of-the-art methods, showcasing remarkable applicability and versatility.","Notably, our algorithm surpasses the best-known results for 56 out of 179 benchmark instances while achieving parity with the remaining instances."],"url":"http://arxiv.org/abs/2403.06211v1","category":"cs.CG"}
{"created":"2024-03-10 13:04:54","title":"Personalized LoRA for Human-Centered Text Understanding","abstract":"Effectively and efficiently adapting a pre-trained language model (PLM) for human-centered text understanding (HCTU) is challenging since user tokens are million-level in most personalized applications and do not have concrete explicit semantics. A standard and parameter-efficient approach (e.g., LoRA) necessitates memorizing numerous suits of adapters for each user. In this work, we introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework for the HCTU task. PLoRA is effective, parameter-efficient, and dynamically deploying in PLMs. Moreover, a personalized dropout and a mutual information maximizing strategies are adopted and hence the proposed PLoRA can be well adapted to few/zero-shot learning scenarios for the cold-start issue. Experiments conducted on four benchmark datasets show that the proposed method outperforms existing methods in full/few/zero-shot learning scenarios for the HCTU task, even though it has fewer trainable parameters. For reproducibility, the code for this paper is available at: https://github.com/yoyo-yun/PLoRA.","sentences":["Effectively and efficiently adapting a pre-trained language model (PLM) for human-centered text understanding (HCTU) is challenging since user tokens are million-level in most personalized applications and do not have concrete explicit semantics.","A standard and parameter-efficient approach (e.g., LoRA) necessitates memorizing numerous suits of adapters for each user.","In this work, we introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework for the HCTU task.","PLoRA is effective, parameter-efficient, and dynamically deploying in PLMs.","Moreover, a personalized dropout and a mutual information maximizing strategies are adopted and hence the proposed PLoRA can be well adapted to few/zero-shot learning scenarios for the cold-start issue.","Experiments conducted on four benchmark datasets show that the proposed method outperforms existing methods in full/few/zero-shot learning scenarios for the HCTU task, even though it has fewer trainable parameters.","For reproducibility, the code for this paper is available at: https://github.com/yoyo-yun/PLoRA."],"url":"http://arxiv.org/abs/2403.06208v1","category":"cs.CL"}
{"created":"2024-03-10 13:04:48","title":"Design and Development of a Multi-Purpose Collaborative Remote Laboratory Platform","abstract":"This work-in-progress paper presents the current development of a new collaborative remote laboratory platform. The results are intended to serve as a foundation for future research on collaborative work in remote laboratories. Our platform, standing out with its adaptive and collaborative capabilities, integrates a distributed web-application for streamlined management and engagement in diverse remote educational environments.","sentences":["This work-in-progress paper presents the current development of a new collaborative remote laboratory platform.","The results are intended to serve as a foundation for future research on collaborative work in remote laboratories.","Our platform, standing out with its adaptive and collaborative capabilities, integrates a distributed web-application for streamlined management and engagement in diverse remote educational environments."],"url":"http://arxiv.org/abs/2403.06207v1","category":"cs.CY"}
{"created":"2024-03-10 11:36:09","title":"The energy-dependent gamma-ray light curves and spectra of the Vela pulsar in the dissipative magnetospheres","abstract":"We study the pulsar energy-dependent $\\gamma$-ray light curves and spectra from curvature radiation in the dissipative magnetospheres. The dissipative magnetospheres with the combined force-free (FFE) and Aristotelian (AE) are computed by a pseudo-spectral method with the high-resolution simulation in the rotating coordinate system, which produces a near force-free field structure with the dissipative region only near the equatorial current sheet outside the light cylinder (LC). We use the test particle trajectory method to compute the energy-dependent $\\gamma$-ray light curves, phase-average and phase-resolved spectra by including both the accelerating electric field and radiation reaction. The predicted energy-dependent $\\gamma$-ray light curves and spectra are then compared with those of the Vela pulsar observed by Fermi. Our results can generally reproduce the observed trends of the energy-dependent $\\gamma$-ray light curves and spectra for the Vela pulsar.","sentences":["We study the pulsar energy-dependent $\\gamma$-ray light curves and spectra from curvature radiation in the dissipative magnetospheres.","The dissipative magnetospheres with the combined force-free (FFE) and Aristotelian (AE) are computed by a pseudo-spectral method with the high-resolution simulation in the rotating coordinate system, which produces a near force-free field structure with the dissipative region only near the equatorial current sheet outside the light cylinder (LC).","We use the test particle trajectory method to compute the energy-dependent $\\gamma$-ray light curves, phase-average and phase-resolved spectra by including both the accelerating electric field and radiation reaction.","The predicted energy-dependent $\\gamma$-ray light curves and spectra are then compared with those of the Vela pulsar observed by Fermi.","Our results can generally reproduce the observed trends of the energy-dependent $\\gamma$-ray light curves and spectra for the Vela pulsar."],"url":"http://arxiv.org/abs/2403.06179v1","category":"astro-ph.HE"}
{"created":"2024-03-10 11:02:58","title":"Universal Origin of Glassy Relaxation as Recognized by Configuration Pattern-matching","abstract":"Relaxation processes are crucial in understanding the structural rearrangements of liquids and amorphous materials. However, the overarching principle that governs these processes across vastly different materials remains an open question. Substantial analysis has been carried out based on the motions of individual particles. Here, alternatively, we propose viewing the global configuration as a single entity. We introduce a global order parameter, namely the inherent structure minimal displacement (IS Dmin), to quantify the variability of configurations by a pattern-matching technique. Through atomic simulations of seven model glass-forming liquids, we unify the influences of temperature, pressure, and perturbation time on the relaxation dissipation, via a scaling law between the mechanical damping factor and IS Dmin. Fundamentally, this scaling reflects the curvature of the local potential energy landscape. Our findings uncover a universal origin of glassy relaxation and offer an alternative approach to studying disordered systems.","sentences":["Relaxation processes are crucial in understanding the structural rearrangements of liquids and amorphous materials.","However, the overarching principle that governs these processes across vastly different materials remains an open question.","Substantial analysis has been carried out based on the motions of individual particles.","Here, alternatively, we propose viewing the global configuration as a single entity.","We introduce a global order parameter, namely the inherent structure minimal displacement (IS Dmin), to quantify the variability of configurations by a pattern-matching technique.","Through atomic simulations of seven model glass-forming liquids, we unify the influences of temperature, pressure, and perturbation time on the relaxation dissipation, via a scaling law between the mechanical damping factor and IS Dmin.","Fundamentally, this scaling reflects the curvature of the local potential energy landscape.","Our findings uncover a universal origin of glassy relaxation and offer an alternative approach to studying disordered systems."],"url":"http://arxiv.org/abs/2403.06175v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-10 08:52:48","title":"RESTORE: Towards Feature Shift for Vision-Language Prompt Learning","abstract":"Prompt learning is effective for fine-tuning foundation models to improve their generalization across a variety of downstream tasks. However, the prompts that are independently optimized along a single modality path, may sacrifice the vision-language alignment of pre-trained models in return for improved performance on specific tasks and classes, leading to poorer generalization. In this paper, we first demonstrate that prompt tuning along only one single branch of CLIP (e.g., language or vision) is the reason why the misalignment occurs. Without proper regularization across the learnable parameters in different modalities, prompt learning violates the original pre-training constraints inherent in the two-tower architecture. To address such misalignment, we first propose feature shift, which is defined as the variation of embeddings after introducing the learned prompts, to serve as an explanatory tool. We dive into its relation with generalizability and thereafter propose RESTORE, a multi-modal prompt learning method that exerts explicit constraints on cross-modal consistency. To be more specific, to prevent feature misalignment, a feature shift consistency is introduced to synchronize inter-modal feature shifts by measuring and regularizing the magnitude of discrepancy during prompt tuning. In addition, we propose a \"surgery\" block to avoid short-cut hacking, where cross-modal misalignment can still be severe if the feature shift of each modality varies drastically at the same rate. It is implemented as feed-forward adapters upon both modalities to alleviate the misalignment problem. Extensive experiments on 15 datasets demonstrate that our method outperforms the state-of-the-art prompt tuning methods without compromising feature alignment.","sentences":["Prompt learning is effective for fine-tuning foundation models to improve their generalization across a variety of downstream tasks.","However, the prompts that are independently optimized along a single modality path, may sacrifice the vision-language alignment of pre-trained models in return for improved performance on specific tasks and classes, leading to poorer generalization.","In this paper, we first demonstrate that prompt tuning along only one single branch of CLIP (e.g., language or vision) is the reason why the misalignment occurs.","Without proper regularization across the learnable parameters in different modalities, prompt learning violates the original pre-training constraints inherent in the two-tower architecture.","To address such misalignment, we first propose feature shift, which is defined as the variation of embeddings after introducing the learned prompts, to serve as an explanatory tool.","We dive into its relation with generalizability and thereafter propose RESTORE, a multi-modal prompt learning method that exerts explicit constraints on cross-modal consistency.","To be more specific, to prevent feature misalignment, a feature shift consistency is introduced to synchronize inter-modal feature shifts by measuring and regularizing the magnitude of discrepancy during prompt tuning.","In addition, we propose a \"surgery\" block to avoid short-cut hacking, where cross-modal misalignment can still be severe if the feature shift of each modality varies drastically at the same rate.","It is implemented as feed-forward adapters upon both modalities to alleviate the misalignment problem.","Extensive experiments on 15 datasets demonstrate that our method outperforms the state-of-the-art prompt tuning methods without compromising feature alignment."],"url":"http://arxiv.org/abs/2403.06136v1","category":"cs.CV"}
{"created":"2024-03-10 08:15:51","title":"In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model","abstract":"Existing pre-trained vision-language models, e.g., CLIP, have demonstrated impressive zero-shot generalization capabilities in various downstream tasks. However, the performance of these models will degrade significantly when test inputs present different distributions. To this end, we explore the concept of test-time prompt tuning (TTPT), which enables the adaptation of the CLIP model to novel downstream tasks through only one step of optimization on an unsupervised objective that involves the test sample. Motivated by in-context learning within field of natural language processing (NLP), we propose In-Context Prompt Learning (InCPL) for test-time visual recognition task. InCPL involves associating a new test sample with very few or even just one labeled example as its in-context prompt. As a result, it can reliably estimate a label for the test sample, thereby facilitating the model adaptation process. InCPL first employs a token net to represent language descriptions as visual prompts that the vision encoder of a CLIP model can comprehend. Paired with in-context examples, we further propose a context-aware unsupervised loss to optimize test sample-aware visual prompts. This optimization allows a pre-trained, frozen CLIP model to be adapted to a test sample from any task using its learned adaptive prompt. Our method has demonstrated superior performance and achieved state-of-the-art results across various downstream datasets.","sentences":["Existing pre-trained vision-language models, e.g., CLIP, have demonstrated impressive zero-shot generalization capabilities in various downstream tasks.","However, the performance of these models will degrade significantly when test inputs present different distributions.","To this end, we explore the concept of test-time prompt tuning (TTPT), which enables the adaptation of the CLIP model to novel downstream tasks through only one step of optimization on an unsupervised objective that involves the test sample.","Motivated by in-context learning within field of natural language processing (NLP), we propose In-Context Prompt Learning (InCPL) for test-time visual recognition task.","InCPL involves associating a new test sample with very few or even just one labeled example as its in-context prompt.","As a result, it can reliably estimate a label for the test sample, thereby facilitating the model adaptation process.","InCPL first employs a token net to represent language descriptions as visual prompts that the vision encoder of a CLIP model can comprehend.","Paired with in-context examples, we further propose a context-aware unsupervised loss to optimize test sample-aware visual prompts.","This optimization allows a pre-trained, frozen CLIP model to be adapted to a test sample from any task using its learned adaptive prompt.","Our method has demonstrated superior performance and achieved state-of-the-art results across various downstream datasets."],"url":"http://arxiv.org/abs/2403.06126v1","category":"cs.CV"}
{"created":"2024-03-10 07:31:06","title":"CLEAR: Cross-Transformers with Pre-trained Language Model is All you need for Person Attribute Recognition and Retrieval","abstract":"Person attribute recognition and attribute-based retrieval are two core human-centric tasks. In the recognition task, the challenge is specifying attributes depending on a person's appearance, while the retrieval task involves searching for matching persons based on attribute queries. There is a significant relationship between recognition and retrieval tasks. In this study, we demonstrate that if there is a sufficiently robust network to solve person attribute recognition, it can be adapted to facilitate better performance for the retrieval task. Another issue that needs addressing in the retrieval task is the modality gap between attribute queries and persons' images. Therefore, in this paper, we present CLEAR, a unified network designed to address both tasks. We introduce a robust cross-transformers network to handle person attribute recognition. Additionally, leveraging a pre-trained language model, we construct pseudo-descriptions for attribute queries and introduce an effective training strategy to train only a few additional parameters for adapters, facilitating the handling of the retrieval task. Finally, the unified CLEAR model is evaluated on five benchmarks: PETA, PA100K, Market-1501, RAPv2, and UPAR-2024. Without bells and whistles, CLEAR achieves state-of-the-art performance or competitive results for both tasks, significantly outperforming other competitors in terms of person retrieval performance on the widely-used Market-1501 dataset.","sentences":["Person attribute recognition and attribute-based retrieval are two core human-centric tasks.","In the recognition task, the challenge is specifying attributes depending on a person's appearance, while the retrieval task involves searching for matching persons based on attribute queries.","There is a significant relationship between recognition and retrieval tasks.","In this study, we demonstrate that if there is a sufficiently robust network to solve person attribute recognition, it can be adapted to facilitate better performance for the retrieval task.","Another issue that needs addressing in the retrieval task is the modality gap between attribute queries and persons' images.","Therefore, in this paper, we present CLEAR, a unified network designed to address both tasks.","We introduce a robust cross-transformers network to handle person attribute recognition.","Additionally, leveraging a pre-trained language model, we construct pseudo-descriptions for attribute queries and introduce an effective training strategy to train only a few additional parameters for adapters, facilitating the handling of the retrieval task.","Finally, the unified CLEAR model is evaluated on five benchmarks: PETA, PA100K, Market-1501, RAPv2, and UPAR-2024.","Without bells and whistles, CLEAR achieves state-of-the-art performance or competitive results for both tasks, significantly outperforming other competitors in terms of person retrieval performance on the widely-used Market-1501 dataset."],"url":"http://arxiv.org/abs/2403.06119v1","category":"cs.CV"}
{"created":"2024-03-10 06:07:16","title":"Generative LSTM Models and Asset Hierarchy Creation in Industrial Facilities","abstract":"In the evolving field of maintenance and reliability engineering, the organization of equipment into hierarchical structures presents both a challenge and a necessity, directly impacting the operational integrity of industrial facilities. This paper introduces an innovative approach employing machine learning, specifically Long Short-Term Memory (LSTM) models, to automate and enhance the creation and management of these hierarchies. By adapting techniques commonly used in natural language processing, the study explores the potential of LSTM models to interpret and predict relationships within equipment tags, offering a novel perspective on understanding facility design. This methodology involved character-wise tokenization of asset tags from approximately 29,000 entries across 50 upstream oil and gas facilities, followed by modeling these sequences using an LSTM-based recurrent neural network. The model's architecture capitalizes on LSTM's ability to learn long-term dependencies, facilitating the prediction of hierarchical relationships and contextual understanding of equipment tags.","sentences":["In the evolving field of maintenance and reliability engineering, the organization of equipment into hierarchical structures presents both a challenge and a necessity, directly impacting the operational integrity of industrial facilities.","This paper introduces an innovative approach employing machine learning, specifically Long Short-Term Memory (LSTM) models, to automate and enhance the creation and management of these hierarchies.","By adapting techniques commonly used in natural language processing, the study explores the potential of LSTM models to interpret and predict relationships within equipment tags, offering a novel perspective on understanding facility design.","This methodology involved character-wise tokenization of asset tags from approximately 29,000 entries across 50 upstream oil and gas facilities, followed by modeling these sequences using an LSTM-based recurrent neural network.","The model's architecture capitalizes on LSTM's ability to learn long-term dependencies, facilitating the prediction of hierarchical relationships and contextual understanding of equipment tags."],"url":"http://arxiv.org/abs/2403.06103v1","category":"cs.CE"}
{"created":"2024-03-10 06:07:06","title":"Coherent Temporal Synthesis for Incremental Action Segmentation","abstract":"Data replay is a successful incremental learning technique for images. It prevents catastrophic forgetting by keeping a reservoir of previous data, original or synthesized, to ensure the model retains past knowledge while adapting to novel concepts. However, its application in the video domain is rudimentary, as it simply stores frame exemplars for action recognition. This paper presents the first exploration of video data replay techniques for incremental action segmentation, focusing on action temporal modeling. We propose a Temporally Coherent Action (TCA) model, which represents actions using a generative model instead of storing individual frames. The integration of a conditioning variable that captures temporal coherence allows our model to understand the evolution of action features over time. Therefore, action segments generated by TCA for replay are diverse and temporally coherent. In a 10-task incremental setup on the Breakfast dataset, our approach achieves significant increases in accuracy for up to 22% compared to the baselines.","sentences":["Data replay is a successful incremental learning technique for images.","It prevents catastrophic forgetting by keeping a reservoir of previous data, original or synthesized, to ensure the model retains past knowledge while adapting to novel concepts.","However, its application in the video domain is rudimentary, as it simply stores frame exemplars for action recognition.","This paper presents the first exploration of video data replay techniques for incremental action segmentation, focusing on action temporal modeling.","We propose a Temporally Coherent Action (TCA) model, which represents actions using a generative model instead of storing individual frames.","The integration of a conditioning variable that captures temporal coherence allows our model to understand the evolution of action features over time.","Therefore, action segments generated by TCA for replay are diverse and temporally coherent.","In a 10-task incremental setup on the Breakfast dataset, our approach achieves significant increases in accuracy for up to 22% compared to the baselines."],"url":"http://arxiv.org/abs/2403.06102v1","category":"cs.CV"}
{"created":"2024-03-10 04:23:24","title":"Diffusion Models Trained with Large Data Are Transferable Visual Models","abstract":"We show that, simply initializing image understanding models using a pre-trained UNet (or transformer) of diffusion models, it is possible to achieve remarkable transferable performance on fundamental vision perception tasks using a moderate amount of target data (even synthetic data only), including monocular depth, surface normal, image segmentation, matting, human pose estimation, among virtually many others. Previous works have adapted diffusion models for various perception tasks, often reformulating these tasks as generation processes to align with the diffusion process. In sharp contrast, we demonstrate that fine-tuning these models with minimal adjustments can be a more effective alternative, offering the advantages of being embarrassingly simple and significantly faster. As the backbone network of Stable Diffusion models is trained on giant datasets comprising billions of images, we observe very robust generalization capabilities of the diffusion backbone. Experimental results showcase the remarkable transferability of the backbone of diffusion models across diverse tasks and real-world datasets.","sentences":["We show that, simply initializing image understanding models using a pre-trained UNet (or transformer) of diffusion models, it is possible to achieve remarkable transferable performance on fundamental vision perception tasks using a moderate amount of target data (even synthetic data only), including monocular depth, surface normal, image segmentation, matting, human pose estimation, among virtually many others.","Previous works have adapted diffusion models for various perception tasks, often reformulating these tasks as generation processes to align with the diffusion process.","In sharp contrast, we demonstrate that fine-tuning these models with minimal adjustments can be a more effective alternative, offering the advantages of being embarrassingly simple and significantly faster.","As the backbone network of Stable Diffusion models is trained on giant datasets comprising billions of images, we observe very robust generalization capabilities of the diffusion backbone.","Experimental results showcase the remarkable transferability of the backbone of diffusion models across diverse tasks and real-world datasets."],"url":"http://arxiv.org/abs/2403.06090v1","category":"cs.CV"}
{"created":"2024-03-10 03:43:02","title":"Multisize Dataset Condensation","abstract":"While dataset condensation effectively enhances training efficiency, its application in on-device scenarios brings unique challenges. 1) Due to the fluctuating computational resources of these devices, there's a demand for a flexible dataset size that diverges from a predefined size. 2) The limited computational power on devices often prevents additional condensation operations. These two challenges connect to the \"subset degradation problem\" in traditional dataset condensation: a subset from a larger condensed dataset is often unrepresentative compared to directly condensing the whole dataset to that smaller size. In this paper, we propose Multisize Dataset Condensation (MDC) by compressing N condensation processes into a single condensation process to obtain datasets with multiple sizes. Specifically, we introduce an \"adaptive subset loss\" on top of the basic condensation loss to mitigate the \"subset degradation problem\". Our MDC method offers several benefits: 1) No additional condensation process is required; 2) reduced storage requirement by reusing condensed images. Experiments validate our findings on networks including ConvNet, ResNet and DenseNet, and datasets including SVHN, CIFAR-10, CIFAR-100 and ImageNet. For example, we achieved 6.40% average accuracy gains on condensing CIFAR-10 to ten images per class. Code is available at: https://github.com/he-y/Multisize-Dataset-Condensation.","sentences":["While dataset condensation effectively enhances training efficiency, its application in on-device scenarios brings unique challenges.","1) Due to the fluctuating computational resources of these devices, there's a demand for a flexible dataset size that diverges from a predefined size.","2) The limited computational power on devices often prevents additional condensation operations.","These two challenges connect to the \"subset degradation problem\" in traditional dataset condensation: a subset from a larger condensed dataset is often unrepresentative compared to directly condensing the whole dataset to that smaller size.","In this paper, we propose Multisize Dataset Condensation (MDC) by compressing N condensation processes into a single condensation process to obtain datasets with multiple sizes.","Specifically, we introduce an \"adaptive subset loss\" on top of the basic condensation loss to mitigate the \"subset degradation problem\".","Our MDC method offers several benefits: 1) No additional condensation process is required; 2) reduced storage requirement by reusing condensed images.","Experiments validate our findings on networks including ConvNet, ResNet and DenseNet, and datasets including SVHN, CIFAR-10, CIFAR-100 and ImageNet.","For example, we achieved 6.40% average accuracy gains on condensing CIFAR-10 to ten images per class.","Code is available at: https://github.com/he-y/Multisize-Dataset-Condensation."],"url":"http://arxiv.org/abs/2403.06075v1","category":"cs.CV"}
{"created":"2024-03-10 03:05:59","title":"Federated Learning: Attacks, Defenses, Opportunities, and Challenges","abstract":"Using dispersed data and training, federated learning (FL) moves AI capabilities to edge devices or does tasks locally. Many consider FL the start of a new era in AI, yet it is still immature. FL has not garnered the community's trust since its security and privacy implications are controversial. FL's security and privacy concerns must be discovered, analyzed, and recorded before widespread usage and adoption. A solid comprehension of risk variables allows an FL practitioner to construct a secure environment and provide researchers with a clear perspective of potential study fields, making FL the best solution in situations where security and privacy are primary issues. This research aims to deliver a complete overview of FL's security and privacy features to help bridge the gap between current federated AI and broad adoption in the future. In this paper, we present a comprehensive overview of the attack surface to investigate FL's existing challenges and defense measures to evaluate its robustness and reliability. According to our study, security concerns regarding FL are more frequent than privacy issues. Communication bottlenecks, poisoning, and backdoor attacks represent FL's privacy's most significant security threats. In the final part, we detail future research that will assist FL in adapting to real-world settings.","sentences":["Using dispersed data and training, federated learning (FL) moves AI capabilities to edge devices or does tasks locally.","Many consider FL the start of a new era in AI, yet it is still immature.","FL has not garnered the community's trust since its security and privacy implications are controversial.","FL's security and privacy concerns must be discovered, analyzed, and recorded before widespread usage and adoption.","A solid comprehension of risk variables allows an FL practitioner to construct a secure environment and provide researchers with a clear perspective of potential study fields, making FL the best solution in situations where security and privacy are primary issues.","This research aims to deliver a complete overview of FL's security and privacy features to help bridge the gap between current federated AI and broad adoption in the future.","In this paper, we present a comprehensive overview of the attack surface to investigate FL's existing challenges and defense measures to evaluate its robustness and reliability.","According to our study, security concerns regarding FL are more frequent than privacy issues.","Communication bottlenecks, poisoning, and backdoor attacks represent FL's privacy's most significant security threats.","In the final part, we detail future research that will assist FL in adapting to real-world settings."],"url":"http://arxiv.org/abs/2403.06067v1","category":"cs.CR"}
{"created":"2024-03-10 01:34:45","title":"Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning","abstract":"Vision-Language Pre-Trained (VLP) models, such as CLIP, have demonstrated remarkable effectiveness in learning generic visual representations. Several approaches aim to efficiently adapt VLP models to downstream tasks with limited supervision, aiming to leverage the acquired knowledge from VLP models. However, these methods suffer from either introducing biased representations or requiring high computational complexity, which hinders their effectiveness in fine-tuning the CLIP model. Moreover, when a model is trained on data specific to a particular domain, its ability to generalize to uncharted domains diminishes. In this work, we propose Test-Time Distribution LearNing Adapter (TT-DNA) which directly works during the testing period. Specifically, we estimate Gaussian distributions to model visual features of the few-shot support images to capture the knowledge from the support set. The cosine similarity between query image and the feature distribution of support images is used as the prediction of visual adapter. Subsequently, the visual adapter's prediction merges with the original CLIP prediction via a residual connection, resulting in the final prediction. Our extensive experimental results on visual reasoning for human object interaction demonstrate that our proposed TT-DNA outperforms existing state-of-the-art methods by large margins.","sentences":["Vision-Language Pre-Trained (VLP) models, such as CLIP, have demonstrated remarkable effectiveness in learning generic visual representations.","Several approaches aim to efficiently adapt VLP models to downstream tasks with limited supervision, aiming to leverage the acquired knowledge from VLP models.","However, these methods suffer from either introducing biased representations or requiring high computational complexity, which hinders their effectiveness in fine-tuning the CLIP model.","Moreover, when a model is trained on data specific to a particular domain, its ability to generalize to uncharted domains diminishes.","In this work, we propose Test-Time Distribution LearNing Adapter (TT-DNA) which directly works during the testing period.","Specifically, we estimate Gaussian distributions to model visual features of the few-shot support images to capture the knowledge from the support set.","The cosine similarity between query image and the feature distribution of support images is used as the prediction of visual adapter.","Subsequently, the visual adapter's prediction merges with the original CLIP prediction via a residual connection, resulting in the final prediction.","Our extensive experimental results on visual reasoning for human object interaction demonstrate that our proposed TT-DNA outperforms existing state-of-the-art methods by large margins."],"url":"http://arxiv.org/abs/2403.06059v1","category":"cs.CV"}
{"created":"2024-03-10 00:39:52","title":"Observation of non-contact Casimir friction","abstract":"Quantum mechanics predicts the occurrence of random electromagnetic field fluctuations, or virtual photons, in vacuum. The exchange of virtual photons between two bodies in relative motion could lead to non-contact quantum vacuum friction or Casimir friction. Despite its theoretical significance, the non-contact Casimir frictional force has not been observed and its theoretical predictions have varied widely. In this work, we report the first measurement of the non-contact Casimir frictional force between two moving bodies. By employing two mechanical oscillators with resonant frequencies far lower than those in Lorentz models of electrons in dielectric materials, we have amplified the Casimir frictional force at low relative velocities by several orders of magnitude. We directly measure the non-contact Casimir frictional force between the two oscillators and show its linear dependence on velocity, proving the dissipative nature of Casimir friction. This advancement marks a pivotal contribution to the field of dissipative quantum electrodynamics and enhances our understanding of friction at the nanoscale.","sentences":["Quantum mechanics predicts the occurrence of random electromagnetic field fluctuations, or virtual photons, in vacuum.","The exchange of virtual photons between two bodies in relative motion could lead to non-contact quantum vacuum friction or Casimir friction.","Despite its theoretical significance, the non-contact Casimir frictional force has not been observed and its theoretical predictions have varied widely.","In this work, we report the first measurement of the non-contact Casimir frictional force between two moving bodies.","By employing two mechanical oscillators with resonant frequencies far lower than those in Lorentz models of electrons in dielectric materials, we have amplified the Casimir frictional force at low relative velocities by several orders of magnitude.","We directly measure the non-contact Casimir frictional force between the two oscillators and show its linear dependence on velocity, proving the dissipative nature of Casimir friction.","This advancement marks a pivotal contribution to the field of dissipative quantum electrodynamics and enhances our understanding of friction at the nanoscale."],"url":"http://arxiv.org/abs/2403.06051v1","category":"quant-ph"}
{"created":"2024-03-10 00:07:47","title":"Texture image retrieval using a classification and contourlet-based features","abstract":"In this paper, we propose a new framework for improving Content Based Image Retrieval (CBIR) for texture images. This is achieved by using a new image representation based on the RCT-Plus transform which is a novel variant of the Redundant Contourlet transform that extracts a richer directional information in the image. Moreover, the process of image search is improved through a learning-based approach where the images of the database are classified using an adapted similarity metric to the statistical modeling of the RCT-Plus transform. A query is then first classified to select the best texture class after which the retained class images are ranked to select top ones. By this, we have achieved significant improvements in the retrieval rates compared to previous CBIR schemes.","sentences":["In this paper, we propose a new framework for improving Content Based Image Retrieval (CBIR) for texture images.","This is achieved by using a new image representation based on the RCT-Plus transform which is a novel variant of the Redundant Contourlet transform that extracts a richer directional information in the image.","Moreover, the process of image search is improved through a learning-based approach where the images of the database are classified using an adapted similarity metric to the statistical modeling of the RCT-Plus transform.","A query is then first classified to select the best texture class after which the retained class images are ranked to select top ones.","By this, we have achieved significant improvements in the retrieval rates compared to previous CBIR schemes."],"url":"http://arxiv.org/abs/2403.06048v1","category":"cs.CV"}
{"created":"2024-03-09 19:29:27","title":"Fault Classification in Electrical Distribution Systems using Grassmann Manifold","abstract":"Electrical fault classification is vital for ensuring the reliability and safety of power systems. Accurate and efficient fault classification methods are essential for timely and effective maintenance. In this paper, we propose a novel approach for effective fault classification through Grassmann manifolds, which is a non-Euclidean space that captures the intrinsic structure of high-dimensional data and offers a robust framework for feature extraction. We use simulated data for electrical distribution systems with various types of electrical faults. The proposed method involves transforming the measurement fault data into Grassmann manifold space using techniques from differential geometry. This transformation aids in uncovering the underlying fault patterns and reducing the computational complexity of subsequent classification steps. To achieve fault classification, we employ a machine learning technique optimized for the Grassmann manifold. The support vector machine classifier is adapted to operate within the Grassmann manifold space, enabling effective discrimination between different fault classes. The results illustrate the efficacy of the proposed Grassmann manifold-based approach for electrical fault classification which showcases its ability to accurately differentiate between various fault types.","sentences":["Electrical fault classification is vital for ensuring the reliability and safety of power systems.","Accurate and efficient fault classification methods are essential for timely and effective maintenance.","In this paper, we propose a novel approach for effective fault classification through Grassmann manifolds, which is a non-Euclidean space that captures the intrinsic structure of high-dimensional data and offers a robust framework for feature extraction.","We use simulated data for electrical distribution systems with various types of electrical faults.","The proposed method involves transforming the measurement fault data into Grassmann manifold space using techniques from differential geometry.","This transformation aids in uncovering the underlying fault patterns and reducing the computational complexity of subsequent classification steps.","To achieve fault classification, we employ a machine learning technique optimized for the Grassmann manifold.","The support vector machine classifier is adapted to operate within the Grassmann manifold space, enabling effective discrimination between different fault classes.","The results illustrate the efficacy of the proposed Grassmann manifold-based approach for electrical fault classification which showcases its ability to accurately differentiate between various fault types."],"url":"http://arxiv.org/abs/2403.05991v1","category":"eess.SY"}
{"created":"2024-03-09 19:17:11","title":"Training physical matter to matter","abstract":"Biological systems offer a great many examples of how sophisticated, highly adapted behavior can emerge from training. Here we discuss how training might be used to impart similarly adaptive properties in physical matter. As a special form of materials processing, training differs in important ways from standard approaches of obtaining sought after material properties. In particular, rather than designing or programming the local configurations and interactions of constituents, training uses externally applied stimuli to evolve material properties. This makes it possible to obtain different functionalities from the same starting material (pluripotency). Furthermore, training evolves a material in-situ or under conditions similar to those during the intended use; thus, material performance can improve rather than degrade over time. We discuss requirements for trainability, outline recently developed training strategies for creating soft materials with multiple, targeted and adaptable functionalities, and provide examples where the concept of training has been applied to materials on length scales from the molecular to the macroscopic.","sentences":["Biological systems offer a great many examples of how sophisticated, highly adapted behavior can emerge from training.","Here we discuss how training might be used to impart similarly adaptive properties in physical matter.","As a special form of materials processing, training differs in important ways from standard approaches of obtaining sought after material properties.","In particular, rather than designing or programming the local configurations and interactions of constituents, training uses externally applied stimuli to evolve material properties.","This makes it possible to obtain different functionalities from the same starting material (pluripotency).","Furthermore, training evolves a material in-situ or under conditions similar to those during the intended use; thus, material performance can improve rather than degrade over time.","We discuss requirements for trainability, outline recently developed training strategies for creating soft materials with multiple, targeted and adaptable functionalities, and provide examples where the concept of training has been applied to materials on length scales from the molecular to the macroscopic."],"url":"http://arxiv.org/abs/2403.05990v1","category":"cond-mat.soft"}
{"created":"2024-03-09 17:35:34","title":"Harmonic Balance for Differential Constitutive Models under Oscillatory Shear","abstract":"Harmonic balance (HB) is a popular Fourier-Galerkin method used in the analysis of nonlinear vibration problems where dynamical systems are subjected to periodic forcing. We adapt HB to find the periodic steady-state response of nonlinear differential constitutive models subjected to large amplitude oscillatory shear flow. By incorporating the alternating-frequency-time scheme into HB, we develop a computer program called FLASH (acronym for Fast Large Amplitude Simulation using Harmonic balance), which makes it convenient to apply HB to any differential constitutive model. We validate FLASH by considering two representative constitutive models, viz., the exponential Phan-Thien Tanner model and a nonlinear temporary network model. In terms of accuracy and speed, FLASH outperforms the conventional approach of solving initial value problems by numerical integration via time-stepping methods often by several orders of magnitude. We discuss how FLASH can be conveniently extended for other nonlinear constitutive models, which opens up potential applications in model calibration and selection, and stability analysis.","sentences":["Harmonic balance (HB) is a popular Fourier-Galerkin method used in the analysis of nonlinear vibration problems where dynamical systems are subjected to periodic forcing.","We adapt HB to find the periodic steady-state response of nonlinear differential constitutive models subjected to large amplitude oscillatory shear flow.","By incorporating the alternating-frequency-time scheme into HB, we develop a computer program called FLASH (acronym for Fast Large Amplitude Simulation using Harmonic balance), which makes it convenient to apply HB to any differential constitutive model.","We validate FLASH by considering two representative constitutive models, viz., the exponential Phan-Thien Tanner model and a nonlinear temporary network model.","In terms of accuracy and speed, FLASH outperforms the conventional approach of solving initial value problems by numerical integration via time-stepping methods often by several orders of magnitude.","We discuss how FLASH can be conveniently extended for other nonlinear constitutive models, which opens up potential applications in model calibration and selection, and stability analysis."],"url":"http://arxiv.org/abs/2403.05971v1","category":"cond-mat.soft"}
{"created":"2024-03-09 17:28:12","title":"Electromagnetic Hybrid Beamforming for Holographic Communications","abstract":"It is well known that there is inherent radiation pattern distortion for the commercial base station antenna array, which usually needs three antenna sectors to cover the whole space. To eliminate pattern distortion and further enhance beamforming performance, we propose an electromagnetic hybrid beamforming (EHB) scheme based on a three-dimensional (3D) superdirective holographic antenna array. Specifically, EHB consists of antenna excitation current vectors (analog beamforming) and digital precoding matrices, where the implementation of analog beamforming involves the real-time adjustment of the radiation pattern to adapt it to the dynamic wireless environment. Meanwhile, the digital beamforming is optimized based on the channel characteristics of analog beamforming to further improve the achievable rate of communication systems. An electromagnetic channel model incorporating array radiation patterns and the mutual coupling effect is also developed to evaluate the benefits of our proposed scheme. Simulation results demonstrate that our proposed EHB scheme with a 3D holographic array achieves a relatively flat superdirective beamforming gain and allows for programmable focusing directions throughout the entire spatial domain. Furthermore, they also verify that the proposed scheme achieves a sum rate gain of over 150% compared to traditional beamforming algorithms.","sentences":["It is well known that there is inherent radiation pattern distortion for the commercial base station antenna array, which usually needs three antenna sectors to cover the whole space.","To eliminate pattern distortion and further enhance beamforming performance, we propose an electromagnetic hybrid beamforming (EHB) scheme based on a three-dimensional (3D) superdirective holographic antenna array.","Specifically, EHB consists of antenna excitation current vectors (analog beamforming) and digital precoding matrices, where the implementation of analog beamforming involves the real-time adjustment of the radiation pattern to adapt it to the dynamic wireless environment.","Meanwhile, the digital beamforming is optimized based on the channel characteristics of analog beamforming to further improve the achievable rate of communication systems.","An electromagnetic channel model incorporating array radiation patterns and the mutual coupling effect is also developed to evaluate the benefits of our proposed scheme.","Simulation results demonstrate that our proposed EHB scheme with a 3D holographic array achieves a relatively flat superdirective beamforming gain and allows for programmable focusing directions throughout the entire spatial domain.","Furthermore, they also verify that the proposed scheme achieves a sum rate gain of over 150% compared to traditional beamforming algorithms."],"url":"http://arxiv.org/abs/2403.05970v1","category":"cs.IT"}
{"created":"2024-03-09 15:34:33","title":"CoNFiLD: Conditional Neural Field Latent Diffusion Model Generating Spatiotemporal Turbulence","abstract":"This study introduces the Conditional Neural Field Latent Diffusion (CoNFiLD) model, a novel generative learning framework designed for rapid simulation of intricate spatiotemporal dynamics in chaotic and turbulent systems within three-dimensional irregular domains. Traditional eddy-resolved numerical simulations, despite offering detailed flow predictions, encounter significant limitations due to their extensive computational demands, restricting their applications in broader engineering contexts. In contrast, deep learning-based surrogate models promise efficient, data-driven solutions. However, their effectiveness is often compromised by a reliance on deterministic frameworks, which fall short in accurately capturing the chaotic and stochastic nature of turbulence. The CoNFiLD model addresses these challenges by synergistically integrating conditional neural field encoding with latent diffusion processes, enabling the memory-efficient and robust probabilistic generation of spatiotemporal turbulence under varied conditions. Leveraging Bayesian conditional sampling, the model can seamlessly adapt to a diverse range of turbulence generation scenarios without the necessity for retraining, covering applications from zero-shot full-field flow reconstruction using sparse sensor measurements to super-resolution generation and spatiotemporal flow data restoration. Comprehensive numerical experiments across a variety of inhomogeneous, anisotropic turbulent flows with irregular geometries have been conducted to evaluate the model's versatility and efficacy, showcasing its transformative potential in the domain of turbulence generation and the broader modeling of spatiotemporal dynamics.","sentences":["This study introduces the Conditional Neural Field Latent Diffusion (CoNFiLD) model, a novel generative learning framework designed for rapid simulation of intricate spatiotemporal dynamics in chaotic and turbulent systems within three-dimensional irregular domains.","Traditional eddy-resolved numerical simulations, despite offering detailed flow predictions, encounter significant limitations due to their extensive computational demands, restricting their applications in broader engineering contexts.","In contrast, deep learning-based surrogate models promise efficient, data-driven solutions.","However, their effectiveness is often compromised by a reliance on deterministic frameworks, which fall short in accurately capturing the chaotic and stochastic nature of turbulence.","The CoNFiLD model addresses these challenges by synergistically integrating conditional neural field encoding with latent diffusion processes, enabling the memory-efficient and robust probabilistic generation of spatiotemporal turbulence under varied conditions.","Leveraging Bayesian conditional sampling, the model can seamlessly adapt to a diverse range of turbulence generation scenarios without the necessity for retraining, covering applications from zero-shot full-field flow reconstruction using sparse sensor measurements to super-resolution generation and spatiotemporal flow data restoration.","Comprehensive numerical experiments across a variety of inhomogeneous, anisotropic turbulent flows with irregular geometries have been conducted to evaluate the model's versatility and efficacy, showcasing its transformative potential in the domain of turbulence generation and the broader modeling of spatiotemporal dynamics."],"url":"http://arxiv.org/abs/2403.05940v1","category":"physics.flu-dyn"}
{"created":"2024-03-09 14:21:30","title":"BEACON: A Bayesian Evolutionary Approach for Counterexample Generation of Control Systems","abstract":"The rigorous safety verification of control systems in critical applications is essential, given their increasing complexity and integration into everyday life. Simulation-based falsification approaches play a pivotal role in the safety verification of control systems, particularly within critical applications. These methods systematically explore the operational space of systems to identify configurations that result in violations of safety specifications. However, the effectiveness of traditional simulation-based falsification is frequently limited by the high dimensionality of the search space and the substantial computational resources required for exhaustive exploration. This paper presents BEACON, a novel framework that enhances the falsification process through a combination of Bayesian optimization and covariance matrix adaptation evolutionary strategy. By exploiting quantitative metrics to evaluate how closely a system adheres to safety specifications, BEACON advances the state-of-the-art in testing methodologies. It employs a model-based test point selection approach, designed to facilitate exploration across dynamically evolving search zones to efficiently uncover safety violations. Our findings demonstrate that BEACON not only locates a higher percentage of counterexamples compared to standalone BO but also achieves this with significantly fewer simulations than required by CMA-ES, highlighting its potential to optimize the verification process of control systems. This framework offers a promising direction for achieving thorough and resource-efficient safety evaluations, ensuring the reliability of control systems in critical applications.","sentences":["The rigorous safety verification of control systems in critical applications is essential, given their increasing complexity and integration into everyday life.","Simulation-based falsification approaches play a pivotal role in the safety verification of control systems, particularly within critical applications.","These methods systematically explore the operational space of systems to identify configurations that result in violations of safety specifications.","However, the effectiveness of traditional simulation-based falsification is frequently limited by the high dimensionality of the search space and the substantial computational resources required for exhaustive exploration.","This paper presents BEACON, a novel framework that enhances the falsification process through a combination of Bayesian optimization and covariance matrix adaptation evolutionary strategy.","By exploiting quantitative metrics to evaluate how closely a system adheres to safety specifications, BEACON advances the state-of-the-art in testing methodologies.","It employs a model-based test point selection approach, designed to facilitate exploration across dynamically evolving search zones to efficiently uncover safety violations.","Our findings demonstrate that BEACON not only locates a higher percentage of counterexamples compared to standalone BO but also achieves this with significantly fewer simulations than required by CMA-ES, highlighting its potential to optimize the verification process of control systems.","This framework offers a promising direction for achieving thorough and resource-efficient safety evaluations, ensuring the reliability of control systems in critical applications."],"url":"http://arxiv.org/abs/2403.05925v1","category":"eess.SY"}
{"created":"2024-03-09 13:37:02","title":"Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic Segmentation","abstract":"Tumor lesion segmentation on CT or MRI images plays a critical role in cancer diagnosis and treatment planning. Considering the inherent differences in tumor lesion segmentation data across various medical imaging modalities and equipment, integrating medical knowledge into the Segment Anything Model (SAM) presents promising capability due to its versatility and generalization potential. Recent studies have attempted to enhance SAM with medical expertise by pre-training on large-scale medical segmentation datasets. However, challenges still exist in 3D tumor lesion segmentation owing to tumor complexity and the imbalance in foreground and background regions. Therefore, we introduce Mask-Enhanced SAM (M-SAM), an innovative architecture tailored for 3D tumor lesion segmentation. We propose a novel Mask-Enhanced Adapter (MEA) within M-SAM that enriches the semantic information of medical images with positional data from coarse segmentation masks, facilitating the generation of more precise segmentation masks. Furthermore, an iterative refinement scheme is implemented in M-SAM to refine the segmentation masks progressively, leading to improved performance. Extensive experiments on seven tumor lesion segmentation datasets indicate that our M-SAM not only achieves high segmentation accuracy but also exhibits robust generalization.","sentences":["Tumor lesion segmentation on CT or MRI images plays a critical role in cancer diagnosis and treatment planning.","Considering the inherent differences in tumor lesion segmentation data across various medical imaging modalities and equipment, integrating medical knowledge into the Segment Anything Model (SAM) presents promising capability due to its versatility and generalization potential.","Recent studies have attempted to enhance SAM with medical expertise by pre-training on large-scale medical segmentation datasets.","However, challenges still exist in 3D tumor lesion segmentation owing to tumor complexity and the imbalance in foreground and background regions.","Therefore, we introduce Mask-Enhanced SAM (M-SAM), an innovative architecture tailored for 3D tumor lesion segmentation.","We propose a novel Mask-Enhanced Adapter (MEA) within M-SAM that enriches the semantic information of medical images with positional data from coarse segmentation masks, facilitating the generation of more precise segmentation masks.","Furthermore, an iterative refinement scheme is implemented in M-SAM to refine the segmentation masks progressively, leading to improved performance.","Extensive experiments on seven tumor lesion segmentation datasets indicate that our M-SAM not only achieves high segmentation accuracy but also exhibits robust generalization."],"url":"http://arxiv.org/abs/2403.05912v1","category":"eess.IV"}
{"created":"2024-03-09 13:28:42","title":"XFLUIDS: A SYCL-based unified cross-architecture heterogeneous simulation solver for compressible reacting flows","abstract":"We present a cross-architecture large-scale heterogeneous Navier-Stokes equation solver, XFLUIDS, for compressible reacting multicomponent flows in the three-dimensional structured mesh crossing different GPU platforms. The multi-component reacting flows are ubiquitous in many situations, with high-order FDM schemes used and calculation of reaction source integration, their numerical simulations are time-consuming to achieve high accuracy and capture the underlying multiscale features, and GPU-based acceleration methods are extremely helpful. At the same time, various accelerators have been released in the market, leading to a challenge of applications' adaptability. Based on an implementation of a heterogeneous programming standard, SYCL, it is now possible to develop heterogeneous parallel acceleration applications directly targeting multi-core CPUs, FPGAs, and GPUs from Nvidia, AMD, as well as Intel without translating any source code, demonstrating its high portability. MPI library is used to extend the solver to multi-GPU accelerated computing, with the GPU-ENABLED MPI technology supporting the direct data transmission between GPUs to reduce communication latency, and the weak-scaling and strong-scaling efficiency tested. This solver has been validated by several benchmark cases, including shock tube problems, diffusion problems, nitrogen dissociation problems, shock-bubble interaction problems, etc.","sentences":["We present a cross-architecture large-scale heterogeneous Navier-Stokes equation solver, XFLUIDS, for compressible reacting multicomponent flows in the three-dimensional structured mesh crossing different GPU platforms.","The multi-component reacting flows are ubiquitous in many situations, with high-order FDM schemes used and calculation of reaction source integration, their numerical simulations are time-consuming to achieve high accuracy and capture the underlying multiscale features, and GPU-based acceleration methods are extremely helpful.","At the same time, various accelerators have been released in the market, leading to a challenge of applications' adaptability.","Based on an implementation of a heterogeneous programming standard, SYCL, it is now possible to develop heterogeneous parallel acceleration applications directly targeting multi-core CPUs, FPGAs, and GPUs from Nvidia, AMD, as well as Intel without translating any source code, demonstrating its high portability.","MPI library is used to extend the solver to multi-GPU accelerated computing, with the GPU-ENABLED MPI technology supporting the direct data transmission between GPUs to reduce communication latency, and the weak-scaling and strong-scaling efficiency tested.","This solver has been validated by several benchmark cases, including shock tube problems, diffusion problems, nitrogen dissociation problems, shock-bubble interaction problems, etc."],"url":"http://arxiv.org/abs/2403.05910v1","category":"physics.comp-ph"}
{"created":"2024-03-09 13:23:14","title":"Low-Rank Variational Quantum Algorithm for the Dynamics of Open Quantum Systems","abstract":"The simulation of many-body open quantum systems is key to solving numerous outstanding problems in physics, chemistry, material science, and in the development of quantum technologies. Near-term quantum computers may bring considerable advantage for the efficient simulation of their static and dynamical properties, thanks to hybrid quantum-classical variational algorithms to approximate the dynamics of the density matrix describing the quantum state in terms of an ensemble average. Here, a variational quantum algorithm is developed to simulate the real-time evolution of the density matrix governed by the Lindblad master equation, under the assumption that the quantum state has a bounded entropy along the dynamics, entailing a low-rank representation of its density matrix. The algorithm encodes each pure state of the statistical mixture as a parametrized quantum circuit, and the associated probabilities as additional variational parameters stored classically, thereby requiring a significantly lower number of qubits than algorithms where the full density matrix is encoded in the quantum memory. Two variational Ans\\\"atze are proposed, and their effectiveness is assessed in the simulation of the dynamics of a 2D dissipative transverse field Ising model. The results underscore the algorithm's efficiency in simulating the dynamics of open quantum systems in the low-rank regime with limited quantum resources on a near-term quantum device.","sentences":["The simulation of many-body open quantum systems is key to solving numerous outstanding problems in physics, chemistry, material science, and in the development of quantum technologies.","Near-term quantum computers may bring considerable advantage for the efficient simulation of their static and dynamical properties, thanks to hybrid quantum-classical variational algorithms to approximate the dynamics of the density matrix describing the quantum state in terms of an ensemble average.","Here, a variational quantum algorithm is developed to simulate the real-time evolution of the density matrix governed by the Lindblad master equation, under the assumption that the quantum state has a bounded entropy along the dynamics, entailing a low-rank representation of its density matrix.","The algorithm encodes each pure state of the statistical mixture as a parametrized quantum circuit, and the associated probabilities as additional variational parameters stored classically, thereby requiring a significantly lower number of qubits than algorithms where the full density matrix is encoded in the quantum memory.","Two variational Ans\\\"atze are proposed, and their effectiveness is assessed in the simulation of the dynamics of a 2D dissipative transverse field Ising model.","The results underscore the algorithm's efficiency in simulating the dynamics of open quantum systems in the low-rank regime with limited quantum resources on a near-term quantum device."],"url":"http://arxiv.org/abs/2403.05908v1","category":"quant-ph"}
{"created":"2024-03-09 12:38:35","title":"Unleashing the Power of T1-cells in SFQ Arithmetic Circuits","abstract":"Rapid single-flux quantum (RSFQ), a leading cryogenic superconductive electronics (SCE) technology, offers extremely low power dissipation and high speed. However, implementing RSFQ systems at VLSI complexity faces challenges, such as substantial area overhead from gate-level pipelining and path balancing, exacerbated by RSFQ's limited layout density. T1 flip-flop (T1-FF) is an RSFQ logic cell operating as a pulse counter. Using T1-FF the full adder function can be realized with only 40% of the area required by the conventional realization. This cell however imposes complex constraints on input signal timing, complicating its use. Multiphase clocking has been recently proposed to alleviate gate-level pipelining overhead. The fanin signals can be efficiently controlled using multiphase clocking. We present the novel two-stage SFQ technology mapping methodology supporting the T1-FF. Compatible parts of the SFQ network are first replaced by the efficient T1-FFs. Multiphase retiming is next applied to assign clock phases to each logic gate and insert DFFs to satisfy the input timing. Using our flow, the area of the SFQ networks is reduced, on average, by 6% with up to 25% reduction in optimizing the 128-bit adder.","sentences":["Rapid single-flux quantum (RSFQ), a leading cryogenic superconductive electronics (SCE) technology, offers extremely low power dissipation and high speed.","However, implementing RSFQ systems at VLSI complexity faces challenges, such as substantial area overhead from gate-level pipelining and path balancing, exacerbated by RSFQ's limited layout density.","T1 flip-flop (T1-FF) is an RSFQ logic cell operating as a pulse counter.","Using T1-FF the full adder function can be realized with only 40% of the area required by the conventional realization.","This cell however imposes complex constraints on input signal timing, complicating its use.","Multiphase clocking has been recently proposed to alleviate gate-level pipelining overhead.","The fanin signals can be efficiently controlled using multiphase clocking.","We present the novel two-stage SFQ technology mapping methodology supporting the T1-FF.","Compatible parts of the SFQ network are first replaced by the efficient T1-FFs.","Multiphase retiming is next applied to assign clock phases to each logic gate and insert DFFs to satisfy the input timing.","Using our flow, the area of the SFQ networks is reduced, on average, by 6% with up to 25% reduction in optimizing the 128-bit adder."],"url":"http://arxiv.org/abs/2403.05901v1","category":"cs.ET"}
{"created":"2024-03-09 12:25:01","title":"RealNet: A Feature Selection Network with Realistic Synthetic Anomaly for Anomaly Detection","abstract":"Self-supervised feature reconstruction methods have shown promising advances in industrial image anomaly detection and localization. Despite this progress, these methods still face challenges in synthesizing realistic and diverse anomaly samples, as well as addressing the feature redundancy and pre-training bias of pre-trained feature. In this work, we introduce RealNet, a feature reconstruction network with realistic synthetic anomaly and adaptive feature selection. It is incorporated with three key innovations: First, we propose Strength-controllable Diffusion Anomaly Synthesis (SDAS), a diffusion process-based synthesis strategy capable of generating samples with varying anomaly strengths that mimic the distribution of real anomalous samples. Second, we develop Anomaly-aware Features Selection (AFS), a method for selecting representative and discriminative pre-trained feature subsets to improve anomaly detection performance while controlling computational costs. Third, we introduce Reconstruction Residuals Selection (RRS), a strategy that adaptively selects discriminative residuals for comprehensive identification of anomalous regions across multiple levels of granularity. We assess RealNet on four benchmark datasets, and our results demonstrate significant improvements in both Image AUROC and Pixel AUROC compared to the current state-o-the-art methods. The code, data, and models are available at https://github.com/cnulab/RealNet.","sentences":["Self-supervised feature reconstruction methods have shown promising advances in industrial image anomaly detection and localization.","Despite this progress, these methods still face challenges in synthesizing realistic and diverse anomaly samples, as well as addressing the feature redundancy and pre-training bias of pre-trained feature.","In this work, we introduce RealNet, a feature reconstruction network with realistic synthetic anomaly and adaptive feature selection.","It is incorporated with three key innovations: First, we propose Strength-controllable Diffusion Anomaly Synthesis (SDAS), a diffusion process-based synthesis strategy capable of generating samples with varying anomaly strengths that mimic the distribution of real anomalous samples.","Second, we develop Anomaly-aware Features Selection (AFS), a method for selecting representative and discriminative pre-trained feature subsets to improve anomaly detection performance while controlling computational costs.","Third, we introduce Reconstruction Residuals Selection (RRS), a strategy that adaptively selects discriminative residuals for comprehensive identification of anomalous regions across multiple levels of granularity.","We assess RealNet on four benchmark datasets, and our results demonstrate significant improvements in both Image AUROC and Pixel AUROC compared to the current state-o-the-art methods.","The code, data, and models are available at https://github.com/cnulab/RealNet."],"url":"http://arxiv.org/abs/2403.05897v1","category":"cs.CV"}
{"created":"2024-03-09 11:56:26","title":"Generalizing to Out-of-Sample Degradations via Model Reprogramming","abstract":"Existing image restoration models are typically designed for specific tasks and struggle to generalize to out-of-sample degradations not encountered during training. While zero-shot methods can address this limitation by fine-tuning model parameters on testing samples, their effectiveness relies on predefined natural priors and physical models of specific degradations. Nevertheless, determining out-of-sample degradations faced in real-world scenarios is always impractical. As a result, it is more desirable to train restoration models with inherent generalization ability. To this end, this work introduces the Out-of-Sample Restoration (OSR) task, which aims to develop restoration models capable of handling out-of-sample degradations. An intuitive solution involves pre-translating out-of-sample degradations to known degradations of restoration models. However, directly translating them in the image space could lead to complex image translation issues. To address this issue, we propose a model reprogramming framework, which translates out-of-sample degradations by quantum mechanic and wave functions. Specifically, input images are decoupled as wave functions of amplitude and phase terms. The translation of out-of-sample degradation is performed by adapting the phase term. Meanwhile, the image content is maintained and enhanced in the amplitude term. By taking these two terms as inputs, restoration models are able to handle out-of-sample degradations without fine-tuning. Through extensive experiments across multiple evaluation cases, we demonstrate the effectiveness and flexibility of our proposed framework. Our codes are available at \\href{https://github.com/ddghjikle/Out-of-sample-restoration}{Github}.","sentences":["Existing image restoration models are typically designed for specific tasks and struggle to generalize to out-of-sample degradations not encountered during training.","While zero-shot methods can address this limitation by fine-tuning model parameters on testing samples, their effectiveness relies on predefined natural priors and physical models of specific degradations.","Nevertheless, determining out-of-sample degradations faced in real-world scenarios is always impractical.","As a result, it is more desirable to train restoration models with inherent generalization ability.","To this end, this work introduces the Out-of-Sample Restoration (OSR) task, which aims to develop restoration models capable of handling out-of-sample degradations.","An intuitive solution involves pre-translating out-of-sample degradations to known degradations of restoration models.","However, directly translating them in the image space could lead to complex image translation issues.","To address this issue, we propose a model reprogramming framework, which translates out-of-sample degradations by quantum mechanic and wave functions.","Specifically, input images are decoupled as wave functions of amplitude and phase terms.","The translation of out-of-sample degradation is performed by adapting the phase term.","Meanwhile, the image content is maintained and enhanced in the amplitude term.","By taking these two terms as inputs, restoration models are able to handle out-of-sample degradations without fine-tuning.","Through extensive experiments across multiple evaluation cases, we demonstrate the effectiveness and flexibility of our proposed framework.","Our codes are available at \\href{https://github.com/ddghjikle/Out-of-sample-restoration}{Github}."],"url":"http://arxiv.org/abs/2403.05886v1","category":"cs.CV"}
{"created":"2024-03-09 11:10:38","title":"A molecular dynamics simulation of the abrupt changes in the thermodynamic properties of water after formation of nano-bubbles / nano-cavities induced by passage of charged particles","abstract":"We present a multi-scale formalism that accounts for the formation of nano-scale bubbles/cavities owing to a burst of water molecules after the passage of high energy charged particles that leads to the formation of hot non-ionizing excitations or thermal spikes (TS). We demonstrate the coexistence of a rapidly growing condensed state of water and a hot spot that forms a stable state of diluted water at high temperatures and pressures, possibly at a supercritical phase. Depending on the temperature of TS, the thin shell of a highly dense state of water grows by three to five times the speed of sound in water, forming a thin layer of shock wave (SW) buffer, wrapping around the nano-scale cylindrical symmetric cavity. The stability of the cavity, as a result of the incompressibility of water at ambient conditions and the surface tension, allows the transition of supersonic SW to a subsonic contact discontinuity and dissipation to thermo-acoustic sound waves. We further study the mergers of nanobubbles that lead to fountain-like or jet-flow structures at the collision interface. We introduce a time delay in the nucleation of nano-bubbles, a novel mechanism, responsible for the growth and stability of much larger or even micro-bubbles, possibly relevant to FLASH ultra-high dose rate (UHDR). The current study is potentially significant at FLASH-UHDRs. Our analysis predicts the black-body radiation from the transient supercritical state of water localized in nano-cavities wrapping around the track of charged particles can be manifested in the (indirect) water luminescence spectrum.","sentences":["We present a multi-scale formalism that accounts for the formation of nano-scale bubbles/cavities owing to a burst of water molecules after the passage of high energy charged particles that leads to the formation of hot non-ionizing excitations or thermal spikes (TS).","We demonstrate the coexistence of a rapidly growing condensed state of water and a hot spot that forms a stable state of diluted water at high temperatures and pressures, possibly at a supercritical phase.","Depending on the temperature of TS, the thin shell of a highly dense state of water grows by three to five times the speed of sound in water, forming a thin layer of shock wave (SW) buffer, wrapping around the nano-scale cylindrical symmetric cavity.","The stability of the cavity, as a result of the incompressibility of water at ambient conditions and the surface tension, allows the transition of supersonic SW to a subsonic contact discontinuity and dissipation to thermo-acoustic sound waves.","We further study the mergers of nanobubbles that lead to fountain-like or jet-flow structures at the collision interface.","We introduce a time delay in the nucleation of nano-bubbles, a novel mechanism, responsible for the growth and stability of much larger or even micro-bubbles, possibly relevant to FLASH ultra-high dose rate (UHDR).","The current study is potentially significant at FLASH-UHDRs.","Our analysis predicts the black-body radiation from the transient supercritical state of water localized in nano-cavities wrapping around the track of charged particles can be manifested in the (indirect) water luminescence spectrum."],"url":"http://arxiv.org/abs/2403.05880v1","category":"physics.med-ph"}
{"created":"2024-03-09 11:02:47","title":"A Performance Analysis of Basin Hopping Compared to Established Metaheuristics for Global Optimization","abstract":"During the last decades many metaheuristics for global numerical optimization have been proposed. Among them, Basin Hopping is very simple and straightforward to implement, although rarely used outside its original Physical Chemistry community. In this work, our aim is to compare Basin Hopping, and two population variants of it, with readily available implementations of the well known metaheuristics Differential Evolution, Particle Swarm Optimization, and Covariance Matrix Adaptation Evolution Strategy. We perform numerical experiments using the IOH profiler environment with the BBOB test function set and two difficult real-world problems. The experiments were carried out in two different but complementary ways: by measuring the performance under a fixed budget of function evaluations and by considering a fixed target value. The general conclusion is that Basin Hopping and its newly introduced population variant are almost as good as Covariance Matrix Adaptation on the synthetic benchmark functions and better than it on the two hard cluster energy minimization problems. Thus, the proposed analyses show that Basin Hopping can be considered a good candidate for global numerical optimization problems along with the more established metaheuristics, especially if one wants to obtain quick and reliable results on an unknown problem.","sentences":["During the last decades many metaheuristics for global numerical optimization have been proposed.","Among them, Basin Hopping is very simple and straightforward to implement, although rarely used outside its original Physical Chemistry community.","In this work, our aim is to compare Basin Hopping, and two population variants of it, with readily available implementations of the well known metaheuristics Differential Evolution, Particle Swarm Optimization, and Covariance Matrix Adaptation Evolution Strategy.","We perform numerical experiments using the IOH profiler environment with the BBOB test function set and two difficult real-world problems.","The experiments were carried out in two different but complementary ways: by measuring the performance under a fixed budget of function evaluations and by considering a fixed target value.","The general conclusion is that Basin Hopping and its newly introduced population variant are almost as good as Covariance Matrix Adaptation on the synthetic benchmark functions and better than it on the two hard cluster energy minimization problems.","Thus, the proposed analyses show that Basin Hopping can be considered a good candidate for global numerical optimization problems along with the more established metaheuristics, especially if one wants to obtain quick and reliable results on an unknown problem."],"url":"http://arxiv.org/abs/2403.05877v1","category":"cs.NE"}
{"created":"2024-03-09 10:24:12","title":"PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems","abstract":"Reinforcement Learning (RL) has increasingly become a preferred method over traditional rule-based systems in diverse human-in-the-loop (HITL) applications due to its adaptability to the dynamic nature of human interactions. However, integrating RL in such settings raises significant privacy concerns, as it might inadvertently expose sensitive user information. Addressing this, our paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy through exploiting an early-exit approach designed explicitly for privacy preservation in HITL environments. This approach dynamically adjusts the tradeoff between privacy protection and system utility, tailoring its operation to individual behavioral patterns and preferences. We mainly highlight the challenge of dealing with the variable and evolving nature of human behavior, which renders static privacy models ineffective. PAPER-HILT's effectiveness is evaluated through its application in two distinct contexts: Smart Home environments and Virtual Reality (VR) Smart Classrooms. The empirical results demonstrate PAPER-HILT's capability to provide a personalized equilibrium between user privacy and application utility, adapting effectively to individual user needs and preferences. On average for both experiments, utility (performance) drops by 24%, and privacy (state prediction) improves by 31%.","sentences":["Reinforcement Learning (RL) has increasingly become a preferred method over traditional rule-based systems in diverse human-in-the-loop (HITL) applications due to its adaptability to the dynamic nature of human interactions.","However, integrating RL in such settings raises significant privacy concerns, as it might inadvertently expose sensitive user information.","Addressing this, our paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy through exploiting an early-exit approach designed explicitly for privacy preservation in HITL environments.","This approach dynamically adjusts the tradeoff between privacy protection and system utility, tailoring its operation to individual behavioral patterns and preferences.","We mainly highlight the challenge of dealing with the variable and evolving nature of human behavior, which renders static privacy models ineffective.","PAPER-HILT's effectiveness is evaluated through its application in two distinct contexts: Smart Home environments and Virtual Reality (VR) Smart Classrooms.","The empirical results demonstrate PAPER-HILT's capability to provide a personalized equilibrium between user privacy and application utility, adapting effectively to individual user needs and preferences.","On average for both experiments, utility (performance) drops by 24%, and privacy (state prediction) improves by 31%."],"url":"http://arxiv.org/abs/2403.05864v1","category":"cs.LG"}
{"created":"2024-03-09 09:54:44","title":"POV: Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object Interaction in the Multi-View World","abstract":"We humans are good at translating third-person observations of hand-object interactions (HOI) into an egocentric view. However, current methods struggle to replicate this ability of view adaptation from third-person to first-person. Although some approaches attempt to learn view-agnostic representation from large-scale video datasets, they ignore the relationships among multiple third-person views. To this end, we propose a Prompt-Oriented View-agnostic learning (POV) framework in this paper, which enables this view adaptation with few egocentric videos. Specifically, We introduce interactive masking prompts at the frame level to capture fine-grained action information, and view-aware prompts at the token level to learn view-agnostic representation. To verify our method, we establish two benchmarks for transferring from multiple third-person views to the egocentric view. Our extensive experiments on these benchmarks demonstrate the efficiency and effectiveness of our POV framework and prompt tuning techniques in terms of view adaptation and view generalization. Our code is available at \\url{https://github.com/xuboshen/pov_acmmm2023}.","sentences":["We humans are good at translating third-person observations of hand-object interactions (HOI) into an egocentric view.","However, current methods struggle to replicate this ability of view adaptation from third-person to first-person.","Although some approaches attempt to learn view-agnostic representation from large-scale video datasets, they ignore the relationships among multiple third-person views.","To this end, we propose a Prompt-Oriented View-agnostic learning (POV) framework in this paper, which enables this view adaptation with few egocentric videos.","Specifically, We introduce interactive masking prompts at the frame level to capture fine-grained action information, and view-aware prompts at the token level to learn view-agnostic representation.","To verify our method, we establish two benchmarks for transferring from multiple third-person views to the egocentric view.","Our extensive experiments on these benchmarks demonstrate the efficiency and effectiveness of our POV framework and prompt tuning techniques in terms of view adaptation and view generalization.","Our code is available at \\url{https://github.com/xuboshen/pov_acmmm2023}."],"url":"http://arxiv.org/abs/2403.05856v1","category":"cs.CV"}
{"created":"2024-03-11 17:50:20","title":"POD-ROM methods: from a finite set of snapshots to continuous-in-time approximations","abstract":"This paper studies discretization of time-dependent partial differential equations (PDEs) by proper orthogonal decomposition reduced order models (POD-ROMs). Most of the analysis in the literature has been performed on fully-discrete methods using first order methods in time, typically the implicit Euler time integrator. Our aim is to show which kind of error bounds can be obtained using any time integrator, both in the full order model (FOM), applied to compute the snapshots, and in the POD-ROM method. To this end, we analyze in this paper the continuous-in-time case for both the FOM and POD-ROM methods, although the POD basis is obtained from snapshots taken at a discrete (i.e., not continuous) set times. Two cases for the set of snapshots are considered: The case in which the snapshots are based on first order divided differences in time and the case in which they are based on temporal derivatives. Optimal pointwise-in-time error bounds {between the FOM and the POD-ROM solutions} are proved for the $L^2(\\Omega)$ norm of the error for a semilinear reaction-diffusion model problem. The dependency of the errors on the distance in time between two consecutive snapshots and on the tail of the POD eigenvalues is tracked. Our detailed analysis allows to show that, in some situations, a small number of snapshots in a given time interval might be sufficient to accurately approximate the solution in the full interval. Numerical studies support the error analysis.","sentences":["This paper studies discretization of time-dependent partial differential equations (PDEs) by proper orthogonal decomposition reduced order models (POD-ROMs).","Most of the analysis in the literature has been performed on fully-discrete methods using first order methods in time, typically the implicit Euler time integrator.","Our aim is to show which kind of error bounds can be obtained using any time integrator, both in the full order model (FOM), applied to compute the snapshots, and in the POD-ROM method.","To this end, we analyze in this paper the continuous-in-time case for both the FOM and POD-ROM methods, although the POD basis is obtained from snapshots taken at a discrete (i.e., not continuous) set times.","Two cases for the set of snapshots are considered: The case in which the snapshots are based on first order divided differences in time and the case in which they are based on temporal derivatives.","Optimal pointwise-in-time error bounds {between the FOM and the POD-ROM solutions} are proved for the $L^2(\\Omega)$ norm of the error for a semilinear reaction-diffusion model problem.","The dependency of the errors on the distance in time between two consecutive snapshots and on the tail of the POD eigenvalues is tracked.","Our detailed analysis allows to show that, in some situations, a small number of snapshots in a given time interval might be sufficient to accurately approximate the solution in the full interval.","Numerical studies support the error analysis."],"url":"http://arxiv.org/abs/2403.06967v1","category":"math.NA"}
{"created":"2024-03-11 17:19:40","title":"On the stability of fully nonlinear hydraulic-fall solutions to the forced water-wave problem","abstract":"Two-dimensional free-surface flow over localised topography is examined with the emphasis on the stability of hydraulic-fall solutions. A Gaussian topography profile is assumed with a positive or negative amplitude modelling a bump or a dip, respectively. Steady hydraulic-fall solutions to the full incompressible, irrotational Euler equations are computed, and their linear and nonlinear stability is analysed by computing eigenspectra of the pertinent linearised operator and by solving an initial value problem. The computations are carried out numerically using a specially developed computational framework based on the finite element method. The Hamiltonian structure of the problem is demonstrated and stability is determined by computing eigenspectra of the pertinent linearised operator. It is found that a hydraulic-fall flow over a bump is spectrally stable. The corresponding flow over a dip is found to be linearly unstable. In the latter case, time-dependent simulations show that the flow ultimately settles into a time-periodic motion that corresponds to an invariant solution in an appropriately defined phase space. Physically, the solution consists of a localised large amplitude wave that pulsates above the dip while simultaneously emitting nonlinear cnoidal waves in the upstream direction and multi-harmonic linear waves in the downstream direction.","sentences":["Two-dimensional free-surface flow over localised topography is examined with the emphasis on the stability of hydraulic-fall solutions.","A Gaussian topography profile is assumed with a positive or negative amplitude modelling a bump or a dip, respectively.","Steady hydraulic-fall solutions to the full incompressible, irrotational Euler equations are computed, and their linear and nonlinear stability is analysed by computing eigenspectra of the pertinent linearised operator and by solving an initial value problem.","The computations are carried out numerically using a specially developed computational framework based on the finite element method.","The Hamiltonian structure of the problem is demonstrated and stability is determined by computing eigenspectra of the pertinent linearised operator.","It is found that a hydraulic-fall flow over a bump is spectrally stable.","The corresponding flow over a dip is found to be linearly unstable.","In the latter case, time-dependent simulations show that the flow ultimately settles into a time-periodic motion that corresponds to an invariant solution in an appropriately defined phase space.","Physically, the solution consists of a localised large amplitude wave that pulsates above the dip while simultaneously emitting nonlinear cnoidal waves in the upstream direction and multi-harmonic linear waves in the downstream direction."],"url":"http://arxiv.org/abs/2403.06933v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 16:22:03","title":"Exact Multi-Point Correlations in the Stochastic Heat Equation for Strictly Sublinear Coordinates","abstract":"We consider the Stochastic Heat Equation (SHE) in $(1+1)$ dimensions with delta Dirac initial data and spacetime white noise. We prove exact large-time asymptotics for multi-point correlations of the SHE for strictly sublinear space coordinates. The sublinear condition is optimal, in the sense that different asymptotics are known to occur when the space coordinates grow linearly [Lin 2023, Theorem 1.1]. Lastly, a notable feature of our result is that it confirms the connection between multi-point correlations in the SHE and the ground state of the Hamiltonian of the delta-Bose gas.","sentences":["We consider the Stochastic Heat Equation (SHE) in $(1+1)$ dimensions with delta Dirac initial data and spacetime white noise.","We prove exact large-time asymptotics for multi-point correlations of the SHE for strictly sublinear space coordinates.","The sublinear condition is optimal, in the sense that different asymptotics are known to occur when the space coordinates grow linearly [Lin 2023, Theorem 1.1].","Lastly, a notable feature of our result is that it confirms the connection between multi-point correlations in the SHE and the ground state of the Hamiltonian of the delta-Bose gas."],"url":"http://arxiv.org/abs/2403.06868v1","category":"math.PR"}
{"created":"2024-03-11 15:31:25","title":"Deep Learning Approaches for Human Action Recognition in Video Data","abstract":"Human action recognition in videos is a critical task with significant implications for numerous applications, including surveillance, sports analytics, and healthcare. The challenge lies in creating models that are both precise in their recognition capabilities and efficient enough for practical use. This study conducts an in-depth analysis of various deep learning models to address this challenge. Utilizing a subset of the UCF101 Videos dataset, we focus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Two-Stream ConvNets. The research reveals that while CNNs effectively capture spatial features and RNNs encode temporal sequences, Two-Stream ConvNets exhibit superior performance by integrating spatial and temporal dimensions. These insights are distilled from the evaluation metrics of accuracy, precision, recall, and F1-score. The results of this study underscore the potential of composite models in achieving robust human action recognition and suggest avenues for future research in optimizing these models for real-world deployment.","sentences":["Human action recognition in videos is a critical task with significant implications for numerous applications, including surveillance, sports analytics, and healthcare.","The challenge lies in creating models that are both precise in their recognition capabilities and efficient enough for practical use.","This study conducts an in-depth analysis of various deep learning models to address this challenge.","Utilizing a subset of the UCF101 Videos dataset, we focus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Two-Stream ConvNets.","The research reveals that while CNNs effectively capture spatial features and RNNs encode temporal sequences, Two-Stream ConvNets exhibit superior performance by integrating spatial and temporal dimensions.","These insights are distilled from the evaluation metrics of accuracy, precision, recall, and F1-score.","The results of this study underscore the potential of composite models in achieving robust human action recognition and suggest avenues for future research in optimizing these models for real-world deployment."],"url":"http://arxiv.org/abs/2403.06810v1","category":"cs.CV"}
{"created":"2024-03-11 14:58:02","title":"Uniqueness of the critical points of solutions to two kinds of semilinear elliptic equations in higher dimensional domains","abstract":"In this paper, we provide an affirmative answer to the conjecture A for bounded simple rotationally symmetric domains $\\Omega\\subset \\mathbb{R}^n(n\\geq 3)$ along $x_n$ axis. Precisely, we use a new simple argument to study the symmetry of positive solutions for two kinds of semilinear elliptic equations. To do this, when $f(\\cdot,s)$ is strictly convex with respect to $s$, we show that the nonnegativity of the first eigenvalue of the corresponding linearized operator in somehow symmetric domains is a sufficient condition for the symmetry of $u$. Moreover, we prove the uniqueness of critical points of a positive solution to semilinear elliptic equation $-\\triangle u=f(\\cdot,u)$ with zero Dirichlet boundary condition for simple rotationally symmetric domains in $\\mathbb{R}^n$ by continuity method and a variety of maximum principles.","sentences":["In this paper, we provide an affirmative answer to the conjecture A for bounded simple rotationally symmetric domains $\\Omega\\subset \\mathbb{R}^n(n\\geq 3)$ along $x_n$ axis.","Precisely, we use a new simple argument to study the symmetry of positive solutions for two kinds of semilinear elliptic equations.","To do this, when $f(\\cdot,s)$ is strictly convex with respect to $s$, we show that the nonnegativity of the first eigenvalue of the corresponding linearized operator in somehow symmetric domains is a sufficient condition for the symmetry of $u$. Moreover, we prove the uniqueness of critical points of a positive solution to semilinear elliptic equation $-\\triangle u=f(\\cdot,u)$ with zero Dirichlet boundary condition for simple rotationally symmetric domains in $\\mathbb{R}^n$ by continuity method and a variety of maximum principles."],"url":"http://arxiv.org/abs/2403.06784v1","category":"math.AP"}
{"created":"2024-03-11 14:31:03","title":"Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation","abstract":"Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS","sentences":["Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations.","Such miscalibration, challenges their clinical translation.","We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality.","We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches.","Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level.","Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset.","We share our code here: https://github.com/cai4cai/ACE-DLIRIS"],"url":"http://arxiv.org/abs/2403.06759v1","category":"cs.CV"}
{"created":"2024-03-11 13:37:18","title":"The Ouroboros of Memristors: Neural Networks Facilitating Memristor Programming","abstract":"Memristive devices hold promise to improve the scale and efficiency of machine learning and neuromorphic hardware, thanks to their compact size, low power consumption, and the ability to perform matrix multiplications in constant time. However, on-chip training with memristor arrays still faces challenges, including device-to-device and cycle-to-cycle variations, switching non-linearity, and especially SET and RESET asymmetry. To combat device non-linearity and asymmetry, we propose to program memristors by harnessing neural networks that map desired conductance updates to the required pulse times. With our method, approximately 95% of devices can be programmed within a relative percentage difference of +-50% from the target conductance after just one attempt. Our approach substantially reduces memristor programming delays compared to traditional write-and-verify methods, presenting an advantageous solution for on-chip training scenarios. Furthermore, our proposed neural network can be accelerated by memristor arrays upon deployment, providing assistance while reducing hardware overhead compared with previous works.   This work contributes significantly to the practical application of memristors, particularly in reducing delays in memristor programming. It also envisions the future development of memristor-based machine learning accelerators.","sentences":["Memristive devices hold promise to improve the scale and efficiency of machine learning and neuromorphic hardware, thanks to their compact size, low power consumption, and the ability to perform matrix multiplications in constant time.","However, on-chip training with memristor arrays still faces challenges, including device-to-device and cycle-to-cycle variations, switching non-linearity, and especially SET and RESET asymmetry.","To combat device non-linearity and asymmetry, we propose to program memristors by harnessing neural networks that map desired conductance updates to the required pulse times.","With our method, approximately 95% of devices can be programmed within a relative percentage difference of +-50% from the target conductance after just one attempt.","Our approach substantially reduces memristor programming delays compared to traditional write-and-verify methods, presenting an advantageous solution for on-chip training scenarios.","Furthermore, our proposed neural network can be accelerated by memristor arrays upon deployment, providing assistance while reducing hardware overhead compared with previous works.   ","This work contributes significantly to the practical application of memristors, particularly in reducing delays in memristor programming.","It also envisions the future development of memristor-based machine learning accelerators."],"url":"http://arxiv.org/abs/2403.06712v1","category":"cs.ET"}
{"created":"2024-03-11 13:29:23","title":"Propagation of Solar Energetic Particles in 3D MHD Simulations of the Solar Wind","abstract":"We propagate relativistic test particles in the field of a steady 3D MHD simulations of the solar wind. We use the MPI-AMRVAC code for the wind simulations and integrate the relativistic guiding center equations using a new third-order accurate time integration scheme to solve the particle trajectories. Diffusion in velocity space, given a particle-turbulence mean free path $\\lambda_\\parallel$ along the magnetic field, is also included. Preliminary results for $81\\:{\\rm keV}$ electrons injected at 0.139 AU heliocentric distance and mean free path $\\lambda_\\parallel =0.5\\:{\\rm AU}$ are in a good qualitative agreement with measurements at 1 AU.","sentences":["We propagate relativistic test particles in the field of a steady 3D MHD simulations of the solar wind.","We use the MPI-AMRVAC code for the wind simulations and integrate the relativistic guiding center equations using a new third-order accurate time integration scheme to solve the particle trajectories.","Diffusion in velocity space, given a particle-turbulence mean free path $\\lambda_\\parallel$ along the magnetic field, is also included.","Preliminary results for $81\\:{\\rm keV}$ electrons injected at 0.139 AU heliocentric distance and mean free path $\\lambda_\\parallel =0.5\\:{\\rm AU}$ are in a good qualitative agreement with measurements at 1 AU."],"url":"http://arxiv.org/abs/2403.06706v1","category":"astro-ph.SR"}
{"created":"2024-03-11 11:25:09","title":"A time-dependent formulation for nonlinear gravity-capillary surface waves with viscosity","abstract":"We develop a time-dependent conformal method to study the effect of viscosity on steep surface waves. When the effect of surface tension is included, numerical solutions are found that contain highly oscillatory parasitic capillary ripples. These small amplitude ripples are associated with the high curvature at the crest of the underlying viscous-gravity wave, and display asymmetry about the wave crest. Previous inviscid studies of steep surface waves have calculated intricate bifurcation structures that appear for small surface tension. We show numerically that viscosity suppresses these. While the discrete solution branches still appear, they collapse to form a single smooth branch in limit of small surface tension. Temporal stability is also studied, and these solutions are seen to be stable when time-evolved from different initial conditions. Our work provides a convenient method for the numerical computation and analysis of water waves with viscosity, without evaluating the free-boundary problem for the full Navier-Stokes equations.","sentences":["We develop a time-dependent conformal method to study the effect of viscosity on steep surface waves.","When the effect of surface tension is included, numerical solutions are found that contain highly oscillatory parasitic capillary ripples.","These small amplitude ripples are associated with the high curvature at the crest of the underlying viscous-gravity wave, and display asymmetry about the wave crest.","Previous inviscid studies of steep surface waves have calculated intricate bifurcation structures that appear for small surface tension.","We show numerically that viscosity suppresses these.","While the discrete solution branches still appear, they collapse to form a single smooth branch in limit of small surface tension.","Temporal stability is also studied, and these solutions are seen to be stable when time-evolved from different initial conditions.","Our work provides a convenient method for the numerical computation and analysis of water waves with viscosity, without evaluating the free-boundary problem for the full Navier-Stokes equations."],"url":"http://arxiv.org/abs/2403.06620v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 10:49:17","title":"Isoperimetric Inequalities on Slabs with applications to Cubes and Gaussian Slabs","abstract":"We study isoperimetric inequalities on \"slabs\", namely weighted Riemannian manifolds obtained as the product of the uniform measure on a finite length interval with a codimension-one base. As our two main applications, we consider the case when the base is the flat torus $\\mathbb{R}^2 / 2 \\mathbb{Z}^2$ and the standard Gaussian measure in $\\mathbb{R}^{n-1}$.   The isoperimetric conjecture on the three-dimensional cube predicts that minimizers are enclosed by spheres about a corner, cylinders about an edge and coordinate planes. Our analysis confirms the isoperimetric conjecture on the three-dimensional cube with side lengths $(\\beta,1,1)$ in a new range of relatives volumes $\\bar v \\in [0,1/2]$. In particular, we confirm the conjecture for the standard cube ($\\beta=1$) for all $\\bar v \\leq 0.120582$, when $\\beta \\leq 0.919431$ for the entire range where spheres are conjectured to be minimizing, and also for all $\\bar v \\in [0,1/2] \\setminus (\\frac{1}{\\pi} - \\frac{\\beta}{4},\\frac{1}{\\pi} + \\frac{\\beta}{4})$. When $\\beta \\leq 0.919431$ we reduce the validity of the full conjecture to establishing that the half-plane $\\{ x \\in [0,\\beta] \\times [0,1]^2 \\; ; \\; x_3 \\leq \\frac{1}{\\pi} \\}$ is an isoperimetric minimizer. We also show that the analogous conjecture on a high-dimensional cube $[0,1]^n$ is false for $n \\geq 10$.   In the case of a slab with a Gaussian base of width $T>0$, we identify a phase transition when $T = \\sqrt{2 \\pi}$ and when $T = \\pi$. In particular, while products of half-planes with $[0,T]$ are always minimizing when $T \\leq \\sqrt{2 \\pi}$, when $T > \\pi$ they are never minimizing, being beaten by Gaussian unduloids. In the range $T \\in (\\sqrt{2 \\pi},\\pi]$, a potential trichotomy occurs.","sentences":["We study isoperimetric inequalities on \"slabs\", namely weighted Riemannian manifolds obtained as the product of the uniform measure on a finite length interval with a codimension-one base.","As our two main applications, we consider the case when the base is the flat torus $\\mathbb{R}^2 / 2 \\mathbb{Z}^2$ and the standard Gaussian measure in $\\mathbb{R}^{n-1}$.   The isoperimetric conjecture on the three-dimensional cube predicts that minimizers are enclosed by spheres about a corner, cylinders about an edge and coordinate planes.","Our analysis confirms the isoperimetric conjecture on the three-dimensional cube with side lengths $(\\beta,1,1)$ in a new range of relatives volumes $\\bar v \\in [0,1/2]$. In particular, we confirm the conjecture for the standard cube ($\\beta=1$) for all $\\bar v \\leq 0.120582$, when $\\beta \\leq 0.919431$ for the entire range where spheres are conjectured to be minimizing, and also for all $\\bar v \\in [0,1/2] \\setminus (\\frac{1}{\\pi} - \\frac{\\beta}{4},\\frac{1}{\\pi} + \\frac{\\beta}{4})$. When $\\beta \\leq 0.919431$ we reduce the validity of the full conjecture to establishing that the half-plane $\\{ x \\in","[0,\\beta] \\times","[0,1]^2 \\; ; \\; x_3 \\leq \\frac{1}{\\pi} \\}$ is an isoperimetric minimizer.","We also show that the analogous conjecture on a high-dimensional cube $[0,1]^n$ is false for $n \\geq 10$.   ","In the case of a slab with a Gaussian base of width $T>0$, we identify a phase transition when $T = \\sqrt{2 \\pi}$ and when $T = \\pi$. In particular, while products of half-planes with $[0,T]$ are always minimizing when $T \\leq \\sqrt{2 \\pi}$, when $T > \\pi$ they are never minimizing, being beaten by Gaussian unduloids.","In the range $T \\in (\\sqrt{2 \\pi},\\pi]$, a potential trichotomy occurs."],"url":"http://arxiv.org/abs/2403.06602v1","category":"math.DG"}
{"created":"2024-03-11 10:42:53","title":"Quantitative Alexandrov theorem for capillary hypersurfaces in the half-space","abstract":"In this paper, we prove the quantitative version of the Alexandrov theorem for capillary hypersurfaces in the half-space. The proof is based on the quantitative analysis of the Montiel-Ros-type argument, carried out in the joint works with Wang-Xia \\cites{JWXZ22-B,JWXZ23}. As by-products, we obtain new Michael-Simon-type inequality and Topping-type inequality for capillary hypersurfaces in the half-space.","sentences":["In this paper, we prove the quantitative version of the Alexandrov theorem for capillary hypersurfaces in the half-space.","The proof is based on the quantitative analysis of the Montiel-Ros-type argument, carried out in the joint works with Wang-Xia \\cites{JWXZ22-B,JWXZ23}.","As by-products, we obtain new Michael-Simon-type inequality and Topping-type inequality for capillary hypersurfaces in the half-space."],"url":"http://arxiv.org/abs/2403.06597v1","category":"math.DG"}
{"created":"2024-03-11 10:39:26","title":"Stability of Hardy-Sobolev Inequality","abstract":"Given $N\\geq 3,$ we consider the critical Hardy-Sobolev equation $-\\Delta u-\\frac{\\gamma}{|x|^2}u=\\frac{|u|^{2^*(s)-2}u}{|x|^s}$ in $\\mathbb{R}^N\\setminus \\{0\\},$ where $0<\\gamma<\\gamma_{H}:=\\left(\\frac{N-2}{2}\\right)^2,\\,s\\in (0,2)$ and $2^*(s)=\\frac{2(N-s)}{(N-2)}.$ We prove a stability estimate for the corresponding Hardy-Sobolev inequality in the spirit of Bianchi-Egnell (1991). Also, we obtain a Struwe-type decomposition (1984) for the corresponding Euler-Lagrange equation. Finally, we prove a quantitative bound for one bubble, namely $\\operatorname{dist}(u,\\mathcal{M})\\lesssim \\Gamma(u)$ in the spirit of Ciraolo-Figalli-Maggi (2017).","sentences":["Given $N\\geq 3,$ we consider the critical Hardy-Sobolev equation $-\\Delta u-\\frac{\\gamma}{|x|^2}u=\\frac{|u|^{2^*(s)-2}u}{|x|^s}$ in $\\mathbb{R}^N\\setminus \\{0\\},$ where $0<\\gamma<\\gamma_{H}:=\\left(\\frac{N-2}{2}\\right)^2,\\,s\\in (0,2)$ and $2^*(s)=\\frac{2(N-s)}{(N-2)}.$ We prove a stability estimate for the corresponding Hardy-Sobolev inequality in the spirit of Bianchi-Egnell (1991).","Also, we obtain a Struwe-type decomposition (1984) for the corresponding Euler-Lagrange equation.","Finally, we prove a quantitative bound for one bubble, namely $\\operatorname{dist}(u,\\mathcal{M})\\lesssim \\Gamma(u)$ in the spirit of Ciraolo-Figalli-Maggi (2017)."],"url":"http://arxiv.org/abs/2403.06594v1","category":"math.AP"}
{"created":"2024-03-11 09:35:10","title":"Uniform estimates for solutions of nonlinear focusing damped wave equations","abstract":"For a damped wave (or Klein-Gordon) equation on a bounded domain, with a focusing power-like nonlinearity satisfying some growth conditions, we prove that a global solution is bounded in the energy space, uniformly in time. Our result applies in particular to the case of a cubic equation on a bounded domain of dimension 3.","sentences":["For a damped wave (or Klein-Gordon) equation on a bounded domain, with a focusing power-like nonlinearity satisfying some growth conditions, we prove that a global solution is bounded in the energy space, uniformly in time.","Our result applies in particular to the case of a cubic equation on a bounded domain of dimension 3."],"url":"http://arxiv.org/abs/2403.06541v1","category":"math.AP"}
{"created":"2024-03-11 08:38:14","title":"Moduli space theory for complete, constant Q-curvature metrics on finitely punctured spheres","abstract":"We study constant Q-curvature metrics conformal to the round metric on the sphere with finitely many point singularities. We show that the moduli space of solutions with finitely many punctures in fixed positions, equipped with the Gromov-Hausdorff topology, has the local structure of a real analytic variety with formal dimension equal to the number of the punctures. If a nondegeneracy hypothesis holds, we show that a neighborhood in the moduli space is actually a real-analytic manifold of the expected dimension. We also construct a geometrically natural set of parameters, construct a symplectic structure on this parameter space and show that in the smooth case a small neighborhood of the moduli space embeds as a Lagrangian submanifold in the parameter space.","sentences":["We study constant Q-curvature metrics conformal to the round metric on the sphere with finitely many point singularities.","We show that the moduli space of solutions with finitely many punctures in fixed positions, equipped with the Gromov-Hausdorff topology, has the local structure of a real analytic variety with formal dimension equal to the number of the punctures.","If a nondegeneracy hypothesis holds, we show that a neighborhood in the moduli space is actually a real-analytic manifold of the expected dimension.","We also construct a geometrically natural set of parameters, construct a symplectic structure on this parameter space and show that in the smooth case a small neighborhood of the moduli space embeds as a Lagrangian submanifold in the parameter space."],"url":"http://arxiv.org/abs/2403.06511v1","category":"math.DG"}
{"created":"2024-03-11 08:32:26","title":"Kinetic modelling of rarefied gas mixtures with disparate mass","abstract":"The simulation of rarefied gas flow based on the Boltzmann equation is challenging, especially when the gas mixtures have disparate molecular masses. In this paper, a computationally tractable kinetic model is proposed for monatomic gas mixtures, to mimic the Boltzmann collision operator as closely as possible. The intra- and inter-collisions are modelled separately using relaxation approximations, to correctly recover the relaxation timescales that could span several orders of magnitude. The proposed kinetic model preserves the accuracy of the Boltzmann equation in the continuum regime by recovering the four critical transport properties of a gas mixture: the shear viscosity, the thermal conductivity, the coefficients of diffusion and the thermal diffusion. While in the rarefied flow regimes, the kinetic model is found to be accurate when comparing its solutions with those from the direct simulation Monte Carlo method in several representative cases (e.g. one-dimensional normal shock wave, Fourier flow and Couette flow, two-dimensional supersonic flow passing a cylinder and nozzle flow into a vacuum), for binary mixtures with a wide range of mass ratios (up to 1000), species concentrations, and different intermolecular potentials. Pronounced separations in species properties have been observed, and the flow characteristics of gas mixtures in shock waves are found to change as the mass difference increases from moderate to substantial.","sentences":["The simulation of rarefied gas flow based on the Boltzmann equation is challenging, especially when the gas mixtures have disparate molecular masses.","In this paper, a computationally tractable kinetic model is proposed for monatomic gas mixtures, to mimic the Boltzmann collision operator as closely as possible.","The intra- and inter-collisions are modelled separately using relaxation approximations, to correctly recover the relaxation timescales that could span several orders of magnitude.","The proposed kinetic model preserves the accuracy of the Boltzmann equation in the continuum regime by recovering the four critical transport properties of a gas mixture: the shear viscosity, the thermal conductivity, the coefficients of diffusion and the thermal diffusion.","While in the rarefied flow regimes, the kinetic model is found to be accurate when comparing its solutions with those from the direct simulation Monte Carlo method in several representative cases (e.g. one-dimensional normal shock wave, Fourier flow and Couette flow, two-dimensional supersonic flow passing a cylinder and nozzle flow into a vacuum), for binary mixtures with a wide range of mass ratios (up to 1000), species concentrations, and different intermolecular potentials.","Pronounced separations in species properties have been observed, and the flow characteristics of gas mixtures in shock waves are found to change as the mass difference increases from moderate to substantial."],"url":"http://arxiv.org/abs/2403.06507v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 08:28:51","title":"Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis","abstract":"The neural radiance field (NeRF) has emerged as a prominent methodology for synthesizing realistic images of novel views. While neural radiance representations based on voxels or mesh individually offer distinct advantages, excelling in either rendering quality or speed, each has limitations in the other aspect. In response, we propose a pioneering hybrid representation named Vosh, seamlessly combining both voxel and mesh components in hybrid rendering for view synthesis. Vosh is meticulously crafted by optimizing the voxel grid of NeRF, strategically with selected voxels replaced by mesh. Therefore, it excels in fast rendering scenes with simple geometry and textures through its mesh component, while simultaneously enabling high-quality rendering in intricate regions by leveraging voxel component. The flexibility of Vosh is showcased through the ability to adjust hybrid ratios, providing users the ability to control the balance between rendering quality and speed based on flexible usage. Experimental results demonstrates that our method achieves commendable trade-off between rendering quality and speed, and notably has real-time performance on mobile devices.","sentences":["The neural radiance field (NeRF) has emerged as a prominent methodology for synthesizing realistic images of novel views.","While neural radiance representations based on voxels or mesh individually offer distinct advantages, excelling in either rendering quality or speed, each has limitations in the other aspect.","In response, we propose a pioneering hybrid representation named Vosh, seamlessly combining both voxel and mesh components in hybrid rendering for view synthesis.","Vosh is meticulously crafted by optimizing the voxel grid of NeRF, strategically with selected voxels replaced by mesh.","Therefore, it excels in fast rendering scenes with simple geometry and textures through its mesh component, while simultaneously enabling high-quality rendering in intricate regions by leveraging voxel component.","The flexibility of Vosh is showcased through the ability to adjust hybrid ratios, providing users the ability to control the balance between rendering quality and speed based on flexible usage.","Experimental results demonstrates that our method achieves commendable trade-off between rendering quality and speed, and notably has real-time performance on mobile devices."],"url":"http://arxiv.org/abs/2403.06505v1","category":"cs.CV"}
{"created":"2024-03-11 08:11:52","title":"Detection of Unobserved Common Causes based on NML Code in Discrete, Mixed, and Continuous Variables","abstract":"Causal discovery in the presence of unobserved common causes from observational data only is a crucial but challenging problem. We categorize all possible causal relationships between two random variables into the following four categories and aim to identify one from observed data: two cases in which either of the direct causality exists, a case that variables are independent, and a case that variables are confounded by latent confounders. Although existing methods have been proposed to tackle this problem, they require unobserved variables to satisfy assumptions on the form of their equation models. In our previous study (Kobayashi et al., 2022), the first causal discovery method without such assumptions is proposed for discrete data and named CLOUD. Using Normalized Maximum Likelihood (NML) Code, CLOUD selects a model that yields the minimum codelength of the observed data from a set of model candidates. This paper extends CLOUD to apply for various data types across discrete, mixed, and continuous. We not only performed theoretical analysis to show the consistency of CLOUD in terms of the model selection, but also demonstrated that CLOUD is more effective than existing methods in inferring causal relationships by extensive experiments on both synthetic and real-world data.","sentences":["Causal discovery in the presence of unobserved common causes from observational data only is a crucial but challenging problem.","We categorize all possible causal relationships between two random variables into the following four categories and aim to identify one from observed data: two cases in which either of the direct causality exists, a case that variables are independent, and a case that variables are confounded by latent confounders.","Although existing methods have been proposed to tackle this problem, they require unobserved variables to satisfy assumptions on the form of their equation models.","In our previous study (Kobayashi et al., 2022), the first causal discovery method without such assumptions is proposed for discrete data and named CLOUD.","Using Normalized Maximum Likelihood (NML) Code, CLOUD selects a model that yields the minimum codelength of the observed data from a set of model candidates.","This paper extends CLOUD to apply for various data types across discrete, mixed, and continuous.","We not only performed theoretical analysis to show the consistency of CLOUD in terms of the model selection, but also demonstrated that CLOUD is more effective than existing methods in inferring causal relationships by extensive experiments on both synthetic and real-world data."],"url":"http://arxiv.org/abs/2403.06499v1","category":"stat.ML"}
{"created":"2024-03-11 07:44:22","title":"Ghost Martin-Siggia-Rose (MSR) applications to Kardar-Parizi-Zhang (KPZ) equation with non-linear noise","abstract":"In this work we modify the Kardar-Parisi-Zhang (KPZ) equation for growing surfaces to include the effect of surface tilt on the noise (that is have non-linearly coupled noise). We introduce ghost Martin-Siggia-Rose (gMSR) action for the KPZ equation with Faddeev-Popov ghosts to compute an auxiliary functional determinant. We integrate out the Faddeev-Popov ghosts to leading order and the auxiliary MSR Lagrange multiplierexactly to obtain and action for the height. We analyze it within the gaussian, meanfield and Landau free energy like approximations and find various instabilities to rough surfaces.","sentences":["In this work we modify the Kardar-Parisi-Zhang (KPZ) equation for growing surfaces to include the effect of surface tilt on the noise (that is have non-linearly coupled noise).","We introduce ghost Martin-Siggia-Rose (gMSR) action for the KPZ equation with Faddeev-Popov ghosts to compute an auxiliary functional determinant.","We integrate out the Faddeev-Popov ghosts to leading order and the auxiliary MSR Lagrange multiplierexactly to obtain and action for the height.","We analyze it within the gaussian, meanfield and Landau free energy like approximations and find various instabilities to rough surfaces."],"url":"http://arxiv.org/abs/2403.06481v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-11 05:52:49","title":"On the blowup of solutions for a nonlocal multi-dimensional transport equation","abstract":"In this paper, we revisit the problem of finite-time blowup for a multi-dimensional nonlocal transport equation studied in [Dong, Adv. Math. 264 (2014) 747-761]. Inspired by a one-dimensional analogous model considered in [Li-Rodrigo, Adv. Math. 374 (2020) 1-26], we establish a new weighted nonlinear inequality implying the blow-up by a completely real variable based technique.","sentences":["In this paper, we revisit the problem of finite-time blowup for a multi-dimensional nonlocal transport equation studied in [Dong, Adv.","Math. 264 (2014) 747-761].","Inspired by a one-dimensional analogous model considered in [Li-Rodrigo, Adv.","Math. 374 (2020) 1-26], we establish a new weighted nonlinear inequality implying the blow-up by a completely real variable based technique."],"url":"http://arxiv.org/abs/2403.06449v1","category":"math.AP"}
{"created":"2024-03-11 05:39:58","title":"On infinite matrices","abstract":"We consider linear bounded operators acting in Banach spaces with a basis, such operators can be represented by an infinite matrix. We prove that for an invertible operator there exists a sequence of invertible finite-dimensional operators so that the family of norms of their inverses is uniformly bounded. It leads to the fact that solutions of finite-dimensional equations converge to the solution of initial operator equation with infinite-dimensional matrix.","sentences":["We consider linear bounded operators acting in Banach spaces with a basis, such operators can be represented by an infinite matrix.","We prove that for an invertible operator there exists a sequence of invertible finite-dimensional operators so that the family of norms of their inverses is uniformly bounded.","It leads to the fact that solutions of finite-dimensional equations converge to the solution of initial operator equation with infinite-dimensional matrix."],"url":"http://arxiv.org/abs/2403.06445v1","category":"math.FA"}
{"created":"2024-03-11 05:02:48","title":"A Naive Model of Covid-19 Spread From A Dry Cough","abstract":"Health experts have suggested that social distancing measures are one of the most effective ways of preventing the spread of Covid-19. Research primarily focused on large Covid filled droplets suggested that these droplets can move further than regulated social distancing guidelines (2 meters apart) in the presence of wind. This project aims to model the paths of smaller Covid virions that last longer in the air to see if they also move beyond social distancing norms in the presence of wind. By numerically solving a 2-dimensional Langevin equation for 3000 particles and modeling wind as one-dimensional and steady, velocities were found that translated into particles' positions. With wind, infectious doses of virions appeared to travel farther and faster, increasing the risk of infection before dispersion. The two-dimensional model given implies that social distancing norms are reasonable in cases with no wind, yet not conservative enough when there is wind.","sentences":["Health experts have suggested that social distancing measures are one of the most effective ways of preventing the spread of Covid-19.","Research primarily focused on large Covid filled droplets suggested that these droplets can move further than regulated social distancing guidelines (2 meters apart) in the presence of wind.","This project aims to model the paths of smaller Covid virions that last longer in the air to see if they also move beyond social distancing norms in the presence of wind.","By numerically solving a 2-dimensional Langevin equation for 3000 particles and modeling wind as one-dimensional and steady, velocities were found that translated into particles' positions.","With wind, infectious doses of virions appeared to travel farther and faster, increasing the risk of infection before dispersion.","The two-dimensional model given implies that social distancing norms are reasonable in cases with no wind, yet not conservative enough when there is wind."],"url":"http://arxiv.org/abs/2403.06435v1","category":"physics.comp-ph"}
{"created":"2024-03-11 04:28:38","title":"Asymptotic behavior of unstable perturbations of the Fubini-Study metric in Ricci flow","abstract":"Kr\\\"oncke has shown that the Fubini-Study metric is an unstable generalized stationary solution of Ricci flow [Kr\\\"o20]. In this paper, we carry out numerical simulations which indicate that Ricci flow solutions originating at unstable perturbations of the Fubini-Study metric develop local singularities modeled by the blowdown soliton discovered in [FIK03].","sentences":["Kr\\\"oncke has shown that the Fubini-Study metric is an unstable generalized stationary solution of Ricci flow [Kr\\\"o20].","In this paper, we carry out numerical simulations which indicate that Ricci flow solutions originating at unstable perturbations of the Fubini-Study metric develop local singularities modeled by the blowdown soliton discovered in [FIK03]."],"url":"http://arxiv.org/abs/2403.06427v1","category":"math.DG"}
{"created":"2024-03-11 04:09:48","title":"Efficient Estimation of the Convective Cooling Rate of Photovoltaic Arrays with Various Geometric Configurations: a Physics-Informed Machine Learning Approach","abstract":"Convective heat transfer is crucial for photovoltaic (PV) systems, as the power generation of PV is sensitive to temperature. The configuration of PV arrays have a significant impact on convective heat transfer by influencing turbulent characteristics. Conventional methods of quantifying the configuration effects are either through Computational Fluid Dynamics (CFD) simulations or empirical methods, which face the challenge of either high computational demand or low accuracy, especially when complex array configurations are considered. This work introduces a novel methodology to quantify the impact of geometric configurations of PV arrays on their convective heat transfer rate in wind field. The methodology combines Physics Informed Machine Learning (PIML) and Deep Convolution Neural Network (DCNN) to construct a robust PIML-DCNN model to predict convective heat transfer rates. In addition, an innovative loss function, termed Pocket Loss is proposed to enhance the interpretability of the PIML-DCNN model. The model exhibits promising performance, with a relative error of 1.9\\% and overall $R^2$ of 0.99 over all CFD cases in estimating the coefficient of convective heat transfer, when compared with full CFD simulations. Therefore, the proposed model has the potential to efficiently guide the configuration design of PV arrays for power generation enhancement in real-world operations.","sentences":["Convective heat transfer is crucial for photovoltaic (PV) systems, as the power generation of PV is sensitive to temperature.","The configuration of PV arrays have a significant impact on convective heat transfer by influencing turbulent characteristics.","Conventional methods of quantifying the configuration effects are either through Computational Fluid Dynamics (CFD) simulations or empirical methods, which face the challenge of either high computational demand or low accuracy, especially when complex array configurations are considered.","This work introduces a novel methodology to quantify the impact of geometric configurations of PV arrays on their convective heat transfer rate in wind field.","The methodology combines Physics Informed Machine Learning (PIML) and Deep Convolution Neural Network (DCNN) to construct a robust PIML-DCNN model to predict convective heat transfer rates.","In addition, an innovative loss function, termed Pocket Loss is proposed to enhance the interpretability of the PIML-DCNN model.","The model exhibits promising performance, with a relative error of 1.9\\% and overall $R^2$ of 0.99 over all CFD cases in estimating the coefficient of convective heat transfer, when compared with full CFD simulations.","Therefore, the proposed model has the potential to efficiently guide the configuration design of PV arrays for power generation enhancement in real-world operations."],"url":"http://arxiv.org/abs/2403.06418v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 03:47:55","title":"Effect of non-equilibrium thermochemistry on Pitot pressure measurements in shock tunnels (or: Is 0.92 really the magic number?)","abstract":"Pitot pressure is the most common measurement in high total enthalpy shock tunnels for test condition verification. Nozzle calculations using multi-temperature non-equilibrium thermochemistry are needed in conjunction with Pitot measurements to quantify freestream properties. Pitot pressure is typically matched by tuning the boundary layer transition location in these simulations. However, non-equilibrium thermochemistry effects on the Pitot probe are commonly ignored. A computational study was undertaken to estimate the effect of nonequilibrium thermochemistry on Pitot pressure and freestream conditions. The test flow was produced by a Mach 7 nozzle in a reflected shock tunnel for air at a relatively low total enthalpy of 2.67 MJ/kg. Three different thermochemical models (equilibrium, finite-rate chemistry and two-temperature thermochemistry) were employed to compute flow variables at the nozzle exit and Pitot probe. Pitot pressures from these simulations were compared against those obtained via experiments. The results show a departure from the commonly utilized C of 0.92 in the reduced Rayleigh-Pitot equation form for high Mach numbers. Additionally, calculations were done with a sweep of free-stream conditions and resulting in C approximately 0.94. These results show that the influence of finite-rate thermochemistry should be taken into account, even at relatively low flow enthalpies.","sentences":["Pitot pressure is the most common measurement in high total enthalpy shock tunnels for test condition verification.","Nozzle calculations using multi-temperature non-equilibrium thermochemistry are needed in conjunction with Pitot measurements to quantify freestream properties.","Pitot pressure is typically matched by tuning the boundary layer transition location in these simulations.","However, non-equilibrium thermochemistry effects on the Pitot probe are commonly ignored.","A computational study was undertaken to estimate the effect of nonequilibrium thermochemistry on Pitot pressure and freestream conditions.","The test flow was produced by a Mach 7 nozzle in a reflected shock tunnel for air at a relatively low total enthalpy of 2.67 MJ/kg.","Three different thermochemical models (equilibrium, finite-rate chemistry and two-temperature thermochemistry) were employed to compute flow variables at the nozzle exit and Pitot probe.","Pitot pressures from these simulations were compared against those obtained via experiments.","The results show a departure from the commonly utilized C of 0.92 in the reduced Rayleigh-Pitot equation form for high Mach numbers.","Additionally, calculations were done with a sweep of free-stream conditions and resulting in C approximately 0.94.","These results show that the influence of finite-rate thermochemistry should be taken into account, even at relatively low flow enthalpies."],"url":"http://arxiv.org/abs/2403.06411v1","category":"physics.flu-dyn"}
{"created":"2024-03-11 03:35:41","title":"Comparison of No-Reference Image Quality Models via MAP Estimation in Diffusion Latents","abstract":"Contemporary no-reference image quality assessment (NR-IQA) models can effectively quantify the perceived image quality, with high correlations between model predictions and human perceptual scores on fixed test sets. However, little progress has been made in comparing NR-IQA models from a perceptual optimization perspective. Here, for the first time, we demonstrate that NR-IQA models can be plugged into the maximum a posteriori (MAP) estimation framework for image enhancement. This is achieved by taking the gradients in differentiable and bijective diffusion latents rather than in the raw pixel domain. Different NR-IQA models are likely to induce different enhanced images, which are ultimately subject to psychophysical testing. This leads to a new computational method for comparing NR-IQA models within the analysis-by-synthesis framework. Compared to conventional correlation-based metrics, our method provides complementary insights into the relative strengths and weaknesses of the competing NR-IQA models in the context of perceptual optimization.","sentences":["Contemporary no-reference image quality assessment (NR-IQA) models can effectively quantify the perceived image quality, with high correlations between model predictions and human perceptual scores on fixed test sets.","However, little progress has been made in comparing NR-IQA models from a perceptual optimization perspective.","Here, for the first time, we demonstrate that NR-IQA models can be plugged into the maximum a posteriori (MAP) estimation framework for image enhancement.","This is achieved by taking the gradients in differentiable and bijective diffusion latents rather than in the raw pixel domain.","Different NR-IQA models are likely to induce different enhanced images, which are ultimately subject to psychophysical testing.","This leads to a new computational method for comparing NR-IQA models within the analysis-by-synthesis framework.","Compared to conventional correlation-based metrics, our method provides complementary insights into the relative strengths and weaknesses of the competing NR-IQA models in the context of perceptual optimization."],"url":"http://arxiv.org/abs/2403.06406v1","category":"cs.CV"}
{"created":"2024-03-11 03:02:22","title":"A Linear Algebra approach to monomiality and operational methods","abstract":"We use linear algebraic methods to obtain general results about linear operators on a space of polynomials that we apply to the operators associated with a polynomial sequence by the monomiality property. We show that all such operators are differential operators with polynomial coefficients of finite of infinite order. We consider the monomiality operators associated with several classes of polynomial sequences, such as Appell and Sheffer, and also orthogonal polynomial sequences that include the Meixner, Krawtchouk, Laguerre, Meixner-Pollaczek, and Hermite families.","sentences":["We use linear algebraic methods to obtain general results about linear operators on a space of polynomials that we apply to the operators associated with a polynomial sequence by the monomiality property.","We show that all such operators are differential operators with polynomial coefficients of finite of infinite order.","We consider the monomiality operators associated with several classes of polynomial sequences, such as Appell and Sheffer, and also orthogonal polynomial sequences that include the Meixner, Krawtchouk, Laguerre, Meixner-Pollaczek, and Hermite families."],"url":"http://arxiv.org/abs/2403.06395v1","category":"math.CA"}
{"created":"2024-03-11 02:55:01","title":"The Phenomenology of Sphaleron in Modified Mirror Model","abstract":"We investigate sphaleron solutions of the field equations in the modified mirror model. This model is based on SU(3)$_1$ $\\otimes$ SU(3)$_2$ $\\otimes$ SU(2)$_L$ $\\otimes$ SU(2)$_R$ $\\otimes$ U(1)$_{Y}$ $\\otimes$ U(1)$_{X}$ gauge group. Different from the usual Weinberg-Salam theory, we will have two types of fields, the ordinary (standard model) and its mirror partners. In this paper, we will focus only on the case when the doublet scalar of the mirror sector has a very large vacuum expectation value (VEV) compared to the VEV of the SM Higgs. We study two scenarios related to the sphaleron solutions in this model, depending on the value of the coupling $\\alpha$. For the case of $\\alpha=0$, each sector has its sphaleron and the sphaleron energy depends on the VEV of scalars for each sector. In the case $\\alpha \\neq 0$, we find that the sphaleron energy in the SM sector can be either increased or decreased depending on the relative sign of the coupling $\\alpha$.","sentences":["We investigate sphaleron solutions of the field equations in the modified mirror model.","This model is based on SU(3)$_1$ $\\otimes$ SU(3)$_2$ $\\otimes$ SU(2)$_L$ $\\otimes$ SU(2)$_R$ $\\otimes$ U(1)$_{Y}$ $\\otimes$ U(1)$_{X}$ gauge group.","Different from the usual Weinberg-Salam theory, we will have two types of fields, the ordinary (standard model) and its mirror partners.","In this paper, we will focus only on the case when the doublet scalar of the mirror sector has a very large vacuum expectation value (VEV) compared to the VEV of the SM Higgs.","We study two scenarios related to the sphaleron solutions in this model, depending on the value of the coupling $\\alpha$. For the case of $\\alpha=0$, each sector has its sphaleron and the sphaleron energy depends on the VEV of scalars for each sector.","In the case $\\alpha \\neq 0$, we find that the sphaleron energy in the SM sector can be either increased or decreased depending on the relative sign of the coupling $\\alpha$."],"url":"http://arxiv.org/abs/2403.06390v1","category":"hep-ph"}
{"created":"2024-03-11 02:39:40","title":"Dyakonov-Shur instability of electronic fluid: Spectral effect of weak magnetic field","abstract":"We study numerically and analytically how the Dyakonov-Shur instability for a two-dimensional (2D) inviscid electronic fluid in a long channel can be affected by an external, out-of-plane static magnetic field. By linear stability analysis for a model based on the shallow-water equations, we describe the discrete spectrum of frequencies in the complex plane. When the fluid system is near the subsonic-to-supersonic transition point, a magnetically controlled gap between the stability and instability spectra of the complex eigen-frequencies is evident by our computations. This suggests that the passage from stability to instability (and vice versa) is no longer continuous in the effective Mach parameter of the boundary conditions. We connect this complex-valued gap to a multivalued function characterizing the exact electronic steady state. In a regime of weak magnetic fields, we derive a scaling law for the eigen-frequencies by perturbation theory. We discuss implications of our results in efforts to generate terahertz electromagnetic radiation by 2D electronic transport.","sentences":["We study numerically and analytically how the Dyakonov-Shur instability for a two-dimensional (2D) inviscid electronic fluid in a long channel can be affected by an external, out-of-plane static magnetic field.","By linear stability analysis for a model based on the shallow-water equations, we describe the discrete spectrum of frequencies in the complex plane.","When the fluid system is near the subsonic-to-supersonic transition point, a magnetically controlled gap between the stability and instability spectra of the complex eigen-frequencies is evident by our computations.","This suggests that the passage from stability to instability (and vice versa) is no longer continuous in the effective Mach parameter of the boundary conditions.","We connect this complex-valued gap to a multivalued function characterizing the exact electronic steady state.","In a regime of weak magnetic fields, we derive a scaling law for the eigen-frequencies by perturbation theory.","We discuss implications of our results in efforts to generate terahertz electromagnetic radiation by 2D electronic transport."],"url":"http://arxiv.org/abs/2403.06386v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-10 23:44:55","title":"Separable Physics-informed Neural Networks for Solving the BGK Model of the Boltzmann Equation","abstract":"In this study, we introduce a method based on Separable Physics-Informed Neural Networks (SPINNs) for effectively solving the BGK model of the Boltzmann equation. While the mesh-free nature of PINNs offers significant advantages in handling high-dimensional partial differential equations (PDEs), challenges arise when applying quadrature rules for accurate integral evaluation in the BGK operator, which can compromise the mesh-free benefit and increase computational costs. To address this, we leverage the canonical polyadic decomposition structure of SPINNs and the linear nature of moment calculation, achieving a substantial reduction in computational expense for quadrature rule application. The multi-scale nature of the particle density function poses difficulties in precisely approximating macroscopic moments using neural networks. To improve SPINN training, we introduce the integration of Gaussian functions into SPINNs, coupled with a relative loss approach. This modification enables SPINNs to decay as rapidly as Maxwellian distributions, thereby enhancing the accuracy of macroscopic moment approximations. The relative loss design further ensures that both large and small-scale features are effectively captured by the SPINNs. The efficacy of our approach is demonstrated through a series of five numerical experiments, including the solution to a challenging 3D Riemann problem. These results highlight the potential of our novel method in efficiently and accurately addressing complex challenges in computational physics.","sentences":["In this study, we introduce a method based on Separable Physics-Informed Neural Networks (SPINNs) for effectively solving the BGK model of the Boltzmann equation.","While the mesh-free nature of PINNs offers significant advantages in handling high-dimensional partial differential equations (PDEs), challenges arise when applying quadrature rules for accurate integral evaluation in the BGK operator, which can compromise the mesh-free benefit and increase computational costs.","To address this, we leverage the canonical polyadic decomposition structure of SPINNs and the linear nature of moment calculation, achieving a substantial reduction in computational expense for quadrature rule application.","The multi-scale nature of the particle density function poses difficulties in precisely approximating macroscopic moments using neural networks.","To improve SPINN training, we introduce the integration of Gaussian functions into SPINNs, coupled with a relative loss approach.","This modification enables SPINNs to decay as rapidly as Maxwellian distributions, thereby enhancing the accuracy of macroscopic moment approximations.","The relative loss design further ensures that both large and small-scale features are effectively captured by the SPINNs.","The efficacy of our approach is demonstrated through a series of five numerical experiments, including the solution to a challenging 3D Riemann problem.","These results highlight the potential of our novel method in efficiently and accurately addressing complex challenges in computational physics."],"url":"http://arxiv.org/abs/2403.06342v1","category":"math.NA"}
{"created":"2024-03-10 23:12:40","title":"FOAA: Flattened Outer Arithmetic Attention For Multimodal Tumor Classification","abstract":"Fusion of multimodal healthcare data holds great promise to provide a holistic view of a patient's health, taking advantage of the complementarity of different modalities while leveraging their correlation. This paper proposes a simple and effective approach, inspired by attention, to fuse discriminative features from different modalities. We propose a novel attention mechanism, called Flattened Outer Arithmetic Attention (FOAA), which relies on outer arithmetic operators (addition, subtraction, product, and division) to compute attention scores from keys, queries and values derived from flattened embeddings of each modality. We demonstrate how FOAA can be implemented for self-attention and cross-attention, providing a reusable component in neural network architectures. We evaluate FOAA on two datasets for multimodal tumor classification and achieve state-of-the-art results, and we demonstrate that features enriched by FOAA are superior to those derived from other fusion approaches. The code is publicly available at \\href{https://github.com/omniaalwazzan/FOAA}{https://github.com/omniaalwazzan/FOAA}","sentences":["Fusion of multimodal healthcare data holds great promise to provide a holistic view of a patient's health, taking advantage of the complementarity of different modalities while leveraging their correlation.","This paper proposes a simple and effective approach, inspired by attention, to fuse discriminative features from different modalities.","We propose a novel attention mechanism, called Flattened Outer Arithmetic Attention (FOAA), which relies on outer arithmetic operators (addition, subtraction, product, and division) to compute attention scores from keys, queries and values derived from flattened embeddings of each modality.","We demonstrate how FOAA can be implemented for self-attention and cross-attention, providing a reusable component in neural network architectures.","We evaluate FOAA on two datasets for multimodal tumor classification and achieve state-of-the-art results, and we demonstrate that features enriched by FOAA are superior to those derived from other fusion approaches.","The code is publicly available at \\href{https://github.com/omniaalwazzan/FOAA}{https://github.com/omniaalwazzan/FOAA}"],"url":"http://arxiv.org/abs/2403.06339v1","category":"cs.CV"}
{"created":"2024-03-10 21:28:47","title":"Effects of a dark matter caustic passing through the Oort Cloud","abstract":"We investigate the effect of a dark matter caustic passing through the Solar System. We find, confirming a previous result, that the Sun tracks the caustic surface for some time. We integrate numerically the equations of motion of the Sun and a comet for a large number of initial conditions and of caustic passage properties. We calculate the probability for the comet to escape the Solar System and the probability for it to fall within 50 A.U. of the Sun, given the initial semi-major axis and eccentricity of its orbit. We find that the average probability for a comet to fall within 50 A.U. of the Sun is of order $3 \\cdot 10^{-4}$ and that comets which are initially at a distance larger than about $10^5$ A.U. have a probability of order one to be ejected from the Solar System.","sentences":["We investigate the effect of a dark matter caustic passing through the Solar System.","We find, confirming a previous result, that the Sun tracks the caustic surface for some time.","We integrate numerically the equations of motion of the Sun and a comet for a large number of initial conditions and of caustic passage properties.","We calculate the probability for the comet to escape the Solar System and the probability for it to fall within 50 A.U. of the Sun, given the initial semi-major axis and eccentricity of its orbit.","We find that the average probability for a comet to fall within 50 A.U. of the Sun is of order $3 \\cdot 10^{-4}$ and that comets which are initially at a distance larger than about $10^5$ A.U. have a probability of order one to be ejected from the Solar System."],"url":"http://arxiv.org/abs/2403.06314v1","category":"astro-ph.CO"}
{"created":"2024-03-10 19:33:34","title":"Cyclic variations of the structure and energetics of solar magnetic fields","abstract":"The solar cycle is a complex phenomenon, a comprehensive understanding of which requires the study of various tracers. Here, we consider the solar cycle as manifested in the harmonics of the solar large-scale surface magnetic field, including zonal, sectorial and tesseral harmonics, divided into odd and even relative to the solar equator. In addition to considering the amplitudes of the harmonics, we analyze their contribution to the magnetic energy. It turns out that the relative contribution of different types of harmonics to the magnetic energy is virtually independent of the cycle height. We identify different phases of the activity cycle using harmonics of different symmetries. A possible way to incorporate the obtained result into the solar dynamo theory is proposed.","sentences":["The solar cycle is a complex phenomenon, a comprehensive understanding of which requires the study of various tracers.","Here, we consider the solar cycle as manifested in the harmonics of the solar large-scale surface magnetic field, including zonal, sectorial and tesseral harmonics, divided into odd and even relative to the solar equator.","In addition to considering the amplitudes of the harmonics, we analyze their contribution to the magnetic energy.","It turns out that the relative contribution of different types of harmonics to the magnetic energy is virtually independent of the cycle height.","We identify different phases of the activity cycle using harmonics of different symmetries.","A possible way to incorporate the obtained result into the solar dynamo theory is proposed."],"url":"http://arxiv.org/abs/2403.06293v1","category":"astro-ph.SR"}
{"created":"2024-03-10 18:53:13","title":"Fractional Quantum Hall Effect as Consequence of Symmetry Invariance","abstract":"The non-relativistic Hamiltonian for an electron in the presence of an electromagnetic field, described using Landau's gauge, is solved analytically based on the conserved operators of the system: one for the canonical momentum in the $x$-axis, a second one for the canonical momentum in the $y$-axis, and the final one for the energy operator. It is shown that the Lorentz force can be recovered only if both of the conserved momenta are considered; otherwise, the system cannot be fully described. The wave function is calculated by solving an eigenvalue equation for the momentum operators, and a ground state of this function is then constructed. Based on the conserved properties of the system, a set of unitary operators defining the symmetries of the Hamiltonian is established. However, in Schr\\\"odinger's scheme, the necessary conditions for the invariance of the wave function after a unitary transformation give rise to a couple of quantized identities: one for the electric field and the second one for the magnetic field. Using the electric current expression defined by the continuity equation, the Hall and longitudinal resistivity were calculated, showing that the former is proportional to von Klitzing's constant and the latter vanishes when the time increment is $\\Delta t<<\\frac{m\\omega_{c}}{q{\\cal E}}\\Delta x$. Finally, if the invariance condition is satisfied, then the Hall resistivity is quantized in integer multiples proportional to von Klitzing's constant. This implies that the fractional quantum Hall effect is a manifestation of symmetry invariance.","sentences":["The non-relativistic Hamiltonian for an electron in the presence of an electromagnetic field, described using Landau's gauge, is solved analytically based on the conserved operators of the system: one for the canonical momentum in the $x$-axis, a second one for the canonical momentum in the $y$-axis, and the final one for the energy operator.","It is shown that the Lorentz force can be recovered only if both of the conserved momenta are considered; otherwise, the system cannot be fully described.","The wave function is calculated by solving an eigenvalue equation for the momentum operators, and a ground state of this function is then constructed.","Based on the conserved properties of the system, a set of unitary operators defining the symmetries of the Hamiltonian is established.","However, in Schr\\\"odinger's scheme, the necessary conditions for the invariance of the wave function after a unitary transformation give rise to a couple of quantized identities: one for the electric field and the second one for the magnetic field.","Using the electric current expression defined by the continuity equation, the Hall and longitudinal resistivity were calculated, showing that the former is proportional to von Klitzing's constant and the latter vanishes when the time increment is $\\Delta t<<\\frac{m\\omega_{c}}{q{\\cal E}}\\Delta x$.","Finally, if the invariance condition is satisfied, then the Hall resistivity is quantized in integer multiples proportional to von Klitzing's constant.","This implies that the fractional quantum Hall effect is a manifestation of symmetry invariance."],"url":"http://arxiv.org/abs/2403.06287v1","category":"quant-ph"}
{"created":"2024-03-10 18:24:32","title":"f(R,T) Analogue Gravity in (2+1) D graphene sheet","abstract":"We examine the analogue gravity model within the context of f(R,T) gravity applied to graphene. The derivation of the Lagrangian density in two dimensions (2D) is undertaken, accounting for the altered gravitational effects as characterized by the function f(R,T). The Lagrangian encompasses the quasiparticle field $\\psi(x)$, its adjoint $\\overline{\\psi}$, the effective metric tensor $g^{\\mu\\nu}$, and the gauge field $A_{\\nu}$. The equations of motion are established through variational principles applied to the Lagrangian, resulting in modified Dirac equations. We discuss the interpretation of the additional terms in the equations of motion and their significance in capturing the modified gravitational dynamics in the graphene system. Our findings contribute to the understanding of analogue gravity models and their applications in condensed matter systems.","sentences":["We examine the analogue gravity model within the context of f(R,T) gravity applied to graphene.","The derivation of the Lagrangian density in two dimensions (2D) is undertaken, accounting for the altered gravitational effects as characterized by the function f(R,T).","The Lagrangian encompasses the quasiparticle field $\\psi(x)$, its adjoint $\\overline{\\psi}$, the effective metric tensor $g^{\\mu\\nu}$, and the gauge field $A_{\\nu}$.","The equations of motion are established through variational principles applied to the Lagrangian, resulting in modified Dirac equations.","We discuss the interpretation of the additional terms in the equations of motion and their significance in capturing the modified gravitational dynamics in the graphene system.","Our findings contribute to the understanding of analogue gravity models and their applications in condensed matter systems."],"url":"http://arxiv.org/abs/2403.06283v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-10 17:49:27","title":"The Numerical Solution in the Sense of Prager&Synge","abstract":"The solution in sense of Prager&Synge is the alternative to the commonly used notion of the numerical solution, which is considered as a limit of grid functions at mesh refinement. Prager&Synge solution is defined as a hypersphere containing the projection of the true solution of the system of partial differentiation equations (PDE) onto the computational grid and does not use any asymptotics. In the original variant it is determined using orthogonal properties specific for certain equations. In the proposed variant, the center and radius of the hypersphere is estimated using the ensemble of numerical solutions obtained by independent algorithms. This approach may be easily expanded for solutions of an arbitrary system of partial differentiation equations that significantly expands the domain of its applicability. Several options for the computation of the Prager&Synge solution are considered and compared herein. The first one is based on the search for the orthogonal truncation errors and their transformation. The second is based on the orthogonalization of approximation errors obtained using the defect correction method and applies a superposition of numerical solutions. These options are intrusive. In third option (nonintrusive) the information regarding orthogonality of errors, which is crucial for the Prager&Synge approach method, is replaced by information that stems from the properties of the ensemble of numerical solutions, obtained by independent numerical algorithms. The values of the angle between the truncation errors on such ensemble or the distances between elements of the ensemble may be used to replace the orthogonality. The variant based on the width of the ensemble of independent numerical solutions does not require any additional a priori information and is the approximate nonintrusive version of the method based on the orthogonalization of approximation errors.","sentences":["The solution in sense of Prager&Synge is the alternative to the commonly used notion of the numerical solution, which is considered as a limit of grid functions at mesh refinement.","Prager&Synge solution is defined as a hypersphere containing the projection of the true solution of the system of partial differentiation equations (PDE) onto the computational grid and does not use any asymptotics.","In the original variant it is determined using orthogonal properties specific for certain equations.","In the proposed variant, the center and radius of the hypersphere is estimated using the ensemble of numerical solutions obtained by independent algorithms.","This approach may be easily expanded for solutions of an arbitrary system of partial differentiation equations that significantly expands the domain of its applicability.","Several options for the computation of the Prager&Synge solution are considered and compared herein.","The first one is based on the search for the orthogonal truncation errors and their transformation.","The second is based on the orthogonalization of approximation errors obtained using the defect correction method and applies a superposition of numerical solutions.","These options are intrusive.","In third option (nonintrusive) the information regarding orthogonality of errors, which is crucial for the Prager&Synge approach method, is replaced by information that stems from the properties of the ensemble of numerical solutions, obtained by independent numerical algorithms.","The values of the angle between the truncation errors on such ensemble or the distances between elements of the ensemble may be used to replace the orthogonality.","The variant based on the width of the ensemble of independent numerical solutions does not require any additional a priori information and is the approximate nonintrusive version of the method based on the orthogonalization of approximation errors."],"url":"http://arxiv.org/abs/2403.06273v1","category":"math.NA"}
{"created":"2024-03-10 16:55:05","title":"Control of flow behavior in complex fluids using automatic differentiation","abstract":"Inverse design of complex flows is notoriously challenging because of the high cost of high dimensional optimization. Usually, optimization problems are either restricted to few control parameters, or adjoint-based approaches are used to convert the optimization problem into a boundary value problem. Here, we show that the recent advances in automatic differentiation (AD) provide a generic platform for solving inverse problems in complex fluids. To demonstrate the versatility of the approach, we solve an array of optimization problems related to active matter motion in Newtonian fluids, dispersion in structured porous media, and mixing in journal bearing. Each of these problems highlights the advantages of AD in ease of implementation and computational efficiency to solve high-dimensional optimization problems involving particle-laden flows.","sentences":["Inverse design of complex flows is notoriously challenging because of the high cost of high dimensional optimization.","Usually, optimization problems are either restricted to few control parameters, or adjoint-based approaches are used to convert the optimization problem into a boundary value problem.","Here, we show that the recent advances in automatic differentiation (AD) provide a generic platform for solving inverse problems in complex fluids.","To demonstrate the versatility of the approach, we solve an array of optimization problems related to active matter motion in Newtonian fluids, dispersion in structured porous media, and mixing in journal bearing.","Each of these problems highlights the advantages of AD in ease of implementation and computational efficiency to solve high-dimensional optimization problems involving particle-laden flows."],"url":"http://arxiv.org/abs/2403.06257v1","category":"physics.flu-dyn"}
{"created":"2024-03-10 16:42:20","title":"Flat or crumpled: states of active symmetric membranes","abstract":"We set up and study the hydrodynamic theory for active fluid and tethered membranes. We focus on those membranes which are inversion-symmetric. We show that such membranes are either described by appropriate linear hydrodynamic equations, which are exact in the asymptotic long wavelength limit, giving stable flat phases with positional quasi long range orders, or be linearly unstable, implying crumpling of the membranes. We argue that in a such an active membrane thermal noises dominate over any active noises, and use it to calculate the correlation functions of the undulation and in-plane displacements of the membrane in the stable case, and the associated correlation functions of the embedding bulk flow velocities.","sentences":["We set up and study the hydrodynamic theory for active fluid and tethered membranes.","We focus on those membranes which are inversion-symmetric.","We show that such membranes are either described by appropriate linear hydrodynamic equations, which are exact in the asymptotic long wavelength limit, giving stable flat phases with positional quasi long range orders, or be linearly unstable, implying crumpling of the membranes.","We argue that in a such an active membrane thermal noises dominate over any active noises, and use it to calculate the correlation functions of the undulation and in-plane displacements of the membrane in the stable case, and the associated correlation functions of the embedding bulk flow velocities."],"url":"http://arxiv.org/abs/2403.06256v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-10 16:34:21","title":"Online Multi-spectral Neuron Tracing","abstract":"In this paper, we propose an online multi-spectral neuron tracing method with uniquely designed modules, where no offline training are required. Our method is trained online to update our enhanced discriminative correlation filter to conglutinate the tracing process. This distinctive offline-training-free schema differentiates us from other training-dependent tracing approaches like deep learning methods since no annotation is needed for our method. Besides, compared to other tracing methods requiring complicated set-up such as for clustering and graph multi-cut, our approach is much easier to be applied to new images. In fact, it only needs a starting bounding box of the tracing neuron, significantly reducing users' configuration effort. Our extensive experiments show that our training-free and easy-configured methodology allows fast and accurate neuron reconstructions in multi-spectral images.","sentences":["In this paper, we propose an online multi-spectral neuron tracing method with uniquely designed modules, where no offline training are required.","Our method is trained online to update our enhanced discriminative correlation filter to conglutinate the tracing process.","This distinctive offline-training-free schema differentiates us from other training-dependent tracing approaches like deep learning methods since no annotation is needed for our method.","Besides, compared to other tracing methods requiring complicated set-up such as for clustering and graph multi-cut, our approach is much easier to be applied to new images.","In fact, it only needs a starting bounding box of the tracing neuron, significantly reducing users' configuration effort.","Our extensive experiments show that our training-free and easy-configured methodology allows fast and accurate neuron reconstructions in multi-spectral images."],"url":"http://arxiv.org/abs/2403.06251v1","category":"q-bio.NC"}
{"created":"2024-03-10 16:23:29","title":"Averaging operators on groups, racks and Leibniz algebras","abstract":"This paper considers averaging operators on various algebraic structures and studies the induced structures. We first introduce the notion of an averaging operator on a group $G$ and show that it induces a rack structure. Moreover, the given group structure and the induced rack structure form a group-rack. We observe that any pointed group-rack can be embedded into an averaging group. We show that the differentiation of a smooth pointed averaging operator on a Lie group gives rise to an averaging operator on the corresponding Lie algebra. Next, we consider averaging operators on a rack that induces a hierarchy of new rack structures. Moreover, any two racks with increasing hierarchy levels form a rack-pairing, a structure that is related to two-sided skew braces by conjugation. We also consider averaging operators on cocommutative Hopf algebras and braided vector spaces in relations to averaging operators on groups, Lie algebras and racks. In the end, we define averaging operators on a Leibniz algebra, find the induced structure and show that the differentiation of a smooth pointed averaging operator on a pointed Lie rack yields an averaging operator on the corresponding Leibniz algebra.","sentences":["This paper considers averaging operators on various algebraic structures and studies the induced structures.","We first introduce the notion of an averaging operator on a group $G$ and show that it induces a rack structure.","Moreover, the given group structure and the induced rack structure form a group-rack.","We observe that any pointed group-rack can be embedded into an averaging group.","We show that the differentiation of a smooth pointed averaging operator on a Lie group gives rise to an averaging operator on the corresponding Lie algebra.","Next, we consider averaging operators on a rack that induces a hierarchy of new rack structures.","Moreover, any two racks with increasing hierarchy levels form a rack-pairing, a structure that is related to two-sided skew braces by conjugation.","We also consider averaging operators on cocommutative Hopf algebras and braided vector spaces in relations to averaging operators on groups, Lie algebras and racks.","In the end, we define averaging operators on a Leibniz algebra, find the induced structure and show that the differentiation of a smooth pointed averaging operator on a pointed Lie rack yields an averaging operator on the corresponding Leibniz algebra."],"url":"http://arxiv.org/abs/2403.06250v1","category":"math.RA"}
{"created":"2024-03-10 15:15:35","title":"Finding Visual Saliency in Continuous Spike Stream","abstract":"As a bio-inspired vision sensor, the spike camera emulates the operational principles of the fovea, a compact retinal region, by employing spike discharges to encode the accumulation of per-pixel luminance intensity. Leveraging its high temporal resolution and bio-inspired neuromorphic design, the spike camera holds significant promise for advancing computer vision applications. Saliency detection mimics the behavior of human beings and captures the most salient region from the scenes. In this paper, we investigate the visual saliency in the continuous spike stream for the first time. To effectively process the binary spike stream, we propose a Recurrent Spiking Transformer (RST) framework, which is based on a full spiking neural network. Our framework enables the extraction of spatio-temporal features from the continuous spatio-temporal spike stream while maintaining low power consumption. To facilitate the training and validation of our proposed model, we build a comprehensive real-world spike-based visual saliency dataset, enriched with numerous light conditions. Extensive experiments demonstrate the superior performance of our Recurrent Spiking Transformer framework in comparison to other spike neural network-based methods. Our framework exhibits a substantial margin of improvement in capturing and highlighting visual saliency in the spike stream, which not only provides a new perspective for spike-based saliency segmentation but also shows a new paradigm for full SNN-based transformer models. The code and dataset are available at \\url{https://github.com/BIT-Vision/SVS}.","sentences":["As a bio-inspired vision sensor, the spike camera emulates the operational principles of the fovea, a compact retinal region, by employing spike discharges to encode the accumulation of per-pixel luminance intensity.","Leveraging its high temporal resolution and bio-inspired neuromorphic design, the spike camera holds significant promise for advancing computer vision applications.","Saliency detection mimics the behavior of human beings and captures the most salient region from the scenes.","In this paper, we investigate the visual saliency in the continuous spike stream for the first time.","To effectively process the binary spike stream, we propose a Recurrent Spiking Transformer (RST) framework, which is based on a full spiking neural network.","Our framework enables the extraction of spatio-temporal features from the continuous spatio-temporal spike stream while maintaining low power consumption.","To facilitate the training and validation of our proposed model, we build a comprehensive real-world spike-based visual saliency dataset, enriched with numerous light conditions.","Extensive experiments demonstrate the superior performance of our Recurrent Spiking Transformer framework in comparison to other spike neural network-based methods.","Our framework exhibits a substantial margin of improvement in capturing and highlighting visual saliency in the spike stream, which not only provides a new perspective for spike-based saliency segmentation but also shows a new paradigm for full SNN-based transformer models.","The code and dataset are available at \\url{https://github.com/BIT-Vision/SVS}."],"url":"http://arxiv.org/abs/2403.06233v1","category":"cs.CV"}
{"created":"2024-03-10 15:12:48","title":"Yu-Shiba-Rusinov states in the s-wave superconducting kagome Hubbard model: Self-consistent Bogoliubov-de Gennes calculations","abstract":"The Yu-Shiba-Rusinov (YSR) states in kagome superconductors have been studied recently in theory and experiments. However, more details of local superconductivity around the YSR states in kagome superconductors and the existence of the related first-order phase transition remain unclear. Based on the self-consistent Bogoliubov-de Gennes equations for the $s-$wave superconducting kagome Hubbard model, we show that first-order phase transitions occur when varying the spin-magnetic interaction $J$ of the magnetic impurity, and discontinuities of hysteresis loops in the plots of the local pair potential versus $J$ happen at the upper and lower critical interactions $J_{\\rm c1}$ and $J_{\\rm c2}$, rather than the conventional critical interaction $J_c$. The minimal energy of the stable YSR state $E_{\\rm YSR, min}$ is non-zero for the chemical potential at the van Hove singularities, Dirac point, and the flat band. Furthermore, $J_{\\rm c1}$, $J_{\\rm c2}$, and $E_{\\rm YSR, min}$ are dependent on the chemical potential and the superconducting coupling interaction.","sentences":["The Yu-Shiba-Rusinov (YSR) states in kagome superconductors have been studied recently in theory and experiments.","However, more details of local superconductivity around the YSR states in kagome superconductors and the existence of the related first-order phase transition remain unclear.","Based on the self-consistent Bogoliubov-de Gennes equations for the $s-$wave superconducting kagome Hubbard model, we show that first-order phase transitions occur when varying the spin-magnetic interaction $J$ of the magnetic impurity, and discontinuities of hysteresis loops in the plots of the local pair potential versus $J$ happen at the upper and lower critical interactions $J_{\\rm c1}$ and $J_{\\rm c2}$, rather than the conventional critical interaction $J_c$. The minimal energy of the stable YSR state $E_{\\rm YSR, min}$ is non-zero for the chemical potential at the van Hove singularities, Dirac point, and the flat band.","Furthermore, $J_{\\rm c1}$, $J_{\\rm c2}$, and $E_{\\rm YSR, min}$ are dependent on the chemical potential and the superconducting coupling interaction."],"url":"http://arxiv.org/abs/2403.06231v1","category":"cond-mat.supr-con"}
{"created":"2024-03-10 14:33:55","title":"PEPSI: Pathology-Enhanced Pulse-Sequence-Invariant Representations for Brain MRI","abstract":"Remarkable progress has been made by data-driven machine-learning methods in the analysis of MRI scans. However, most existing MRI analysis approaches are crafted for specific MR pulse sequences (MR contrasts) and usually require nearly isotropic acquisitions. This limits their applicability to diverse real-world clinical data, where scans commonly exhibit variations in appearances due to being obtained with varying sequence parameters, resolutions, and orientations -- especially in the presence of pathology. In this paper, we propose PEPSI, the first pathology-enhanced, and pulse-sequence-invariant feature representation learning model for brain MRI. PEPSI is trained entirely on synthetic images with a novel pathology encoding strategy, and enables co-training across datasets with diverse pathologies and missing modalities. Despite variations in pathology appearances across different MR pulse sequences or the quality of acquired images (e.g., resolution, orientation, artifacts, etc), PEPSI produces a high-resolution image of reference contrast (MP-RAGE) that captures anatomy, along with an image specifically highlighting the pathology. Our experiments demonstrate PEPSI's remarkable capability for image synthesis compared with the state-of-the-art, contrast-agnostic synthesis models, as it accurately reconstructs anatomical structures while differentiating between pathology and normal tissue. We further illustrate the efficiency and effectiveness of PEPSI features for downstream pathology segmentations on five public datasets covering white matter hyperintensities and stroke lesions. Code is available at https://github.com/peirong26/PEPSI.","sentences":["Remarkable progress has been made by data-driven machine-learning methods in the analysis of MRI scans.","However, most existing MRI analysis approaches are crafted for specific MR pulse sequences (MR contrasts) and usually require nearly isotropic acquisitions.","This limits their applicability to diverse real-world clinical data, where scans commonly exhibit variations in appearances due to being obtained with varying sequence parameters, resolutions, and orientations -- especially in the presence of pathology.","In this paper, we propose PEPSI, the first pathology-enhanced, and pulse-sequence-invariant feature representation learning model for brain MRI.","PEPSI is trained entirely on synthetic images with a novel pathology encoding strategy, and enables co-training across datasets with diverse pathologies and missing modalities.","Despite variations in pathology appearances across different MR pulse sequences or the quality of acquired images (e.g., resolution, orientation, artifacts, etc), PEPSI produces a high-resolution image of reference contrast (MP-RAGE) that captures anatomy, along with an image specifically highlighting the pathology.","Our experiments demonstrate PEPSI's remarkable capability for image synthesis compared with the state-of-the-art, contrast-agnostic synthesis models, as it accurately reconstructs anatomical structures while differentiating between pathology and normal tissue.","We further illustrate the efficiency and effectiveness of PEPSI features for downstream pathology segmentations on five public datasets covering white matter hyperintensities and stroke lesions.","Code is available at https://github.com/peirong26/PEPSI."],"url":"http://arxiv.org/abs/2403.06227v1","category":"eess.IV"}
{"created":"2024-03-10 13:41:23","title":"Quasi-spherical metrics and the static Minkowski inequality","abstract":"We prove that equality within the Minkowski inequality for asymptotically flat static spaces is achieved only by slices in Schwarzschild space for mean-convex, non-negative scalar curvature boundaries. In order to establish this, we prove uniqueness of quasi-spherical static metrics: any quasi-spherical metric with vanishing shear vector that admits a bounded static vacuum potential is a rotationally symmetric piece of Schwarzschild space. We also observe that the static Minkowski inequality extends to all dimensions when the boundary is connected and to weakly asymptotically flat static spaces. As a result, the uniqueness theorems for photon surfaces and static metric extensions from our prequel extend to this larger class and to all dimensions.","sentences":["We prove that equality within the Minkowski inequality for asymptotically flat static spaces is achieved only by slices in Schwarzschild space for mean-convex, non-negative scalar curvature boundaries.","In order to establish this, we prove uniqueness of quasi-spherical static metrics: any quasi-spherical metric with vanishing shear vector that admits a bounded static vacuum potential is a rotationally symmetric piece of Schwarzschild space.","We also observe that the static Minkowski inequality extends to all dimensions when the boundary is connected and to weakly asymptotically flat static spaces.","As a result, the uniqueness theorems for photon surfaces and static metric extensions from our prequel extend to this larger class and to all dimensions."],"url":"http://arxiv.org/abs/2403.06216v1","category":"math.DG"}
{"created":"2024-03-10 13:28:56","title":"Distributed quantum architecture search","abstract":"Variational quantum algorithms, inspired by neural networks, have become a novel approach in quantum computing. However, designing efficient parameterized quantum circuits remains a challenge. Quantum architecture search tackles this by adjusting circuit structures along with gate parameters to automatically discover high-performance circuit structures. In this study, we propose an end-to-end distributed quantum architecture search framework, where we aim to automatically design distributed quantum circuit structures for interconnected quantum processing units with specific qubit connectivity. We devise a circuit generation algorithm which incorporates TeleGate and TeleData methods to enable nonlocal gate implementation across quantum processing units. While taking into account qubit connectivity, we also incorporate qubit assignment from logical to physical qubits within our QAS framework. A two-stage progressive training-free strategy is employed to evaluate extensive circuit structures without circuit training costs. Through numerical experiments on three VQE tasks, the efficacy and efficiency of our scheme is demonstrated.","sentences":["Variational quantum algorithms, inspired by neural networks, have become a novel approach in quantum computing.","However, designing efficient parameterized quantum circuits remains a challenge.","Quantum architecture search tackles this by adjusting circuit structures along with gate parameters to automatically discover high-performance circuit structures.","In this study, we propose an end-to-end distributed quantum architecture search framework, where we aim to automatically design distributed quantum circuit structures for interconnected quantum processing units with specific qubit connectivity.","We devise a circuit generation algorithm which incorporates TeleGate and TeleData methods to enable nonlocal gate implementation across quantum processing units.","While taking into account qubit connectivity, we also incorporate qubit assignment from logical to physical qubits within our QAS framework.","A two-stage progressive training-free strategy is employed to evaluate extensive circuit structures without circuit training costs.","Through numerical experiments on three VQE tasks, the efficacy and efficiency of our scheme is demonstrated."],"url":"http://arxiv.org/abs/2403.06214v1","category":"quant-ph"}
{"created":"2024-03-10 13:05:20","title":"Homogeneous quandles with abelian inner automorphism groups and vertex-transitive graphs","abstract":"A quandle is an algebraic system originated in knot theory, and can be regarded as a generalization of symmetric spaces. The inner automorphism group of a quandle is defined as the group generated by the point symmetries (right multiplications). In this paper, starting from any simple graphs, we construct quandles whose inner automorphism groups are abelian. We also prove that the constructed quandle is homogeneous if and only if the graph is vertex-transitive. This shows that there is a wide family of quandles with abelian inner automorphism groups, even if we impose the homogeneity. The key examples of such quandles are realized as subquandles of oriented real Grassmannian manifolds.","sentences":["A quandle is an algebraic system originated in knot theory, and can be regarded as a generalization of symmetric spaces.","The inner automorphism group of a quandle is defined as the group generated by the point symmetries (right multiplications).","In this paper, starting from any simple graphs, we construct quandles whose inner automorphism groups are abelian.","We also prove that the constructed quandle is homogeneous if and only if the graph is vertex-transitive.","This shows that there is a wide family of quandles with abelian inner automorphism groups, even if we impose the homogeneity.","The key examples of such quandles are realized as subquandles of oriented real Grassmannian manifolds."],"url":"http://arxiv.org/abs/2403.06209v1","category":"math.GT"}
{"created":"2024-03-10 12:42:57","title":"Theory on new fractional operators using normalization and probability tools","abstract":"Recently, a theory on L-fractional differential equations was developed in the work [arXiv:2403.00341]. These are based on normalizing the Caputo operator, so that alternative properties hold, such as smoothness of the solution, finite derivative at the initial instant, and units time$^{-1}$ of the system. The present paper follows that line of research. On the one hand, we show how a rescaling of fractional operators with bounded kernels may help circumvent their documented deficiencies, for example, the inconsistency at zero or the lack of inverse integral operator. On the other hand, we build a novel class of linear operators with memory effects to extend the L-fractional and the ordinary derivatives, using probability tools. A Mittag-Leffler-type function is introduced to solve linear problems, and nonlinear equations are solved with power series (as an analogue of the Cauchy-Kovalevskaya theorem), illustrating the methods for the SIR model. The inverse operator is constructed, and a fundamental theorem of calculus and an existence-and-uniqueness result for differential equations are proved. A conjecture on deconvolution is raised, that would permit completing the proposed theory.","sentences":["Recently, a theory on L-fractional differential equations was developed in the work [arXiv:2403.00341].","These are based on normalizing the Caputo operator, so that alternative properties hold, such as smoothness of the solution, finite derivative at the initial instant, and units time$^{-1}$ of the system.","The present paper follows that line of research.","On the one hand, we show how a rescaling of fractional operators with bounded kernels may help circumvent their documented deficiencies, for example, the inconsistency at zero or the lack of inverse integral operator.","On the other hand, we build a novel class of linear operators with memory effects to extend the L-fractional and the ordinary derivatives, using probability tools.","A Mittag-Leffler-type function is introduced to solve linear problems, and nonlinear equations are solved with power series (as an analogue of the Cauchy-Kovalevskaya theorem), illustrating the methods for the SIR model.","The inverse operator is constructed, and a fundamental theorem of calculus and an existence-and-uniqueness result for differential equations are proved.","A conjecture on deconvolution is raised, that would permit completing the proposed theory."],"url":"http://arxiv.org/abs/2403.06198v1","category":"math.PR"}
{"created":"2024-03-10 12:14:24","title":"Hairer-Quastel universality for KPZ -- polynomial smoothing mechanisms, general nonlinearities and Poisson noise","abstract":"We consider a class of weakly asymmetric continuous microscopic growth models with polynomial smoothing mechanisms, general nonlinearities and a Poisson type noise. We show that they converge to the KPZ equation after proper rescaling and re-centering, where the coupling constant depends nontrivially on all details of the smoothing and growth mechanisms in the microscopic model. This confirms some of the predictions in [HQ18].   The proof builds on the general discretisation framework of regularity structures ([EH19]), and employs the idea of using the spectral gap inequality to control stochastic objects as developed and systematised in [LOTT21, HS24], together with a new observation on structures of the Malliavin derivatives in our situation.","sentences":["We consider a class of weakly asymmetric continuous microscopic growth models with polynomial smoothing mechanisms, general nonlinearities and a Poisson type noise.","We show that they converge to the KPZ equation after proper rescaling and re-centering, where the coupling constant depends nontrivially on all details of the smoothing and growth mechanisms in the microscopic model.","This confirms some of the predictions in [HQ18].   ","The proof builds on the general discretisation framework of regularity structures ([EH19]), and employs the idea of using the spectral gap inequality to control stochastic objects as developed and systematised in [LOTT21, HS24], together with a new observation on structures of the Malliavin derivatives in our situation."],"url":"http://arxiv.org/abs/2403.06191v1","category":"math.PR"}
{"created":"2024-03-10 11:37:17","title":"Transient growth in diabatic boundary layers with fluids at supercritical pressure","abstract":"In the thermodynamic region close to the vapour-liquid critical point and in the proximity of the pseudo-boiling (Widom) line, strong property variations extensively alter the growth of modal instabilities, as revealed in Ren et al. (J. Fluid Mech., vol. 871, 2019, pp. 831-864). Here, we study non-modal disturbances in the spatial framework using an eigenvector decomposition of the linearized Navier-Stokes equations for locally parallel flow. To account for non-ideality, a new energy norm is derived. At a constant free-stream Mach number of $10^{-3}$ along an isobar, three different regimes are considered by regulating the diabatic wall temperature: subcritical (liquid-like), supercritical (gas-like), and transcritical (across the Widom line). In the non-transcritical regimes, the resulting streamwise-independent streaks originate from the lift-up effect. Wall cooling is found to enhance the energy amplification for both subcritical and supercritical regimes. However, when the temperature profile is increased beyond the Widom line, a strong sub-optimal growth is observed over very short streamwise distances due to the Orr mechanism. The optimal energy growth at large distances is found to arise from an interplay between lift-up and Orr mechanism due to the additional presence of the transcritical Mode II. As a result, optimal disturbances are streamwise-modulated streaks with strong thermal components and with a propagation angle inversely proportional to the local Reynolds number. An $N$-factor comparison with modal growth reveals that transient growth is scarcely affected by a wall-temperature variation in the non-transcritical regimes. In contrast, in the transcritical regime, transient growth is greatly enhanced, especially when cooling beyond the Widom line. Here, non-modal $N$-factors are found to resemble the imposition of an adverse pressure gradient in the ideal-gas regime.","sentences":["In the thermodynamic region close to the vapour-liquid critical point and in the proximity of the pseudo-boiling (Widom) line, strong property variations extensively alter the growth of modal instabilities, as revealed in Ren et al.","(J. Fluid Mech., vol.","871, 2019, pp. 831-864).","Here, we study non-modal disturbances in the spatial framework using an eigenvector decomposition of the linearized Navier-Stokes equations for locally parallel flow.","To account for non-ideality, a new energy norm is derived.","At a constant free-stream Mach number of $10^{-3}$ along an isobar, three different regimes are considered by regulating the diabatic wall temperature: subcritical (liquid-like), supercritical (gas-like), and transcritical (across the Widom line).","In the non-transcritical regimes, the resulting streamwise-independent streaks originate from the lift-up effect.","Wall cooling is found to enhance the energy amplification for both subcritical and supercritical regimes.","However, when the temperature profile is increased beyond the Widom line, a strong sub-optimal growth is observed over very short streamwise distances due to the Orr mechanism.","The optimal energy growth at large distances is found to arise from an interplay between lift-up and Orr mechanism due to the additional presence of the transcritical Mode II.","As a result, optimal disturbances are streamwise-modulated streaks with strong thermal components and with a propagation angle inversely proportional to the local Reynolds number.","An $N$-factor comparison with modal growth reveals that transient growth is scarcely affected by a wall-temperature variation in the non-transcritical regimes.","In contrast, in the transcritical regime, transient growth is greatly enhanced, especially when cooling beyond the Widom line.","Here, non-modal $N$-factors are found to resemble the imposition of an adverse pressure gradient in the ideal-gas regime."],"url":"http://arxiv.org/abs/2403.06181v1","category":"physics.flu-dyn"}
{"created":"2024-03-10 11:30:38","title":"Estimating the mass of galactic components using machine learning algorithms","abstract":"The estimation of the bulge and disk massses, the main baryonic components of a galaxy, can be performed using various approaches, but their implementation tend to be challenging as they often rely on strong assumptions about either the baryon dynamics or the dark matter model. In this work, we present an alternative method for predicting the masses of galactic components, including the disk, bulge, stellar and total mass, using a set of machine learning algorithms: KNN-neighbours (KNN), Linear Regression (LR), Random Forest (RF) and Neural Network (NN). The rest-frame absolute magnitudes in the ugriz-photometric system were selected as input features, and the training was performed using a sample of spiral galaxies hosting a bulge from Guo's mock catalogue \\citep{Guo-Catalog} derived from the Millennium simulation. In general, all the algorithms provide good predictions for the galaxy's mass components ranging from $10^9\\,M_\\odot$ to $10^{11}\\,M_\\odot$, corresponding to the central region of the training mass domain; however, the NN give rise to the most precise predictions in comparison to other methods. Additionally, to test the performance of the NN architecture, we used a sample of observed galaxies from the SDSS survey whose mass components are known. We found that the NN can predict the luminous masses of disk-dominant galaxies within the same range of magnitudes that for the synthetic sample up to a $99\\%$ level of confidence, while mass components of galaxies hosting larger bulges are well predicted up to $95\\%$ level of confidence. The NN algorithm can also bring up scaling relations between masses of different components and magnitudes.","sentences":["The estimation of the bulge and disk massses, the main baryonic components of a galaxy, can be performed using various approaches, but their implementation tend to be challenging as they often rely on strong assumptions about either the baryon dynamics or the dark matter model.","In this work, we present an alternative method for predicting the masses of galactic components, including the disk, bulge, stellar and total mass, using a set of machine learning algorithms: KNN-neighbours (KNN), Linear Regression (LR), Random Forest (RF) and Neural Network (NN).","The rest-frame absolute magnitudes in the ugriz-photometric system were selected as input features, and the training was performed using a sample of spiral galaxies hosting a bulge from Guo's mock catalogue \\citep{Guo-Catalog} derived from the Millennium simulation.","In general, all the algorithms provide good predictions for the galaxy's mass components ranging from $10^9\\,M_\\odot$ to $10^{11}\\,M_\\odot$, corresponding to the central region of the training mass domain; however, the NN give rise to the most precise predictions in comparison to other methods.","Additionally, to test the performance of the NN architecture, we used a sample of observed galaxies from the SDSS survey whose mass components are known.","We found that the NN can predict the luminous masses of disk-dominant galaxies within the same range of magnitudes that for the synthetic sample up to a $99\\%$ level of confidence, while mass components of galaxies hosting larger bulges are well predicted up to $95\\%$ level of confidence.","The NN algorithm can also bring up scaling relations between masses of different components and magnitudes."],"url":"http://arxiv.org/abs/2403.06178v1","category":"astro-ph.GA"}
{"created":"2024-03-10 10:19:59","title":"Nonequilibrium Phase Transition in a 2D Ferromagnetic Spins with Effective Interactions","abstract":"We investigate nonequilibrium phase transitions (PTs) in a 2D ferromagnetic Ising model on a square lattice with effective interactions via Monte Carlo (MC) based computational algorithms. Possibly, we simplify the generic complexity of energy function describing the system model to manage the algorithms. The importance of accounting for the effective interactions to admit establishing the nature of nonequilibrium PT emphasizes. The existence of an effective parameter $h$ is verified employing mean-field theory, and the self-consistent equations (SCEs) are derived based on the two familiar dynamics$-$Metropolis and Glauber algorithms. The results qualitatively demonstrate that both dynamics estimate the same SCE for $-1<h<1$. We show the critical temperature $T_{c}$ of the model {\\it with} effective interactions to be given as $T_{c}(h\\neq 0)=4(1+h)/\\ln(3 + 2\\sqrt{2})$ where $T_{c}^{0}=T_{c}(h=0)$ retrieves the well-known analytical result of equilibrium Ising model {\\it without} the effective interactions. Furthermore, we perform Metropolis MC simulations for a finite system of different lattice sizes with periodic boundary conditions and measure physical quantities of interest. By using data of the measurements, $T_{c}(h)$ and relevant critical { \\it exponents} for various values of $h$ are determined employing finite-size scaling (FSS) techniques. The FSS result of $T_{c}(h\\neq 0)$ obtained from numerical data is accurately in agreement with analytical results and quite different from $T_{c}^{0}$ as expected. Even so, the numerical results of the exponents are consistent with analytical values of the equilibrium 2D Ising model$-$belonging to the same universality class.","sentences":["We investigate nonequilibrium phase transitions (PTs) in a 2D ferromagnetic Ising model on a square lattice with effective interactions via Monte Carlo (MC) based computational algorithms.","Possibly, we simplify the generic complexity of energy function describing the system model to manage the algorithms.","The importance of accounting for the effective interactions to admit establishing the nature of nonequilibrium PT emphasizes.","The existence of an effective parameter $h$ is verified employing mean-field theory, and the self-consistent equations (SCEs) are derived based on the two familiar dynamics$-$Metropolis and Glauber algorithms.","The results qualitatively demonstrate that both dynamics estimate the same SCE for $-1<h<1$. We show the critical temperature $T_{c}$ of the model {\\it with} effective interactions to be given as $T_{c}(h\\neq 0)=4(1+h)/\\ln(3","+ 2\\sqrt{2})$ where $T_{c}^{0}=T_{c}(h=0)$ retrieves the well-known analytical result of equilibrium Ising model {\\it without} the effective interactions.","Furthermore, we perform Metropolis MC simulations for a finite system of different lattice sizes with periodic boundary conditions and measure physical quantities of interest.","By using data of the measurements, $T_{c}(h)$ and relevant critical { \\it exponents} for various values of $h$ are determined employing finite-size scaling (FSS) techniques.","The FSS result of $T_{c}(h\\neq 0)$ obtained from numerical data is accurately in agreement with analytical results and quite different from $T_{c}^{0}$ as expected.","Even so, the numerical results of the exponents are consistent with analytical values of the equilibrium 2D Ising model$-$belonging to the same universality class."],"url":"http://arxiv.org/abs/2403.06162v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-10 10:15:21","title":"An approach using the null space to implement Dirichlet and constraint boundary conditions into FEM","abstract":"A handy technique for the Finite Element Method (FEM) is presented that uses the null space for the implementation of Dirichlet and constraint boundary conditions. The focus of this method is to present an illustrative approach to modeling boundary constraints within FEM simulations for teaching. It presents a consistent way of including the boundary terms in the forcing and constructing the field solution after solving the algebraic system of equations.","sentences":["A handy technique for the Finite Element Method (FEM) is presented that uses the null space for the implementation of Dirichlet and constraint boundary conditions.","The focus of this method is to present an illustrative approach to modeling boundary constraints within FEM simulations for teaching.","It presents a consistent way of including the boundary terms in the forcing and constructing the field solution after solving the algebraic system of equations."],"url":"http://arxiv.org/abs/2403.06160v1","category":"math.NA"}
{"created":"2024-03-10 10:12:32","title":"Cracking the neural code for word recognition in convolutional neural networks","abstract":"Learning to read places a strong challenge on the visual system. Years of expertise lead to a remarkable capacity to separate highly similar letters and encode their relative positions, thus distinguishing words such as FORM and FROM, invariantly over a large range of sizes and absolute positions. How neural circuits achieve invariant word recognition remains unknown. Here, we address this issue by training deep neural network models to recognize written words and then analyzing how reading-specialized units emerge and operate across different layers of the network. With literacy, a small subset of units becomes specialized for word recognition in the learned script, similar to the \"visual word form area\" of the human brain. We show that these units are sensitive to specific letter identities and their distance from the blank space at the left or right of a word, thus acting as \"space bigrams\". These units specifically encode ordinal positions and operate by pooling across low and high-frequency detector units from early layers of the network. The proposed neural code provides a mechanistic insight into how information on letter identity and position is extracted and allow for invariant word recognition, and leads to predictions for reading behavior, error patterns, and the neurophysiology of reading.","sentences":["Learning to read places a strong challenge on the visual system.","Years of expertise lead to a remarkable capacity to separate highly similar letters and encode their relative positions, thus distinguishing words such as FORM and FROM, invariantly over a large range of sizes and absolute positions.","How neural circuits achieve invariant word recognition remains unknown.","Here, we address this issue by training deep neural network models to recognize written words and then analyzing how reading-specialized units emerge and operate across different layers of the network.","With literacy, a small subset of units becomes specialized for word recognition in the learned script, similar to the \"visual word form area\" of the human brain.","We show that these units are sensitive to specific letter identities and their distance from the blank space at the left or right of a word, thus acting as \"space bigrams\".","These units specifically encode ordinal positions and operate by pooling across low and high-frequency detector units from early layers of the network.","The proposed neural code provides a mechanistic insight into how information on letter identity and position is extracted and allow for invariant word recognition, and leads to predictions for reading behavior, error patterns, and the neurophysiology of reading."],"url":"http://arxiv.org/abs/2403.06159v1","category":"cs.CV"}
{"created":"2024-03-10 10:02:26","title":"Nonlinear Schr\u00f6dinger equation in terms of elliptic and hyperelliptic $\u03c3$ functions","abstract":"It is known that the elliptic function solutions of the nonlinear Schr\\\"odinger equation are reduced to the algebraic differential relation in terms of the Weierstrass sigma function, $\\displaystyle{ \\left[-\\sqrt{-1}\\frac{\\partial}{\\partial t} -\\alpha \\frac{\\partial}{\\partial u}\\right]\\Psi +\\frac{1}{2} \\frac{\\partial^2}{\\partial u^2}\\Psi +(\\Psi^* \\Psi) \\Psi = \\frac12 (2\\beta+\\wp(v)+\\lambda_2-\\alpha^2)\\Psi }$, where $\\Psi(u;v, t):=\\mathrm{e}^{\\alpha u+\\sqrt{-1}\\beta t+c}$ $\\displaystyle{\\frac{\\mathrm{e}^{-\\zeta(v)u}\\sigma(u+v)}{\\sigma(u)\\sigma(v)}}$, its dual $\\Psi^*(u; v,t)$, and certain complex numbers $\\alpha, \\beta$. In this paper, we generalize the algebraic differential relation to those of genera two and three in terms of the hyperelliptic sigma functions.","sentences":["It is known that the elliptic function solutions of the nonlinear Schr\\\"odinger equation are reduced to the algebraic differential relation in terms of the Weierstrass sigma function, $\\displaystyle{ \\left[-\\sqrt{-1}\\frac{\\partial}{\\partial t} -\\alpha \\frac{\\partial}{\\partial u}\\right]\\Psi +\\frac{1}{2} \\frac{\\partial^2}{\\partial","u^2}\\Psi +(\\Psi^* \\Psi) \\Psi = \\frac12 (2\\beta+\\wp(v)+\\lambda_2-\\alpha^2)\\Psi }$, where $\\Psi(u;v, t):=\\mathrm{e}^{\\alpha u+\\sqrt{-1}\\beta t+c}$ $\\displaystyle{\\frac{\\mathrm{e}^{-\\zeta(v)u}\\sigma(u+v)}{\\sigma(u)\\sigma(v)}}$, its dual $\\Psi^*(u; v,t)$, and certain complex numbers $\\alpha, \\beta$.","In this paper, we generalize the algebraic differential relation to those of genera two and three in terms of the hyperelliptic sigma functions."],"url":"http://arxiv.org/abs/2403.06156v1","category":"nlin.SI"}
{"created":"2024-03-10 09:57:55","title":"A Parisi Formula for Quantum Spin Glasses","abstract":"We establish three equivalent versions of a Parisi formula for the free energy of mean-field spin glasses in a transversal magnetic field. These results are derived from available results for classical vector spin glasses by an approximation method using the functional integral representation of the partition function. In this approach, the order parameter is a non-decreasing function with values in the non-negative, real hermitian Hilbert-Schmidt operators. For the quantum Sherrington-Kirkpatrick model, we also show that under the assumption of self-averaging of the self-overlap, the optimising Parisi order parameter is found within a two-dimensional subspace spanned by the self-overlap and the fully stationary overlap.","sentences":["We establish three equivalent versions of a Parisi formula for the free energy of mean-field spin glasses in a transversal magnetic field.","These results are derived from available results for classical vector spin glasses by an approximation method using the functional integral representation of the partition function.","In this approach, the order parameter is a non-decreasing function with values in the non-negative, real hermitian Hilbert-Schmidt operators.","For the quantum Sherrington-Kirkpatrick model, we also show that under the assumption of self-averaging of the self-overlap, the optimising Parisi order parameter is found within a two-dimensional subspace spanned by the self-overlap and the fully stationary overlap."],"url":"http://arxiv.org/abs/2403.06155v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-10 08:56:02","title":"Bayesian Random Semantic Data Augmentation for Medical Image Classification","abstract":"Data augmentation is a critical regularization technique for deep neural networks, particularly in medical image classification. Popular data augmentation approaches include image transformation-based methods, generative data augmentation, and automatic data augmentation. However, these approaches encounter notable limitations: image transformation-based and automated data augmentation techniques cannot implement semantic transformations, leading to a constrained variety of augmented samples, and generative data augmentation methods are computationally expensive. In response to these challenges, we proposed Bayesian Random Semantic Data Augmentation (BRSDA), a novel, efficient, and plug-and-play semantic data augmentation method. BRSDA is motivated by a simple translation in the feature space along specific directions that can effectuate semantic transformations. When given a feature, we define its augmentable semantic magnitude as a random variable and estimate its distribution using variational Bayesian, then sample semantic magnitude and add to the randomly selected semantic direction to achieve semantic data augmentation. We demonstrate the effectiveness of BRSDA on five 2D and six 3D medical image datasets covering nine modalities. We also test BRSDA with mainstream neural network architectures, showcasing its robustness. Furthermore, combining BRSDA with other leading data augmentation methods achieves superior performance. Code is available online at \\url{https://github.com/YaoyaoZhu19/BRSDA}.","sentences":["Data augmentation is a critical regularization technique for deep neural networks, particularly in medical image classification.","Popular data augmentation approaches include image transformation-based methods, generative data augmentation, and automatic data augmentation.","However, these approaches encounter notable limitations: image transformation-based and automated data augmentation techniques cannot implement semantic transformations, leading to a constrained variety of augmented samples, and generative data augmentation methods are computationally expensive.","In response to these challenges, we proposed Bayesian Random Semantic Data Augmentation (BRSDA), a novel, efficient, and plug-and-play semantic data augmentation method.","BRSDA is motivated by a simple translation in the feature space along specific directions that can effectuate semantic transformations.","When given a feature, we define its augmentable semantic magnitude as a random variable and estimate its distribution using variational Bayesian, then sample semantic magnitude and add to the randomly selected semantic direction to achieve semantic data augmentation.","We demonstrate the effectiveness of BRSDA on five 2D and six 3D medical image datasets covering nine modalities.","We also test BRSDA with mainstream neural network architectures, showcasing its robustness.","Furthermore, combining BRSDA with other leading data augmentation methods achieves superior performance.","Code is available online at \\url{https://github.com/YaoyaoZhu19/BRSDA}."],"url":"http://arxiv.org/abs/2403.06138v1","category":"cs.CV"}
{"created":"2024-03-10 07:30:38","title":"Dynamical generation of skyrmion and bimeron crystals by a circularly polarized electric field in frustrated magnets","abstract":"A skyrmion crystal (SkX) has attracted much attention in condensed matter physics, since topologically nontrivial structures induce fascinating physical phenomena. The SkXs have been experimentally observed in a variety of materials, where the Zeeman coupling to the static magnetic field plays an important role in the formation of the SkXs. In this study, we theoretically propose another route to generate the SkXs by using a circularly polarized electric field. We investigate a non-equilibrium steady state in a classical frustrated Heisenberg magnet under the circularly polarized electric field, where the electric field is coupled to the electric polarization via the spin-current mechanism. By numerically solving the Landau-Lifshitz-Gilbert equation at zero temperature, we show that the electric field radiation generates a SkX with a high topological number in the high-frequency regime, where the sign of the skyrmion number is fixed to be negative (positive) under the left (right) circularly polarized field. The intense electric field melts these SkXs and generates isolated skyrmions. We clarify that the microscopic origin is effective electric-field-induced three-spin interactions by adopting the high-frequency expansion in the Floquet formalism. Furthermore, we find that the electric field radiation generates another type of SkXs, a bimeron crystal, in the low-frequency regime. Our results provide a way to generate the SkXs and control the topology by the circularly polarized electric field.","sentences":["A skyrmion crystal (SkX) has attracted much attention in condensed matter physics, since topologically nontrivial structures induce fascinating physical phenomena.","The SkXs have been experimentally observed in a variety of materials, where the Zeeman coupling to the static magnetic field plays an important role in the formation of the SkXs.","In this study, we theoretically propose another route to generate the SkXs by using a circularly polarized electric field.","We investigate a non-equilibrium steady state in a classical frustrated Heisenberg magnet under the circularly polarized electric field, where the electric field is coupled to the electric polarization via the spin-current mechanism.","By numerically solving the Landau-Lifshitz-Gilbert equation at zero temperature, we show that the electric field radiation generates a SkX with a high topological number in the high-frequency regime, where the sign of the skyrmion number is fixed to be negative (positive) under the left (right) circularly polarized field.","The intense electric field melts these SkXs and generates isolated skyrmions.","We clarify that the microscopic origin is effective electric-field-induced three-spin interactions by adopting the high-frequency expansion in the Floquet formalism.","Furthermore, we find that the electric field radiation generates another type of SkXs, a bimeron crystal, in the low-frequency regime.","Our results provide a way to generate the SkXs and control the topology by the circularly polarized electric field."],"url":"http://arxiv.org/abs/2403.06118v1","category":"cond-mat.str-el"}
{"created":"2024-03-10 06:56:49","title":"The Neumann problem of special Lagrangian type equations","abstract":"We study the Neumann problem for special Lagrangian type equations with critical and supercritical phases. These equations naturally generalize the special Lagrangian equation and the k-Hessian equation. By establishing uniform a priori estimates up to the second order, we obtain the existence result using the continuity method. The new technical aspect is our direct proof of boundary double normal derivative estimates. In particular, we directly prove the double normal estimates for the 2-Hessian equation in dimension 3. Moreover, we solve the classical Neumann problem by proving the uniform gradient estimate.","sentences":["We study the Neumann problem for special Lagrangian type equations with critical and supercritical phases.","These equations naturally generalize the special Lagrangian equation and the k-Hessian equation.","By establishing uniform a priori estimates up to the second order, we obtain the existence result using the continuity method.","The new technical aspect is our direct proof of boundary double normal derivative estimates.","In particular, we directly prove the double normal estimates for the 2-Hessian equation in dimension 3.","Moreover, we solve the classical Neumann problem by proving the uniform gradient estimate."],"url":"http://arxiv.org/abs/2403.06110v1","category":"math.AP"}
{"created":"2024-03-10 06:17:23","title":"Connection Laplacian on discrete tori with converging property","abstract":"This paper presents a comprehensive analysis of the spectral properties of the connection Laplacian for both real and discrete tori. We introduce novel methods to examine these eigenvalues by employing parallel orthonormal basis in the pullback bundle on universal covering spaces. Our main results reveal that the eigenvalues of the connection Laplacian on a real torus can be expressed in terms of standard Laplacian eigenvalues, with a unique twist encapsulated in the torsion matrix. This connection is further investigated in the context of discrete tori, where we demonstrate similar results.   A significant portion of the paper is dedicated to exploring the convergence properties of a family of discrete tori towards a real torus. We extend previous findings on the spectrum of the standard Laplacian to include the connection Laplacian, revealing that the rescaled eigenvalues of discrete tori converge to those of the real torus. Furthermore, our analysis of the discrete torus occurs within a broader context, where it is not constrained to being a product of cyclic groups. Additionally, we delve into the theta functions associated with these structures, providing a detailed analysis of their behavior and convergence.   The paper culminates in a study of the regularized log-determinant of the connection Laplacian and the converging results of it. We derive formulae for both real and discrete tori, emphasizing their dependence on the spectral zeta function and theta functions.","sentences":["This paper presents a comprehensive analysis of the spectral properties of the connection Laplacian for both real and discrete tori.","We introduce novel methods to examine these eigenvalues by employing parallel orthonormal basis in the pullback bundle on universal covering spaces.","Our main results reveal that the eigenvalues of the connection Laplacian on a real torus can be expressed in terms of standard Laplacian eigenvalues, with a unique twist encapsulated in the torsion matrix.","This connection is further investigated in the context of discrete tori, where we demonstrate similar results.   ","A significant portion of the paper is dedicated to exploring the convergence properties of a family of discrete tori towards a real torus.","We extend previous findings on the spectrum of the standard Laplacian to include the connection Laplacian, revealing that the rescaled eigenvalues of discrete tori converge to those of the real torus.","Furthermore, our analysis of the discrete torus occurs within a broader context, where it is not constrained to being a product of cyclic groups.","Additionally, we delve into the theta functions associated with these structures, providing a detailed analysis of their behavior and convergence.   ","The paper culminates in a study of the regularized log-determinant of the connection Laplacian and the converging results of it.","We derive formulae for both real and discrete tori, emphasizing their dependence on the spectral zeta function and theta functions."],"url":"http://arxiv.org/abs/2403.06105v1","category":"math.SP"}
{"created":"2024-03-10 04:27:06","title":"Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?","abstract":"Neural Radiance Field (NeRF) has achieved superior performance for novel view synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a volume rendering procedure, however, when fewer known views are given (i.e., few-shot view synthesis), the model is prone to overfit the given views. To handle this issue, previous efforts have been made towards leveraging learned priors or introducing additional regularizations. In contrast, in this paper, we for the first time provide an orthogonal method from the perspective of network structure. Given the observation that trivially reducing the number of model parameters alleviates the overfitting issue, but at the cost of missing details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs (i.e., location and viewing direction) of the vanilla MLP into each layer to prevent the overfitting issue without harming detailed synthesis. To further reduce the artifacts, we propose to model colors and volume density separately and present two regularization terms. Extensive experiments on multiple datasets demonstrate that: 1) although the proposed mi-MLP is easy to implement, it is surprisingly effective as it boosts the PSNR of the baseline from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art results on a wide range of benchmarks. We will release the code upon publication.","sentences":["Neural Radiance Field (NeRF) has achieved superior performance for novel view synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a volume rendering procedure, however, when fewer known views are given (i.e., few-shot view synthesis), the model is prone to overfit the given views.","To handle this issue, previous efforts have been made towards leveraging learned priors or introducing additional regularizations.","In contrast, in this paper, we for the first time provide an orthogonal method from the perspective of network structure.","Given the observation that trivially reducing the number of model parameters alleviates the overfitting issue, but at the cost of missing details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs (i.e., location and viewing direction) of the vanilla MLP into each layer to prevent the overfitting issue without harming detailed synthesis.","To further reduce the artifacts, we propose to model colors and volume density separately and present two regularization terms.","Extensive experiments on multiple datasets demonstrate that: 1) although the proposed mi-MLP is easy to implement, it is surprisingly effective as it boosts the PSNR of the baseline from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art results on a wide range of benchmarks.","We will release the code upon publication."],"url":"http://arxiv.org/abs/2403.06092v1","category":"cs.CV"}
{"created":"2024-03-10 04:20:51","title":"Knowledge Distillation of Convolutional Neural Networks through Feature Map Transformation using Decision Trees","abstract":"The interpretation of reasoning by Deep Neural Networks (DNN) is still challenging due to their perceived black-box nature. Therefore, deploying DNNs in several real-world tasks is restricted by the lack of transparency of these models. We propose a distillation approach by extracting features from the final layer of the convolutional neural network (CNN) to address insights to its reasoning. The feature maps in the final layer of a CNN are transformed into a one-dimensional feature vector using a fully connected layer. Subsequently, the extracted features are used to train a decision tree to achieve the best accuracy under constraints of depth and nodes. We use the medical images of dermaMNIST, octMNIST, and pneumoniaMNIST from the medical MNIST datasets to demonstrate our proposed work. We observed that performance of the decision tree is as good as a CNN with minimum complexity. The results encourage interpreting decisions made by the CNNs using decision trees.","sentences":["The interpretation of reasoning by Deep Neural Networks (DNN) is still challenging due to their perceived black-box nature.","Therefore, deploying DNNs in several real-world tasks is restricted by the lack of transparency of these models.","We propose a distillation approach by extracting features from the final layer of the convolutional neural network (CNN) to address insights to its reasoning.","The feature maps in the final layer of a CNN are transformed into a one-dimensional feature vector using a fully connected layer.","Subsequently, the extracted features are used to train a decision tree to achieve the best accuracy under constraints of depth and nodes.","We use the medical images of dermaMNIST, octMNIST, and pneumoniaMNIST from the medical MNIST datasets to demonstrate our proposed work.","We observed that performance of the decision tree is as good as a CNN with minimum complexity.","The results encourage interpreting decisions made by the CNNs using decision trees."],"url":"http://arxiv.org/abs/2403.06089v1","category":"cs.CV"}
{"created":"2024-03-10 04:05:45","title":"pETNNs: Partial Evolutionary Tensor Neural Networks for Solving Time-dependent Partial Differential Equations","abstract":"We present partial evolutionary tensor neural networks (pETNNs), a novel framework for solving time-dependent partial differential equations with both of high accuracy and remarkable extrapolation. Our proposed architecture leverages the inherent accuracy of tensor neural networks, while incorporating evolutionary parameters that enable remarkable extrapolation capabilities. By adopting innovative parameter update strategies, the pETNNs achieve a significant reduction in computational cost while maintaining precision and robustness. Notably, the pETNNs enhance the accuracy of conventional evolutional deep neural networks and empowers computational abilities to address high-dimensional problems. Numerical experiments demonstrate the superior performance of the pETNNs in solving time-dependent complex equations, including the Navier-Stokes equations, high-dimensional heat equation, high-dimensional transport equation and Korteweg-de Vries type equation.","sentences":["We present partial evolutionary tensor neural networks (pETNNs), a novel framework for solving time-dependent partial differential equations with both of high accuracy and remarkable extrapolation.","Our proposed architecture leverages the inherent accuracy of tensor neural networks, while incorporating evolutionary parameters that enable remarkable extrapolation capabilities.","By adopting innovative parameter update strategies, the pETNNs achieve a significant reduction in computational cost while maintaining precision and robustness.","Notably, the pETNNs enhance the accuracy of conventional evolutional deep neural networks and empowers computational abilities to address high-dimensional problems.","Numerical experiments demonstrate the superior performance of the pETNNs in solving time-dependent complex equations, including the Navier-Stokes equations, high-dimensional heat equation, high-dimensional transport equation and Korteweg-de Vries type equation."],"url":"http://arxiv.org/abs/2403.06084v1","category":"math.NA"}
{"created":"2024-03-10 04:00:55","title":"A remark on the first eigenvalue of the p-Laplacian on compact submanifolds in the unit sphere","abstract":"An integral inequality for the singular p-laplacian is established for 3/2<p<2. As consequence, lower bounds for the first eigenvalue of the p-laplacian are obtained for minimal submanifolds and prescribed scalar curvature submanifolds in the unit sphere.","sentences":["An integral inequality for the singular p-laplacian is established for 3/2<p<2.","As consequence, lower bounds for the first eigenvalue of the p-laplacian are obtained for minimal submanifolds and prescribed scalar curvature submanifolds in the unit sphere."],"url":"http://arxiv.org/abs/2403.06081v1","category":"math.DG"}
{"created":"2024-03-10 03:59:24","title":"Local Vertex Colouring Graph Neural Networks","abstract":"In recent years, there has been a significant amount of research focused on expanding the expressivity of Graph Neural Networks (GNNs) beyond the Weisfeiler-Lehman (1-WL) framework. While many of these studies have yielded advancements in expressivity, they have frequently come at the expense of decreased efficiency or have been restricted to specific types of graphs. In this study, we investigate the expressivity of GNNs from the perspective of graph search. Specifically, we propose a new vertex colouring scheme and demonstrate that classical search algorithms can efficiently compute graph representations that extend beyond the 1-WL. We show the colouring scheme inherits useful properties from graph search that can help solve problems like graph biconnectivity. Furthermore, we show that under certain conditions, the expressivity of GNNs increases hierarchically with the radius of the search neighbourhood. To further investigate the proposed scheme, we develop a new type of GNN based on two search strategies, breadth-first search and depth-first search, highlighting the graph properties they can capture on top of 1-WL. Our code is available at https://github.com/seanli3/lvc.","sentences":["In recent years, there has been a significant amount of research focused on expanding the expressivity of Graph Neural Networks (GNNs) beyond the Weisfeiler-Lehman (1-WL) framework.","While many of these studies have yielded advancements in expressivity, they have frequently come at the expense of decreased efficiency or have been restricted to specific types of graphs.","In this study, we investigate the expressivity of GNNs from the perspective of graph search.","Specifically, we propose a new vertex colouring scheme and demonstrate that classical search algorithms can efficiently compute graph representations that extend beyond the 1-WL.","We show the colouring scheme inherits useful properties from graph search that can help solve problems like graph biconnectivity.","Furthermore, we show that under certain conditions, the expressivity of GNNs increases hierarchically with the radius of the search neighbourhood.","To further investigate the proposed scheme, we develop a new type of GNN based on two search strategies, breadth-first search and depth-first search, highlighting the graph properties they can capture on top of 1-WL.","Our code is available at https://github.com/seanli3/lvc."],"url":"http://arxiv.org/abs/2403.06080v1","category":"cs.LG"}
{"created":"2024-03-10 03:51:59","title":"Generalization of Graph Neural Networks through the Lens of Homomorphism","abstract":"Despite the celebrated popularity of Graph Neural Networks (GNNs) across numerous applications, the ability of GNNs to generalize remains less explored. In this work, we propose to study the generalization of GNNs through a novel perspective - analyzing the entropy of graph homomorphism. By linking graph homomorphism with information-theoretic measures, we derive generalization bounds for both graph and node classifications. These bounds are capable of capturing subtleties inherent in various graph structures, including but not limited to paths, cycles and cliques. This enables a data-dependent generalization analysis with robust theoretical guarantees. To shed light on the generality of of our proposed bounds, we present a unifying framework that can characterize a broad spectrum of GNN models through the lens of graph homomorphism. We validate the practical applicability of our theoretical findings by showing the alignment between the proposed bounds and the empirically observed generalization gaps over both real-world and synthetic datasets.","sentences":["Despite the celebrated popularity of Graph Neural Networks (GNNs) across numerous applications, the ability of GNNs to generalize remains less explored.","In this work, we propose to study the generalization of GNNs through a novel perspective - analyzing the entropy of graph homomorphism.","By linking graph homomorphism with information-theoretic measures, we derive generalization bounds for both graph and node classifications.","These bounds are capable of capturing subtleties inherent in various graph structures, including but not limited to paths, cycles and cliques.","This enables a data-dependent generalization analysis with robust theoretical guarantees.","To shed light on the generality of of our proposed bounds, we present a unifying framework that can characterize a broad spectrum of GNN models through the lens of graph homomorphism.","We validate the practical applicability of our theoretical findings by showing the alignment between the proposed bounds and the empirically observed generalization gaps over both real-world and synthetic datasets."],"url":"http://arxiv.org/abs/2403.06079v1","category":"cs.LG"}
{"created":"2024-03-10 00:50:32","title":"Direct linearization of the SU(2) anti-self-dual Yang-Mills equation in various spaces","abstract":"The paper establishes a direct linearization scheme for the SU(2) anti-self-dual Yang-Mills (ASDYM) equation.The scheme starts from a set of linear integral equations with general measures and plane wave factors. After introducing infinite-dimensional matrices as master functions, we are able to investigate evolution relations and recurrence relations of these functions, which lead us to the unreduced ASDYM equation. It is then reduced to the ASDYM equation in the Euclidean space and two ultrahyperbolic spaces by reductions to meet the reality conditions and gauge conditions, respectively. Special solutions can be obtained by choosing suitable measures.","sentences":["The paper establishes a direct linearization scheme for the SU(2) anti-self-dual Yang-Mills (ASDYM) equation.","The scheme starts from a set of linear integral equations with general measures and plane wave factors.","After introducing infinite-dimensional matrices as master functions, we are able to investigate evolution relations and recurrence relations of these functions, which lead us to the unreduced ASDYM equation.","It is then reduced to the ASDYM equation in the Euclidean space and two ultrahyperbolic spaces by reductions to meet the reality conditions and gauge conditions, respectively.","Special solutions can be obtained by choosing suitable measures."],"url":"http://arxiv.org/abs/2403.06055v1","category":"nlin.SI"}
{"created":"2024-03-10 00:44:19","title":"Accessing the speed of sound in relativistic ultracentral nucleus-nucleus collisions using the mean transverse momentum","abstract":"It has been argued that the speed of sound of the strong interaction at high temperature can be measured using the variation of the mean transverse momentum with the particle multiplicity in ultracentral heavy-ion collisions. We test this correspondence by running hydrodynamic simulations at zero impact parameter with several equations of state, at several colliding energies from 0.2 TeV to 15 TeV per nucleon pair. The correspondence is found to be precise and robust for a smooth, boost-invariant fluid and an ideal detector. We discuss the differences between this ideal setup and an actual experiment. We conclude that the extraction of the speed of sound from data is reliable, and that the main uncertainty comes from our poor knowledge of the distribution of density fluctuations at the early stages of the collision.","sentences":["It has been argued that the speed of sound of the strong interaction at high temperature can be measured using the variation of the mean transverse momentum with the particle multiplicity in ultracentral heavy-ion collisions.","We test this correspondence by running hydrodynamic simulations at zero impact parameter with several equations of state, at several colliding energies from 0.2 TeV to 15 TeV per nucleon pair.","The correspondence is found to be precise and robust for a smooth, boost-invariant fluid and an ideal detector.","We discuss the differences between this ideal setup and an actual experiment.","We conclude that the extraction of the speed of sound from data is reliable, and that the main uncertainty comes from our poor knowledge of the distribution of density fluctuations at the early stages of the collision."],"url":"http://arxiv.org/abs/2403.06052v1","category":"nucl-th"}
{"created":"2024-03-10 00:12:43","title":"X-ray and molecular dynamics study of the temperature-dependent structure of molten NaF-ZrF4","abstract":"The local atomic structure of NaF-ZrF$_4$ (53-47 mol%) molten system and its evolution with temperature are examined with x-ray scattering measurements and compared with $ab-initio$ and Neural Network-based molecular dynamics (NNMD) simulations in the temperature range 515-700 {\\deg}C. The machine-learning enhanced NNMD calculations offer improved efficiency while maintaining accuracy at higher distances compared to ab-initio calculations. Looking at the evolution of the Pair Distribution Function with increasing temperature, a fundamental change in the liquid structure within the selected temperature range, accompanied by a slight decrease in overall correlation is revealed. NNMD calculations indicate the co-existence of three different fluorozirconate complexes: [ZrF$_6$]$^{2-}$, [ZrF$_7$]$^{3-}$, and [ZrF$_8$]$^{4-}$, with a temperature-dependent shift in the dominant coordination state towards a 6-coordinated Zr ion at 700{\\deg}C. The study also highlights the metastability of different coordination structures, with frequent interconversions between 6 and 7 coordinate states for the fluorozirconate complex from 525 {\\deg}C to 700 {\\deg}C. Analysis of the Zr-F-Zr angular distribution function reveals the presence of both $\"$edge-sharing$\"$ and $\"$corner-sharing$\"$ fluorozirconate complexes with specific bond angles and distances in accord with previous studies, while the next-nearest neighbor cation-cation correlations demonstrate a clear preference for unlike cations as nearest-neighbor pairs, emphasizing non-random arrangement. These findings contribute to a comprehensive understanding of the complex local structure of the molten salt, providing insights into temperature-dependent preferences and correlations within the molten system.","sentences":["The local atomic structure of NaF-ZrF$_4$ (53-47 mol%) molten system and its evolution with temperature are examined with x-ray scattering measurements and compared with $ab-initio$ and Neural Network-based molecular dynamics (NNMD) simulations in the temperature range 515-700 {\\deg}C.","The machine-learning enhanced NNMD calculations offer improved efficiency while maintaining accuracy at higher distances compared to ab-initio calculations.","Looking at the evolution of the Pair Distribution Function with increasing temperature, a fundamental change in the liquid structure within the selected temperature range, accompanied by a slight decrease in overall correlation is revealed.","NNMD calculations indicate the co-existence of three different fluorozirconate complexes:","[ZrF$_6$]$^{2-}$, [ZrF$_7$]$^{3-}$, and [ZrF$_8$]$^{4-}$, with a temperature-dependent shift in the dominant coordination state towards a 6-coordinated Zr ion at 700{\\deg}C. The study also highlights the metastability of different coordination structures, with frequent interconversions between 6 and 7 coordinate states for the fluorozirconate complex from 525 {\\deg}C to 700 {\\deg}C. Analysis of the Zr-F-Zr angular distribution function reveals the presence of both $\"$edge-sharing$\"$ and $\"$corner-sharing$\"$ fluorozirconate complexes with specific bond angles and distances in accord with previous studies, while the next-nearest neighbor cation-cation correlations demonstrate a clear preference for unlike cations as nearest-neighbor pairs, emphasizing non-random arrangement.","These findings contribute to a comprehensive understanding of the complex local structure of the molten salt, providing insights into temperature-dependent preferences and correlations within the molten system."],"url":"http://arxiv.org/abs/2403.06049v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-09 22:34:15","title":"Estimates of the Kolmogorov n-width for nonlinear transformations with application to distributed-parameter control systems","abstract":"This paper aims at characterizing the approximability of bounded sets in the range of nonlinear operators in Banach spaces by finite-dimensional linear varieties. In particular, the class of operators we consider includes the endpoint maps of nonlinear distributed-parameter control systems. The concept of Kolmogorov widths is developed to describe the relation between the n-width of a bounded subset and the width of its image by applying essentially nonlinear transformation. We propose explicit estimates of the n-width in the space of images in terms of the affine part of the corresponding operator and the width of its nonlinear perturbation. These n-width estimates enable us to describe the reachable sets for infinite-dimensional bilinear control systems, with applications to controlling the Euler-Bernoulli beam using a contraction force and to a single-input Schr\\\"odinger equation.","sentences":["This paper aims at characterizing the approximability of bounded sets in the range of nonlinear operators in Banach spaces by finite-dimensional linear varieties.","In particular, the class of operators we consider includes the endpoint maps of nonlinear distributed-parameter control systems.","The concept of Kolmogorov widths is developed to describe the relation between the n-width of a bounded subset and the width of its image by applying essentially nonlinear transformation.","We propose explicit estimates of the n-width in the space of images in terms of the affine part of the corresponding operator and the width of its nonlinear perturbation.","These n-width estimates enable us to describe the reachable sets for infinite-dimensional bilinear control systems, with applications to controlling the Euler-Bernoulli beam using a contraction force and to a single-input Schr\\\"odinger equation."],"url":"http://arxiv.org/abs/2403.06029v1","category":"math.OC"}
{"created":"2024-03-09 21:45:31","title":"Multi-conditioned Graph Diffusion for Neural Architecture Search","abstract":"Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space. To advance the architecture search, we present a graph diffusion-based NAS approach that uses discrete conditional graph diffusion processes to generate high-performing neural network architectures. We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency. Unlike the related work, our method is completely differentiable and requires only a single model training. In our evaluations, we show promising results on six standard benchmarks, yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset.","sentences":["Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space.","To advance the architecture search, we present a graph diffusion-based NAS approach that uses discrete conditional graph diffusion processes to generate high-performing neural network architectures.","We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency.","Unlike the related work, our method is completely differentiable and requires only a single model training.","In our evaluations, we show promising results on six standard benchmarks, yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture.","Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset."],"url":"http://arxiv.org/abs/2403.06020v1","category":"cs.LG"}
{"created":"2024-03-09 19:04:14","title":"The Nature of Dark Energy and Constraints on Its Hypothetical Constituents from Force Measurements","abstract":"This review considers the theoretical approaches to the understanding of dark energy which comprises approximately 68\\% of the energy of our Universe and explains an acceleration in its expansion. Following a discussion of the main approach based on Einstein's equations with the cosmological term, the explanations of dark energy using the concept of some kind of scalar field are elucidated. These include the concept of a quintessence and modifications of the general theory of relativity by means of the scalar-tensor gravity exploiting the chameleon, symmetron, and environment-dependent dilaton fields and corresponding particles. After mentioning several laboratory experiments allowing to constrain the hypothetical scalar fields modeling the dark energy, special attention is devoted to the possibility of constraining the parameters of chameleon, symmetron, and environment-dependent dilaton fields from measuring the Casimir force. It is concluded that the parameters of each of these fields can be significantly strengthened in near future by using the next generation setups in preparation suitable for measuring the Casimir force at larger separations.","sentences":["This review considers the theoretical approaches to the understanding of dark energy which comprises approximately 68\\% of the energy of our Universe and explains an acceleration in its expansion.","Following a discussion of the main approach based on Einstein's equations with the cosmological term, the explanations of dark energy using the concept of some kind of scalar field are elucidated.","These include the concept of a quintessence and modifications of the general theory of relativity by means of the scalar-tensor gravity exploiting the chameleon, symmetron, and environment-dependent dilaton fields and corresponding particles.","After mentioning several laboratory experiments allowing to constrain the hypothetical scalar fields modeling the dark energy, special attention is devoted to the possibility of constraining the parameters of chameleon, symmetron, and environment-dependent dilaton fields from measuring the Casimir force.","It is concluded that the parameters of each of these fields can be significantly strengthened in near future by using the next generation setups in preparation suitable for measuring the Casimir force at larger separations."],"url":"http://arxiv.org/abs/2403.05988v1","category":"gr-qc"}
{"created":"2024-03-09 18:59:22","title":"Local and global blow-downs of transport twistor space","abstract":"Transport twistor spaces are degenerate complex $2$-dimensional manifolds $Z$ that complexify transport problems on Riemannian surfaces, appearing e.g.~in geometric inverse problems. This article considers maps $\\beta\\colon Z\\to \\mathbb{C}^2$ with a holomorphic blow-down structure that resolve the degeneracy of the complex structure and allow to gain insight into the complex geometry of $Z$. The main theorems provide global $\\beta$-maps for constant curvature metrics and their perturbations and local $\\beta$-maps for arbitrary metrics, thereby proving a version of the classical Newlander--Nirenberg theorem for degenerate complex structures.","sentences":["Transport twistor spaces are degenerate complex $2$-dimensional manifolds $Z$ that complexify transport problems on Riemannian surfaces, appearing e.g.~in geometric inverse problems.","This article considers maps $\\beta\\colon Z\\to \\mathbb{C}^2$ with a holomorphic blow-down structure that resolve the degeneracy of the complex structure and allow to gain insight into the complex geometry of $Z$. The main theorems provide global $\\beta$-maps for constant curvature metrics and their perturbations and local $\\beta$-maps for arbitrary metrics, thereby proving a version of the classical Newlander--Nirenberg theorem for degenerate complex structures."],"url":"http://arxiv.org/abs/2403.05985v1","category":"math.DG"}
{"created":"2024-03-09 18:43:48","title":"Enhanced Auto Language Prediction with Dictionary Capsule -- A Novel Approach","abstract":"The paper presents a novel Auto Language Prediction Dictionary Capsule (ALPDC) framework for language prediction and machine translation. The model uses a combination of neural networks and symbolic representations to predict the language of a given input text and then translate it to a target language using pre-built dictionaries. This research work also aims to translate the text of various languages to its literal meaning in English. The proposed model achieves state-of-the-art results on several benchmark datasets and significantly improves translation accuracy compared to existing methods. The results show the potential of the proposed method for practical use in multilingual communication and natural language processing tasks.","sentences":["The paper presents a novel Auto Language Prediction Dictionary Capsule (ALPDC) framework for language prediction and machine translation.","The model uses a combination of neural networks and symbolic representations to predict the language of a given input text and then translate it to a target language using pre-built dictionaries.","This research work also aims to translate the text of various languages to its literal meaning in English.","The proposed model achieves state-of-the-art results on several benchmark datasets and significantly improves translation accuracy compared to existing methods.","The results show the potential of the proposed method for practical use in multilingual communication and natural language processing tasks."],"url":"http://arxiv.org/abs/2403.05982v1","category":"cs.CL"}
{"created":"2024-03-09 18:40:24","title":"A Path Integral for Chord Diagrams and Chaotic-Integrable Transitions in Double Scaled SYK","abstract":"We study transitions from chaotic to integrable Hamiltonians in the double scaled SYK and $p$-spin systems. The dynamics of our models is described by chord diagrams with two species. We begin by developing a path integral formalism of coarse graining chord diagrams with a single species of chords, which has the same equations of motion as the bi-local ($G\\Sigma$) Liouville action, yet appears otherwise to be different and in particular well defined. We then develop a similar formalism for two types of chords, allowing us to study different types of deformations of double scaled SYK and in particular a deformation by an integrable Hamiltonian. The system has two distinct thermodynamic phases: one is continuously connected to the chaotic SYK Hamiltonian, the other is continuously connected to the integrable Hamiltonian, separated at low temperature by a first order phase transition. We also analyze the phase diagram for generic deformations, which in some cases includes a zero-temperature phase transition.","sentences":["We study transitions from chaotic to integrable Hamiltonians in the double scaled SYK and $p$-spin systems.","The dynamics of our models is described by chord diagrams with two species.","We begin by developing a path integral formalism of coarse graining chord diagrams with a single species of chords, which has the same equations of motion as the bi-local ($G\\Sigma$) Liouville action, yet appears otherwise to be different and in particular well defined.","We then develop a similar formalism for two types of chords, allowing us to study different types of deformations of double scaled SYK and in particular a deformation by an integrable Hamiltonian.","The system has two distinct thermodynamic phases: one is continuously connected to the chaotic SYK Hamiltonian, the other is continuously connected to the integrable Hamiltonian, separated at low temperature by a first order phase transition.","We also analyze the phase diagram for generic deformations, which in some cases includes a zero-temperature phase transition."],"url":"http://arxiv.org/abs/2403.05980v1","category":"hep-th"}
{"created":"2024-03-09 16:58:59","title":"Unitary Friedberg--Jacquet periods and anticyclotomic p-adic L-functions","abstract":"We extend the construction of the $p$-adic $L$-function interpolating unitary Friedberg--Jacquet periods in arXiv:2110.05426 to include the $p$-adic variation of Maass--Shimura differential operators. In particular, we develop a theory of nearly overconvergent automorphic forms in higher degrees of coherent cohomology for unitary Shimura varieties generalising the results in arXiv:2311.14438. The construction of this $p$-adic $L$-function can be viewed as a higher-dimensional generalisation of the work of Bertolini--Darmon--Prasanna and Castella--Hsieh, and the inclusion of this extra variable arising from the $p$-adic iteration of differential operators will play a key role in relating values of this $p$-adic $L$-function to $p$-adic regulators of special cycles on unitary Shimura varieties.","sentences":["We extend the construction of the $p$-adic $L$-function interpolating unitary Friedberg--Jacquet periods in arXiv:2110.05426 to include the $p$-adic variation of Maass--Shimura differential operators.","In particular, we develop a theory of nearly overconvergent automorphic forms in higher degrees of coherent cohomology for unitary Shimura varieties generalising the results in arXiv:2311.14438.","The construction of this $p$-adic $L$-function can be viewed as a higher-dimensional generalisation of the work of Bertolini--Darmon--Prasanna and Castella--Hsieh, and the inclusion of this extra variable arising from the $p$-adic iteration of differential operators will play a key role in relating values of this $p$-adic $L$-function to $p$-adic regulators of special cycles on unitary Shimura varieties."],"url":"http://arxiv.org/abs/2403.05960v1","category":"math.NT"}
{"created":"2024-03-09 16:22:00","title":"Discrete Boltzmann model with split collision for combustion with nonequilibrium effects","abstract":"A multi-relaxation-time discrete Boltzmann model (DBM) with split collision is proposed for both subsonic and supersonic compressible combustion where the chemical reaction takes place among various components. The physical model is based on a unified set of discrete Boltzmann equations which describes the evolution of each chemical species with an adjustable acceleration, specific heat ratio and Prandtl number. On the righ-hand side of discrete Boltzmann equations, the collision, force, and reaction terms denote the change rates of distribution functions due to self- and cross-collisions, external forces, and chemical reactions. The source terms can be calculated in three ways among which the matrix inversion method possesses the highest physical accuracy and computational efficiency. Via the Chapman-Enskog analysis, it is proved that the DBM is consistent with the reactive Navier-Stokes equations with external forces, Fick's law and Stefan-Maxwell diffusion equation in the hydrodynamic limit. Compared with the one-step-relaxation model, the hydrodynamic, thermodynamic, and chemical nonequilibrium effects can be captured and measured more detailedly and precisely by the split collision model. Finally, the model is validated by six benchmarks, i.e., the multicomponent diffusion, Kelvin-Helmholtz instability, mixture in the force field, flame at constant pressure, opposing chemical reaction, and steady detonation.","sentences":["A multi-relaxation-time discrete Boltzmann model (DBM) with split collision is proposed for both subsonic and supersonic compressible combustion where the chemical reaction takes place among various components.","The physical model is based on a unified set of discrete Boltzmann equations which describes the evolution of each chemical species with an adjustable acceleration, specific heat ratio and Prandtl number.","On the righ-hand side of discrete Boltzmann equations, the collision, force, and reaction terms denote the change rates of distribution functions due to self- and cross-collisions, external forces, and chemical reactions.","The source terms can be calculated in three ways among which the matrix inversion method possesses the highest physical accuracy and computational efficiency.","Via the Chapman-Enskog analysis, it is proved that the DBM is consistent with the reactive Navier-Stokes equations with external forces, Fick's law and Stefan-Maxwell diffusion equation in the hydrodynamic limit.","Compared with the one-step-relaxation model, the hydrodynamic, thermodynamic, and chemical nonequilibrium effects can be captured and measured more detailedly and precisely by the split collision model.","Finally, the model is validated by six benchmarks, i.e., the multicomponent diffusion, Kelvin-Helmholtz instability, mixture in the force field, flame at constant pressure, opposing chemical reaction, and steady detonation."],"url":"http://arxiv.org/abs/2403.05953v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-09 15:55:59","title":"Discrete and Continuum Area-Preserving Mean-Curvature Flow of Rectangles","abstract":"We investigate the area-preserving mean-curvature-type motion of a two-dimensional lattice crystal obtained by coupling constrained minimizing movements scheme introduced by Almgren, Taylor and Wang with a discrete-to-continuous analysis. We first examine the continuum counterpart of the model and establish the existence and uniqueness of the flat flow, originating from a rectangle. Additionally, we characterize the governing system of ordinary differential equations. Subsequently, in the atomistic setting, we identify geometric properties of the discrete-in-time flow and describe the governing system of finite-difference inclusions. Finally, in the limit where both spatial and time scales vanish at the same rate, we prove that a discrete-to-continuum evolution is expressed through a system of differential inclusions which does never reduce to a system of ODEs.","sentences":["We investigate the area-preserving mean-curvature-type motion of a two-dimensional lattice crystal obtained by coupling constrained minimizing movements scheme introduced by Almgren, Taylor and Wang with a discrete-to-continuous analysis.","We first examine the continuum counterpart of the model and establish the existence and uniqueness of the flat flow, originating from a rectangle.","Additionally, we characterize the governing system of ordinary differential equations.","Subsequently, in the atomistic setting, we identify geometric properties of the discrete-in-time flow and describe the governing system of finite-difference inclusions.","Finally, in the limit where both spatial and time scales vanish at the same rate, we prove that a discrete-to-continuum evolution is expressed through a system of differential inclusions which does never reduce to a system of ODEs."],"url":"http://arxiv.org/abs/2403.05947v1","category":"math.AP"}
{"created":"2024-03-09 15:13:49","title":"Wavelet-Like Transform-Based Technology in Response to the Call for Proposals on Neural Network-Based Image Coding","abstract":"Neural network-based image coding has been developing rapidly since its birth. Until 2022, its performance has surpassed that of the best-performing traditional image coding framework -- H.266/VVC. Witnessing such success, the IEEE 1857.11 working subgroup initializes a neural network-based image coding standard project and issues a corresponding call for proposals (CfP). In response to the CfP, this paper introduces a novel wavelet-like transform-based end-to-end image coding framework -- iWaveV3. iWaveV3 incorporates many new features such as affine wavelet-like transform, perceptual-friendly quality metric, and more advanced training and online optimization strategies into our previous wavelet-like transform-based framework iWave++. While preserving the features of supporting lossy and lossless compression simultaneously, iWaveV3 also achieves state-of-the-art compression efficiency for objective quality and is very competitive for perceptual quality. As a result, iWaveV3 is adopted as a candidate scheme for developing the IEEE Standard for neural-network-based image coding.","sentences":["Neural network-based image coding has been developing rapidly since its birth.","Until 2022, its performance has surpassed that of the best-performing traditional image coding framework -- H.266/VVC.","Witnessing such success, the IEEE 1857.11 working subgroup initializes a neural network-based image coding standard project and issues a corresponding call for proposals (CfP).","In response to the CfP, this paper introduces a novel wavelet-like transform-based end-to-end image coding framework -- iWaveV3.","iWaveV3 incorporates many new features such as affine wavelet-like transform, perceptual-friendly quality metric, and more advanced training and online optimization strategies into our previous wavelet-like transform-based framework iWave++.","While preserving the features of supporting lossy and lossless compression simultaneously, iWaveV3 also achieves state-of-the-art compression efficiency for objective quality and is very competitive for perceptual quality.","As a result, iWaveV3 is adopted as a candidate scheme for developing the IEEE Standard for neural-network-based image coding."],"url":"http://arxiv.org/abs/2403.05937v1","category":"cs.CV"}
{"created":"2024-03-09 15:01:56","title":"On the first eigenvalue of the generalized laplacian","abstract":"In this work we investigate the energy of minimizers of Rayleigh-type quotients of the form $$ \\frac{\\int_\\Omega A(|\\nabla u|)\\, dx}{\\int_\\Omega A(|u|)\\, dx}. $$   These minimizers are eigenfunctions of the generalized laplacian defined as $\\Delta_a u = \\text{div}\\left(a(|\\nabla u|)\\frac{\\nabla u}{|\\nabla u|}\\right)$ where $a(t)=A'(t)$ and the Rayleigh quotient is comparable to the associated eigenvalue. On the function $A$ we only assume that it is a Young function but no $\\Delta_2$ condition is imposed.   Since the problem is not homogeneous, the energy of minimizers is known to strongly depend on the normalization parameter $\\alpha =\\int_\\Omega A(|u|)\\, dx$. In this work we precisely analyze this dependence and show differentiability of the energy with respect to $\\alpha$ and, moreover, the limits as $\\alpha\\to 0$ and $\\alpha\\to \\infty$ of the Rayleigh quotient.   The nonlocal version of this problem is also analyzed.","sentences":["In this work we investigate the energy of minimizers of Rayleigh-type quotients of the form $$ \\frac{\\int_\\Omega A(|\\nabla u|)\\, dx}{\\int_\\Omega A(|u|)\\, dx}.","$$   These minimizers are eigenfunctions of the generalized laplacian defined as $\\Delta_a u = \\text{div}\\left(a(|\\nabla u|)\\frac{\\nabla u}{|\\nabla u|}\\right)$ where $a(t)=A'(t)$ and the Rayleigh quotient is comparable to the associated eigenvalue.","On the function $A$ we only assume that it is a Young function but no $\\Delta_2$ condition is imposed.   ","Since the problem is not homogeneous, the energy of minimizers is known to strongly depend on the normalization parameter $\\alpha =\\int_\\Omega A(|u|)\\, dx$. In this work we precisely analyze this dependence and show differentiability of the energy with respect to $\\alpha$ and, moreover, the limits as $\\alpha\\to 0$ and $\\alpha\\to \\infty$ of the Rayleigh quotient.   ","The nonlocal version of this problem is also analyzed."],"url":"http://arxiv.org/abs/2403.05933v1","category":"math.AP"}
{"created":"2024-03-09 14:17:30","title":"Global solutions for stochastically controlled fluid dynamics models","abstract":"For a class of evolution equations that possibly have only local solutions, we introduce a stochastic component that ensures that the solutions of the corresponding stochastically perturbed equations are global. The class of partial differential equations amenable for this type of treatment includes the 3D Navier-Stokes equation, the rotating shallow water equation (viscous and inviscid), 3D Euler equation (in vorticity form), 2D Burgers' equation and many other fluid dynamics models.","sentences":["For a class of evolution equations that possibly have only local solutions, we introduce a stochastic component that ensures that the solutions of the corresponding stochastically perturbed equations are global.","The class of partial differential equations amenable for this type of treatment includes the 3D Navier-Stokes equation, the rotating shallow water equation (viscous and inviscid), 3D Euler equation (in vorticity form), 2D Burgers' equation and many other fluid dynamics models."],"url":"http://arxiv.org/abs/2403.05923v1","category":"math.AP"}
{"created":"2024-03-09 13:24:55","title":"An in-silico approach to meniscus tissue regeneration: Modeling, numerical simulation, and experimental analysis","abstract":"We develop a model the dynamics of human mesenchymal stem cells (hMSCs) and chondrocytes evolving in a nonwoven polyethylene terephtalate (PET) scaffold impregnated with hyaluron and supplied with a differentiation medium. The scaffold and the cells are assumed to be contained in a bioreactor with fluid perfusion. The differentiation of hMSCs into chondrocytes favors the production of extracellular matrix (ECM) and is influenced by fluid stress. The model takes deformations of ECM and PET scaffold into account. The scaffold structure is explicitly included by statistical assessment of the fibre distribution from CT images. The effective macroscopic equations are obtained by appropriate upscaling from dynamics on lower (microscopic and mesoscopic) scales and feature in the motility terms an explicit cell diffusion tensor encoding the assessed anisotropic scaffold structure. Numerical simulations show its influence on the overall cell and tissue dynamics.","sentences":["We develop a model the dynamics of human mesenchymal stem cells (hMSCs) and chondrocytes evolving in a nonwoven polyethylene terephtalate (PET) scaffold impregnated with hyaluron and supplied with a differentiation medium.","The scaffold and the cells are assumed to be contained in a bioreactor with fluid perfusion.","The differentiation of hMSCs into chondrocytes favors the production of extracellular matrix (ECM) and is influenced by fluid stress.","The model takes deformations of ECM and PET scaffold into account.","The scaffold structure is explicitly included by statistical assessment of the fibre distribution from CT images.","The effective macroscopic equations are obtained by appropriate upscaling from dynamics on lower (microscopic and mesoscopic) scales and feature in the motility terms an explicit cell diffusion tensor encoding the assessed anisotropic scaffold structure.","Numerical simulations show its influence on the overall cell and tissue dynamics."],"url":"http://arxiv.org/abs/2403.05909v1","category":"q-bio.TO"}
{"created":"2024-03-09 13:11:59","title":"Segmentation Guided Sparse Transformer for Under-Display Camera Image Restoration","abstract":"Under-Display Camera (UDC) is an emerging technology that achieves full-screen display via hiding the camera under the display panel. However, the current implementation of UDC causes serious degradation. The incident light required for camera imaging undergoes attenuation and diffraction when passing through the display panel, leading to various artifacts in UDC imaging. Presently, the prevailing UDC image restoration methods predominantly utilize convolutional neural network architectures, whereas Transformer-based methods have exhibited superior performance in the majority of image restoration tasks. This is attributed to the Transformer's capability to sample global features for the local reconstruction of images, thereby achieving high-quality image restoration. In this paper, we observe that when using the Vision Transformer for UDC degraded image restoration, the global attention samples a large amount of redundant information and noise. Furthermore, compared to the ordinary Transformer employing dense attention, the Transformer utilizing sparse attention can alleviate the adverse impact of redundant information and noise. Building upon this discovery, we propose a Segmentation Guided Sparse Transformer method (SGSFormer) for the task of restoring high-quality images from UDC degraded images. Specifically, we utilize sparse self-attention to filter out redundant information and noise, directing the model's attention to focus on the features more relevant to the degraded regions in need of reconstruction. Moreover, we integrate the instance segmentation map as prior information to guide the sparse self-attention in filtering and focusing on the correct regions.","sentences":["Under-Display Camera (UDC) is an emerging technology that achieves full-screen display via hiding the camera under the display panel.","However, the current implementation of UDC causes serious degradation.","The incident light required for camera imaging undergoes attenuation and diffraction when passing through the display panel, leading to various artifacts in UDC imaging.","Presently, the prevailing UDC image restoration methods predominantly utilize convolutional neural network architectures, whereas Transformer-based methods have exhibited superior performance in the majority of image restoration tasks.","This is attributed to the Transformer's capability to sample global features for the local reconstruction of images, thereby achieving high-quality image restoration.","In this paper, we observe that when using the Vision Transformer for UDC degraded image restoration, the global attention samples a large amount of redundant information and noise.","Furthermore, compared to the ordinary Transformer employing dense attention, the Transformer utilizing sparse attention can alleviate the adverse impact of redundant information and noise.","Building upon this discovery, we propose a Segmentation Guided Sparse Transformer method (SGSFormer) for the task of restoring high-quality images from UDC degraded images.","Specifically, we utilize sparse self-attention to filter out redundant information and noise, directing the model's attention to focus on the features more relevant to the degraded regions in need of reconstruction.","Moreover, we integrate the instance segmentation map as prior information to guide the sparse self-attention in filtering and focusing on the correct regions."],"url":"http://arxiv.org/abs/2403.05906v1","category":"eess.IV"}
{"created":"2024-03-09 12:53:21","title":"CFD analysis of the influence of solvent viscosity ratio on the creeping flow of viscoelastic fluid over a channel-confined circular cylinder","abstract":"In this study, the role of solvent viscosity ratio ($\\beta$) on the creeping flow characteristics of Oldroyd-B fluid over a channel-confined circular cylinder has been explored numerically. The hydrodynamic model equations have been solved by RheoTool, an open-source toolbox based on OpenFOAM, employing the finite volume method for extensive ranges of Deborah number ($De = 0.025-1.5$) and solvent viscosity ratio ($\\beta = 0.1-0.9$) for the fixed wall blockage ($B = 0.5$). The present investigation has undergone extensive validation, with available literature under specific limited conditions, before obtaining detailed results for the relevant flow phenomena such as streamline, pressure and stress contour profiles, pressure coefficient ($C_p$), wall shear stress ($\\tau_w$), normal stress ($\\tau_{xx}$), first normal stress difference ($N_{1}$), and drag coefficient ($C_{\\text{D}}$).The flow profiles have exhibited a distinctive behavior characterized by a loss of symmetry in the presence of pronounced viscoelastic and polymeric effects. The results for low $De$ notably align closely with those for Newtonian fluids, and the drag coefficient ($C_D$) remains relatively constant regardless of $\\beta$, as the viscoelastic influence is somewhat subdued. As $De$ increases, the influence of viscoelasticity becomes more pronounced, while a decrease in $\\beta$ leads to an escalation in polymeric effects; an increase in the $C_D$ value is observed as $\\beta$ increases. Within this parameter range, the prevailing force governing the flow is the pressure drag force.","sentences":["In this study, the role of solvent viscosity ratio ($\\beta$) on the creeping flow characteristics of Oldroyd-B fluid over a channel-confined circular cylinder has been explored numerically.","The hydrodynamic model equations have been solved by RheoTool, an open-source toolbox based on OpenFOAM, employing the finite volume method for extensive ranges of Deborah number ($De = 0.025-1.5$) and solvent viscosity ratio ($\\beta = 0.1-0.9$) for the fixed wall blockage ($B = 0.5$).","The present investigation has undergone extensive validation, with available literature under specific limited conditions, before obtaining detailed results for the relevant flow phenomena such as streamline, pressure and stress contour profiles, pressure coefficient ($C_p$), wall shear stress ($\\tau_w$), normal stress ($\\tau_{xx}$), first normal stress difference ($N_{1}$), and drag coefficient ($C_{\\text{D}}$).The flow profiles have exhibited a distinctive behavior characterized by a loss of symmetry in the presence of pronounced viscoelastic and polymeric effects.","The results for low $De$ notably align closely with those for Newtonian fluids, and the drag coefficient ($C_D$) remains relatively constant regardless of $\\beta$, as the viscoelastic influence is somewhat subdued.","As $De$ increases, the influence of viscoelasticity becomes more pronounced, while a decrease in $\\beta$ leads to an escalation in polymeric effects; an increase in the $C_D$ value is observed as $\\beta$ increases.","Within this parameter range, the prevailing force governing the flow is the pressure drag force."],"url":"http://arxiv.org/abs/2403.05904v1","category":"physics.flu-dyn"}
{"created":"2024-03-09 12:37:45","title":"The exponential trapezoidal method for semilinear integro-differential equations","abstract":"The exponential trapezoidal rule is proposed and analyzed for the numerical integration of semilinear integro-differential equations. Although the method is implicit, the numerical solution is easily obtained by standard fixed-point iteration, making its implementation straightforward. Second-order convergence in time is shown in an abstract Hilbert space framework under reasonable assumptions on the problem. Numerical experiments illustrate the proven order of convergence.","sentences":["The exponential trapezoidal rule is proposed and analyzed for the numerical integration of semilinear integro-differential equations.","Although the method is implicit, the numerical solution is easily obtained by standard fixed-point iteration, making its implementation straightforward.","Second-order convergence in time is shown in an abstract Hilbert space framework under reasonable assumptions on the problem.","Numerical experiments illustrate the proven order of convergence."],"url":"http://arxiv.org/abs/2403.05900v1","category":"math.NA"}
{"created":"2024-03-09 12:24:49","title":"Fast Kernel Scene Flow","abstract":"In contrast to current state-of-the-art methods, such as NSFP [25], which employ deep implicit neural functions for modeling scene flow, we present a novel approach that utilizes classical kernel representations. This representation enables our approach to effectively handle dense lidar points while demonstrating exceptional computational efficiency -- compared to recent deep approaches -- achieved through the solution of a linear system. As a runtime optimization-based method, our model exhibits impressive generalizability across various out-of-distribution scenarios, achieving competitive performance on large-scale lidar datasets. We propose a new positional encoding-based kernel that demonstrates state-of-the-art performance in efficient lidar scene flow estimation on large-scale point clouds. An important highlight of our method is its near real-time performance (~150-170 ms) with dense lidar data (~8k-144k points), enabling a variety of practical applications in robotics and autonomous driving scenarios.","sentences":["In contrast to current state-of-the-art methods, such as NSFP [25], which employ deep implicit neural functions for modeling scene flow, we present a novel approach that utilizes classical kernel representations.","This representation enables our approach to effectively handle dense lidar points while demonstrating exceptional computational efficiency -- compared to recent deep approaches -- achieved through the solution of a linear system.","As a runtime optimization-based method, our model exhibits impressive generalizability across various out-of-distribution scenarios, achieving competitive performance on large-scale lidar datasets.","We propose a new positional encoding-based kernel that demonstrates state-of-the-art performance in efficient lidar scene flow estimation on large-scale point clouds.","An important highlight of our method is its near real-time performance (~150-170 ms) with dense lidar data (~8k-144k points), enabling a variety of practical applications in robotics and autonomous driving scenarios."],"url":"http://arxiv.org/abs/2403.05896v1","category":"cs.CV"}
{"created":"2024-03-09 12:18:48","title":"Frequency Attention for Knowledge Distillation","abstract":"Knowledge distillation is an attractive approach for learning compact deep neural networks, which learns a lightweight student model by distilling knowledge from a complex teacher model. Attention-based knowledge distillation is a specific form of intermediate feature-based knowledge distillation that uses attention mechanisms to encourage the student to better mimic the teacher. However, most of the previous attention-based distillation approaches perform attention in the spatial domain, which primarily affects local regions in the input image. This may not be sufficient when we need to capture the broader context or global information necessary for effective knowledge transfer. In frequency domain, since each frequency is determined from all pixels of the image in spatial domain, it can contain global information about the image. Inspired by the benefits of the frequency domain, we propose a novel module that functions as an attention mechanism in the frequency domain. The module consists of a learnable global filter that can adjust the frequencies of student's features under the guidance of the teacher's features, which encourages the student's features to have patterns similar to the teacher's features. We then propose an enhanced knowledge review-based distillation model by leveraging the proposed frequency attention module. The extensive experiments with various teacher and student architectures on image classification and object detection benchmark datasets show that the proposed approach outperforms other knowledge distillation methods.","sentences":["Knowledge distillation is an attractive approach for learning compact deep neural networks, which learns a lightweight student model by distilling knowledge from a complex teacher model.","Attention-based knowledge distillation is a specific form of intermediate feature-based knowledge distillation that uses attention mechanisms to encourage the student to better mimic the teacher.","However, most of the previous attention-based distillation approaches perform attention in the spatial domain, which primarily affects local regions in the input image.","This may not be sufficient when we need to capture the broader context or global information necessary for effective knowledge transfer.","In frequency domain, since each frequency is determined from all pixels of the image in spatial domain, it can contain global information about the image.","Inspired by the benefits of the frequency domain, we propose a novel module that functions as an attention mechanism in the frequency domain.","The module consists of a learnable global filter that can adjust the frequencies of student's features under the guidance of the teacher's features, which encourages the student's features to have patterns similar to the teacher's features.","We then propose an enhanced knowledge review-based distillation model by leveraging the proposed frequency attention module.","The extensive experiments with various teacher and student architectures on image classification and object detection benchmark datasets show that the proposed approach outperforms other knowledge distillation methods."],"url":"http://arxiv.org/abs/2403.05894v1","category":"cs.CV"}
{"created":"2024-03-09 10:57:13","title":"Detecting quantum chaos via pseudo-entropy and negativity","abstract":"Quantum informatic quantities such as entanglement entropy are useful in detecting quantum phase transitions. Recently, a new entanglement measure called pseudo-entropy was proposed which is a generalization of the more well-known entanglement entropy. It has many nice properties and is useful in the study of post-selection measurements. In this paper, one of our goals is to explore the properties of pseudo-entropy and study the effectiveness of it as a quantum chaos diagnostic, i.e. as a tool to distinguish between chaotic and integrable systems. Using various variants of the SYK model, we study the signal of quantum chaos captured in the pseudo-entropy and relate it to the spectral form factor (SFF) and local operator entanglement (LOE). We also explore another quantity called the negativity of entanglement which is a useful entanglement measure for a mixed state. We generalized it to accommodate the transition matrix and called it pseudo-negativity in analogy to pseudo-entropy. We found that it also nicely captures the spectral properties of a chaotic system and hence also plays a role as a tool of quantum chaos diagnostic.","sentences":["Quantum informatic quantities such as entanglement entropy are useful in detecting quantum phase transitions.","Recently, a new entanglement measure called pseudo-entropy was proposed which is a generalization of the more well-known entanglement entropy.","It has many nice properties and is useful in the study of post-selection measurements.","In this paper, one of our goals is to explore the properties of pseudo-entropy and study the effectiveness of it as a quantum chaos diagnostic, i.e. as a tool to distinguish between chaotic and integrable systems.","Using various variants of the SYK model, we study the signal of quantum chaos captured in the pseudo-entropy and relate it to the spectral form factor (SFF) and local operator entanglement (LOE).","We also explore another quantity called the negativity of entanglement which is a useful entanglement measure for a mixed state.","We generalized it to accommodate the transition matrix and called it pseudo-negativity in analogy to pseudo-entropy.","We found that it also nicely captures the spectral properties of a chaotic system and hence also plays a role as a tool of quantum chaos diagnostic."],"url":"http://arxiv.org/abs/2403.05875v1","category":"hep-th"}
{"created":"2024-03-11 17:55:53","title":"Bayesian Diffusion Models for 3D Shape Reconstruction","abstract":"We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom-up processes where each itself is a diffusion process. We demonstrate state-of-the-art results on both synthetic and real-world benchmarks for 3D shape reconstruction.","sentences":["We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes.","We show the effectiveness of BDM on the 3D shape reconstruction task.","Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction.","As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks.","The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom-up processes where each itself is a diffusion process.","We demonstrate state-of-the-art results on both synthetic and real-world benchmarks for 3D shape reconstruction."],"url":"http://arxiv.org/abs/2403.06973v1","category":"cs.CV"}
{"created":"2024-03-11 17:49:18","title":"Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts","abstract":"Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose \\textbf{Di}verse \\textbf{Skil}l \\textbf{L}earning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distributions and demonstrate how we can efficiently train them using the standard policy gradient objective. We show on challenging robot simulation tasks that Di-SkilL can learn diverse and performant skills.","sentences":["Reinforcement learning (RL) is a powerful approach for acquiring a good-performing policy.","However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization.","We propose \\textbf{Di}verse \\textbf{Skil}l \\textbf{L}earning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive.","Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts.","The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space.","To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment's unknown context probability space, we leverage energy-based models to represent the per-expert context distributions and demonstrate how we can efficiently train them using the standard policy gradient objective.","We show on challenging robot simulation tasks that Di-SkilL can learn diverse and performant skills."],"url":"http://arxiv.org/abs/2403.06966v1","category":"cs.LG"}
{"created":"2024-03-11 17:39:08","title":"Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic Perovskites","abstract":"Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a promising class of electronically active materials for both light absorption and emission. The design space of HOIPs is extremely large, since a diverse space of organic cations can be combined with different inorganic frameworks. This immense design space allows for tunable electronic and mechanical properties, but also necessitates the development of new tools for in silico high throughput analysis of candidate structures. In this work, we present an accurate, efficient, transferable and widely applicable machine learning interatomic potential (MLIP) for predicting the structure of new 2D HOIPs. Using the MACE architecture, an MLIP is trained on 86 diverse experimentally reported HOIP structures. The model is tested on 73 unseen perovskite compositions, and achieves chemical accuracy with respect to the reference electronic structure method. Our model is then combined with a simple random structure search algorithm to predict the structure of hypothetical HOIPs given only the proposed composition. Success is demonstrated by correctly and reliably recovering the crystal structure of a set of experimentally known 2D perovskites. Such a random structure search is impossible with ab initio methods due to the associated computational cost, but is relatively inexpensive with the MACE potential. Finally, the procedure is used to predict the structure formed by a new organic cation with no previously known corresponding perovskite. Laboratory synthesis of the new hybrid perovskite confirms the accuracy of our prediction. This capability, applied at scale, enables efficient screening of thousands of combinations of organic cations and inorganic layers.","sentences":["Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a promising class of electronically active materials for both light absorption and emission.","The design space of HOIPs is extremely large, since a diverse space of organic cations can be combined with different inorganic frameworks.","This immense design space allows for tunable electronic and mechanical properties, but also necessitates the development of new tools for in silico high throughput analysis of candidate structures.","In this work, we present an accurate, efficient, transferable and widely applicable machine learning interatomic potential (MLIP) for predicting the structure of new 2D HOIPs.","Using the MACE architecture, an MLIP is trained on 86 diverse experimentally reported HOIP structures.","The model is tested on 73 unseen perovskite compositions, and achieves chemical accuracy with respect to the reference electronic structure method.","Our model is then combined with a simple random structure search algorithm to predict the structure of hypothetical HOIPs given only the proposed composition.","Success is demonstrated by correctly and reliably recovering the crystal structure of a set of experimentally known 2D perovskites.","Such a random structure search is impossible with ab initio methods due to the associated computational cost, but is relatively inexpensive with the MACE potential.","Finally, the procedure is used to predict the structure formed by a new organic cation with no previously known corresponding perovskite.","Laboratory synthesis of the new hybrid perovskite confirms the accuracy of our prediction.","This capability, applied at scale, enables efficient screening of thousands of combinations of organic cations and inorganic layers."],"url":"http://arxiv.org/abs/2403.06955v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-11 17:35:23","title":"DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations","abstract":"The diffusion-based text-to-image model harbors immense potential in transferring reference style. However, current encoder-based approaches significantly impair the text controllability of text-to-image models while transferring styles. In this paper, we introduce \\textit{DEADiff} to address this issue using the following two strategies: 1) a mechanism to decouple the style and semantics of reference images. The decoupled feature representations are first extracted by Q-Formers which are instructed by different text descriptions. Then they are injected into mutually exclusive subsets of cross-attention layers for better disentanglement. 2) A non-reconstructive learning method. The Q-Formers are trained using paired images rather than the identical target, in which the reference image and the ground-truth image are with the same style or semantics. We show that DEADiff attains the best visual stylization results and optimal balance between the text controllability inherent in the text-to-image model and style similarity to the reference image, as demonstrated both quantitatively and qualitatively. Our project page is~\\href{https://tianhao-qi.github.io/DEADiff/}{https://tianhao-qi.github.io/DEADiff/}.","sentences":["The diffusion-based text-to-image model harbors immense potential in transferring reference style.","However, current encoder-based approaches significantly impair the text controllability of text-to-image models while transferring styles.","In this paper, we introduce \\textit{DEADiff} to address this issue using the following two strategies: 1) a mechanism to decouple the style and semantics of reference images.","The decoupled feature representations are first extracted by Q-Formers which are instructed by different text descriptions.","Then they are injected into mutually exclusive subsets of cross-attention layers for better disentanglement.","2) A non-reconstructive learning method.","The Q-Formers are trained using paired images rather than the identical target, in which the reference image and the ground-truth image are with the same style or semantics.","We show that DEADiff attains the best visual stylization results and optimal balance between the text controllability inherent in the text-to-image model and style similarity to the reference image, as demonstrated both quantitatively and qualitatively.","Our project page is~\\href{https://tianhao-qi.github.io/DEADiff/}{https://tianhao-qi.github.io/DEADiff/}."],"url":"http://arxiv.org/abs/2403.06951v1","category":"cs.CV"}
{"created":"2024-03-11 16:12:08","title":"Concurrent Speaker Detection: A multi-microphone Transformer-Based Approach","abstract":"We present a deep-learning approach for the task of Concurrent Speaker Detection (CSD) using a modified transformer model. Our model is designed to handle multi- microphone data but can also work in the single-microphone case. The method can classify audio segments into one of three classes: 1) no speech activity (noise only), 2) only a single speaker is active, and 3) more than one speaker is active. We incorporate a Cost-Sensitive (CS) loss and a confidence calibration to the training procedure. The approach is evaluated using three real- world databases: AMI, AliMeeting, and CHiME 5, demonstrating an improvement over existing approaches.","sentences":["We present a deep-learning approach for the task of Concurrent Speaker Detection (CSD) using a modified transformer model.","Our model is designed to handle multi- microphone data but can also work in the single-microphone case.","The method can classify audio segments into one of three classes: 1) no speech activity (noise only), 2) only a single speaker is active, and 3) more than one speaker is active.","We incorporate a Cost-Sensitive (CS) loss and a confidence calibration to the training procedure.","The approach is evaluated using three real- world databases: AMI, AliMeeting, and CHiME 5, demonstrating an improvement over existing approaches."],"url":"http://arxiv.org/abs/2403.06856v1","category":"eess.AS"}
{"created":"2024-03-11 16:03:43","title":"DiaLoc: An Iterative Approach to Embodied Dialog Localization","abstract":"Multimodal learning has advanced the performance for many vision-language tasks. However, most existing works in embodied dialog research focus on navigation and leave the localization task understudied. The few existing dialog-based localization approaches assume the availability of entire dialog prior to localizaiton, which is impractical for deployed dialog-based localization. In this paper, we propose DiaLoc, a new dialog-based localization framework which aligns with a real human operator behavior. Specifically, we produce an iterative refinement of location predictions which can visualize current pose believes after each dialog turn. DiaLoc effectively utilizes the multimodal data for multi-shot localization, where a fusion encoder fuses vision and dialog information iteratively. We achieve state-of-the-art results on embodied dialog-based localization task, in single-shot (+7.08% in Acc5@valUnseen) and multi- shot settings (+10.85% in Acc5@valUnseen). DiaLoc narrows the gap between simulation and real-world applications, opening doors for future research on collaborative localization and navigation.","sentences":["Multimodal learning has advanced the performance for many vision-language tasks.","However, most existing works in embodied dialog research focus on navigation and leave the localization task understudied.","The few existing dialog-based localization approaches assume the availability of entire dialog prior to localizaiton, which is impractical for deployed dialog-based localization.","In this paper, we propose DiaLoc, a new dialog-based localization framework which aligns with a real human operator behavior.","Specifically, we produce an iterative refinement of location predictions which can visualize current pose believes after each dialog turn.","DiaLoc effectively utilizes the multimodal data for multi-shot localization, where a fusion encoder fuses vision and dialog information iteratively.","We achieve state-of-the-art results on embodied dialog-based localization task, in single-shot (+7.08% in Acc5@valUnseen) and multi- shot settings (+10.85% in Acc5@valUnseen).","DiaLoc narrows the gap between simulation and real-world applications, opening doors for future research on collaborative localization and navigation."],"url":"http://arxiv.org/abs/2403.06846v1","category":"cs.CV"}
{"created":"2024-03-11 15:48:56","title":"Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?","abstract":"Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating the measure, and we report results on several state-of-the-art open-source and closed LLMs. Finally, we quantitatively demonstrate that all evaluated LLMs fail to achieve a high amount of separation, according to our measure. The source code and SEP dataset are openly accessible at https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.","sentences":["Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications.","However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection.","Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested.","In this work, we aim to close this gap.","We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs.","We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating the measure, and we report results on several state-of-the-art open-source and closed LLMs.","Finally, we quantitatively demonstrate that all evaluated LLMs fail to achieve a high amount of separation, according to our measure.","The source code and SEP dataset are openly accessible at https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed."],"url":"http://arxiv.org/abs/2403.06833v1","category":"cs.LG"}
{"created":"2024-03-11 15:33:55","title":"Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science","abstract":"Maximum entropy (Maxent) models are a class of statistical models that use the maximum entropy principle to estimate probability distributions from data. Due to the size of modern data sets, Maxent models need efficient optimization algorithms to scale well for big data applications. State-of-the-art algorithms for Maxent models, however, were not originally designed to handle big data sets; these algorithms either rely on technical devices that may yield unreliable numerical results, scale poorly, or require smoothness assumptions that many practical Maxent models lack. In this paper, we present novel optimization algorithms that overcome the shortcomings of state-of-the-art algorithms for training large-scale, non-smooth Maxent models. Our proposed first-order algorithms leverage the Kullback-Leibler divergence to train large-scale and non-smooth Maxent models efficiently. For Maxent models with discrete probability distribution of $n$ elements built from samples, each containing $m$ features, the stepsize parameters estimation and iterations in our algorithms scale on the order of $O(mn)$ operations and can be trivially parallelized. Moreover, the strong $\\ell_{1}$ convexity of the Kullback--Leibler divergence allows for larger stepsize parameters, thereby speeding up the convergence rate of our algorithms. To illustrate the efficiency of our novel algorithms, we consider the problem of estimating probabilities of fire occurrences as a function of ecological features in the Western US MTBS-Interagency wildfire data set. Our numerical results show that our algorithms outperform the state of the arts by one order of magnitude and yield results that agree with physical models of wildfire occurrence and previous statistical analyses of wildfire drivers.","sentences":["Maximum entropy (Maxent) models are a class of statistical models that use the maximum entropy principle to estimate probability distributions from data.","Due to the size of modern data sets, Maxent models need efficient optimization algorithms to scale well for big data applications.","State-of-the-art algorithms for Maxent models, however, were not originally designed to handle big data sets; these algorithms either rely on technical devices that may yield unreliable numerical results, scale poorly, or require smoothness assumptions that many practical Maxent models lack.","In this paper, we present novel optimization algorithms that overcome the shortcomings of state-of-the-art algorithms for training large-scale, non-smooth Maxent models.","Our proposed first-order algorithms leverage the Kullback-Leibler divergence to train large-scale and non-smooth Maxent models efficiently.","For Maxent models with discrete probability distribution of $n$ elements built from samples, each containing $m$ features, the stepsize parameters estimation and iterations in our algorithms scale on the order of $O(mn)$ operations and can be trivially parallelized.","Moreover, the strong $\\ell_{1}$ convexity of the Kullback--Leibler divergence allows for larger stepsize parameters, thereby speeding up the convergence rate of our algorithms.","To illustrate the efficiency of our novel algorithms, we consider the problem of estimating probabilities of fire occurrences as a function of ecological features in the Western US MTBS-Interagency wildfire data set.","Our numerical results show that our algorithms outperform the state of the arts by one order of magnitude and yield results that agree with physical models of wildfire occurrence and previous statistical analyses of wildfire drivers."],"url":"http://arxiv.org/abs/2403.06816v1","category":"stat.ML"}
{"created":"2024-03-11 13:50:07","title":"On the Approximation of Kernel functions","abstract":"Various methods in statistical learning build on kernels considered in reproducing kernel Hilbert spaces. In applications, the kernel is often selected based on characteristics of the problem and the data. This kernel is then employed to infer response variables at points, where no explanatory data were observed. The data considered here are located in compact sets in higher dimensions and the paper addresses approximations of the kernel itself. The new approach considers Taylor series approximations of radial kernel functions. For the Gauss kernel on the unit cube, the paper establishes an upper bound of the associated eigenfunctions, which grows only polynomially with respect to the index. The novel approach substantiates smaller regularization parameters than considered in the literature, overall leading to better approximations. This improvement confirms low rank approximation methods such as the Nystr\\\"om method.","sentences":["Various methods in statistical learning build on kernels considered in reproducing kernel Hilbert spaces.","In applications, the kernel is often selected based on characteristics of the problem and the data.","This kernel is then employed to infer response variables at points, where no explanatory data were observed.","The data considered here are located in compact sets in higher dimensions and the paper addresses approximations of the kernel itself.","The new approach considers Taylor series approximations of radial kernel functions.","For the Gauss kernel on the unit cube, the paper establishes an upper bound of the associated eigenfunctions, which grows only polynomially with respect to the index.","The novel approach substantiates smaller regularization parameters than considered in the literature, overall leading to better approximations.","This improvement confirms low rank approximation methods such as the Nystr\\\"om method."],"url":"http://arxiv.org/abs/2403.06731v1","category":"stat.ML"}
{"created":"2024-03-11 12:43:44","title":"Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains","abstract":"Cross-silo federated learning (FL) allows data owners to train accurate machine learning models by benefiting from each others private datasets. Unfortunately, the model accuracy benefits of collaboration are often undermined by privacy defenses. Therefore, to incentivize client participation in privacy-sensitive domains, a FL protocol should strike a delicate balance between privacy guarantees and end-model accuracy. In this paper, we study the question of when and how a server could design a FL protocol provably beneficial for all participants. First, we provide necessary and sufficient conditions for the existence of mutually beneficial protocols in the context of mean estimation and convex stochastic optimization. We also derive protocols that maximize the total clients' utility, given symmetric privacy preferences. Finally, we design protocols maximizing end-model accuracy and demonstrate their benefits in synthetic experiments.","sentences":["Cross-silo federated learning (FL) allows data owners to train accurate machine learning models by benefiting from each others private datasets.","Unfortunately, the model accuracy benefits of collaboration are often undermined by privacy defenses.","Therefore, to incentivize client participation in privacy-sensitive domains, a FL protocol should strike a delicate balance between privacy guarantees and end-model accuracy.","In this paper, we study the question of when and how a server could design a FL protocol provably beneficial for all participants.","First, we provide necessary and sufficient conditions for the existence of mutually beneficial protocols in the context of mean estimation and convex stochastic optimization.","We also derive protocols that maximize the total clients' utility, given symmetric privacy preferences.","Finally, we design protocols maximizing end-model accuracy and demonstrate their benefits in synthetic experiments."],"url":"http://arxiv.org/abs/2403.06672v1","category":"stat.ML"}
{"created":"2024-03-11 12:07:33","title":"Ricci flow-based brain surface covariance descriptors for Alzheimer disease","abstract":"Automated feature extraction from MRI brain scans and diagnosis of Alzheimer's disease are ongoing challenges. With advances in 3D imaging technology, 3D data acquisition is becoming more viable and efficient than its 2D counterpart. Rather than using feature-based vectors, in this paper, for the first time, we suggest a pipeline to extract novel covariance-based descriptors from the cortical surface using the Ricci energy optimization. The covariance descriptors are components of the nonlinear manifold of symmetric positive-definite matrices, thus we focus on using the Gaussian radial basis function to apply manifold-based classification to the 3D shape problem. Applying this novel signature to the analysis of abnormal cortical brain morphometry allows for diagnosing Alzheimer's disease. Experimental studies performed on about two hundred 3D MRI brain models, gathered from Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of our descriptors in achieving remarkable classification accuracy.","sentences":["Automated feature extraction from MRI brain scans and diagnosis of Alzheimer's disease are ongoing challenges.","With advances in 3D imaging technology, 3D data acquisition is becoming more viable and efficient than its 2D counterpart.","Rather than using feature-based vectors, in this paper, for the first time, we suggest a pipeline to extract novel covariance-based descriptors from the cortical surface using the Ricci energy optimization.","The covariance descriptors are components of the nonlinear manifold of symmetric positive-definite matrices, thus we focus on using the Gaussian radial basis function to apply manifold-based classification to the 3D shape problem.","Applying this novel signature to the analysis of abnormal cortical brain morphometry allows for diagnosing Alzheimer's disease.","Experimental studies performed on about two hundred 3D MRI brain models, gathered from Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of our descriptors in achieving remarkable classification accuracy."],"url":"http://arxiv.org/abs/2403.06645v1","category":"eess.IV"}
{"created":"2024-03-11 12:07:13","title":"Elephants Never Forget: Testing Language Models for Memorization of Tabular Data","abstract":"While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Starting with simple qualitative tests for whether an LLM knows the names and values of features, we introduce a variety of different techniques to assess the degrees of contamination, including statistical tests for conditional distribution modeling and four tests that identify memorization. Our investigation reveals that LLMs are pre-trained on many popular tabular datasets. This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to the test set. Interestingly, we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim. On these datasets, although seen during training, good performance on downstream tasks might not be due to overfitting. Our findings underscore the need for ensuring data integrity in machine learning tasks with LLMs. To facilitate future research, we release an open-source tool that can perform various tests for memorization \\url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.","sentences":["While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over.","In this work, we address this concern for tabular data.","Starting with simple qualitative tests for whether an LLM knows the names and values of features, we introduce a variety of different techniques to assess the degrees of contamination, including statistical tests for conditional distribution modeling and four tests that identify memorization.","Our investigation reveals that LLMs are pre-trained on many popular tabular datasets.","This exposure can lead to invalid performance evaluation on downstream tasks because the LLMs have, in effect, been fit to the test set.","Interestingly, we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim.","On these datasets, although seen during training, good performance on downstream tasks might not be due to overfitting.","Our findings underscore the need for ensuring data integrity in machine learning tasks with LLMs.","To facilitate future research, we release an open-source tool that can perform various tests for memorization \\url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}."],"url":"http://arxiv.org/abs/2403.06644v1","category":"cs.LG"}
{"created":"2024-03-11 10:05:29","title":"Unraveling the Mystery of Scaling Laws: Part I","abstract":"Scaling law principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training. These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of large language models such as GPT-4, Llama and Gemini. However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters. Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory. In this technical report, we confirm that the scaling law formulations proposed in the original OpenAI paper remain valid when scaling the model size up to 33 billion, but the constant coefficients in these formulas vary significantly with the experiment setup. We meticulously identify influential factors and provide transparent, step-by-step instructions to estimate all constant terms in scaling-law formulas by training on models with only 1M~60M parameters. Using these estimated formulas, we showcase the capability to accurately predict various attributes for models with up to 33B parameters before their training, including (1) the minimum possible test loss; (2) the minimum required training steps and processed tokens to achieve a specific loss; (3) the critical batch size with an optimal time/computation trade-off at any loss value; and (4) the complete test loss trajectory with arbitrary batch size.","sentences":["Scaling law principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training.","These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of large language models such as GPT-4, Llama and Gemini.","However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters.","Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory.","In this technical report, we confirm that the scaling law formulations proposed in the original OpenAI paper remain valid when scaling the model size up to 33 billion, but the constant coefficients in these formulas vary significantly with the experiment setup.","We meticulously identify influential factors and provide transparent, step-by-step instructions to estimate all constant terms in scaling-law formulas by training on models with only 1M~60M parameters.","Using these estimated formulas, we showcase the capability to accurately predict various attributes for models with up to 33B parameters before their training, including (1) the minimum possible test loss; (2) the minimum required training steps and processed tokens to achieve a specific loss; (3) the critical batch size with an optimal time/computation trade-off at any loss value; and (4) the complete test loss trajectory with arbitrary batch size."],"url":"http://arxiv.org/abs/2403.06563v1","category":"cs.LG"}
{"created":"2024-03-11 08:37:03","title":"Skeleton Supervised Airway Segmentation","abstract":"Fully-supervised airway segmentation has accomplished significant triumphs over the years in aiding pre-operative diagnosis and intra-operative navigation. However, full voxel-level annotation constitutes a labor-intensive and time-consuming task, often plagued by issues such as missing branches, branch annotation discontinuity, or erroneous edge delineation. label-efficient solutions for airway extraction are rarely explored yet primarily demanding in medical practice. To this end, we introduce a novel skeleton-level annotation (SkA) tailored to the airway, which simplifies the annotation workflow while enhancing annotation consistency and accuracy, preserving the complete topology. Furthermore, we propose a skeleton-supervised learning framework to achieve accurate airway segmentation. Firstly, a dual-stream buffer inference is introduced to realize initial label propagation from SkA, avoiding the collapse of direct learning from SkA. Then, we construct a geometry-aware dual-path propagation framework (GDP) to further promote complementary propagation learning, composed of hard geometry-aware propagation learning and soft geometry-aware propagation guidance. Experiments reveal that our proposed framework outperforms the competing methods with SKA, which amounts to only 1.96% airways, and achieves comparable performance with the baseline model that is fully supervised with 100% airways, demonstrating its significant potential in achieving label-efficient segmentation for other tubular structures, such as vessels.","sentences":["Fully-supervised airway segmentation has accomplished significant triumphs over the years in aiding pre-operative diagnosis and intra-operative navigation.","However, full voxel-level annotation constitutes a labor-intensive and time-consuming task, often plagued by issues such as missing branches, branch annotation discontinuity, or erroneous edge delineation.","label-efficient solutions for airway extraction are rarely explored yet primarily demanding in medical practice.","To this end, we introduce a novel skeleton-level annotation (SkA) tailored to the airway, which simplifies the annotation workflow while enhancing annotation consistency and accuracy, preserving the complete topology.","Furthermore, we propose a skeleton-supervised learning framework to achieve accurate airway segmentation.","Firstly, a dual-stream buffer inference is introduced to realize initial label propagation from SkA, avoiding the collapse of direct learning from SkA.","Then, we construct a geometry-aware dual-path propagation framework (GDP) to further promote complementary propagation learning, composed of hard geometry-aware propagation learning and soft geometry-aware propagation guidance.","Experiments reveal that our proposed framework outperforms the competing methods with SKA, which amounts to only 1.96% airways, and achieves comparable performance with the baseline model that is fully supervised with 100% airways, demonstrating its significant potential in achieving label-efficient segmentation for other tubular structures, such as vessels."],"url":"http://arxiv.org/abs/2403.06510v1","category":"cs.CV"}
{"created":"2024-03-11 06:32:32","title":"A Survey of Learned Indexes for the Multi-dimensional Space","abstract":"A recent research trend involves treating database index structures as Machine Learning (ML) models. In this domain, single or multiple ML models are trained to learn the mapping from keys to positions inside a data set. This class of indexes is known as \"Learned Indexes.\" Learned indexes have demonstrated improved search performance and reduced space requirements for one-dimensional data. The concept of one-dimensional learned indexes has naturally been extended to multi-dimensional (e.g., spatial) data, leading to the development of \"Learned Multi-dimensional Indexes\". This survey focuses on learned multi-dimensional index structures. Specifically, it reviews the current state of this research area, explains the core concepts behind each proposed method, and classifies these methods based on several well-defined criteria. We present a taxonomy that classifies and categorizes each learned multi-dimensional index, and survey the existing literature on learned multi-dimensional indexes according to this taxonomy. Additionally, we present a timeline to illustrate the evolution of research on learned indexes. Finally, we highlight several open challenges and future research directions in this emerging and highly active field.","sentences":["A recent research trend involves treating database index structures as Machine Learning (ML) models.","In this domain, single or multiple ML models are trained to learn the mapping from keys to positions inside a data set.","This class of indexes is known as \"Learned Indexes.\"","Learned indexes have demonstrated improved search performance and reduced space requirements for one-dimensional data.","The concept of one-dimensional learned indexes has naturally been extended to multi-dimensional (e.g., spatial) data, leading to the development of \"Learned Multi-dimensional Indexes\".","This survey focuses on learned multi-dimensional index structures.","Specifically, it reviews the current state of this research area, explains the core concepts behind each proposed method, and classifies these methods based on several well-defined criteria.","We present a taxonomy that classifies and categorizes each learned multi-dimensional index, and survey the existing literature on learned multi-dimensional indexes according to this taxonomy.","Additionally, we present a timeline to illustrate the evolution of research on learned indexes.","Finally, we highlight several open challenges and future research directions in this emerging and highly active field."],"url":"http://arxiv.org/abs/2403.06456v1","category":"cs.DB"}
{"created":"2024-03-11 06:15:50","title":"When Crypto Economics Meet Graph Analytics and Learning","abstract":"Utilizing graph analytics and learning has proven to be an effective method for exploring aspects of crypto economics such as network effects, decentralization, tokenomics, and fraud detection. However, the majority of existing research predominantly focuses on leading cryptocurrencies, namely Bitcoin (BTC) and Ethereum (ETH), overlooking the vast diversity among the more than 10,000 cryptocurrency projects. This oversight may result in skewed insights. In our paper, we aim to broaden the scope of investigation to encompass the entire spectrum of cryptocurrencies, examining various coins across their entire life cycles. Furthermore, we intend to pioneer advanced methodologies, including graph transfer learning and the innovative concept of \"graph of graphs\". By extending our research beyond the confines of BTC and ETH, our goal is to enhance the depth of our understanding of crypto economics and to advance the development of more intricate graph-based techniques.","sentences":["Utilizing graph analytics and learning has proven to be an effective method for exploring aspects of crypto economics such as network effects, decentralization, tokenomics, and fraud detection.","However, the majority of existing research predominantly focuses on leading cryptocurrencies, namely Bitcoin (BTC) and Ethereum (ETH), overlooking the vast diversity among the more than 10,000 cryptocurrency projects.","This oversight may result in skewed insights.","In our paper, we aim to broaden the scope of investigation to encompass the entire spectrum of cryptocurrencies, examining various coins across their entire life cycles.","Furthermore, we intend to pioneer advanced methodologies, including graph transfer learning and the innovative concept of \"graph of graphs\".","By extending our research beyond the confines of BTC and ETH, our goal is to enhance the depth of our understanding of crypto economics and to advance the development of more intricate graph-based techniques."],"url":"http://arxiv.org/abs/2403.06454v1","category":"cs.CE"}
{"created":"2024-03-11 04:11:48","title":"Causal Multi-Label Feature Selection in Federated Setting","abstract":"Multi-label feature selection serves as an effective mean for dealing with high-dimensional multi-label data. To achieve satisfactory performance, existing methods for multi-label feature selection often require the centralization of substantial data from multiple sources. However, in Federated setting, centralizing data from all sources and merging them into a single dataset is not feasible. To tackle this issue, in this paper, we study a challenging problem of causal multi-label feature selection in federated setting and propose a Federated Causal Multi-label Feature Selection (FedCMFS) algorithm with three novel subroutines. Specifically, FedCMFS first uses the FedCFL subroutine that considers the correlations among label-label, label-feature, and feature-feature to learn the relevant features (candidate parents and children) of each class label while preserving data privacy without centralizing data. Second, FedCMFS employs the FedCFR subroutine to selectively recover the missed true relevant features. Finally, FedCMFS utilizes the FedCFC subroutine to remove false relevant features. The extensive experiments on 8 datasets have shown that FedCMFS is effect for causal multi-label feature selection in federated setting.","sentences":["Multi-label feature selection serves as an effective mean for dealing with high-dimensional multi-label data.","To achieve satisfactory performance, existing methods for multi-label feature selection often require the centralization of substantial data from multiple sources.","However, in Federated setting, centralizing data from all sources and merging them into a single dataset is not feasible.","To tackle this issue, in this paper, we study a challenging problem of causal multi-label feature selection in federated setting and propose a Federated Causal Multi-label Feature Selection (FedCMFS) algorithm with three novel subroutines.","Specifically, FedCMFS first uses the FedCFL subroutine that considers the correlations among label-label, label-feature, and feature-feature to learn the relevant features (candidate parents and children) of each class label while preserving data privacy without centralizing data.","Second, FedCMFS employs the FedCFR subroutine to selectively recover the missed true relevant features.","Finally, FedCMFS utilizes the FedCFC subroutine to remove false relevant features.","The extensive experiments on 8 datasets have shown that FedCMFS is effect for causal multi-label feature selection in federated setting."],"url":"http://arxiv.org/abs/2403.06419v1","category":"cs.LG"}
{"created":"2024-03-11 03:55:24","title":"Evolving Knowledge Distillation with Large Language Models and Active Learning","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks. However, their computational costs are prohibitively high. To address this issue, previous research has attempted to distill the knowledge of LLMs into smaller models by generating annotated data. Nonetheless, these works have mainly focused on the direct use of LLMs for text generation and labeling, without fully exploring their potential to comprehend the target task and acquire valuable knowledge. In this paper, we propose EvoKD: Evolving Knowledge Distillation, which leverages the concept of active learning to interactively enhance the process of data generation using large language models, simultaneously improving the task capabilities of small domain model (student model). Different from previous work, we actively analyze the student model's weaknesses, and then synthesize labeled samples based on the analysis. In addition, we provide iterative feedback to the LLMs regarding the student model's performance to continuously construct diversified and challenging samples. Experiments and analysis on different NLP tasks, namely, text classification and named entity recognition show the effectiveness of EvoKD.","sentences":["Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks.","However, their computational costs are prohibitively high.","To address this issue, previous research has attempted to distill the knowledge of LLMs into smaller models by generating annotated data.","Nonetheless, these works have mainly focused on the direct use of LLMs for text generation and labeling, without fully exploring their potential to comprehend the target task and acquire valuable knowledge.","In this paper, we propose EvoKD: Evolving Knowledge Distillation, which leverages the concept of active learning to interactively enhance the process of data generation using large language models, simultaneously improving the task capabilities of small domain model (student model).","Different from previous work, we actively analyze the student model's weaknesses, and then synthesize labeled samples based on the analysis.","In addition, we provide iterative feedback to the LLMs regarding the student model's performance to continuously construct diversified and challenging samples.","Experiments and analysis on different NLP tasks, namely, text classification and named entity recognition show the effectiveness of EvoKD."],"url":"http://arxiv.org/abs/2403.06414v1","category":"cs.CL"}
{"created":"2024-03-11 02:57:27","title":"Towards Robust Out-of-Distribution Generalization Bounds via Sharpness","abstract":"Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, the sharpness of learned minima influences OOD generalization. To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by \"robustness\" in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms. It also provides a theoretical backing for \"flat minima leads to better OOD generalization\". Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, resulting in a tighter bound than non-robust guarantees. Our findings are supported by the experiments on a ridge regression model, as well as the experiments on deep learning classification tasks.","sentences":["Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees.","Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model.","As empirically shown in recent work, the sharpness of learned minima influences OOD generalization.","To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by \"robustness\" in generalization.","In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms.","It also provides a theoretical backing for \"flat minima leads to better OOD generalization\".","Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, resulting in a tighter bound than non-robust guarantees.","Our findings are supported by the experiments on a ridge regression model, as well as the experiments on deep learning classification tasks."],"url":"http://arxiv.org/abs/2403.06392v1","category":"cs.LG"}
{"created":"2024-03-11 02:47:21","title":"A Zero Trust Framework for Realization and Defense Against Generative AI Attacks in Power Grid","abstract":"Understanding the potential of generative AI (GenAI)-based attacks on the power grid is a fundamental challenge that must be addressed in order to protect the power grid by realizing and validating risk in new attack vectors. In this paper, a novel zero trust framework for a power grid supply chain (PGSC) is proposed. This framework facilitates early detection of potential GenAI-driven attack vectors (e.g., replay and protocol-type attacks), assessment of tail risk-based stability measures, and mitigation of such threats. First, a new zero trust system model of PGSC is designed and formulated as a zero-trust problem that seeks to guarantee for a stable PGSC by realizing and defending against GenAI-driven cyber attacks. Second, in which a domain-specific generative adversarial networks (GAN)-based attack generation mechanism is developed to create a new vulnerability cyberspace for further understanding that threat. Third, tail-based risk realization metrics are developed and implemented for quantifying the extreme risk of a potential attack while leveraging a trust measurement approach for continuous validation. Fourth, an ensemble learning-based bootstrap aggregation scheme is devised to detect the attacks that are generating synthetic identities with convincing user and distributed energy resources device profiles. Experimental results show the efficacy of the proposed zero trust framework that achieves an accuracy of 95.7% on attack vector generation, a risk measure of 9.61% for a 95% stable PGSC, and a 99% confidence in defense against GenAI-driven attack.","sentences":["Understanding the potential of generative AI (GenAI)-based attacks on the power grid is a fundamental challenge that must be addressed in order to protect the power grid by realizing and validating risk in new attack vectors.","In this paper, a novel zero trust framework for a power grid supply chain (PGSC) is proposed.","This framework facilitates early detection of potential GenAI-driven attack vectors (e.g., replay and protocol-type attacks), assessment of tail risk-based stability measures, and mitigation of such threats.","First, a new zero trust system model of PGSC is designed and formulated as a zero-trust problem that seeks to guarantee for a stable PGSC by realizing and defending against GenAI-driven cyber attacks.","Second, in which a domain-specific generative adversarial networks (GAN)-based attack generation mechanism is developed to create a new vulnerability cyberspace for further understanding that threat.","Third, tail-based risk realization metrics are developed and implemented for quantifying the extreme risk of a potential attack while leveraging a trust measurement approach for continuous validation.","Fourth, an ensemble learning-based bootstrap aggregation scheme is devised to detect the attacks that are generating synthetic identities with convincing user and distributed energy resources device profiles.","Experimental results show the efficacy of the proposed zero trust framework that achieves an accuracy of 95.7% on attack vector generation, a risk measure of 9.61% for a 95% stable PGSC, and a 99% confidence in defense against GenAI-driven attack."],"url":"http://arxiv.org/abs/2403.06388v1","category":"cs.CR"}
{"created":"2024-03-11 02:05:31","title":"Eliminating Warping Shakes for Unsupervised Online Video Stitching","abstract":"In this paper, we retarget video stitching to an emerging issue, named warping shake, when extending image stitching to video stitching. It unveils the temporal instability of warped content in non-overlapping regions, despite image stitching having endeavored to preserve the natural structures. Therefore, in most cases, even if the input videos to be stitched are stable, the stitched video will inevitably cause undesired warping shakes and affect the visual experience. To eliminate the shakes, we propose StabStitch to simultaneously realize video stitching and video stabilization in a unified unsupervised learning framework. Starting from the camera paths in video stabilization, we first derive the expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Then a warp smoothing model is presented to optimize them with a comprehensive consideration regarding content alignment, trajectory smoothness, spatial consistency, and online collaboration. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Compared with existing stitching solutions, StabStitch exhibits significant superiority in scene robustness and inference speed in addition to stitching and stabilization performance, contributing to a robust and real-time online video stitching system. The code and dataset will be available at https://github.com/nie-lang/StabStitch.","sentences":["In this paper, we retarget video stitching to an emerging issue, named warping shake, when extending image stitching to video stitching.","It unveils the temporal instability of warped content in non-overlapping regions, despite image stitching having endeavored to preserve the natural structures.","Therefore, in most cases, even if the input videos to be stitched are stable, the stitched video will inevitably cause undesired warping shakes and affect the visual experience.","To eliminate the shakes, we propose StabStitch to simultaneously realize video stitching and video stabilization in a unified unsupervised learning framework.","Starting from the camera paths in video stabilization, we first derive the expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps.","Then a warp smoothing model is presented to optimize them with a comprehensive consideration regarding content alignment, trajectory smoothness, spatial consistency, and online collaboration.","To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes.","Compared with existing stitching solutions, StabStitch exhibits significant superiority in scene robustness and inference speed in addition to stitching and stabilization performance, contributing to a robust and real-time online video stitching system.","The code and dataset will be available at https://github.com/nie-lang/StabStitch."],"url":"http://arxiv.org/abs/2403.06378v1","category":"cs.CV"}
{"created":"2024-03-11 01:58:04","title":"FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization","abstract":"Generating emotional talking faces is a practical yet challenging endeavor. To create a lifelike avatar, we draw upon two critical insights from a human perspective: 1) The connection between audio and the non-deterministic facial dynamics, encompassing expressions, blinks, poses, should exhibit synchronous and one-to-many mapping. 2) Vibrant expressions are often accompanied by emotion-aware high-definition (HD) textures and finely detailed teeth. However, both aspects are frequently overlooked by existing methods. To this end, this paper proposes using normalizing Flow and Vector-Quantization modeling to produce emotional talking faces that satisfy both insights concurrently (FlowVQTalker). Specifically, we develop a flow-based coefficient generator that encodes the dynamics of facial emotion into a multi-emotion-class latent space represented as a mixture distribution. The generation process commences with random sampling from the modeled distribution, guided by the accompanying audio, enabling both lip-synchronization and the uncertain nonverbal facial cues generation. Furthermore, our designed vector-quantization image generator treats the creation of expressive facial images as a code query task, utilizing a learned codebook to provide rich, high-quality textures that enhance the emotional perception of the results. Extensive experiments are conducted to showcase the effectiveness of our approach.","sentences":["Generating emotional talking faces is a practical yet challenging endeavor.","To create a lifelike avatar, we draw upon two critical insights from a human perspective: 1)","The connection between audio and the non-deterministic facial dynamics, encompassing expressions, blinks, poses, should exhibit synchronous and one-to-many mapping.","2) Vibrant expressions are often accompanied by emotion-aware high-definition (HD) textures and finely detailed teeth.","However, both aspects are frequently overlooked by existing methods.","To this end, this paper proposes using normalizing Flow and Vector-Quantization modeling to produce emotional talking faces that satisfy both insights concurrently (FlowVQTalker).","Specifically, we develop a flow-based coefficient generator that encodes the dynamics of facial emotion into a multi-emotion-class latent space represented as a mixture distribution.","The generation process commences with random sampling from the modeled distribution, guided by the accompanying audio, enabling both lip-synchronization and the uncertain nonverbal facial cues generation.","Furthermore, our designed vector-quantization image generator treats the creation of expressive facial images as a code query task, utilizing a learned codebook to provide rich, high-quality textures that enhance the emotional perception of the results.","Extensive experiments are conducted to showcase the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.06375v1","category":"cs.CV"}
{"created":"2024-03-10 21:41:38","title":"Almost Optimal Agnostic Control of Unknown Linear Dynamics","abstract":"We consider a simple control problem in which the underlying dynamics depend on a parameter $a$ that is unknown and must be learned. We study three variants of the control problem: Bayesian control, in which we have a prior belief about $a$; bounded agnostic control, in which we have no prior belief about $a$ but we assume that $a$ belongs to a bounded set; and fully agnostic control, in which $a$ is allowed to be an arbitrary real number about which we have no prior belief. In the Bayesian variant, a control strategy is optimal if it minimizes a certain expected cost. In the agnostic variants, a control strategy is optimal if it minimizes a quantity called the worst-case regret. For the Bayesian and bounded agnostic variants above, we produce optimal control strategies. For the fully agnostic variant, we produce almost optimal control strategies, i.e., for any $\\varepsilon>0$ we produce a strategy that minimizes the worst-case regret to within a multiplicative factor of $(1+\\varepsilon)$.","sentences":["We consider a simple control problem in which the underlying dynamics depend on a parameter $a$ that is unknown and must be learned.","We study three variants of the control problem: Bayesian control, in which we have a prior belief about $a$; bounded agnostic control, in which we have no prior belief about $","a$","but we assume that $a$ belongs to a bounded set; and fully agnostic control, in which $a$ is allowed to be an arbitrary real number about which we have no prior belief.","In the Bayesian variant, a control strategy is optimal if it minimizes a certain expected cost.","In the agnostic variants, a control strategy is optimal if it minimizes a quantity called the worst-case regret.","For the Bayesian and bounded agnostic variants above, we produce optimal control strategies.","For the fully agnostic variant, we produce almost optimal control strategies, i.e., for any $\\varepsilon>0$ we produce a strategy that minimizes the worst-case regret to within a multiplicative factor of $(1+\\varepsilon)$."],"url":"http://arxiv.org/abs/2403.06320v1","category":"math.OC"}
{"created":"2024-03-10 21:08:29","title":"How much data do you need? Part 2: Predicting DL class specific training dataset sizes","abstract":"This paper targets the question of predicting machine learning classification model performance, when taking into account the number of training examples per class and not just the overall number of training examples. This leads to the a combinatorial question, which combinations of number of training examples per class should be considered, given a fixed overall training dataset size. In order to solve this question, an algorithm is suggested which is motivated from special cases of space filling design of experiments. The resulting data are modeled using models like powerlaw curves and similar models, extended like generalized linear models i.e. by replacing the overall training dataset size by a parametrized linear combination of the number of training examples per label class. The proposed algorithm has been applied on the CIFAR10 and the EMNIST datasets.","sentences":["This paper targets the question of predicting machine learning classification model performance, when taking into account the number of training examples per class and not just the overall number of training examples.","This leads to the a combinatorial question, which combinations of number of training examples per class should be considered, given a fixed overall training dataset size.","In order to solve this question, an algorithm is suggested which is motivated from special cases of space filling design of experiments.","The resulting data are modeled using models like powerlaw curves and similar models, extended like generalized linear models i.e. by replacing the overall training dataset size by a parametrized linear combination of the number of training examples per label class.","The proposed algorithm has been applied on the CIFAR10 and the EMNIST datasets."],"url":"http://arxiv.org/abs/2403.06311v1","category":"cs.LG"}
{"created":"2024-03-10 20:09:23","title":"Disentangling Resilience from Robustness: Contextual Dualism, Interactionism, and Game-Theoretic Paradigms","abstract":"This article explains the distinctions between robustness and resilience in control systems. Resilience confronts a distinct set of challenges, posing new ones for designing controllers for feedback systems, networks, and machines that prioritize resilience over robustness. The concept of resilience is explored through a three-stage model, emphasizing the need for a proactive preparation and automated response to elastic events. A toy model is first used to illustrate the tradeoffs between resilience and robustness. Then, it delves into contextual dualism and interactionism, and introduces game-theoretic paradigms as a unifying framework to consolidate resilience and robustness. The article concludes by discussing the interplay between robustness and resilience, suggesting that a comprehensive theory of resilience and quantification metrics, and formalization through game-theoretic frameworks are necessary. The exploration extends to system-of-systems resilience and various mechanisms, including the integration of AI techniques and non-technical solutions, like cyber insurance, to achieve comprehensive resilience in control systems. As we approach 2030, the systems and control community is at the opportune moment to lay scientific foundations of resilience by bridging feedback control theory, game theory, and learning theory. Resilient control systems will enhance overall quality of life, enable the development of a resilient society, and create a societal-scale impact amid global challenges such as climate change, conflicts, and cyber insecurity.","sentences":["This article explains the distinctions between robustness and resilience in control systems.","Resilience confronts a distinct set of challenges, posing new ones for designing controllers for feedback systems, networks, and machines that prioritize resilience over robustness.","The concept of resilience is explored through a three-stage model, emphasizing the need for a proactive preparation and automated response to elastic events.","A toy model is first used to illustrate the tradeoffs between resilience and robustness.","Then, it delves into contextual dualism and interactionism, and introduces game-theoretic paradigms as a unifying framework to consolidate resilience and robustness.","The article concludes by discussing the interplay between robustness and resilience, suggesting that a comprehensive theory of resilience and quantification metrics, and formalization through game-theoretic frameworks are necessary.","The exploration extends to system-of-systems resilience and various mechanisms, including the integration of AI techniques and non-technical solutions, like cyber insurance, to achieve comprehensive resilience in control systems.","As we approach 2030, the systems and control community is at the opportune moment to lay scientific foundations of resilience by bridging feedback control theory, game theory, and learning theory.","Resilient control systems will enhance overall quality of life, enable the development of a resilient society, and create a societal-scale impact amid global challenges such as climate change, conflicts, and cyber insecurity."],"url":"http://arxiv.org/abs/2403.06299v1","category":"eess.SY"}
{"created":"2024-03-10 19:50:03","title":"A streamlined Approach to Multimodal Few-Shot Class Incremental Learning for Fine-Grained Datasets","abstract":"Few-shot Class-Incremental Learning (FSCIL) poses the challenge of retaining prior knowledge while learning from limited new data streams, all without overfitting. The rise of Vision-Language models (VLMs) has unlocked numerous applications, leveraging their existing knowledge to fine-tune on custom data. However, training the whole model is computationally prohibitive, and VLMs while being versatile in general domains still struggle with fine-grained datasets crucial for many applications. We tackle these challenges with two proposed simple modules. The first, Session-Specific Prompts (SSP), enhances the separability of image-text embeddings across sessions. The second, Hyperbolic distance, compresses representations of image-text pairs within the same class while expanding those from different classes, leading to better representations. Experimental results demonstrate an average 10-point increase compared to baselines while requiring at least 8 times fewer trainable parameters. This improvement is further underscored on our three newly introduced fine-grained datasets.","sentences":["Few-shot Class-Incremental Learning (FSCIL) poses the challenge of retaining prior knowledge while learning from limited new data streams, all without overfitting.","The rise of Vision-Language models (VLMs) has unlocked numerous applications, leveraging their existing knowledge to fine-tune on custom data.","However, training the whole model is computationally prohibitive, and VLMs while being versatile in general domains still struggle with fine-grained datasets crucial for many applications.","We tackle these challenges with two proposed simple modules.","The first, Session-Specific Prompts (SSP), enhances the separability of image-text embeddings across sessions.","The second, Hyperbolic distance, compresses representations of image-text pairs within the same class while expanding those from different classes, leading to better representations.","Experimental results demonstrate an average 10-point increase compared to baselines while requiring at least 8 times fewer trainable parameters.","This improvement is further underscored on our three newly introduced fine-grained datasets."],"url":"http://arxiv.org/abs/2403.06295v1","category":"cs.CV"}
{"created":"2024-03-10 19:31:13","title":"Transformer based Multitask Learning for Image Captioning and Object Detection","abstract":"In several real-world scenarios like autonomous navigation and mobility, to obtain a better visual understanding of the surroundings, image captioning and object detection play a crucial role. This work introduces a novel multitask learning framework that combines image captioning and object detection into a joint model. We propose TICOD, Transformer-based Image Captioning and Object detection model for jointly training both tasks by combining the losses obtained from image captioning and object detection networks. By leveraging joint training, the model benefits from the complementary information shared between the two tasks, leading to improved performance for image captioning. Our approach utilizes a transformer-based architecture that enables end-to-end network integration for image captioning and object detection and performs both tasks jointly. We evaluate the effectiveness of our approach through comprehensive experiments on the MS-COCO dataset. Our model outperforms the baselines from image captioning literature by achieving a 3.65% improvement in BERTScore.","sentences":["In several real-world scenarios like autonomous navigation and mobility, to obtain a better visual understanding of the surroundings, image captioning and object detection play a crucial role.","This work introduces a novel multitask learning framework that combines image captioning and object detection into a joint model.","We propose TICOD, Transformer-based Image Captioning and Object detection model for jointly training both tasks by combining the losses obtained from image captioning and object detection networks.","By leveraging joint training, the model benefits from the complementary information shared between the two tasks, leading to improved performance for image captioning.","Our approach utilizes a transformer-based architecture that enables end-to-end network integration for image captioning and object detection and performs both tasks jointly.","We evaluate the effectiveness of our approach through comprehensive experiments on the MS-COCO dataset.","Our model outperforms the baselines from image captioning literature by achieving a 3.65% improvement in BERTScore."],"url":"http://arxiv.org/abs/2403.06292v1","category":"cs.CV"}
{"created":"2024-03-10 18:58:14","title":"Probing Image Compression For Class-Incremental Learning","abstract":"Image compression emerges as a pivotal tool in the efficient handling and transmission of digital images. Its ability to substantially reduce file size not only facilitates enhanced data storage capacity but also potentially brings advantages to the development of continual machine learning (ML) systems, which learn new knowledge incrementally from sequential data. Continual ML systems often rely on storing representative samples, also known as exemplars, within a limited memory constraint to maintain the performance on previously learned data. These methods are known as memory replay-based algorithms and have proven effective at mitigating the detrimental effects of catastrophic forgetting. Nonetheless, the limited memory buffer size often falls short of adequately representing the entire data distribution. In this paper, we explore the use of image compression as a strategy to enhance the buffer's capacity, thereby increasing exemplar diversity. However, directly using compressed exemplars introduces domain shift during continual ML, marked by a discrepancy between compressed training data and uncompressed testing data. Additionally, it is essential to determine the appropriate compression algorithm and select the most effective rate for continual ML systems to balance the trade-off between exemplar quality and quantity. To this end, we introduce a new framework to incorporate image compression for continual ML including a pre-processing data compression step and an efficient compression rate/algorithm selection method. We conduct extensive experiments on CIFAR-100 and ImageNet datasets and show that our method significantly improves image classification accuracy in continual ML settings.","sentences":["Image compression emerges as a pivotal tool in the efficient handling and transmission of digital images.","Its ability to substantially reduce file size not only facilitates enhanced data storage capacity but also potentially brings advantages to the development of continual machine learning (ML) systems, which learn new knowledge incrementally from sequential data.","Continual ML systems often rely on storing representative samples, also known as exemplars, within a limited memory constraint to maintain the performance on previously learned data.","These methods are known as memory replay-based algorithms and have proven effective at mitigating the detrimental effects of catastrophic forgetting.","Nonetheless, the limited memory buffer size often falls short of adequately representing the entire data distribution.","In this paper, we explore the use of image compression as a strategy to enhance the buffer's capacity, thereby increasing exemplar diversity.","However, directly using compressed exemplars introduces domain shift during continual ML, marked by a discrepancy between compressed training data and uncompressed testing data.","Additionally, it is essential to determine the appropriate compression algorithm and select the most effective rate for continual ML systems to balance the trade-off between exemplar quality and quantity.","To this end, we introduce a new framework to incorporate image compression for continual ML including a pre-processing data compression step and an efficient compression rate/algorithm selection method.","We conduct extensive experiments on CIFAR-100 and ImageNet datasets and show that our method significantly improves image classification accuracy in continual ML settings."],"url":"http://arxiv.org/abs/2403.06288v1","category":"cs.CV"}
{"created":"2024-03-11 17:47:40","title":"Decorrelation of a leader by the increasing number of followers","abstract":"We compute the connected two-time correlator of the maximum $M_N(t)$ of $N$ independent Gaussian stochastic processes (GSP) characterised by a common correlation coefficient $\\rho$ that depends on the two times $t_1$ and $t_2$. We show analytically that this correlator, for fixed times $t_1$ and $t_2$, decays for large $N$ as a power law $N^{-\\gamma}$ (with logarithmic corrections) with a decorrelation exponent $\\gamma = (1-\\rho)/(1+ \\rho)$ that depends only on $\\rho$, but otherwise is universal for any GSP. We study several examples of physical processes including the fractional Brownian motion (fBm) with Hurst exponent $H$ and the Ornstein-Uhlenbeck (OU) process. For the fBm, $\\rho$ is only a function of $\\tau = \\sqrt{t_1/t_2}$ and we find an interesting ``freezing'' transition at a critical value $\\tau= \\tau_c=(3-\\sqrt{5})/2$. For $\\tau < \\tau_c$, there is an optimal $H^*(\\tau) > 0$ that maximises the exponent $\\gamma$ and this maximal value freezes to $\\gamma= 1/3$ for $\\tau >\\tau_c$. For the OU process, we show that $\\gamma = {\\rm tanh}(\\mu \\,|t_1-t_2|/2)$ where $\\mu$ is the stiffness of the harmonic trap. Numerical simulations confirm our analytical predictions.","sentences":["We compute the connected two-time correlator of the maximum $M_N(t)$ of $N$ independent Gaussian stochastic processes (GSP) characterised by a common correlation coefficient $\\rho$ that depends on the two times $t_1$ and $t_2$. We show analytically that this correlator, for fixed times $t_1$ and $t_2$, decays for large $N$ as a power law $N^{-\\gamma}$ (with logarithmic corrections) with a decorrelation exponent $\\gamma = (1-\\rho)/(1+ \\rho)$ that depends only on $\\rho$, but otherwise is universal for any GSP.","We study several examples of physical processes including the fractional Brownian motion (fBm) with Hurst exponent $H$ and the Ornstein-Uhlenbeck (OU) process.","For the fBm, $\\rho$ is only a function of $\\tau = \\sqrt{t_1/t_2}$ and we find an interesting ``freezing'' transition at a critical value $\\tau= \\tau_c=(3-\\sqrt{5})/2$. For $\\tau < \\tau_c$, there is an optimal $H^*(\\tau) > 0$ that maximises the exponent $\\gamma$ and this maximal value freezes to $\\gamma= 1/3$ for $\\tau >\\tau_c$.","For the OU process, we show that $\\gamma = {\\rm tanh}(\\mu \\,|t_1-t_2|/2)$ where $\\mu$ is the stiffness of the harmonic trap.","Numerical simulations confirm our analytical predictions."],"url":"http://arxiv.org/abs/2403.06964v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-11 17:18:50","title":"Optimizing sDTW for AMD GPUs","abstract":"Subsequence Dynamic Time Warping (sDTW) is the metric of choice when performing many sequence matching and alignment tasks. While sDTW is flexible and accurate, it is neither simple nor fast to compute; significant research effort has been spent devising parallel implementations on the GPU that leverage efficient memory access and computation patterns, as well as features offered by specific vendors and architectures (notably NVIDIA's). We present an implementation of sDTW on AMD hardware using HIP and ROCm. Our implementation employs well-known parallel patterns, as well as lower-level features offered by ROCm. We use shuffling for intra-wavefront communication and shared memory to transfer data between consecutive wavefronts. By constraining the input data to batches of 512 queries of length 2,000, we optimized for peak performance the width of reference elements operated on by a single thread.","sentences":["Subsequence Dynamic Time Warping (sDTW) is the metric of choice when performing many sequence matching and alignment tasks.","While sDTW is flexible and accurate, it is neither simple nor fast to compute; significant research effort has been spent devising parallel implementations on the GPU that leverage efficient memory access and computation patterns, as well as features offered by specific vendors and architectures (notably NVIDIA's).","We present an implementation of sDTW on AMD hardware using HIP and ROCm.","Our implementation employs well-known parallel patterns, as well as lower-level features offered by ROCm.","We use shuffling for intra-wavefront communication and shared memory to transfer data between consecutive wavefronts.","By constraining the input data to batches of 512 queries of length 2,000, we optimized for peak performance the width of reference elements operated on by a single thread."],"url":"http://arxiv.org/abs/2403.06931v1","category":"cs.DC"}
{"created":"2024-03-11 14:18:07","title":"Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, Evaluation Strategies, and Future Challenges","abstract":"Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results. However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks. This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks? The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.","sentences":["Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results.","However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks.","This paper reviews the current implementations of LLMs in PM and reflects on three different questions.","1) What is the minimal set of capabilities required for PM on LLMs?","2) Which benchmark strategies help choose optimal LLMs for PM?","3) How do we evaluate the output of LLMs on specific PM tasks?","The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms."],"url":"http://arxiv.org/abs/2403.06749v1","category":"cs.DB"}
{"created":"2024-03-11 13:23:52","title":"Multimodal Transformers for Real-Time Surgical Activity Prediction","abstract":"Real-time recognition and prediction of surgical activities are fundamental to advancing safety and autonomy in robot-assisted surgery. This paper presents a multimodal transformer architecture for real-time recognition and prediction of surgical gestures and trajectories based on short segments of kinematic and video data. We conduct an ablation study to evaluate the impact of fusing different input modalities and their representations on gesture recognition and prediction performance. We perform an end-to-end assessment of the proposed architecture using the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) dataset. Our model outperforms the state-of-the-art (SOTA) with 89.5\\% accuracy for gesture prediction through effective fusion of kinematic features with spatial and contextual video features. It achieves the real-time performance of 1.1-1.3ms for processing a 1-second input window by relying on a computationally efficient model.","sentences":["Real-time recognition and prediction of surgical activities are fundamental to advancing safety and autonomy in robot-assisted surgery.","This paper presents a multimodal transformer architecture for real-time recognition and prediction of surgical gestures and trajectories based on short segments of kinematic and video data.","We conduct an ablation study to evaluate the impact of fusing different input modalities and their representations on gesture recognition and prediction performance.","We perform an end-to-end assessment of the proposed architecture using the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) dataset.","Our model outperforms the state-of-the-art (SOTA) with 89.5\\% accuracy for gesture prediction through effective fusion of kinematic features with spatial and contextual video features.","It achieves the real-time performance of 1.1-1.3ms for processing a 1-second input window by relying on a computationally efficient model."],"url":"http://arxiv.org/abs/2403.06705v1","category":"cs.RO"}
{"created":"2024-03-11 13:10:58","title":"A Minority of C++ Objects Account for the Majority of Allocation CPU Time","abstract":"In C++, an object can be allocated in static memory, on the stack, or on the heap, where the latter is by the order of magnitude more expensive operation, performance wise, than the first two. However, it is not clear how much overall performance loss may be attributed to the use of on-heap objects in C++ applications. This study aims to fill this gap by analyzing object allocation practices in open-source C++ code, investigating the frequency of stack and heap allocations using real-time dynamic analysis with tools such as DynamoRIO and Valgrind. We found out that the majority of objects (97.2%) are allocated on the stack, with only a small portion (2.8%) allocated on the heap. However, when considering the computational cost of each allocation method, we find that heap allocations account for a substantial 85% of the total CPU cycles consumed by object allocations. These findings underscore the importance of optimization of on-heap object allocations, in C++ programming.","sentences":["In C++, an object can be allocated in static memory, on the stack, or on the heap, where the latter is by the order of magnitude more expensive operation, performance wise, than the first two.","However, it is not clear how much overall performance loss may be attributed to the use of on-heap objects in C++ applications.","This study aims to fill this gap by analyzing object allocation practices in open-source C++ code, investigating the frequency of stack and heap allocations using real-time dynamic analysis with tools such as DynamoRIO and Valgrind.","We found out that the majority of objects (97.2%) are allocated on the stack, with only a small portion (2.8%) allocated on the heap.","However, when considering the computational cost of each allocation method, we find that heap allocations account for a substantial 85% of the total CPU cycles consumed by object allocations.","These findings underscore the importance of optimization of on-heap object allocations, in C++ programming."],"url":"http://arxiv.org/abs/2403.06695v1","category":"cs.PL"}
{"created":"2024-03-11 13:02:13","title":"Non-convex relaxation and 1/2-approximation algorithm for the chance-constrained binary knapsack problem","abstract":"We consider the chance-constrained binary knapsack problem (CKP), where the item weights are independent and normally distributed. We introduce a continuous relaxation for the CKP, represented as a non-convex optimization problem, which we call the non-convex relaxation. A comparative study shows that the non-convex relaxation provides an upper bound for the CKP, at least as tight as those obtained from other continuous relaxations for the CKP. Furthermore, the quality of the obtained upper bound is guaranteed to be at most twice the optimal objective value of the CKP. Despite its non-convex nature, we show that the non-convex relaxation can be solved in polynomial time. Subsequently, we proposed a polynomial-time 1/2-approximation algorithm for the CKP based on this relaxation, providing a lower bound for the CKP. Computational test results demonstrate that the non-convex relaxation and the proposed approximation algorithm yields tight lower and upper bounds for the CKP within a short computation time, ensuring the quality of the obtained bounds.","sentences":["We consider the chance-constrained binary knapsack problem (CKP), where the item weights are independent and normally distributed.","We introduce a continuous relaxation for the CKP, represented as a non-convex optimization problem, which we call the non-convex relaxation.","A comparative study shows that the non-convex relaxation provides an upper bound for the CKP, at least as tight as those obtained from other continuous relaxations for the CKP.","Furthermore, the quality of the obtained upper bound is guaranteed to be at most twice the optimal objective value of the CKP.","Despite its non-convex nature, we show that the non-convex relaxation can be solved in polynomial time.","Subsequently, we proposed a polynomial-time 1/2-approximation algorithm for the CKP based on this relaxation, providing a lower bound for the CKP.","Computational test results demonstrate that the non-convex relaxation and the proposed approximation algorithm yields tight lower and upper bounds for the CKP within a short computation time, ensuring the quality of the obtained bounds."],"url":"http://arxiv.org/abs/2403.06686v1","category":"math.OC"}
{"created":"2024-03-11 12:37:47","title":"Performance of Expansive Soil Stabilized with Bamboo Charcoal, Quarry Dust, and Lime for Use as Road Subgrade Material","abstract":"Expansive soils such as Black Cotton Soils (BCS) present significant challenges for road subgrade construction due to their high plasticity, swelling potential, and low strength. This study explores a triphasic stabilization method using Bamboo Charcoal (BC), Quarry Dust (QD), and Lime (L) to enhance the engineering properties of BCS for rural road applications. Initial soil characterization involved standard tests, including Atterberg limits, compaction, and Californian Bearing Ratio (CBR) assessments. The soil was treated with varying BC proportions (5% to 35% at 5% intervals) in the initial phase, leading to a progressive reduction in the Plasticity Index (PI) and swell index and an enhancement in the CBR up to 20% BC content. This further resulted in a soaked CBR value of 2.7%. In the second phase, additional treatment combined with BC and QD, incorporating diverse QD proportions (4% to 24%) relative to the optimal BC content. This further improved the CBR to 7.7% at 12% QD, but the PI exhibited a non-linear trend. Finally, 5% lime was introduced in the final phase. This minimized the PI to 11.2% and significantly increased the CBR to 19%. The optimal combination of 20% BC, 12% QD, and 5% Lime achieved optimal plasticity, compaction, and strength characteristics, demonstrating the viability of this approach for transforming BCS into a sustainable and cost-effective alternative for rural road subgrade construction.","sentences":["Expansive soils such as Black Cotton Soils (BCS) present significant challenges for road subgrade construction due to their high plasticity, swelling potential, and low strength.","This study explores a triphasic stabilization method using Bamboo Charcoal (BC), Quarry Dust (QD), and Lime (L) to enhance the engineering properties of BCS for rural road applications.","Initial soil characterization involved standard tests, including Atterberg limits, compaction, and Californian Bearing Ratio (CBR) assessments.","The soil was treated with varying BC proportions (5% to 35% at 5% intervals) in the initial phase, leading to a progressive reduction in the Plasticity Index (PI) and swell index and an enhancement in the CBR up to 20% BC content.","This further resulted in a soaked CBR value of 2.7%.","In the second phase, additional treatment combined with BC and QD, incorporating diverse QD proportions (4% to 24%) relative to the optimal BC content.","This further improved the CBR to 7.7% at 12% QD, but the PI exhibited a non-linear trend.","Finally, 5% lime was introduced in the final phase.","This minimized the PI to 11.2% and significantly increased the CBR to 19%.","The optimal combination of 20% BC, 12% QD, and 5% Lime achieved optimal plasticity, compaction, and strength characteristics, demonstrating the viability of this approach for transforming BCS into a sustainable and cost-effective alternative for rural road subgrade construction."],"url":"http://arxiv.org/abs/2403.06669v1","category":"cs.CE"}
{"created":"2024-03-11 10:31:20","title":"Strict hierarchy of optimal strategies for global estimations: Mapping global estimations into local ones","abstract":"A crucial yet challenging issue in quantum metrology is to ascertain the ultimate precision achievable in estimation strategies. While there are two paradigms of estimations, local and global, current research is largely confined to local estimations, which are useful once the parameter of interest is approximately known. In this Letter we target at a paradigm shift towards global estimations, which can operate reliably even with a few measurement data and no substantial prior knowledge about the parameter. The key innovation here is to develop a technique, dubbed virtual imaginary time evolution, capable of mapping global estimations into local ones. This offers an intriguing pathway to surmount challenges in the realm of global estimations by leveraging powerful tools tailored for local estimations. We explore our technique to unveil a strict hierarchy of achievable precision for different global estimation strategies, including parallel, sequential, and indefinite-causal-order strategies.","sentences":["A crucial yet challenging issue in quantum metrology is to ascertain the ultimate precision achievable in estimation strategies.","While there are two paradigms of estimations, local and global, current research is largely confined to local estimations, which are useful once the parameter of interest is approximately known.","In this Letter we target at a paradigm shift towards global estimations, which can operate reliably even with a few measurement data and no substantial prior knowledge about the parameter.","The key innovation here is to develop a technique, dubbed virtual imaginary time evolution, capable of mapping global estimations into local ones.","This offers an intriguing pathway to surmount challenges in the realm of global estimations by leveraging powerful tools tailored for local estimations.","We explore our technique to unveil a strict hierarchy of achievable precision for different global estimation strategies, including parallel, sequential, and indefinite-causal-order strategies."],"url":"http://arxiv.org/abs/2403.06585v1","category":"quant-ph"}
{"created":"2024-03-11 08:40:04","title":"Extreme Point Pursuit -- Part II: Further Error Bound Analysis and Applications","abstract":"In the first part of this study, a convex-constrained penalized formulation was studied for a class of constant modulus (CM) problems. In particular, the error bound techniques were shown to play a vital role in providing exact penalization results. In this second part of the study, we continue our error bound analysis for the cases of partial permutation matrices, size-constrained assignment matrices and non-negative semi-orthogonal matrices. We develop new error bounds and penalized formulations for these three cases, and the new formulations possess good structures for building computationally efficient algorithms. Moreover, we provide numerical results to demonstrate our framework in a variety of applications such as the densest k-subgraph problem, graph matching, size-constrained clustering, non-negative orthogonal matrix factorization and sparse fair principal component analysis.","sentences":["In the first part of this study, a convex-constrained penalized formulation was studied for a class of constant modulus (CM) problems.","In particular, the error bound techniques were shown to play a vital role in providing exact penalization results.","In this second part of the study, we continue our error bound analysis for the cases of partial permutation matrices, size-constrained assignment matrices and non-negative semi-orthogonal matrices.","We develop new error bounds and penalized formulations for these three cases, and the new formulations possess good structures for building computationally efficient algorithms.","Moreover, we provide numerical results to demonstrate our framework in a variety of applications such as the densest k-subgraph problem, graph matching, size-constrained clustering, non-negative orthogonal matrix factorization and sparse fair principal component analysis."],"url":"http://arxiv.org/abs/2403.06513v1","category":"eess.SP"}
{"created":"2024-03-11 08:25:53","title":"Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU","abstract":"Recent advances in large language models have brought immense value to the world, with their superior capabilities stemming from the massive number of parameters they utilize. However, even the GPUs with the highest memory capacities, currently peaking at 80GB, are far from sufficient to accommodate these vast parameters and their associated optimizer states when conducting stochastic gradient descent-based optimization. One approach to hosting such huge models is to aggregate device memory from many GPUs. However, this approach introduces prohibitive costs for most academic researchers, who always have a limited budget for many high-end GPU servers. In this paper, we focus on huge model fine-tuning on a single, even low-end, GPU in a commodity server, which is accessible to most AI researchers. In such a scenario, the state-of-the-art work ZeRO-Infinity suffers from two severe issues when running in a commodity server: 1) low GPU utilization due to inefficient swapping, and 2) limited trainable model size due to CPU memory capacity. The underlying reason is that ZeRO-Infinity is optimized for running on high-end GPU servers. To this end, we present Fuyou, a low-cost training framework that enables efficient 100B huge model fine-tuning on a low-end server with a low-end GPU and limited CPU memory capacity. The key idea is to add the SSD-CPU communication as an optimization dimension and thus carefully co-optimize computation and data swapping from a systematic approach to maximize GPU utilization. The experimental results show that 1) Fuyou is able to fine-tune 175B GPT-3 on a consumer GPU RTX 4090 with high GPU utilization, while ZeRO-Infinity fails to fine-tune; and 2) when training a small GPT-3 13B model, Fuyou achieves 156 TFLOPS on an RTX 4090 GPU while ZeRO-Infinity only achieves 45 TFLOPS.","sentences":["Recent advances in large language models have brought immense value to the world, with their superior capabilities stemming from the massive number of parameters they utilize.","However, even the GPUs with the highest memory capacities, currently peaking at 80GB, are far from sufficient to accommodate these vast parameters and their associated optimizer states when conducting stochastic gradient descent-based optimization.","One approach to hosting such huge models is to aggregate device memory from many GPUs.","However, this approach introduces prohibitive costs for most academic researchers, who always have a limited budget for many high-end GPU servers.","In this paper, we focus on huge model fine-tuning on a single, even low-end, GPU in a commodity server, which is accessible to most AI researchers.","In such a scenario, the state-of-the-art work ZeRO-Infinity suffers from two severe issues when running in a commodity server: 1) low GPU utilization due to inefficient swapping, and 2) limited trainable model size due to CPU memory capacity.","The underlying reason is that ZeRO-Infinity is optimized for running on high-end GPU servers.","To this end, we present Fuyou, a low-cost training framework that enables efficient 100B huge model fine-tuning on a low-end server with a low-end GPU and limited CPU memory capacity.","The key idea is to add the SSD-CPU communication as an optimization dimension and thus carefully co-optimize computation and data swapping from a systematic approach to maximize GPU utilization.","The experimental results show that 1) Fuyou is able to fine-tune 175B GPT-3 on a consumer GPU RTX 4090 with high GPU utilization, while ZeRO-Infinity fails to fine-tune; and 2) when training a small GPT-3 13B model, Fuyou achieves 156 TFLOPS on an RTX 4090 GPU while ZeRO-Infinity only achieves 45 TFLOPS."],"url":"http://arxiv.org/abs/2403.06504v1","category":"cs.DC"}
{"created":"2024-03-11 06:46:31","title":"From Pixel to Cancer: Cellular Automata in Computed Tomography","abstract":"AI for cancer detection encounters the bottleneck of data scarcity, annotation difficulty, and low prevalence of early tumors. Tumor synthesis seeks to create artificial tumors in medical images, which can greatly diversify the data and annotations for AI training. However, current tumor synthesis approaches are not applicable across different organs due to their need for specific expertise and design. This paper establishes a set of generic rules to simulate tumor development. Each cell (pixel) is initially assigned a state between zero and ten to represent the tumor population, and a tumor can be developed based on three rules to describe the process of growth, invasion, and death. We apply these three generic rules to simulate tumor development--from pixel to cancer--using cellular automata. We then integrate the tumor state into the original computed tomography (CT) images to generate synthetic tumors across different organs. This tumor synthesis approach allows for sampling tumors at multiple stages and analyzing tumor-organ interaction. Clinically, a reader study involving three expert radiologists reveals that the synthetic tumors and their developing trajectories are convincingly realistic. Technically, we generate tumors at varied stages in 9,262 raw, unlabeled CT images sourced from 68 hospitals worldwide. The performance in segmenting tumors in the liver, pancreas, and kidneys exceeds prevailing literature benchmarks, underlining the immense potential of tumor synthesis, especially for earlier cancer detection. The code and models are available at https://github.com/MrGiovanni/Pixel2Cancer","sentences":["AI for cancer detection encounters the bottleneck of data scarcity, annotation difficulty, and low prevalence of early tumors.","Tumor synthesis seeks to create artificial tumors in medical images, which can greatly diversify the data and annotations for AI training.","However, current tumor synthesis approaches are not applicable across different organs due to their need for specific expertise and design.","This paper establishes a set of generic rules to simulate tumor development.","Each cell (pixel) is initially assigned a state between zero and ten to represent the tumor population, and a tumor can be developed based on three rules to describe the process of growth, invasion, and death.","We apply these three generic rules to simulate tumor development--from pixel to cancer--using cellular automata.","We then integrate the tumor state into the original computed tomography (CT) images to generate synthetic tumors across different organs.","This tumor synthesis approach allows for sampling tumors at multiple stages and analyzing tumor-organ interaction.","Clinically, a reader study involving three expert radiologists reveals that the synthetic tumors and their developing trajectories are convincingly realistic.","Technically, we generate tumors at varied stages in 9,262 raw, unlabeled CT images sourced from 68 hospitals worldwide.","The performance in segmenting tumors in the liver, pancreas, and kidneys exceeds prevailing literature benchmarks, underlining the immense potential of tumor synthesis, especially for earlier cancer detection.","The code and models are available at https://github.com/MrGiovanni/Pixel2Cancer"],"url":"http://arxiv.org/abs/2403.06459v1","category":"eess.IV"}
{"created":"2024-03-11 03:58:45","title":"Elimination by Substitution","abstract":"Let $K$ be a field and $P=K[x_1,\\dots,x_n]$. The technique of elimination by substitution is based on discovering a coherently $Z=(z_1,\\dots,z_s)$-separating tuple of polynomials $(f_1,\\dots,f_s)$ in an ideal $I$, i.e., on finding polynomials such that $f_i = z_i - h_i$ with $h_i \\in K[X \\setminus Z]$. Here we elaborate on this technique in the case when $P$ is non-negatively graded. The existence of a coherently $Z$-separating tuple is reduced to solving several $P_0$-module membership problems. Best separable re-embeddings, i.e., isomorphisms $P/I \\longrightarrow K[X \\setminus Z] / (I \\cap K[X \\setminus Z])$ with maximal $\\#Z$, are found degree-by-degree. They turn out to yield optimal re-embeddings in the positively graded case. Viewing $P_0 \\longrightarrow P/I$ as a fibration over an affine space, we show that its fibers allow optimal $Z$-separating re-embeddings, and we provide a criterion for a fiber to be isomorphic to an affine space. In the last section we introduce a new technique based on the solution of a unimodular matrix problem which enables us to construct automorphisms of $P$ such that additional $Z$-separating re-embeddings are possible. One of the main outcomes is an algorithm which allows us to explicitly compute a homogeneous isomorphism between $P/I$ and a non-negatively graded polynomial ring if $P/I$ is regular.","sentences":["Let $K$ be a field and $P=K[x_1,\\dots,x_n]$.","The technique of elimination by substitution is based on discovering a coherently $Z=(z_1,\\dots,z_s)$-separating tuple of polynomials $(f_1,\\dots,f_s)$ in an ideal $I$, i.e., on finding polynomials such that $f_i = z_i - h_i$ with $h_i \\in K[X \\setminus Z]$.","Here we elaborate on this technique in the case when $P$ is non-negatively graded.","The existence of a coherently $Z$-separating tuple is reduced to solving several $P_0$-module membership problems.","Best separable re-embeddings, i.e., isomorphisms $P/I \\longrightarrow K[X \\setminus Z] / (I \\cap K[X \\setminus Z])$ with maximal $\\#Z$, are found degree-by-degree.","They turn out to yield optimal re-embeddings in the positively graded case.","Viewing $P_0 \\longrightarrow P/I$ as a fibration over an affine space, we show that its fibers allow optimal $Z$-separating re-embeddings, and we provide a criterion for a fiber to be isomorphic to an affine space.","In the last section we introduce a new technique based on the solution of a unimodular matrix problem which enables us to construct automorphisms of $P$ such that additional $Z$-separating re-embeddings are possible.","One of the main outcomes is an algorithm which allows us to explicitly compute a homogeneous isomorphism between $P/I$ and a non-negatively graded polynomial ring if $P/I$ is regular."],"url":"http://arxiv.org/abs/2403.06415v1","category":"math.AC"}
{"created":"2024-03-11 02:18:27","title":"Enhancing Semantic Fidelity in Text-to-Image Synthesis: Attention Regulation in Diffusion Models","abstract":"Recent advancements in diffusion models have notably improved the perceptual quality of generated images in text-to-image synthesis tasks. However, diffusion models often struggle to produce images that accurately reflect the intended semantics of the associated text prompts. We examine cross-attention layers in diffusion models and observe a propensity for these layers to disproportionately focus on certain tokens during the generation process, thereby undermining semantic fidelity. To address the issue of dominant attention, we introduce attention regulation, a computation-efficient on-the-fly optimization approach at inference time to align attention maps with the input text prompt. Notably, our method requires no additional training or fine-tuning and serves as a plug-in module on a model. Hence, the generation capacity of the original model is fully preserved. We compare our approach with alternative approaches across various datasets, evaluation metrics, and diffusion models. Experiment results show that our method consistently outperforms other baselines, yielding images that more faithfully reflect the desired concepts with reduced computation overhead. Code is available at https://github.com/YaNgZhAnG-V5/attention_regulation.","sentences":["Recent advancements in diffusion models have notably improved the perceptual quality of generated images in text-to-image synthesis tasks.","However, diffusion models often struggle to produce images that accurately reflect the intended semantics of the associated text prompts.","We examine cross-attention layers in diffusion models and observe a propensity for these layers to disproportionately focus on certain tokens during the generation process, thereby undermining semantic fidelity.","To address the issue of dominant attention, we introduce attention regulation, a computation-efficient on-the-fly optimization approach at inference time to align attention maps with the input text prompt.","Notably, our method requires no additional training or fine-tuning and serves as a plug-in module on a model.","Hence, the generation capacity of the original model is fully preserved.","We compare our approach with alternative approaches across various datasets, evaluation metrics, and diffusion models.","Experiment results show that our method consistently outperforms other baselines, yielding images that more faithfully reflect the desired concepts with reduced computation overhead.","Code is available at https://github.com/YaNgZhAnG-V5/attention_regulation."],"url":"http://arxiv.org/abs/2403.06381v1","category":"cs.CV"}
{"created":"2024-03-11 01:56:36","title":"Dynamic characteristics of terahertz hot-electron graphene FET bolometers: effect of electron cooling in channel and at side contacts","abstract":"We analyze the operation of the hot-electron FET bolometers with the graphene channels (GCs) and the gate barrier layers (BLs). Such bolometers use the thermionic emission of the hot electrons heated by incident modulated THz radiation. The hot electron transfer from the GC into the metal gate. As the THz detectors, these bolometers can operate at room temperature. We show that the response and ultimate modulation frequency of the GC-FET bolometers are determined by the efficiency of the hot-electron energy transfer to the lattice and the GC side contacts due to the 2DEG lateral thermal conductance. The dependences of these mechanisms on the band structure and geometrical parameters open the way for the GC-FET bolometers optimization, in particular, for the enhancement of the maximum modulation frequency.","sentences":["We analyze the operation of the hot-electron FET bolometers with the graphene channels (GCs) and the gate barrier layers (BLs).","Such bolometers use the thermionic emission of the hot electrons heated by incident modulated THz radiation.","The hot electron transfer from the GC into the metal gate.","As the THz detectors, these bolometers can operate at room temperature.","We show that the response and ultimate modulation frequency of the GC-FET bolometers are determined by the efficiency of the hot-electron energy transfer to the lattice and the GC side contacts due to the 2DEG lateral thermal conductance.","The dependences of these mechanisms on the band structure and geometrical parameters open the way for the GC-FET bolometers optimization, in particular, for the enhancement of the maximum modulation frequency."],"url":"http://arxiv.org/abs/2403.06373v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 01:27:57","title":"MP2-based composite extrapolation schemes can predict core-ionization energies for first-row elements with coupled-cluster level accuracy","abstract":"X-ray photoelectron spectroscopy (XPS) measures core-electron binding energies (CEBEs) to reveal element-specific insights into chemical environment and bonding. Accurate theoretical CEBE prediction aids XPS interpretation but requires proper modeling of orbital relaxation and electron correlation upon core-ionization. This work systematically investigates basis set selection for extrapolation to the complete basis set (CBS) limit of CEBEs from $\\Delta$MP2 and $\\Delta$CC energies across 94 K-edges in diverse organic molecules. We demonstrate that an alternative composite scheme using $\\Delta$MP2 in a large basis corrected by $\\Delta$CC-$\\Delta$MP2 difference in a small basis can quantitatively recover optimally extrapolated $\\Delta$CC CEBEs within 0.02 eV. Unlike $\\Delta$CC, MP2 calculations do not suffer from convergence issues and are computationally cheaper, and, thus, the composite $\\Delta$MP2/$\\Delta$CC scheme balances accuracy and cost, overcoming limitations of solely using either method. We conclude by providing a comprehensive analysis of the choice of small and large basis sets for the composite schemes and provide practical recommendations for highly accurate (within 0.10-0.15 eV MAE) ab initio prediction of XPS spectra.","sentences":["X-ray photoelectron spectroscopy (XPS) measures core-electron binding energies (CEBEs) to reveal element-specific insights into chemical environment and bonding.","Accurate theoretical CEBE prediction aids XPS interpretation but requires proper modeling of orbital relaxation and electron correlation upon core-ionization.","This work systematically investigates basis set selection for extrapolation to the complete basis set (CBS) limit of CEBEs from $\\Delta$MP2 and $\\Delta$CC energies across 94 K-edges in diverse organic molecules.","We demonstrate that an alternative composite scheme using $\\Delta$MP2 in a large basis corrected by $\\Delta$CC-$\\Delta$MP2 difference in a small basis can quantitatively recover optimally extrapolated $\\Delta$CC CEBEs within 0.02 eV. Unlike $\\Delta$CC, MP2 calculations do not suffer from convergence issues and are computationally cheaper, and, thus, the composite $\\Delta$MP2/$\\Delta$CC scheme balances accuracy and cost, overcoming limitations of solely using either method.","We conclude by providing a comprehensive analysis of the choice of small and large basis sets for the composite schemes and provide practical recommendations for highly accurate (within 0.10-0.15 eV MAE) ab initio prediction of XPS spectra."],"url":"http://arxiv.org/abs/2403.06364v1","category":"physics.chem-ph"}
{"created":"2024-03-10 21:04:17","title":"Higher-order spring-coupled multilevel Monte Carlo method for invariant measures","abstract":"A higher-order change-of-measure multilevel Monte Carlo (MLMC) method is developed for computing weak approximations of the invariant measures of SDE with drift coefficients that do not satisfy the contractivity condition. This is achieved by introducing a spring term in the pairwise coupling of the MLMC trajectories employing the order 1.5 strong It\\^o--Taylor method. Through this, we can recover the contractivity property of the drift coefficient while still retaining the telescoping sum property needed for implementing the MLMC method.   We show that the variance of the change-of-measure MLMC method grows linearly in time $T$ for all $T > 0$, and for all sufficiently small timestep size $h > 0$. For a given error tolerance $\\epsilon > 0$, we prove that the method achieves a mean-square-error accuracy of $O(\\epsilon^2)$ with a computational cost of $O(\\epsilon^{-2} \\big\\vert \\log \\epsilon \\big\\vert^{3/2} (\\log \\big\\vert \\log \\epsilon \\big\\vert)^{1/2})$ for uniformly Lipschitz continuous payoff functions and $O \\big( \\epsilon^{-2} \\big\\vert \\log \\epsilon \\big\\vert^{5/3 + \\xi} \\big)$ for discontinuous payoffs, respectively, where $\\xi > 0$. We also observe an improvement in the constant associated with the computational cost of the higher-order change-of-measure MLMC method, marking an improvement over the Milstein change-of-measure method in the aforementioned seminal work by M. Giles and W. Fang. Several numerical tests were performed to verify the theoretical results and assess the robustness of the method.","sentences":["A higher-order change-of-measure multilevel Monte Carlo (MLMC) method is developed for computing weak approximations of the invariant measures of SDE with drift coefficients that do not satisfy the contractivity condition.","This is achieved by introducing a spring term in the pairwise coupling of the MLMC trajectories employing the order 1.5 strong It\\^o--Taylor method.","Through this, we can recover the contractivity property of the drift coefficient while still retaining the telescoping sum property needed for implementing the MLMC method.   ","We show that the variance of the change-of-measure MLMC method grows linearly in time $T$ for all $T > 0$, and for all sufficiently small timestep size $h > 0$.","For a given error tolerance $\\epsilon > 0$, we prove that the method achieves a mean-square-error accuracy of $O(\\epsilon^2)$ with a computational cost of $O(\\epsilon^{-2} \\big\\vert \\log \\epsilon \\big\\vert^{3/2} (\\log \\big\\vert \\log \\epsilon \\big\\vert)^{1/2})$ for uniformly Lipschitz continuous payoff functions and $O \\big( \\epsilon^{-2} \\big\\vert \\log \\epsilon \\big\\vert^{5/3 + \\xi} \\big)$ for discontinuous payoffs, respectively, where $\\xi > 0$. We also observe an improvement in the constant associated with the computational cost of the higher-order change-of-measure MLMC method, marking an improvement over the Milstein change-of-measure method in the aforementioned seminal work by M. Giles and W. Fang.","Several numerical tests were performed to verify the theoretical results and assess the robustness of the method."],"url":"http://arxiv.org/abs/2403.06310v1","category":"math.NA"}
{"created":"2024-03-10 20:53:21","title":"Use of Nash equilibrium in finding game theoretic robust security bound on quantum bit error rate","abstract":"Nash equilibrium is employed to find a game theoretic robust security bound on quantum bit error rate (QBER) for DL04 protocol which is a scheme for quantum secure direct communication that has been experimentally realized recently. The receiver, sender, and eavesdropper (Eve) are considered to be quantum players (players having the capability to perform quantum operations). Specifically, Eve is considered to have the capability of performing quantum attacks (e.g., W\\'ojcik's original attack, W\\'ojcik's symmetrized attack, and Pavi\\v{c}i\\'c attack) and classical intercept and resend attack. Game theoretic analysis of the security of DL04 protocol in the above scenario is performed by considering several sub-game scenarios. The analysis revealed the absence of a Pareto optimal Nash equilibrium point within these sub-games. Consequently, mixed strategy Nash equilibrium points are identified and employed to establish both upper and lower bounds for QBER. Further, the vulnerability of the DL04 protocol to Pavi\\v{c}i\\'c attack in the message mode is established. In addition, it is observed that the quantum attacks performed by Eve are more powerful than the classical attack, as the QBER value and the probability of detecting Eve's presence are found to be lower in quantum attacks compared to classical ones.","sentences":["Nash equilibrium is employed to find a game theoretic robust security bound on quantum bit error rate (QBER) for DL04 protocol which is a scheme for quantum secure direct communication that has been experimentally realized recently.","The receiver, sender, and eavesdropper (Eve) are considered to be quantum players (players having the capability to perform quantum operations).","Specifically, Eve is considered to have the capability of performing quantum attacks (e.g., W\\'ojcik's original attack, W\\'ojcik's symmetrized attack, and Pavi\\v{c}i\\'c attack) and classical intercept and resend attack.","Game theoretic analysis of the security of DL04 protocol in the above scenario is performed by considering several sub-game scenarios.","The analysis revealed the absence of a Pareto optimal Nash equilibrium point within these sub-games.","Consequently, mixed strategy Nash equilibrium points are identified and employed to establish both upper and lower bounds for QBER.","Further, the vulnerability of the DL04 protocol to Pavi\\v{c}i\\'c attack in the message mode is established.","In addition, it is observed that the quantum attacks performed by Eve are more powerful than the classical attack, as the QBER value and the probability of detecting Eve's presence are found to be lower in quantum attacks compared to classical ones."],"url":"http://arxiv.org/abs/2403.06309v1","category":"quant-ph"}
{"created":"2024-03-10 19:20:11","title":"Revisiting Path Contraction and Cycle Contraction","abstract":"The Path Contraction and Cycle Contraction problems take as input an undirected graph $G$ with $n$ vertices, $m$ edges and an integer $k$ and determine whether one can obtain a path or a cycle, respectively, by performing at most $k$ edge contractions in $G$. We revisit these NP-complete problems and prove the following results. Path Contraction admits an algorithm running in $\\mathcal{O}^*(2^{k})$ time. This improves over the current algorithm known for the problem [Algorithmica 2014]. Cycle Contraction admits an algorithm running in $\\mathcal{O}^*((2 + \\epsilon_{\\ell})^k)$ time where $0 < \\epsilon_{\\ell} \\leq 0.5509$ is inversely proportional to $\\ell = n - k$.   Central to these results is an algorithm for a general variant of Path Contraction, namely, Path Contraction With Constrained Ends. We also give an $\\mathcal{O}^*(2.5191^n)$-time algorithm to solve the optimization version of Cycle Contraction.   Next, we turn our attention to restricted graph classes and show the following results. Path Contraction on planar graphs admits a polynomial-time algorithm. Path Contraction on chordal graphs does not admit an algorithm running in time $\\mathcal{O}(n^{2-\\epsilon} \\cdot 2^{o(tw)})$ for any $\\epsilon > 0$, unless the Orthogonal Vectors Conjecture fails. Here, $tw$ is the treewidth of the input graph. The second result complements the $\\mathcal{O}(nm)$-time, i.e., $\\mathcal{O}(n^2 \\cdot tw)$-time, algorithm known for the problem [Discret. Appl. Math. 2014].","sentences":["The Path Contraction and Cycle Contraction problems take as input an undirected graph $G$ with $n$ vertices, $m$ edges and an integer $k$ and determine whether one can obtain a path or a cycle, respectively, by performing at most $k$ edge contractions in $G$. We revisit these NP-complete problems and prove the following results.","Path Contraction admits an algorithm running in $\\mathcal{O}^*(2^{k})$ time.","This improves over the current algorithm known for the problem [Algorithmica 2014].","Cycle Contraction admits an algorithm running in $\\mathcal{O}^*((2 + \\epsilon_{\\ell})^k)$ time where $0 < \\epsilon_{\\ell} \\leq 0.5509$ is inversely proportional to $\\ell = n - k$.   Central to these results is an algorithm for a general variant of Path Contraction, namely, Path Contraction With Constrained Ends.","We also give an $\\mathcal{O}^*(2.5191^n)$-time algorithm to solve the optimization version of Cycle Contraction.   ","Next, we turn our attention to restricted graph classes and show the following results.","Path Contraction on planar graphs admits a polynomial-time algorithm.","Path Contraction on chordal graphs does not admit an algorithm running in time $\\mathcal{O}(n^{2-\\epsilon} \\cdot 2^{o(tw)})$ for any $\\epsilon > 0$, unless the Orthogonal Vectors Conjecture fails.","Here, $tw$ is the treewidth of the input graph.","The second result complements the $\\mathcal{O}(nm)$-time, i.e., $\\mathcal{O}(n^2 \\cdot tw)$-time, algorithm known for the problem","[Discret.","Appl.","Math. 2014]."],"url":"http://arxiv.org/abs/2403.06290v1","category":"cs.DS"}
{"created":"2024-03-10 18:13:22","title":"Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond","abstract":"This paper aims to develop and provide a rigorous treatment to the problem of entropy regularized fine-tuning in the context of continuous-time diffusion models, which was recently proposed by Uehara et al. ( arXiv:2402.15194, 2024). We also show how the analysis can be extended to fine-tuning involving a general $f$-divergence regularizer.","sentences":["This paper aims to develop and provide a rigorous treatment to the problem of entropy regularized fine-tuning in the context of continuous-time diffusion models, which was recently proposed by Uehara et al.","( arXiv:2402.15194, 2024).","We also show how the analysis can be extended to fine-tuning involving a general $f$-divergence regularizer."],"url":"http://arxiv.org/abs/2403.06279v1","category":"math.OC"}
{"created":"2024-03-10 18:09:21","title":"Pre- and Post-Auction Discounts in First-Price Auctions","abstract":"One method to offer some bidders a discount in a first-price auction is to augment their bids when selecting a winner but only charge them their original bids should they win. Another method is to use their original bids to select a winner, then charge them a discounted price that is lower than their bid should they win. We show that the two methods have equivalent auction outcomes, for equal additive discounts and for multiplicative ones with appropriate adjustments to discount amounts. As a result, they have corresponding equilibria when equilibria exist. We also show that with the same level of multiplicative adjustments, bidders with discounts should prefer an augmented bid to a discounted price. Then we estimate optimal bid functions for valuation distributions based on data from online advertising auctions, and show how different discount levels affect auction outcomes for those bid functions.","sentences":["One method to offer some bidders a discount in a first-price auction is to augment their bids when selecting a winner but only charge them their original bids should they win.","Another method is to use their original bids to select a winner, then charge them a discounted price that is lower than their bid should they win.","We show that the two methods have equivalent auction outcomes, for equal additive discounts and for multiplicative ones with appropriate adjustments to discount amounts.","As a result, they have corresponding equilibria when equilibria exist.","We also show that with the same level of multiplicative adjustments, bidders with discounts should prefer an augmented bid to a discounted price.","Then we estimate optimal bid functions for valuation distributions based on data from online advertising auctions, and show how different discount levels affect auction outcomes for those bid functions."],"url":"http://arxiv.org/abs/2403.06278v1","category":"cs.GT"}
{"created":"2024-03-10 16:15:42","title":"Spectral Lower Bounds for Local Search","abstract":"Local search is a powerful heuristic in optimization and computer science, the complexity of which has been studied in the white box and black box models. In the black box model, we are given a graph $G = (V,E)$ and oracle access to a function $f : V \\to \\mathbb{R}$. The local search problem is to find a vertex $v$ that is a local minimum, i.e. with $f(v) \\leq f(u)$ for all $(u,v) \\in E$, using as few queries to the oracle as possible.   We show that if a graph $G$ admits a lazy, irreducible, and reversible Markov chain with stationary distribution $\\pi$, then the randomized query complexity of local search on $G$ is $\\Omega\\left( \\frac{\\sqrt{n}}{t_{mix} \\cdot \\exp(3\\sigma)}\\right)$, where $t_{mix}$ is the mixing time of the chain and $\\sigma = \\max_{u,v \\in V(G)} \\frac{\\pi(v)}{\\pi(u)}.$ This theorem formally establishes a connection between the query complexity of local search and the mixing time of the fastest mixing Markov chain for the given graph. We also get several corollaries that lower bound the complexity as a function of the spectral gap, one of which slightly improves a result from prior work.","sentences":["Local search is a powerful heuristic in optimization and computer science, the complexity of which has been studied in the white box and black box models.","In the black box model, we are given a graph $G = (V,E)$ and oracle access to a function $f : V \\to \\mathbb{R}$.","The local search problem is to find a vertex $v$ that is a local minimum, i.e. with $f(v)","\\leq f(u)$ for all $(u,v) \\in E$, using as few queries to the oracle as possible.   ","We show that if a graph $G$ admits a lazy, irreducible, and reversible Markov chain with stationary distribution $\\pi$, then the randomized query complexity of local search on $G$ is $\\Omega\\left( \\frac{\\sqrt{n}}{t_{mix} \\cdot \\exp(3\\sigma)}\\right)$, where $t_{mix}$ is the mixing time of the chain and $\\sigma = \\max_{u,v \\in V(G)} \\frac{\\pi(v)}{\\pi(u)}.$ This theorem formally establishes a connection between the query complexity of local search and the mixing time of the fastest mixing Markov chain for the given graph.","We also get several corollaries that lower bound the complexity as a function of the spectral gap, one of which slightly improves a result from prior work."],"url":"http://arxiv.org/abs/2403.06248v1","category":"cs.CC"}
{"created":"2024-03-10 14:59:42","title":"Optimal parametric control of transport across a Josephson junction","abstract":"We present optimal control strategies for the DC transport across a Josephson junction. Specifically, we consider a junction in which the Josephson coupling is driven parametrically, with either a bichromatic or a trichromatic driving protocol, and optimize the prefactor of the 1/$\\omega$ divergence of the imaginary part of the conductivity. We demonstrate that for an optimal bichromatic protocol an enhancement of 70 can be reached, and for an optimal trichromatic protocol an enhancement of 135. This is motivated by pump-probe experiments that have demonstrated light-enhanced superconductivity along the c-axis of underdoped YBCO, where the junction serves as a minimal model for the c-axis coupling of superconducting layers. Therefore, the significant enhancement of superconductivity that we show for multi-frequency protocols demonstrates that the advancement of pump-probe technology towards these strategies is highly desirable.","sentences":["We present optimal control strategies for the DC transport across a Josephson junction.","Specifically, we consider a junction in which the Josephson coupling is driven parametrically, with either a bichromatic or a trichromatic driving protocol, and optimize the prefactor of the 1/$\\omega$ divergence of the imaginary part of the conductivity.","We demonstrate that for an optimal bichromatic protocol an enhancement of 70 can be reached, and for an optimal trichromatic protocol an enhancement of 135.","This is motivated by pump-probe experiments that have demonstrated light-enhanced superconductivity along the c-axis of underdoped YBCO, where the junction serves as a minimal model for the c-axis coupling of superconducting layers.","Therefore, the significant enhancement of superconductivity that we show for multi-frequency protocols demonstrates that the advancement of pump-probe technology towards these strategies is highly desirable."],"url":"http://arxiv.org/abs/2403.06229v1","category":"cond-mat.supr-con"}
{"created":"2024-03-10 13:50:36","title":"Fast Truncated SVD of Sparse and Dense Matrices on Graphics Processors","abstract":"We investigate the solution of low-rank matrix approximation problems using the truncated SVD. For this purpose, we develop and optimize GPU implementations for the randomized SVD and a blocked variant of the Lanczos approach. Our work takes advantage of the fact that the two methods are composed of very similar linear algebra building blocks, which can be assembled using numerical kernels from existing high-performance linear algebra libraries. Furthermore, the experiments with several sparse matrices arising in representative real-world applications and synthetic dense test matrices reveal a performance advantage of the block Lanczos algorithm when targeting the same approximation accuracy.","sentences":["We investigate the solution of low-rank matrix approximation problems using the truncated SVD.","For this purpose, we develop and optimize GPU implementations for the randomized SVD and a blocked variant of the Lanczos approach.","Our work takes advantage of the fact that the two methods are composed of very similar linear algebra building blocks, which can be assembled using numerical kernels from existing high-performance linear algebra libraries.","Furthermore, the experiments with several sparse matrices arising in representative real-world applications and synthetic dense test matrices reveal a performance advantage of the block Lanczos algorithm when targeting the same approximation accuracy."],"url":"http://arxiv.org/abs/2403.06218v1","category":"cs.DC"}
{"created":"2024-03-10 13:02:27","title":"Identifying and interpreting non-aligned human conceptual representations using language modeling","abstract":"The question of whether people's experience in the world shapes conceptual representation and lexical semantics is longstanding. Word-association, feature-listing and similarity rating tasks aim to address this question but require a subjective interpretation of the latent dimensions identified. In this study, we introduce a supervised representational-alignment method that (i) determines whether two groups of individuals share the same basis of a certain category, and (ii) explains in what respects they differ. In applying this method, we show that congenital blindness induces conceptual reorganization in both a-modal and sensory-related verbal domains, and we identify the associated semantic shifts. We first apply supervised feature-pruning to a language model (GloVe) to optimize prediction accuracy of human similarity judgments from word embeddings. Pruning identifies one subset of retained GloVe features that optimizes prediction of judgments made by sighted individuals and another subset that optimizes judgments made by blind. A linear probing analysis then interprets the latent semantics of these feature-subsets by learning a mapping from the retained GloVe features to 65 interpretable semantic dimensions. We applied this approach to seven semantic domains, including verbs related to motion, sight, touch, and amodal verbs related to knowledge acquisition. We find that blind individuals more strongly associate social and cognitive meanings to verbs related to motion or those communicating non-speech vocal utterances (e.g., whimper, moan). Conversely, for amodal verbs, they demonstrate much sparser information. Finally, for some verbs, representations of blind and sighted are highly similar. The study presents a formal approach for studying interindividual differences in word meaning, and the first demonstration of how blindness impacts conceptual representation of everyday verbs.","sentences":["The question of whether people's experience in the world shapes conceptual representation and lexical semantics is longstanding.","Word-association, feature-listing and similarity rating tasks aim to address this question but require a subjective interpretation of the latent dimensions identified.","In this study, we introduce a supervised representational-alignment method that (i) determines whether two groups of individuals share the same basis of a certain category, and (ii) explains in what respects they differ.","In applying this method, we show that congenital blindness induces conceptual reorganization in both a-modal and sensory-related verbal domains, and we identify the associated semantic shifts.","We first apply supervised feature-pruning to a language model (GloVe) to optimize prediction accuracy of human similarity judgments from word embeddings.","Pruning identifies one subset of retained GloVe features that optimizes prediction of judgments made by sighted individuals and another subset that optimizes judgments made by blind.","A linear probing analysis then interprets the latent semantics of these feature-subsets by learning a mapping from the retained GloVe features to 65 interpretable semantic dimensions.","We applied this approach to seven semantic domains, including verbs related to motion, sight, touch, and amodal verbs related to knowledge acquisition.","We find that blind individuals more strongly associate social and cognitive meanings to verbs related to motion or those communicating non-speech vocal utterances (e.g., whimper, moan).","Conversely, for amodal verbs, they demonstrate much sparser information.","Finally, for some verbs, representations of blind and sighted are highly similar.","The study presents a formal approach for studying interindividual differences in word meaning, and the first demonstration of how blindness impacts conceptual representation of everyday verbs."],"url":"http://arxiv.org/abs/2403.06204v1","category":"cs.CL"}
{"created":"2024-03-10 12:43:27","title":"A Comprehensive Overhaul of Multimodal Assistant with Small Language Models","abstract":"Multimodal Large Language Models (MLLMs) have showcased impressive skills in tasks related to visual understanding and reasoning. Yet, their widespread application faces obstacles due to the high computational demands during both the training and inference phases, restricting their use to a limited audience within the research and user communities. In this paper, we investigate the design aspects of Multimodal Small Language Models (MSLMs) and propose an efficient multimodal assistant named Mipha, which is designed to create synergy among various aspects: visual representation, language models, and optimization strategies. We show that without increasing the volume of training data, our Mipha-3B outperforms the state-of-the-art large MLLMs, especially LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide insights and guidelines for developing strong MSLMs that rival the capabilities of MLLMs. Our code is available at https://github.com/zhuyiche/Mipha.","sentences":["Multimodal Large Language Models (MLLMs) have showcased impressive skills in tasks related to visual understanding and reasoning.","Yet, their widespread application faces obstacles due to the high computational demands during both the training and inference phases, restricting their use to a limited audience within the research and user communities.","In this paper, we investigate the design aspects of Multimodal Small Language Models (MSLMs) and propose an efficient multimodal assistant named Mipha, which is designed to create synergy among various aspects: visual representation, language models, and optimization strategies.","We show that without increasing the volume of training data, our Mipha-3B outperforms the state-of-the-art large MLLMs, especially LLaVA-1.5-13B, on multiple benchmarks.","Through detailed discussion, we provide insights and guidelines for developing strong MSLMs that rival the capabilities of MLLMs.","Our code is available at https://github.com/zhuyiche/Mipha."],"url":"http://arxiv.org/abs/2403.06199v1","category":"cs.CV"}
{"created":"2024-03-10 12:12:01","title":"Monotone Mean-Variance Portfolio Selection in Semimartingale Markets: Martingale Method","abstract":"We use the martingale method to discuss the relationship between mean-variance (MV) and monotone mean-variance (MMV) portfolio selections. We propose a unified framework to discuss the relationship in general financial markets without any specific setting or completeness requirement. We apply this framework to a semimartingale market and find that MV and MMV are consistent if and only if the variance-optimal signed martingale measure keeps non-negative. Further, we provide an example to show the application of our result.","sentences":["We use the martingale method to discuss the relationship between mean-variance (MV) and monotone mean-variance (MMV) portfolio selections.","We propose a unified framework to discuss the relationship in general financial markets without any specific setting or completeness requirement.","We apply this framework to a semimartingale market and find that MV and MMV are consistent if and only if the variance-optimal signed martingale measure keeps non-negative.","Further, we provide an example to show the application of our result."],"url":"http://arxiv.org/abs/2403.06190v1","category":"math.OC"}
{"created":"2024-03-10 12:11:34","title":"Harmonious Group Choreography with Trajectory-Controllable Diffusion","abstract":"Creating group choreography from music has gained attention in cultural entertainment and virtual reality, aiming to coordinate visually cohesive and diverse group movements. Despite increasing interest, recent works face challenges in achieving aesthetically appealing choreography, primarily for two key issues: multi-dancer collision and single-dancer foot slide. To address these issues, we propose a Trajectory-Controllable Diffusion (TCDiff), a novel approach that harnesses non-overlapping trajectories to facilitate coherent dance movements. Specifically, to tackle dancer collisions, we introduce a Dance-Beat Navigator capable of generating trajectories for multiple dancers based on the music, complemented by a Distance-Consistency loss to maintain appropriate spacing among trajectories within a reasonable threshold. To mitigate foot sliding, we present a Footwork Adaptor that utilizes trajectory displacement from adjacent frames to enable flexible footwork, coupled with a Relative Forward-Kinematic loss to adjust the positioning of individual dancers' root nodes and joints. Extensive experiments demonstrate that our method achieves state-of-the-art results.","sentences":["Creating group choreography from music has gained attention in cultural entertainment and virtual reality, aiming to coordinate visually cohesive and diverse group movements.","Despite increasing interest, recent works face challenges in achieving aesthetically appealing choreography, primarily for two key issues: multi-dancer collision and single-dancer foot slide.","To address these issues, we propose a Trajectory-Controllable Diffusion (TCDiff), a novel approach that harnesses non-overlapping trajectories to facilitate coherent dance movements.","Specifically, to tackle dancer collisions, we introduce a Dance-Beat Navigator capable of generating trajectories for multiple dancers based on the music, complemented by a Distance-Consistency loss to maintain appropriate spacing among trajectories within a reasonable threshold.","To mitigate foot sliding, we present a Footwork Adaptor that utilizes trajectory displacement from adjacent frames to enable flexible footwork, coupled with a Relative Forward-Kinematic loss to adjust the positioning of individual dancers' root nodes and joints.","Extensive experiments demonstrate that our method achieves state-of-the-art results."],"url":"http://arxiv.org/abs/2403.06189v1","category":"cs.CV"}
{"created":"2024-03-10 12:06:45","title":"Mind Meets Robots: A Review of EEG-Based Brain-Robot Interaction Systems","abstract":"Brain-robot interaction (BRI) empowers individuals to control (semi-)automated machines through their brain activity, either passively or actively. In the past decade, BRI systems have achieved remarkable success, predominantly harnessing electroencephalogram (EEG) signals as the central component. This paper offers an up-to-date and exhaustive examination of 87 curated studies published during the last five years (2018-2023), focusing on identifying the research landscape of EEG-based BRI systems. This review aims to consolidate and underscore methodologies, interaction modes, application contexts, system evaluation, existing challenges, and potential avenues for future investigations in this domain. Based on our analysis, we present a BRI system model with three entities: Brain, Robot, and Interaction, depicting the internal relationships of a BRI system. We especially investigate the essence and principles on interaction modes between human brains and robots, a domain that has not yet been identified anywhere. We then discuss these entities with different dimensions encompassed. Within this model, we scrutinize and classify current research, reveal insights, specify challenges, and provide recommendations for future research trajectories in this field. Meanwhile, we envision our findings offer a design space for future human-robot interaction (HRI) research, informing the creation of efficient BRI frameworks.","sentences":["Brain-robot interaction (BRI) empowers individuals to control (semi-)automated machines through their brain activity, either passively or actively.","In the past decade, BRI systems have achieved remarkable success, predominantly harnessing electroencephalogram (EEG) signals as the central component.","This paper offers an up-to-date and exhaustive examination of 87 curated studies published during the last five years (2018-2023), focusing on identifying the research landscape of EEG-based BRI systems.","This review aims to consolidate and underscore methodologies, interaction modes, application contexts, system evaluation, existing challenges, and potential avenues for future investigations in this domain.","Based on our analysis, we present a BRI system model with three entities: Brain, Robot, and Interaction, depicting the internal relationships of a BRI system.","We especially investigate the essence and principles on interaction modes between human brains and robots, a domain that has not yet been identified anywhere.","We then discuss these entities with different dimensions encompassed.","Within this model, we scrutinize and classify current research, reveal insights, specify challenges, and provide recommendations for future research trajectories in this field.","Meanwhile, we envision our findings offer a design space for future human-robot interaction (HRI) research, informing the creation of efficient BRI frameworks."],"url":"http://arxiv.org/abs/2403.06186v1","category":"cs.RO"}
{"created":"2024-03-10 11:50:34","title":"An Improved Analysis of Langevin Algorithms with Prior Diffusion for Non-Log-Concave Sampling","abstract":"Understanding the dimension dependency of computational complexity in high-dimensional sampling problem is a fundamental problem, both from a practical and theoretical perspective. Compared with samplers with unbiased stationary distribution, e.g., Metropolis-adjusted Langevin algorithm (MALA), biased samplers, e.g., Underdamped Langevin Dynamics (ULD), perform better in low-accuracy cases just because a lower dimension dependency in their complexities. Along this line, Freund et al. (2022) suggest that the modified Langevin algorithm with prior diffusion is able to converge dimension independently for strongly log-concave target distributions. Nonetheless, it remains open whether such property establishes for more general cases. In this paper, we investigate the prior diffusion technique for the target distributions satisfying log-Sobolev inequality (LSI), which covers a much broader class of distributions compared to the strongly log-concave ones. In particular, we prove that the modified Langevin algorithm can also obtain the dimension-independent convergence of KL divergence with different step size schedules. The core of our proof technique is a novel construction of an interpolating SDE, which significantly helps to conduct a more accurate characterization of the discrete updates of the overdamped Langevin dynamics. Our theoretical analysis demonstrates the benefits of prior diffusion for a broader class of target distributions and provides new insights into developing faster sampling algorithms.","sentences":["Understanding the dimension dependency of computational complexity in high-dimensional sampling problem is a fundamental problem, both from a practical and theoretical perspective.","Compared with samplers with unbiased stationary distribution, e.g., Metropolis-adjusted Langevin algorithm (MALA), biased samplers, e.g., Underdamped Langevin Dynamics (ULD), perform better in low-accuracy cases just because a lower dimension dependency in their complexities.","Along this line, Freund et al. (2022) suggest that the modified Langevin algorithm with prior diffusion is able to converge dimension independently for strongly log-concave target distributions.","Nonetheless, it remains open whether such property establishes for more general cases.","In this paper, we investigate the prior diffusion technique for the target distributions satisfying log-Sobolev inequality (LSI), which covers a much broader class of distributions compared to the strongly log-concave ones.","In particular, we prove that the modified Langevin algorithm can also obtain the dimension-independent convergence of KL divergence with different step size schedules.","The core of our proof technique is a novel construction of an interpolating SDE, which significantly helps to conduct a more accurate characterization of the discrete updates of the overdamped Langevin dynamics.","Our theoretical analysis demonstrates the benefits of prior diffusion for a broader class of target distributions and provides new insights into developing faster sampling algorithms."],"url":"http://arxiv.org/abs/2403.06183v1","category":"cs.LG"}
{"created":"2024-03-10 10:58:54","title":"Speeding up 6-DoF Grasp Sampling with Quality-Diversity","abstract":"Recent advances in AI have led to significant results in robotic learning, including natural language-conditioned planning and efficient optimization of controllers using generative models. However, the interaction data remains the bottleneck for generalization. Getting data for grasping is a critical challenge, as this skill is required to complete many manipulation tasks. Quality-Diversity (QD) algorithms optimize a set of solutions to get diverse, high-performing solutions to a given problem. This paper investigates how QD can be combined with priors to speed up the generation of diverse grasps poses in simulation compared to standard 6-DoF grasp sampling schemes. Experiments conducted on 4 grippers with 2-to-5 fingers on standard objects show that QD outperforms commonly used methods by a large margin. Further experiments show that QD optimization automatically finds some efficient priors that are usually hard coded. The deployment of generated grasps on a 2-finger gripper and an Allegro hand shows that the diversity produced maintains sim-to-real transferability. We believe these results to be a significant step toward the generation of large datasets that can lead to robust and generalizing robotic grasping policies.","sentences":["Recent advances in AI have led to significant results in robotic learning, including natural language-conditioned planning and efficient optimization of controllers using generative models.","However, the interaction data remains the bottleneck for generalization.","Getting data for grasping is a critical challenge, as this skill is required to complete many manipulation tasks.","Quality-Diversity (QD) algorithms optimize a set of solutions to get diverse, high-performing solutions to a given problem.","This paper investigates how QD can be combined with priors to speed up the generation of diverse grasps poses in simulation compared to standard 6-DoF grasp sampling schemes.","Experiments conducted on 4 grippers with 2-to-5 fingers on standard objects show that QD outperforms commonly used methods by a large margin.","Further experiments show that QD optimization automatically finds some efficient priors that are usually hard coded.","The deployment of generated grasps on a 2-finger gripper and an Allegro hand shows that the diversity produced maintains sim-to-real transferability.","We believe these results to be a significant step toward the generation of large datasets that can lead to robust and generalizing robotic grasping policies."],"url":"http://arxiv.org/abs/2403.06173v1","category":"cs.RO"}
{"created":"2024-03-10 09:46:28","title":"Decoupled Contrastive Learning for Long-Tailed Recognition","abstract":"Supervised Contrastive Loss (SCL) is popular in visual representation learning. Given an anchor image, SCL pulls two types of positive samples, i.e., its augmentation and other images from the same class together, while pushes negative images apart to optimize the learned embedding. In the scenario of long-tailed recognition, where the number of samples in each class is imbalanced, treating two types of positive samples equally leads to the biased optimization for intra-category distance. In addition, similarity relationship among negative samples, that are ignored by SCL, also presents meaningful semantic cues. To improve the performance on long-tailed recognition, this paper addresses those two issues of SCL by decoupling the training objective. Specifically, it decouples two types of positives in SCL and optimizes their relations toward different objectives to alleviate the influence of the imbalanced dataset. We further propose a patch-based self distillation to transfer knowledge from head to tail classes to relieve the under-representation of tail classes. It uses patch-based features to mine shared visual patterns among different instances and leverages a self distillation procedure to transfer such knowledge. Experiments on different long-tailed classification benchmarks demonstrate the superiority of our method. For instance, it achieves the 57.7% top-1 accuracy on the ImageNet-LT dataset. Combined with the ensemble-based method, the performance can be further boosted to 59.7%, which substantially outperforms many recent works. The code is available at https://github.com/SY-Xuan/DSCL.","sentences":["Supervised Contrastive Loss (SCL) is popular in visual representation learning.","Given an anchor image, SCL pulls two types of positive samples, i.e., its augmentation and other images from the same class together, while pushes negative images apart to optimize the learned embedding.","In the scenario of long-tailed recognition, where the number of samples in each class is imbalanced, treating two types of positive samples equally leads to the biased optimization for intra-category distance.","In addition, similarity relationship among negative samples, that are ignored by SCL, also presents meaningful semantic cues.","To improve the performance on long-tailed recognition, this paper addresses those two issues of SCL by decoupling the training objective.","Specifically, it decouples two types of positives in SCL and optimizes their relations toward different objectives to alleviate the influence of the imbalanced dataset.","We further propose a patch-based self distillation to transfer knowledge from head to tail classes to relieve the under-representation of tail classes.","It uses patch-based features to mine shared visual patterns among different instances and leverages a self distillation procedure to transfer such knowledge.","Experiments on different long-tailed classification benchmarks demonstrate the superiority of our method.","For instance, it achieves the 57.7% top-1 accuracy on the ImageNet-LT dataset.","Combined with the ensemble-based method, the performance can be further boosted to 59.7%, which substantially outperforms many recent works.","The code is available at https://github.com/SY-Xuan/DSCL."],"url":"http://arxiv.org/abs/2403.06151v1","category":"cs.CV"}
{"created":"2024-03-10 09:34:58","title":"OS-FPI: A Coarse-to-Fine One-Stream Network for UAV Geo-Localization","abstract":"The geo-localization and navigation technology of unmanned aerial vehicles (UAVs) in denied environments is currently a prominent research area. Prior approaches mainly employed a two-stream network with non-shared weights to extract features from UAV and satellite images separately, followed by related modeling to obtain the response map. However, the two-stream network extracts UAV and satellite features independently. This approach significantly affects the efficiency of feature extraction and increases the computational load. To address these issues, we propose a novel coarse-to-fine one-stream network (OS-FPI). Our approach allows information exchange between UAV and satellite features during early image feature extraction. To improve the model's performance, the framework retains feature maps generated at different stages of the feature extraction process for the feature fusion network, and establishes additional connections between UAV and satellite feature maps in the feature fusion network. Additionally, the framework introduces offset prediction to further refine and optimize the model's prediction results based on the classification tasks. Our proposed model, boasts a similar inference speed to FPI while significantly reducing the number of parameters. It can achieve better performance with fewer parameters under the same conditions. Moreover, it achieves state-of-the-art performance on the UL14 dataset. Compared to previous models, our model achieved a significant 10.92-point improvement on the RDS metric, reaching 76.25. Furthermore, its performance in meter-level localization accuracy is impressive, with 182.62% improvement in 3-meter accuracy, 164.17% improvement in 5-meter accuracy, and 137.43% improvement in 10-meter accuracy.","sentences":["The geo-localization and navigation technology of unmanned aerial vehicles (UAVs) in denied environments is currently a prominent research area.","Prior approaches mainly employed a two-stream network with non-shared weights to extract features from UAV and satellite images separately, followed by related modeling to obtain the response map.","However, the two-stream network extracts UAV and satellite features independently.","This approach significantly affects the efficiency of feature extraction and increases the computational load.","To address these issues, we propose a novel coarse-to-fine one-stream network (OS-FPI).","Our approach allows information exchange between UAV and satellite features during early image feature extraction.","To improve the model's performance, the framework retains feature maps generated at different stages of the feature extraction process for the feature fusion network, and establishes additional connections between UAV and satellite feature maps in the feature fusion network.","Additionally, the framework introduces offset prediction to further refine and optimize the model's prediction results based on the classification tasks.","Our proposed model, boasts a similar inference speed to FPI while significantly reducing the number of parameters.","It can achieve better performance with fewer parameters under the same conditions.","Moreover, it achieves state-of-the-art performance on the UL14 dataset.","Compared to previous models, our model achieved a significant 10.92-point improvement on the RDS metric, reaching 76.25.","Furthermore, its performance in meter-level localization accuracy is impressive, with 182.62% improvement in 3-meter accuracy, 164.17% improvement in 5-meter accuracy, and 137.43% improvement in 10-meter accuracy."],"url":"http://arxiv.org/abs/2403.06148v1","category":"eess.IV"}
{"created":"2024-03-10 09:07:11","title":"RADS : Restricted Anisotropic Diffusion Spectrum model for Axonal Health quantification in Multiple Sclerosis","abstract":"Axonal damage is the primary pathological correlate of long-term impairment in multiple sclerosis (MS). Our previous work using our method - diffusion basis spectrum imaging (DBSI) - demonstrated a strong, quantitative relationship between axial diffusivity and axonal damage. In the present work, we develop an extension of DBSI which can be used to quantify the fraction of diseased and healthy axons in MS. In this method, we model the MRI signal with the axial diffusion (AD) spectrum for each fiber orientation. We use two component restricted anisotropic diffusion spectrum (RADS) to model the anisotropic component of the diffusion-weighted MRI signal. Diffusion coefficients and signal fractions are computed for the optimal model with the lowest Bayesian information criterion (BIC) score. This gives us the fractions of diseased and healthy axons based on the axial diffusivities of the diseased and healthy axons. We test our method using Monte-Carlo (MC) simulations with the MC simulation package developed as part of this work. First we test and validate our MC simulations for the basic RADS model. It accurately recovers the fiber and cell fractions simulated as well as the simulated diffusivities. For testing and validating RADS to quantify axonal loss, we simulate different fractions of diseased and healthy axons. Our method produces highly accurate quantification of diseased and healthy axons with Pearson's correlation (predicted vs true proportion) of $ r = 0.99 $ (p-value = 0.001); the one Sample t-test for proportion error gives the mean error of 2\\% (p-value = 0.034). Furthermore, the method finds the axial diffusivities of the diseased and healthy axons very accurately with mean error of 4\\% (p-value = 0.001). RADS modeling of the diffusion-weighted MRI signal has the potential to be used for Axonal Health quantification in Multiple Sclerosis.","sentences":["Axonal damage is the primary pathological correlate of long-term impairment in multiple sclerosis (MS).","Our previous work using our method - diffusion basis spectrum imaging (DBSI) - demonstrated a strong, quantitative relationship between axial diffusivity and axonal damage.","In the present work, we develop an extension of DBSI which can be used to quantify the fraction of diseased and healthy axons in MS.","In this method, we model the MRI signal with the axial diffusion (AD) spectrum for each fiber orientation.","We use two component restricted anisotropic diffusion spectrum (RADS) to model the anisotropic component of the diffusion-weighted MRI signal.","Diffusion coefficients and signal fractions are computed for the optimal model with the lowest Bayesian information criterion (BIC) score.","This gives us the fractions of diseased and healthy axons based on the axial diffusivities of the diseased and healthy axons.","We test our method using Monte-Carlo (MC) simulations with the MC simulation package developed as part of this work.","First we test and validate our MC simulations for the basic RADS model.","It accurately recovers the fiber and cell fractions simulated as well as the simulated diffusivities.","For testing and validating RADS to quantify axonal loss, we simulate different fractions of diseased and healthy axons.","Our method produces highly accurate quantification of diseased and healthy axons with Pearson's correlation (predicted vs true proportion) of $ r = 0.99 $ (p-value = 0.001); the one Sample t-test for proportion error gives the mean error of 2\\% (p-value = 0.034).","Furthermore, the method finds the axial diffusivities of the diseased and healthy axons very accurately with mean error of 4\\% (p-value = 0.001).","RADS modeling of the diffusion-weighted MRI signal has the potential to be used for Axonal Health quantification in Multiple Sclerosis."],"url":"http://arxiv.org/abs/2403.06140v1","category":"cs.CE"}
{"created":"2024-03-10 08:21:50","title":"Low-dose CT Denoising with Language-engaged Dual-space Alignment","abstract":"While various deep learning methods were proposed for low-dose computed tomography (CT) denoising, they often suffer from over-smoothing, blurring, and lack of explainability. To alleviate these issues, we propose a plug-and-play Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT denoising models. Our idea is to leverage large language models (LLMs) to align denoised CT and normal dose CT images in both the continuous perceptual space and discrete semantic space, which is the first LLM-based scheme for low-dose CT denoising. LEDA involves two steps: the first is to pretrain an LLM-guided CT autoencoder, which can encode a CT image into continuous high-level features and quantize them into a token space to produce semantic tokens derived from the LLM's vocabulary; and the second is to minimize the discrepancy between the denoised CT images and normal dose CT in terms of both encoded high-level features and quantized token embeddings derived by the LLM-guided CT autoencoder. Extensive experimental results on two public LDCT denoising datasets demonstrate that our LEDA can enhance existing denoising models in terms of quantitative metrics and qualitative evaluation, and also provide explainability through language-level image understanding. Source code is available at https://github.com/hao1635/LEDA.","sentences":["While various deep learning methods were proposed for low-dose computed tomography (CT) denoising, they often suffer from over-smoothing, blurring, and lack of explainability.","To alleviate these issues, we propose a plug-and-play Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT denoising models.","Our idea is to leverage large language models (LLMs) to align denoised CT and normal dose CT images in both the continuous perceptual space and discrete semantic space, which is the first LLM-based scheme for low-dose CT denoising.","LEDA involves two steps: the first is to pretrain an LLM-guided CT autoencoder, which can encode a CT image into continuous high-level features and quantize them into a token space to produce semantic tokens derived from the LLM's vocabulary; and the second is to minimize the discrepancy between the denoised CT images and normal dose CT in terms of both encoded high-level features and quantized token embeddings derived by the LLM-guided CT autoencoder.","Extensive experimental results on two public LDCT denoising datasets demonstrate that our LEDA can enhance existing denoising models in terms of quantitative metrics and qualitative evaluation, and also provide explainability through language-level image understanding.","Source code is available at https://github.com/hao1635/LEDA."],"url":"http://arxiv.org/abs/2403.06128v1","category":"eess.IV"}
{"created":"2024-03-10 06:15:42","title":"Universal Debiased Editing for Fair Medical Image Classification","abstract":"In the era of Foundation Models' (FMs) rising prominence in AI, our study addresses the challenge of biases in medical images while using FM API, particularly spurious correlations between pixels and sensitive attributes. Traditional methods for bias mitigation face limitations due to the restricted access to web-hosted FMs and difficulties in addressing the underlying bias encoded within the FM API. We propose an U(niversal) D(ebiased) E(diting) strategy, termed UDE, which generates UDE noise to mask such spurious correlation. UDE is capable of mitigating bias both within the FM API embedding and the images themselves. Furthermore, UDE is suitable for both white-box and black-box FM APIs, where we introduced G(reedy) (Z)eroth-O(rder) (GeZO) optimization for it when the gradient is inaccessible in black-box APIs. Our whole pipeline enables fairness-aware image editing that can be applied across various medical contexts without requiring direct model manipulation or significant computational resources. Our empirical results demonstrate the method's effectiveness in maintaining fairness and utility across different patient groups and diseases. In the era of AI-driven medicine, this work contributes to making healthcare diagnostics more equitable, showcasing a practical solution for bias mitigation in pre-trained image FMs.","sentences":["In the era of Foundation Models' (FMs) rising prominence in AI, our study addresses the challenge of biases in medical images while using FM API, particularly spurious correlations between pixels and sensitive attributes.","Traditional methods for bias mitigation face limitations due to the restricted access to web-hosted FMs and difficulties in addressing the underlying bias encoded within the FM API.","We propose an U(niversal) D(ebiased) E(diting) strategy, termed UDE, which generates UDE noise to mask such spurious correlation.","UDE is capable of mitigating bias both within the FM API embedding and the images themselves.","Furthermore, UDE is suitable for both white-box and black-box FM APIs, where we introduced G(reedy) (Z)eroth-O(rder) (GeZO) optimization for it when the gradient is inaccessible in black-box APIs.","Our whole pipeline enables fairness-aware image editing that can be applied across various medical contexts without requiring direct model manipulation or significant computational resources.","Our empirical results demonstrate the method's effectiveness in maintaining fairness and utility across different patient groups and diseases.","In the era of AI-driven medicine, this work contributes to making healthcare diagnostics more equitable, showcasing a practical solution for bias mitigation in pre-trained image FMs."],"url":"http://arxiv.org/abs/2403.06104v1","category":"cs.CV"}
{"created":"2024-03-10 05:55:00","title":"Automatic design optimization of preference-based subjective evaluation with online learning in crowdsourcing environment","abstract":"A preference-based subjective evaluation is a key method for evaluating generative media reliably. However, its huge combinations of pairs prohibit it from being applied to large-scale evaluation using crowdsourcing. To address this issue, we propose an automatic optimization method for preference-based subjective evaluation in terms of pair combination selections and allocation of evaluation volumes with online learning in a crowdsourcing environment. We use a preference-based online learning method based on a sorting algorithm to identify the total order of evaluation targets with minimum sample volumes. Our online learning algorithm supports parallel and asynchronous execution under fixed-budget conditions required for crowdsourcing. Our experiment on preference-based subjective evaluation of synthetic speech shows that our method successfully optimizes the test by reducing pair combinations from 351 to 83 and allocating optimal evaluation volumes for each pair ranging from 30 to 663 without compromising evaluation accuracies and wasting budget allocations.","sentences":["A preference-based subjective evaluation is a key method for evaluating generative media reliably.","However, its huge combinations of pairs prohibit it from being applied to large-scale evaluation using crowdsourcing.","To address this issue, we propose an automatic optimization method for preference-based subjective evaluation in terms of pair combination selections and allocation of evaluation volumes with online learning in a crowdsourcing environment.","We use a preference-based online learning method based on a sorting algorithm to identify the total order of evaluation targets with minimum sample volumes.","Our online learning algorithm supports parallel and asynchronous execution under fixed-budget conditions required for crowdsourcing.","Our experiment on preference-based subjective evaluation of synthetic speech shows that our method successfully optimizes the test by reducing pair combinations from 351 to 83 and allocating optimal evaluation volumes for each pair ranging from 30 to 663 without compromising evaluation accuracies and wasting budget allocations."],"url":"http://arxiv.org/abs/2403.06100v1","category":"cs.HC"}
{"created":"2024-03-10 04:17:42","title":"Learning the irreversible progression trajectory of Alzheimer's disease","abstract":"Alzheimer's disease (AD) is a progressive and irreversible brain disorder that unfolds over the course of 30 years. Therefore, it is critical to capture the disease progression in an early stage such that intervention can be applied before the onset of symptoms. Machine learning (ML) models have been shown effective in predicting the onset of AD. Yet for subjects with follow-up visits, existing techniques for AD classification only aim for accurate group assignment, where the monotonically increasing risk across follow-up visits is usually ignored. Resulted fluctuating risk scores across visits violate the irreversibility of AD, hampering the trustworthiness of models and also providing little value to understanding the disease progression. To address this issue, we propose a novel regularization approach to predict AD longitudinally. Our technique aims to maintain the expected monotonicity of increasing disease risk during progression while preserving expressiveness. Specifically, we introduce a monotonicity constraint that encourages the model to predict disease risk in a consistent and ordered manner across follow-up visits. We evaluate our method using the longitudinal structural MRI and amyloid-PET imaging data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our model outperforms existing techniques in capturing the progressiveness of disease risk, and at the same time preserves prediction accuracy.","sentences":["Alzheimer's disease (AD) is a progressive and irreversible brain disorder that unfolds over the course of 30 years.","Therefore, it is critical to capture the disease progression in an early stage such that intervention can be applied before the onset of symptoms.","Machine learning (ML) models have been shown effective in predicting the onset of AD.","Yet for subjects with follow-up visits, existing techniques for AD classification only aim for accurate group assignment, where the monotonically increasing risk across follow-up visits is usually ignored.","Resulted fluctuating risk scores across visits violate the irreversibility of AD, hampering the trustworthiness of models and also providing little value to understanding the disease progression.","To address this issue, we propose a novel regularization approach to predict AD longitudinally.","Our technique aims to maintain the expected monotonicity of increasing disease risk during progression while preserving expressiveness.","Specifically, we introduce a monotonicity constraint that encourages the model to predict disease risk in a consistent and ordered manner across follow-up visits.","We evaluate our method using the longitudinal structural MRI and amyloid-PET imaging data from the Alzheimer's Disease Neuroimaging Initiative (ADNI).","Our model outperforms existing techniques in capturing the progressiveness of disease risk, and at the same time preserves prediction accuracy."],"url":"http://arxiv.org/abs/2403.06087v1","category":"cs.LG"}
{"created":"2024-03-10 03:33:59","title":"Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing","abstract":"Unsupervised semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in unsupervised semantic hashing models can yield substantial improvements. However, the inference delay has become increasingly difficult to overlook. Knowledge distillation provides a means for practical model compression to alleviate this delay. Nevertheless, the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing, the inherent necessities of the distillation process, and the property of hash codes. In this paper, we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically devised for the distillation of semantic hashing models. To ensure the effectiveness of two kinds of search paradigms in the context of semantic hashing, BRCD first aligns the semantic spaces between the teacher and student models through a contrastive knowledge distillation objective. Additionally, to eliminate noisy augmentations and ensure robust optimization, a cluster-based method within the knowledge distillation process is introduced. Furthermore, through a bit-level analysis, we uncover the presence of redundancy bits resulting from the bit independence property. To mitigate these effects, we introduce a bit mask mechanism in our knowledge distillation objective. Finally, extensive experiments not only showcase the noteworthy performance of our BRCD method in comparison to other knowledge distillation methods but also substantiate the generality of our methods across diverse semantic hashing models and backbones. The code for BRCD is available at https://github.com/hly1998/BRCD.","sentences":["Unsupervised semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels.","Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in unsupervised semantic hashing models can yield substantial improvements.","However, the inference delay has become increasingly difficult to overlook.","Knowledge distillation provides a means for practical model compression to alleviate this delay.","Nevertheless, the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing.","They ignore the unique search paradigm of semantic hashing, the inherent necessities of the distillation process, and the property of hash codes.","In this paper, we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically devised for the distillation of semantic hashing models.","To ensure the effectiveness of two kinds of search paradigms in the context of semantic hashing, BRCD first aligns the semantic spaces between the teacher and student models through a contrastive knowledge distillation objective.","Additionally, to eliminate noisy augmentations and ensure robust optimization, a cluster-based method within the knowledge distillation process is introduced.","Furthermore, through a bit-level analysis, we uncover the presence of redundancy bits resulting from the bit independence property.","To mitigate these effects, we introduce a bit mask mechanism in our knowledge distillation objective.","Finally, extensive experiments not only showcase the noteworthy performance of our BRCD method in comparison to other knowledge distillation methods but also substantiate the generality of our methods across diverse semantic hashing models and backbones.","The code for BRCD is available at https://github.com/hly1998/BRCD."],"url":"http://arxiv.org/abs/2403.06071v1","category":"cs.CV"}
{"created":"2024-03-10 01:07:22","title":"Absence of spurious solutions far from ground truth: A low-rank analysis with high-order losses","abstract":"Matrix sensing problems exhibit pervasive non-convexity, plaguing optimization with a proliferation of suboptimal spurious solutions. Avoiding convergence to these critical points poses a major challenge. This work provides new theoretical insights that help demystify the intricacies of the non-convex landscape. In this work, we prove that under certain conditions, critical points sufficiently distant from the ground truth matrix exhibit favorable geometry by being strict saddle points rather than troublesome local minima. Moreover, we introduce the notion of higher-order losses for the matrix sensing problem and show that the incorporation of such losses into the objective function amplifies the negative curvature around those distant critical points. This implies that increasing the complexity of the objective function via high-order losses accelerates the escape from such critical points and acts as a desirable alternative to increasing the complexity of the optimization problem via over-parametrization. By elucidating key characteristics of the non-convex optimization landscape, this work makes progress towards a comprehensive framework for tackling broader machine learning objectives plagued by non-convexity.","sentences":["Matrix sensing problems exhibit pervasive non-convexity, plaguing optimization with a proliferation of suboptimal spurious solutions.","Avoiding convergence to these critical points poses a major challenge.","This work provides new theoretical insights that help demystify the intricacies of the non-convex landscape.","In this work, we prove that under certain conditions, critical points sufficiently distant from the ground truth matrix exhibit favorable geometry by being strict saddle points rather than troublesome local minima.","Moreover, we introduce the notion of higher-order losses for the matrix sensing problem and show that the incorporation of such losses into the objective function amplifies the negative curvature around those distant critical points.","This implies that increasing the complexity of the objective function via high-order losses accelerates the escape from such critical points and acts as a desirable alternative to increasing the complexity of the optimization problem via over-parametrization.","By elucidating key characteristics of the non-convex optimization landscape, this work makes progress towards a comprehensive framework for tackling broader machine learning objectives plagued by non-convexity."],"url":"http://arxiv.org/abs/2403.06056v1","category":"math.OC"}
{"created":"2024-03-10 00:02:38","title":"Conventional Superconductivity in the Doped Kagome Superconductor Cs(V0.86Ta0.14)3Sb5 from Vortex Lattice Studies","abstract":"A hallmark of unconventional superconductors is their complex electronic phase diagrams where \"intertwined orders\" of charge-spin-lattice degrees of freedom compete and coexist as in copper oxides and iron pnictides. While the electronic phase diagram of kagome lattice superconductor such as CsV3Sb5 also exhibits complex behavior involving coexisting and competing charge density wave order and superconductivity, much is unclear about the microscopic origin of superconductivity. Here, we study the vortex lattice (VL) in superconducting state of Cs(V0.86Ta0.14)3Sb5, where the Ta-doping suppresses charge order and enhances superconductivity. Using small-angle neutron scattering, a strictly bulk probe, we show that the VL exhibits a strikingly conventional behavior. This includes a triangular VL with a period consistent with 2e-pairing, a field dependent scattering intensity that follows a London model, and a temperature dependence consistent with a uniform superconducting gap expected for s-wave pairing. These results suggest that optimal bulk superconductivity in Cs(V1-xTax)3Sb5 arises from a conventional Bardeen-Cooper-Schrieffer electron-lattice coupling, different from spin fluctuation mediated unconventional copper and iron based superconductors.","sentences":["A hallmark of unconventional superconductors is their complex electronic phase diagrams where \"intertwined orders\" of charge-spin-lattice degrees of freedom compete and coexist as in copper oxides and iron pnictides.","While the electronic phase diagram of kagome lattice superconductor such as CsV3Sb5 also exhibits complex behavior involving coexisting and competing charge density wave order and superconductivity, much is unclear about the microscopic origin of superconductivity.","Here, we study the vortex lattice (VL) in superconducting state of Cs(V0.86Ta0.14)3Sb5, where the Ta-doping suppresses charge order and enhances superconductivity.","Using small-angle neutron scattering, a strictly bulk probe, we show that the VL exhibits a strikingly conventional behavior.","This includes a triangular VL with a period consistent with 2e-pairing, a field dependent scattering intensity that follows a London model, and a temperature dependence consistent with a uniform superconducting gap expected for s-wave pairing.","These results suggest that optimal bulk superconductivity in Cs(V1-xTax)3Sb5 arises from a conventional Bardeen-Cooper-Schrieffer electron-lattice coupling, different from spin fluctuation mediated unconventional copper and iron based superconductors."],"url":"http://arxiv.org/abs/2403.06046v1","category":"cond-mat.supr-con"}
{"created":"2024-03-09 23:59:07","title":"Sample-Optimal Zero-Violation Safety For Continuous Control","abstract":"In this paper, we study the problem of ensuring safety with a few shots of samples for partially unknown systems. We first characterize a fundamental limit when producing safe actions is not possible due to insufficient information or samples. Then, we develop a technique that can generate provably safe actions and recovery behaviors using a minimum number of samples. In the performance analysis, we also establish Nagumos theorem - like results with relaxed assumptions, which is potentially useful in other contexts. Finally, we discuss how the proposed method can be integrated into a policy gradient algorithm to assure safety and stability with a handful of samples without stabilizing initial policies or generative models to probe safe actions.","sentences":["In this paper, we study the problem of ensuring safety with a few shots of samples for partially unknown systems.","We first characterize a fundamental limit when producing safe actions is not possible due to insufficient information or samples.","Then, we develop a technique that can generate provably safe actions and recovery behaviors using a minimum number of samples.","In the performance analysis, we also establish Nagumos theorem - like results with relaxed assumptions, which is potentially useful in other contexts.","Finally, we discuss how the proposed method can be integrated into a policy gradient algorithm to assure safety and stability with a handful of samples without stabilizing initial policies or generative models to probe safe actions."],"url":"http://arxiv.org/abs/2403.06045v1","category":"eess.SY"}
{"created":"2024-03-09 23:05:47","title":"The Flow Game: Leximin and Leximax Core Imputations","abstract":"Recently [Vaz24] gave mechanisms for finding leximin and leximax core imputations for the assignment game and remarked, \"Within the area of algorithm design, the \"right\" technique for solving several types of algorithmic questions was first discovered in the context of matching and later these insights were applied to other problems. We expect a similar phenomenon here.\" One of the games explicitly mentioned in this context was the flow game of Kalai and Zemel [KZ82]. In this paper, we give strongly polynomial time mechanisms for computing the leximin and leximax core imputations for the flow game, among the set of core imputations that are captured as optimal solutions to the dual LP. We address two versions: 1. The imputations are leximin and leximax with respect to the distance labels of edges. 2. The imputations are leximin and leximax with respect to the product of capacities of edges and their distance labels.","sentences":["Recently [Vaz24] gave mechanisms for finding leximin and leximax core imputations for the assignment game and remarked, \"Within the area of algorithm design, the \"right\" technique for solving several types of algorithmic questions was first discovered in the context of matching and later these insights were applied to other problems.","We expect a similar phenomenon here.\"","One of the games explicitly mentioned in this context was the flow game of Kalai and Zemel","[KZ82].","In this paper, we give strongly polynomial time mechanisms for computing the leximin and leximax core imputations for the flow game, among the set of core imputations that are captured as optimal solutions to the dual LP.","We address two versions: 1.","The imputations are leximin and leximax with respect to the distance labels of edges.","2.","The imputations are leximin and leximax with respect to the product of capacities of edges and their distance labels."],"url":"http://arxiv.org/abs/2403.06037v1","category":"cs.GT"}
{"created":"2024-03-09 22:33:19","title":"Fully discretized Sobolev gradient flow for the Gross-Pitaevskii eigenvalue problem","abstract":"For the ground state of the Gross-Pitaevskii (GP) eigenvalue problem, we consider a fully discretized Sobolev gradient flow, which can be regarded as the Riemannian gradient descent on the sphere under a metric induced by a modified $H^1$-norm. We prove its global convergence to a critical point of the discrete GP energy and its local exponential convergence to the ground state of the discrete GP energy. The local exponential convergence rate depends on the eigengap of the discrete GP energy. When the discretization is the classical second-order finite difference in two dimensions, such an eigengap can be further proven to be mesh independent, i.e., it has a uniform positive lower bound, thus the local exponential convergence rate is mesh independent. Numerical experiments with discretization by high order $Q^k$ spectral element methods in two and three dimensions are provided to validate the efficiency of the proposed method.","sentences":["For the ground state of the Gross-Pitaevskii (GP) eigenvalue problem, we consider a fully discretized Sobolev gradient flow, which can be regarded as the Riemannian gradient descent on the sphere under a metric induced by a modified $H^1$-norm.","We prove its global convergence to a critical point of the discrete GP energy and its local exponential convergence to the ground state of the discrete GP energy.","The local exponential convergence rate depends on the eigengap of the discrete GP energy.","When the discretization is the classical second-order finite difference in two dimensions, such an eigengap can be further proven to be mesh independent, i.e., it has a uniform positive lower bound, thus the local exponential convergence rate is mesh independent.","Numerical experiments with discretization by high order $Q^k$ spectral element methods in two and three dimensions are provided to validate the efficiency of the proposed method."],"url":"http://arxiv.org/abs/2403.06028v1","category":"math.NA"}
{"created":"2024-03-09 21:10:10","title":"Reinforcement Learning Paycheck Optimization for Multivariate Financial Goals","abstract":"We study paycheck optimization, which examines how to allocate income in order to achieve several competing financial goals. For paycheck optimization, a quantitative methodology is missing, due to a lack of a suitable problem formulation. To deal with this issue, we formulate the problem as a utility maximization problem. The proposed formulation is able to (i) unify different financial goals; (ii) incorporate user preferences regarding the goals; (iii) handle stochastic interest rates. The proposed formulation also facilitates an end-to-end reinforcement learning solution, which is implemented on a variety of problem settings.","sentences":["We study paycheck optimization, which examines how to allocate income in order to achieve several competing financial goals.","For paycheck optimization, a quantitative methodology is missing, due to a lack of a suitable problem formulation.","To deal with this issue, we formulate the problem as a utility maximization problem.","The proposed formulation is able to (i) unify different financial goals; (ii) incorporate user preferences regarding the goals; (iii) handle stochastic interest rates.","The proposed formulation also facilitates an end-to-end reinforcement learning solution, which is implemented on a variety of problem settings."],"url":"http://arxiv.org/abs/2403.06011v1","category":"cs.LG"}
{"created":"2024-03-09 20:09:40","title":"Locally Regular and Efficient Tests in Non-Regular Semiparametric Models","abstract":"This paper considers hypothesis testing in semiparametric models which may be non-regular. I show that C($\\alpha$) style tests are locally regular under mild conditions, including in cases where locally regular estimators do not exist, such as models which are (semi-parametrically) weakly identified. I characterise the appropriate limit experiment in which to study local (asymptotic) optimality of tests in the non-regular case, permitting the generalisation of classical power bounds to this case. I give conditions under which these power bounds are attained by the proposed C($\\alpha$) style tests. The application of the theory to a single index model and an instrumental variables model is worked out in detail.","sentences":["This paper considers hypothesis testing in semiparametric models which may be non-regular.","I show that C($\\alpha$) style tests are locally regular under mild conditions, including in cases where locally regular estimators do not exist, such as models which are (semi-parametrically) weakly identified.","I characterise the appropriate limit experiment in which to study local (asymptotic) optimality of tests in the non-regular case, permitting the generalisation of classical power bounds to this case.","I give conditions under which these power bounds are attained by the proposed C($\\alpha$) style tests.","The application of the theory to a single index model and an instrumental variables model is worked out in detail."],"url":"http://arxiv.org/abs/2403.05999v1","category":"econ.EM"}
{"created":"2024-03-09 19:40:24","title":"Efficient Fault Detection and Categorization in Electrical Distribution Systems Using Hessian Locally Linear Embedding on Measurement Data","abstract":"Faults on electrical power lines could severely compromise both the reliability and safety of power systems, leading to unstable power delivery and increased outage risks. They pose significant safety hazards, necessitating swift detection and mitigation to maintain electrical infrastructure integrity and ensure continuous power supply. Hence, accurate detection and categorization of electrical faults are pivotal for optimized power system maintenance and operation. In this work, we propose a novel approach for detecting and categorizing electrical faults using the Hessian locally linear embedding (HLLE) technique and subsequent clustering with t-SNE (t-distributed stochastic neighbor embedding) and Gaussian mixture model (GMM). First, we employ HLLE to transform high-dimensional (HD) electrical data into low-dimensional (LD) embedding coordinates. This technique effectively captures the inherent variations and patterns in the data, enabling robust feature extraction. Next, we perform the Mann-Whitney U test based on the feature space of the embedding coordinates for fault detection. This statistical approach allows us to detect electrical faults providing an efficient means of system monitoring and control. Furthermore, to enhance fault categorization, we employ t-SNE with GMM to cluster the detected faults into various categories. To evaluate the performance of the proposed method, we conduct extensive simulations on an electrical system integrated with solar farm. Our results demonstrate that the proposed approach exhibits effective fault detection and clustering across a range of fault types with different variations of the same fault. Overall, this research presents an effective methodology for robust fault detection and categorization in electrical systems, contributing to the advancement of fault management practices and the prevention of system failures.","sentences":["Faults on electrical power lines could severely compromise both the reliability and safety of power systems, leading to unstable power delivery and increased outage risks.","They pose significant safety hazards, necessitating swift detection and mitigation to maintain electrical infrastructure integrity and ensure continuous power supply.","Hence, accurate detection and categorization of electrical faults are pivotal for optimized power system maintenance and operation.","In this work, we propose a novel approach for detecting and categorizing electrical faults using the Hessian locally linear embedding (HLLE) technique and subsequent clustering with t-SNE (t-distributed stochastic neighbor embedding) and Gaussian mixture model (GMM).","First, we employ HLLE to transform high-dimensional (HD) electrical data into low-dimensional (LD) embedding coordinates.","This technique effectively captures the inherent variations and patterns in the data, enabling robust feature extraction.","Next, we perform the Mann-Whitney U test based on the feature space of the embedding coordinates for fault detection.","This statistical approach allows us to detect electrical faults providing an efficient means of system monitoring and control.","Furthermore, to enhance fault categorization, we employ t-SNE with GMM to cluster the detected faults into various categories.","To evaluate the performance of the proposed method, we conduct extensive simulations on an electrical system integrated with solar farm.","Our results demonstrate that the proposed approach exhibits effective fault detection and clustering across a range of fault types with different variations of the same fault.","Overall, this research presents an effective methodology for robust fault detection and categorization in electrical systems, contributing to the advancement of fault management practices and the prevention of system failures."],"url":"http://arxiv.org/abs/2403.05995v1","category":"eess.SY"}
{"created":"2024-03-09 18:34:59","title":"Enhancing Classification Performance via Reinforcement Learning for Feature Selection","abstract":"Feature selection plays a crucial role in improving predictive accuracy by identifying relevant features while filtering out irrelevant ones. This study investigates the importance of effective feature selection in enhancing the performance of classification models. By employing reinforcement learning (RL) algorithms, specifically Q-learning (QL) and SARSA learning, this paper addresses the feature selection challenge. Using the Breast Cancer Coimbra dataset (BCCDS) and three normalization methods (Min-Max, l1, and l2), the study evaluates the performance of these algorithms. Results show that QL@Min-Max and SARSA@l2 achieve the highest classification accuracies, reaching 87% and 88%, respectively. This highlights the effectiveness of RL-based feature selection methods in optimizing classification tasks, contributing to improved model accuracy and efficiency.","sentences":["Feature selection plays a crucial role in improving predictive accuracy by identifying relevant features while filtering out irrelevant ones.","This study investigates the importance of effective feature selection in enhancing the performance of classification models.","By employing reinforcement learning (RL) algorithms, specifically Q-learning (QL) and SARSA learning, this paper addresses the feature selection challenge.","Using the Breast Cancer Coimbra dataset (BCCDS) and three normalization methods (Min-Max, l1, and l2), the study evaluates the performance of these algorithms.","Results show that QL@Min-Max and SARSA@l2 achieve the highest classification accuracies, reaching 87% and 88%, respectively.","This highlights the effectiveness of RL-based feature selection methods in optimizing classification tasks, contributing to improved model accuracy and efficiency."],"url":"http://arxiv.org/abs/2403.05979v1","category":"cs.LG"}
{"created":"2024-03-09 18:28:25","title":"An Event-Based Approach for the Conservative Compression of Covariance Matrices","abstract":"This work introduces a flexible and versatile method for the data-efficient yet conservative transmission of covariance matrices, where a matrix element is only transmitted if a so-called triggering condition is satisfied for the element. Here, triggering conditions can be parametrized on a per-element basis, applied simultaneously to yield combined triggering conditions or applied only to certain subsets of elements. This allows, e.g., to specify transmission accuracies for individual elements or to constrain the bandwidth available for the transmission of subsets of elements. Additionally, a methodology for learning triggering condition parameters from an application-specific dataset is presented. The performance of the proposed approach is quantitatively assessed in terms of data reduction and conservativeness using estimate data derived from real-world vehicle trajectories from the InD-dataset, demonstrating substantial data reduction ratios with minimal over-conservativeness. The feasibility of learning triggering condition parameters is demonstrated.","sentences":["This work introduces a flexible and versatile method for the data-efficient yet conservative transmission of covariance matrices, where a matrix element is only transmitted if a so-called triggering condition is satisfied for the element.","Here, triggering conditions can be parametrized on a per-element basis, applied simultaneously to yield combined triggering conditions or applied only to certain subsets of elements.","This allows, e.g., to specify transmission accuracies for individual elements or to constrain the bandwidth available for the transmission of subsets of elements.","Additionally, a methodology for learning triggering condition parameters from an application-specific dataset is presented.","The performance of the proposed approach is quantitatively assessed in terms of data reduction and conservativeness using estimate data derived from real-world vehicle trajectories from the InD-dataset, demonstrating substantial data reduction ratios with minimal over-conservativeness.","The feasibility of learning triggering condition parameters is demonstrated."],"url":"http://arxiv.org/abs/2403.05977v1","category":"cs.RO"}
{"created":"2024-03-09 18:27:09","title":"Extracting Kinetic Information from Short-Time Trajectories: Relaxation and Disorder of Lossy Cavity Polaritons","abstract":"The emerging field of molecular cavity polaritons has stimulated a surge of experimental and theoretical activities and presents a unique opportunity to develop the many-body simulation methodology. This paper presents a numerical scheme for the extraction of key kinetic information of lossy cavity polaritons based on the transfer tensor method (TTM). Steady state, relaxation timescales and oscillatory phenomena can all be deduced directly from a set of transfer tensors without the need for long-time simulation. Moreover, we generalize TTM to disordered systems by sampling dynamical maps and achieve fast convergence to disordered-averaged dynamics using a small set of realizations. Together, these techniques provide a toolbox for characterizing the interplay of cavity loss, disorder, and cooperativity in polariton relaxation and allow us to predict unusual dependences on the initial excitation state, photon decay rate, strength of disorder, and the type of cavity models. Thus, we have demonstrated significant potential in the use of the TTM towards both the efficient computation of long-time polariton dynamics and the extraction of crucial kinetic information about polariton relaxation from a small set of short-time trajectories.","sentences":["The emerging field of molecular cavity polaritons has stimulated a surge of experimental and theoretical activities and presents a unique opportunity to develop the many-body simulation methodology.","This paper presents a numerical scheme for the extraction of key kinetic information of lossy cavity polaritons based on the transfer tensor method (TTM).","Steady state, relaxation timescales and oscillatory phenomena can all be deduced directly from a set of transfer tensors without the need for long-time simulation.","Moreover, we generalize TTM to disordered systems by sampling dynamical maps and achieve fast convergence to disordered-averaged dynamics using a small set of realizations.","Together, these techniques provide a toolbox for characterizing the interplay of cavity loss, disorder, and cooperativity in polariton relaxation and allow us to predict unusual dependences on the initial excitation state, photon decay rate, strength of disorder, and the type of cavity models.","Thus, we have demonstrated significant potential in the use of the TTM towards both the efficient computation of long-time polariton dynamics and the extraction of crucial kinetic information about polariton relaxation from a small set of short-time trajectories."],"url":"http://arxiv.org/abs/2403.05976v1","category":"quant-ph"}
{"created":"2024-03-09 17:37:05","title":"C3D: Cascade Control with Change Point Detection and Deep Koopman Learning for Autonomous Surface Vehicles","abstract":"In this paper, we discuss the development and deployment of a robust autonomous system capable of performing various tasks in the maritime domain under unknown dynamic conditions. We investigate a data-driven approach based on modular design for ease of transfer of autonomy across different maritime surface vessel platforms. The data-driven approach alleviates issues related to a priori identification of system models that may become deficient under evolving system behaviors or shifting, unanticipated, environmental influences. Our proposed learning-based platform comprises a deep Koopman system model and a change point detector that provides guidance on domain shifts prompting relearning under severe exogenous and endogenous perturbations. Motion control of the autonomous system is achieved via an optimal controller design. The Koopman linearized model naturally lends itself to a linear-quadratic regulator (LQR) control design. We propose the C3D control architecture Cascade Control with Change Point Detection and Deep Koopman Learning. The framework is verified in station keeping task on an ASV in both simulation and real experiments. The approach achieved at least 13.9 percent improvement in mean distance error in all test cases compared to the methods that do not consider system changes.","sentences":["In this paper, we discuss the development and deployment of a robust autonomous system capable of performing various tasks in the maritime domain under unknown dynamic conditions.","We investigate a data-driven approach based on modular design for ease of transfer of autonomy across different maritime surface vessel platforms.","The data-driven approach alleviates issues related to a priori identification of system models that may become deficient under evolving system behaviors or shifting, unanticipated, environmental influences.","Our proposed learning-based platform comprises a deep Koopman system model and a change point detector that provides guidance on domain shifts prompting relearning under severe exogenous and endogenous perturbations.","Motion control of the autonomous system is achieved via an optimal controller design.","The Koopman linearized model naturally lends itself to a linear-quadratic regulator (LQR) control design.","We propose the C3D control architecture Cascade Control with Change Point Detection and Deep Koopman Learning.","The framework is verified in station keeping task on an ASV in both simulation and real experiments.","The approach achieved at least 13.9 percent improvement in mean distance error in all test cases compared to the methods that do not consider system changes."],"url":"http://arxiv.org/abs/2403.05972v1","category":"cs.RO"}
{"created":"2024-03-09 17:17:07","title":"Can Generative Models Improve Self-Supervised Representation Learning?","abstract":"The rapid advancement in self-supervised learning (SSL) has highlighted its potential to leverage unlabeled data for learning powerful visual representations. However, existing SSL approaches, particularly those employing different views of the same image, often rely on a limited set of predefined data augmentations. This constrains the diversity and quality of transformations, which leads to sub-optimal representations. In this paper, we introduce a novel framework that enriches the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations. By directly conditioning generative models on a source image representation, our method enables the generation of diverse augmentations while maintaining the semantics of the source image, thus offering a richer set of data for self-supervised learning. Our experimental results demonstrate that our framework significantly enhances the quality of learned visual representations. This research demonstrates that incorporating generative models into the SSL workflow opens new avenues for exploring the potential of unlabeled visual data. This development paves the way for more robust and versatile representation learning techniques.","sentences":["The rapid advancement in self-supervised learning (SSL) has highlighted its potential to leverage unlabeled data for learning powerful visual representations.","However, existing SSL approaches, particularly those employing different views of the same image, often rely on a limited set of predefined data augmentations.","This constrains the diversity and quality of transformations, which leads to sub-optimal representations.","In this paper, we introduce a novel framework that enriches the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations.","By directly conditioning generative models on a source image representation, our method enables the generation of diverse augmentations while maintaining the semantics of the source image, thus offering a richer set of data for self-supervised learning.","Our experimental results demonstrate that our framework significantly enhances the quality of learned visual representations.","This research demonstrates that incorporating generative models into the SSL workflow opens new avenues for exploring the potential of unlabeled visual data.","This development paves the way for more robust and versatile representation learning techniques."],"url":"http://arxiv.org/abs/2403.05966v1","category":"cs.CV"}
{"created":"2024-03-09 17:04:55","title":"Multi-Robot Communication-Aware Cooperative Belief Space Planning with Inconsistent Beliefs: An Action-Consistent Approach","abstract":"Multi-robot belief space planning (MR-BSP) is essential for reliable and safe autonomy. While planning, each robot maintains a belief over the state of the environment and reasons how the belief would evolve in the future for different candidate actions. Yet, existing MR-BSP works have a common assumption that the beliefs of different robots are consistent at planning time. Such an assumption is often highly unrealistic, as it requires prohibitively extensive and frequent communication capabilities. In practice, each robot may have a different belief about the state of the environment. Crucially, when the beliefs of different robots are inconsistent, state-of-the-art MR-BSP approaches could result in a lack of coordination between the robots, and in general, could yield dangerous, unsafe and sub-optimal decisions. In this paper, we tackle this crucial gap. We develop a novel decentralized algorithm that is guaranteed to find a consistent joint action. For a given robot, our algorithm reasons for action preferences about 1) its local information, 2) what it perceives about the reasoning of the other robot, and 3) what it perceives about the reasoning of itself perceived by the other robot. This algorithm finds a consistent joint action whenever these steps yield the same best joint action obtained by reasoning about action preferences; otherwise, it self-triggers communication between the robots. Experimental results show efficacy of our algorithm in comparison with two baseline algorithms.","sentences":["Multi-robot belief space planning (MR-BSP) is essential for reliable and safe autonomy.","While planning, each robot maintains a belief over the state of the environment and reasons how the belief would evolve in the future for different candidate actions.","Yet, existing MR-BSP works have a common assumption that the beliefs of different robots are consistent at planning time.","Such an assumption is often highly unrealistic, as it requires prohibitively extensive and frequent communication capabilities.","In practice, each robot may have a different belief about the state of the environment.","Crucially, when the beliefs of different robots are inconsistent, state-of-the-art MR-BSP approaches could result in a lack of coordination between the robots, and in general, could yield dangerous, unsafe and sub-optimal decisions.","In this paper, we tackle this crucial gap.","We develop a novel decentralized algorithm that is guaranteed to find a consistent joint action.","For a given robot, our algorithm reasons for action preferences about 1) its local information, 2) what it perceives about the reasoning of the other robot, and 3) what it perceives about the reasoning of itself perceived by the other robot.","This algorithm finds a consistent joint action whenever these steps yield the same best joint action obtained by reasoning about action preferences; otherwise, it self-triggers communication between the robots.","Experimental results show efficacy of our algorithm in comparison with two baseline algorithms."],"url":"http://arxiv.org/abs/2403.05962v1","category":"cs.RO"}
{"created":"2024-03-09 16:38:18","title":"Operational Ergotropy: suboptimality of the geodesic drive","abstract":"We put forth a notion of optimality for extracting ergotropic work, derived from an energy constraint governing the necessary dynamics for work extraction in a quantum system. Within the traditional ergotropy framework, which predicts an infinite set of equivalent pacifying unitaries, we demonstrate that the optimal choice lies in driving along the geodesic connecting a given state to its corresponding passive state. Moreover, in a practical scenario where unitaries are inevitably affected by environmental factors, we refine the existing definition of ergotropy and introduce the notion of operational ergotropy. It enables the characterization of work extraction in noisy scenarios. We find that for certain typical noise models, the optimal choice which governs the Schrodinger part of the dynamics, aligns with the optimal drive in the unperturbed scenario. However, we demonstrate that such optimality is not universal by presenting an explicit counterexample. Additionally, within this generalized framework, we discuss the potential for faster work extraction from quantum systems in the presence of noise.","sentences":["We put forth a notion of optimality for extracting ergotropic work, derived from an energy constraint governing the necessary dynamics for work extraction in a quantum system.","Within the traditional ergotropy framework, which predicts an infinite set of equivalent pacifying unitaries, we demonstrate that the optimal choice lies in driving along the geodesic connecting a given state to its corresponding passive state.","Moreover, in a practical scenario where unitaries are inevitably affected by environmental factors, we refine the existing definition of ergotropy and introduce the notion of operational ergotropy.","It enables the characterization of work extraction in noisy scenarios.","We find that for certain typical noise models, the optimal choice which governs the Schrodinger part of the dynamics, aligns with the optimal drive in the unperturbed scenario.","However, we demonstrate that such optimality is not universal by presenting an explicit counterexample.","Additionally, within this generalized framework, we discuss the potential for faster work extraction from quantum systems in the presence of noise."],"url":"http://arxiv.org/abs/2403.05956v1","category":"quant-ph"}
{"created":"2024-03-09 16:27:15","title":"Unlocking Heisenberg Sensitivity with Sequential Weak Measurement Preparation","abstract":"We propose a state preparation protocol based on sequential measurements of a central spin coupled with a spin ensemble, and investigate the usefulness of the generated multi-spin states for quantum enhanced metrology. Our protocol is shown to generate highly entangled spin states, devoid of the necessity for non-linear spin interactions. The metrological sensitivity of the resulting state surpasses the standard quantum limit, reaching the Heisenberg limit under symmetric coupling strength conditions. We also explore asymmetric coupling strengths, identifying specific preparation windows in time for optimal sensitivity. Our findings introduce a novel method for generating large-scale, non-classical, entangled states, enhancing quantum-enhanced metrology within current experimental capabilities.","sentences":["We propose a state preparation protocol based on sequential measurements of a central spin coupled with a spin ensemble, and investigate the usefulness of the generated multi-spin states for quantum enhanced metrology.","Our protocol is shown to generate highly entangled spin states, devoid of the necessity for non-linear spin interactions.","The metrological sensitivity of the resulting state surpasses the standard quantum limit, reaching the Heisenberg limit under symmetric coupling strength conditions.","We also explore asymmetric coupling strengths, identifying specific preparation windows in time for optimal sensitivity.","Our findings introduce a novel method for generating large-scale, non-classical, entangled states, enhancing quantum-enhanced metrology within current experimental capabilities."],"url":"http://arxiv.org/abs/2403.05954v1","category":"quant-ph"}
{"created":"2024-03-09 15:50:25","title":"Model-Predictive Trajectory Generation for Autonomous Aerial Search and Coverage","abstract":"This paper addresses the trajectory planning problem for search and coverage missions with an Unmanned Aerial Vehicle (UAV). The objective is to devise optimal coverage trajectories based on a utility map describing prior region information, assumed to be effectively approximated by a Gaussian Mixture Model (GMM). We introduce a Model Predictive Control (MPC) algorithm employing a relaxed formulation that promotes the exploration of the map by preventing the UAV from revisiting previously covered areas. This is achieved by penalizing intersections between the UAV's visibility regions along its trajectory. The algorithm is assessed in MATLAB and validated in Gazebo, as well as in outdoor experimental tests. The results show that the proposed strategy can generate efficient and smooth trajectories for search and coverage missions.","sentences":["This paper addresses the trajectory planning problem for search and coverage missions with an Unmanned Aerial Vehicle (UAV).","The objective is to devise optimal coverage trajectories based on a utility map describing prior region information, assumed to be effectively approximated by a Gaussian Mixture Model (GMM).","We introduce a Model Predictive Control (MPC) algorithm employing a relaxed formulation that promotes the exploration of the map by preventing the UAV from revisiting previously covered areas.","This is achieved by penalizing intersections between the UAV's visibility regions along its trajectory.","The algorithm is assessed in MATLAB and validated in Gazebo, as well as in outdoor experimental tests.","The results show that the proposed strategy can generate efficient and smooth trajectories for search and coverage missions."],"url":"http://arxiv.org/abs/2403.05944v1","category":"cs.RO"}
{"created":"2024-03-11 17:45:18","title":"Gluon Double-Spin Asymmetry in the Longitudinally Polarized $p+p$ Collisions","abstract":"We derive the first-ever small-$x$ expression for the inclusive gluon production cross section in the central rapidity region of the longitudinally polarized proton-proton collisions. The cross section depends on the polarizations of both protons, therefore comprising the numerator of the longitudinal double-spin asymmetry $A_{LL}$ for the produced gluons. The cross section is calculated in the shock wave formalism and is expressed in terms of the polarized dipole scattering amplitudes on the projectile and target protons. We show that the small-$x$ evolution corrections are included into our cross section expression if one evolves these polarized dipole amplitudes using the double-logarithmic helicity evolution derived in \\cite{Kovchegov:2015pbl, Kovchegov:2016zex, Kovchegov:2018znm, Cougoulic:2022gbk}. Our calculation is performed for the gluon sector only, with the quark contribution left for future work. When that work is complete, the resulting formula will be applicable to longitudinally polarized proton-proton and proton-nucleus collisions, as well as to polarized semi-inclusive deep inelastic scattering (SIDIS) on a proton or a nucleus. Our results should allow one to extend the small-$x$ helicity phenomenology analysis of \\cite{Adamiak:2023yhz} to the jet/hadron production data reported for the longitudinally polarized proton-proton collisions at RHIC and to polarized SIDIS measurements at central rapidities to be performed at the EIC.","sentences":["We derive the first-ever small-$x$ expression for the inclusive gluon production cross section in the central rapidity region of the longitudinally polarized proton-proton collisions.","The cross section depends on the polarizations of both protons, therefore comprising the numerator of the longitudinal double-spin asymmetry $A_{LL}$ for the produced gluons.","The cross section is calculated in the shock wave formalism and is expressed in terms of the polarized dipole scattering amplitudes on the projectile and target protons.","We show that the small-$x$ evolution corrections are included into our cross section expression if one evolves these polarized dipole amplitudes using the double-logarithmic helicity evolution derived in \\cite{Kovchegov:2015pbl, Kovchegov:2016zex, Kovchegov:2018znm, Cougoulic:2022gbk}.","Our calculation is performed for the gluon sector only, with the quark contribution left for future work.","When that work is complete, the resulting formula will be applicable to longitudinally polarized proton-proton and proton-nucleus collisions, as well as to polarized semi-inclusive deep inelastic scattering (SIDIS) on a proton or a nucleus.","Our results should allow one to extend the small-$x$ helicity phenomenology analysis of \\cite{Adamiak:2023yhz} to the jet/hadron production data reported for the longitudinally polarized proton-proton collisions at RHIC and to polarized SIDIS measurements at central rapidities to be performed at the EIC."],"url":"http://arxiv.org/abs/2403.06959v1","category":"hep-ph"}
{"created":"2024-03-11 17:33:30","title":"On the face stratification of the $m=2$ amplituhedron","abstract":"We define and study the face stratification of the m=2 amplituhedron. We show that the face poset is an upper order ideal in the face poset of the totally nonnegative Grassmannian. Our construction is consistent with earlier work of Lukowski, and we confirm various predictions of Lukowski.","sentences":["We define and study the face stratification of the m=2 amplituhedron.","We show that the face poset is an upper order ideal in the face poset of the totally nonnegative Grassmannian.","Our construction is consistent with earlier work of Lukowski, and we confirm various predictions of Lukowski."],"url":"http://arxiv.org/abs/2403.06948v1","category":"math.CO"}
{"created":"2024-03-11 17:30:22","title":"AI as a Child of Mother Earth: Regrounding Human-AI Interaction in Ecological Thinking","abstract":"The anthropocentric cultural idea that humans are active agents exerting control over their environments has been largely normalized and inscribed in practices, policies, and products of contemporary industrialized societies. This view underlies a human-ecology relationship based on resource and knowledge extraction. To create a more sustainable and equitable future, it is essential to consider alternative cultural ideas rooted in ecological thinking. This perspective underscores the interconnectedness between humans and more-than-human worlds. We propose a path to reshape the human-ecology relationship by advocating for alternative human-AI interactions. In this paper, we undertake a critical comparison between anthropocentrism and ecological thinking, using storytelling to illustrate various human-AI interactions that embody ecological thinking. We also delineate a set of design principles aimed at guiding AI developments toward fostering a more caring human-ecology relationship.","sentences":["The anthropocentric cultural idea that humans are active agents exerting control over their environments has been largely normalized and inscribed in practices, policies, and products of contemporary industrialized societies.","This view underlies a human-ecology relationship based on resource and knowledge extraction.","To create a more sustainable and equitable future, it is essential to consider alternative cultural ideas rooted in ecological thinking.","This perspective underscores the interconnectedness between humans and more-than-human worlds.","We propose a path to reshape the human-ecology relationship by advocating for alternative human-AI interactions.","In this paper, we undertake a critical comparison between anthropocentrism and ecological thinking, using storytelling to illustrate various human-AI interactions that embody ecological thinking.","We also delineate a set of design principles aimed at guiding AI developments toward fostering a more caring human-ecology relationship."],"url":"http://arxiv.org/abs/2403.06943v1","category":"cs.HC"}
{"created":"2024-03-11 17:25:24","title":"Surface lattice resonance lasers with epitaxial InP gain medium","abstract":"Surface lattice resonance (SLR) lasers, where gain is supplied by a thin film active material and the feedback comes from multiple scattering by plasmonic nanoparticles, have shown both low threshold lasing and tunability of the angular and spectral emission. However, typically used materials such as organic dyes and QD films suffer from photo-degradation which hampers practical applications. Here, we demonstrate photo-stable single-mode lasing of SLR modes sustained in an epitaxial solid-state InP slab waveguide. The nanoparticle array is weakly coupled to the optical modes, which decreases the scattering losses and hence the experimental lasing threshold is as low as 90 $\\mu$J/cm$^{2}$. The nanoparticle periodicity defines the lasing wavelength and enables tuneable emission wavelengths over a 70 nm spectral range. Combining plasmonic nanoparticles with an epitaxial solid-state gain medium paves the way for large-area on-chip integrated SLR lasers for applications including optical communication, optical computing, sensing, and LiDAR.","sentences":["Surface lattice resonance (SLR) lasers, where gain is supplied by a thin film active material and the feedback comes from multiple scattering by plasmonic nanoparticles, have shown both low threshold lasing and tunability of the angular and spectral emission.","However, typically used materials such as organic dyes and QD films suffer from photo-degradation which hampers practical applications.","Here, we demonstrate photo-stable single-mode lasing of SLR modes sustained in an epitaxial solid-state InP slab waveguide.","The nanoparticle array is weakly coupled to the optical modes, which decreases the scattering losses and hence the experimental lasing threshold is as low as 90 $\\mu$J/cm$^{2}$.","The nanoparticle periodicity defines the lasing wavelength and enables tuneable emission wavelengths over a 70 nm spectral range.","Combining plasmonic nanoparticles with an epitaxial solid-state gain medium paves the way for large-area on-chip integrated SLR lasers for applications including optical communication, optical computing, sensing, and LiDAR."],"url":"http://arxiv.org/abs/2403.06939v1","category":"physics.optics"}
{"created":"2024-03-11 17:17:08","title":"NLO+NLL' accurate predictions for three-jet event shapes in hadronic Higgs decays","abstract":"We present resummed predictions at next-to-leading logarithmic accuracy matched to the exact next-to-leading order results for a set of classical event-shape observables in hadronic Higgs decays, i.e., for the channels $H\\to gg$ and $H\\to b\\bar{b}$. We furthermore consider soft-drop grooming of the hadronic final states and derive corresponding $\\text{NLO}+\\text{NLL}^\\prime$ predictions for the groomed thrust observable. Differences in the QCD radiation pattern of gluon- and quark-initiated final states are imprinted in the event-shape distributions, offering separation power for the two decay channels. In particular, we show that ungroomed event shapes in $H\\to gg$ decays develop a considerably harder spectrum than in $H\\to b\\bar b$ decays. We highlight that soft-drop grooming can substantially alter this behaviour, unless rather inclusive grooming parameters are chosen.","sentences":["We present resummed predictions at next-to-leading logarithmic accuracy matched to the exact next-to-leading order results for a set of classical event-shape observables in hadronic Higgs decays, i.e., for the channels $H\\to gg$ and $H\\to b\\bar{b}$. We furthermore consider soft-drop grooming of the hadronic final states and derive corresponding $\\text{NLO}+\\text{NLL}^\\prime$ predictions for the groomed thrust observable.","Differences in the QCD radiation pattern of gluon- and quark-initiated final states are imprinted in the event-shape distributions, offering separation power for the two decay channels.","In particular, we show that ungroomed event shapes in $H\\to gg$ decays develop a considerably harder spectrum than in $H\\to b\\bar b$ decays.","We highlight that soft-drop grooming can substantially alter this behaviour, unless rather inclusive grooming parameters are chosen."],"url":"http://arxiv.org/abs/2403.06929v1","category":"hep-ph"}
{"created":"2024-03-11 17:06:25","title":"Hydrogen Column Density Variability in a Sample of Local Compton-Thin AGN II","abstract":"We present the multi-epoch analysis of 13 variable, nearby (z<0.1), Compton-thin (22<logN_H<24) active galactic nuclei (AGN) selected from the 105-month BAT catalog. Analyzing all available archival soft and hard X-ray observations, we investigate the line-of-sight hydrogen column density (N_H) variability on timescales ranging from a few days to approximately 20 years. Each source is analyzed by simultaneously modeling the data with three physical torus models, providing tight constraints on torus properties, including the covering factor, the cloud dispersion, and the torus average hydrogen column density (N_H,av). For each epoch, we measure the N_H and categorize the source as `N_H Variable', `Non-variable in N_H', or `Undetermined' based on the degree of variability. Our final sample includes 27 variable, Compton-thin AGN after implementing another 14 AGN analyzed in our previous work. We find that all sources require either flux or N_H variability. We classify 37% of them as `N_H Variable', 44% as `Non-variable in N_H', and 19% as `Undetermined'. Noticeably, there is no discernible difference between geometrical and intrinsic properties among the three variability classes, suggesting no intrinsic differences between the N_H-variable and non-variable sources. We measure the median variation in N_H between any observation pair of the same source to be 25% with respect to the lowest N_H measure in the pair. Furthermore, 48% of the analyzed sources require the inclusion of a Compton-thick reflector in the spectral fitting. Among these, the 30% exhibits recorded 22 GHz water megamaser emission, suggesting a potential shared nature between the two structures.","sentences":["We present the multi-epoch analysis of 13 variable, nearby (z<0.1), Compton-thin (22<logN_H<24) active galactic nuclei (AGN) selected from the 105-month BAT catalog.","Analyzing all available archival soft and hard X-ray observations, we investigate the line-of-sight hydrogen column density (N_H) variability on timescales ranging from a few days to approximately 20 years.","Each source is analyzed by simultaneously modeling the data with three physical torus models, providing tight constraints on torus properties, including the covering factor, the cloud dispersion, and the torus average hydrogen column density (N_H,av).","For each epoch, we measure the N_H and categorize the source as `N_H Variable', `Non-variable in N_H', or `Undetermined' based on the degree of variability.","Our final sample includes 27 variable, Compton-thin AGN after implementing another 14 AGN analyzed in our previous work.","We find that all sources require either flux or N_H variability.","We classify 37% of them as `N_H Variable', 44% as `Non-variable in N_H', and 19% as `Undetermined'.","Noticeably, there is no discernible difference between geometrical and intrinsic properties among the three variability classes, suggesting no intrinsic differences between the N_H-variable and non-variable sources.","We measure the median variation in N_H between any observation pair of the same source to be 25% with respect to the lowest N_H measure in the pair.","Furthermore, 48% of the analyzed sources require the inclusion of a Compton-thick reflector in the spectral fitting.","Among these, the 30% exhibits recorded 22 GHz water megamaser emission, suggesting a potential shared nature between the two structures."],"url":"http://arxiv.org/abs/2403.06919v1","category":"astro-ph.HE"}
{"created":"2024-03-11 16:58:13","title":"Towards Incident Response Orchestration and Automation for the Advanced Metering Infrastructure","abstract":"The threat landscape of industrial infrastructures has expanded exponentially over the last few years. Such infrastructures include services such as the smart meter data exchange that should have real-time availability. Smart meters constitute the main component of the Advanced Metering Infrastructure, and their measurements are also used as historical data for forecasting the energy demand to avoid load peaks that could lead to blackouts within specific areas. Hence, a comprehensive Incident Response plan must be in place to ensure high service availability in case of cyber-attacks or operational errors. Currently, utility operators execute such plans mostly manually, requiring extensive time, effort, and domain expertise, and they are prone to human errors. In this paper, we present a method to provide an orchestrated and highly automated Incident Response plan targeting specific use cases and attack scenarios in the energy sector, including steps for preparedness, detection and analysis, containment, eradication, recovery, and post-incident activity through the use of playbooks. In particular, we use the OASIS Collaborative Automated Course of Action Operations (CACAO) standard to define highly automatable workflows in support of cyber security operations for the Advanced Metering Infrastructure. The proposed method is validated through an Advanced Metering Infrastructure testbed where the most prominent cyber-attacks are emulated, and playbooks are instantiated to ensure rapid response for the containment and eradication of the threat, business continuity on the smart meter data exchange service, and compliance with incident reporting requirements.","sentences":["The threat landscape of industrial infrastructures has expanded exponentially over the last few years.","Such infrastructures include services such as the smart meter data exchange that should have real-time availability.","Smart meters constitute the main component of the Advanced Metering Infrastructure, and their measurements are also used as historical data for forecasting the energy demand to avoid load peaks that could lead to blackouts within specific areas.","Hence, a comprehensive Incident Response plan must be in place to ensure high service availability in case of cyber-attacks or operational errors.","Currently, utility operators execute such plans mostly manually, requiring extensive time, effort, and domain expertise, and they are prone to human errors.","In this paper, we present a method to provide an orchestrated and highly automated Incident Response plan targeting specific use cases and attack scenarios in the energy sector, including steps for preparedness, detection and analysis, containment, eradication, recovery, and post-incident activity through the use of playbooks.","In particular, we use the OASIS Collaborative Automated Course of Action Operations (CACAO) standard to define highly automatable workflows in support of cyber security operations for the Advanced Metering Infrastructure.","The proposed method is validated through an Advanced Metering Infrastructure testbed where the most prominent cyber-attacks are emulated, and playbooks are instantiated to ensure rapid response for the containment and eradication of the threat, business continuity on the smart meter data exchange service, and compliance with incident reporting requirements."],"url":"http://arxiv.org/abs/2403.06907v1","category":"cs.CR"}
{"created":"2024-03-11 16:49:59","title":"GRITv2: Efficient and Light-weight Social Relation Recognition","abstract":"Our research focuses on the analysis and improvement of the Graph-based Relation Inference Transformer (GRIT), which serves as an important benchmark in the field. We conduct a comprehensive ablation study using the PISC-fine dataset, to find and explore improvement in efficiency and performance of GRITv2. Our research has provided a new state-of-the-art relation recognition model on the PISC relation dataset. We introduce several features in the GRIT model and analyse our new benchmarks in two versions: GRITv2-L (large) and GRITv2-S (small). Our proposed GRITv2-L surpasses existing methods on relation recognition and the GRITv2-S is within 2% performance gap of GRITv2-L, which has only 0.0625x the model size and parameters of GRITv2-L. Furthermore, we also address the need for model compression, an area crucial for deploying efficient models on resource-constrained platforms. By applying quantization techniques, we efficiently reduced the GRITv2-S size to 22MB and deployed it on the flagship OnePlus 12 mobile which still surpasses the PISC-fine benchmarks in performance, highlighting the practical viability and improved efficiency of our model on mobile devices.","sentences":["Our research focuses on the analysis and improvement of the Graph-based Relation Inference Transformer (GRIT), which serves as an important benchmark in the field.","We conduct a comprehensive ablation study using the PISC-fine dataset, to find and explore improvement in efficiency and performance of GRITv2.","Our research has provided a new state-of-the-art relation recognition model on the PISC relation dataset.","We introduce several features in the GRIT model and analyse our new benchmarks in two versions: GRITv2-L (large) and GRITv2-S (small).","Our proposed GRITv2-L surpasses existing methods on relation recognition and the GRITv2-S is within 2% performance gap of GRITv2-L, which has only 0.0625x the model size and parameters of GRITv2-L.","Furthermore, we also address the need for model compression, an area crucial for deploying efficient models on resource-constrained platforms.","By applying quantization techniques, we efficiently reduced the GRITv2-S size to 22MB and deployed it on the flagship OnePlus 12 mobile which still surpasses the PISC-fine benchmarks in performance, highlighting the practical viability and improved efficiency of our model on mobile devices."],"url":"http://arxiv.org/abs/2403.06895v1","category":"cs.CV"}
{"created":"2024-03-11 16:48:25","title":"Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head","abstract":"End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO. Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied. Notably, in zero-shot scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art supervised models. Furthermore, it establishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an AP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of OmDet-Turbo in industrial applications is underscored by its exceptional performance on benchmark datasets and superior inference speed, positioning it as a compelling choice for real-time object detection tasks. Code: \\url{https://github.com/om-ai-lab/OmDet}","sentences":["End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities.","However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios.","In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and Grounding-DINO, and introduce OmDet-Turbo.","This novel transformer-based real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO.","Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied.","Notably, in zero-shot scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art supervised models.","Furthermore, it establishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an AP of 30.1 and an NMS-AP of 26.86, respectively.","The practicality of OmDet-Turbo in industrial applications is underscored by its exceptional performance on benchmark datasets and superior inference speed, positioning it as a compelling choice for real-time object detection tasks.","Code: \\url{https://github.com/om-ai-lab/OmDet}"],"url":"http://arxiv.org/abs/2403.06892v1","category":"cs.CV"}
{"created":"2024-03-11 16:33:56","title":"Partially identified heteroskedastic SVARs","abstract":"This paper studies the identification of Structural Vector Autoregressions (SVARs) exploiting a break in the variances of the structural shocks. Point-identification for this class of models relies on an eigen-decomposition involving the covariance matrices of reduced-form errors and requires that all the eigenvalues are distinct. This point-identification, however, fails in the presence of multiplicity of eigenvalues. This occurs in an empirically relevant scenario where, for instance, only a subset of structural shocks had the break in their variances, or where a group of variables shows a variance shift of the same amount. Together with zero or sign restrictions on the structural parameters and impulse responses, we derive the identified sets for impulse responses and show how to compute them. We perform inference on the impulse response functions, building on the robust Bayesian approach developed for set identified SVARs. To illustrate our proposal, we present an empirical example based on the literature on the global crude oil market where the identification is expected to fail due to multiplicity of eigenvalues.","sentences":["This paper studies the identification of Structural Vector Autoregressions (SVARs) exploiting a break in the variances of the structural shocks.","Point-identification for this class of models relies on an eigen-decomposition involving the covariance matrices of reduced-form errors and requires that all the eigenvalues are distinct.","This point-identification, however, fails in the presence of multiplicity of eigenvalues.","This occurs in an empirically relevant scenario where, for instance, only a subset of structural shocks had the break in their variances, or where a group of variables shows a variance shift of the same amount.","Together with zero or sign restrictions on the structural parameters and impulse responses, we derive the identified sets for impulse responses and show how to compute them.","We perform inference on the impulse response functions, building on the robust Bayesian approach developed for set identified SVARs.","To illustrate our proposal, we present an empirical example based on the literature on the global crude oil market where the identification is expected to fail due to multiplicity of eigenvalues."],"url":"http://arxiv.org/abs/2403.06879v1","category":"econ.EM"}
{"created":"2024-03-11 16:13:33","title":"Radial Tully-Fisher relation and the local variance of Hubble parameter","abstract":"Utilizing the well-established Radial Tully-Fisher (RTF) relation observed in a `large' (843) sample of local galaxies, we report the maximum allowed variance in the Hubble parameter, $H_0$. We estimate the total intrinsic scatter in the magnitude of the RTF relation(s) implementing a cosmological model-independent cosmographic expansion. We find that the maximum allowed local variation in our baseline analysis, using 4 RTF relations in the galaxy sample is $\\Delta H_0/H_0 \\lesssim 3 \\%$ at a $95\\%$ C.L. significance. Which is implied form a constraint of $\\Delta H_0/H_0 = 0.54^{+1.32}_{-1.37} \\%$ estimated at $D_{\\rm{L}}\\sim 10\\, [\\rm{Mpc}]$. Using only one `best-constrained' radial bin we report a conservative $95\\%$ C.L. limit of $\\Delta H_0/H_0 \\lesssim 4 \\%$. Through our estimate of maximum variation, we propose a novel method to validate several late-time/local modifications put forth to alleviate the $H_0$ tension. We find that within the range of the current galaxy sample redshift distribution $10 \\, [\\rm{Mpc}] \\le D_{\\rm{L}} \\le 140\\, [\\rm{Mpc}]$, it is highly unlikely to obtain a variation of $\\Delta H_0/H_0 \\sim 9\\%$, necessary to alleviate the $H_0$-tension. However, we also elaborate on the possible alternative inferences when the innermost radial bin is included in the analysis. Alongside the primary analysis of fitting the individual RTF relations independently, we propose and perform a joint analysis of the RTF relations useful to create a pseudo-standardizable sample of galaxies. We also test for the spatial variation of $H_0$, finding that the current samples' galaxies distributed only in the southern hemisphere support the null hypothesis of isotropy, within the allowed noise levels.","sentences":["Utilizing the well-established Radial Tully-Fisher (RTF) relation observed in a `large' (843) sample of local galaxies, we report the maximum allowed variance in the Hubble parameter, $H_0$. We estimate the total intrinsic scatter in the magnitude of the RTF relation(s) implementing a cosmological model-independent cosmographic expansion.","We find that the maximum allowed local variation in our baseline analysis, using 4 RTF relations in the galaxy sample is $\\Delta H_0/H_0 \\lesssim 3 \\%$ at a $95\\%$ C.L. significance.","Which is implied form a constraint of $\\Delta H_0/H_0 = 0.54^{+1.32}_{-1.37} \\%$ estimated at $D_{\\rm{L}}\\sim 10\\, [\\rm{Mpc}]$. Using only one `best-constrained' radial bin we report a conservative $95\\%$ C.L. limit of $\\Delta H_0/H_0 \\lesssim 4 \\%$. Through our estimate of maximum variation, we propose a novel method to validate several late-time/local modifications put forth to alleviate the $H_0$ tension.","We find that within the range of the current galaxy sample redshift distribution $10 \\, [\\rm{Mpc}] \\le D_{\\rm{L}} \\le 140\\, [\\rm{Mpc}]$, it is highly unlikely to obtain a variation of $\\Delta H_0/H_0 \\sim 9\\%$, necessary to alleviate the $H_0$-tension.","However, we also elaborate on the possible alternative inferences when the innermost radial bin is included in the analysis.","Alongside the primary analysis of fitting the individual RTF relations independently, we propose and perform a joint analysis of the RTF relations useful to create a pseudo-standardizable sample of galaxies.","We also test for the spatial variation of $H_0$, finding that the current samples' galaxies distributed only in the southern hemisphere support the null hypothesis of isotropy, within the allowed noise levels."],"url":"http://arxiv.org/abs/2403.06859v1","category":"astro-ph.CO"}
{"created":"2024-03-11 15:56:36","title":"The metallicity gradients of star-forming regions store information of the assembly history of galaxies","abstract":"The variations in metallicity and spatial patterns within star-forming regions of galaxies result from diverse physical processes unfolding throughout their evolutionary history, with a particular emphasis in recent events. Analysing MaNGA and \\textsc{eagle} galaxies, we discovered an additional dependence of the mass-metallicity relation (MZR) on metallicity gradients ($\\nabla_{{\\rm (O/H)}}$). Two regimes emerged for low and high stellar mass galaxies, distinctly separated at approximately ${\\rm M_{\\star}} >10^{9.75}$.   Low-mass galaxies with strong positive $\\nabla_{{\\rm (O/H)}}$ appear less enriched than the MZR median, while those with strong negative gradients are consistently more enriched in both simulated and observed samples. Interestingly, low-mass galaxies with strong negative $\\nabla_{{\\rm (O/H)}}$ exhibit high star-forming activity, regardless of stellar surface density or $\\nabla_{{\\rm (O/H)}}$. In contrast, a discrepancy arises for massive galaxies between MaNGA and \\textsc{eagle} datasets. The latter exhibit a notable anticorrelation between specific star formation rate and stellar surface density, independent of $\\nabla_{{\\rm (O/H)}}$, while MaNGA galaxies show this trend mainly for strong positive $\\nabla_{{\\rm (O/H)}}$. Further investigation indicates that galaxies with strong negative gradients tend to host smaller central black holes in observed datasets, a trend not replicated in simulations. These findings suggest disparities in metallicity recycling and mixing history between observations and simulations, particularly in massive galaxies with varying metallicity gradients. These distinctions could contribute to a more comprehensive understanding of the underlying physics.","sentences":["The variations in metallicity and spatial patterns within star-forming regions of galaxies result from diverse physical processes unfolding throughout their evolutionary history, with a particular emphasis in recent events.","Analysing MaNGA and \\textsc{eagle} galaxies, we discovered an additional dependence of the mass-metallicity relation (MZR) on metallicity gradients ($\\nabla_{{\\rm (O/H)}}$).","Two regimes emerged for low and high stellar mass galaxies, distinctly separated at approximately ${\\rm M_{\\star}} >10^{9.75}$.   Low-mass galaxies with strong positive $\\nabla_{{\\rm (O/H)}}$ appear less enriched than the MZR median, while those with strong negative gradients are consistently more enriched in both simulated and observed samples.","Interestingly, low-mass galaxies with strong negative $\\nabla_{{\\rm (O/H)}}$ exhibit high star-forming activity, regardless of stellar surface density or $\\nabla_{{\\rm (O/H)}}$.","In contrast, a discrepancy arises for massive galaxies between MaNGA and \\textsc{eagle} datasets.","The latter exhibit a notable anticorrelation between specific star formation rate and stellar surface density, independent of $\\nabla_{{\\rm (O/H)}}$, while MaNGA galaxies show this trend mainly for strong positive $\\nabla_{{\\rm (O/H)}}$. Further investigation indicates that galaxies with strong negative gradients tend to host smaller central black holes in observed datasets, a trend not replicated in simulations.","These findings suggest disparities in metallicity recycling and mixing history between observations and simulations, particularly in massive galaxies with varying metallicity gradients.","These distinctions could contribute to a more comprehensive understanding of the underlying physics."],"url":"http://arxiv.org/abs/2403.06836v1","category":"astro-ph.GA"}
{"created":"2024-03-11 15:40:35","title":"Dynamics of Polyalkylfluorene Conjugated Polymers: Insights from Neutron Spectroscopy and Molecular Dynamics Simulations","abstract":"Dynamics of the conjugated polymers poly(9,9-dioctylfluorene) (PF8) and poly(9,9-didodecylfluorene) (PF12), differing by the length of their side chains, is investigated in the amorphous phase using the quasielastic neutron scattering (QENS) technique. The neutron spectroscopy measurements are synergistically underpinned by molecular dynamics (MD) simulations. The probe is focused on the picosecond time scale where the structural dynamics of both PF8 and PF12 would mainly be dominated by the motions of their side chains. The measurements highlighted temperature-induced dynamics reflected in the broadening of the QENS spectra upon heating. The MD simulations reproduced well the observations, hence the neutron measurements validate the MD force fields, the adopted amorphous model structures and the numerical procedure. As the QENS spectra are dominated by the signal from the hydrogens on the backbones and side chains of PF8 and PF12, extensive analysis of the MD simulations allowed: (i) tagging these hydrogens, (ii) estimating their contributions to the self part of the van Hove functions and hence to the QENS spectra, and (iii) determining the activation energies of the different motions involving the tagged hydrogens. PF12 is found to exhibit broader QENS spectra than PF8, signature of a more pronounced motions of the didodecyl chains as compared to dioctyl chains. This is in agreement with the outcome of our MD analysis: (i) confirming a lower glass transition temperature of PF12 compared to PF8, (ii) showing PF12 having a lower density than PF8, and (iii) highlighting lower activation energies of the motions of PF12 in comparison with PF8. This study helped gaining insights into the temperature-induced side chains dynamics of the PF8 and PF12 conjugated polymers influencing their stability, and thus could practically impact the performance of the associated optoelectronic active layer.","sentences":["Dynamics of the conjugated polymers poly(9,9-dioctylfluorene) (PF8) and poly(9,9-didodecylfluorene) (PF12), differing by the length of their side chains, is investigated in the amorphous phase using the quasielastic neutron scattering (QENS) technique.","The neutron spectroscopy measurements are synergistically underpinned by molecular dynamics (MD) simulations.","The probe is focused on the picosecond time scale where the structural dynamics of both PF8 and PF12 would mainly be dominated by the motions of their side chains.","The measurements highlighted temperature-induced dynamics reflected in the broadening of the QENS spectra upon heating.","The MD simulations reproduced well the observations, hence the neutron measurements validate the MD force fields, the adopted amorphous model structures and the numerical procedure.","As the QENS spectra are dominated by the signal from the hydrogens on the backbones and side chains of PF8 and PF12, extensive analysis of the MD simulations allowed: (i) tagging these hydrogens, (ii) estimating their contributions to the self part of the van Hove functions and hence to the QENS spectra, and (iii) determining the activation energies of the different motions involving the tagged hydrogens.","PF12 is found to exhibit broader QENS spectra than PF8, signature of a more pronounced motions of the didodecyl chains as compared to dioctyl chains.","This is in agreement with the outcome of our MD analysis: (i) confirming a lower glass transition temperature of PF12 compared to PF8, (ii) showing PF12 having a lower density than PF8, and (iii) highlighting lower activation energies of the motions of PF12 in comparison with PF8.","This study helped gaining insights into the temperature-induced side chains dynamics of the PF8 and PF12 conjugated polymers influencing their stability, and thus could practically impact the performance of the associated optoelectronic active layer."],"url":"http://arxiv.org/abs/2403.06822v1","category":"cond-mat.soft"}
{"created":"2024-03-11 15:31:30","title":"Weak form Shallow Ice Approximation models with an improved time step restriction","abstract":"The Shallow Ice Approximation (SIA) model written on strong form is commonly used for inferring the dynamics of ice sheets and glaciers. The model describes non-Newtonian, viscous, and gravity driven flow of ice in grounded ice sheets. The solution to the SIA model is a closed-form expression for the velocity field. A disadvantage is that when using the SIA velocities to advance the ice surface in time, the time step restriction has a quadratic scaling in terms of the horizontal mesh size. In this paper we write the SIA model on weak form, and add in the Free Surface Stabilization Algorithm (FSSA) terms. We find numerically that the time step restriction scaling is improved from quadratic to linear, but only for large horizontal mesh sizes. We then extend the weak formulation by adding in the normal stress terms which are originally neglected. This allows for a linear time step restriction across the whole range of the horizontal mesh sizes and as such leads to a computationally more efficient SIA model. To support the numerical results we theoretically show that the addition of the FSSA stabilization terms switches the explicit time stepping treatment of the second derivative surface terms to an implicit time stepping treatment. In addition we perform a computational cost analysis, which, when combined with the numerical results on stability properties and accuracy, speaks for favouring SIA models on weak form over the standard SIA model.","sentences":["The Shallow Ice Approximation (SIA) model written on strong form is commonly used for inferring the dynamics of ice sheets and glaciers.","The model describes non-Newtonian, viscous, and gravity driven flow of ice in grounded ice sheets.","The solution to the SIA model is a closed-form expression for the velocity field.","A disadvantage is that when using the SIA velocities to advance the ice surface in time, the time step restriction has a quadratic scaling in terms of the horizontal mesh size.","In this paper we write the SIA model on weak form, and add in the Free Surface Stabilization Algorithm (FSSA) terms.","We find numerically that the time step restriction scaling is improved from quadratic to linear, but only for large horizontal mesh sizes.","We then extend the weak formulation by adding in the normal stress terms which are originally neglected.","This allows for a linear time step restriction across the whole range of the horizontal mesh sizes and as such leads to a computationally more efficient SIA model.","To support the numerical results we theoretically show that the addition of the FSSA stabilization terms switches the explicit time stepping treatment of the second derivative surface terms to an implicit time stepping treatment.","In addition we perform a computational cost analysis, which, when combined with the numerical results on stability properties and accuracy, speaks for favouring SIA models on weak form over the standard SIA model."],"url":"http://arxiv.org/abs/2403.06811v1","category":"math.NA"}
{"created":"2024-03-11 14:57:48","title":"A doubly robust estimator for the Mann Whitney Wilcoxon Rank Sum Test when applied for causal inference in observational studies","abstract":"The Mann-Whitney-Wilcoxon rank sum test (MWWRST) is a widely used method for comparing two treatment groups in randomized control trials, particularly when dealing with highly skewed data. However, when applied to observational study data, the MWWRST often yields invalid results for causal inference. To address this limitation, Wu et al. (2014) introduced an approach that incorporates inverse probability weighting (IPW) into this rank-based statistics to mitigate confounding effects. Subsequently, Mao (2018), Zhang et al. (2019), and Ai et al. (2020) extended this IPW estimator to develop doubly robust estimators.   Nevertheless, each of these approaches has notable limitations. Mao's method imposes stringent assumptions that may not align with real-world study data. Zhang et al.'s (2019) estimators rely on bootstrap inference, which suffers from computational inefficiency and lacks known asymptotic properties. Meanwhile, Ai et al. (2020) primarily focus on testing the null hypothesis of equal distributions between two groups, which is a more stringent assumption that may not be well-suited to the primary practical application of MWWRST.   In this paper, we aim to address these limitations by leveraging functional response models (FRM) to develop doubly robust estimators. We demonstrate the performance of our proposed approach using both simulated and real study data.","sentences":["The Mann-Whitney-Wilcoxon rank sum test (MWWRST) is a widely used method for comparing two treatment groups in randomized control trials, particularly when dealing with highly skewed data.","However, when applied to observational study data, the MWWRST often yields invalid results for causal inference.","To address this limitation, Wu et al.","(2014) introduced an approach that incorporates inverse probability weighting (IPW) into this rank-based statistics to mitigate confounding effects.","Subsequently, Mao (2018), Zhang et al. (2019), and Ai et al.","(2020) extended this IPW estimator to develop doubly robust estimators.   ","Nevertheless, each of these approaches has notable limitations.","Mao's method imposes stringent assumptions that may not align with real-world study data.","Zhang et al.'s (2019) estimators rely on bootstrap inference, which suffers from computational inefficiency and lacks known asymptotic properties.","Meanwhile, Ai et al. (2020) primarily focus on testing the null hypothesis of equal distributions between two groups, which is a more stringent assumption that may not be well-suited to the primary practical application of MWWRST.   ","In this paper, we aim to address these limitations by leveraging functional response models (FRM) to develop doubly robust estimators.","We demonstrate the performance of our proposed approach using both simulated and real study data."],"url":"http://arxiv.org/abs/2403.06783v1","category":"stat.ME"}
{"created":"2024-03-11 14:45:20","title":"Topological solitons stabilized by a background gauge field and soliton-anti-soliton asymmetry","abstract":"We study topological lumps supported by the second homotopy group $\\pi_2(S^2) \\simeq {\\mathbb Z}$ in a gauged $O(3)$ model without any potential term coupled with a (non)dynamical $U(1)$ gauge field. It is known that gauged-lumps are stable with an easy-plane potential term but are unstable to expand if the model has no potential term. In this paper, we find that these gauged lumps without a potential term can be made stable by putting them in a uniform magnetic field, irrespective of whether the gauge field is dynamical or not. In the case of the non-dynamical gauge field, only either of lumps or anti-lumps stably exists depending on the sign of the background magnetic field, and the other is unstable to shrink to be singular. We also construct coaxial multiple lumps whose size and mass exhibit a behaviour of droplets. In the case of the dynamical gauge field, both the lumps and anti-lumps stably exist with different masses; the lighter (heavier) one corresponds to the (un)stable one in the case of the nondynamical gauge field. We find that a lump behaves as a superconducting ring and traps magnetic field in its inside, with the total magnetic field reduced from the background magnetic field.","sentences":["We study topological lumps supported by the second homotopy group $\\pi_2(S^2) \\simeq {\\mathbb Z}$ in a gauged $O(3)$ model without any potential term coupled with a (non)dynamical $U(1)$ gauge field.","It is known that gauged-lumps are stable with an easy-plane potential term but are unstable to expand if the model has no potential term.","In this paper, we find that these gauged lumps without a potential term can be made stable by putting them in a uniform magnetic field, irrespective of whether the gauge field is dynamical or not.","In the case of the non-dynamical gauge field, only either of lumps or anti-lumps stably exists depending on the sign of the background magnetic field, and the other is unstable to shrink to be singular.","We also construct coaxial multiple lumps whose size and mass exhibit a behaviour of droplets.","In the case of the dynamical gauge field, both the lumps and anti-lumps stably exist with different masses; the lighter (heavier) one corresponds to the (un)stable one in the case of the nondynamical gauge field.","We find that a lump behaves as a superconducting ring and traps magnetic field in its inside, with the total magnetic field reduced from the background magnetic field."],"url":"http://arxiv.org/abs/2403.06778v1","category":"hep-th"}
{"created":"2024-03-11 14:44:59","title":"Fast classical simulation of quantum circuits via parametric rewriting in the ZX-calculus","abstract":"The ZX-calculus is an algebraic formalism that allows quantum computations to be simplified via a small number of simple graphical rewrite rules. Recently, it was shown that, when combined with a family of \"sum-over-Cliffords\" techniques, the ZX-calculus provides a powerful tool for classical simulation of quantum circuits. However, for several important classical simulation tasks, such as computing the probabilities associated with many measurement outcomes of a single quantum circuit, this technique results in reductions over many very similar diagrams, where much of the same computational work is repeated. In this paper, we show that the majority of this work can be shared across branches, by developing reduction strategies that can be run parametrically on diagrams with boolean free parameters. As parameters only need to be fixed after the bulk of the simplification work is already done, we show that it is possible to perform the final stage of classical simulation quickly utilising a high degree of GPU parallelism. Using these methods, we demonstrate speedups upwards of 100x for certain classical simulation tasks vs. the non-parametric approach.","sentences":["The ZX-calculus is an algebraic formalism that allows quantum computations to be simplified via a small number of simple graphical rewrite rules.","Recently, it was shown that, when combined with a family of \"sum-over-Cliffords\" techniques, the ZX-calculus provides a powerful tool for classical simulation of quantum circuits.","However, for several important classical simulation tasks, such as computing the probabilities associated with many measurement outcomes of a single quantum circuit, this technique results in reductions over many very similar diagrams, where much of the same computational work is repeated.","In this paper, we show that the majority of this work can be shared across branches, by developing reduction strategies that can be run parametrically on diagrams with boolean free parameters.","As parameters only need to be fixed after the bulk of the simplification work is already done, we show that it is possible to perform the final stage of classical simulation quickly utilising a high degree of GPU parallelism.","Using these methods, we demonstrate speedups upwards of 100x for certain classical simulation tasks vs. the non-parametric approach."],"url":"http://arxiv.org/abs/2403.06777v1","category":"quant-ph"}
{"created":"2024-03-11 14:38:28","title":"Estimates on the convergence of expansions at finite baryon chemical potentials","abstract":"Convergence of three different expansion schemes at finite baryon chemical potentials, including the conventional Taylor expansion, the Pad\\'e approximants, and the $T'$ expansion proposed recently in lattice QCD simulations, have been investigated in a low energy effective theory within the fRG approach. It is found that the $T'$ expansion or the Pad\\'e approximants would hardly improve the convergence of expansion in comparison to the conventional Taylor expansion, within the expansion orders considered in this work. Furthermore, we find that the consistent regions of the three different expansions are in agreement with the convergence radius of the Lee-Yang edge singularities.","sentences":["Convergence of three different expansion schemes at finite baryon chemical potentials, including the conventional Taylor expansion, the Pad\\'e approximants, and the $T'$ expansion proposed recently in lattice QCD simulations, have been investigated in a low energy effective theory within the fRG approach.","It is found that the $T'$ expansion or the Pad\\'e approximants would hardly improve the convergence of expansion in comparison to the conventional Taylor expansion, within the expansion orders considered in this work.","Furthermore, we find that the consistent regions of the three different expansions are in agreement with the convergence radius of the Lee-Yang edge singularities."],"url":"http://arxiv.org/abs/2403.06770v1","category":"hep-ph"}
{"created":"2024-03-11 14:37:57","title":"Continuity and equivariant dimension","abstract":"We study the local-triviality dimensions of actions on $C^*$-algebras, which are invariants developed for noncommutative Borsuk-Ulam theory. While finiteness of the local-triviality dimensions is known to guarantee freeness of an action, we show that free actions need not have finite weak local-triviality dimension. Moreover, the local-triviality dimensions of a continuous field may be greater than those of its individual fibers, and the dimensions may fail to vary continuously across the fibers. However, in certain circumstances upper semicontinuity of the weak local-triviality dimension is guaranteed. We examine these results and counterexamples with a focus on noncommutative tori and noncommutative spheres, both in terms of computation and theory.","sentences":["We study the local-triviality dimensions of actions on $C^*$-algebras, which are invariants developed for noncommutative Borsuk-Ulam theory.","While finiteness of the local-triviality dimensions is known to guarantee freeness of an action, we show that free actions need not have finite weak local-triviality dimension.","Moreover, the local-triviality dimensions of a continuous field may be greater than those of its individual fibers, and the dimensions may fail to vary continuously across the fibers.","However, in certain circumstances upper semicontinuity of the weak local-triviality dimension is guaranteed.","We examine these results and counterexamples with a focus on noncommutative tori and noncommutative spheres, both in terms of computation and theory."],"url":"http://arxiv.org/abs/2403.06767v1","category":"math.OA"}
{"created":"2024-03-11 13:36:03","title":"Identifying plasma fractionation processes in the chromosphere using IRIS","abstract":"The composition of the solar corona differs from that of the photosphere, with the plasma thought to fractionate in the solar chromosphere according to the First Ionisation Potential (FIP) of the different elements. This produces a FIP bias, wherein elements with a low FIP are preferentially enhanced in the corona compared to their photospheric abundance, but direct observations of this process remain elusive. Here we use a series of spectroscopic observations of Active Region AR 12759 as it transited the solar disc over a period of 6 days from 2-7 April 2020 taken using the Hinode Extreme ultraviolet Imaging Spectrometer (EIS) and Interface Region Imaging Spectrograph (IRIS) instruments to look for signatures of plasma fractionation in the solar chromosphere. Using the Si X/S X and Ca XIV/Ar XIV diagnostics, we find distinct differences between the FIP bias of the leading and following polarities of the active region. The widths of the IRIS Si IV lines exhibited clear differences between the leading and following polarity regions, indicating increased unresolved wave activity in the following polarity region compared to the leading polarity region, with the chromospheric velocities derived using the Mg II lines exhibiting comparable, albeit much weaker, behaviour. These results are consistent with plasma fractionation via resonant/non-resonant waves at different locations in the solar chromosphere following the ponderomotive force model, and indicate that IRIS could be used to further study this fundamental physical process.","sentences":["The composition of the solar corona differs from that of the photosphere, with the plasma thought to fractionate in the solar chromosphere according to the First Ionisation Potential (FIP) of the different elements.","This produces a FIP bias, wherein elements with a low FIP are preferentially enhanced in the corona compared to their photospheric abundance, but direct observations of this process remain elusive.","Here we use a series of spectroscopic observations of Active Region AR 12759 as it transited the solar disc over a period of 6 days from 2-7 April 2020 taken using the Hinode Extreme ultraviolet Imaging Spectrometer (EIS) and Interface Region Imaging Spectrograph (IRIS) instruments to look for signatures of plasma fractionation in the solar chromosphere.","Using the Si X/S X and Ca XIV/Ar XIV diagnostics, we find distinct differences between the FIP bias of the leading and following polarities of the active region.","The widths of the IRIS Si IV lines exhibited clear differences between the leading and following polarity regions, indicating increased unresolved wave activity in the following polarity region compared to the leading polarity region, with the chromospheric velocities derived using the Mg II lines exhibiting comparable, albeit much weaker, behaviour.","These results are consistent with plasma fractionation via resonant/non-resonant waves at different locations in the solar chromosphere following the ponderomotive force model, and indicate that IRIS could be used to further study this fundamental physical process."],"url":"http://arxiv.org/abs/2403.06711v1","category":"astro-ph.SR"}
{"created":"2024-03-11 13:33:09","title":"Deriving Dependently-Typed OOP from First Principles - Extended Version with Additional Appendices","abstract":"The expression problem describes how most types can easily be extended with new ways to produce the type or new ways to consume the type, but not both. When abstract syntax trees are defined as an algebraic data type, for example, they can easily be extended with new consumers, such as print or eval, but adding a new constructor requires the modification of all existing pattern matches. The expression problem is one way to elucidate the difference between functional or data-oriented programs (easily extendable by new consumers) and object-oriented programs (easily extendable by new producers). This difference between programs which are extensible by new producers or new consumers also exists for dependently typed programming, but with one core difference: Dependently-typed programming almost exclusively follows the functional programming model and not the object-oriented model, which leaves an interesting space in the programming language landscape unexplored. In this paper, we explore the field of dependently-typed object-oriented programming by deriving it from first principles using the principle of duality. That is, we do not extend an existing object-oriented formalism with dependent types in an ad-hoc fashion, but instead start from a familiar data-oriented language and derive its dual fragment by the systematic use of defunctionalization and refunctionalization. Our central contribution is a dependently typed calculus which contains two dual language fragments. We provide type- and semantics-preserving transformations between these two language fragments: defunctionalization and refunctionalization. We have implemented this language and these transformations and use this implementation to explain the various ways in which constructions in dependently typed programming can be explained as special instances of the phenomenon of duality.","sentences":["The expression problem describes how most types can easily be extended with new ways to produce the type or new ways to consume the type, but not both.","When abstract syntax trees are defined as an algebraic data type, for example, they can easily be extended with new consumers, such as print or eval, but adding a new constructor requires the modification of all existing pattern matches.","The expression problem is one way to elucidate the difference between functional or data-oriented programs (easily extendable by new consumers) and object-oriented programs (easily extendable by new producers).","This difference between programs which are extensible by new producers or new consumers also exists for dependently typed programming, but with one core difference: Dependently-typed programming almost exclusively follows the functional programming model and not the object-oriented model, which leaves an interesting space in the programming language landscape unexplored.","In this paper, we explore the field of dependently-typed object-oriented programming by deriving it from first principles using the principle of duality.","That is, we do not extend an existing object-oriented formalism with dependent types in an ad-hoc fashion, but instead start from a familiar data-oriented language and derive its dual fragment by the systematic use of defunctionalization and refunctionalization.","Our central contribution is a dependently typed calculus which contains two dual language fragments.","We provide type- and semantics-preserving transformations between these two language fragments: defunctionalization and refunctionalization.","We have implemented this language and these transformations and use this implementation to explain the various ways in which constructions in dependently typed programming can be explained as special instances of the phenomenon of duality."],"url":"http://arxiv.org/abs/2403.06707v1","category":"cs.PL"}
{"created":"2024-03-11 13:05:22","title":"Approximating Maximum Edge 2-Coloring by Normalizing Graphs","abstract":"In a simple, undirected graph G, an edge 2-coloring is a coloring of the edges such that no vertex is incident to edges with more than 2 distinct colors. The problem maximum edge 2-coloring (ME2C) is to find an edge 2-coloring in a graph G with the goal to maximize the number of colors. For a relevant graph class, ME2C models anti-Ramsey numbers and it was considered in network applications. For the problem a 2-approximation algorithm is known, and if the input graph has a perfect matching, the same algorithm has been shown to have a performance guarantee of 5/3. It is known that ME2C is APX-hard and that it is UG-hard to obtain an approximation ratio better than 1.5. We show that if the input graph has a perfect matching, there is a polynomial time 1.625-approximation and if the graph is claw-free or if the maximum degree of the input graph is at most three (i.e., the graph is subcubic), there is a polynomial time 1.5-approximation algorithm for ME2C","sentences":["In a simple, undirected graph G, an edge 2-coloring is a coloring of the edges such that no vertex is incident to edges with more than 2 distinct colors.","The problem maximum edge 2-coloring (ME2C) is to find an edge 2-coloring in a graph G with the goal to maximize the number of colors.","For a relevant graph class, ME2C models anti-Ramsey numbers and it was considered in network applications.","For the problem a 2-approximation algorithm is known, and if the input graph has a perfect matching, the same algorithm has been shown to have a performance guarantee of 5/3.","It is known that ME2C is APX-hard and that it is UG-hard to obtain an approximation ratio better than 1.5.","We show that if the input graph has a perfect matching, there is a polynomial time 1.625-approximation and if the graph is claw-free or if the maximum degree of the input graph is at most three (i.e., the graph is subcubic), there is a polynomial time 1.5-approximation algorithm for ME2C"],"url":"http://arxiv.org/abs/2403.06691v1","category":"cs.DM"}
{"created":"2024-03-11 12:50:02","title":"Observing neutrinos from failed Supernovae at LNGS","abstract":"We discuss the possibility to observe neutrinos emitted from a failed core collapse Supernova in the various experiments at Laboratori Nazionali del Gran Sasso. We show that the veto regions of dark matter and neutrinoless double beta decay experiments can be used as a network of small detectors to measure Supernova neutrinos. In addition we show that this network can measure very precisely the moment of black hole formation, which can be then used in the nearby VIRGO detector and future Einstein Telescope to look for the gravitational wave counterpart to the neutrino signal.","sentences":["We discuss the possibility to observe neutrinos emitted from a failed core collapse Supernova in the various experiments at Laboratori Nazionali del Gran Sasso.","We show that the veto regions of dark matter and neutrinoless double beta decay experiments can be used as a network of small detectors to measure Supernova neutrinos.","In addition we show that this network can measure very precisely the moment of black hole formation, which can be then used in the nearby VIRGO detector and future Einstein Telescope to look for the gravitational wave counterpart to the neutrino signal."],"url":"http://arxiv.org/abs/2403.06678v1","category":"astro-ph.HE"}
{"created":"2024-03-11 12:31:49","title":"Anick resolution for the free unitary quantum group","abstract":"A resolution $P$ of the counit of the Hopf $\\ast$-algebra $\\mathcal{O}(U_n^+)$ of representative functions on van Daele and Wang's free unitary quantum group $U_n^+$ in terms of free $\\mathcal{O}(U_n^+)$-modules is computed for arbitrary $n$. A different such resolution was recently found by Baraquin, Franz, Gerhold, Kula and Tobolski. While theirs has desirable properties which $P$ lacks, $P$ is still good enough to compute the (previously known) quantum group cohomology and comes instead with an important advantage: $P$ can be arrived at without the clever combination of certain results potentially very particular to $U_n^+$ that enabled the aforementioned authors to find their resolution. Especially, $P$ relies neither on the resolution for $O_n^+$ obtained by Collins, H\\\"artel and Thom nor the one for $SL_2(q)$ found by Hadfield and Kr\\\"ahmer. Rather, as shown in the present article, the recursion defining the Anick resolution of the counit of $\\mathcal{O}(U_n^+)$ can be solved in closed form. That suggests a potential strategy for determining the cohomologies of arbitrary easy quantum groups.","sentences":["A resolution $P$ of the counit of the Hopf $\\ast$-algebra $\\mathcal{O}(U_n^+)$ of representative functions on van Daele and Wang's free unitary quantum group $U_n^+$ in terms of free $\\mathcal{O}(U_n^+)$-modules is computed for arbitrary $n$. A different such resolution was recently found by Baraquin, Franz, Gerhold, Kula and Tobolski.","While theirs has desirable properties which $P$ lacks, $P$ is still good enough to compute the (previously known) quantum group cohomology and comes instead with an important advantage: $P$ can be arrived at without the clever combination of certain results potentially very particular to $U_n^+$ that enabled the aforementioned authors to find their resolution.","Especially, $P$ relies neither on the resolution for $O_n^+$ obtained by Collins, H\\\"artel and Thom nor the one for $SL_2(q)$ found by Hadfield and Kr\\\"ahmer.","Rather, as shown in the present article, the recursion defining the Anick resolution of the counit of $\\mathcal{O}(U_n^+)$ can be solved in closed form.","That suggests a potential strategy for determining the cohomologies of arbitrary easy quantum groups."],"url":"http://arxiv.org/abs/2403.06663v1","category":"math.QA"}
{"created":"2024-03-11 11:45:32","title":"Fractal time scale-free agnostic messaging: amplitude modulation of carriers given by the Weierstrass function's components","abstract":"In the context of interstellar communication, it is not known beforehand if the receiver of a given signal would be a plant, an insect, or even life forms unknown to terrestrial scientists. Regardless of the situation, the message time scale could be too fast or too slow and those beings would probably never decode it. Therefore, it is of interest to devise a way to encode messages agnostic of time scale. Fractal messaging would allow one to do this due to their structural self-similarity and, sometimes, scale invariance. By starting from a spatial embedding rationale, a framework is developed for a time scale-free messaging alternative. When one considers a time-agnostic framework for message transmission, it would be interesting to encode a message such that it could be decoded along several spatio-temporal scales. This way, the core idea of the framework hereby proposed is to encode a binary message as waves along infinitely many (power-like distributed) frequencies and amplitudes, transmit such message, and then decode and reproduce it. To do so, the components of the Weierstrass function, a known fractal, are used as the carriers of the message. Each component will have its amplitude modulated to embed the binary stream, allowing for a time-agnostic approach to messaging.","sentences":["In the context of interstellar communication, it is not known beforehand if the receiver of a given signal would be a plant, an insect, or even life forms unknown to terrestrial scientists.","Regardless of the situation, the message time scale could be too fast or too slow and those beings would probably never decode it.","Therefore, it is of interest to devise a way to encode messages agnostic of time scale.","Fractal messaging would allow one to do this due to their structural self-similarity and, sometimes, scale invariance.","By starting from a spatial embedding rationale, a framework is developed for a time scale-free messaging alternative.","When one considers a time-agnostic framework for message transmission, it would be interesting to encode a message such that it could be decoded along several spatio-temporal scales.","This way, the core idea of the framework hereby proposed is to encode a binary message as waves along infinitely many (power-like distributed) frequencies and amplitudes, transmit such message, and then decode and reproduce it.","To do so, the components of the Weierstrass function, a known fractal, are used as the carriers of the message.","Each component will have its amplitude modulated to embed the binary stream, allowing for a time-agnostic approach to messaging."],"url":"http://arxiv.org/abs/2403.06633v1","category":"cs.IT"}
{"created":"2024-03-11 11:39:49","title":"Hot Carrier Nanowire Transistors at the Ballistic Limit","abstract":"We demonstrate experimentally non-equilibrium transport in unipolar quasi-1D hot electron devices reaching ballistic limit. The devices are realized with heterostructure engineering in nanowires to obtain dopant- and dislocation-free 1D-epitaxy and flexible bandgap engineering. We show experimentally the control of hot electron injection with a graded conduction band profile and subsequent filtering of hot and relaxed electrons with rectangular energy barriers. The number of electron passing the barrier depends exponentially on the transport length with a mean free path of 200 - 260 nm and reaches ballistic transport regime for the shortest devices with 70 % of the electrons flying freely through the base electrode and the barrier reflections limiting the transport to the collector.","sentences":["We demonstrate experimentally non-equilibrium transport in unipolar quasi-1D hot electron devices reaching ballistic limit.","The devices are realized with heterostructure engineering in nanowires to obtain dopant- and dislocation-free 1D-epitaxy and flexible bandgap engineering.","We show experimentally the control of hot electron injection with a graded conduction band profile and subsequent filtering of hot and relaxed electrons with rectangular energy barriers.","The number of electron passing the barrier depends exponentially on the transport length with a mean free path of 200 - 260 nm and reaches ballistic transport regime for the shortest devices with 70 % of the electrons flying freely through the base electrode and the barrier reflections limiting the transport to the collector."],"url":"http://arxiv.org/abs/2403.06630v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-11 11:38:32","title":"Search for a new charged particle in the mass range of 2-100 MeV","abstract":"In the theory of electroweak interactions, there is no prohibition on the existence of particles with a mass other than that of an electron, muon and tauon. The task is to search for a new particle in the mass range of 2-100 MeV. The search was performed using the photo material of the 2-m propane bubble chamber. The chamber has been exposed in a 10 GeV proton beam at the JINR Synchrophasotron. For its search, ~55 thousand stereo photos have been scanned. The events of the $\\gamma$ quantum conversion into a pair of charged particles have been analyzed. 50 anomalous events have been found in which one particle of the pair stops in the volume of the chamber, has an increased track density at the end of the range and, upon identification, detects a mass of ~ 8 MeV. The average value of the mass of the new particle is (8.5 $\\pm$ 2.5) MeV","sentences":["In the theory of electroweak interactions, there is no prohibition on the existence of particles with a mass other than that of an electron, muon and tauon.","The task is to search for a new particle in the mass range of 2-100 MeV. The search was performed using the photo material of the 2-m propane bubble chamber.","The chamber has been exposed in a 10 GeV proton beam at the JINR Synchrophasotron.","For its search, ~55 thousand stereo photos have been scanned.","The events of the $\\gamma$ quantum conversion into a pair of charged particles have been analyzed.","50 anomalous events have been found in which one particle of the pair stops in the volume of the chamber, has an increased track density at the end of the range and, upon identification, detects a mass of ~ 8 MeV.","The average value of the mass of the new particle is (8.5 $\\pm$ 2.5) MeV"],"url":"http://arxiv.org/abs/2403.06628v1","category":"nucl-ex"}
{"created":"2024-03-11 10:26:55","title":"Limiting absorption principle for long-range perturbation in the discrete triangular lattice setting","abstract":"We study the discrete Laplacian acting on a triangular lattice. We perturb the metric and the potential in a long-range way. We aim at proving a Limiting Absorption Principle away the possible embedded eigenvalues. The approach is based on a positive commutator technique.","sentences":["We study the discrete Laplacian acting on a triangular lattice.","We perturb the metric and the potential in a long-range way.","We aim at proving a Limiting Absorption Principle away the possible embedded eigenvalues.","The approach is based on a positive commutator technique."],"url":"http://arxiv.org/abs/2403.06578v1","category":"math.FA"}
{"created":"2024-03-11 10:04:37","title":"Distinguishing Dirac/Majorana Heavy Neutrino at Future Lepton Colliders","abstract":"We propose to identify whether a sterile neutrino is Dirac-type or Majorana-type by counting the peak of the rapidity distribution at lepton colliders. Our method requires only one charged-lepton tagging, and the nature of sterile neutrinos can be pinned down once they are confirmed.","sentences":["We propose to identify whether a sterile neutrino is Dirac-type or Majorana-type by counting the peak of the rapidity distribution at lepton colliders.","Our method requires only one charged-lepton tagging, and the nature of sterile neutrinos can be pinned down once they are confirmed."],"url":"http://arxiv.org/abs/2403.06561v1","category":"hep-ph"}
{"created":"2024-03-11 09:05:42","title":"Comparison between InAs-based and GaSb-based Interband cascade lasers with hybrid superlattice plasmon-enhanced claddings","abstract":"We compare InAs-based and GaSb-based interband cascade lasers (ICLs) with the same 12 stages active region designed to emit at a wavelength of 4.6 {\\mu}m. They employ a hybrid cladding architecture with the same geometry and inner claddings consisting of InAs/AlSb superlattices but different outer claddings: The InAs-based ICL employs plasmon enhanced n-type doped InAs layers while the GaSb-based ICL employs plasmon-enhanced n-type doped InAs_0.915 Sb_0.085 claddings lattice matched to GaSb. Due to the lower refractive index of n+-InAsSb (n=2.88) compared to n+-InAs (n=3.10), the GaSb-based ICL shows a 3.8 % higher optical mode confinement in the active region compared to the InAs-based ICL. Experimentally, the GaSb-based ICL shows a 17.3 % lower threshold current density in pulsed operation at room temperature. Also presented is the influence of geometry and doping variation on confinement factors and calculated free carrier absorption losses in the GaSb-based ICL.","sentences":["We compare InAs-based and GaSb-based interband cascade lasers (ICLs) with the same 12 stages active region designed to emit at a wavelength of 4.6 {\\mu}m.","They employ a hybrid cladding architecture with the same geometry and inner claddings consisting of InAs/AlSb superlattices but different outer claddings: The InAs-based ICL employs plasmon enhanced n-type doped InAs layers while the GaSb-based ICL employs plasmon-enhanced n-type doped InAs_0.915 Sb_0.085 claddings lattice matched to GaSb.","Due to the lower refractive index of n+-InAsSb (n=2.88) compared to n+-InAs (n=3.10), the GaSb-based ICL shows a 3.8 % higher optical mode confinement in the active region compared to the InAs-based ICL.","Experimentally, the GaSb-based ICL shows a 17.3 % lower threshold current density in pulsed operation at room temperature.","Also presented is the influence of geometry and doping variation on confinement factors and calculated free carrier absorption losses in the GaSb-based ICL."],"url":"http://arxiv.org/abs/2403.06525v1","category":"physics.optics"}
{"created":"2024-03-11 05:35:38","title":"Latent Semantic Consensus For Deterministic Geometric Model Fitting","abstract":"Estimating reliable geometric model parameters from the data with severe outliers is a fundamental and important task in computer vision. This paper attempts to sample high-quality subsets and select model instances to estimate parameters in the multi-structural data. To address this, we propose an effective method called Latent Semantic Consensus (LSC). The principle of LSC is to preserve the latent semantic consensus in both data points and model hypotheses. Specifically, LSC formulates the model fitting problem into two latent semantic spaces based on data points and model hypotheses, respectively. Then, LSC explores the distributions of points in the two latent semantic spaces, to remove outliers, generate high-quality model hypotheses, and effectively estimate model instances. Finally, LSC is able to provide consistent and reliable solutions within only a few milliseconds for general multi-structural model fitting, due to its deterministic fitting nature and efficiency. Compared with several state-of-the-art model fitting methods, our LSC achieves significant superiority for the performance of both accuracy and speed on synthetic data and real images. The code will be available at https://github.com/guobaoxiao/LSC.","sentences":["Estimating reliable geometric model parameters from the data with severe outliers is a fundamental and important task in computer vision.","This paper attempts to sample high-quality subsets and select model instances to estimate parameters in the multi-structural data.","To address this, we propose an effective method called Latent Semantic Consensus (LSC).","The principle of LSC is to preserve the latent semantic consensus in both data points and model hypotheses.","Specifically, LSC formulates the model fitting problem into two latent semantic spaces based on data points and model hypotheses, respectively.","Then, LSC explores the distributions of points in the two latent semantic spaces, to remove outliers, generate high-quality model hypotheses, and effectively estimate model instances.","Finally, LSC is able to provide consistent and reliable solutions within only a few milliseconds for general multi-structural model fitting, due to its deterministic fitting nature and efficiency.","Compared with several state-of-the-art model fitting methods, our LSC achieves significant superiority for the performance of both accuracy and speed on synthetic data and real images.","The code will be available at https://github.com/guobaoxiao/LSC."],"url":"http://arxiv.org/abs/2403.06444v1","category":"cs.CV"}
{"created":"2024-03-11 05:16:56","title":"The role of the short-distance interaction in $e^+e^-\\to \u03b3X(3872)$","abstract":"In this study, we analyze the cross-section data from the $e^+e^-\\to \\gamma J/\\psi\\omega$ process to explore both short-distance and long-distance interactions for the radiative transition $Y(4200)\\to \\gamma X(3872)$. We investigate the short-distance effects through the E1 transition among the $c\\bar{c}$ components, and the long-distance effects via hadronic loop diagrams. Our numerical analysis reveals that short-distance interactions play a significantly larger role in the radiative transition than that of the long-distance interactions. This finding underscores the importance of the compact $c\\bar{c}$ components in both the initial and final states for accurately understanding the cross-section $\\sigma[e^+e^-\\to \\gamma J/\\psi\\omega]$. Furthermore, with the help of relative branch ratio $\\mathcal{R}$ we estimate the $\\Gamma[Y(4200)\\to\\gamma X(3872)]$ implied in the the experimental study. Finally, we also discuss the possible existence of $\\psi(4040)$ signal within the cross-section data.","sentences":["In this study, we analyze the cross-section data from the $e^+e^-\\to \\gamma J/\\psi\\omega$ process to explore both short-distance and long-distance interactions for the radiative transition $Y(4200)\\to","\\gamma X(3872)$. We investigate the short-distance effects through the E1 transition among the $c\\bar{c}$ components, and the long-distance effects via hadronic loop diagrams.","Our numerical analysis reveals that short-distance interactions play a significantly larger role in the radiative transition than that of the long-distance interactions.","This finding underscores the importance of the compact $c\\bar{c}$ components in both the initial and final states for accurately understanding the cross-section $\\sigma[e^+e^-\\to \\gamma J/\\psi\\omega]$. Furthermore, with the help of relative branch ratio $\\mathcal{R}$ we estimate the $\\Gamma[Y(4200)\\to\\gamma X(3872)]$ implied in the the experimental study.","Finally, we also discuss the possible existence of $\\psi(4040)$ signal within the cross-section data."],"url":"http://arxiv.org/abs/2403.06440v1","category":"hep-ph"}
{"created":"2024-03-11 05:13:00","title":"Distribution of merging and post-merging galaxies in nearby galaxy clusters","abstract":"We study the incidence and spatial distribution of galaxies that are currently undergoing gravitational merging (M) or that have signs of a post merger (PM) in six galaxy clusters (A754, A2399, A2670, A3558, A3562, and A3716) within the redshift range, 0.05$\\lesssim$$z$$\\lesssim$0.08. To this aim, we obtained Dark Energy Camera (DECam) mosaics in $u^{\\prime}$, $g^{\\prime}$, and $r^{\\prime}$-bands covering up to $3\\times R_{200}$ of the clusters, reaching 28 mag/arcsec$^2$ surface brightness limits. We visually inspect $u^{\\prime}$$g^{\\prime}$$r^{\\prime}$ color-composite images of volume-limited ($M_r < -20$) cluster-member galaxies to identify whether galaxies are of M or PM types. We find 4% M-type and 7% PM-type galaxies in the galaxy clusters studied. By adding spectroscopic data and studying the projected phase space diagram (PPSD) of the projected clustocentric radius and the line-of-sight velocity, we find that PM-type galaxies are more virialized than M-type galaxies, having 1--5% point higher fraction within the escape-velocity region, while the fraction of M-type was $\\sim$10% point higher than PM-type in the intermediate environment. Similarly, on a substructure analysis, M types were found in the outskirt groups, while PM types populated groups in ubiquitous regions of the PPSD. Adopting literature-derived dynamical state indicator values, we observed a higher abundance of M types in dynamically relaxed clusters. This finding suggests that galaxies displaying post-merging features within clusters likely merged in low-velocity environments, including cluster outskirts and dynamically relaxed clusters.","sentences":["We study the incidence and spatial distribution of galaxies that are currently undergoing gravitational merging (M) or that have signs of a post merger (PM) in six galaxy clusters (A754, A2399, A2670, A3558, A3562, and A3716) within the redshift range, 0.05$\\lesssim$$z$$\\lesssim$0.08.","To this aim, we obtained Dark Energy Camera (DECam) mosaics in $u^{\\prime}$, $g^{\\prime}$, and $r^{\\prime}$-bands covering up to $3\\times R_{200}$ of the clusters, reaching 28 mag/arcsec$^2$ surface brightness limits.","We visually inspect $u^{\\prime}$$g^{\\prime}$$r^{\\prime}$ color-composite images of volume-limited ($M_r < -20$) cluster-member galaxies to identify whether galaxies are of M or PM types.","We find 4% M-type and 7% PM-type galaxies in the galaxy clusters studied.","By adding spectroscopic data and studying the projected phase space diagram (PPSD) of the projected clustocentric radius and the line-of-sight velocity, we find that PM-type galaxies are more virialized than M-type galaxies, having 1--5% point higher fraction within the escape-velocity region, while the fraction of M-type was $\\sim$10% point higher than PM-type in the intermediate environment.","Similarly, on a substructure analysis, M types were found in the outskirt groups, while PM types populated groups in ubiquitous regions of the PPSD.","Adopting literature-derived dynamical state indicator values, we observed a higher abundance of M types in dynamically relaxed clusters.","This finding suggests that galaxies displaying post-merging features within clusters likely merged in low-velocity environments, including cluster outskirts and dynamically relaxed clusters."],"url":"http://arxiv.org/abs/2403.06437v1","category":"astro-ph.GA"}
{"created":"2024-03-11 04:05:13","title":"Evidence of a Past Merger of the Galactic Center Black Hole","abstract":"The origin of supermassive black holes (SMBHs) residing in the centers of most galaxies remains a mystery. Various growth models, such as accretion and hierarchical mergers, have been proposed to explain the existence and cosmological evolution of these SMBHs, but no direct observational evidence is available to test these models. The Event Horizon Telescope (EHT) offered direct imaging of nearby SMBHs, in particular, the one at the center of the Milky Way Galaxy named Sgr~A*. Measurements suggest that the Sgr~A* BH spins rapidly with significant spin axis misalignment relative to the angular momentum of the Galactic plane. Through investigating various SMBH growth models, here we show that the spin properties of Sgr~A* provides strong evidence of a past SMBH merger. Inspired by the merger between the Milky Way and Gaia-Enceladus, which has a 4:1 mass ratio as inferred from Gaia data, we have discovered that a 4:1 major merger of SMBH with a binary angular momentum inclination angle of 15-45 degrees with respect to the line of sight (LOS), can successfully replicate the measured spin properties of Sgr A*. This merger event in our galaxy provides observational support for the theory of hierarchical BH mergers in the formation and growth of SMBHs. The inferred merger rate, consistent with theoretical predictions, suggests a promising detection rate of SMBH mergers for space-borne gravitational wave detectors expected to operate in 2030s.","sentences":["The origin of supermassive black holes (SMBHs) residing in the centers of most galaxies remains a mystery.","Various growth models, such as accretion and hierarchical mergers, have been proposed to explain the existence and cosmological evolution of these SMBHs, but no direct observational evidence is available to test these models.","The Event Horizon Telescope (EHT) offered direct imaging of nearby SMBHs, in particular, the one at the center of the Milky Way Galaxy named Sgr~A*.","Measurements suggest that the Sgr~A* BH spins rapidly with significant spin axis misalignment relative to the angular momentum of the Galactic plane.","Through investigating various SMBH growth models, here we show that the spin properties of Sgr~A* provides strong evidence of a past SMBH merger.","Inspired by the merger between the Milky Way and Gaia-Enceladus, which has a 4:1 mass ratio as inferred from Gaia data, we have discovered that a 4:1 major merger of SMBH with a binary angular momentum inclination angle of 15-45 degrees with respect to the line of sight (LOS), can successfully replicate the measured spin properties of Sgr A*.","This merger event in our galaxy provides observational support for the theory of hierarchical BH mergers in the formation and growth of SMBHs.","The inferred merger rate, consistent with theoretical predictions, suggests a promising detection rate of SMBH mergers for space-borne gravitational wave detectors expected to operate in 2030s."],"url":"http://arxiv.org/abs/2403.06416v1","category":"astro-ph.GA"}
{"created":"2024-03-11 03:43:42","title":"Observational Properties of AGN Obscuration During the Peak of Accretion Growth","abstract":"We investigated the gas obscuration and host galaxy properties of active galactic nuclei (AGN) during the peak of cosmic accretion growth of supermassive black holes (SMBHs) at redshift 0.8-1.8 using X-ray detected AGN with mid-infrared and far-infrared detection. The sample was classified as type-1 and type-2 AGN using optical spectral and morphological classification while the host galaxy properties were estimated with multiwavelength SED fitting. For type-1 AGN, the black hole mass was determined from MgII emission lines while the black hole mass of type-2 AGN was inferred from the host galaxy's stellar mass. Based on the derived parameters, the distribution of the sample in the absorption hydrogen column density ($N_{\\rm H}$) vs. Eddington ratio diagram is examined. Among the type-2 AGN, $28\\pm5$\\% are in the forbidden zone, where the obscuration by dust torus cannot be maintained due to radiation pressure on dusty material. The fraction is higher than that observed in the local universe from the BAT AGN Spectroscopic Survey (BASS) data release 2 ($11\\pm3$\\%). The higher fraction implies that the obscuration of the majority of AGN is consistent with the radiation pressure regulated unified model but with an increased incidence of interstellar matter (ISM) obscured AGN. We discuss the possibility of dust-free absorption in type-1 AGN and heavy ISM absorption in type-2 AGN. We also find no statistical difference in the star-formation activity between type-1 and type-2 AGN which may suggest that obscuration triggered by a gas-rich merging is not common among X-ray detected AGN in this epoch.","sentences":["We investigated the gas obscuration and host galaxy properties of active galactic nuclei (AGN) during the peak of cosmic accretion growth of supermassive black holes (SMBHs) at redshift 0.8-1.8 using X-ray detected AGN with mid-infrared and far-infrared detection.","The sample was classified as type-1 and type-2 AGN using optical spectral and morphological classification while the host galaxy properties were estimated with multiwavelength SED fitting.","For type-1 AGN, the black hole mass was determined from MgII emission lines while the black hole mass of type-2 AGN was inferred from the host galaxy's stellar mass.","Based on the derived parameters, the distribution of the sample in the absorption hydrogen column density ($N_{\\rm H}$) vs. Eddington ratio diagram is examined.","Among the type-2 AGN, $28\\pm5$\\% are in the forbidden zone, where the obscuration by dust torus cannot be maintained due to radiation pressure on dusty material.","The fraction is higher than that observed in the local universe from the BAT AGN Spectroscopic Survey (BASS) data release 2 ($11\\pm3$\\%).","The higher fraction implies that the obscuration of the majority of AGN is consistent with the radiation pressure regulated unified model but with an increased incidence of interstellar matter (ISM) obscured AGN.","We discuss the possibility of dust-free absorption in type-1 AGN and heavy ISM absorption in type-2 AGN.","We also find no statistical difference in the star-formation activity between type-1 and type-2 AGN which may suggest that obscuration triggered by a gas-rich merging is not common among X-ray detected AGN in this epoch."],"url":"http://arxiv.org/abs/2403.06409v1","category":"astro-ph.GA"}
{"created":"2024-03-11 03:34:46","title":"Chemical Potentials and the One-Electron Hamiltonian of the Second-Order Perturbation Theory from the Functional Derivative Approach","abstract":"We develop a functional derivative approach to calculate the chemical potentials of the second-order perturbation theory (MP2). In the functional derivative approach, the correlation part of the MP2 chemical potential, which is the derivative of the MP2 correlation energy with respect to the occupation number of frontier orbitals, is obtained from the chain rule via the non-interacting Green's function. First, the MP2 correlation energy is expressed in terms of the non-interacting Green's function and its functional derivative to the non-interacting Green's function is the second-order self-energy. Then the derivative of the non-interacting Green's function to the occupation number is obtained by including the orbital relaxation effect. We show that the MP2 chemical potentials obtained from the functional derivative approach agrees with that obtained from the finite difference approach. The one-electron Hamiltonian, defined as the derivative of the MP2 energy with respect to the one particle density matrix, is also derived using the functional derivative approach, which can be used in the self-consistent calculations of MP2 and double-hybrid density functionals. The developed functional derivative approach is promising for calculating the chemical potentials and the one-electron Hamiltonian of approximate functionals and many-body perturbation approaches dependent explicitly on the non-interacting Green's function.","sentences":["We develop a functional derivative approach to calculate the chemical potentials of the second-order perturbation theory (MP2).","In the functional derivative approach, the correlation part of the MP2 chemical potential, which is the derivative of the MP2 correlation energy with respect to the occupation number of frontier orbitals, is obtained from the chain rule via the non-interacting Green's function.","First, the MP2 correlation energy is expressed in terms of the non-interacting Green's function and its functional derivative to the non-interacting Green's function is the second-order self-energy.","Then the derivative of the non-interacting Green's function to the occupation number is obtained by including the orbital relaxation effect.","We show that the MP2 chemical potentials obtained from the functional derivative approach agrees with that obtained from the finite difference approach.","The one-electron Hamiltonian, defined as the derivative of the MP2 energy with respect to the one particle density matrix, is also derived using the functional derivative approach, which can be used in the self-consistent calculations of MP2 and double-hybrid density functionals.","The developed functional derivative approach is promising for calculating the chemical potentials and the one-electron Hamiltonian of approximate functionals and many-body perturbation approaches dependent explicitly on the non-interacting Green's function."],"url":"http://arxiv.org/abs/2403.06405v1","category":"physics.chem-ph"}
{"created":"2024-03-11 03:24:58","title":"Refining Segmentation On-the-Fly: An Interactive Framework for Point Cloud Semantic Segmentation","abstract":"Existing interactive point cloud segmentation approaches primarily focus on the object segmentation, which aim to determine which points belong to the object of interest guided by user interactions. This paper concentrates on an unexplored yet meaningful task, i.e., interactive point cloud semantic segmentation, which assigns high-quality semantic labels to all points in a scene with user corrective clicks. Concretely, we presents the first interactive framework for point cloud semantic segmentation, named InterPCSeg, which seamlessly integrates with off-the-shelf semantic segmentation networks without offline re-training, enabling it to run in an on-the-fly manner. To achieve online refinement, we treat user interactions as sparse training examples during the test-time. To address the instability caused by the sparse supervision, we design a stabilization energy to regulate the test-time training process. For objective and reproducible evaluation, we develop an interaction simulation scheme tailored for the interactive point cloud semantic segmentation task. We evaluate our framework on the S3DIS and ScanNet datasets with off-the-shelf segmentation networks, incorporating interactions from both the proposed interaction simulator and real users. Quantitative and qualitative experimental results demonstrate the efficacy of our framework in refining the semantic segmentation results with user interactions. The source code will be publicly available.","sentences":["Existing interactive point cloud segmentation approaches primarily focus on the object segmentation, which aim to determine which points belong to the object of interest guided by user interactions.","This paper concentrates on an unexplored yet meaningful task, i.e., interactive point cloud semantic segmentation, which assigns high-quality semantic labels to all points in a scene with user corrective clicks.","Concretely, we presents the first interactive framework for point cloud semantic segmentation, named InterPCSeg, which seamlessly integrates with off-the-shelf semantic segmentation networks without offline re-training, enabling it to run in an on-the-fly manner.","To achieve online refinement, we treat user interactions as sparse training examples during the test-time.","To address the instability caused by the sparse supervision, we design a stabilization energy to regulate the test-time training process.","For objective and reproducible evaluation, we develop an interaction simulation scheme tailored for the interactive point cloud semantic segmentation task.","We evaluate our framework on the S3DIS and ScanNet datasets with off-the-shelf segmentation networks, incorporating interactions from both the proposed interaction simulator and real users.","Quantitative and qualitative experimental results demonstrate the efficacy of our framework in refining the semantic segmentation results with user interactions.","The source code will be publicly available."],"url":"http://arxiv.org/abs/2403.06401v1","category":"cs.CV"}
{"created":"2024-03-11 02:59:23","title":"A Functionally Connected Element Method for Solving Boundary Value Problems","abstract":"We present the general forms of piece-wise functions on partitioned domains satisfying an intrinsic $C^0$ or $C^1$ continuity across the sub-domain boundaries. These general forms are constructed based on a strategy stemming from the theory of functional connections, and we refer to partitioned domains endowed with these general forms as functionally connected elements (FCE). We further present a method, incorporating functionally connected elements and a least squares collocation approach, for solving boundary and initial value problems. This method exhibits a spectral-like accuracy, with the free functions involved in the FCE form represented by polynomial bases or by non-polynomial bases of quasi-random sinusoidal functions. The FCE method offers a unique advantage over traditional element-based methods for boundary value problems involving relative boundary conditions. A number of linear and nonlinear numerical examples in one and two dimensions are presented to demonstrate the performance of the FCE method developed herein.","sentences":["We present the general forms of piece-wise functions on partitioned domains satisfying an intrinsic $C^0$ or $C^1$ continuity across the sub-domain boundaries.","These general forms are constructed based on a strategy stemming from the theory of functional connections, and we refer to partitioned domains endowed with these general forms as functionally connected elements (FCE).","We further present a method, incorporating functionally connected elements and a least squares collocation approach, for solving boundary and initial value problems.","This method exhibits a spectral-like accuracy, with the free functions involved in the FCE form represented by polynomial bases or by non-polynomial bases of quasi-random sinusoidal functions.","The FCE method offers a unique advantage over traditional element-based methods for boundary value problems involving relative boundary conditions.","A number of linear and nonlinear numerical examples in one and two dimensions are presented to demonstrate the performance of the FCE method developed herein."],"url":"http://arxiv.org/abs/2403.06393v1","category":"math.NA"}
{"created":"2024-03-11 02:03:19","title":"Adiabatic versus instantaneous transitions from a harmonic oscillator to an inverted oscillator","abstract":"We have obtained explicit analytical formulas for the mean energy and its variance (characterizing the energy fluctuations) of a quantum harmonic oscillator with time-dependent frequency in the adiabatic regimes after the frequency passes through zero. The behavior of energy turns out to be quite different in two cases: when the frequency remains real and when it becomes imaginary. In the first case, the mean energy always increases when the frequency returns to its initial value, and the increment coefficient is determined by the exponent in the power law of the frequency crossing zero. On the other hand, if the frequency becomes imaginary, the mean energy becomes equal to zero exactly, for all initial Fock states and for any value of the power law exponent.","sentences":["We have obtained explicit analytical formulas for the mean energy and its variance (characterizing the energy fluctuations) of a quantum harmonic oscillator with time-dependent frequency in the adiabatic regimes after the frequency passes through zero.","The behavior of energy turns out to be quite different in two cases: when the frequency remains real and when it becomes imaginary.","In the first case, the mean energy always increases when the frequency returns to its initial value, and the increment coefficient is determined by the exponent in the power law of the frequency crossing zero.","On the other hand, if the frequency becomes imaginary, the mean energy becomes equal to zero exactly, for all initial Fock states and for any value of the power law exponent."],"url":"http://arxiv.org/abs/2403.06377v1","category":"quant-ph"}
{"created":"2024-03-11 01:46:51","title":"Insights into Galaxy Morphology and Star Formation: Unveiling Filamentary Structures around an Extreme Overdensity at $z \\sim 1.5$ Traced by [OII] Emitters","abstract":"We explore the morphological features and star formation activities of [OII] emitters in the COSMOS UltraDeep field at $z \\sim 1.5$ using JWST NIRCam data from the COSMOS-Web survey and Subaru Hyper Suprime-Cam. We also report the discovery of large filamentary structures traced by [OII] emitters, surrounding an extremely overdense core with a galaxy number density $\\sim11\\times$ higher than the field average. These structures span over 50 cMpc, underscoring their large scale in the cosmic web at this epoch. After matching the stellar mass distributions, the core galaxies show a higher frequency of disturbances (50$\\%$ $ \\pm$ 9$\\%$) than those in outskirts (41$\\%$ $ \\pm$ 9$\\%$) and the field (21$\\%$ $ \\pm$ 5$\\%$), indicative of more frequent mergers and interactions in the innermost $\\lesssim1.5 $ arcmin region. Additionally, we observe that specific star formation rates are elevated in denser environments. A Kolmogorov-Smirnov (KS) test comparing the distribution of specific star formation rates of core and field galaxies yields a $\\textit{p}$-value of 0.02, suggesting an enhancement of star-formation activity driven by the dense environment. Our findings underscore the environmental impact on galaxy evolution during a pivotal cosmic epoch and set the stage for further investigation with the increasing larger data from upcoming surveys.","sentences":["We explore the morphological features and star formation activities of [OII] emitters in the COSMOS UltraDeep field at $z \\sim 1.5$ using JWST NIRCam data from the COSMOS-Web survey and Subaru Hyper Suprime-Cam.","We also report the discovery of large filamentary structures traced by [OII] emitters, surrounding an extremely overdense core with a galaxy number density $\\sim11\\times$ higher than the field average.","These structures span over 50 cMpc, underscoring their large scale in the cosmic web at this epoch.","After matching the stellar mass distributions, the core galaxies show a higher frequency of disturbances (50$\\%$ $ \\pm$ 9$\\%$) than those in outskirts (41$\\%$ $ \\pm$ 9$\\%$) and the field (21$\\%$ $ \\pm$ 5$\\%$), indicative of more frequent mergers and interactions in the innermost $\\lesssim1.5 $ arcmin region.","Additionally, we observe that specific star formation rates are elevated in denser environments.","A Kolmogorov-Smirnov (KS) test comparing the distribution of specific star formation rates of core and field galaxies yields a $\\textit{p}$-value of 0.02, suggesting an enhancement of star-formation activity driven by the dense environment.","Our findings underscore the environmental impact on galaxy evolution during a pivotal cosmic epoch and set the stage for further investigation with the increasing larger data from upcoming surveys."],"url":"http://arxiv.org/abs/2403.06369v1","category":"astro-ph.GA"}
{"created":"2024-03-11 01:32:29","title":"Style2Talker: High-Resolution Talking Head Generation with Emotion Style and Art Style","abstract":"Although automatically animating audio-driven talking heads has recently received growing interest, previous efforts have mainly concentrated on achieving lip synchronization with the audio, neglecting two crucial elements for generating expressive videos: emotion style and art style. In this paper, we present an innovative audio-driven talking face generation method called Style2Talker. It involves two stylized stages, namely Style-E and Style-A, which integrate text-controlled emotion style and picture-controlled art style into the final output. In order to prepare the scarce emotional text descriptions corresponding to the videos, we propose a labor-free paradigm that employs large-scale pretrained models to automatically annotate emotional text labels for existing audiovisual datasets. Incorporating the synthetic emotion texts, the Style-E stage utilizes a large-scale CLIP model to extract emotion representations, which are combined with the audio, serving as the condition for an efficient latent diffusion model designed to produce emotional motion coefficients of a 3DMM model. Moving on to the Style-A stage, we develop a coefficient-driven motion generator and an art-specific style path embedded in the well-known StyleGAN. This allows us to synthesize high-resolution artistically stylized talking head videos using the generated emotional motion coefficients and an art style source picture. Moreover, to better preserve image details and avoid artifacts, we provide StyleGAN with the multi-scale content features extracted from the identity image and refine its intermediate feature maps by the designed content encoder and refinement network, respectively. Extensive experimental results demonstrate our method outperforms existing state-of-the-art methods in terms of audio-lip synchronization and performance of both emotion style and art style.","sentences":["Although automatically animating audio-driven talking heads has recently received growing interest, previous efforts have mainly concentrated on achieving lip synchronization with the audio, neglecting two crucial elements for generating expressive videos: emotion style and art style.","In this paper, we present an innovative audio-driven talking face generation method called Style2Talker.","It involves two stylized stages, namely Style-E and Style-A, which integrate text-controlled emotion style and picture-controlled art style into the final output.","In order to prepare the scarce emotional text descriptions corresponding to the videos, we propose a labor-free paradigm that employs large-scale pretrained models to automatically annotate emotional text labels for existing audiovisual datasets.","Incorporating the synthetic emotion texts, the Style-E stage utilizes a large-scale CLIP model to extract emotion representations, which are combined with the audio, serving as the condition for an efficient latent diffusion model designed to produce emotional motion coefficients of a 3DMM model.","Moving on to the Style-A stage, we develop a coefficient-driven motion generator and an art-specific style path embedded in the well-known StyleGAN.","This allows us to synthesize high-resolution artistically stylized talking head videos using the generated emotional motion coefficients and an art style source picture.","Moreover, to better preserve image details and avoid artifacts, we provide StyleGAN with the multi-scale content features extracted from the identity image and refine its intermediate feature maps by the designed content encoder and refinement network, respectively.","Extensive experimental results demonstrate our method outperforms existing state-of-the-art methods in terms of audio-lip synchronization and performance of both emotion style and art style."],"url":"http://arxiv.org/abs/2403.06365v1","category":"cs.CV"}
{"created":"2024-03-11 01:12:36","title":"Compact Objects in close orbits as Gravitational Wave Sources: Formation Scenarios and Properties","abstract":"Gravitational Waves (GWs) provide a unique way to explore our Universe. The ongoing ground-based detectors, e.g., LIGO, Virgo, and KAGRA, and the upcoming next-generation detectors, e.g., Cosmic Explorer and Einstein Telescope, as well as the future space-borne GW antennas, e.g., LISA, TianQin, and TaiJi, cover a wide range of GW frequencies {from $\\sim 10^{-4}\\;\\rm Hz$ to $\\sim 10^3\\;\\rm Hz$} and almost all types of compact objects in close orbits serve as the potential target sources for these GW detectors. The synergistic multi-band GW and EM observations would allow us to study fundamental physics from stars to cosmology. {The formation of stellar GW sources has been extensively explored in recent years, and progress on physical processes in binary interaction has been made as well. Furthermore, some studies have shown that the progress in binary evolution may significantly affect the properties of the stellar GW sources.}","sentences":["Gravitational Waves (GWs) provide a unique way to explore our Universe.","The ongoing ground-based detectors, e.g., LIGO, Virgo, and KAGRA, and the upcoming next-generation detectors, e.g., Cosmic Explorer and Einstein Telescope, as well as the future space-borne GW antennas, e.g., LISA, TianQin, and TaiJi, cover a wide range of GW frequencies {from $\\sim 10^{-4}\\;\\rm Hz$ to $\\sim 10^3\\;\\rm Hz$} and almost all types of compact objects in close orbits serve as the potential target sources for these GW detectors.","The synergistic multi-band GW and EM observations would allow us to study fundamental physics from stars to cosmology.","{The formation of stellar GW sources has been extensively explored in recent years, and progress on physical processes in binary interaction has been made as well.","Furthermore, some studies have shown that the progress in binary evolution may significantly affect the properties of the stellar GW sources.}"],"url":"http://arxiv.org/abs/2403.06358v1","category":"astro-ph.SR"}
{"created":"2024-03-11 00:08:25","title":"Active Control of Bound States in the Continuum in Toroidal Metasurfaces","abstract":"The remarkable properties of toroidal metasurfaces, featuring ultrahigh-Q bound states in the continuum (BIC) resonances and nonradiating anapole modes, have garnered significant attention. The active manipulation of toroidal resonance characteristics offers substantial potential for advancing tunable metasurfaces. Our study specifically explores the application of vanadium dioxide, a widely used phase change material in active photonics and room-temperature bolometric detectors, to control BIC resonances in toroidal metasurfaces. The phase change transition of vanadium dioxide occurs in a narrow temperature range providing a large variation in material resistivity. Through heating thin film patches of vanadium dioxide integrated into a metasurface comprising gold split-ring resonators on a sapphire substrate, we achieve remarkable control over the amplitude and frequency of toroidal dipole BIC resonances due to their high sensitivity to losses present in the system. Breaking the symmetry of meta-atoms reveals enhanced tunability. The predicted maximum change in the amplitude of toroidal dipole BIC resonances reaches 14 dB with a temperature variation of approximately 10oC. The proposed tunable metasurface holds promise for various applications, including active photonic systems and room temperature bolometers.","sentences":["The remarkable properties of toroidal metasurfaces, featuring ultrahigh-Q bound states in the continuum (BIC) resonances and nonradiating anapole modes, have garnered significant attention.","The active manipulation of toroidal resonance characteristics offers substantial potential for advancing tunable metasurfaces.","Our study specifically explores the application of vanadium dioxide, a widely used phase change material in active photonics and room-temperature bolometric detectors, to control BIC resonances in toroidal metasurfaces.","The phase change transition of vanadium dioxide occurs in a narrow temperature range providing a large variation in material resistivity.","Through heating thin film patches of vanadium dioxide integrated into a metasurface comprising gold split-ring resonators on a sapphire substrate, we achieve remarkable control over the amplitude and frequency of toroidal dipole BIC resonances due to their high sensitivity to losses present in the system.","Breaking the symmetry of meta-atoms reveals enhanced tunability.","The predicted maximum change in the amplitude of toroidal dipole BIC resonances reaches 14 dB with a temperature variation of approximately 10oC.","The proposed tunable metasurface holds promise for various applications, including active photonic systems and room temperature bolometers."],"url":"http://arxiv.org/abs/2403.06345v1","category":"physics.optics"}
