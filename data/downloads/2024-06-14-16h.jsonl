{"created":"2024-06-12 17:59:52","title":"ICE-G: Image Conditional Editing of 3D Gaussian Splats","abstract":"Recently many techniques have emerged to create high quality 3D assets and scenes. When it comes to editing of these objects, however, existing approaches are either slow, compromise on quality, or do not provide enough customization. We introduce a novel approach to quickly edit a 3D model from a single reference view. Our technique first segments the edit image, and then matches semantically corresponding regions across chosen segmented dataset views using DINO features. A color or texture change from a particular region of the edit image can then be applied to other views automatically in a semantically sensible manner. These edited views act as an updated dataset to further train and re-style the 3D scene. The end-result is therefore an edited 3D model. Our framework enables a wide variety of editing tasks such as manual local edits, correspondence based style transfer from any example image, and a combination of different styles from multiple example images. We use Gaussian Splats as our primary 3D representation due to their speed and ease of local editing, but our technique works for other methods such as NeRFs as well. We show through multiple examples that our method produces higher quality results while offering fine-grained control of editing. Project page: ice-gaussian.github.io","sentences":["Recently many techniques have emerged to create high quality 3D assets and scenes.","When it comes to editing of these objects, however, existing approaches are either slow, compromise on quality, or do not provide enough customization.","We introduce a novel approach to quickly edit a 3D model from a single reference view.","Our technique first segments the edit image, and then matches semantically corresponding regions across chosen segmented dataset views using DINO features.","A color or texture change from a particular region of the edit image can then be applied to other views automatically in a semantically sensible manner.","These edited views act as an updated dataset to further train and re-style the 3D scene.","The end-result is therefore an edited 3D model.","Our framework enables a wide variety of editing tasks such as manual local edits, correspondence based style transfer from any example image, and a combination of different styles from multiple example images.","We use Gaussian Splats as our primary 3D representation due to their speed and ease of local editing, but our technique works for other methods such as NeRFs as well.","We show through multiple examples that our method produces higher quality results while offering fine-grained control of editing.","Project page: ice-gaussian.github.io"],"url":"http://arxiv.org/abs/2406.08488v1","category":"cs.CV"}
{"created":"2024-06-12 17:59:08","title":"Real3D: Scaling Up Large Reconstruction Models with Real-World Images","abstract":"The default strategy for training single-view Large Reconstruction Models (LRMs) follows the fully supervised route using large-scale datasets of synthetic 3D assets or multi-view captures. Although these resources simplify the training procedure, they are hard to scale up beyond the existing datasets and they are not necessarily representative of the real distribution of object shapes. To address these limitations, in this paper, we introduce Real3D, the first LRM system that can be trained using single-view real-world images. Real3D introduces a novel self-training framework that can benefit from both the existing synthetic data and diverse single-view real images. We propose two unsupervised losses that allow us to supervise LRMs at the pixel- and semantic-level, even for training examples without ground-truth 3D or novel views. To further improve performance and scale up the image data, we develop an automatic data curation approach to collect high-quality examples from in-the-wild images. Our experiments show that Real3D consistently outperforms prior work in four diverse evaluation settings that include real and synthetic data, as well as both in-domain and out-of-domain shapes. Code and model can be found here: https://hwjiang1510.github.io/Real3D/","sentences":["The default strategy for training single-view Large Reconstruction Models (LRMs) follows the fully supervised route using large-scale datasets of synthetic 3D assets or multi-view captures.","Although these resources simplify the training procedure, they are hard to scale up beyond the existing datasets and they are not necessarily representative of the real distribution of object shapes.","To address these limitations, in this paper, we introduce Real3D, the first LRM system that can be trained using single-view real-world images.","Real3D introduces a novel self-training framework that can benefit from both the existing synthetic data and diverse single-view real images.","We propose two unsupervised losses that allow us to supervise LRMs at the pixel- and semantic-level, even for training examples without ground-truth 3D or novel views.","To further improve performance and scale up the image data, we develop an automatic data curation approach to collect high-quality examples from in-the-wild images.","Our experiments show that Real3D consistently outperforms prior work in four diverse evaluation settings that include real and synthetic data, as well as both in-domain and out-of-domain shapes.","Code and model can be found here: https://hwjiang1510.github.io/Real3D/"],"url":"http://arxiv.org/abs/2406.08479v1","category":"cs.CV"}
{"created":"2024-06-12 17:59:04","title":"RMem: Restricted Memory Banks Improve Video Object Segmentation","abstract":"With recent video object segmentation (VOS) benchmarks evolving to challenging scenarios, we revisit a simple but overlooked strategy: restricting the size of memory banks. This diverges from the prevalent practice of expanding memory banks to accommodate extensive historical information. Our specially designed \"memory deciphering\" study offers a pivotal insight underpinning such a strategy: expanding memory banks, while seemingly beneficial, actually increases the difficulty for VOS modules to decode relevant features due to the confusion from redundant information. By restricting memory banks to a limited number of essential frames, we achieve a notable improvement in VOS accuracy. This process balances the importance and freshness of frames to maintain an informative memory bank within a bounded capacity. Additionally, restricted memory banks reduce the training-inference discrepancy in memory lengths compared with continuous expansion. This fosters new opportunities in temporal reasoning and enables us to introduce the previously overlooked \"temporal positional embedding.\" Finally, our insights are embodied in \"RMem\" (\"R\" for restricted), a simple yet effective VOS modification that excels at challenging VOS scenarios and establishes new state of the art for object state changes (on the VOST dataset) and long videos (on the Long Videos dataset). Our code and demo are available at https://restricted-memory.github.io/.","sentences":["With recent video object segmentation (VOS) benchmarks evolving to challenging scenarios, we revisit a simple but overlooked strategy: restricting the size of memory banks.","This diverges from the prevalent practice of expanding memory banks to accommodate extensive historical information.","Our specially designed \"memory deciphering\" study offers a pivotal insight underpinning such a strategy: expanding memory banks, while seemingly beneficial, actually increases the difficulty for VOS modules to decode relevant features due to the confusion from redundant information.","By restricting memory banks to a limited number of essential frames, we achieve a notable improvement in VOS accuracy.","This process balances the importance and freshness of frames to maintain an informative memory bank within a bounded capacity.","Additionally, restricted memory banks reduce the training-inference discrepancy in memory lengths compared with continuous expansion.","This fosters new opportunities in temporal reasoning and enables us to introduce the previously overlooked \"temporal positional embedding.\"","Finally, our insights are embodied in \"RMem\" (\"R\" for restricted), a simple yet effective VOS modification that excels at challenging VOS scenarios and establishes new state of the art for object state changes (on the VOST dataset) and long videos (on the Long Videos dataset).","Our code and demo are available at https://restricted-memory.github.io/."],"url":"http://arxiv.org/abs/2406.08476v1","category":"cs.CV"}
{"created":"2024-06-12 17:57:06","title":"Real2Code: Reconstruct Articulated Objects via Code Generation","abstract":"We present Real2Code, a novel approach to reconstructing articulated objects via code generation. Given visual observations of an object, we first reconstruct its part geometry using an image segmentation model and a shape completion model. We then represent the object parts with oriented bounding boxes, which are input to a fine-tuned large language model (LLM) to predict joint articulation as code. By leveraging pre-trained vision and language models, our approach scales elegantly with the number of articulated parts, and generalizes from synthetic training data to real world objects in unstructured environments. Experimental results demonstrate that Real2Code significantly outperforms previous state-of-the-art in reconstruction accuracy, and is the first approach to extrapolate beyond objects' structural complexity in the training set, and reconstructs objects with up to 10 articulated parts. When incorporated with a stereo reconstruction model, Real2Code also generalizes to real world objects from a handful of multi-view RGB images, without the need for depth or camera information.","sentences":["We present Real2Code, a novel approach to reconstructing articulated objects via code generation.","Given visual observations of an object, we first reconstruct its part geometry using an image segmentation model and a shape completion model.","We then represent the object parts with oriented bounding boxes, which are input to a fine-tuned large language model (LLM) to predict joint articulation as code.","By leveraging pre-trained vision and language models, our approach scales elegantly with the number of articulated parts, and generalizes from synthetic training data to real world objects in unstructured environments.","Experimental results demonstrate that Real2Code significantly outperforms previous state-of-the-art in reconstruction accuracy, and is the first approach to extrapolate beyond objects' structural complexity in the training set, and reconstructs objects with up to 10 articulated parts.","When incorporated with a stereo reconstruction model, Real2Code also generalizes to real world objects from a handful of multi-view RGB images, without the need for depth or camera information."],"url":"http://arxiv.org/abs/2406.08474v2","category":"cs.CV"}
{"created":"2024-06-12 17:56:31","title":"RILe: Reinforced Imitation Learning","abstract":"Reinforcement Learning has achieved significant success in generating complex behavior but often requires extensive reward function engineering. Adversarial variants of Imitation Learning and Inverse Reinforcement Learning offer an alternative by learning policies from expert demonstrations via a discriminator. Employing discriminators increases their data- and computational efficiency over the standard approaches; however, results in sensitivity to imperfections in expert data. We propose RILe, a teacher-student system that achieves both robustness to imperfect data and efficiency. In RILe, the student learns an action policy while the teacher dynamically adjusts a reward function based on the student's performance and its alignment with expert demonstrations. By tailoring the reward function to both performance of the student and expert similarity, our system reduces dependence on the discriminator and, hence, increases robustness against data imperfections. Experiments show that RILe outperforms existing methods by 2x in settings with limited or noisy expert data.","sentences":["Reinforcement Learning has achieved significant success in generating complex behavior but often requires extensive reward function engineering.","Adversarial variants of Imitation Learning and Inverse Reinforcement Learning offer an alternative by learning policies from expert demonstrations via a discriminator.","Employing discriminators increases their data- and computational efficiency over the standard approaches; however, results in sensitivity to imperfections in expert data.","We propose RILe, a teacher-student system that achieves both robustness to imperfect data and efficiency.","In RILe, the student learns an action policy while the teacher dynamically adjusts a reward function based on the student's performance and its alignment with expert demonstrations.","By tailoring the reward function to both performance of the student and expert similarity, our system reduces dependence on the discriminator and, hence, increases robustness against data imperfections.","Experiments show that RILe outperforms existing methods by 2x in settings with limited or noisy expert data."],"url":"http://arxiv.org/abs/2406.08472v1","category":"cs.LG"}
{"created":"2024-06-12 17:56:15","title":"Surprise! Using Physiological Stress for Allostatic Regulation Under the Active Inference Framework [Pre-Print]","abstract":"Allostasis proposes that long-term viability of a living system is achieved through anticipatory adjustments of its physiology and behaviour: emphasising physiological and affective stress as an adaptive state of adaptation that minimizes long-term prediction errors. More recently, the active inference framework (AIF) has also sought to explain action and long-term adaptation through the minimization of future errors (free energy), through the learning of statistical contingencies of the world, offering a formalism for allostatic regulation. We suggest that framing prediction errors through the lens of biological hormonal dynamics proposed by allostasis offers a way to integrate these two models together in a biologically-plausible manner. In this paper, we describe our initial work in developing a model that grounds prediction errors (surprisal) into the secretion of a physiological stress hormone (cortisol) acting as an adaptive, allostatic mediator on a homeostatically-controlled physiology. We evaluate this using a computational model in simulations using an active inference agent endowed with an artificial physiology, regulated through homeostatic and allostatic control in a stochastic environment. Our results find that allostatic functions of cortisol (stress), secreted as a function of prediction errors, provide adaptive advantages to the agent's long-term physiological regulation. We argue that the coupling of information-theoretic prediction errors to low-level, biological hormonal dynamics of stress can provide a computationally efficient model to long-term regulation for embodied intelligent systems.","sentences":["Allostasis proposes that long-term viability of a living system is achieved through anticipatory adjustments of its physiology and behaviour: emphasising physiological and affective stress as an adaptive state of adaptation that minimizes long-term prediction errors.","More recently, the active inference framework (AIF) has also sought to explain action and long-term adaptation through the minimization of future errors (free energy), through the learning of statistical contingencies of the world, offering a formalism for allostatic regulation.","We suggest that framing prediction errors through the lens of biological hormonal dynamics proposed by allostasis offers a way to integrate these two models together in a biologically-plausible manner.","In this paper, we describe our initial work in developing a model that grounds prediction errors (surprisal) into the secretion of a physiological stress hormone (cortisol) acting as an adaptive, allostatic mediator on a homeostatically-controlled physiology.","We evaluate this using a computational model in simulations using an active inference agent endowed with an artificial physiology, regulated through homeostatic and allostatic control in a stochastic environment.","Our results find that allostatic functions of cortisol (stress), secreted as a function of prediction errors, provide adaptive advantages to the agent's long-term physiological regulation.","We argue that the coupling of information-theoretic prediction errors to low-level, biological hormonal dynamics of stress can provide a computationally efficient model to long-term regulation for embodied intelligent systems."],"url":"http://arxiv.org/abs/2406.08471v1","category":"cs.AI"}
{"created":"2024-06-12 17:54:54","title":"PAL: Pluralistic Alignment Framework for Learning from Heterogeneous Preferences","abstract":"Large foundation models pretrained on raw web-scale data are not readily deployable without additional step of extensive alignment to human preferences. Such alignment is typically done by collecting large amounts of pairwise comparisons from humans (\"Do you prefer output A or B?\") and learning a reward model or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a human's underlying implicit preferences. These methods generally suffer from assuming a universal preference shared by all humans, which lacks the flexibility of adapting to plurality of opinions and preferences. In this work, we propose PAL, a framework to model human preference complementary to existing pretraining strategies, which incorporates plurality from the ground up. We propose using the ideal point model as a lens to view alignment using preference comparisons. Together with our novel reformulation and using mixture modeling, our framework captures the plurality of population preferences while simultaneously learning a common preference latent space across different preferences, which can few-shot generalize to new, unseen users. Our approach enables us to use the penultimate-layer representation of large foundation models and simple MLP layers to learn reward functions that are on-par with the existing large state-of-the-art reward models, thereby enhancing efficiency of reward modeling significantly. We show that PAL achieves competitive reward model accuracy compared to strong baselines on 1) Language models with Summary dataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new semisynthetic heterogeneous dataset generated using Anthropic Personas. Finally, our experiments also highlight the shortcoming of current preference datasets that are created using rigid rubrics which wash away heterogeneity, and call for more nuanced data collection approaches.","sentences":["Large foundation models pretrained on raw web-scale data are not readily deployable without additional step of extensive alignment to human preferences.","Such alignment is typically done by collecting large amounts of pairwise comparisons from humans (\"Do you prefer output A or B?\") and learning a reward model or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a human's underlying implicit preferences.","These methods generally suffer from assuming a universal preference shared by all humans, which lacks the flexibility of adapting to plurality of opinions and preferences.","In this work, we propose PAL, a framework to model human preference complementary to existing pretraining strategies, which incorporates plurality from the ground up.","We propose using the ideal point model as a lens to view alignment using preference comparisons.","Together with our novel reformulation and using mixture modeling, our framework captures the plurality of population preferences while simultaneously learning a common preference latent space across different preferences, which can few-shot generalize to new, unseen users.","Our approach enables us to use the penultimate-layer representation of large foundation models and simple MLP layers to learn reward functions that are on-par with the existing large state-of-the-art reward models, thereby enhancing efficiency of reward modeling significantly.","We show that PAL achieves competitive reward model accuracy compared to strong baselines on 1) Language models with Summary dataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new semisynthetic heterogeneous dataset generated using Anthropic Personas.","Finally, our experiments also highlight the shortcoming of current preference datasets that are created using rigid rubrics which wash away heterogeneity, and call for more nuanced data collection approaches."],"url":"http://arxiv.org/abs/2406.08469v1","category":"cs.LG"}
{"created":"2024-06-12 17:53:31","title":"DafnyBench: A Benchmark for Formal Software Verification","abstract":"We introduce DafnyBench, the largest benchmark of its kind for training and evaluating machine learning systems for formal software verification. We test the ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough hints for the Dafny formal verification engine to successfully verify over 750 programs with about 53,000 lines of code. The best model and prompting scheme achieved 68% success rate, and we quantify how this rate improves when retrying with error message feedback and how it deteriorates with the amount of required code and hints. We hope that DafnyBench will enable rapid improvements from this baseline as LLMs and verification techniques grow in quality.","sentences":["We introduce DafnyBench, the largest benchmark of its kind for training and evaluating machine learning systems for formal software verification.","We test the ability of LLMs such as GPT-4 and Claude 3 to auto-generate enough hints for the Dafny formal verification engine to successfully verify over 750 programs with about 53,000 lines of code.","The best model and prompting scheme achieved 68% success rate, and we quantify how this rate improves when retrying with error message feedback and how it deteriorates with the amount of required code and hints.","We hope that DafnyBench will enable rapid improvements from this baseline as LLMs and verification techniques grow in quality."],"url":"http://arxiv.org/abs/2406.08467v1","category":"cs.SE"}
{"created":"2024-06-12 17:53:29","title":"Scaling Laws in Linear Regression: Compute, Parameters, and Data","abstract":"Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.   We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$, we show that the reducible part of the test error is $\\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation.","sentences":["Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow.","However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size.","This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.   ","We study the theory of scaling laws in an infinite dimensional linear regression setup.","Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates.","The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data.","Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$, we show that the reducible part of the test error is $\\Theta(M^{-(a-1)}","+ N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound.","Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation."],"url":"http://arxiv.org/abs/2406.08466v1","category":"cs.LG"}
{"created":"2024-06-12 17:52:30","title":"Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing","abstract":"High-quality instruction data is critical for aligning large language models (LLMs). Although some models, such as Llama-3-Instruct, have open weights, their alignment data remain private, which hinders the democratization of AI. High human labor costs and a limited, predefined scope for prompting prevent existing open-source data creation methods from scaling effectively, potentially limiting the diversity and quality of public alignment datasets. Is it possible to synthesize high-quality instruction data at scale by extracting it directly from an aligned LLM? We present a self-synthesis method for generating large-scale alignment data named Magpie. Our key observation is that aligned LLMs like Llama-3-Instruct can generate a user query when we input only the left-side templates up to the position reserved for user messages, thanks to their auto-regressive nature. We use this method to prompt Llama-3-Instruct and generate 4 million instructions along with their corresponding responses. We perform a comprehensive analysis of the extracted data and select 300K high-quality instances. To compare Magpie data with other public instruction datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the performance of the fine-tuned models. Our results indicate that in some tasks, models fine-tuned with Magpie perform comparably to the official Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data points through supervised fine-tuning (SFT) and subsequent feedback learning. We also show that using Magpie solely for SFT can surpass the performance of previous public datasets utilized for both SFT and preference optimization, such as direct preference optimization with UltraFeedback. This advantage is evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.","sentences":["High-quality instruction data is critical for aligning large language models (LLMs).","Although some models, such as Llama-3-Instruct, have open weights, their alignment data remain private, which hinders the democratization of AI.","High human labor costs and a limited, predefined scope for prompting prevent existing open-source data creation methods from scaling effectively, potentially limiting the diversity and quality of public alignment datasets.","Is it possible to synthesize high-quality instruction data at scale by extracting it directly from an aligned LLM?","We present a self-synthesis method for generating large-scale alignment data named Magpie.","Our key observation is that aligned LLMs like Llama-3-Instruct can generate a user query when we input only the left-side templates up to the position reserved for user messages, thanks to their auto-regressive nature.","We use this method to prompt Llama-3-Instruct and generate 4 million instructions along with their corresponding responses.","We perform a comprehensive analysis of the extracted data and select 300K high-quality instances.","To compare Magpie data with other public instruction datasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the performance of the fine-tuned models.","Our results indicate that in some tasks, models fine-tuned with Magpie perform comparably to the official Llama-3-8B-Instruct, despite the latter being enhanced with 10 million data points through supervised fine-tuning (SFT) and subsequent feedback learning.","We also show that using Magpie solely for SFT can surpass the performance of previous public datasets utilized for both SFT and preference optimization, such as direct preference optimization with UltraFeedback.","This advantage is evident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench."],"url":"http://arxiv.org/abs/2406.08464v1","category":"cs.CL"}
{"created":"2024-06-12 17:38:20","title":"The Impact of Initialization on LoRA Finetuning Dynamics","abstract":"In this paper, we study the role of initialization in Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from the pretrained model as initialization for finetuning, one can either initialize B to zero and A to random (default initialization in PEFT package), or vice-versa. In both cases, the product BA is equal to zero at initialization, which makes finetuning starts from the pretrained model. These two initialization schemes are seemingly similar. They should in-principle yield the same performance and share the same optimal learning rate. We demonstrate that this is an incorrect intuition and that the first scheme (initializing B to zero and A to random) on average yields better performance compared to the other scheme. Our theoretical analysis shows that the reason behind this might be that the first initialization allows the use of larger learning rates (without causing output instability) compared to the second initialization, resulting in more efficient learning of the first scheme. We validate our results with extensive experiments on LLMs.","sentences":["In this paper, we study the role of initialization in Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021).","Essentially, to start from the pretrained model as initialization for finetuning, one can either initialize B to zero and A to random (default initialization in PEFT package), or vice-versa.","In both cases, the product BA is equal to zero at initialization, which makes finetuning starts from the pretrained model.","These two initialization schemes are seemingly similar.","They should in-principle yield the same performance and share the same optimal learning rate.","We demonstrate that this is an incorrect intuition and that the first scheme (initializing B to zero and A to random) on average yields better performance compared to the other scheme.","Our theoretical analysis shows that the reason behind this might be that the first initialization allows the use of larger learning rates (without causing output instability) compared to the second initialization, resulting in more efficient learning of the first scheme.","We validate our results with extensive experiments on LLMs."],"url":"http://arxiv.org/abs/2406.08447v1","category":"cs.LG"}
{"created":"2024-06-12 17:37:09","title":"OLMES: A Standard for Language Model Evaluations","abstract":"Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities. Evaluating language models in particular is challenging, as small changes to how a model is evaluated on a task can lead to large changes in measured performance. There is no common standard setup, so different models are evaluated on the same tasks in different ways, leading to claims about which models perform best not being reproducible. We propose OLMES, a completely documented, practical, open standard for reproducible LLM evaluations. In developing this standard, we identify and review the varying factors in evaluation practices adopted by the community - such as details of prompt formatting, choice of in-context examples, probability normalizations, and task formulation. In particular, OLMES supports meaningful comparisons between smaller base models that require the unnatural \"cloze\" formulation of multiple-choice questions against larger models that can utilize the original formulation. OLMES includes well-considered recommendations guided by results from existing literature as well as new experiments investigating open questions.","sentences":["Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities.","Evaluating language models in particular is challenging, as small changes to how a model is evaluated on a task can lead to large changes in measured performance.","There is no common standard setup, so different models are evaluated on the same tasks in different ways, leading to claims about which models perform best not being reproducible.","We propose OLMES, a completely documented, practical, open standard for reproducible LLM evaluations.","In developing this standard, we identify and review the varying factors in evaluation practices adopted by the community - such as details of prompt formatting, choice of in-context examples, probability normalizations, and task formulation.","In particular, OLMES supports meaningful comparisons between smaller base models that require the unnatural \"cloze\" formulation of multiple-choice questions against larger models that can utilize the original formulation.","OLMES includes well-considered recommendations guided by results from existing literature as well as new experiments investigating open questions."],"url":"http://arxiv.org/abs/2406.08446v1","category":"cs.CL"}
{"created":"2024-06-12 17:28:32","title":"$\\texttt{DiffLense}$: A Conditional Diffusion Model for Super-Resolution of Gravitational Lensing Data","abstract":"Gravitational lensing data is frequently collected at low resolution due to instrumental limitations and observing conditions. Machine learning-based super-resolution techniques offer a method to enhance the resolution of these images, enabling more precise measurements of lensing effects and a better understanding of the matter distribution in the lensing system. This enhancement can significantly improve our knowledge of the distribution of mass within the lensing galaxy and its environment, as well as the properties of the background source being lensed. Traditional super-resolution techniques typically learn a mapping function from lower-resolution to higher-resolution samples. However, these methods are often constrained by their dependence on optimizing a fixed distance function, which can result in the loss of intricate details crucial for astrophysical analysis. In this work, we introduce $\\texttt{DiffLense}$, a novel super-resolution pipeline based on a conditional diffusion model specifically designed to enhance the resolution of gravitational lensing images obtained from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP). Our approach adopts a generative model, leveraging the detailed structural information present in Hubble Space Telescope (HST) counterparts. The diffusion model, trained to generate HST data, is conditioned on HSC data pre-processed with denoising techniques and thresholding to significantly reduce noise and background interference. This process leads to a more distinct and less overlapping conditional distribution during the model's training phase. We demonstrate that $\\texttt{DiffLense}$ outperforms existing state-of-the-art single-image super-resolution techniques, particularly in retaining the fine details necessary for astrophysical analyses.","sentences":["Gravitational lensing data is frequently collected at low resolution due to instrumental limitations and observing conditions.","Machine learning-based super-resolution techniques offer a method to enhance the resolution of these images, enabling more precise measurements of lensing effects and a better understanding of the matter distribution in the lensing system.","This enhancement can significantly improve our knowledge of the distribution of mass within the lensing galaxy and its environment, as well as the properties of the background source being lensed.","Traditional super-resolution techniques typically learn a mapping function from lower-resolution to higher-resolution samples.","However, these methods are often constrained by their dependence on optimizing a fixed distance function, which can result in the loss of intricate details crucial for astrophysical analysis.","In this work, we introduce $\\texttt{DiffLense}$, a novel super-resolution pipeline based on a conditional diffusion model specifically designed to enhance the resolution of gravitational lensing images obtained from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP).","Our approach adopts a generative model, leveraging the detailed structural information present in Hubble Space Telescope (HST) counterparts.","The diffusion model, trained to generate HST data, is conditioned on HSC data pre-processed with denoising techniques and thresholding to significantly reduce noise and background interference.","This process leads to a more distinct and less overlapping conditional distribution during the model's training phase.","We demonstrate that $\\texttt{DiffLense}$ outperforms existing state-of-the-art single-image super-resolution techniques, particularly in retaining the fine details necessary for astrophysical analyses."],"url":"http://arxiv.org/abs/2406.08442v1","category":"astro-ph.IM"}
{"created":"2024-06-12 17:21:21","title":"TasTe: Teaching Large Language Models to Translate through Self-Reflection","abstract":"Large language models (LLMs) have exhibited remarkable performance in various natural language processing tasks. Techniques like instruction tuning have effectively enhanced the proficiency of LLMs in the downstream task of machine translation. However, the existing approaches fail to yield satisfactory translation outputs that match the quality of supervised neural machine translation (NMT) systems. One plausible explanation for this discrepancy is that the straightforward prompts employed in these methodologies are unable to fully exploit the acquired instruction-following capabilities. To this end, we propose the TasTe framework, which stands for translating through self-reflection. The self-reflection process includes two stages of inference. In the first stage, LLMs are instructed to generate preliminary translations and conduct self-assessments on these translations simultaneously. In the second stage, LLMs are tasked to refine these preliminary translations according to the evaluation results. The evaluation results in four language directions on the WMT22 benchmark reveal the effectiveness of our approach compared to existing methods. Our work presents a promising approach to unleash the potential of LLMs and enhance their capabilities in MT. The codes and datasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT.","sentences":["Large language models (LLMs) have exhibited remarkable performance in various natural language processing tasks.","Techniques like instruction tuning have effectively enhanced the proficiency of LLMs in the downstream task of machine translation.","However, the existing approaches fail to yield satisfactory translation outputs that match the quality of supervised neural machine translation (NMT) systems.","One plausible explanation for this discrepancy is that the straightforward prompts employed in these methodologies are unable to fully exploit the acquired instruction-following capabilities.","To this end, we propose the TasTe framework, which stands for translating through self-reflection.","The self-reflection process includes two stages of inference.","In the first stage, LLMs are instructed to generate preliminary translations and conduct self-assessments on these translations simultaneously.","In the second stage, LLMs are tasked to refine these preliminary translations according to the evaluation results.","The evaluation results in four language directions on the WMT22 benchmark reveal the effectiveness of our approach compared to existing methods.","Our work presents a promising approach to unleash the potential of LLMs and enhance their capabilities in MT.","The codes and datasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT."],"url":"http://arxiv.org/abs/2406.08434v1","category":"cs.CL"}
{"created":"2024-06-12 17:16:16","title":"Diffusion Soup: Model Merging for Text-to-Image Diffusion Models","abstract":"We present Diffusion Soup, a compartmentalization method for Text-to-Image Generation that averages the weights of diffusion models trained on sharded data. By construction, our approach enables training-free continual learning and unlearning with no additional memory or inference costs, since models corresponding to data shards can be added or removed by re-averaging. We show that Diffusion Soup samples from a point in weight space that approximates the geometric mean of the distributions of constituent datasets, which offers anti-memorization guarantees and enables zero-shot style mixing. Empirically, Diffusion Soup outperforms a paragon model trained on the union of all data shards and achieves a 30% improvement in Image Reward (.34 $\\to$ .44) on domain sharded data, and a 59% improvement in IR (.37 $\\to$ .59) on aesthetic data. In both cases, souping also prevails in TIFA score (respectively, 85.5 $\\to$ 86.5 and 85.6 $\\to$ 86.8). We demonstrate robust unlearning -- removing any individual domain shard only lowers performance by 1% in IR (.45 $\\to$ .44) -- and validate our theoretical insights on anti-memorization using real data. Finally, we showcase Diffusion Soup's ability to blend the distinct styles of models finetuned on different shards, resulting in the zero-shot generation of hybrid styles.","sentences":["We present Diffusion Soup, a compartmentalization method for Text-to-Image Generation that averages the weights of diffusion models trained on sharded data.","By construction, our approach enables training-free continual learning and unlearning with no additional memory or inference costs, since models corresponding to data shards can be added or removed by re-averaging.","We show that Diffusion Soup samples from a point in weight space that approximates the geometric mean of the distributions of constituent datasets, which offers anti-memorization guarantees and enables zero-shot style mixing.","Empirically, Diffusion Soup outperforms a paragon model trained on the union of all data shards and achieves a 30% improvement in Image Reward (.34 $\\to$ .44) on domain sharded data, and a 59% improvement in IR (.37 $\\to$ .59) on aesthetic data.","In both cases, souping also prevails in TIFA score (respectively, 85.5 $\\to$ 86.5 and 85.6 $\\to$ 86.8).","We demonstrate robust unlearning -- removing any individual domain shard only lowers performance by 1% in IR (.45 $\\to$ .44) -- and validate our theoretical insights on anti-memorization using real data.","Finally, we showcase Diffusion Soup's ability to blend the distinct styles of models finetuned on different shards, resulting in the zero-shot generation of hybrid styles."],"url":"http://arxiv.org/abs/2406.08431v1","category":"cs.CV"}
