{"created":"2024-03-25 14:34:06","title":"Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making","abstract":"In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans' appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.","sentences":["In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole.","In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur.","To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making.","Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates.","To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision.","An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans' appropriate reliance and task performance.","Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design."],"url":"http://arxiv.org/abs/2403.16812v1","category":"cs.HC"}
{"created":"2024-03-25 14:32:28","title":"An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems","abstract":"The increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting real-time feedback on human preferences in such human-in-the-loop (HITL) systems, however, is difficult in practice. We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which employs the LLM as a dynamic simulation of the physical environment to learn how to balance between energy savings and occupant comfort. Our results show that LLMs are capable of simulating complex population movements within large open spaces. Besides, AitL-RL demonstrates superior performance compared to the popular existing policy of set point control, suggesting that adaptive and personalized decision-making is critical for efficient optimization in CPS-IoT applications. Through this case study, we demonstrate the potential of integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system adaptability and efficiency. The project's code can be found on our GitHub repository.","sentences":["The increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment.","For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption.","Collecting real-time feedback on human preferences in such human-in-the-loop (HITL) systems, however, is difficult in practice.","We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization.","In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall.","The aggregated thermal preferences are integrated into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which employs the LLM as a dynamic simulation of the physical environment to learn how to balance between energy savings and occupant comfort.","Our results show that LLMs are capable of simulating complex population movements within large open spaces.","Besides, AitL-RL demonstrates superior performance compared to the popular existing policy of set point control, suggesting that adaptive and personalized decision-making is critical for efficient optimization in CPS-IoT applications.","Through this case study, we demonstrate the potential of integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system adaptability and efficiency.","The project's code can be found on our GitHub repository."],"url":"http://arxiv.org/abs/2403.16809v1","category":"eess.SY"}
{"created":"2024-03-25 14:32:18","title":"Navigating the EU AI Act: A Methodological Approach to Compliance for Safety-critical Products","abstract":"In December 2023, the European Parliament provisionally agreed on the EU AI Act. This unprecedented regulatory framework for AI systems lays out guidelines to ensure the safety, legality, and trustworthiness of AI products. This paper presents a methodology for interpreting the EU AI Act requirements for high-risk AI systems by leveraging product quality models. We first propose an extended product quality model for AI systems, incorporating attributes relevant to the Act not covered by current quality models. We map the Act requirements to relevant quality attributes with the goal of refining them into measurable characteristics. We then propose a contract-based approach to derive technical requirements at the stakeholder level. This facilitates the development and assessment of AI systems that not only adhere to established quality standards, but also comply with the regulatory requirements outlined in the Act for high-risk (including safety-critical) AI systems. We demonstrate the applicability of this methodology on an exemplary automotive supply chain use case, where several stakeholders interact to achieve EU AI Act compliance.","sentences":["In December 2023, the European Parliament provisionally agreed on the EU AI Act.","This unprecedented regulatory framework for AI systems lays out guidelines to ensure the safety, legality, and trustworthiness of AI products.","This paper presents a methodology for interpreting the EU AI Act requirements for high-risk AI systems by leveraging product quality models.","We first propose an extended product quality model for AI systems, incorporating attributes relevant to the Act not covered by current quality models.","We map the Act requirements to relevant quality attributes with the goal of refining them into measurable characteristics.","We then propose a contract-based approach to derive technical requirements at the stakeholder level.","This facilitates the development and assessment of AI systems that not only adhere to established quality standards, but also comply with the regulatory requirements outlined in the Act for high-risk (including safety-critical) AI systems.","We demonstrate the applicability of this methodology on an exemplary automotive supply chain use case, where several stakeholders interact to achieve EU AI Act compliance."],"url":"http://arxiv.org/abs/2403.16808v1","category":"cs.AI"}
{"created":"2024-03-25 14:30:06","title":"A serendipitous discovery of HI-rich galaxy groups with MeerKAT","abstract":"We report on the serendipitous discovery of 49 HI-rich galaxies in a 2.3 hour Open Time observation with MeerKAT. We present their properties including their HI masses, intensity and velocity maps, and spectra. We determine that at least three HI-rich galaxy groups have been detected, potentially as part of a supergroup. Some members of these galaxy groups show clear interaction with each other in their HI emission. We cross-match the detections with PanSTARRS, WISE and GALEX, and obtain stellar masses and star formation rates. One source is found to be a potential OH megamaser, but further follow-up is required to confidently determine this. For 6 sources with sufficient spatial resolution in HI we produce rotation curves with BBarolo, generate mass models, and derive a dark matter halo mass. While the number of galaxies detected in this relatively short pointing appears to be at the high end of expectations compared to other MeerKAT observations and group HIMF studies, this finding highlights the capability of MeerKAT for other serendipitous discoveries, and the potential for many more HI-rich galaxies to be revealed within both existing and upcoming Open Time datasets.","sentences":["We report on the serendipitous discovery of 49 HI-rich galaxies in a 2.3 hour Open Time observation with MeerKAT.","We present their properties including their HI masses, intensity and velocity maps, and spectra.","We determine that at least three HI-rich galaxy groups have been detected, potentially as part of a supergroup.","Some members of these galaxy groups show clear interaction with each other in their HI emission.","We cross-match the detections with PanSTARRS, WISE and GALEX, and obtain stellar masses and star formation rates.","One source is found to be a potential OH megamaser, but further follow-up is required to confidently determine this.","For 6 sources with sufficient spatial resolution in HI we produce rotation curves with BBarolo, generate mass models, and derive a dark matter halo mass.","While the number of galaxies detected in this relatively short pointing appears to be at the high end of expectations compared to other MeerKAT observations and group HIMF studies, this finding highlights the capability of MeerKAT for other serendipitous discoveries, and the potential for many more HI-rich galaxies to be revealed within both existing and upcoming Open Time datasets."],"url":"http://arxiv.org/abs/2403.16807v1","category":"astro-ph.GA"}
{"created":"2024-03-25 14:26:09","title":"Hadamard Regularization of the Graviton Stress Tensor","abstract":"We present the details for the covariant renormalization of the stress tensor for vacuum tensor perturbations at the level of the effective action, adopting Hadamard regularization techniques to isolate short distance divergences and gauge fixing via the Faddeev-Popov procedure. The subsequently derived renormalized stress tensor can be related to more familiar forms reliant upon an averaging prescription, such as the Isaacson or Misner-Thorne-Wheeler forms. The latter, however, are premised on a prior scale separation (beyond which the averaging is invoked) and therefore unsuited for the purposes of renormalization. This can lead to potentially unphysical conclusions when taken as a starting point for the computation of any observable that needs regularization, such as the energy density associated to a stochastic background. Any averaging prescription, if needed, should only be invoked at the end of the renormalization procedure. The latter necessarily involves the imposition of renormalization conditions via a physical measurement at some fixed scale, which we retrace for primordial gravitational waves sourced from vacuum fluctuations through direct or indirect observation.","sentences":["We present the details for the covariant renormalization of the stress tensor for vacuum tensor perturbations at the level of the effective action, adopting Hadamard regularization techniques to isolate short distance divergences and gauge fixing via the Faddeev-Popov procedure.","The subsequently derived renormalized stress tensor can be related to more familiar forms reliant upon an averaging prescription, such as the Isaacson or Misner-Thorne-Wheeler forms.","The latter, however, are premised on a prior scale separation (beyond which the averaging is invoked) and therefore unsuited for the purposes of renormalization.","This can lead to potentially unphysical conclusions when taken as a starting point for the computation of any observable that needs regularization, such as the energy density associated to a stochastic background.","Any averaging prescription, if needed, should only be invoked at the end of the renormalization procedure.","The latter necessarily involves the imposition of renormalization conditions via a physical measurement at some fixed scale, which we retrace for primordial gravitational waves sourced from vacuum fluctuations through direct or indirect observation."],"url":"http://arxiv.org/abs/2403.16806v1","category":"hep-th"}
{"created":"2024-03-25 14:21:49","title":"Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View Planning","abstract":"Object reconstruction is relevant for many autonomous robotic tasks that require interaction with the environment. A key challenge in such scenarios is planning view configurations to collect informative measurements for reconstructing an initially unknown object. One-shot view planning enables efficient data collection by predicting view configurations and planning the globally shortest path connecting all views at once. However, geometric priors about the object are required to conduct one-shot view planning. In this work, we propose a novel one-shot view planning approach that utilizes the powerful 3D generation capabilities of diffusion models as priors. By incorporating such geometric priors into our pipeline, we achieve effective one-shot view planning starting with only a single RGB image of the object to be reconstructed. Our planning experiments in simulation and real-world setups indicate that our approach balances well between object reconstruction quality and movement cost.","sentences":["Object reconstruction is relevant for many autonomous robotic tasks that require interaction with the environment.","A key challenge in such scenarios is planning view configurations to collect informative measurements for reconstructing an initially unknown object.","One-shot view planning enables efficient data collection by predicting view configurations and planning the globally shortest path connecting all views at once.","However, geometric priors about the object are required to conduct one-shot view planning.","In this work, we propose a novel one-shot view planning approach that utilizes the powerful 3D generation capabilities of diffusion models as priors.","By incorporating such geometric priors into our pipeline, we achieve effective one-shot view planning starting with only a single RGB image of the object to be reconstructed.","Our planning experiments in simulation and real-world setups indicate that our approach balances well between object reconstruction quality and movement cost."],"url":"http://arxiv.org/abs/2403.16803v1","category":"cs.RO"}
{"created":"2024-03-25 14:21:02","title":"Nambu-Goldstone Modes in Magnetized $T^{2n}$ Extra Dimensions","abstract":"We consider a $U(1)$ gauge theory on $M^4\\times T^4$ with background magnetic fluxes. We show that a theory including arbitrary fluxes can always be studied in a theory involving only diagonal fluxes by appropriate coordinate transformations. It is found that the number of independent magnetic fluxes is equal to the rank of the classical value of the field strength matrix, ${\\rm rank}\\langle F\\rangle$. The number of massless zero modes induced from extra components of higher-dimensional gauge field (Wilson-line scalar field), is also determined by ${\\rm rank}\\langle F\\rangle$. We explicitly confirm that the quantum corrections due to the matter fermion to the squared mass of Wilson-line scalar field cancel out at one-loop level. For this purpose, we derive the fermion mass spectrum on $M^4\\times T^4$ with arbitrary fluxes. By taking the flux diagonal basis, creation and annihilation operators for Kaluza-Klein quantum numbers are defined appropriately. Our results are easily generalized to the case of $M^4\\times T^{2n}~(n\\geq3)$.","sentences":["We consider a $U(1)$ gauge theory on $M^4\\times T^4$ with background magnetic fluxes.","We show that a theory including arbitrary fluxes can always be studied in a theory involving only diagonal fluxes by appropriate coordinate transformations.","It is found that the number of independent magnetic fluxes is equal to the rank of the classical value of the field strength matrix,","${\\rm rank}\\langle F\\rangle$.","The number of massless zero modes induced from extra components of higher-dimensional gauge field (Wilson-line scalar field), is also determined by ${\\rm rank}\\langle F\\rangle$. We explicitly confirm that the quantum corrections due to the matter fermion to the squared mass of Wilson-line scalar field cancel out at one-loop level.","For this purpose, we derive the fermion mass spectrum on $M^4\\times T^4$ with arbitrary fluxes.","By taking the flux diagonal basis, creation and annihilation operators for Kaluza-Klein quantum numbers are defined appropriately.","Our results are easily generalized to the case of $M^4\\times T^{2n}~(n\\geq3)$."],"url":"http://arxiv.org/abs/2403.16801v1","category":"hep-th"}
{"created":"2024-03-25 14:17:38","title":"Cluster-Based Normalization Layer for Neural Networks","abstract":"Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions. This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration. For SCB-Norm, a supervised variant, the novel mechanism involves introducing predefined data partitioning, termed clusters, to normalize activations based on the assigned cluster. This cluster-driven approach creates a space that conforms to a Gaussian mixture model. On the other hand, UCB-Norm, an unsupervised counterpart, dynamically clusters neuron activations during training, adapting to task-specific challenges without relying on predefined data partitions (clusters). This dual approach ensures flexibility in addressing diverse learning scenarios. CB-Norm innovatively uses a one-step normalization approach, where parameters of each mixture component (cluster in activation space) serve as weights for deep neural networks. This adaptive clustering process tackles both clustering and resolution of deep neural network tasks concurrently during training, signifying a notable advancement in the field.","sentences":["Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity.","While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability.","Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions.","This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach.","CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration.","For SCB-Norm, a supervised variant, the novel mechanism involves introducing predefined data partitioning, termed clusters, to normalize activations based on the assigned cluster.","This cluster-driven approach creates a space that conforms to a Gaussian mixture model.","On the other hand, UCB-Norm, an unsupervised counterpart, dynamically clusters neuron activations during training, adapting to task-specific challenges without relying on predefined data partitions (clusters).","This dual approach ensures flexibility in addressing diverse learning scenarios.","CB-Norm innovatively uses a one-step normalization approach, where parameters of each mixture component (cluster in activation space) serve as weights for deep neural networks.","This adaptive clustering process tackles both clustering and resolution of deep neural network tasks concurrently during training, signifying a notable advancement in the field."],"url":"http://arxiv.org/abs/2403.16798v1","category":"cs.LG"}
{"created":"2024-03-25 14:13:09","title":"CurbNet: Curb Detection Framework Based on LiDAR Point Cloud Segmentation","abstract":"Curb detection is an important function in intelligent driving and can be used to determine drivable areas of the road. However, curbs are difficult to detect due to the complex road environment. This paper introduces CurbNet, a novel framework for curb detection, leveraging point cloud segmentation. Addressing the dearth of comprehensive curb datasets and the absence of 3D annotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames, which represents the largest and most categorically diverse collection of curb point clouds currently available. Recognizing that curbs are primarily characterized by height variations, our approach harnesses spatially-rich 3D point clouds for training. To tackle the challenges presented by the uneven distribution of curb features on the xy-plane and their reliance on z-axis high-frequency features, we introduce the multi-scale and channel attention (MSCA) module, a bespoke solution designed to optimize detection performance. Moreover, we propose an adaptive weighted loss function group, specifically formulated to counteract the imbalance in the distribution of curb point clouds relative to other categories. Our extensive experimentation on 2 major datasets has yielded results that surpass existing benchmarks set by leading curb detection and point cloud segmentation models. By integrating multi-clustering and curve fitting techniques in our post-processing stage, we have substantially reduced noise in curb detection, thereby enhancing precision to 0.8744. Notably, CurbNet has achieved an exceptional average metrics of over 0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark. Furthermore, corroborative real-world experiments and dataset analyzes mutually validate each other, solidifying CurbNet's superior detection proficiency and its robust generalizability.","sentences":["Curb detection is an important function in intelligent driving and can be used to determine drivable areas of the road.","However, curbs are difficult to detect due to the complex road environment.","This paper introduces CurbNet, a novel framework for curb detection, leveraging point cloud segmentation.","Addressing the dearth of comprehensive curb datasets and the absence of 3D annotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames, which represents the largest and most categorically diverse collection of curb point clouds currently available.","Recognizing that curbs are primarily characterized by height variations, our approach harnesses spatially-rich 3D point clouds for training.","To tackle the challenges presented by the uneven distribution of curb features on the xy-plane and their reliance on z-axis high-frequency features, we introduce the multi-scale and channel attention (MSCA) module, a bespoke solution designed to optimize detection performance.","Moreover, we propose an adaptive weighted loss function group, specifically formulated to counteract the imbalance in the distribution of curb point clouds relative to other categories.","Our extensive experimentation on 2 major datasets has yielded results that surpass existing benchmarks set by leading curb detection and point cloud segmentation models.","By integrating multi-clustering and curve fitting techniques in our post-processing stage, we have substantially reduced noise in curb detection, thereby enhancing precision to 0.8744.","Notably, CurbNet has achieved an exceptional average metrics of over 0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark.","Furthermore, corroborative real-world experiments and dataset analyzes mutually validate each other, solidifying CurbNet's superior detection proficiency and its robust generalizability."],"url":"http://arxiv.org/abs/2403.16794v1","category":"cs.CV"}
{"created":"2024-03-25 14:11:59","title":"Fidelity of Wormhole Teleportation in Finite-qubit Systems","abstract":"The rapid development of quantum science and technology is leading us into an era where quantum many-body systems can be comprehended through quantum simulations. Holographic duality, which states gravity and spacetime can emerge from strongly interacting systems, then offers a natural avenue for the experimental study of gravity physics without delving into experimentally infeasible high energies. A prominent example is the simulation of traversable wormholes through the wormhole teleportation protocol, attracting both theoretical and experimental attention. In this work, we develop the theoretical framework for computing the fidelity of wormhole teleportation in $N$-qubit systems with all-to-all interactions, quantified by mutual information and entanglement negativity. The main technique is the scramblon effective theory, which captures universal out-of-time-order correlations in generic chaotic systems. We clarify that strong couplings between the two systems are essential for simulating the probe limit of semi-classical traversable wormholes using strongly interacting systems with near-maximal chaos. However, the teleportation signal diminishes rapidly when reducing the system size $N$, requiring a large number of qubits to observe a sharp signature of emergent geometry by simulating the Sachdev-Ye-Kitaev model. This includes both the causal time-order of signals and the asymmetry of the teleportation signal for coupling with different signs. As a comparison, the teleportation signal increases when reducing $N$ in weakly interacting systems. We also analyze the fidelity of the generalized encoding scheme in fermionic string operators.","sentences":["The rapid development of quantum science and technology is leading us into an era where quantum many-body systems can be comprehended through quantum simulations.","Holographic duality, which states gravity and spacetime can emerge from strongly interacting systems, then offers a natural avenue for the experimental study of gravity physics without delving into experimentally infeasible high energies.","A prominent example is the simulation of traversable wormholes through the wormhole teleportation protocol, attracting both theoretical and experimental attention.","In this work, we develop the theoretical framework for computing the fidelity of wormhole teleportation in $N$-qubit systems with all-to-all interactions, quantified by mutual information and entanglement negativity.","The main technique is the scramblon effective theory, which captures universal out-of-time-order correlations in generic chaotic systems.","We clarify that strong couplings between the two systems are essential for simulating the probe limit of semi-classical traversable wormholes using strongly interacting systems with near-maximal chaos.","However, the teleportation signal diminishes rapidly when reducing the system size $N$, requiring a large number of qubits to observe a sharp signature of emergent geometry by simulating the Sachdev-Ye-Kitaev model.","This includes both the causal time-order of signals and the asymmetry of the teleportation signal for coupling with different signs.","As a comparison, the teleportation signal increases when reducing $N$ in weakly interacting systems.","We also analyze the fidelity of the generalized encoding scheme in fermionic string operators."],"url":"http://arxiv.org/abs/2403.16793v1","category":"quant-ph"}
{"created":"2024-03-25 14:07:27","title":"Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback","abstract":"Large language models (LLMs) have shown remarkable progress in automated code generation. Yet, incorporating LLM-based code generation into real-life software projects poses challenges, as the generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. To this end, this paper puts forward a novel approach, termed ProCoder, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback. In particular, ProCoder first leverages compiler techniques to identify a mismatch between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate ProCoder with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that ProCoder significantly improves the vanilla LLMs by over 80% in generating code dependent on project context, and consistently outperforms the existing retrieval-based code generation baselines.","sentences":["Large language models (LLMs) have shown remarkable progress in automated code generation.","Yet, incorporating LLM-based code generation into real-life software projects poses challenges, as the generated code may contain errors in API usage, class, data structure, or missing project-specific information.","As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context.","To this end, this paper puts forward a novel approach, termed ProCoder, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback.","In particular, ProCoder first leverages compiler techniques to identify a mismatch between the generated code and the project's context.","It then iteratively aligns and fixes the identified errors using information extracted from the code repository.","We integrate ProCoder with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation.","Experimental results show that ProCoder significantly improves the vanilla LLMs by over 80% in generating code dependent on project context, and consistently outperforms the existing retrieval-based code generation baselines."],"url":"http://arxiv.org/abs/2403.16792v1","category":"cs.CL"}
{"created":"2024-03-25 14:06:18","title":"Nonlinear dynamics as a ground-state solution on quantum computers","abstract":"For the solution of time-dependent nonlinear differential equations, we present variational quantum algorithms (VQAs) that encode both space and time in qubit registers. The spacetime encoding enables us to obtain the entire time evolution from a single ground-state computation. We describe a general procedure to construct efficient quantum circuits for the cost function evaluation required by VQAs. To mitigate the barren plateau problem during the optimization, we propose an adaptive multigrid strategy. The approach is illustrated for the nonlinear Burgers equation. We classically optimize quantum circuits to represent the desired ground-state solutions, run them on IBM Q System One and Quantinuum System Model H1, and demonstrate that current quantum computers are capable of accurately reproducing the exact results.","sentences":["For the solution of time-dependent nonlinear differential equations, we present variational quantum algorithms (VQAs) that encode both space and time in qubit registers.","The spacetime encoding enables us to obtain the entire time evolution from a single ground-state computation.","We describe a general procedure to construct efficient quantum circuits for the cost function evaluation required by VQAs.","To mitigate the barren plateau problem during the optimization, we propose an adaptive multigrid strategy.","The approach is illustrated for the nonlinear Burgers equation.","We classically optimize quantum circuits to represent the desired ground-state solutions, run them on IBM Q System One and Quantinuum System Model H1, and demonstrate that current quantum computers are capable of accurately reproducing the exact results."],"url":"http://arxiv.org/abs/2403.16791v1","category":"quant-ph"}
{"created":"2024-03-25 14:05:52","title":"Iso-Diffusion: Improving Diffusion Probabilistic Models Using the Isotropy of the Additive Gaussian Noise","abstract":"Denoising Diffusion Probabilistic Models (DDPMs) have accomplished much in the realm of generative AI. Despite their high performance, there is room for improvement, especially in terms of sample fidelity by utilizing statistical properties that impose structural integrity, such as isotropy. Minimizing the mean squared error between the additive and predicted noise alone does not impose constraints on the predicted noise to be isotropic. Thus, we were motivated to utilize the isotropy of the additive noise as a constraint on the objective function to enhance the fidelity of DDPMs. Our approach is simple and can be applied to any DDPM variant. We validate our approach by presenting experiments conducted on four synthetic 2D datasets as well as on unconditional image generation. As demonstrated by the results, the incorporation of this constraint improves the fidelity metrics, Precision and Density for the 2D datasets as well as for the unconditional image generation.","sentences":["Denoising Diffusion Probabilistic Models (DDPMs) have accomplished much in the realm of generative AI.","Despite their high performance, there is room for improvement, especially in terms of sample fidelity by utilizing statistical properties that impose structural integrity, such as isotropy.","Minimizing the mean squared error between the additive and predicted noise alone does not impose constraints on the predicted noise to be isotropic.","Thus, we were motivated to utilize the isotropy of the additive noise as a constraint on the objective function to enhance the fidelity of DDPMs.","Our approach is simple and can be applied to any DDPM variant.","We validate our approach by presenting experiments conducted on four synthetic 2D datasets as well as on unconditional image generation.","As demonstrated by the results, the incorporation of this constraint improves the fidelity metrics, Precision and Density for the 2D datasets as well as for the unconditional image generation."],"url":"http://arxiv.org/abs/2403.16790v1","category":"cs.LG"}
{"created":"2024-03-25 14:05:21","title":"H-Clique-Width and a Hereditary Analogue of Product Structure","abstract":"We introduce a novel generalization of the notion of clique-width which aims to bridge the gap between classical hereditary width measures and the recently introduced graph product structure theory. Bounding the new H-clique-width, in the special case of H being the class of paths, is equivalent to admitting a hereditary (i.e., induced) product structure of a path times a graph of bounded clique-width. Furthermore, every graph admitting the usual (non-induced) product structure of a path times a graph of bounded tree-width, has bounded H-clique-width, i.e., it admits the new product structure in an induced way. We prove further basic properties of H-clique-width in general.","sentences":["We introduce a novel generalization of the notion of clique-width which aims to bridge the gap between classical hereditary width measures and the recently introduced graph product structure theory.","Bounding the new H-clique-width, in the special case of H being the class of paths, is equivalent to admitting a hereditary (i.e., induced) product structure of a path times a graph of bounded clique-width.","Furthermore, every graph admitting the usual (non-induced) product structure of a path times a graph of bounded tree-width, has bounded H-clique-width, i.e., it admits the new product structure in an induced way.","We prove further basic properties of H-clique-width in general."],"url":"http://arxiv.org/abs/2403.16789v1","category":"math.CO"}
{"created":"2024-03-25 14:02:33","title":"HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation","abstract":"Event-based semantic segmentation has gained popularity due to its capability to deal with scenarios under high-speed motion and extreme lighting conditions, which cannot be addressed by conventional RGB cameras. Since it is hard to annotate event data, previous approaches rely on event-to-image reconstruction to obtain pseudo labels for training. However, this will inevitably introduce noise, and learning from noisy pseudo labels, especially when generated from a single source, may reinforce the errors. This drawback is also called confirmation bias in pseudo-labeling. In this paper, we propose a novel hybrid pseudo-labeling framework for unsupervised event-based semantic segmentation, HPL-ESS, to alleviate the influence of noisy pseudo labels. In particular, we first employ a plain unsupervised domain adaptation framework as our baseline, which can generate a set of pseudo labels through self-training. Then, we incorporate offline event-to-image reconstruction into the framework, and obtain another set of pseudo labels by predicting segmentation maps on the reconstructed images. A noisy label learning strategy is designed to mix the two sets of pseudo labels and enhance the quality. Moreover, we propose a soft prototypical alignment module to further improve the consistency of target domain features. Extensive experiments show that our proposed method outperforms existing state-of-the-art methods by a large margin on the DSEC-Semantic dataset (+5.88% accuracy, +10.32% mIoU), which even surpasses several supervised methods.","sentences":["Event-based semantic segmentation has gained popularity due to its capability to deal with scenarios under high-speed motion and extreme lighting conditions, which cannot be addressed by conventional RGB cameras.","Since it is hard to annotate event data, previous approaches rely on event-to-image reconstruction to obtain pseudo labels for training.","However, this will inevitably introduce noise, and learning from noisy pseudo labels, especially when generated from a single source, may reinforce the errors.","This drawback is also called confirmation bias in pseudo-labeling.","In this paper, we propose a novel hybrid pseudo-labeling framework for unsupervised event-based semantic segmentation, HPL-ESS, to alleviate the influence of noisy pseudo labels.","In particular, we first employ a plain unsupervised domain adaptation framework as our baseline, which can generate a set of pseudo labels through self-training.","Then, we incorporate offline event-to-image reconstruction into the framework, and obtain another set of pseudo labels by predicting segmentation maps on the reconstructed images.","A noisy label learning strategy is designed to mix the two sets of pseudo labels and enhance the quality.","Moreover, we propose a soft prototypical alignment module to further improve the consistency of target domain features.","Extensive experiments show that our proposed method outperforms existing state-of-the-art methods by a large margin on the DSEC-Semantic dataset (+5.88% accuracy, +10.32% mIoU), which even surpasses several supervised methods."],"url":"http://arxiv.org/abs/2403.16788v1","category":"cs.CV"}
{"created":"2024-03-25 14:01:58","title":"DBPF: A Framework for Efficient and Robust Dynamic Bin-Picking","abstract":"Efficiency and reliability are critical in robotic bin-picking as they directly impact the productivity of automated industrial processes. However, traditional approaches, demanding static objects and fixed collisions, lead to deployment limitations, operational inefficiencies, and process unreliability. This paper introduces a Dynamic Bin-Picking Framework (DBPF) that challenges traditional static assumptions. The DBPF endows the robot with the reactivity to pick multiple moving arbitrary objects while avoiding dynamic obstacles, such as the moving bin. Combined with scene-level pose generation, the proposed pose selection metric leverages the Tendency-Aware Manipulability Network optimizing suction pose determination. Heuristic task-specific designs like velocity-matching, dynamic obstacle avoidance, and the resight policy, enhance the picking success rate and reliability. Empirical experiments demonstrate the importance of these components. Our method achieves an average 84% success rate, surpassing the 60% of the most comparable baseline, crucially, with zero collisions. Further evaluations under diverse dynamic scenarios showcase DBPF's robust performance in dynamic bin-picking. Results suggest that our framework offers a promising solution for efficient and reliable robotic bin-picking under dynamics.","sentences":["Efficiency and reliability are critical in robotic bin-picking as they directly impact the productivity of automated industrial processes.","However, traditional approaches, demanding static objects and fixed collisions, lead to deployment limitations, operational inefficiencies, and process unreliability.","This paper introduces a Dynamic Bin-Picking Framework (DBPF) that challenges traditional static assumptions.","The DBPF endows the robot with the reactivity to pick multiple moving arbitrary objects while avoiding dynamic obstacles, such as the moving bin.","Combined with scene-level pose generation, the proposed pose selection metric leverages the Tendency-Aware Manipulability Network optimizing suction pose determination.","Heuristic task-specific designs like velocity-matching, dynamic obstacle avoidance, and the resight policy, enhance the picking success rate and reliability.","Empirical experiments demonstrate the importance of these components.","Our method achieves an average 84% success rate, surpassing the 60% of the most comparable baseline, crucially, with zero collisions.","Further evaluations under diverse dynamic scenarios showcase DBPF's robust performance in dynamic bin-picking.","Results suggest that our framework offers a promising solution for efficient and reliable robotic bin-picking under dynamics."],"url":"http://arxiv.org/abs/2403.16786v1","category":"cs.RO"}
{"created":"2024-03-25 13:59:07","title":"Enhanced extracellular matrix remodeling due to embedded spheroid fluidization","abstract":"Tumor spheroids are in vitro three-dimensional, cellular collectives consisting of cancerous cells. Embedding these spheroids in an in vitro fibrous environment, such as a collagen network, to mimic the extracellular matrix (ECM) provides an essential platform to quantitatively investigate the biophysical mechanisms leading to tumor invasion of the ECM. To understand the mechanical interplay between tumor spheroids and the ECM, we computationally construct and study a three-dimensional vertex model for a tumor spheroid that is mechanically coupled to a cross-linked network of fibers. In such a vertex model, cells are represented as deformable polyhedrons that share faces. Some fraction of the boundary faces of the tumor spheroid contain linker springs connecting the center of the boundary face to the nearest node in the fiber network. As these linker springs actively contract, the fiber network remodels. By toggling between fluid-like and solid-like spheroids via changing the dimensionless cell shape index, we find that the spheroid rheology affects the remodeling of the fiber network. More precisely, fluid-like spheroids displace the fiber network more on average near the vicinity of the spheroid than solid-like spheroids. We also find more densification of the fiber network near the spheroid for the fluid-like spheroids. These spheroid rheology-dependent effects are the result of cellular motility due to active cellular rearrangements that emerge over time in the fluid-like spheroids to generate spheroid shape fluctuations. Our results uncover intricate morphological-mechanical interplay between an embedded spheroid and its surrounding fiber network with both spheroid contractile strength and spheroid shape fluctuations playing important roles in the pre-invasion stages of tumor invasion.","sentences":["Tumor spheroids are in vitro three-dimensional, cellular collectives consisting of cancerous cells.","Embedding these spheroids in an in vitro fibrous environment, such as a collagen network, to mimic the extracellular matrix (ECM) provides an essential platform to quantitatively investigate the biophysical mechanisms leading to tumor invasion of the ECM.","To understand the mechanical interplay between tumor spheroids and the ECM, we computationally construct and study a three-dimensional vertex model for a tumor spheroid that is mechanically coupled to a cross-linked network of fibers.","In such a vertex model, cells are represented as deformable polyhedrons that share faces.","Some fraction of the boundary faces of the tumor spheroid contain linker springs connecting the center of the boundary face to the nearest node in the fiber network.","As these linker springs actively contract, the fiber network remodels.","By toggling between fluid-like and solid-like spheroids via changing the dimensionless cell shape index, we find that the spheroid rheology affects the remodeling of the fiber network.","More precisely, fluid-like spheroids displace the fiber network more on average near the vicinity of the spheroid than solid-like spheroids.","We also find more densification of the fiber network near the spheroid for the fluid-like spheroids.","These spheroid rheology-dependent effects are the result of cellular motility due to active cellular rearrangements that emerge over time in the fluid-like spheroids to generate spheroid shape fluctuations.","Our results uncover intricate morphological-mechanical interplay between an embedded spheroid and its surrounding fiber network with both spheroid contractile strength and spheroid shape fluctuations playing important roles in the pre-invasion stages of tumor invasion."],"url":"http://arxiv.org/abs/2403.16784v1","category":"physics.bio-ph"}
{"created":"2024-03-25 13:58:34","title":"Concavity for elliptic and parabolic equations in complex projective space","abstract":"We establish a concavity principle for solutions to elliptic and parabolic equations on complex projective space, generalizing the results of Langford and Scheuer. To our knowledge, this is the first example of a general concavity principle outside the constant sectional curvature regime, and in particular, our result partially answers a question raised by Korevaar in 1985 regarding the concavity of solutions to elliptic equations on manifolds with non-constant sectional curvature.","sentences":["We establish a concavity principle for solutions to elliptic and parabolic equations on complex projective space, generalizing the results of Langford and Scheuer.","To our knowledge, this is the first example of a general concavity principle outside the constant sectional curvature regime, and in particular, our result partially answers a question raised by Korevaar in 1985 regarding the concavity of solutions to elliptic equations on manifolds with non-constant sectional curvature."],"url":"http://arxiv.org/abs/2403.16783v1","category":"math.AP"}
{"created":"2024-03-25 13:57:45","title":"The Anatomy of Adversarial Attacks: Concept-based XAI Dissection","abstract":"Adversarial attacks (AAs) pose a significant threat to the reliability and robustness of deep neural networks. While the impact of these attacks on model predictions has been extensively studied, their effect on the learned representations and concepts within these models remains largely unexplored. In this work, we perform an in-depth analysis of the influence of AAs on the concepts learned by convolutional neural networks (CNNs) using eXplainable artificial intelligence (XAI) techniques. Through an extensive set of experiments across various network architectures and targeted AA techniques, we unveil several key findings. First, AAs induce substantial alterations in the concept composition within the feature space, introducing new concepts or modifying existing ones. Second, the adversarial perturbation itself can be linearly decomposed into a set of latent vector components, with a subset of these being responsible for the attack's success. Notably, we discover that these components are target-specific, i.e., are similar for a given target class throughout different AA techniques and starting classes. Our findings provide valuable insights into the nature of AAs and their impact on learned representations, paving the way for the development of more robust and interpretable deep learning models, as well as effective defenses against adversarial threats.","sentences":["Adversarial attacks (AAs) pose a significant threat to the reliability and robustness of deep neural networks.","While the impact of these attacks on model predictions has been extensively studied, their effect on the learned representations and concepts within these models remains largely unexplored.","In this work, we perform an in-depth analysis of the influence of AAs on the concepts learned by convolutional neural networks (CNNs) using eXplainable artificial intelligence (XAI) techniques.","Through an extensive set of experiments across various network architectures and targeted AA techniques, we unveil several key findings.","First, AAs induce substantial alterations in the concept composition within the feature space, introducing new concepts or modifying existing ones.","Second, the adversarial perturbation itself can be linearly decomposed into a set of latent vector components, with a subset of these being responsible for the attack's success.","Notably, we discover that these components are target-specific, i.e., are similar for a given target class throughout different AA techniques and starting classes.","Our findings provide valuable insights into the nature of AAs and their impact on learned representations, paving the way for the development of more robust and interpretable deep learning models, as well as effective defenses against adversarial threats."],"url":"http://arxiv.org/abs/2403.16782v1","category":"cs.LG"}
{"created":"2024-03-25 13:52:48","title":"Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases","abstract":"Anatomical atlases are widely used for population analysis. Conditional atlases target a particular sub-population defined via certain conditions (e.g. demographics or pathologies) and allow for the investigation of fine-grained anatomical differences - such as morphological changes correlated with age. Existing approaches use either registration-based methods that are unable to handle large anatomical variations or generative models, which can suffer from training instabilities and hallucinations. To overcome these limitations, we use latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. By generating a deformation field and registering the conditional atlas to a neighbourhood of images, we ensure structural plausibility and avoid hallucinations, which can occur during direct image synthesis. We compare our method to several state-of-the-art atlas generation methods in experiments using 5000 brain as well as whole-body MR images from UK Biobank. Our method generates highly realistic atlases with smooth transformations and high anatomical fidelity, outperforming the baselines.","sentences":["Anatomical atlases are widely used for population analysis.","Conditional atlases target a particular sub-population defined via certain conditions (e.g. demographics or pathologies) and allow for the investigation of fine-grained anatomical differences - such as morphological changes correlated with age.","Existing approaches use either registration-based methods that are unable to handle large anatomical variations or generative models, which can suffer from training instabilities and hallucinations.","To overcome these limitations, we use latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population.","By generating a deformation field and registering the conditional atlas to a neighbourhood of images, we ensure structural plausibility and avoid hallucinations, which can occur during direct image synthesis.","We compare our method to several state-of-the-art atlas generation methods in experiments using 5000 brain as well as whole-body MR images from UK Biobank.","Our method generates highly realistic atlases with smooth transformations and high anatomical fidelity, outperforming the baselines."],"url":"http://arxiv.org/abs/2403.16776v1","category":"eess.IV"}
{"created":"2024-03-25 13:52:36","title":"Stochastic Inertial Dynamics Via Time Scaling and Averaging","abstract":"Our work is part of the close link between continuous-time dissipative dynamical systems and optimization algorithms, and more precisely here, in the stochastic setting. We aim to study stochastic convex minimization problems through the lens of stochastic inertial differential inclusions that are driven by the subgradient of a convex objective function. This will provide a general mathematical framework for analyzing the convergence properties of stochastic second-order inertial continuous-time dynamics involving vanishing viscous damping and measurable stochastic subgradient selections. Our chief goal in this paper is to develop a systematic and unified way that transfers the properties recently studied for first-order stochastic differential equations to second-order ones involving even subgradients in lieu of gradients. This program will rely on two tenets: time scaling and averaging, following an approach recently developed in the literature by one of the co-authors in the deterministic case.   Under a mild integrability assumption involving the diffusion term and the viscous damping, our first main result shows that almost surely, there is weak convergence of the trajectory towards a minimizer of the objective function and fast convergence of the values and gradients. We also provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex, strongly convex, and (local) Polyak-Lojasiewicz case. Finally, using Tikhonov regularization with a properly tuned vanishing parameter, we can obtain almost sure strong convergence of the trajectory towards the minimum norm solution.","sentences":["Our work is part of the close link between continuous-time dissipative dynamical systems and optimization algorithms, and more precisely here, in the stochastic setting.","We aim to study stochastic convex minimization problems through the lens of stochastic inertial differential inclusions that are driven by the subgradient of a convex objective function.","This will provide a general mathematical framework for analyzing the convergence properties of stochastic second-order inertial continuous-time dynamics involving vanishing viscous damping and measurable stochastic subgradient selections.","Our chief goal in this paper is to develop a systematic and unified way that transfers the properties recently studied for first-order stochastic differential equations to second-order ones involving even subgradients in lieu of gradients.","This program will rely on two tenets: time scaling and averaging, following an approach recently developed in the literature by one of the co-authors in the deterministic case.   ","Under a mild integrability assumption involving the diffusion term and the viscous damping, our first main result shows that almost surely, there is weak convergence of the trajectory towards a minimizer of the objective function and fast convergence of the values and gradients.","We also provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex, strongly convex, and (local) Polyak-Lojasiewicz case.","Finally, using Tikhonov regularization with a properly tuned vanishing parameter, we can obtain almost sure strong convergence of the trajectory towards the minimum norm solution."],"url":"http://arxiv.org/abs/2403.16775v1","category":"math.OC"}
{"created":"2024-03-25 13:50:11","title":"Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation","abstract":"The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish to English translation. Our evaluation and comprehensive analyses qualitatively and quantitatively demonstrate the superiority of RCMT over state-of-the-art code-mixed and robust translation methods.","sentences":["The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance.","This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise.","A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation.","In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation.","First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs.","Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words.","Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish to English translation.","Our evaluation and comprehensive analyses qualitatively and quantitatively demonstrate the superiority of RCMT over state-of-the-art code-mixed and robust translation methods."],"url":"http://arxiv.org/abs/2403.16771v1","category":"cs.CL"}
{"created":"2024-03-25 13:46:09","title":"DeepKnowledge: Generalisation-Driven Deep Learning Testing","abstract":"Despite their unprecedented success, DNNs are notoriously fragile to small shifts in data distribution, demanding effective testing techniques that can assess their dependability. Despite recent advances in DNN testing, there is a lack of systematic testing approaches that assess the DNN's capability to generalise and operate comparably beyond data in their training distribution. We address this gap with DeepKnowledge, a systematic testing methodology for DNN-based systems founded on the theory of knowledge generalisation, which aims to enhance DNN robustness and reduce the residual risk of 'black box' models. Conforming to this theory, DeepKnowledge posits that core computational DNN units, termed Transfer Knowledge neurons, can generalise under domain shift. DeepKnowledge provides an objective confidence measurement on testing activities of DNN given data distribution shifts and uses this information to instrument a generalisation-informed test adequacy criterion to check the transfer knowledge capacity of a test set. Our empirical evaluation of several DNNs, across multiple datasets and state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepKnowledge and its ability to support the engineering of more dependable DNNs. We report improvements of up to 10 percentage points over state-of-the-art coverage criteria for detecting adversarial attacks on several benchmarks, including MNIST, SVHN, and CIFAR.","sentences":["Despite their unprecedented success, DNNs are notoriously fragile to small shifts in data distribution, demanding effective testing techniques that can assess their dependability.","Despite recent advances in DNN testing, there is a lack of systematic testing approaches that assess the DNN's capability to generalise and operate comparably beyond data in their training distribution.","We address this gap with DeepKnowledge, a systematic testing methodology for DNN-based systems founded on the theory of knowledge generalisation, which aims to enhance DNN robustness and reduce the residual risk of 'black box' models.","Conforming to this theory, DeepKnowledge posits that core computational DNN units, termed Transfer Knowledge neurons, can generalise under domain shift.","DeepKnowledge provides an objective confidence measurement on testing activities of DNN given data distribution shifts and uses this information to instrument a generalisation-informed test adequacy criterion to check the transfer knowledge capacity of a test set.","Our empirical evaluation of several DNNs, across multiple datasets and state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepKnowledge and its ability to support the engineering of more dependable DNNs.","We report improvements of up to 10 percentage points over state-of-the-art coverage criteria for detecting adversarial attacks on several benchmarks, including MNIST, SVHN, and CIFAR."],"url":"http://arxiv.org/abs/2403.16768v1","category":"cs.LG"}
{"created":"2024-03-25 13:39:50","title":"On the Continued Fraction Expansion of Almost All Real Numbers","abstract":"By a classical result of Gauss and Kuzmin, the continued fraction expansion of a ``random'' real number contains each digit $a\\in\\mathbb{N}$ with asymptotic frequency $\\log_2(1+1/(a(a+2)))$.   We generalize this result in two directions: First, for certain sets $A\\subset\\mathbb{N}$, we establish simple explicit formulas for the frequency with which the continued fraction expansion of a random real number contains a digit from the set $A$. For example, we show that digits of the form $p-1$, where $p$ is prime, appear with frequency $\\log_2(\\pi^2/6)$.   Second, we obtain a simple formula for the frequency with which a string of $k$ consecutive digits $a$ appears in the continued fraction expansion of a random real number. In particular, when $a=1$, this frequency is given by $|\\log_2(1+(-1)^k/F_{k+2})|$, where $F_n$ is the $n$th Fibonacci number.   Finally, we compare the frequencies predicted by these results with actual frequencies found among the first 300 million continued fraction digits of $\\pi$, and we provide strong statistical evidence that the continued fraction expansion of $\\pi$ behaves like that of a random real number.","sentences":["By a classical result of Gauss and Kuzmin, the continued fraction expansion of a ``random'' real number contains each digit $a\\in\\mathbb{N}$ with asymptotic frequency $\\log_2(1+1/(a(a+2)))$.   We generalize this result in two directions: First, for certain sets $A\\subset\\mathbb{N}$, we establish simple explicit formulas for the frequency with which the continued fraction expansion of a random real number contains a digit from the set $A$.","For example, we show that digits of the form $p-1$, where $p$ is prime, appear with frequency $\\log_2(\\pi^2/6)$.   Second, we obtain a simple formula for the frequency with which a string of $k$ consecutive digits $a$ appears in the continued fraction expansion of a random real number.","In particular, when $a=1$, this frequency is given by $|\\log_2(1+(-1)^k/F_{k+2})|$, where $F_n$ is the $n$th Fibonacci number.   ","Finally, we compare the frequencies predicted by these results with actual frequencies found among the first 300 million continued fraction digits of $\\pi$, and we provide strong statistical evidence that the continued fraction expansion of $\\pi$ behaves like that of a random real number."],"url":"http://arxiv.org/abs/2403.16761v1","category":"math.NT"}
{"created":"2024-03-25 13:39:33","title":"As Good As A Coin Toss Human detection of AI-generated images, videos, audio, and audiovisual stimuli","abstract":"As synthetic media becomes progressively more realistic and barriers to using it continue to lower, the technology has been increasingly utilized for malicious purposes, from financial fraud to nonconsensual pornography. Today, the principal defense against being misled by synthetic media relies on the ability of the human observer to visually and auditorily discern between real and fake. However, it remains unclear just how vulnerable people actually are to deceptive synthetic media in the course of their day to day lives. We conducted a perceptual study with 1276 participants to assess how accurate people were at distinguishing synthetic images, audio only, video only, and audiovisual stimuli from authentic. To reflect the circumstances under which people would likely encounter synthetic media in the wild, testing conditions and stimuli emulated a typical online platform, while all synthetic media used in the survey was sourced from publicly accessible generative AI technology.   We find that overall, participants struggled to meaningfully discern between synthetic and authentic content. We also find that detection performance worsens when the stimuli contains synthetic content as compared to authentic content, images featuring human faces as compared to non face objects, a single modality as compared to multimodal stimuli, mixed authenticity as compared to being fully synthetic for audiovisual stimuli, and features foreign languages as compared to languages the observer is fluent in. Finally, we also find that prior knowledge of synthetic media does not meaningfully impact their detection performance. Collectively, these results indicate that people are highly susceptible to being tricked by synthetic media in their daily lives and that human perceptual detection capabilities can no longer be relied upon as an effective counterdefense.","sentences":["As synthetic media becomes progressively more realistic and barriers to using it continue to lower, the technology has been increasingly utilized for malicious purposes, from financial fraud to nonconsensual pornography.","Today, the principal defense against being misled by synthetic media relies on the ability of the human observer to visually and auditorily discern between real and fake.","However, it remains unclear just how vulnerable people actually are to deceptive synthetic media in the course of their day to day lives.","We conducted a perceptual study with 1276 participants to assess how accurate people were at distinguishing synthetic images, audio only, video only, and audiovisual stimuli from authentic.","To reflect the circumstances under which people would likely encounter synthetic media in the wild, testing conditions and stimuli emulated a typical online platform, while all synthetic media used in the survey was sourced from publicly accessible generative AI technology.   ","We find that overall, participants struggled to meaningfully discern between synthetic and authentic content.","We also find that detection performance worsens when the stimuli contains synthetic content as compared to authentic content, images featuring human faces as compared to non face objects, a single modality as compared to multimodal stimuli, mixed authenticity as compared to being fully synthetic for audiovisual stimuli, and features foreign languages as compared to languages the observer is fluent in.","Finally, we also find that prior knowledge of synthetic media does not meaningfully impact their detection performance.","Collectively, these results indicate that people are highly susceptible to being tricked by synthetic media in their daily lives and that human perceptual detection capabilities can no longer be relied upon as an effective counterdefense."],"url":"http://arxiv.org/abs/2403.16760v1","category":"cs.HC"}
{"created":"2024-03-25 13:37:19","title":"The spectral continuum in the Rabi-Stark model","abstract":"The Rabi-Stark model is a non-linear generalization of the quantum Rabi model including the dynamical Stark shift as a tunable term, which can be realized via quantum simulation on a cavity QED platform. When the Stark coupling becomes equal to the mode frequency, the spectrum changes drastically, a transition usually termed \"spectral collapse\" because numerical studies indicate an infinitely degenerate ground state. We show that the spectrum extends continuously from a threshold value up to infinity. A set of normalizable states are embedded in the continuum which furnishes an unexpected analogy to the atomic Stark effect. Bound states and continuum can be obtained analytically through two equally justified, but different confluence processes of the associated differential equation in Bargmann space. Moreover, these results are obtained independently using a method based on adiabatic elimination of the spin degree of freedom and corroborated through large-scale numerical checks.","sentences":["The Rabi-Stark model is a non-linear generalization of the quantum Rabi model including the dynamical Stark shift as a tunable term, which can be realized via quantum simulation on a cavity QED platform.","When the Stark coupling becomes equal to the mode frequency, the spectrum changes drastically, a transition usually termed \"spectral collapse\" because numerical studies indicate an infinitely degenerate ground state.","We show that the spectrum extends continuously from a threshold value up to infinity.","A set of normalizable states are embedded in the continuum which furnishes an unexpected analogy to the atomic Stark effect.","Bound states and continuum can be obtained analytically through two equally justified, but different confluence processes of the associated differential equation in Bargmann space.","Moreover, these results are obtained independently using a method based on adiabatic elimination of the spin degree of freedom and corroborated through large-scale numerical checks."],"url":"http://arxiv.org/abs/2403.16758v1","category":"quant-ph"}
{"created":"2024-03-25 13:36:20","title":"Bi-objective Optimization in Role Mining","abstract":"Role mining is a technique used to derive a role-based authorization policy from an existing policy. Given a set of users $U$, a set of permissions $P$ and a user-permission authorization relation $\\mahtit{UPA}\\subseteq U\\times P$, a role mining algorithm seeks to compute a set of roles $R$, a user-role authorization relation $\\mathit{UA}\\subseteq U\\times R$ and a permission-role authorization relation $\\mathit{PA}\\subseteq R\\times P$, such that the composition of $\\mathit{UA}$ and $\\mathit{PA}$ is close (in some appropriate sense) to $\\mathit{UPA}$.   In this paper, we first introduce the Generalized Noise Role Mining problem (GNRM) -- a generalization of the MinNoise Role Mining problem -- which we believe has considerable practical relevance. Extending work of Fomin et al., we show that GNRM is fixed parameter tractable, with parameter $r + k$, where $r$ is the number of roles in the solution and $k$ is the number of discrepancies between $\\mathit{UPA}$ and the relation defined by the composition of $\\mathit{UA}$ and $\\mathit{PA}$. We further introduce a bi-objective optimization variant of GNRM, where we wish to minimize both $r$ and $k$ subject to upper bounds $r\\le \\bar{r}$ and $k\\le \\bar{k}$, where $\\bar{r}$ and $\\bar{k}$ are constants. We show that the Pareto front of this bi-objective optimization problem (BO-GNRM) can be computed in fixed-parameter tractable time with parameter $\\bar{r}+\\bar{k}$.   We then report the results of our experimental work using the integer programming solver Gurobi to solve instances of BO-GNRM. Our key findings are that (a) we obtained strong support that Gurobi's performance is fixed-parameter tractable, (b) our results suggest that our techniques may be useful for role mining in practice, based on our experiments in the context of three well-known real-world authorization policies.","sentences":["Role mining is a technique used to derive a role-based authorization policy from an existing policy.","Given a set of users $U$, a set of permissions $P$ and a user-permission authorization relation $\\mahtit{UPA}\\subseteq U\\times P$, a role mining algorithm seeks to compute a set of roles $R$, a user-role authorization relation $\\mathit{UA}\\subseteq U\\times R$ and a permission-role authorization relation $\\mathit{PA}\\subseteq R\\times P$, such that the composition of $\\mathit{UA}$ and $\\mathit{PA}$ is close (in some appropriate sense) to $\\mathit{UPA}$.   In this paper, we first introduce the Generalized Noise Role Mining problem (GNRM) -- a generalization of the MinNoise Role Mining problem -- which we believe has considerable practical relevance.","Extending work of Fomin et al., we show that GNRM is fixed parameter tractable, with parameter $r + k$, where $r$ is the number of roles in the solution and $k$ is the number of discrepancies between $\\mathit{UPA}$ and the relation defined by the composition of $\\mathit{UA}$ and $\\mathit{PA}$. We further introduce a bi-objective optimization variant of GNRM, where we wish to minimize both $r$ and $k$ subject to upper bounds $r\\le \\bar{r}$ and $k\\le \\bar{k}$, where $\\bar{r}$ and $\\bar{k}$ are constants.","We show that the Pareto front of this bi-objective optimization problem (BO-GNRM) can be computed in fixed-parameter tractable time with parameter $\\bar{r}+\\bar{k}$.   ","We then report the results of our experimental work using the integer programming solver Gurobi to solve instances of BO-GNRM.","Our key findings are that (a) we obtained strong support that Gurobi's performance is fixed-parameter tractable, (b) our results suggest that our techniques may be useful for role mining in practice, based on our experiments in the context of three well-known real-world authorization policies."],"url":"http://arxiv.org/abs/2403.16757v1","category":"cs.CR"}
{"created":"2024-03-25 13:34:31","title":"A Blotto Game Approach to Ride-hailing Markets with Electric Vehicles","abstract":"When a centrally operated ride-hailing company considers to enter a market already served by another company, it has to make a strategic decision about how to distribute its fleet among different regions in the area. This decision will be influenced by the market share the company can secure and the costs associated with charging the vehicles in each region, all while competing with the company already operating in the area. In this paper, we propose a Colonel Blotto-like game to model this decision-making. For the class of games that we study, we first prove the existence and uniqueness of a Nash Equilibrium. Subsequently, we provide its general characterization and present an algorithm for computing the ones in the feasible set's interior. Additionally, for a simplified scenario involving two regions, which would correspond to a city area with a downtown and a suburban region, we also provide a method to check for the equilibria on the feasible set's boundary. Finally, through a numerical case study, we illustrate the impact of charging prices on the position of the Nash equilibrium.","sentences":["When a centrally operated ride-hailing company considers to enter a market already served by another company, it has to make a strategic decision about how to distribute its fleet among different regions in the area.","This decision will be influenced by the market share the company can secure and the costs associated with charging the vehicles in each region, all while competing with the company already operating in the area.","In this paper, we propose a Colonel Blotto-like game to model this decision-making.","For the class of games that we study, we first prove the existence and uniqueness of a Nash Equilibrium.","Subsequently, we provide its general characterization and present an algorithm for computing the ones in the feasible set's interior.","Additionally, for a simplified scenario involving two regions, which would correspond to a city area with a downtown and a suburban region, we also provide a method to check for the equilibria on the feasible set's boundary.","Finally, through a numerical case study, we illustrate the impact of charging prices on the position of the Nash equilibrium."],"url":"http://arxiv.org/abs/2403.16755v1","category":"eess.SY"}
{"created":"2024-03-25 13:34:13","title":"Radial Acceleration Relation of HI-rich Low Surface Brightness Galaxies","abstract":"We investigate the radial acceleration relation (RAR) in low surface brightness galaxies selected from the Arecibo Legacy Fast ALFA survey. We find that the dynamical acceleration $g_{\\rm obs}$ and baryonic gravitational acceleration $g_{\\rm bar}$ of the HI-rich low surface brightness galaxies still follow the universal RAR of typical late-type galaxies. The universal RAR signifies a consistent correlation between the distribution of baryonic matter and dark matter across galaxies with diverse morphologies and properties. Our findings suggest that the matter distributions in low surface brightness galaxies may indeed resemble that of general late-type galaxies. This implies that low surface brightness galaxies may not originate from dark matter halos with lower densities; instead, they may originate from the dark matter halos with high spins or form through feedback processes.","sentences":["We investigate the radial acceleration relation (RAR) in low surface brightness galaxies selected from the Arecibo Legacy Fast ALFA survey.","We find that the dynamical acceleration $g_{\\rm obs}$ and baryonic gravitational acceleration $g_{\\rm bar}$ of the HI-rich low surface brightness galaxies still follow the universal RAR of typical late-type galaxies.","The universal RAR signifies a consistent correlation between the distribution of baryonic matter and dark matter across galaxies with diverse morphologies and properties.","Our findings suggest that the matter distributions in low surface brightness galaxies may indeed resemble that of general late-type galaxies.","This implies that low surface brightness galaxies may not originate from dark matter halos with lower densities; instead, they may originate from the dark matter halos with high spins or form through feedback processes."],"url":"http://arxiv.org/abs/2403.16754v1","category":"astro-ph.GA"}
{"created":"2024-03-25 13:27:11","title":"Recent advances on CO2-assisted synthesis of metal nanoparticles for the upgrading of biomass-derived compounds","abstract":"Nanostructured catalysts have attracted the increased attention for biomass conversion into high-valued chemicals due to the rapid depletion of fossil resources and increasingly severe environmental issues. Supercritical carbon dioxide (scCO2) fluid is an attractive medium for synthesizing nanostructured materials due to its favorable properties. In this review, the properties of scCO2 and the roles of scCO2 in the fabrication of metal nanoparticles were assessed in detailed. A general overview of the synthesis of different types of metal nanoparticles (including metal oxide nanoparticles) using scCO2 and the relationship between the structure of the obtained metal nanoparticles and the preparation conditions such as reaction temperature and pressure, types of metal precursors, and deposition time are system summarized and compared in tables. Besides, compared to the meatal catalysts using the conventional methods, the catalysts obtained using scCO2 exhibited excellent catalytic performance on biomass conversion reactions, mainly focused on oxidation, hydrogenation reactions. Finally, opportunities and challenges of metal nanoparticle preparation using scCO2 for biomass valorization to chemicals and liquid fuels are highlighted. This review could be helpful for the rational design of more efficient metal catalysts for the selective synthesis of fine chemicals and fuels from biomass-derived chemicals.","sentences":["Nanostructured catalysts have attracted the increased attention for biomass conversion into high-valued chemicals due to the rapid depletion of fossil resources and increasingly severe environmental issues.","Supercritical carbon dioxide (scCO2) fluid is an attractive medium for synthesizing nanostructured materials due to its favorable properties.","In this review, the properties of scCO2 and the roles of scCO2 in the fabrication of metal nanoparticles were assessed in detailed.","A general overview of the synthesis of different types of metal nanoparticles (including metal oxide nanoparticles) using scCO2 and the relationship between the structure of the obtained metal nanoparticles and the preparation conditions such as reaction temperature and pressure, types of metal precursors, and deposition time are system summarized and compared in tables.","Besides, compared to the meatal catalysts using the conventional methods, the catalysts obtained using scCO2 exhibited excellent catalytic performance on biomass conversion reactions, mainly focused on oxidation, hydrogenation reactions.","Finally, opportunities and challenges of metal nanoparticle preparation using scCO2 for biomass valorization to chemicals and liquid fuels are highlighted.","This review could be helpful for the rational design of more efficient metal catalysts for the selective synthesis of fine chemicals and fuels from biomass-derived chemicals."],"url":"http://arxiv.org/abs/2403.16751v1","category":"physics.chem-ph"}
{"created":"2024-03-25 13:23:24","title":"All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification","abstract":"Modern hardware designs have grown increasingly efficient and complex. However, they are often susceptible to Common Weakness Enumerations (CWEs). This paper is focused on the formal verification of CWEs in a dataset of hardware designs written in SystemVerilog from Regenerative Artificial Intelligence (AI) powered by Large Language Models (LLMs). We applied formal verification to categorize each hardware design as vulnerable or CWE-free. This dataset was generated by 4 different LLMs and features a unique set of designs for each of the 10 CWEs we target in our paper. We have associated the identified vulnerabilities with CWE numbers for a dataset of 60,000 generated SystemVerilog Register Transfer Level (RTL) code. It was also found that most LLMs are not aware of any hardware CWEs; hence they are usually not considered when generating the hardware code. Our study reveals that approximately 60% of the hardware designs generated by LLMs are prone to CWEs, posing potential safety and security risks. The dataset could be ideal for training LLMs and Machine Learning (ML) algorithms to abstain from generating CWE-prone hardware designs.","sentences":["Modern hardware designs have grown increasingly efficient and complex.","However, they are often susceptible to Common Weakness Enumerations (CWEs).","This paper is focused on the formal verification of CWEs in a dataset of hardware designs written in SystemVerilog from Regenerative Artificial Intelligence (AI) powered by Large Language Models (LLMs).","We applied formal verification to categorize each hardware design as vulnerable or CWE-free.","This dataset was generated by 4 different LLMs and features a unique set of designs for each of the 10 CWEs we target in our paper.","We have associated the identified vulnerabilities with CWE numbers for a dataset of 60,000 generated SystemVerilog Register Transfer Level (RTL) code.","It was also found that most LLMs are not aware of any hardware CWEs; hence they are usually not considered when generating the hardware code.","Our study reveals that approximately 60% of the hardware designs generated by LLMs are prone to CWEs, posing potential safety and security risks.","The dataset could be ideal for training LLMs and Machine Learning (ML) algorithms to abstain from generating CWE-prone hardware designs."],"url":"http://arxiv.org/abs/2403.16750v1","category":"cs.AI"}
{"created":"2024-03-25 13:20:32","title":"K-moduli of Fano threefolds and genus four curves","abstract":"In this article, we study the K-moduli space of Fano threefolds obtained by blowing up $\\mathbb{P}^3$ along $(2,3)$-complete intersection curves. This K-moduli space is a two-step birational modification of the GIT moduli space of $(3,3)$-curves on $\\mathbb{P}^1 \\times \\mathbb{P}^1$. As an application, we show that our K-moduli space appears as one model of the Hassett--Keel program for $\\overline{M}_4$. In particular, we classify all K-(semi/poly)stable members in this deformation family of Fano varieties. We follow the moduli continuity method with moduli of lattice-polarized K3 surfaces, general elephants and Sarkisov links as new ingredients.","sentences":["In this article, we study the K-moduli space of Fano threefolds obtained by blowing up $\\mathbb{P}^3$ along $(2,3)$-complete intersection curves.","This K-moduli space is a two-step birational modification of the GIT moduli space of $(3,3)$-curves on $\\mathbb{P}^1 \\times \\mathbb{P}^1$. As an application, we show that our K-moduli space appears as one model of the Hassett--Keel program for $\\overline{M}_4$. In particular, we classify all K-(semi/poly)stable members in this deformation family of Fano varieties.","We follow the moduli continuity method with moduli of lattice-polarized K3 surfaces, general elephants and Sarkisov links as new ingredients."],"url":"http://arxiv.org/abs/2403.16747v1","category":"math.AG"}
{"created":"2024-03-25 13:20:28","title":"Polarisation properties of X-ray emission from accreting supermassive black holes","abstract":"This dissertation elaborates on X-ray polarisation features of astrophysical environments near accreting black holes. Although the work was originally assigned to supermassive black holes in active galactic nuclei, the results are also largely applicable to stellar-mass black holes in X-ray binary systems. Several numerical models predicting the X-ray polarisation from these sources are presented, including their immediate applications in the interpretation of the latest discoveries achieved thanks to the Imaging X-ray Polarimetry Explorer (IXPE) mission that began operating in December 2021. The modeling ranges from radiative transfer effects in atmospheres of accretion discs to general-relativistic signatures of X-rays travelling in vacuum near the central black holes to reprocessing events in distant, circumnuclear components. Various scales in physical and computational complexity are examined. A unifying element of this dissertation is the focus on reflection of X-rays from partially ionized matter.","sentences":["This dissertation elaborates on X-ray polarisation features of astrophysical environments near accreting black holes.","Although the work was originally assigned to supermassive black holes in active galactic nuclei, the results are also largely applicable to stellar-mass black holes in X-ray binary systems.","Several numerical models predicting the X-ray polarisation from these sources are presented, including their immediate applications in the interpretation of the latest discoveries achieved thanks to the Imaging X-ray Polarimetry Explorer (IXPE) mission that began operating in December 2021.","The modeling ranges from radiative transfer effects in atmospheres of accretion discs to general-relativistic signatures of X-rays travelling in vacuum near the central black holes to reprocessing events in distant, circumnuclear components.","Various scales in physical and computational complexity are examined.","A unifying element of this dissertation is the focus on reflection of X-rays from partially ionized matter."],"url":"http://arxiv.org/abs/2403.16746v1","category":"astro-ph.HE"}
{"created":"2024-03-25 13:15:00","title":"Influence of primary hair and plasma on intensity distribution of black hole shadows","abstract":"In this paper, we investigate the influence of primary hair ($l$) on the shadows of hairy Schwarzschild and Reissner-Nordstr\\\"om black holes obtained through gravitational decoupling. In the context of hairy Schwarzschild black holes, $l$ either has no effect or consistently enlarges the photon sphere radius. Notably, even when it violates the strong energy condition, it can decrease the radius. For Reissner-Nordstr\\\"om black holes, an additional matter field consistently expands the photon sphere radius, potentially reaching $3M$, akin to the pure Schwarzschild case. Remarkably, we demonstrate that black holes can exist even when overcharged ($Q^2 > M^2$), casting shadows. Specific intensity calculations reveal $l$ consistently reduces it in both scenarios. Furthermore, we investigate the impact of pressureless plasma, finding $l$ exerts a stronger influence on visible size than plasma. These results can help in our understanding of theoretical models of black hole shadows and can be tested by comparison with the images obtained by EHT collaboration.","sentences":["In this paper, we investigate the influence of primary hair ($l$) on the shadows of hairy Schwarzschild and Reissner-Nordstr\\\"om black holes obtained through gravitational decoupling.","In the context of hairy Schwarzschild black holes, $l$ either has no effect or consistently enlarges the photon sphere radius.","Notably, even when it violates the strong energy condition, it can decrease the radius.","For Reissner-Nordstr\\\"om black holes, an additional matter field consistently expands the photon sphere radius, potentially reaching $3M$, akin to the pure Schwarzschild case.","Remarkably, we demonstrate that black holes can exist even when overcharged ($Q^2 > M^2$), casting shadows.","Specific intensity calculations reveal $l$ consistently reduces it in both scenarios.","Furthermore, we investigate the impact of pressureless plasma, finding $l$ exerts a stronger influence on visible size than plasma.","These results can help in our understanding of theoretical models of black hole shadows and can be tested by comparison with the images obtained by EHT collaboration."],"url":"http://arxiv.org/abs/2403.16743v1","category":"gr-qc"}
{"created":"2024-03-25 13:13:39","title":"A Branch and Bound method for the exact parameter identification of the PK/PD model for anesthetic drugs","abstract":"We address the problem of parameter identification for the standard pharmacokinetic/pharmacodynamic (PK/PD) model for anesthetic drugs. Our main contribution is the development of a global optimization method that guarantees finding the parameters that minimize the one-step ahead prediction error. The method is based on a branch-and-bound algorithm, that can be applied to solve a more general class of nonlinear regression problems. We present some simulation results, based on a dataset of twelve patients. In these simulations, we are always able to identify the exact parameters, despite the non-convexity of the overall identification problem.","sentences":["We address the problem of parameter identification for the standard pharmacokinetic/pharmacodynamic (PK/PD) model for anesthetic drugs.","Our main contribution is the development of a global optimization method that guarantees finding the parameters that minimize the one-step ahead prediction error.","The method is based on a branch-and-bound algorithm, that can be applied to solve a more general class of nonlinear regression problems.","We present some simulation results, based on a dataset of twelve patients.","In these simulations, we are always able to identify the exact parameters, despite the non-convexity of the overall identification problem."],"url":"http://arxiv.org/abs/2403.16742v1","category":"eess.SY"}
{"created":"2024-03-25 13:12:50","title":"Spherically symmetric configurations in the quadratic $f(R)$ gravity","abstract":"We study spherically symmetric configurations of the quadratic $f(R)$ gravity in the Einstein frame. We found the global qualitative behavior of the metric and the scalaron field for all static solutions satisfying the conditions of asymptotic flatness. These solutions are proved to be regular everywhere except for a naked singularity at the center; they are uniquely determined by the total mass $\\mathfrak{M}$ and the ``scalar charge'' $Q$ characterizing the strength of the scalaron field at spatial infinity. The case $Q=0$ yields the Schwarzschild solution, but an arbitrarily small $Q\\ne 0$ leads to the appearance of a central naked singularity having a significant effect on the neighboring region, even though the space-time metric in the outer region is practically insensitive to the scalaron field. Approximation procedures are developed to derive asymptotic relations near the naked singularity and at spatial infinity, and the leading terms of the solutions are presented. We investigate the linear stability of the static solutions with respect to radial perturbations and numerically estimate the range of parameters corresponding to stable/unstable configurations.","sentences":["We study spherically symmetric configurations of the quadratic $f(R)$ gravity in the Einstein frame.","We found the global qualitative behavior of the metric and the scalaron field for all static solutions satisfying the conditions of asymptotic flatness.","These solutions are proved to be regular everywhere except for a naked singularity at the center; they are uniquely determined by the total mass $\\mathfrak{M}$ and the ``scalar charge'' $Q$ characterizing the strength of the scalaron field at spatial infinity.","The case $Q=0$ yields the Schwarzschild solution, but an arbitrarily small $Q\\ne 0$","leads to the appearance of a central naked singularity having a significant effect on the neighboring region, even though the space-time metric in the outer region is practically insensitive to the scalaron field.","Approximation procedures are developed to derive asymptotic relations near the naked singularity and at spatial infinity, and the leading terms of the solutions are presented.","We investigate the linear stability of the static solutions with respect to radial perturbations and numerically estimate the range of parameters corresponding to stable/unstable configurations."],"url":"http://arxiv.org/abs/2403.16741v1","category":"gr-qc"}
{"created":"2024-03-25 13:09:53","title":"A data-based comparison of methods for reducing the peak volume flow rate in a district heating system","abstract":"This work concerns reduction of the peak flow rate of a district heating grid, a key system property which is bounded by pipe dimensions and pumping capacity. The peak flow rate constrains the number of additional consumers that can be connected, and may be a limiting factor in reducing supply temperatures when transitioning to the 4th generation of district heating. We evaluate a full year of operational data from a subset of customer meters in a district heating system in Germany. We consider the peak flow rate reduction that could be achieved with full a posteriori knowledge of this data. Three strategies for reducing the peak flow rate are investigated: A load shifting demand response strategy, an upper limitation in substation return temperatures, and an upper limitation on each substation's volume flow rate. We show that imposing up to to 18 % load flexibility for the customers provides an equal reduction in the peak system flow rate under the load shifting strategy. The limited return temperature strategy is less efficient at curtailing the peak flow rate, but provides an overall reduction of volume flow rates. Finally, the flow rate limitation method can introduce new, higher flow rate peaks, reducing performance.","sentences":["This work concerns reduction of the peak flow rate of a district heating grid, a key system property which is bounded by pipe dimensions and pumping capacity.","The peak flow rate constrains the number of additional consumers that can be connected, and may be a limiting factor in reducing supply temperatures when transitioning to the 4th generation of district heating.","We evaluate a full year of operational data from a subset of customer meters in a district heating system in Germany.","We consider the peak flow rate reduction that could be achieved with full a posteriori knowledge of this data.","Three strategies for reducing the peak flow rate are investigated: A load shifting demand response strategy, an upper limitation in substation return temperatures, and an upper limitation on each substation's volume flow rate.","We show that imposing up to to 18 % load flexibility for the customers provides an equal reduction in the peak system flow rate under the load shifting strategy.","The limited return temperature strategy is less efficient at curtailing the peak flow rate, but provides an overall reduction of volume flow rates.","Finally, the flow rate limitation method can introduce new, higher flow rate peaks, reducing performance."],"url":"http://arxiv.org/abs/2403.16738v1","category":"eess.SY"}
{"created":"2024-03-25 13:09:40","title":"Creating a Digital Twin of Spinal Surgery: A Proof of Concept","abstract":"Surgery digitalization is the process of creating a virtual replica of real-world surgery, also referred to as a surgical digital twin (SDT). It has significant applications in various fields such as education and training, surgical planning, and automation of surgical tasks. Given their detailed representations of surgical procedures, SDTs are an ideal foundation for machine learning methods, enabling automatic generation of training data. In robotic surgery, SDTs can provide realistic virtual environments in which robots may learn through trial and error. In this paper, we present a proof of concept (PoC) for surgery digitalization that is applied to an ex-vivo spinal surgery performed in realistic conditions. The proposed digitalization focuses on the acquisition and modelling of the geometry and appearance of the entire surgical scene. We employ five RGB-D cameras for dynamic 3D reconstruction of the surgeon, a high-end camera for 3D reconstruction of the anatomy, an infrared stereo camera for surgical instrument tracking, and a laser scanner for 3D reconstruction of the operating room and data fusion. We justify the proposed methodology, discuss the challenges faced and further extensions of our prototype. While our PoC partially relies on manual data curation, its high quality and great potential motivate the development of automated methods for the creation of SDTs. The quality of our SDT can be assessed in a rendered video available at https://youtu.be/LqVaWGgaTMY .","sentences":["Surgery digitalization is the process of creating a virtual replica of real-world surgery, also referred to as a surgical digital twin (SDT).","It has significant applications in various fields such as education and training, surgical planning, and automation of surgical tasks.","Given their detailed representations of surgical procedures, SDTs are an ideal foundation for machine learning methods, enabling automatic generation of training data.","In robotic surgery, SDTs can provide realistic virtual environments in which robots may learn through trial and error.","In this paper, we present a proof of concept (PoC) for surgery digitalization that is applied to an ex-vivo spinal surgery performed in realistic conditions.","The proposed digitalization focuses on the acquisition and modelling of the geometry and appearance of the entire surgical scene.","We employ five RGB-D cameras for dynamic 3D reconstruction of the surgeon, a high-end camera for 3D reconstruction of the anatomy, an infrared stereo camera for surgical instrument tracking, and a laser scanner for 3D reconstruction of the operating room and data fusion.","We justify the proposed methodology, discuss the challenges faced and further extensions of our prototype.","While our PoC partially relies on manual data curation, its high quality and great potential motivate the development of automated methods for the creation of SDTs.","The quality of our SDT can be assessed in a rendered video available at https://youtu.be/LqVaWGgaTMY ."],"url":"http://arxiv.org/abs/2403.16736v1","category":"cs.CV"}
{"created":"2024-03-25 13:09:23","title":"New congruences for Partitions where the Even Parts are Distinct","abstract":"We denote the number of partitions of $n$ wherein the even parts are distinct (and the odd parts are unrestricted) by $ped(n)$. In this paper, we will use generating function manipulations to obtain new congruences for $ped(n)$ modulo $24$.","sentences":["We denote the number of partitions of $n$ wherein the even parts are distinct (and the odd parts are unrestricted) by $ped(n)$. In this paper, we will use generating function manipulations to obtain new congruences for $ped(n)$ modulo $24$."],"url":"http://arxiv.org/abs/2403.16735v1","category":"math.NT"}
{"created":"2024-03-25 13:06:31","title":"Enabling Uncertainty Estimation in Iterative Neural Networks","abstract":"Turning pass-through network architectures into iterative ones, which use their own output as input, is a well-known approach for boosting performance. In this paper, we argue that such architectures offer an additional benefit: The convergence rate of their successive outputs is highly correlated with the accuracy of the value to which they converge. Thus, we can use the convergence rate as a useful proxy for uncertainty. This results in an approach to uncertainty estimation that provides state-of-the-art estimates at a much lower computational cost than techniques like Ensembles, and without requiring any modifications to the original iterative model. We demonstrate its practical value by embedding it in two application domains: road detection in aerial images and the estimation of aerodynamic properties of 2D and 3D shapes.","sentences":["Turning pass-through network architectures into iterative ones, which use their own output as input, is a well-known approach for boosting performance.","In this paper, we argue that such architectures offer an additional benefit: The convergence rate of their successive outputs is highly correlated with the accuracy of the value to which they converge.","Thus, we can use the convergence rate as a useful proxy for uncertainty.","This results in an approach to uncertainty estimation that provides state-of-the-art estimates at a much lower computational cost than techniques like Ensembles, and without requiring any modifications to the original iterative model.","We demonstrate its practical value by embedding it in two application domains: road detection in aerial images and the estimation of aerodynamic properties of 2D and 3D shapes."],"url":"http://arxiv.org/abs/2403.16732v1","category":"cs.AI"}
{"created":"2024-03-25 13:05:39","title":"A generalization of Boole's formula derived from a system of linear equations","abstract":"We analyze a system of linear algebraic equations whose solutions lead to a proof of a generalization of Boole's formula. In particular, our approach provides an elementary and short alternative to Katsuura's proof of this generalization.","sentences":["We analyze a system of linear algebraic equations whose solutions lead to a proof of a generalization of Boole's formula.","In particular, our approach provides an elementary and short alternative to Katsuura's proof of this generalization."],"url":"http://arxiv.org/abs/2403.16731v1","category":"math.CO"}
{"created":"2024-03-25 13:02:43","title":"Improving Diffusion Models's Data-Corruption Resistance using Scheduled Pseudo-Huber Loss","abstract":"Diffusion models are known to be vulnerable to outliers in training data. In this paper we study an alternative diffusion loss function, which can preserve the high quality of generated data like the original squared $L_{2}$ loss while at the same time being robust to outliers. We propose to use pseudo-Huber loss function with a time-dependent parameter to allow for the trade-off between robustness on the most vulnerable early reverse-diffusion steps and fine details restoration on the final steps. We show that pseudo-Huber loss with the time-dependent parameter exhibits better performance on corrupted datasets in both image and audio domains. In addition, the loss function we propose can potentially help diffusion models to resist dataset corruption while not requiring data filtering or purification compared to conventional training algorithms.","sentences":["Diffusion models are known to be vulnerable to outliers in training data.","In this paper we study an alternative diffusion loss function, which can preserve the high quality of generated data like the original squared $L_{2}$ loss while at the same time being robust to outliers.","We propose to use pseudo-Huber loss function with a time-dependent parameter to allow for the trade-off between robustness on the most vulnerable early reverse-diffusion steps and fine details restoration on the final steps.","We show that pseudo-Huber loss with the time-dependent parameter exhibits better performance on corrupted datasets in both image and audio domains.","In addition, the loss function we propose can potentially help diffusion models to resist dataset corruption while not requiring data filtering or purification compared to conventional training algorithms."],"url":"http://arxiv.org/abs/2403.16728v1","category":"cs.AI"}
{"created":"2024-03-25 13:01:18","title":"Skyrmionic device for three dimensional magnetic field sensing enabled by spin-orbit torques","abstract":"Magnetic skyrmions are topologically protected local magnetic solitons that are promising for storage, logic or general computing applications. In this work, we demonstrate that we can use a skyrmion device based on [W/CoFeB/MgO] 1 0 multilayers for three-dimensional magnetic field sensing enabled by spin-orbit torques (SOT). We stabilize isolated chiral skyrmions and stripe domains in the multilayers, as shown by magnetic force microscopy images and micromagnetic simulations. We perform magnetic transport measurements to show that we can sense both in-plane and out-of-plane magnetic fields by means of a differential measurement scheme in which the symmetry of the SOT leads to cancelation of the DC offset. With the magnetic parameters obtained by vibrating sample magnetometry and ferromagnetic resonance measurements, we perform finite-temperature micromagnetic simulations, where we investigate the fundamental origin of the sensing signal. We identify the topological transformation between skyrmions, stripes and type-II bubbles that leads to a change in the resistance that is read-out by the anomalous Hall effect. Our study presents a novel application for skyrmions, where a differential measurement sensing concept is applied to quantify external magnetic fields paving the way towards more energy efficient applications in skyrmionics based spintronics.","sentences":["Magnetic skyrmions are topologically protected local magnetic solitons that are promising for storage, logic or general computing applications.","In this work, we demonstrate that we can use a skyrmion device based on [W/CoFeB/MgO] 1 0 multilayers for three-dimensional magnetic field sensing enabled by spin-orbit torques (SOT).","We stabilize isolated chiral skyrmions and stripe domains in the multilayers, as shown by magnetic force microscopy images and micromagnetic simulations.","We perform magnetic transport measurements to show that we can sense both in-plane and out-of-plane magnetic fields by means of a differential measurement scheme in which the symmetry of the SOT leads to cancelation of the DC offset.","With the magnetic parameters obtained by vibrating sample magnetometry and ferromagnetic resonance measurements, we perform finite-temperature micromagnetic simulations, where we investigate the fundamental origin of the sensing signal.","We identify the topological transformation between skyrmions, stripes and type-II bubbles that leads to a change in the resistance that is read-out by the anomalous Hall effect.","Our study presents a novel application for skyrmions, where a differential measurement sensing concept is applied to quantify external magnetic fields paving the way towards more energy efficient applications in skyrmionics based spintronics."],"url":"http://arxiv.org/abs/2403.16725v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-25 13:41:33","title":"LiteBIRD Science Goals and Forecasts: Primordial Magnetic Fields","abstract":"We present detailed forecasts for the constraints on primordial magnetic fields (PMFs) that will be obtained with the LiteBIRD satellite. The constraints are driven by the effects of PMFs on the CMB anisotropies: the gravitational effects of magnetically-induced perturbations; the effects on the thermal and ionization history of the Universe; the Faraday rotation imprint on the CMB polarization; and the non-Gaussianities induced in polarization anisotropies. LiteBIRD represents a sensitive probe for PMFs and by exploiting all the physical effects, it will be able to improve the current limit coming from Planck. In particular, thanks to its accurate $B$-mode polarization measurement, LiteBIRD will improve the constraints on infrared configurations for the gravitational effect, giving $B_{\\rm 1\\,Mpc}^{n_{\\rm B} =-2.9} < 0.8$ nG at 95% C.L., potentially opening the possibility to detect nanogauss fields with high significance. We also observe a significant improvement in the limits when marginalized over the spectral index, $B_{1\\,{\\rm Mpc}}^{\\rm marg}< 2.2$ nG at 95% C.L. From the thermal history effect, which relies mainly on $E$-mode polarization data, we obtain a significant improvement for all PMF configurations, with the marginalized case, $\\sqrt{\\langle B^2\\rangle}^{\\rm marg}<0.50$ nG at 95% C.L. Faraday rotation constraints will take advantage of the wide frequency coverage of LiteBIRD and the high sensitivity in $B$ modes, improving the limits by orders of magnitude with respect to current results, $B_{1\\,{\\rm Mpc}}^{n_{\\rm B} =-2.9} < 3.2$ nG at 95% C.L. Finally, non-Gaussianities of the $B$-mode polarization can probe PMFs at the level of 1 nG, again significantly improving the current bounds from Planck. Altogether our forecasts represent a broad collection of complementary probes, providing conservative limits on PMF characteristics that will be achieved with LiteBIRD.","sentences":["We present detailed forecasts for the constraints on primordial magnetic fields (PMFs) that will be obtained with the LiteBIRD satellite.","The constraints are driven by the effects of PMFs on the CMB anisotropies: the gravitational effects of magnetically-induced perturbations; the effects on the thermal and ionization history of the Universe; the Faraday rotation imprint on the CMB polarization; and the non-Gaussianities induced in polarization anisotropies.","LiteBIRD represents a sensitive probe for PMFs and by exploiting all the physical effects, it will be able to improve the current limit coming from Planck.","In particular, thanks to its accurate $B$-mode polarization measurement, LiteBIRD will improve the constraints on infrared configurations for the gravitational effect, giving $B_{\\rm 1\\,Mpc}^{n_{\\rm B} =-2.9} < 0.8$ nG at 95% C.L., potentially opening the possibility to detect nanogauss fields with high significance.","We also observe a significant improvement in the limits when marginalized over the spectral index, $B_{1\\,{\\rm Mpc}}^{\\rm marg}< 2.2$ nG at 95% C.L. From the thermal history effect, which relies mainly on $E$-mode polarization data, we obtain a significant improvement for all PMF configurations, with the marginalized case, $\\sqrt{\\langle B^2\\rangle}^{\\rm marg}<0.50$ nG at 95% C.L. Faraday rotation constraints will take advantage of the wide frequency coverage of LiteBIRD and the high sensitivity in $B$ modes, improving the limits by orders of magnitude with respect to current results, $B_{1\\,{\\rm Mpc}}^{n_{\\rm B} =-2.9} < 3.2$ nG at 95%","C.L. Finally, non-Gaussianities of the $B$-mode polarization can probe PMFs at the level of 1 nG, again significantly improving the current bounds from Planck.","Altogether our forecasts represent a broad collection of complementary probes, providing conservative limits on PMF characteristics that will be achieved with LiteBIRD."],"url":"http://arxiv.org/abs/2403.16763v1","category":"astro-ph.CO"}
{"created":"2024-03-25 14:32:40","title":"Holographic Gaussian Boson Sampling with Matrix Product States on 3D cQED Processors","abstract":"We introduce quantum circuits for simulations of multi-mode state-vectors on 3D cQED processors, using matrix product state representations. The circuits are demonstrated as applied to simulations of molecular docking based on holographic Gaussian boson sampling, as illustrated for binding of a thiol-containing aryl sulfonamide ligand to the tumor necrosis factor-$\\alpha$ converting enzyme receptor. We show that cQED devices with a modest number of modes could be employed to simulate multimode systems by re-purposing working modes through measurement and re-initialization. We anticipate a wide range of GBS applications could be implemented on compact 3D cQED processors analogously, using the holographic approach. Simulations on qubit-based quantum computers could be implemented analogously, using circuits that represent continuous variables in terms of truncated expansions of Fock states.","sentences":["We introduce quantum circuits for simulations of multi-mode state-vectors on 3D cQED processors, using matrix product state representations.","The circuits are demonstrated as applied to simulations of molecular docking based on holographic Gaussian boson sampling, as illustrated for binding of a thiol-containing aryl sulfonamide ligand to the tumor necrosis factor-$\\alpha$ converting enzyme receptor.","We show that cQED devices with a modest number of modes could be employed to simulate multimode systems by re-purposing working modes through measurement and re-initialization.","We anticipate a wide range of GBS applications could be implemented on compact 3D cQED processors analogously, using the holographic approach.","Simulations on qubit-based quantum computers could be implemented analogously, using circuits that represent continuous variables in terms of truncated expansions of Fock states."],"url":"http://arxiv.org/abs/2403.16810v1","category":"quant-ph"}
{"created":"2024-03-25 14:23:43","title":"Two-Dimensional Frequency-Difference-of-Arrival Varieties","abstract":"This paper studies Frequency-Difference-of-Arrival (FDOA) curves for the 2-dimensional, 2-sensor case. The primary focus of this paper is to give a description of curves associated to the FDOA problem from the algebro-geometric point of view. To be more precise, the complex projective picture of the family of FDOA curves for all possible relative velocities is described.","sentences":["This paper studies Frequency-Difference-of-Arrival (FDOA) curves for the 2-dimensional, 2-sensor case.","The primary focus of this paper is to give a description of curves associated to the FDOA problem from the algebro-geometric point of view.","To be more precise, the complex projective picture of the family of FDOA curves for all possible relative velocities is described."],"url":"http://arxiv.org/abs/2403.16805v1","category":"eess.SP"}
{"created":"2024-03-25 14:23:03","title":"TEI2GO: A Multilingual Approach for Fast Temporal Expression Identification","abstract":"Temporal expression identification is crucial for understanding texts written in natural language. Although highly effective systems such as HeidelTime exist, their limited runtime performance hampers adoption in large-scale applications and production environments. In this paper, we introduce the TEI2GO models, matching HeidelTime's effectiveness but with significantly improved runtime, supporting six languages, and achieving state-of-the-art results in four of them. To train the TEI2GO models, we used a combination of manually annotated reference corpus and developed ``Professor HeidelTime'', a comprehensive weakly labeled corpus of news texts annotated with HeidelTime. This corpus comprises a total of $138,069$ documents (over six languages) with $1,050,921$ temporal expressions, the largest open-source annotated dataset for temporal expression identification to date. By describing how the models were produced, we aim to encourage the research community to further explore, refine, and extend the set of models to additional languages and domains. Code, annotations, and models are openly available for community exploration and use. The models are conveniently on HuggingFace for seamless integration and application.","sentences":["Temporal expression identification is crucial for understanding texts written in natural language.","Although highly effective systems such as HeidelTime exist, their limited runtime performance hampers adoption in large-scale applications and production environments.","In this paper, we introduce the TEI2GO models, matching HeidelTime's effectiveness but with significantly improved runtime, supporting six languages, and achieving state-of-the-art results in four of them.","To train the TEI2GO models, we used a combination of manually annotated reference corpus and developed ``Professor HeidelTime'', a comprehensive weakly labeled corpus of news texts annotated with HeidelTime.","This corpus comprises a total of $138,069$ documents (over six languages) with $1,050,921$ temporal expressions, the largest open-source annotated dataset for temporal expression identification to date.","By describing how the models were produced, we aim to encourage the research community to further explore, refine, and extend the set of models to additional languages and domains.","Code, annotations, and models are openly available for community exploration and use.","The models are conveniently on HuggingFace for seamless integration and application."],"url":"http://arxiv.org/abs/2403.16804v1","category":"cs.CL"}
{"created":"2024-03-25 14:21:30","title":"Unveiling extended gamma-ray emission around HESS J1813-178","abstract":"HESS J1813$-$178 is a very-high-energy $\\gamma$-ray source spatially coincident with the young and energetic pulsar PSR J1813$-$1749 and thought to be associated with its pulsar wind nebula (PWN). Recently, evidence for extended high-energy emission in the vicinity of the pulsar has been revealed in the Fermi Large Area Telescope (LAT) data. This motivates revisiting the HESS J1813$-$178 region, taking advantage of improved analysis methods and an extended data set. Using data taken by the High Energy Stereoscopic System (H.E.S.S.) experiment and the Fermi-LAT, we aim to describe the $\\gamma$-ray emission in the region with a consistent model, to provide insights into its origin. We performed a likelihood-based analysis on 32 hours of H.E.S.S. data and 12 years of Fermi-LAT data and fit a spectro-morphological model to the combined datasets. These results allowed us to develop a physical model for the origin of the observed $\\gamma$-ray emission in the region. In addition to the compact very-high-energy $\\gamma$-ray emission centered on the pulsar, we find a significant yet previously undetected component along the Galactic plane. With Fermi-LAT data, we confirm extended high-energy emission consistent with the position and elongation of the extended emission observed with H.E.S.S. These results establish a consistent description of the emission in the region from GeV energies to several tens of TeV. This study suggests that HESS J1813$-$178 is associated with a $\\gamma$-ray PWN powered by PSR J1813$-$1749. A possible origin of the extended emission component is inverse Compton emission from electrons and positrons that have escaped the confines of the pulsar and form a halo around the PWN.","sentences":["HESS J1813$-$178 is a very-high-energy $\\gamma$-ray source spatially coincident with the young and energetic pulsar PSR J1813$-$1749 and thought to be associated with its pulsar wind nebula (PWN).","Recently, evidence for extended high-energy emission in the vicinity of the pulsar has been revealed in the Fermi Large Area Telescope (LAT) data.","This motivates revisiting the HESS J1813$-$178 region, taking advantage of improved analysis methods and an extended data set.","Using data taken by the High Energy Stereoscopic System (H.E.S.S.) experiment and the Fermi-LAT, we aim to describe the $\\gamma$-ray emission in the region with a consistent model, to provide insights into its origin.","We performed a likelihood-based analysis on 32 hours of H.E.S.S. data and 12 years of Fermi-LAT data and fit a spectro-morphological model to the combined datasets.","These results allowed us to develop a physical model for the origin of the observed $\\gamma$-ray emission in the region.","In addition to the compact very-high-energy $\\gamma$-ray emission centered on the pulsar, we find a significant yet previously undetected component along the Galactic plane.","With Fermi-LAT data, we confirm extended high-energy emission consistent with the position and elongation of the extended emission observed with H.E.S.S.","These results establish a consistent description of the emission in the region from GeV energies to several tens of TeV. This study suggests that HESS J1813$-$178 is associated with a $\\gamma$-ray PWN powered by PSR J1813$-$1749.","A possible origin of the extended emission component is inverse Compton emission from electrons and positrons that have escaped the confines of the pulsar and form a halo around the PWN."],"url":"http://arxiv.org/abs/2403.16802v1","category":"astro-ph.HE"}
{"created":"2024-03-25 14:14:39","title":"Privacy Preservation by Intermittent Transmission in Cooperative LQG Control Systems","abstract":"In this paper, we study a cooperative linear quadratic Gaussian (LQG) control system with a single user and a server. In this system, the user runs a process and employs the server to meet the needs of computation. However, the user regards its state trajectories as privacy. Therefore, we propose a privacy scheme, in which the user sends data to the server intermittently. By this scheme, the server's received information of the user is reduced, and consequently the user's privacy is preserved. In this paper, we consider a periodic transmission scheme. We analyze the performance of privacy preservation and LQG control of different transmission periods. Under the given threshold of the control performance loss, a trade-off optimization problem is proposed. Finally, we give the solution to the optimization problem.","sentences":["In this paper, we study a cooperative linear quadratic Gaussian (LQG) control system with a single user and a server.","In this system, the user runs a process and employs the server to meet the needs of computation.","However, the user regards its state trajectories as privacy.","Therefore, we propose a privacy scheme, in which the user sends data to the server intermittently.","By this scheme, the server's received information of the user is reduced, and consequently the user's privacy is preserved.","In this paper, we consider a periodic transmission scheme.","We analyze the performance of privacy preservation and LQG control of different transmission periods.","Under the given threshold of the control performance loss, a trade-off optimization problem is proposed.","Finally, we give the solution to the optimization problem."],"url":"http://arxiv.org/abs/2403.16797v1","category":"eess.SY"}
{"created":"2024-03-25 14:13:43","title":"\"We Have No Idea How Models will Behave in Production until Production\": How Engineers Operationalize Machine Learning","abstract":"Organizations rely on machine learning engineers (MLEs) to deploy models and maintain ML pipelines in production. Due to models' extensive reliance on fresh data, the operationalization of machine learning, or MLOps, requires MLEs to have proficiency in data science and engineering. When considered holistically, the job seems staggering -- how do MLEs do MLOps, and what are their unaddressed challenges? To address these questions, we conducted semi-structured ethnographic interviews with 18 MLEs working on various applications, including chatbots, autonomous vehicles, and finance. We find that MLEs engage in a workflow of (i) data preparation, (ii) experimentation, (iii) evaluation throughout a multi-staged deployment, and (iv) continual monitoring and response. Throughout this workflow, MLEs collaborate extensively with data scientists, product stakeholders, and one another, supplementing routine verbal exchanges with communication tools ranging from Slack to organization-wide ticketing and reporting systems. We introduce the 3Vs of MLOps: velocity, visibility, and versioning -- three virtues of successful ML deployments that MLEs learn to balance and grow as they mature. Finally, we discuss design implications and opportunities for future work.","sentences":["Organizations rely on machine learning engineers (MLEs) to deploy models and maintain ML pipelines in production.","Due to models' extensive reliance on fresh data, the operationalization of machine learning, or MLOps, requires MLEs to have proficiency in data science and engineering.","When considered holistically, the job seems staggering -- how do MLEs do MLOps, and what are their unaddressed challenges?","To address these questions, we conducted semi-structured ethnographic interviews with 18 MLEs working on various applications, including chatbots, autonomous vehicles, and finance.","We find that MLEs engage in a workflow of (i) data preparation, (ii) experimentation, (iii) evaluation throughout a multi-staged deployment, and (iv) continual monitoring and response.","Throughout this workflow, MLEs collaborate extensively with data scientists, product stakeholders, and one another, supplementing routine verbal exchanges with communication tools ranging from Slack to organization-wide ticketing and reporting systems.","We introduce the 3Vs of MLOps: velocity, visibility, and versioning -- three virtues of successful ML deployments that MLEs learn to balance and grow as they mature.","Finally, we discuss design implications and opportunities for future work."],"url":"http://arxiv.org/abs/2403.16795v1","category":"cs.HC"}
{"created":"2024-03-25 13:55:49","title":"Visual Action Planning with Multiple Heterogeneous Agents","abstract":"Visual planning methods are promising to handle complex settings where extracting the system state is challenging. However, none of the existing works tackles the case of multiple heterogeneous agents which are characterized by different capabilities and/or embodiment. In this work, we propose a method to realize visual action planning in multi-agent settings by exploiting a roadmap built in a low-dimensional structured latent space and used for planning. To enable multi-agent settings, we infer possible parallel actions from a dataset composed of tuples associated with individual actions. Next, we evaluate feasibility and cost of them based on the capabilities of the multi-agent system and endow the roadmap with this information, building a capability latent space roadmap (C-LSR). Additionally, a capability suggestion strategy is designed to inform the human operator about possible missing capabilities when no paths are found. The approach is validated in a simulated burger cooking task and a real-world box packing task.","sentences":["Visual planning methods are promising to handle complex settings where extracting the system state is challenging.","However, none of the existing works tackles the case of multiple heterogeneous agents which are characterized by different capabilities and/or embodiment.","In this work, we propose a method to realize visual action planning in multi-agent settings by exploiting a roadmap built in a low-dimensional structured latent space and used for planning.","To enable multi-agent settings, we infer possible parallel actions from a dataset composed of tuples associated with individual actions.","Next, we evaluate feasibility and cost of them based on the capabilities of the multi-agent system and endow the roadmap with this information, building a capability latent space roadmap (C-LSR).","Additionally, a capability suggestion strategy is designed to inform the human operator about possible missing capabilities when no paths are found.","The approach is validated in a simulated burger cooking task and a real-world box packing task."],"url":"http://arxiv.org/abs/2403.16781v1","category":"cs.RO"}
{"created":"2024-03-25 13:54:44","title":"GloSIS: The Global Soil Information System Web Ontology","abstract":"Established in 2012 by members of the Food and Agriculture Organisation (FAO), the Global Soil Partnership (GSP) is a global network of stakeholders promoting sound land and soil management practices towards a sustainable world food system. However, soil survey largely remains a local or regional activity, bound to heterogeneous methods and conventions. Recognising the relevance of global and trans-national policies towards sustainable land management practices, the GSP elected data harmonisation and exchange as one of its key lines of action. Building upon international standards and previous work towards a global soil data ontology, an improved domain model was eventually developed within the GSP [54], the basis for a Global Soil Information System (GloSIS). This work also identified the Semantic Web as a possible avenue to operationalise the domain model. This article presents the GloSIS web ontology, an implementation of the GloSIS domain model with the Web Ontology Language (OWL). Thoroughly employing a host of Semantic Web standards (SOSA, SKOS, GeoSPARQL, QUDT), GloSIS lays out not only a soil data ontology but also an extensive set of ready-to-use code-lists for soil description and physio-chemical analysis. Various examples are provided on the provision and use of GloSIS-compliant linked data, showcasing the contribution of this ontology to the discovery, exploration, integration and access of soil data.","sentences":["Established in 2012 by members of the Food and Agriculture Organisation (FAO), the Global Soil Partnership (GSP) is a global network of stakeholders promoting sound land and soil management practices towards a sustainable world food system.","However, soil survey largely remains a local or regional activity, bound to heterogeneous methods and conventions.","Recognising the relevance of global and trans-national policies towards sustainable land management practices, the GSP elected data harmonisation and exchange as one of its key lines of action.","Building upon international standards and previous work towards a global soil data ontology, an improved domain model was eventually developed within the GSP","[54], the basis for a Global Soil Information System (GloSIS).","This work also identified the Semantic Web as a possible avenue to operationalise the domain model.","This article presents the GloSIS web ontology, an implementation of the GloSIS domain model with the Web Ontology Language (OWL).","Thoroughly employing a host of Semantic Web standards (SOSA, SKOS, GeoSPARQL, QUDT), GloSIS lays out not only a soil data ontology but also an extensive set of ready-to-use code-lists for soil description and physio-chemical analysis.","Various examples are provided on the provision and use of GloSIS-compliant linked data, showcasing the contribution of this ontology to the discovery, exploration, integration and access of soil data."],"url":"http://arxiv.org/abs/2403.16778v1","category":"cs.IR"}
{"created":"2024-03-25 13:49:55","title":"Chemical homogenization for non-mixing reactive interfaces in porous media","abstract":"Porous media, while ubiquitous across many engineering disciplines, is inherently difficult to characterize due to their innate stochasticity and heterogeneity. The key for predicting porous material behavior comes down to the structuring of its microstructure, where the linkages of microstructural properties to mesoscale effects remain as one of the key questions in unlocking understanding of this class of materials. One proposed method of linking scales comes down to using Minkowski functionals -- geometric morphometers that describe the spatial and topological features of a convex space -- to draw connections from microstructural form to mesoscale features. In this work, chemical equilibrium and kinetics on a microstructure surface were explored, with Minkowski functionals used as the basis for relating microstructural geometry to chemical performance. Using surface CRNs to model chemical behavior -- a novel asynchronous cellular automaton -- linkages were found between the Minkowski functionals and equilibrium equilibrium constant, as well as properties related to the dynamics of the system's reaction quotient.","sentences":["Porous media, while ubiquitous across many engineering disciplines, is inherently difficult to characterize due to their innate stochasticity and heterogeneity.","The key for predicting porous material behavior comes down to the structuring of its microstructure, where the linkages of microstructural properties to mesoscale effects remain as one of the key questions in unlocking understanding of this class of materials.","One proposed method of linking scales comes down to using Minkowski functionals -- geometric morphometers that describe the spatial and topological features of a convex space -- to draw connections from microstructural form to mesoscale features.","In this work, chemical equilibrium and kinetics on a microstructure surface were explored, with Minkowski functionals used as the basis for relating microstructural geometry to chemical performance.","Using surface CRNs to model chemical behavior -- a novel asynchronous cellular automaton -- linkages were found between the Minkowski functionals and equilibrium equilibrium constant, as well as properties related to the dynamics of the system's reaction quotient."],"url":"http://arxiv.org/abs/2403.16770v1","category":"cond-mat.soft"}
{"created":"2024-03-25 13:46:18","title":"Dynamics of primitive elements under group actions","abstract":"We investigate group actions in which certain primitive elements fix a point, while not all group elements possess this property when acting upon some space. Using similar dynamical tools, we introduce the notion of Nielsen girth and prove the existence of groups with infinite girth but having finite Nielsen girth.","sentences":["We investigate group actions in which certain primitive elements fix a point, while not all group elements possess this property when acting upon some space.","Using similar dynamical tools, we introduce the notion of Nielsen girth and prove the existence of groups with infinite girth but having finite Nielsen girth."],"url":"http://arxiv.org/abs/2403.16769v1","category":"math.GR"}
{"created":"2024-03-25 13:45:55","title":"Policy Gradient-based Model Free Optimal LQG Control with a Probabilistic Risk Constraint","abstract":"In this paper, we investigate a model-free optimal control design that minimizes an infinite horizon average expected quadratic cost of states and control actions subject to a probabilistic risk or chance constraint using input-output data. In particular, we consider linear time-invariant systems and design an optimal controller within the class of linear state feedback control. Three different policy gradient (PG) based algorithms, natural policy gradient (NPG), Gauss-Newton policy gradient (GNPG), and deep deterministic policy gradient (DDPG), are developed, and compared with the optimal risk-neutral linear-quadratic regulator (LQR) and a scenario-based model predictive control (MPC) technique via numerical simulations. The convergence properties and the accuracy of all the algorithms are compared numerically. We also establish analytical convergence properties of the NPG and GNPG algorithms under the known model scenario, while the proof of convergence for the unknown model scenario is part of our ongoing work.","sentences":["In this paper, we investigate a model-free optimal control design that minimizes an infinite horizon average expected quadratic cost of states and control actions subject to a probabilistic risk or chance constraint using input-output data.","In particular, we consider linear time-invariant systems and design an optimal controller within the class of linear state feedback control.","Three different policy gradient (PG) based algorithms, natural policy gradient (NPG), Gauss-Newton policy gradient (GNPG), and deep deterministic policy gradient (DDPG), are developed, and compared with the optimal risk-neutral linear-quadratic regulator (LQR) and a scenario-based model predictive control (MPC) technique via numerical simulations.","The convergence properties and the accuracy of all the algorithms are compared numerically.","We also establish analytical convergence properties of the NPG and GNPG algorithms under the known model scenario, while the proof of convergence for the unknown model scenario is part of our ongoing work."],"url":"http://arxiv.org/abs/2403.16767v1","category":"eess.SY"}
{"created":"2024-03-25 13:42:13","title":"The stability of the multivariate geometric Brownian motion as a bilinear matrix inequality problem","abstract":"In this manuscript, we study the stability of the origin for the multivariate geometric Brownian motion. More precisely, under suitable sufficient conditions, we construct a Lyapunov function such that the origin of the multivariate geometric Brownian motion is globally asymptotically stable in probability. Moreover, we show that such conditions can be rewritten as a Bilinear Matrix Inequality (BMI) feasibility problem. We stress that no commutativity relations between the drift matrix and the noise dispersion matrices are assumed and therefore the so-called Magnus representation of the solution of the multivariate geometric Brownian motion is complicated. In addition, we exemplify our method in numerous specific models from the literature such as random linear oscillators, satellite dynamics, inertia systems, diagonal noise systems, cancer self-remission and smoking.","sentences":["In this manuscript, we study the stability of the origin for the multivariate geometric Brownian motion.","More precisely, under suitable sufficient conditions, we construct a Lyapunov function such that the origin of the multivariate geometric Brownian motion is globally asymptotically stable in probability.","Moreover, we show that such conditions can be rewritten as a Bilinear Matrix Inequality (BMI) feasibility problem.","We stress that no commutativity relations between the drift matrix and the noise dispersion matrices are assumed and therefore the so-called Magnus representation of the solution of the multivariate geometric Brownian motion is complicated.","In addition, we exemplify our method in numerous specific models from the literature such as random linear oscillators, satellite dynamics, inertia systems, diagonal noise systems, cancer self-remission and smoking."],"url":"http://arxiv.org/abs/2403.16765v1","category":"math.PR"}
{"created":"2024-03-25 13:41:57","title":"Low-Cost Teleoperation with Haptic Feedback through Vision-based Tactile Sensors for Rigid and Soft Object Manipulation","abstract":"Haptic feedback is essential for humans to successfully perform complex and delicate manipulation tasks. A recent rise in tactile sensors has enabled robots to leverage the sense of touch and expand their capability drastically. However, many tasks still need human intervention/guidance. For this reason, we present a teleoperation framework designed to provide haptic feedback to human operators based on the data from camera-based tactile sensors mounted on the robot gripper. Partial autonomy is introduced to prevent slippage of grasped objects during task execution. Notably, we rely exclusively on low-cost off-the-shelf hardware to realize an affordable solution. We demonstrate the versatility of the framework on nine different objects ranging from rigid to soft and fragile ones, using three different operators on real hardware.","sentences":["Haptic feedback is essential for humans to successfully perform complex and delicate manipulation tasks.","A recent rise in tactile sensors has enabled robots to leverage the sense of touch and expand their capability drastically.","However, many tasks still need human intervention/guidance.","For this reason, we present a teleoperation framework designed to provide haptic feedback to human operators based on the data from camera-based tactile sensors mounted on the robot gripper.","Partial autonomy is introduced to prevent slippage of grasped objects during task execution.","Notably, we rely exclusively on low-cost off-the-shelf hardware to realize an affordable solution.","We demonstrate the versatility of the framework on nine different objects ranging from rigid to soft and fragile ones, using three different operators on real hardware."],"url":"http://arxiv.org/abs/2403.16764v1","category":"cs.RO"}
{"created":"2024-03-25 13:34:55","title":"RKFNet: A Novel Neural Network Aided Robust Kalman Filter","abstract":"Driven by the filtering challenges in linear systems disturbed by non-Gaussian heavy-tailed noise, the robust Kalman filters (RKFs) leveraging diverse heavy-tailed distributions have been introduced. However, the RKFs rely on precise noise models, and large model errors can degrade their filtering performance. Also, the posterior approximation by the employed variational Bayesian (VB) method can further decrease the estimation precision. Here, we introduce an innovative RKF method, the RKFNet, which combines the heavy-tailed-distribution-based RKF framework with the deep learning (DL) technique and eliminates the need for the precise parameters of the heavy-tailed distributions. To reduce the VB approximation error, the mixing-parameter-based function and the scale matrix are estimated by the incorporated neural network structures. Also, the stable training process is achieved by our proposed unsupervised scheduled sampling (USS) method, where a loss function based on the Student's t (ST) distribution is utilised to overcome the disturbance of the noise outliers and the filtering results of the traditional RKFs are employed as reference sequences. Furthermore, the RKFNet is evaluated against various RKFs and recurrent neural networks (RNNs) under three kinds of heavy-tailed measurement noises, and the simulation results showcase its efficacy in terms of estimation accuracy and efficiency.","sentences":["Driven by the filtering challenges in linear systems disturbed by non-Gaussian heavy-tailed noise, the robust Kalman filters (RKFs) leveraging diverse heavy-tailed distributions have been introduced.","However, the RKFs rely on precise noise models, and large model errors can degrade their filtering performance.","Also, the posterior approximation by the employed variational Bayesian (VB) method can further decrease the estimation precision.","Here, we introduce an innovative RKF method, the RKFNet, which combines the heavy-tailed-distribution-based RKF framework with the deep learning (DL) technique and eliminates the need for the precise parameters of the heavy-tailed distributions.","To reduce the VB approximation error, the mixing-parameter-based function and the scale matrix are estimated by the incorporated neural network structures.","Also, the stable training process is achieved by our proposed unsupervised scheduled sampling (USS) method, where a loss function based on the Student's t (ST) distribution is utilised to overcome the disturbance of the noise outliers and the filtering results of the traditional RKFs are employed as reference sequences.","Furthermore, the RKFNet is evaluated against various RKFs and recurrent neural networks (RNNs) under three kinds of heavy-tailed measurement noises, and the simulation results showcase its efficacy in terms of estimation accuracy and efficiency."],"url":"http://arxiv.org/abs/2403.16756v1","category":"eess.SP"}
{"created":"2024-03-25 13:28:23","title":"Modeling the secular evolution of embedded protoplanetary discs","abstract":"Context: Protoplanetary discs are known to form around nascent stars from their parent molecular cloud as a result of angular momentum conservation. As they progressively evolve and dissipate, they also form planets. While a lot of modeling efforts have been dedicated to their formation, the question of their secular evolution, from the so-called class 0 embedded phase to the class II phase where discs are believed to be isolated, remains poorly understood. Aims: We aim to explore the evolution between the embedded stages and the class II stage. We focus on the magnetic field evolution and the long-term interaction between the disc and the envelope. Methods: We use the GPU-accelerated code \\textsc{Idefix} to perform a 3D, barotropic, non-ideal magnetohydrodynamic (MHD) secular core collapse simulation that covers the system evolution from the collapse of the pre-stellar core until 100 kyr after the first hydrostatic core formation and the disc settling while ensuring sufficient vertical and azimuthal resolutions (down to $10^{-2}$ au) to properly resolve the disc internal dynamics and non-axisymmetric perturbations. Results: The disc evolution leads to a power-law gas surface density in Keplerian rotation that extends up to a few 10 au. The magnetic flux trapped in the disc during the initial collapse decreases from 100 mG at disc formation down to 1 mG by the end of the simulation. After the formation of the first hydrostatic core, the system evolves in three phases. A first phase with a small ($\\sim 10$ au), unstable, strongly accreting ($\\sim10^{-5}$ $\\mathrm{M_\\odot \\, yr^{-1}}$) disc that loses magnetic flux over the first 15 kyr, a second phase where the magnetic flux is advected with a smooth, expanding disc fed by the angular momentum of the infalling material...","sentences":["Context: Protoplanetary discs are known to form around nascent stars from their parent molecular cloud as a result of angular momentum conservation.","As they progressively evolve and dissipate, they also form planets.","While a lot of modeling efforts have been dedicated to their formation, the question of their secular evolution, from the so-called class 0 embedded phase to the class II phase where discs are believed to be isolated, remains poorly understood.","Aims:","We aim to explore the evolution between the embedded stages and the class II stage.","We focus on the magnetic field evolution and the long-term interaction between the disc and the envelope.","Methods: We use the GPU-accelerated code \\textsc{Idefix} to perform a 3D, barotropic, non-ideal magnetohydrodynamic (MHD) secular core collapse simulation that covers the system evolution from the collapse of the pre-stellar core until 100 kyr after the first hydrostatic core formation and the disc settling while ensuring sufficient vertical and azimuthal resolutions (down to $10^{-2}$ au) to properly resolve the disc internal dynamics and non-axisymmetric perturbations.","Results:","The disc evolution leads to a power-law gas surface density in Keplerian rotation that extends up to a few 10 au.","The magnetic flux trapped in the disc during the initial collapse decreases from 100 mG at disc formation down to 1 mG by the end of the simulation.","After the formation of the first hydrostatic core, the system evolves in three phases.","A first phase with a small ($\\sim 10$ au), unstable, strongly accreting ($\\sim10^{-5}$ $\\mathrm{M_\\odot \\, yr^{-1}}$) disc that loses magnetic flux over the first 15 kyr, a second phase where the magnetic flux is advected with a smooth, expanding disc fed by the angular momentum of the infalling material..."],"url":"http://arxiv.org/abs/2403.16753v1","category":"astro-ph.SR"}
{"created":"2024-03-25 13:12:39","title":"Looking back and forward: A retrospective and future directions on Software Engineering for systems-of-systems","abstract":"Modern systems are increasingly connected and more integrated with other existing systems, giving rise to systems-of-systems (SoS). An SoS consists of a set of independent, heterogeneous systems that interact to provide new functionalities and accomplish global missions through emergent behavior manifested at runtime. The distinctive characteristics of SoS, when contrasted to traditional systems, pose significant research challenges within Software Engineering. These challenges motivate the need for a paradigm shift and the exploration of novel approaches for designing, developing, deploying, and evolving these systems. The International Workshop on Software Engineering for Systems-of-Systems (SESoS) series started in 2013 to fill a gap in scientific forums addressing SoS from the Software Engineering perspective, becoming the first venue for this purpose. This article presents a study aimed at outlining the evolution and future trajectory of Software Engineering for SoS based on the examination of 57 papers spanning the 11 editions of the SESoS workshop (2013-2023). The study combined scoping review and scientometric analysis methods to categorize and analyze the research contributions concerning temporal and geographic distribution, topics of interest, research methodologies employed, application domains, and research impact. Based on such a comprehensive overview, this article discusses current and future directions in Software Engineering for SoS.","sentences":["Modern systems are increasingly connected and more integrated with other existing systems, giving rise to systems-of-systems (SoS).","An SoS consists of a set of independent, heterogeneous systems that interact to provide new functionalities and accomplish global missions through emergent behavior manifested at runtime.","The distinctive characteristics of SoS, when contrasted to traditional systems, pose significant research challenges within Software Engineering.","These challenges motivate the need for a paradigm shift and the exploration of novel approaches for designing, developing, deploying, and evolving these systems.","The International Workshop on Software Engineering for Systems-of-Systems (SESoS) series started in 2013 to fill a gap in scientific forums addressing SoS from the Software Engineering perspective, becoming the first venue for this purpose.","This article presents a study aimed at outlining the evolution and future trajectory of Software Engineering for SoS based on the examination of 57 papers spanning the 11 editions of the SESoS workshop (2013-2023).","The study combined scoping review and scientometric analysis methods to categorize and analyze the research contributions concerning temporal and geographic distribution, topics of interest, research methodologies employed, application domains, and research impact.","Based on such a comprehensive overview, this article discusses current and future directions in Software Engineering for SoS."],"url":"http://arxiv.org/abs/2403.16740v1","category":"cs.SE"}
{"created":"2024-03-25 13:12:00","title":"Emergent strength-dependent scale-free mobility edge in a non-reciprocal long-range Aubry-Andr\u00e9-Harper model","abstract":"We investigate the properties of mobility edge in an Aubry-Andr\\'e-Harper model with non-reciprocal long-range hopping. The results reveal that there can be a new type of mobility edge featuring both strength-dependent and scale-free properties. By calculating the fractal dimension, we find that the positions of mobility edges are robust to the strength of non-reciprocal long-range hopping. Furthermore, through scale analysis of the observables such as fractal dimension, eigenenergy and eigenstate, etc., we prove that four different specific mobility edges can be observed in the system. This paper extends the family tree of mobility edges and hopefully it will shed more light on the related theory and experiment.","sentences":["We investigate the properties of mobility edge in an Aubry-Andr\\'e-Harper model with non-reciprocal long-range hopping.","The results reveal that there can be a new type of mobility edge featuring both strength-dependent and scale-free properties.","By calculating the fractal dimension, we find that the positions of mobility edges are robust to the strength of non-reciprocal long-range hopping.","Furthermore, through scale analysis of the observables such as fractal dimension, eigenenergy and eigenstate, etc., we prove that four different specific mobility edges can be observed in the system.","This paper extends the family tree of mobility edges and hopefully it will shed more light on the related theory and experiment."],"url":"http://arxiv.org/abs/2403.16739v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-25 13:09:44","title":"Integrating Port-Hamiltonian Systems with Neural Networks: From Deterministic to Stochastic Frameworks","abstract":"This article presents an innovative approach to integrating port-Hamiltonian systems with neural network architectures, transitioning from deterministic to stochastic models. The study presents novel mathematical formulations and computational models that extend the understanding of dynamical systems under uncertainty and complex interactions. It emphasizes the significant progress in learning and predicting the dynamics of non-autonomous systems using port-Hamiltonian neural networks (pHNNs). It also explores the implications of stochastic neural networks in various dynamical systems.","sentences":["This article presents an innovative approach to integrating port-Hamiltonian systems with neural network architectures, transitioning from deterministic to stochastic models.","The study presents novel mathematical formulations and computational models that extend the understanding of dynamical systems under uncertainty and complex interactions.","It emphasizes the significant progress in learning and predicting the dynamics of non-autonomous systems using port-Hamiltonian neural networks (pHNNs).","It also explores the implications of stochastic neural networks in various dynamical systems."],"url":"http://arxiv.org/abs/2403.16737v1","category":"math.DS"}
{"created":"2024-03-25 13:06:34","title":"Direct activation of PMS by highly dispersed amorphous CoOx clusters in anatase TiO2 nanosheets for efficient oxidation of biomass-derived alcohols","abstract":"Developing a green and cost-effective catalytic system for the selective oxidation of biomass-derived alcohols is vital for the sustainable synthesis of fine chemicals. Herein, highly dispersed subnanometric amorphous CoOx clusters in anatase TiO2 nanosheets (Co-TiO2) fabricated by green solvent CO2 assisted approach could directly activate peroxymonosulfate (PMS) for the highly selective oxidation of various biomass-derived alcohols. Advanced characterizations (e.g., EXAFS, EPR, AC HAADF-STEM) reveal that a strong interaction of CoOx clusters and the anatase TiO2 support exist in Co-TiO2 and Co atom in Co-TiO2 is mainly consisted of Co2+ and Co3+. The Co-TiO2 catalyst offers superior catalytic performance in the conversion of six types of alcohols (e.g., benzyl alcohol (BAL), 5-hydroxymethylfurfural (HMF)) with high selectivity to produce corresponding aldehydes. Highly dispersed CoOx clusters and the interaction between CoOx clusters and TiO2 support contribute to the superior performance. Mechanism studies show that SO4 radicals play the dominant role in the selective oxidation of model reactant BAL and 1O2 participates in the non-radical pathway. DFT calculations are well matched with experiment and decipher that the strong interaction between CoOx clusters and TiO2 support promotes the formation of SO4 and SO5.","sentences":["Developing a green and cost-effective catalytic system for the selective oxidation of biomass-derived alcohols is vital for the sustainable synthesis of fine chemicals.","Herein, highly dispersed subnanometric amorphous CoOx clusters in anatase TiO2 nanosheets (Co-TiO2) fabricated by green solvent CO2 assisted approach could directly activate peroxymonosulfate (PMS) for the highly selective oxidation of various biomass-derived alcohols.","Advanced characterizations (e.g., EXAFS, EPR, AC HAADF-STEM) reveal that a strong interaction of CoOx clusters and the anatase TiO2 support exist in Co-TiO2 and Co atom in Co-TiO2 is mainly consisted of Co2+ and Co3+.","The Co-TiO2 catalyst offers superior catalytic performance in the conversion of six types of alcohols (e.g., benzyl alcohol (BAL), 5-hydroxymethylfurfural (HMF)) with high selectivity to produce corresponding aldehydes.","Highly dispersed CoOx clusters and the interaction between CoOx clusters and TiO2 support contribute to the superior performance.","Mechanism studies show that SO4 radicals play the dominant role in the selective oxidation of model reactant BAL and 1O2 participates in the non-radical pathway.","DFT calculations are well matched with experiment and decipher that the strong interaction between CoOx clusters and TiO2 support promotes the formation of SO4 and SO5."],"url":"http://arxiv.org/abs/2403.16733v1","category":"physics.chem-ph"}
{"created":"2024-03-25 13:04:20","title":"A Robotic Skill Learning System Built Upon Diffusion Policies and Foundation Models","abstract":"In this paper, we build upon two major recent developments in the field, Diffusion Policies for visuomotor manipulation and large pre-trained multimodal foundational models to obtain a robotic skill learning system. The system can obtain new skills via the behavioral cloning approach of visuomotor diffusion policies given teleoperated demonstrations. Foundational models are being used to perform skill selection given the user's prompt in natural language. Before executing a skill the foundational model performs a precondition check given an observation of the workspace. We compare the performance of different foundational models to this end as well as give a detailed experimental evaluation of the skills taught by the user in simulation and the real world. Finally, we showcase the combined system on a challenging food serving scenario in the real world. Videos of all experimental executions, as well as the process of teaching new skills in simulation and the real world, are available on the project's website.","sentences":["In this paper, we build upon two major recent developments in the field, Diffusion Policies for visuomotor manipulation and large pre-trained multimodal foundational models to obtain a robotic skill learning system.","The system can obtain new skills via the behavioral cloning approach of visuomotor diffusion policies given teleoperated demonstrations.","Foundational models are being used to perform skill selection given the user's prompt in natural language.","Before executing a skill the foundational model performs a precondition check given an observation of the workspace.","We compare the performance of different foundational models to this end as well as give a detailed experimental evaluation of the skills taught by the user in simulation and the real world.","Finally, we showcase the combined system on a challenging food serving scenario in the real world.","Videos of all experimental executions, as well as the process of teaching new skills in simulation and the real world, are available on the project's website."],"url":"http://arxiv.org/abs/2403.16730v1","category":"cs.RO"}
{"created":"2024-03-25 13:01:29","title":"SIS epidemics on open networks: A replacement-based approximation","abstract":"In this paper we analyze continuous-time SIS epidemics subject to arrivals and departures of agents, by using an approximated process based on replacements. In defining the SIS dynamics in an open network, we consider a stochastic setting in which arrivals and departures take place according to Poisson processes with similar rates, and the new value of the infection probability of an arriving agent is drawn from a continuous distribution. Since the system size changes with time, we define an approximated process, in which replacements take place instead of arrivals and departures, and we focus on the evolution of an aggregate measure of the level of infection. So long as the reproduction number is less than one, the long-term behavior of this function measures the impact of the changes of the set of agents in the epidemic. We derive upper bounds for the expectation and variance of this function and we include a numerical example to show that the approximated process is close to the original SIS process.","sentences":["In this paper we analyze continuous-time SIS epidemics subject to arrivals and departures of agents, by using an approximated process based on replacements.","In defining the SIS dynamics in an open network, we consider a stochastic setting in which arrivals and departures take place according to Poisson processes with similar rates, and the new value of the infection probability of an arriving agent is drawn from a continuous distribution.","Since the system size changes with time, we define an approximated process, in which replacements take place instead of arrivals and departures, and we focus on the evolution of an aggregate measure of the level of infection.","So long as the reproduction number is less than one, the long-term behavior of this function measures the impact of the changes of the set of agents in the epidemic.","We derive upper bounds for the expectation and variance of this function and we include a numerical example to show that the approximated process is close to the original SIS process."],"url":"http://arxiv.org/abs/2403.16727v1","category":"eess.SY"}
{"created":"2024-03-25 13:51:22","title":"Privacy-Protected Spatial Autoregressive Model","abstract":"Spatial autoregressive (SAR) models are important tools for studying network effects. However, with an increasing emphasis on data privacy, data providers often implement privacy protection measures that make classical SAR models inapplicable. In this study, we introduce a privacy-protected SAR model with noise-added response and covariates to meet privacy-protection requirements. However, in this scenario, the traditional quasi-maximum likelihood estimator becomes infeasible because the likelihood function cannot be formulated. To address this issue, we first consider an explicit expression for the likelihood function with only noise-added responses. However, the derivatives are biased owing to the noise in the covariates. Therefore, we develop techniques that can correct the biases introduced by noise. Correspondingly, a Newton-Raphson-type algorithm is proposed to obtain the estimator, leading to a corrected likelihood estimator. To further enhance computational efficiency, we introduce a corrected least squares estimator based on the idea of bias correction. These two estimation methods ensure both data security and the attainment of statistically valid estimators. Theoretical analysis of both estimators is carefully conducted, and statistical inference methods are discussed. The finite sample performances of different methods are demonstrated through extensive simulations and the analysis of a real dataset.","sentences":["Spatial autoregressive (SAR) models are important tools for studying network effects.","However, with an increasing emphasis on data privacy, data providers often implement privacy protection measures that make classical SAR models inapplicable.","In this study, we introduce a privacy-protected SAR model with noise-added response and covariates to meet privacy-protection requirements.","However, in this scenario, the traditional quasi-maximum likelihood estimator becomes infeasible because the likelihood function cannot be formulated.","To address this issue, we first consider an explicit expression for the likelihood function with only noise-added responses.","However, the derivatives are biased owing to the noise in the covariates.","Therefore, we develop techniques that can correct the biases introduced by noise.","Correspondingly, a Newton-Raphson-type algorithm is proposed to obtain the estimator, leading to a corrected likelihood estimator.","To further enhance computational efficiency, we introduce a corrected least squares estimator based on the idea of bias correction.","These two estimation methods ensure both data security and the attainment of statistically valid estimators.","Theoretical analysis of both estimators is carefully conducted, and statistical inference methods are discussed.","The finite sample performances of different methods are demonstrated through extensive simulations and the analysis of a real dataset."],"url":"http://arxiv.org/abs/2403.16773v1","category":"stat.ME"}
{"created":"2024-03-25 13:15:11","title":"Dimerizing hard spherocylinders in porous media","abstract":"This research focuses on the unique phase behavior of non-spherical patchy colloids in porous environments. Based on the theory of scaled particle (SPT), methods have been refined and applied to analyze the thermodynamic properties of non-spherical patchy particles in a disordered porous medium. Utilizing the associative theory of liquids in conjunction with SPT, we investigated the impact of associative interactions and connections between the functional nodes of particles on the formation of the nematic phase. Calculations of orientational and spatial distributions were conducted, which helped to understand the phase behavior of particles during the transition from isotropic to nematic phase under the spatial constraints imposed by the disordered matrix of the porous medium.","sentences":["This research focuses on the unique phase behavior of non-spherical patchy colloids in porous environments.","Based on the theory of scaled particle (SPT), methods have been refined and applied to analyze the thermodynamic properties of non-spherical patchy particles in a disordered porous medium.","Utilizing the associative theory of liquids in conjunction with SPT, we investigated the impact of associative interactions and connections between the functional nodes of particles on the formation of the nematic phase.","Calculations of orientational and spatial distributions were conducted, which helped to understand the phase behavior of particles during the transition from isotropic to nematic phase under the spatial constraints imposed by the disordered matrix of the porous medium."],"url":"http://arxiv.org/abs/2403.16744v1","category":"cond-mat.soft"}
{"created":"2024-03-25 14:02:22","title":"Local search and trajectory metaheuristics for the flexible job shop scheduling problem with sequencing flexibility and position-based learning effect","abstract":"The flexible job shop scheduling problem with sequencing flexibility and position-based learning effect is considered in the present work. In [K. A. G. Araujo, E. G. Birgin, and D. P. Ronconi, Technical Report MCDO02022024, 2024], models, constructive heuristics, and benchmark instances for the same problem were introduced. In the present work, we are concerned with the development of effective and efficient methods for its resolution. For this purpose, a local search method and four trajectory metaheuristics are considered. In the local search, we show that the classical strategy of only reallocating operations that are part of the critical path can miss better quality neighbors, as opposed to what happens in the case where there is no learning effect. Consequently, we analyze an alternative type of neighborhood reduction that eliminates only neighbors that are not better than the current solution. In addition, we also suggest a neighborhood cut and experimentally verify that this significantly reduces the neighborhood size, bringing efficiency, with minimal loss in effectiveness. Extensive numerical experiments with the local search and the metaheuristics are carried on. The experiments show that tabu search, built on the reduced neighborhood, when applied to large-sized instances, stands out in relation to other the other three metaheuristics, namely, iterated local search, greedy randomized adaptive search procedure, and simulating annealing. Experiments with classical instances without sequencing flexibility show that the introduced methods also stand out in relation to methods from the literature. All the methods introduced, as well as the instances and solutions found, are freely available. As a whole, we build a test suite that can be used in future work.","sentences":["The flexible job shop scheduling problem with sequencing flexibility and position-based learning effect is considered in the present work.","In [K. A. G. Araujo, E. G. Birgin, and D. P. Ronconi, Technical Report MCDO02022024, 2024], models, constructive heuristics, and benchmark instances for the same problem were introduced.","In the present work, we are concerned with the development of effective and efficient methods for its resolution.","For this purpose, a local search method and four trajectory metaheuristics are considered.","In the local search, we show that the classical strategy of only reallocating operations that are part of the critical path can miss better quality neighbors, as opposed to what happens in the case where there is no learning effect.","Consequently, we analyze an alternative type of neighborhood reduction that eliminates only neighbors that are not better than the current solution.","In addition, we also suggest a neighborhood cut and experimentally verify that this significantly reduces the neighborhood size, bringing efficiency, with minimal loss in effectiveness.","Extensive numerical experiments with the local search and the metaheuristics are carried on.","The experiments show that tabu search, built on the reduced neighborhood, when applied to large-sized instances, stands out in relation to other the other three metaheuristics, namely, iterated local search, greedy randomized adaptive search procedure, and simulating annealing.","Experiments with classical instances without sequencing flexibility show that the introduced methods also stand out in relation to methods from the literature.","All the methods introduced, as well as the instances and solutions found, are freely available.","As a whole, we build a test suite that can be used in future work."],"url":"http://arxiv.org/abs/2403.16787v1","category":"math.OC"}
{"created":"2024-03-25 14:20:36","title":"Non-unique stationary solutions of even active scalar equations","abstract":"We study a class of active scalar equations with even non-local operator in the drift term. Non-trivial stationary weak solutions in the space $C^{0-}$ are constructed using the iterative convex integration approach.","sentences":["We study a class of active scalar equations with even non-local operator in the drift term.","Non-trivial stationary weak solutions in the space $C^{0-}$ are constructed using the iterative convex integration approach."],"url":"http://arxiv.org/abs/2403.16800v1","category":"math.AP"}
{"created":"2024-03-25 14:00:07","title":"Approximating maps into manifolds with lower curvature bounds","abstract":"Many interesting functions arising in applications map into Riemannian manifolds. We present an algorithm, using the manifold exponential and logarithm, for approximating such functions. Our approach extends approximation techniques for functions into linear spaces so that we can upper bound the forward error in terms of a lower bound on the manifold's sectional curvature. Furthermore, when the sectional curvature of a manifold is nonnegative, such as for compact Lie groups, the error is guaranteed to not be worse than in the linear case. We implement the algorithm in a Julia package and apply it to two example problems from Krylov subspaces and dynamic low-rank approximation, respectively. For these examples, the maps are confirmed to be well approximated by our algorithm.","sentences":["Many interesting functions arising in applications map into Riemannian manifolds.","We present an algorithm, using the manifold exponential and logarithm, for approximating such functions.","Our approach extends approximation techniques for functions into linear spaces so that we can upper bound the forward error in terms of a lower bound on the manifold's sectional curvature.","Furthermore, when the sectional curvature of a manifold is nonnegative, such as for compact Lie groups, the error is guaranteed to not be worse than in the linear case.","We implement the algorithm in a Julia package and apply it to two example problems from Krylov subspaces and dynamic low-rank approximation, respectively.","For these examples, the maps are confirmed to be well approximated by our algorithm."],"url":"http://arxiv.org/abs/2403.16785v1","category":"math.NA"}
{"created":"2024-03-25 13:50:22","title":"The cubic nonlinear Schr\u00f6dinger equation with rough potential","abstract":"We consider the cubic nonlinear Schr\\\"odinger equation with a spatially rough potential, a key equation in the mathematical setup for nonlinear Anderson localization. Our study comprises two main parts: new optimal results on the well-posedness analysis on the PDE level, and subsequently a new efficient numerical method, its convergence analysis and simulations that illustrate our analytical results. In the analysis part, our results focus on understanding how the regularity of the solution is influenced by the regularity of the potential, where we provide quantitative and explicit characterizations. Ill-posedness results are also established to demonstrate the sharpness of the obtained regularity characterizations and to indicate the minimum regularity required from the potential for the NLS to be solvable. Building upon the obtained regularity results, we design an appropriate numerical discretization for the model and establish its convergence with an optimal error bound. The numerical experiments in the end not only verify the theoretical regularity results, but also confirm the established convergence rate of the proposed scheme. Additionally, a comparison with other existing schemes is conducted to demonstrate the better accuracy of our new scheme in the case of a rough potential.","sentences":["We consider the cubic nonlinear Schr\\\"odinger equation with a spatially rough potential, a key equation in the mathematical setup for nonlinear Anderson localization.","Our study comprises two main parts: new optimal results on the well-posedness analysis on the PDE level, and subsequently a new efficient numerical method, its convergence analysis and simulations that illustrate our analytical results.","In the analysis part, our results focus on understanding how the regularity of the solution is influenced by the regularity of the potential, where we provide quantitative and explicit characterizations.","Ill-posedness results are also established to demonstrate the sharpness of the obtained regularity characterizations and to indicate the minimum regularity required from the potential for the NLS to be solvable.","Building upon the obtained regularity results, we design an appropriate numerical discretization for the model and establish its convergence with an optimal error bound.","The numerical experiments in the end not only verify the theoretical regularity results, but also confirm the established convergence rate of the proposed scheme.","Additionally, a comparison with other existing schemes is conducted to demonstrate the better accuracy of our new scheme in the case of a rough potential."],"url":"http://arxiv.org/abs/2403.16772v1","category":"math.NA"}
{"created":"2024-03-25 13:07:19","title":"Anderson Acceleration Without Restart: A Novel Method with $n$-Step Super Quadratic Convergence Rate","abstract":"In this paper, we propose a novel Anderson's acceleration method to solve nonlinear equations, which does \\emph{not} require a restart strategy to achieve numerical stability. We propose the greedy and random versions of our algorithm. Specifically, the greedy version selects the direction to maximize a certain measure of progress for approximating the current Jacobian matrix. In contrast, the random version chooses the random Gaussian vector as the direction to update the approximate Jacobian. Furthermore, our algorithm, including both greedy and random versions, has an $n$-step super quadratic convergence rate, where $n$ is the dimension of the objective problem. For example, the explicit convergence rate of the random version can be presented as $ \\norm{\\vx_{k+n+1} - \\vx_*} / \\norm{\\vx_k- \\vx_*}^2 = \\cO\\left(\\left(1-\\frac{1}{n}\\right)^{kn}\\right)$ for any $k\\geq 0$ where $\\vx_*$ is the optimum of the objective problem. This kind of convergence rate is new to Anderson's acceleration and quasi-Newton methods. The experiments also validate the fast convergence rate of our algorithm.","sentences":["In this paper, we propose a novel Anderson's acceleration method to solve nonlinear equations, which does \\emph{not} require a restart strategy to achieve numerical stability.","We propose the greedy and random versions of our algorithm.","Specifically, the greedy version selects the direction to maximize a certain measure of progress for approximating the current Jacobian matrix.","In contrast, the random version chooses the random Gaussian vector as the direction to update the approximate Jacobian.","Furthermore, our algorithm, including both greedy and random versions, has an $n$-step super quadratic convergence rate, where $n$ is the dimension of the objective problem.","For example, the explicit convergence rate of the random version can be presented as $ \\norm{\\vx_{k+n+1} - \\vx_*} / \\norm{\\vx_k- \\vx_*}^2","= \\cO\\left(\\left(1-\\frac{1}{n}\\right)^{kn}\\right)$ for any $k\\geq 0$ where $\\vx_*$ is the optimum of the objective problem.","This kind of convergence rate is new to Anderson's acceleration and quasi-Newton methods.","The experiments also validate the fast convergence rate of our algorithm."],"url":"http://arxiv.org/abs/2403.16734v1","category":"math.OC"}
{"created":"2024-03-25 13:53:04","title":"Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?","abstract":"Multilingual pretraining and fine-tuning have remarkably succeeded in various natural language processing tasks. Transferring representations from one language to another is especially crucial for cross-lingual learning. One can expect machine translation objectives to be well suited to fostering such capabilities, as they involve the explicit alignment of semantically equivalent sentences from different languages. This paper investigates the potential benefits of employing machine translation as a continued training objective to enhance language representation learning, bridging multilingual pretraining and cross-lingual applications. We study this question through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their latent representations. Our results show that, contrary to expectations, machine translation as the continued training fails to enhance cross-lingual representation learning in multiple cross-lingual natural language understanding tasks. We conclude that explicit sentence-level alignment in the cross-lingual scenario is detrimental to cross-lingual transfer pretraining, which has important implications for future cross-lingual transfer studies. We furthermore provide evidence through similarity measures and investigation of parameters that this lack of positive influence is due to output separability -- which we argue is of use for machine translation but detrimental elsewhere.","sentences":["Multilingual pretraining and fine-tuning have remarkably succeeded in various natural language processing tasks.","Transferring representations from one language to another is especially crucial for cross-lingual learning.","One can expect machine translation objectives to be well suited to fostering such capabilities, as they involve the explicit alignment of semantically equivalent sentences from different languages.","This paper investigates the potential benefits of employing machine translation as a continued training objective to enhance language representation learning, bridging multilingual pretraining and cross-lingual applications.","We study this question through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their latent representations.","Our results show that, contrary to expectations, machine translation as the continued training fails to enhance cross-lingual representation learning in multiple cross-lingual natural language understanding tasks.","We conclude that explicit sentence-level alignment in the cross-lingual scenario is detrimental to cross-lingual transfer pretraining, which has important implications for future cross-lingual transfer studies.","We furthermore provide evidence through similarity measures and investigation of parameters that this lack of positive influence is due to output separability -- which we argue is of use for machine translation but detrimental elsewhere."],"url":"http://arxiv.org/abs/2403.16777v1","category":"cs.CL"}
{"created":"2024-03-25 13:42:47","title":"Models, constructive heuristics, and benchmark instances for the flexible job shop scheduling problem with sequencing flexibility and position-based learning effect","abstract":"This paper addresses the flexible job shop scheduling problem with sequencing flexibility and position-based learning effect. In this variant of the flexible job shop scheduling problem, precedence constraints of the operations constituting a job are given by an arbitrary directed acyclic graph, in opposition to the classical case in which a total order is imposed. Additionally, it is assumed that the processing time of an operation in a machine is subject to a learning process such that the larger the position of the operation in the machine, the faster the operation is processed. Mixed integer programming and constraint programming models are presented and compared in the present work. In addition, constructive heuristics are introduced to provide an initial solution to the models' solvers. Sets of benchmark instances are also introduced. The problem considered corresponds to modern problems of great relevance in the printing industry. The models and instances presented are intended to support the development of new heuristic and metaheuristics methods for this problem.","sentences":["This paper addresses the flexible job shop scheduling problem with sequencing flexibility and position-based learning effect.","In this variant of the flexible job shop scheduling problem, precedence constraints of the operations constituting a job are given by an arbitrary directed acyclic graph, in opposition to the classical case in which a total order is imposed.","Additionally, it is assumed that the processing time of an operation in a machine is subject to a learning process such that the larger the position of the operation in the machine, the faster the operation is processed.","Mixed integer programming and constraint programming models are presented and compared in the present work.","In addition, constructive heuristics are introduced to provide an initial solution to the models' solvers.","Sets of benchmark instances are also introduced.","The problem considered corresponds to modern problems of great relevance in the printing industry.","The models and instances presented are intended to support the development of new heuristic and metaheuristics methods for this problem."],"url":"http://arxiv.org/abs/2403.16766v1","category":"math.OC"}
{"created":"2024-03-25 13:20:59","title":"Enhancing Software Effort Estimation through Reinforcement Learning-based Project Management-Oriented Feature Selection","abstract":"Purpose: The study aims to investigate the application of the data element market in software project management, focusing on improving effort estimation by addressing challenges faced by traditional methods. Design/methodology/approach: This study proposes a solution based on feature selection, utilizing the data element market and reinforcement learning-based algorithms to enhance the accuracy of software effort estimation. It explores the application of the MARLFS algorithm, customizing improvements to the algorithm and reward function. Findings: This study demonstrates that the proposed approach achieves more precise estimation compared to traditional methods, leveraging feature selection to guide project management in software development. Originality/value: This study contributes to the field by offering a novel approach that combines the data element market, machine learning, and feature selection to improve software effort estimation, addressing limitations of traditional methods and providing insights for future research in project management.","sentences":["Purpose:","The study aims to investigate the application of the data element market in software project management, focusing on improving effort estimation by addressing challenges faced by traditional methods.","Design/methodology/approach: This study proposes a solution based on feature selection, utilizing the data element market and reinforcement learning-based algorithms to enhance the accuracy of software effort estimation.","It explores the application of the MARLFS algorithm, customizing improvements to the algorithm and reward function.","Findings:","This study demonstrates that the proposed approach achieves more precise estimation compared to traditional methods, leveraging feature selection to guide project management in software development.","Originality/value: This study contributes to the field by offering a novel approach that combines the data element market, machine learning, and feature selection to improve software effort estimation, addressing limitations of traditional methods and providing insights for future research in project management."],"url":"http://arxiv.org/abs/2403.16749v1","category":"cs.SE"}
{"created":"2024-03-25 13:03:35","title":"Quantum State Preparation for Probability Distributions with Mirror Symmetry Using Matrix Product States","abstract":"Quantum circuits for loading probability distributions into quantum states are essential subroutines in quantum algorithms used in physics, finance engineering, and machine learning. The ability to implement these with high accuracy in shallow quantum circuits is a critical issue. We propose a novel quantum state preparation method for probability distribution with mirror symmetry using matrix product states. By considering mirror symmetry, our method reduces the entanglement of probability distributions and improves the accuracy of approximations by matrix product states. As a result, we improved the accuracy by two orders of magnitude over existing methods using matrix product states. Our approach, characterized by a shallow quantum circuit primarily comprising nearest-neighbor qubit gates and linear scalability with qubit count, is highly advantageous for noisy quantum devices. Also, our experimental findings reveal that the approximation accuracy in tensor networks depends heavily on the bond dimension, with minimal reliance on the number of qubits. Our method is experimentally demonstrated for a normal distribution encoded into 10 and 20 qubits on a real quantum processor.","sentences":["Quantum circuits for loading probability distributions into quantum states are essential subroutines in quantum algorithms used in physics, finance engineering, and machine learning.","The ability to implement these with high accuracy in shallow quantum circuits is a critical issue.","We propose a novel quantum state preparation method for probability distribution with mirror symmetry using matrix product states.","By considering mirror symmetry, our method reduces the entanglement of probability distributions and improves the accuracy of approximations by matrix product states.","As a result, we improved the accuracy by two orders of magnitude over existing methods using matrix product states.","Our approach, characterized by a shallow quantum circuit primarily comprising nearest-neighbor qubit gates and linear scalability with qubit count, is highly advantageous for noisy quantum devices.","Also, our experimental findings reveal that the approximation accuracy in tensor networks depends heavily on the bond dimension, with minimal reliance on the number of qubits.","Our method is experimentally demonstrated for a normal distribution encoded into 10 and 20 qubits on a real quantum processor."],"url":"http://arxiv.org/abs/2403.16729v1","category":"quant-ph"}
{"created":"2024-03-25 14:19:48","title":"Efficient Method for Finding Optimal Strategies in Chopstick Auctions with Uniform Objects Values","abstract":"We propose an algorithm for computing Nash equilibria (NE) in a class of conflicts with multiple battlefields with uniform battlefield values and a non-linear aggregation function. By expanding the symmetrization idea of Hart [9], proposed for the Colonel Blotto game, to the wider class of symmetric conflicts with multiple battlefields, we reduce the number of strategies of the players by an exponential factor. We propose a clash matrix algorithm which allows for computing the payoffs in the symmetrized model in polynomial time. Combining symmetrization and clash matrix algorithm with the double oracle algorithm we obtain an algorithm for computing NE in the models in question that achieves a significant speed-up as compared to the standard, LP-based, approach. We also introduce a heuristic to further speed up the process. Overall, our approach offers an efficient and novel method for computing NE in a specific class of conflicts, with potential practical applications in various fields.","sentences":["We propose an algorithm for computing Nash equilibria (NE) in a class of conflicts with multiple battlefields with uniform battlefield values and a non-linear aggregation function.","By expanding the symmetrization idea of Hart [9], proposed for the Colonel Blotto game, to the wider class of symmetric conflicts with multiple battlefields, we reduce the number of strategies of the players by an exponential factor.","We propose a clash matrix algorithm which allows for computing the payoffs in the symmetrized model in polynomial time.","Combining symmetrization and clash matrix algorithm with the double oracle algorithm we obtain an algorithm for computing NE in the models in question that achieves a significant speed-up as compared to the standard, LP-based, approach.","We also introduce a heuristic to further speed up the process.","Overall, our approach offers an efficient and novel method for computing NE in a specific class of conflicts, with potential practical applications in various fields."],"url":"http://arxiv.org/abs/2403.16799v1","category":"cs.GT"}
{"created":"2024-03-25 13:55:20","title":"Integer Fluxonium Qubit","abstract":"We describe a superconducting qubit derived from operating a properly designed fluxonium circuit in a zero magnetic field. The qubit has a frequency of about 4 GHz and the energy relaxation quality factor $Q \\approx 0.7\\times 10^7$, even though the dielectric loss quality factor of the circuit components is in the low $10^5$ range. The Ramsey coherence time exceeds 100 us, and the average fidelity of Clifford gates is benchmarked to $\\mathcal{F} > 0.999$. These figures are likely to improve by an order of magnitude with optimized fabrication and measurement procedures. Our work establishes a ready-to-use ``partially protected\" superconducting qubit with an error rate comparable to the best transmons.","sentences":["We describe a superconducting qubit derived from operating a properly designed fluxonium circuit in a zero magnetic field.","The qubit has a frequency of about 4 GHz and the energy relaxation quality factor $Q \\approx 0.7\\times 10^7$, even though the dielectric loss quality factor of the circuit components is in the low $10^5$ range.","The Ramsey coherence time exceeds 100 us, and the average fidelity of Clifford gates is benchmarked to $\\mathcal{F} > 0.999$. These figures are likely to improve by an order of magnitude with optimized fabrication and measurement procedures.","Our work establishes a ready-to-use ``partially protected\" superconducting qubit with an error rate comparable to the best transmons."],"url":"http://arxiv.org/abs/2403.16780v1","category":"quant-ph"}
{"created":"2024-03-25 14:33:11","title":"Cross section measurement of $e^+e^-\\to \u03b7\u03c8(2S)$ and search for $e^+e^-\\to\u03b7\\tilde{X}(3872)$","abstract":"The energy-dependent cross section for $e^+e^-\\to \\eta\\psi(2S)$ is measured at eighteen center of mass energies from 4.288 GeV to 4.951 GeV using the BESIII detector. Using the same data samples, we also perform the first search for the reaction $e^+e^-\\to\\eta\\tilde{X}(3872)$, but no evidence is found for the $\\tilde{X}(3872)$ in the $\\pi^+\\pi^- J/\\psi$ mass distribution. At each of the eighteen center of mass energies, upper limits at the 90\\% confidence level on the cross section for $e^+e^-\\to\\eta\\psi(2S)$ and on the product of the $e^+e^-\\to\\eta\\tilde{X}(3872)$ cross section with the branching fraction of $\\tilde{X}(3872)\\to\\pi^+\\pi^- J/\\psi$ are reported.","sentences":["The energy-dependent cross section for $e^+e^-\\to \\eta\\psi(2S)$ is measured at eighteen center of mass energies from 4.288 GeV to 4.951 GeV using the BESIII detector.","Using the same data samples, we also perform the first search for the reaction $e^+e^-\\to\\eta\\tilde{X}(3872)$, but no evidence is found for the $\\tilde{X}(3872)$ in the $\\pi^+\\pi^- J/\\psi$ mass distribution.","At each of the eighteen center of mass energies, upper limits at the 90\\% confidence level on the cross section for $e^+e^-\\to\\eta\\psi(2S)$ and on the product of the $e^+e^-\\to\\eta\\tilde{X}(3872)$ cross section with the branching fraction of $\\tilde{X}(3872)\\to\\pi^+\\pi^- J/\\psi$ are reported."],"url":"http://arxiv.org/abs/2403.16811v1","category":"hep-ex"}
{"created":"2024-03-25 13:52:05","title":"A gate tunable transmon qubit in planar Ge","abstract":"Gate-tunable transmons (gatemons) employing semiconductor Josephson junctions have recently emerged as building blocks for hybrid quantum circuits. In this study, we present a gatemon fabricated in planar Germanium. We induce superconductivity in a two-dimensional hole gas by evaporating aluminum atop a thin spacer, which separates the superconductor from the Ge quantum well. The Josephson junction is then integrated into an Xmon circuit and capacitively coupled to a transmission line resonator. We showcase the qubit tunability in a broad frequency range with resonator and two-tone spectroscopy. Time-domain characterizations reveal energy relaxation and coherence times up to 75 ns. Our results, combined with the recent advances in the spin qubit field, pave the way towards novel hybrid and protected qubits in a group IV, CMOS-compatible material.","sentences":["Gate-tunable transmons (gatemons) employing semiconductor Josephson junctions have recently emerged as building blocks for hybrid quantum circuits.","In this study, we present a gatemon fabricated in planar Germanium.","We induce superconductivity in a two-dimensional hole gas by evaporating aluminum atop a thin spacer, which separates the superconductor from the Ge quantum well.","The Josephson junction is then integrated into an Xmon circuit and capacitively coupled to a transmission line resonator.","We showcase the qubit tunability in a broad frequency range with resonator and two-tone spectroscopy.","Time-domain characterizations reveal energy relaxation and coherence times up to 75 ns.","Our results, combined with the recent advances in the spin qubit field, pave the way towards novel hybrid and protected qubits in a group IV, CMOS-compatible material."],"url":"http://arxiv.org/abs/2403.16774v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-25 13:38:52","title":"A spectacular galactic scale magnetohydrodynamic powered wind in ESO 320-G030","abstract":"How galaxies regulate nuclear growth through gas accretion by supermassive black holes (SMBHs) is one of the most fundamental questions in galaxy evolution. One potential way to regulate nuclear growth is through a galactic wind that removes gas from the nucleus. It is unclear whether galactic winds are powered by jets, mechanical winds, radiation, or via magnetohydrodynamic (MHD) processes. Compact obscured nuclei (CONs) represent a significant phase of galactic nuclear growth. These galaxies hide growing SMBHs or unusual starbursts in their very opaque, extremely compact (r $<$ 100 pc) centres. They are found in approximately 30 % of the luminous and ultra-luminous infrared galaxy (LIRG and ULIRG) population. Here, we present high-resolution ALMA observations ($\\sim$30 mas, $\\sim$5 pc) of ground-state and vibrationally excited HCN towards ESO 320-G030 (IRAS 11506-3851). ESO 320-G030 is an isolated luminous infrared galaxy known to host a compact obscured nucleus and a kiloparsec-scale molecular wind. Our analysis of these high-resolution observations excludes the possibility of a starburst-driven wind, a mechanically or energy driven active galactic nucleus (AGN) wind, and exposes a molecular MDH wind. These results imply that the nuclear evolution of galaxies and the growth of SMBHs are similar to the growth of hot cores or protostars where gravitational collapse of the nuclear torus drives a MHD wind. These results mean galaxies are capable, in part, of regulating the evolution of their nuclei without feedback.","sentences":["How galaxies regulate nuclear growth through gas accretion by supermassive black holes (SMBHs) is one of the most fundamental questions in galaxy evolution.","One potential way to regulate nuclear growth is through a galactic wind that removes gas from the nucleus.","It is unclear whether galactic winds are powered by jets, mechanical winds, radiation, or via magnetohydrodynamic (MHD) processes.","Compact obscured nuclei (CONs) represent a significant phase of galactic nuclear growth.","These galaxies hide growing SMBHs or unusual starbursts in their very opaque, extremely compact (r $<$ 100 pc) centres.","They are found in approximately 30 % of the luminous and ultra-luminous infrared galaxy (LIRG and ULIRG) population.","Here, we present high-resolution ALMA observations ($\\sim$30 mas, $\\sim$5 pc) of ground-state and vibrationally excited HCN towards ESO 320-G030 (IRAS 11506-3851).","ESO 320-G030 is an isolated luminous infrared galaxy known to host a compact obscured nucleus and a kiloparsec-scale molecular wind.","Our analysis of these high-resolution observations excludes the possibility of a starburst-driven wind, a mechanically or energy driven active galactic nucleus (AGN) wind, and exposes a molecular MDH wind.","These results imply that the nuclear evolution of galaxies and the growth of SMBHs are similar to the growth of hot cores or protostars where gravitational collapse of the nuclear torus drives a MHD wind.","These results mean galaxies are capable, in part, of regulating the evolution of their nuclei without feedback."],"url":"http://arxiv.org/abs/2403.16759v1","category":"astro-ph.GA"}
{"created":"2024-03-25 13:27:39","title":"Action of the axial $U(1)$ non-invertible symmetry on the 't~Hooft line operator: A lattice gauge theory study","abstract":"We study how the symmetry operator of the axial $U(1)$ non-invertible symmetry acts on the 't~Hooft line operator in the $U(1)$ gauge theory by employing the modified Villain-type lattice formulation. We model the axial anomaly by a compact scalar boson, the ``QED axion''. For the gauge invariance, the simple 't~Hooft line operator, which is defined by a line integral of the dual $U(1)$ gauge potential, must be ``dressed'' by the scalar and $U(1)$ gauge fields. The anomalous Ward--Takahashi identity with the dressing factor shows that, when the symmetry operator sweep out a 't~Hooft loop, the loop acquires a surface operator of the $U(1)$ field strength with a correction of a higher cup product. This result appears consistent with the phenomenon observed in the continuum theory. A similar analysis for the axion string operator shows that it is not affected by the symmetry operator.","sentences":["We study how the symmetry operator of the axial $U(1)$ non-invertible symmetry acts on the 't~Hooft line operator in the $U(1)$ gauge theory by employing the modified Villain-type lattice formulation.","We model the axial anomaly by a compact scalar boson, the ``QED axion''.","For the gauge invariance, the simple 't~Hooft line operator, which is defined by a line integral of the dual $U(1)$ gauge potential, must be ``dressed'' by the scalar and $U(1)$ gauge fields.","The anomalous Ward--Takahashi identity with the dressing factor shows that, when the symmetry operator sweep out a 't~Hooft loop, the loop acquires a surface operator of the $U(1)$ field strength with a correction of a higher cup product.","This result appears consistent with the phenomenon observed in the continuum theory.","A similar analysis for the axion string operator shows that it is not affected by the symmetry operator."],"url":"http://arxiv.org/abs/2403.16752v1","category":"hep-lat"}
