{"created":"2024-02-12 18:59:39","title":"FAST: Factorizable Attention for Speeding up Transformers","abstract":"Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.","sentences":["Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions.","This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens.","We explore the properties of our new attention metric and conduct tests in various standard settings.","Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used."],"url":"http://arxiv.org/abs/2402.07901v1","category":"cs.LG"}
{"created":"2024-02-12 18:58:47","title":"CHIME/FRB Outriggers: KKO Station System and Commissioning Results","abstract":"Localizing fast radio bursts (FRBs) to their host galaxies is an essential step to better understanding their origins and using them as cosmic probes. The CHIME/FRB Outrigger program aims to add VLBI-localization capabilities to CHIME, such that FRBs may be localized to tens of milliarcsecond precision at the time of their discovery, more than sufficient for host galaxy identification. The first-built outrigger telescope is KKO, located 66 kilometers west of CHIME. Cross-correlating KKO with CHIME can achieve arcsecond-scale localization in right ascension while avoiding the worst effects of the ionosphere. This paper presents measurements of KKO's performance throughout its commissioning phase, as well as a summary of its design and function. We demonstrate KKO's capabilities as a standalone instrument by producing full-sky images, mapping the angular and frequency structure of the primary beam, and measuring feed positions. To demonstrate the localization capabilities of the CHIME -- KKO baseline, we collected five separate observations each for a set of twenty bright pulsars, and aimed to measure their positions to within 5~arcseconds. All of these pulses were successfully localized to within this specification. The next two outriggers are expected to be commissioned in 2024, and will enable subarcsecond localizations for approximately hundreds of FRBs each year.","sentences":["Localizing fast radio bursts (FRBs) to their host galaxies is an essential step to better understanding their origins and using them as cosmic probes.","The CHIME/FRB Outrigger program aims to add VLBI-localization capabilities to CHIME, such that FRBs may be localized to tens of milliarcsecond precision at the time of their discovery, more than sufficient for host galaxy identification.","The first-built outrigger telescope is KKO, located 66 kilometers west of CHIME.","Cross-correlating KKO with CHIME can achieve arcsecond-scale localization in right ascension while avoiding the worst effects of the ionosphere.","This paper presents measurements of KKO's performance throughout its commissioning phase, as well as a summary of its design and function.","We demonstrate KKO's capabilities as a standalone instrument by producing full-sky images, mapping the angular and frequency structure of the primary beam, and measuring feed positions.","To demonstrate the localization capabilities of the CHIME -- KKO baseline, we collected five separate observations each for a set of twenty bright pulsars, and aimed to measure their positions to within 5~arcseconds.","All of these pulses were successfully localized to within this specification.","The next two outriggers are expected to be commissioned in 2024, and will enable subarcsecond localizations for approximately hundreds of FRBs each year."],"url":"http://arxiv.org/abs/2402.07898v1","category":"astro-ph.IM"}
{"created":"2024-02-12 18:57:06","title":"Detection of Spider Mites on Labrador Beans through Machine Learning Approaches Using Custom Datasets","abstract":"Amidst growing food production demands, early plant disease detection is essential to safeguard crops; this study proposes a visual machine learning approach for plant disease detection, harnessing RGB and NIR data collected in real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN dataset. A two-stage early plant disease detection model with YOLOv8 and a sequential CNN was used to train on a dataset with partial labels, which showed a 3.6% increase in mAP compared to a single-stage end-to-end segmentation model. The sequential CNN model achieved 90.62% validation accuracy utilising RGBN data. An average of 6.25% validation accuracy increase is found using RGBN in classification compared to RGB using ResNet15 and the sequential CNN models. Further research and dataset improvements are needed to meet food production demands.","sentences":["Amidst growing food production demands, early plant disease detection is essential to safeguard crops; this study proposes a visual machine learning approach for plant disease detection, harnessing RGB and NIR data collected in real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN dataset.","A two-stage early plant disease detection model with YOLOv8 and a sequential CNN was used to train on a dataset with partial labels, which showed a 3.6% increase in mAP compared to a single-stage end-to-end segmentation model.","The sequential CNN model achieved 90.62% validation accuracy utilising RGBN data.","An average of 6.25% validation accuracy increase is found using RGBN in classification compared to RGB using ResNet15 and the sequential CNN models.","Further research and dataset improvements are needed to meet food production demands."],"url":"http://arxiv.org/abs/2402.07895v1","category":"cs.CV"}
{"created":"2024-02-12 18:54:02","title":"Label-Efficient Model Selection for Text Generation","abstract":"Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models. DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation reliability.","sentences":["Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models.","We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models.","DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation.","DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs.","Thus, it is able to identify a subset of examples that are more informative for preference decisions.","Our method is model-agnostic, and can be applied to any text generation model.","Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate.","In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation reliability."],"url":"http://arxiv.org/abs/2402.07891v1","category":"cs.CL"}
{"created":"2024-02-12 18:53:20","title":"MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning","abstract":"Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems. To mitigate this complexity, MAIDRL presents a semi-centralized Dense Reinforcement Learning algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL, by incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios. We further investigate the stability and robustness of our model. The statistics reflect that our model not only achieves higher winning rate in all the given scenarios but also boosts the agent's learning process in fine-grained decision-making.","sentences":["Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems.","To mitigate this complexity, MAIDRL presents a semi-centralized Dense Reinforcement Learning algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios.","In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL, by incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios.","The results show that the CNN-enabled MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios.","We further investigate the stability and robustness of our model.","The statistics reflect that our model not only achieves higher winning rate in all the given scenarios but also boosts the agent's learning process in fine-grained decision-making."],"url":"http://arxiv.org/abs/2402.07890v1","category":"cs.AI"}
{"created":"2024-02-12 18:52:39","title":"Toward an Android Static Analysis Approach for Data Protection","abstract":"Android applications collecting data from users must protect it according to the current legal frameworks. Such data protection has become even more important since the European Union rolled out the General Data Protection Regulation (GDPR). Since app developers are not legal experts, they find it difficult to write privacy-aware source code. Moreover, they have limited tool support to reason about data protection throughout their app development process.   This paper motivates the need for a static analysis approach to diagnose and explain data protection in Android apps. The analysis will recognize personal data sources in the source code, and aims to further examine the data flow originating from these sources. App developers can then address key questions about data manipulation, derived data, and the presence of technical measures. Despite challenges, we explore to what extent one can realize this analysis through static taint analysis, a common method for identifying security vulnerabilities. This is a first step towards designing a tool-based approach that aids app developers and assessors in ensuring data protection in Android apps, based on automated static program analysis.","sentences":["Android applications collecting data from users must protect it according to the current legal frameworks.","Such data protection has become even more important since the European Union rolled out the General Data Protection Regulation (GDPR).","Since app developers are not legal experts, they find it difficult to write privacy-aware source code.","Moreover, they have limited tool support to reason about data protection throughout their app development process.   ","This paper motivates the need for a static analysis approach to diagnose and explain data protection in Android apps.","The analysis will recognize personal data sources in the source code, and aims to further examine the data flow originating from these sources.","App developers can then address key questions about data manipulation, derived data, and the presence of technical measures.","Despite challenges, we explore to what extent one can realize this analysis through static taint analysis, a common method for identifying security vulnerabilities.","This is a first step towards designing a tool-based approach that aids app developers and assessors in ensuring data protection in Android apps, based on automated static program analysis."],"url":"http://arxiv.org/abs/2402.07889v1","category":"cs.SE"}
{"created":"2024-02-12 18:44:02","title":"Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks","abstract":"Early detection of network intrusions and cyber threats is one of the main pillars of cybersecurity. One of the most effective approaches for this purpose is to analyze network traffic with the help of artificial intelligence algorithms, with the aim of detecting the possible presence of an attacker by distinguishing it from a legitimate user. This is commonly done by collecting the traffic exchanged between terminals in a network and analyzing it on a per-packet or per-connection basis. In this paper, we propose instead to perform pre-processing of network traffic under analysis with the aim of extracting some new metrics on which we can perform more efficient detection and overcome some limitations of classical approaches. These new metrics are based on graph theory, and consider the network as a whole, rather than focusing on individual packets or connections. Our approach is validated through experiments performed on publicly available data sets, from which it results that it can not only overcome some of the limitations of classical approaches, but also achieve a better detection capability of cyber threats.","sentences":["Early detection of network intrusions and cyber threats is one of the main pillars of cybersecurity.","One of the most effective approaches for this purpose is to analyze network traffic with the help of artificial intelligence algorithms, with the aim of detecting the possible presence of an attacker by distinguishing it from a legitimate user.","This is commonly done by collecting the traffic exchanged between terminals in a network and analyzing it on a per-packet or per-connection basis.","In this paper, we propose instead to perform pre-processing of network traffic under analysis with the aim of extracting some new metrics on which we can perform more efficient detection and overcome some limitations of classical approaches.","These new metrics are based on graph theory, and consider the network as a whole, rather than focusing on individual packets or connections.","Our approach is validated through experiments performed on publicly available data sets, from which it results that it can not only overcome some of the limitations of classical approaches, but also achieve a better detection capability of cyber threats."],"url":"http://arxiv.org/abs/2402.07878v1","category":"cs.CR"}
{"created":"2024-02-12 18:41:55","title":"WildfireGPT: Tailored Large Language Model for Wildfire Analysis","abstract":"The recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence (AI) and machine learning (ML). However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change. For decision-makers and policymakers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific, rather than generic. To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks. We enrich WildfireGPT by providing additional context such as climate projections and scientific literature to ensure its information is current, relevant, and scientifically accurate. This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including researchers, engineers, urban planners, emergency managers, and infrastructure operators.","sentences":["The recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence (AI) and machine learning (ML).","However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge such as wildfire details within the broader context of climate change.","For decision-makers and policymakers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific, rather than generic.","To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks.","We enrich WildfireGPT by providing additional context such as climate projections and scientific literature to ensure its information is current, relevant, and scientifically accurate.","This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including researchers, engineers, urban planners, emergency managers, and infrastructure operators."],"url":"http://arxiv.org/abs/2402.07877v1","category":"cs.AI"}
{"created":"2024-02-12 18:41:34","title":"Policy Improvement using Language Feedback Models","abstract":"We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.","sentences":["We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following.","To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions.","First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld).","Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens.","Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation.","Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning."],"url":"http://arxiv.org/abs/2402.07876v1","category":"cs.LG"}
{"created":"2024-02-12 18:41:31","title":"Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States","abstract":"In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Experiments corroborate our theory, and demonstrate its conclusions on problems beyond LQR, where systems are non-linear and controllers are neural networks. We hypothesize that real-world optimal control may be greatly improved by developing methods for informed selection of initial states to train on.","sentences":["In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not.","Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data.","This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning).","There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states.","This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states.","Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training.","Experiments corroborate our theory, and demonstrate its conclusions on problems beyond LQR, where systems are non-linear and controllers are neural networks.","We hypothesize that real-world optimal control may be greatly improved by developing methods for informed selection of initial states to train on."],"url":"http://arxiv.org/abs/2402.07875v1","category":"cs.LG"}
{"created":"2024-02-12 18:33:47","title":"Scaling Laws for Fine-Grained Mixture of Experts","abstract":"Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.","sentences":["Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models.","In this work, we analyze their scaling properties, incorporating an expanded range of variables.","Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts.","Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity.","Leveraging these laws, we derive the optimal training configuration for a given computational budget.","Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget.","Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget."],"url":"http://arxiv.org/abs/2402.07871v1","category":"cs.LG"}
{"created":"2024-02-12 18:28:36","title":"PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models","abstract":"Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations. In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM. For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge poisoning attacks to RAG, where an attacker could inject a few poisoned texts into the knowledge database such that the LLM generates an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge poisoning attacks as an optimization problem, whose solution is a set of poisoned texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on the RAG, we propose two solutions to solve the optimization problem, respectively. Our results on multiple benchmark datasets and LLMs show our attacks could achieve 90% attack success rates when injecting 5 poisoned texts for each target question into a database with millions of texts. We also evaluate recent defenses and our results show they are insufficient to defend against our attacks, highlighting the need for new defenses.","sentences":["Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities.","Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination.","Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations.","In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM.","For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia.","As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question.","Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored.","We aim to bridge the gap in this work.","Particularly, we propose PoisonedRAG , a set of knowledge poisoning attacks to RAG, where an attacker could inject a few poisoned texts into the knowledge database such that the LLM generates an attacker-chosen target answer for an attacker-chosen target question.","We formulate knowledge poisoning attacks as an optimization problem, whose solution is a set of poisoned texts.","Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on the RAG, we propose two solutions to solve the optimization problem, respectively.","Our results on multiple benchmark datasets and LLMs show our attacks could achieve 90% attack success rates when injecting 5 poisoned texts for each target question into a database with millions of texts.","We also evaluate recent defenses and our results show they are insufficient to defend against our attacks, highlighting the need for new defenses."],"url":"http://arxiv.org/abs/2402.07867v1","category":"cs.CR"}
{"created":"2024-02-12 18:21:14","title":"Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models","abstract":"Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible code for VLM training, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open-source VLMs.","sentences":["Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3.","Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations.","To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities.","Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using base vs. instruct-tuned language models, amongst others.","We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible code for VLM training, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open-source VLMs."],"url":"http://arxiv.org/abs/2402.07865v1","category":"cs.CV"}
{"created":"2024-02-12 18:14:43","title":"AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy","abstract":"Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy. Exploratory analyses showed a pronounced effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 43%, compared with 28% for the biased assistant. We further examine whether LLM augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our findings do not consistently support these hypotheses. Our results suggest that access to an LLM assistant, even a biased one, can be a helpful decision aid in cognitively demanding tasks where the answer is not known at the time of interaction.","sentences":["Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains.","This study explores the potential of LLMs to augment judgement in forecasting tasks.","We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting.","Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support.","Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group.","This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy.","Exploratory analyses showed a pronounced effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 43%, compared with 28% for the biased assistant.","We further examine whether LLM augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty.","Our findings do not consistently support these hypotheses.","Our results suggest that access to an LLM assistant, even a biased one, can be a helpful decision aid in cognitively demanding tasks where the answer is not known at the time of interaction."],"url":"http://arxiv.org/abs/2402.07862v1","category":"cs.CY"}
{"created":"2024-02-12 18:12:09","title":"On the Detection of Reviewer-Author Collusion Rings From Paper Bidding","abstract":"A major threat to the peer-review systems of computer science conferences is the existence of \"collusion rings\" between reviewers. In such collusion rings, reviewers who have also submitted their own papers to the conference work together to manipulate the conference's paper assignment, with the aim of being assigned to review each other's papers. The most straightforward way that colluding reviewers can manipulate the paper assignment is by indicating their interest in each other's papers through strategic paper bidding. One potential approach to solve this important problem would be to detect the colluding reviewers from their manipulated bids, after which the conference can take appropriate action. While prior work has has developed effective techniques to detect other kinds of fraud, no research has yet established that detecting collusion rings is even possible. In this work, we tackle the question of whether it is feasible to detect collusion rings from the paper bidding. To answer this question, we conduct empirical analysis of two realistic conference bidding datasets, including evaluations of existing algorithms for fraud detection in other applications. We find that collusion rings can achieve considerable success at manipulating the paper assignment while remaining hidden from detection: for example, in one dataset, undetected colluders are able to achieve assignment to up to 30% of the papers authored by other colluders. In addition, when 10 colluders bid on all of each other's papers, no detection algorithm outputs a group of reviewers with more than 31% overlap with the true colluders. These results suggest that collusion cannot be effectively detected from the bidding, demonstrating the need to develop more complex detection algorithms that leverage additional metadata.","sentences":["A major threat to the peer-review systems of computer science conferences is the existence of \"collusion rings\" between reviewers.","In such collusion rings, reviewers who have also submitted their own papers to the conference work together to manipulate the conference's paper assignment, with the aim of being assigned to review each other's papers.","The most straightforward way that colluding reviewers can manipulate the paper assignment is by indicating their interest in each other's papers through strategic paper bidding.","One potential approach to solve this important problem would be to detect the colluding reviewers from their manipulated bids, after which the conference can take appropriate action.","While prior work has has developed effective techniques to detect other kinds of fraud, no research has yet established that detecting collusion rings is even possible.","In this work, we tackle the question of whether it is feasible to detect collusion rings from the paper bidding.","To answer this question, we conduct empirical analysis of two realistic conference bidding datasets, including evaluations of existing algorithms for fraud detection in other applications.","We find that collusion rings can achieve considerable success at manipulating the paper assignment while remaining hidden from detection: for example, in one dataset, undetected colluders are able to achieve assignment to up to 30% of the papers authored by other colluders.","In addition, when 10 colluders bid on all of each other's papers, no detection algorithm outputs a group of reviewers with more than 31% overlap with the true colluders.","These results suggest that collusion cannot be effectively detected from the bidding, demonstrating the need to develop more complex detection algorithms that leverage additional metadata."],"url":"http://arxiv.org/abs/2402.07860v1","category":"cs.SI"}
{"created":"2024-02-12 18:10:17","title":"Lissard: Long and Simple Sequential Reasoning Datasets","abstract":"Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at https://github.com/unicamp-dl/Lissard","sentences":["Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens.","However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training.","For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items.","In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution.","Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases.","The datasets and code are available at https://github.com/unicamp-dl/Lissard"],"url":"http://arxiv.org/abs/2402.07859v1","category":"cs.CL"}
{"created":"2024-02-12 17:53:43","title":"An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering","abstract":"Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information. Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection. In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth. Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance. We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance. To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach. The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in attributed graphs. We conclude that modularity can be used for hyperparameter optimisation and model selection on real-world datasets as well as being a suitable proxy for predicting ground-truth performance, however, GNNs fail to balance the information duality when the spaces contain conflicting signals.","sentences":["Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information.","Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection.","In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth.","Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance.","We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance.","To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach.","The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in attributed graphs.","We conclude that modularity can be used for hyperparameter optimisation and model selection on real-world datasets as well as being a suitable proxy for predicting ground-truth performance, however, GNNs fail to balance the information duality when the spaces contain conflicting signals."],"url":"http://arxiv.org/abs/2402.07845v1","category":"cs.LG"}
{"created":"2024-02-12 17:51:22","title":"Creating pair plasmas with observable collective effects","abstract":"Although existing technology cannot yet directly produce fields at the Schwinger level, experimental facilities can already explore strong-field QED phenomena by taking advantage of the Lorentz boost of energetic electron beams. Recent studies show that QED cascades can create electron-positron pairs at sufficiently high density to exhibit collective plasma effects. Signatures of the collective pair plasma effects can appear in exquisite detail through plasma-induced frequency upshifts and chirps in the laser spectrum. Maximizing the magnitude of the QED plasma signature demands high pair density and low pair energy, which suits the configuration of colliding an over $10^{18}{Jm^{-3}}$ energy-density electron beam with a $10^{22}\\mathrm{-}10^{23}{Wcm^{-2}}$ intensity laser pulse. The collision creates pairs that have a large plasma frequency, made even larger as they slow down or reverse direction due to both the radiation reaction and laser pressure. This paper explains at a tutorial level the key properties of the QED cascades and laser frequency upshift, and at the same time finds the minimum parameters that can be used to produce observable QED plasma.","sentences":["Although existing technology cannot yet directly produce fields at the Schwinger level, experimental facilities can already explore strong-field QED phenomena by taking advantage of the Lorentz boost of energetic electron beams.","Recent studies show that QED cascades can create electron-positron pairs at sufficiently high density to exhibit collective plasma effects.","Signatures of the collective pair plasma effects can appear in exquisite detail through plasma-induced frequency upshifts and chirps in the laser spectrum.","Maximizing the magnitude of the QED plasma signature demands high pair density and low pair energy, which suits the configuration of colliding an over $10^{18}{Jm^{-3}}$ energy-density electron beam with a $10^{22}\\mathrm{-}10^{23}{Wcm^{-2}}$ intensity laser pulse.","The collision creates pairs that have a large plasma frequency, made even larger as they slow down or reverse direction due to both the radiation reaction and laser pressure.","This paper explains at a tutorial level the key properties of the QED cascades and laser frequency upshift, and at the same time finds the minimum parameters that can be used to produce observable QED plasma."],"url":"http://arxiv.org/abs/2402.07840v1","category":"physics.plasm-ph"}
{"created":"2024-02-12 17:47:15","title":"2D MoS2 under switching field conditions: study of high-frequency noise from velocity fluctuations","abstract":"The transient high-frequency noise response of two-dimensional MoS2 under abrupt large signal switching field conditions is studied by means of an ensemble Monte Carlo simulator. Low-to-high and high-to-low transitions are analyzed at low (77 K) and room temperature, considering several underlying substrates. The incorporation of stochastic individual scattering events allows capturing the transient collective phonon-electron coupling, which is shown to be responsible for the appearance of an oscillatory behaviour in the average velocity and energy at low temperature in the case of MoS2 on SiO2, hBN and Al2O3. Activation and deactivation of surface polar phonon emissions in the low-to-high field switching process yield to the appearance of a relevant peak in the power spectral density of velocity fluctuations in the THz range. The results show the important influence of the substrate type in the noise behaviour of MoS2 at very high frequencies, which is critical for the design of future FET devices based on 2D TMD technology.","sentences":["The transient high-frequency noise response of two-dimensional MoS2 under abrupt large signal switching field conditions is studied by means of an ensemble Monte Carlo simulator.","Low-to-high and high-to-low transitions are analyzed at low (77 K) and room temperature, considering several underlying substrates.","The incorporation of stochastic individual scattering events allows capturing the transient collective phonon-electron coupling, which is shown to be responsible for the appearance of an oscillatory behaviour in the average velocity and energy at low temperature in the case of MoS2 on SiO2, hBN and Al2O3.","Activation and deactivation of surface polar phonon emissions in the low-to-high field switching process yield to the appearance of a relevant peak in the power spectral density of velocity fluctuations in the THz range.","The results show the important influence of the substrate type in the noise behaviour of MoS2 at very high frequencies, which is critical for the design of future FET devices based on 2D TMD technology."],"url":"http://arxiv.org/abs/2402.07838v1","category":"physics.app-ph"}
{"created":"2024-02-12 17:26:35","title":"Understanding fitness landscapes in morpho-evolution via local optima networks","abstract":"Morpho-evolution (ME) refers to the simultaneous optimisation of a robot's design and controller to maximise performance given a task and environment. Many genetic encodings have been proposed which are capable of representing design and control. Previous research has provided empirical comparisons between encodings in terms of their performance with respect to an objective function and the diversity of designs that are evaluated, however there has been no attempt to explain the observed findings. We address this by applying Local Optima Network (LON) analysis to investigate the structure of the fitness landscapes induced by three different encodings when evolving a robot for a locomotion task, shedding new light on the ease by which different fitness landscapes can be traversed by a search process. This is the first time LON analysis has been applied in the field of ME despite its popularity in combinatorial optimisation domains; the findings will facilitate design of new algorithms or operators that are customised to ME landscapes in the future.","sentences":["Morpho-evolution (ME) refers to the simultaneous optimisation of a robot's design and controller to maximise performance given a task and environment.","Many genetic encodings have been proposed which are capable of representing design and control.","Previous research has provided empirical comparisons between encodings in terms of their performance with respect to an objective function and the diversity of designs that are evaluated, however there has been no attempt to explain the observed findings.","We address this by applying Local Optima Network (LON) analysis to investigate the structure of the fitness landscapes induced by three different encodings when evolving a robot for a locomotion task, shedding new light on the ease by which different fitness landscapes can be traversed by a search process.","This is the first time LON analysis has been applied in the field of ME despite its popularity in combinatorial optimisation domains; the findings will facilitate design of new algorithms or operators that are customised to ME landscapes in the future."],"url":"http://arxiv.org/abs/2402.07822v1","category":"cs.AI"}
{"created":"2024-02-12 17:24:35","title":"A Benchmark Grocery Dataset of Realworld Point Clouds From Single View","abstract":"Fine-grained grocery object recognition is an important computer vision problem with broad applications in automatic checkout, in-store robotic navigation, and assistive technologies for the visually impaired. Existing datasets on groceries are mainly 2D images. Models trained on these datasets are limited to learning features from the regular 2D grids. While portable 3D sensors such as Kinect were commonly available for mobile phones, sensors such as LiDAR and TrueDepth, have recently been integrated into mobile phones. Despite the availability of mobile 3D sensors, there are currently no dedicated real-world large-scale benchmark 3D datasets for grocery. In addition, existing 3D datasets lack fine-grained grocery categories and have limited training samples. Furthermore, collecting data by going around the object versus the traditional photo capture makes data collection cumbersome. Thus, we introduce a large-scale grocery dataset called 3DGrocery100. It constitutes 100 classes, with a total of 87,898 3D point clouds created from 10,755 RGB-D single-view images. We benchmark our dataset on six recent state-of-the-art 3D point cloud classification models. Additionally, we also benchmark the dataset on few-shot and continual learning point cloud classification tasks. Project Page: https://bigdatavision.org/3DGrocery100/.","sentences":["Fine-grained grocery object recognition is an important computer vision problem with broad applications in automatic checkout, in-store robotic navigation, and assistive technologies for the visually impaired.","Existing datasets on groceries are mainly 2D images.","Models trained on these datasets are limited to learning features from the regular 2D grids.","While portable 3D sensors such as Kinect were commonly available for mobile phones, sensors such as LiDAR and TrueDepth, have recently been integrated into mobile phones.","Despite the availability of mobile 3D sensors, there are currently no dedicated real-world large-scale benchmark 3D datasets for grocery.","In addition, existing 3D datasets lack fine-grained grocery categories and have limited training samples.","Furthermore, collecting data by going around the object versus the traditional photo capture makes data collection cumbersome.","Thus, we introduce a large-scale grocery dataset called 3DGrocery100.","It constitutes 100 classes, with a total of 87,898 3D point clouds created from 10,755 RGB-D single-view images.","We benchmark our dataset on six recent state-of-the-art 3D point cloud classification models.","Additionally, we also benchmark the dataset on few-shot and continual learning point cloud classification tasks.","Project Page: https://bigdatavision.org/3DGrocery100/."],"url":"http://arxiv.org/abs/2402.07819v1","category":"cs.CV"}
{"created":"2024-02-12 17:24:15","title":"Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning","abstract":"Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replacement for SGD, this paper presents a comprehensive study both theoretically and empirically. First, we propose the stagewise DP zeroth-order method that dynamically schedules key hyperparameters. This design is grounded on the synergy between DP random perturbation and the gradient approximation error of the zeroth-order method, and its effect on finetuning trajectory. Second, we further enhance the scalability by reducing the trainable parameters that are identified by repurposing a data-free pruning technique requiring no additional data or extra privacy budget. We provide theoretical analysis for both proposed methods. We conduct extensive empirical analysis on both encoder-only masked language model and decoder-only autoregressive language model, achieving impressive results in terms of scalability and utility.","sentences":["Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks.","Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets.","Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability.","Most existing methods build upon the seminal work of DP-SGD.","Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD.","In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient.","Rather than treating the zeroth-order method as a drop-in replacement for SGD, this paper presents a comprehensive study both theoretically and empirically.","First, we propose the stagewise DP zeroth-order method that dynamically schedules key hyperparameters.","This design is grounded on the synergy between DP random perturbation and the gradient approximation error of the zeroth-order method, and its effect on finetuning trajectory.","Second, we further enhance the scalability by reducing the trainable parameters that are identified by repurposing a data-free pruning technique requiring no additional data or extra privacy budget.","We provide theoretical analysis for both proposed methods.","We conduct extensive empirical analysis on both encoder-only masked language model and decoder-only autoregressive language model, achieving impressive results in terms of scalability and utility."],"url":"http://arxiv.org/abs/2402.07818v1","category":"cs.LG"}
{"created":"2024-02-12 17:18:51","title":"PBADet: A One-Stage Anchor-Free Approach for Part-Body Association","abstract":"The detection of human parts (e.g., hands, face) and their correct association with individuals is an essential task, e.g., for ubiquitous human-machine interfaces and action recognition. Traditional methods often employ multi-stage processes, rely on cumbersome anchor-based systems, or do not scale well to larger part sets. This paper presents PBADet, a novel one-stage, anchor-free approach for part-body association detection. Building upon the anchor-free object representation across multi-scale feature maps, we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies. Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness. Comprehensive experiments on various datasets underscore the efficacy of our approach, which not only outperforms existing state-of-the-art techniques but also offers a more streamlined and efficient solution to the part-body association challenge.","sentences":["The detection of human parts (e.g., hands, face) and their correct association with individuals is an essential task, e.g., for ubiquitous human-machine interfaces and action recognition.","Traditional methods often employ multi-stage processes, rely on cumbersome anchor-based systems, or do not scale well to larger part sets.","This paper presents PBADet, a novel one-stage, anchor-free approach for part-body association detection.","Building upon the anchor-free object representation across multi-scale feature maps, we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies.","Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness.","Comprehensive experiments on various datasets underscore the efficacy of our approach, which not only outperforms existing state-of-the-art techniques but also offers a more streamlined and efficient solution to the part-body association challenge."],"url":"http://arxiv.org/abs/2402.07814v1","category":"cs.CV"}
{"created":"2024-02-12 17:17:50","title":"Retrieval-Augmented Thought Process as Sequential Decision Making","abstract":"Large Language Models (LLMs) have demonstrated their strong ability to assist people and show \"sparks of intelligence\". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.","sentences":["Large Language Models (LLMs) have demonstrated their strong ability to assist people and show \"sparks of intelligence\".","However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts.","In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP).","Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process.","To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference.","In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models."],"url":"http://arxiv.org/abs/2402.07812v1","category":"cs.CL"}
{"created":"2024-02-12 17:03:58","title":"Generalising Planning Environment Redesign","abstract":"In Environment Design, one interested party seeks to affect another agent's decisions by applying changes to the environment. Most research on planning environment (re)design assumes the interested party's objective is to facilitate the recognition of goals and plans, and search over the space of environment modifications to find the minimal set of changes that simplify those tasks and optimise a particular metric. This search space is usually intractable, so existing approaches devise metric-dependent pruning techniques for performing search more efficiently. This results in approaches that are not able to generalise across different objectives and/or metrics. In this paper, we argue that the interested party could have objectives and metrics that are not necessarily related to recognising agents' goals or plans. Thus, to generalise the task of Planning Environment Redesign, we develop a general environment redesign approach that is metric-agnostic and leverages recent research on top-quality planning to efficiently redesign planning environments according to any interested party's objective and metric. Experiments over a set of environment redesign benchmarks show that our general approach outperforms existing approaches when using well-known metrics, such as facilitating the recognition of goals, as well as its effectiveness when solving environment redesign tasks that optimise a novel set of different metrics.","sentences":["In Environment Design, one interested party seeks to affect another agent's decisions by applying changes to the environment.","Most research on planning environment (re)design assumes the interested party's objective is to facilitate the recognition of goals and plans, and search over the space of environment modifications to find the minimal set of changes that simplify those tasks and optimise a particular metric.","This search space is usually intractable, so existing approaches devise metric-dependent pruning techniques for performing search more efficiently.","This results in approaches that are not able to generalise across different objectives and/or metrics.","In this paper, we argue that the interested party could have objectives and metrics that are not necessarily related to recognising agents' goals or plans.","Thus, to generalise the task of Planning Environment Redesign, we develop a general environment redesign approach that is metric-agnostic and leverages recent research on top-quality planning to efficiently redesign planning environments according to any interested party's objective and metric.","Experiments over a set of environment redesign benchmarks show that our general approach outperforms existing approaches when using well-known metrics, such as facilitating the recognition of goals, as well as its effectiveness when solving environment redesign tasks that optimise a novel set of different metrics."],"url":"http://arxiv.org/abs/2402.07799v1","category":"cs.AI"}
{"created":"2024-02-12 16:59:05","title":"Empowering Federated Learning for Massive Models with NVIDIA FLARE","abstract":"In the ever-evolving landscape of artificial intelligence (AI) and large language models (LLMs), handling and leveraging data effectively has become a critical challenge. Most state-of-the-art machine learning algorithms are data-centric. However, as the lifeblood of model performance, necessary data cannot always be centralized due to various factors such as privacy, regulation, geopolitics, copyright issues, and the sheer effort required to move vast datasets. In this paper, we explore how federated learning enabled by NVIDIA FLARE can address these challenges with easy and scalable integration capabilities, enabling parameter-efficient and full supervised fine-tuning of LLMs for natural language processing and biopharmaceutical applications to enhance their accuracy and robustness.","sentences":["In the ever-evolving landscape of artificial intelligence (AI) and large language models (LLMs), handling and leveraging data effectively has become a critical challenge.","Most state-of-the-art machine learning algorithms are data-centric.","However, as the lifeblood of model performance, necessary data cannot always be centralized due to various factors such as privacy, regulation, geopolitics, copyright issues, and the sheer effort required to move vast datasets.","In this paper, we explore how federated learning enabled by NVIDIA FLARE can address these challenges with easy and scalable integration capabilities, enabling parameter-efficient and full supervised fine-tuning of LLMs for natural language processing and biopharmaceutical applications to enhance their accuracy and robustness."],"url":"http://arxiv.org/abs/2402.07792v1","category":"cs.LG"}
{"created":"2024-02-12 16:55:58","title":"Continuous Assurance of Autonomous Vehicle Behavior Through Machine Learned Correctness Properties","abstract":"Correctness properties are critical to conducting verification and validation on software systems, especially those cyberphysical systems whose functionality changes frequently due to software updates, changes in the operating environment, or newly learned behaviors. We detail a novel method to automatically construct expressive, executable correctness properties in the form of machine-learned correctness properties which can be used to ensure that a system's behavior is correct with respect to its design and operating requirements. We propose a method to bootstrap the creation of these correctness properties using a novel simulation-based generation of training and testing data using multiple extensions to the Cross Entropy algorithm for search-based optimization. Then, we apply this method to a software-in-the-loop evaluation of an autonomous vehicle to demonstrate that such models can assert about important properties of multi-agent cyberphysical systems. We demonstrate that this process brings the task of developing robust correctness properties from the realm of formal methods experts into the domain of system developers and engineers, and that machine-learned correctness properties are expressive enough to capture the correct behavior of cyberphysical systems in their complex environments. This advancement can provide evidence of dependability to system designers and users, enhancing trust in the deployment of autonomous vehicles and other intelligent transportation systems.","sentences":["Correctness properties are critical to conducting verification and validation on software systems, especially those cyberphysical systems whose functionality changes frequently due to software updates, changes in the operating environment, or newly learned behaviors.","We detail a novel method to automatically construct expressive, executable correctness properties in the form of machine-learned correctness properties which can be used to ensure that a system's behavior is correct with respect to its design and operating requirements.","We propose a method to bootstrap the creation of these correctness properties using a novel simulation-based generation of training and testing data using multiple extensions to the Cross Entropy algorithm for search-based optimization.","Then, we apply this method to a software-in-the-loop evaluation of an autonomous vehicle to demonstrate that such models can assert about important properties of multi-agent cyberphysical systems.","We demonstrate that this process brings the task of developing robust correctness properties from the realm of formal methods experts into the domain of system developers and engineers, and that machine-learned correctness properties are expressive enough to capture the correct behavior of cyberphysical systems in their complex environments.","This advancement can provide evidence of dependability to system designers and users, enhancing trust in the deployment of autonomous vehicles and other intelligent transportation systems."],"url":"http://arxiv.org/abs/2402.07791v1","category":"cs.SE"}
{"created":"2024-02-12 16:52:26","title":"Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis","abstract":"Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of each granularity feature and their synergistic interactions, resulting in a cumulative effect without additional computational expenses. Experimental findings on SemEval 2014 and Twitter datasets confirm EMGF's superiority over existing ABSA methods.","sentences":["Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information.","Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models.","Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis.","With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion.","As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist.","This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs.","EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of each granularity feature and their synergistic interactions, resulting in a cumulative effect without additional computational expenses.","Experimental findings on SemEval 2014 and Twitter datasets confirm EMGF's superiority over existing ABSA methods."],"url":"http://arxiv.org/abs/2402.07787v2","category":"cs.AI"}
{"created":"2024-02-12 16:49:37","title":"Cygnus OB2 as a test case for particle acceleration in young massive star clusters","abstract":"In this paper, we focus on the scientific case of Cygnus OB2, a northern sky young massive stellar cluster (YMSC) located towards the Cygnus X star-forming complex. We consider a model that assumes cosmic ray acceleration occurring only at the termination shock of the collective wind of the YMSC and address the question of whether, and under what hypotheses, hadronic emission by the accelerated particles can account for the observations of Cygnus OB2 obtained by Fermi-LAT, HAWC and LHAASO. In order to do so, we carefully review the available information on this source, also confronting different estimates of the relevant parameters with ad hoc developed simulations. Once other model parameters are fixed, the spectral and spatial properties of the emission are found to be very sensitive to the unknown properties of the turbulent magnetic field. Comparison with the data shows that our suggested scenario is incompatible with Kolmogorov turbulence. Assuming Kraichnan or Bohm type turbulence spectra, the model accounts well for the Very High Energy (VHE) data, but fails to reproduce the centrally peaked morphology observed by Fermi-LAT, suggesting that additional effects might be important for lower energy $\\gamma$-ray emission. We discuss how additional progress can be made with a more detailed and extended knowledge of the spectral and morphological properties of the emission.","sentences":["In this paper, we focus on the scientific case of Cygnus OB2, a northern sky young massive stellar cluster (YMSC) located towards the Cygnus X star-forming complex.","We consider a model that assumes cosmic ray acceleration occurring only at the termination shock of the collective wind of the YMSC and address the question of whether, and under what hypotheses, hadronic emission by the accelerated particles can account for the observations of Cygnus OB2 obtained by Fermi-LAT, HAWC and LHAASO.","In order to do so, we carefully review the available information on this source, also confronting different estimates of the relevant parameters with ad hoc developed simulations.","Once other model parameters are fixed, the spectral and spatial properties of the emission are found to be very sensitive to the unknown properties of the turbulent magnetic field.","Comparison with the data shows that our suggested scenario is incompatible with Kolmogorov turbulence.","Assuming Kraichnan or Bohm type turbulence spectra, the model accounts well for the Very High Energy (VHE) data, but fails to reproduce the centrally peaked morphology observed by Fermi-LAT, suggesting that additional effects might be important for lower energy $\\gamma$-ray emission.","We discuss how additional progress can be made with a more detailed and extended knowledge of the spectral and morphological properties of the emission."],"url":"http://arxiv.org/abs/2402.07784v1","category":"astro-ph.HE"}
{"created":"2024-02-12 16:43:55","title":"Algorithmic Fairness and Color-blind Racism: Navigating the Intersection","abstract":"Our focus lies at the intersection between two broader research perspectives: (1) the scientific study of algorithms and (2) the scholarship on race and racism. Many streams of research related to algorithmic fairness have been born out of interest at this intersection. We think about this intersection as the product of work derived from both sides. From (1) algorithms to (2) racism, the starting place might be an algorithmic question or method connected to a conceptualization of racism. On the other hand, from (2) racism to (1) algorithms, the starting place could be recognizing a setting where a legacy of racism is known to persist and drawing connections between that legacy and the introduction of algorithms into this setting. In either direction, meaningful disconnection can occur when conducting research at the intersection of racism and algorithms. The present paper urges collective reflection on research directions at this intersection. Despite being primarily motivated by instances of racial bias, research in algorithmic fairness remains mostly disconnected from scholarship on racism. In particular, there has not been an examination connecting algorithmic fairness discussions directly to the ideology of color-blind racism; we aim to fill this gap. We begin with a review of an essential account of color-blind racism then we review racial discourse within algorithmic fairness research and underline significant patterns, shifts and disconnects. Ultimately, we argue that researchers can improve the navigation of the landscape at the intersection by recognizing ideological shifts as such and iteratively re-orienting towards maintaining meaningful connections across interdisciplinary lines.","sentences":["Our focus lies at the intersection between two broader research perspectives: (1) the scientific study of algorithms and (2) the scholarship on race and racism.","Many streams of research related to algorithmic fairness have been born out of interest at this intersection.","We think about this intersection as the product of work derived from both sides.","From (1) algorithms to (2) racism, the starting place might be an algorithmic question or method connected to a conceptualization of racism.","On the other hand, from (2) racism to (1) algorithms, the starting place could be recognizing a setting where a legacy of racism is known to persist and drawing connections between that legacy and the introduction of algorithms into this setting.","In either direction, meaningful disconnection can occur when conducting research at the intersection of racism and algorithms.","The present paper urges collective reflection on research directions at this intersection.","Despite being primarily motivated by instances of racial bias, research in algorithmic fairness remains mostly disconnected from scholarship on racism.","In particular, there has not been an examination connecting algorithmic fairness discussions directly to the ideology of color-blind racism; we aim to fill this gap.","We begin with a review of an essential account of color-blind racism then we review racial discourse within algorithmic fairness research and underline significant patterns, shifts and disconnects.","Ultimately, we argue that researchers can improve the navigation of the landscape at the intersection by recognizing ideological shifts as such and iteratively re-orienting towards maintaining meaningful connections across interdisciplinary lines."],"url":"http://arxiv.org/abs/2402.07778v1","category":"cs.CY"}
{"created":"2024-02-12 16:35:02","title":"The Goodwillie calculus of polyhedral products","abstract":"We describe the Goodwillie calculus of polyhedral products in the case that the fat wedge filtration on the associated real moment-angle complex is trivial. We do this by analysing the behaviour on calculus of the Denham-Suciu fibre sequence, the Iriye-Kishimoto decomposition of the polyhedral product constructed from a collection of pairs of cones and their bases, and the Hilton-Milnor decomposition. As a corollary we show that the Goodwillie calculus of these polyhedral products converges integrally and diverges in $v_h$-periodic homotopy unless the simplicial complex is a full simplex.","sentences":["We describe the Goodwillie calculus of polyhedral products in the case that the fat wedge filtration on the associated real moment-angle complex is trivial.","We do this by analysing the behaviour on calculus of the Denham-Suciu fibre sequence, the Iriye-Kishimoto decomposition of the polyhedral product constructed from a collection of pairs of cones and their bases, and the Hilton-Milnor decomposition.","As a corollary we show that the Goodwillie calculus of these polyhedral products converges integrally and diverges in $v_h$-periodic homotopy unless the simplicial complex is a full simplex."],"url":"http://arxiv.org/abs/2402.07774v1","category":"math.AT"}
{"created":"2024-02-12 16:33:35","title":"End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty","abstract":"Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data. The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization. This requires backpropagation through the optimization problem using approximation techniques specific to the problem's form, especially for nondifferentiable linear and mixed-integer programs. This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of fairness and robustness in decision models. Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric prediction for fair and robust optimization under uncertainty.","sentences":["Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data.","The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization.","This requires backpropagation through the optimization problem using approximation techniques specific to the problem's form, especially for nondifferentiable linear and mixed-integer programs.","This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of fairness and robustness in decision models.","Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric prediction for fair and robust optimization under uncertainty."],"url":"http://arxiv.org/abs/2402.07772v1","category":"cs.AI"}
{"created":"2024-02-12 16:28:42","title":"Decay rate measurements $^{137}$Cs at J\u00e1nossy Underground Research Laboratory","abstract":"The question whether an annual modulation is observable during nuclear decay rate measurements has long been the subject of research. One of the possible explanations for the annual variations would be the effect of solar neutrinos, the flux of which changes in correlation with the Earth-Sun distance. A decay rate measurement with a $^{137}$Cs source and a HPGe detector is currently being conducted 30 meters below the ground at J\\'anossy Underground Research Laboratory (Csilleb\\'erc, Hungary). The laboratory is part of the Vesztergombi High Energy Laboratory (VLAB), one of the TOP 50 research infrastructures in Hungary. From October 2022 to March 2023, data of six months' worth has been collected, and hence this is a new opportunity to check whether the annual variation in decay rate can be observed. The laboratory, the experiment, the data processing method, and the first results are presented in this study.","sentences":["The question whether an annual modulation is observable during nuclear decay rate measurements has long been the subject of research.","One of the possible explanations for the annual variations would be the effect of solar neutrinos, the flux of which changes in correlation with the Earth-Sun distance.","A decay rate measurement with a $^{137}$Cs source and a HPGe detector is currently being conducted 30 meters below the ground at J\\'anossy Underground Research Laboratory (Csilleb\\'erc, Hungary).","The laboratory is part of the Vesztergombi High Energy Laboratory (VLAB), one of the TOP 50 research infrastructures in Hungary.","From October 2022 to March 2023, data of six months' worth has been collected, and hence this is a new opportunity to check whether the annual variation in decay rate can be observed.","The laboratory, the experiment, the data processing method, and the first results are presented in this study."],"url":"http://arxiv.org/abs/2402.07761v1","category":"nucl-ex"}
{"created":"2024-02-12 16:25:47","title":"Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model","abstract":"Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.","sentences":["Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems.","Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive.","To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful.","Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph.","Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars.","Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon."],"url":"http://arxiv.org/abs/2402.07757v1","category":"cs.LG"}
{"created":"2024-02-12 16:23:28","title":"Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models","abstract":"Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capabilities in diffusion language models.","sentences":["Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models.","This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models.","We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process.","In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance.","Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems.","Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding.","Our findings contribute to the understanding and development of reasoning capabilities in diffusion language models."],"url":"http://arxiv.org/abs/2402.07754v1","category":"cs.CL"}
{"created":"2024-02-12 16:14:22","title":"Towards Unified Alignment Between Agents, Humans, and Environment","abstract":"The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\\mathbf{U}$nified $\\mathbf{A}$lignment for $\\mathbf{A}$gents ($\\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost statistics to reflect self-constraints. We then follow the principles of $\\mathbf{UA}^2$ to propose an initial design of our agent, and benchmark its performance with several candidate baselines in the retrofitted WebShop. The extensive experimental results further prove the importance of the principles of $\\mathbf{UA}^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities.","sentences":["The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction.","However, the efficacy of agents remains limited when operating in intricate, realistic environments.","In this work, we introduce the principles of $\\mathbf{U}$nified $\\mathbf{A}$lignment for $\\mathbf{A}$gents ($\\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets.","From the perspective of $\\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates.","We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost statistics to reflect self-constraints.","We then follow the principles of $\\mathbf{UA}^2$ to propose an initial design of our agent, and benchmark its performance with several candidate baselines in the retrofitted WebShop.","The extensive experimental results further prove the importance of the principles of $\\mathbf{UA}^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities."],"url":"http://arxiv.org/abs/2402.07744v1","category":"cs.AI"}
{"created":"2024-02-12 16:04:01","title":"Asking Multimodal Clarifying Questions in Mixed-Initiative Conversational Search","abstract":"In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query. These questions aim to uncover user's information needs and resolve query ambiguities. We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information. Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems. To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images. We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts. Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase. Experimental results indicate that the addition of images leads to significant improvements of up to 90% in retrieval performance when selecting the relevant images. Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency.","sentences":["In mixed-initiative conversational search systems, clarifying questions are used to help users who struggle to express their intentions in a single query.","These questions aim to uncover user's information needs and resolve query ambiguities.","We hypothesize that in scenarios where multimodal information is pertinent, the clarification process can be improved by using non-textual information.","Therefore, we propose to add images to clarifying questions and formulate the novel task of asking multimodal clarifying questions in open-domain, mixed-initiative conversational search systems.","To facilitate research into this task, we collect a dataset named Melon that contains over 4k multimodal clarifying questions, enriched with over 14k images.","We also propose a multimodal query clarification model named Marto and adopt a prompt-based, generative fine-tuning strategy to perform the training of different stages with different prompts.","Several analyses are conducted to understand the importance of multimodal contents during the query clarification phase.","Experimental results indicate that the addition of images leads to significant improvements of up to 90% in retrieval performance when selecting the relevant images.","Extensive analyses are also performed to show the superiority of Marto compared with discriminative baselines in terms of effectiveness and efficiency."],"url":"http://arxiv.org/abs/2402.07742v1","category":"cs.CL"}
{"created":"2024-02-12 15:26:37","title":"Adaptive Artificial Immune Networks for Mitigating DoS flooding Attacks","abstract":"Denial of service attacks pose a threat in constant growth. This is mainly due to their tendency to gain in sophistication, ease of implementation, obfuscation and the recent improvements in occultation of fingerprints. On the other hand, progress towards self-organizing networks, and the different techniques involved in their development, such as software-defined networking, network-function virtualization, artificial intelligence or cloud computing, facilitates the design of new defensive strategies, more complete, consistent and able to adapt the defensive deployment to the current status of the network. In order to contribute to their development, in this paper, the use of artificial immune systems to mitigate denial of service attacks is proposed. The approach is based on building networks of distributed sensors suited to the requirements of the monitored environment. These components are capable of identifying threats and reacting according to the behavior of the biological defense mechanisms in human beings. It is accomplished by emulating the different immune reactions, the establishment of quarantine areas and the construction of immune memory. For their assessment, experiments with public domain datasets (KDD'99, CAIDA'07 and CAIDA'08) and simulations on various network configurations based on traffic samples gathered by the University Complutense of Madrid and flooding attacks generated by the tool DDoSIM were performed.","sentences":["Denial of service attacks pose a threat in constant growth.","This is mainly due to their tendency to gain in sophistication, ease of implementation, obfuscation and the recent improvements in occultation of fingerprints.","On the other hand, progress towards self-organizing networks, and the different techniques involved in their development, such as software-defined networking, network-function virtualization, artificial intelligence or cloud computing, facilitates the design of new defensive strategies, more complete, consistent and able to adapt the defensive deployment to the current status of the network.","In order to contribute to their development, in this paper, the use of artificial immune systems to mitigate denial of service attacks is proposed.","The approach is based on building networks of distributed sensors suited to the requirements of the monitored environment.","These components are capable of identifying threats and reacting according to the behavior of the biological defense mechanisms in human beings.","It is accomplished by emulating the different immune reactions, the establishment of quarantine areas and the construction of immune memory.","For their assessment, experiments with public domain datasets (KDD'99, CAIDA'07 and CAIDA'08) and simulations on various network configurations based on traffic samples gathered by the University Complutense of Madrid and flooding attacks generated by the tool DDoSIM were performed."],"url":"http://arxiv.org/abs/2402.07714v1","category":"cs.CR"}
{"created":"2024-02-12 15:26:01","title":"Model Collapse Demystified: The Case of Regression","abstract":"In the era of large language models like ChatGPT, the phenomenon of \"model collapse\" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.","sentences":["In the era of large language models like ChatGPT, the phenomenon of \"model collapse\" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses.","In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses.","Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates.","We also propose a simple strategy based on adaptive regularization to mitigate model collapse.","Our theoretical results are validated with experiments."],"url":"http://arxiv.org/abs/2402.07712v1","category":"cs.LG"}
{"created":"2024-02-12 15:23:19","title":"Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA","abstract":"In recent years, there has been a significant increase in the utilization of deep learning methods, particularly convolutional neural networks (CNNs), which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing. Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds. The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment. In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues.","sentences":["In recent years, there has been a significant increase in the utilization of deep learning methods, particularly convolutional neural networks (CNNs), which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing.","Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds.","The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment.","In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues."],"url":"http://arxiv.org/abs/2402.07710v1","category":"cs.AI"}
{"created":"2024-02-12 15:17:31","title":"Online Sequential Decision-Making with Unknown Delays","abstract":"In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of general convexity and relative strong convexity, respectively. We also demonstrate the efficiency of each algorithm under different norms through concrete examples. Furthermore, our theoretical results are consistent with the current best bounds when degenerated to standard settings.","sentences":["In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay.","Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback.","Our proposed algorithms are versatile and applicable to universal norms.","Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points.","For each type of algorithm, we provide corresponding regret bounds under cases of general convexity and relative strong convexity, respectively.","We also demonstrate the efficiency of each algorithm under different norms through concrete examples.","Furthermore, our theoretical results are consistent with the current best bounds when degenerated to standard settings."],"url":"http://arxiv.org/abs/2402.07703v1","category":"cs.LG"}
{"created":"2024-02-12 14:53:37","title":"OrderBkd: Textual backdoor attack through repositioning","abstract":"The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.","sentences":["The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks.","Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected.","Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger.","By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples.","In addition, we show the robustness of our attack to the ONION defense method.","All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd."],"url":"http://arxiv.org/abs/2402.07689v1","category":"cs.CL"}
{"created":"2024-02-12 14:53:28","title":"CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity","abstract":"Large Language Models (LLMs) excel across various domains, from computer vision to medical diagnostics. However, understanding the diverse landscape of cybersecurity, encompassing cryptography, reverse engineering, and managerial facets like risk assessment, presents a challenge, even for human experts. In this paper, we introduce CyberMetric, a benchmark dataset comprising 10,000 questions sourced from standards, certifications, research papers, books, and other publications in the cybersecurity domain. The questions are created through a collaborative process, i.e., merging expert knowledge with LLMs, including GPT-3.5 and Falcon-180B. Human experts spent over 200 hours verifying their accuracy and relevance. Beyond assessing LLMs' knowledge, the dataset's main goal is to facilitate a fair comparison between humans and different LLMs in cybersecurity. To achieve this, we carefully selected 80 questions covering a wide range of topics within cybersecurity and involved 30 participants of diverse expertise levels, facilitating a comprehensive comparison between human and machine intelligence in this area. The findings revealed that LLMs outperformed humans in almost every aspect of cybersecurity.","sentences":["Large Language Models (LLMs) excel across various domains, from computer vision to medical diagnostics.","However, understanding the diverse landscape of cybersecurity, encompassing cryptography, reverse engineering, and managerial facets like risk assessment, presents a challenge, even for human experts.","In this paper, we introduce CyberMetric, a benchmark dataset comprising 10,000 questions sourced from standards, certifications, research papers, books, and other publications in the cybersecurity domain.","The questions are created through a collaborative process, i.e., merging expert knowledge with LLMs, including GPT-3.5 and Falcon-180B.","Human experts spent over 200 hours verifying their accuracy and relevance.","Beyond assessing LLMs' knowledge, the dataset's main goal is to facilitate a fair comparison between humans and different LLMs in cybersecurity.","To achieve this, we carefully selected 80 questions covering a wide range of topics within cybersecurity and involved 30 participants of diverse expertise levels, facilitating a comprehensive comparison between human and machine intelligence in this area.","The findings revealed that LLMs outperformed humans in almost every aspect of cybersecurity."],"url":"http://arxiv.org/abs/2402.07688v1","category":"cs.AI"}
{"created":"2024-02-12 14:40:54","title":"Large Language Models \"Ad Referendum\": How Good Are They at Machine Translation in the Legal Domain?","abstract":"This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain. It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy. The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality. The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations.","sentences":["This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain.","It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy.","The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations.","This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality.","The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations."],"url":"http://arxiv.org/abs/2402.07681v1","category":"cs.CL"}
{"created":"2024-02-12 14:40:43","title":"AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer","abstract":"Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems. Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras. Besides, discrepancies in the two data representations further complicate fusion methods. We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies. AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods. Our code is publicly available at https://github.com/sanjay-810/AYDIV2","sentences":["Combining LiDAR and camera data has shown potential in enhancing short-distance object detection in autonomous driving systems.","Yet, the fusion encounters difficulties with extended distance detection due to the contrast between LiDAR's sparse data and the dense resolution of cameras.","Besides, discrepancies in the two data representations further complicate fusion methods.","We introduce AYDIV, a novel framework integrating a tri-phase alignment process specifically designed to enhance long-distance detection even amidst data discrepancies.","AYDIV consists of the Global Contextual Fusion Alignment Transformer (GCFAT), which improves the extraction of camera features and provides a deeper understanding of large-scale patterns; the Sparse Fused Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial data fusion.","AYDIV's performance on the Waymo Open Dataset (WOD) with an improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset with a performance improvement of 7.40% in AP value demonstrates its efficacy in comparison to other existing fusion-based methods.","Our code is publicly available at https://github.com/sanjay-810/AYDIV2"],"url":"http://arxiv.org/abs/2402.07680v1","category":"cs.CV"}
{"created":"2024-02-12 14:07:16","title":"Combining Evolutionary Strategies and Novelty Detection to go Beyond the Alignment Limit of the $Z_3$ 3HDM","abstract":"We present a novel Artificial Intelligence approach for Beyond the Standard Model parameter space scans by augmenting an Evolutionary Strategy with Novelty Detection. Our approach leverages the power of Evolutionary Strategies, previously shown to quickly converge to the valid regions of the parameter space, with a \\emph{novelty reward} to continue exploration once converged. Taking the $Z_3$ 3HDM as our Physics case, we show how our methodology allows us to quickly explore highly constrained multidimensional parameter spaces, providing up to eight orders of magnitude higher sampling efficiency when compared with pure random sampling and up to four orders of magnitude when compared to random sampling around the alignment limit. In turn, this enables us to explore regions of the parameter space that have been hitherto overlooked, leading to the possibility of novel phenomenological realisations of the $Z_3$ 3HDM that had not been considered before.","sentences":["We present a novel Artificial Intelligence approach for Beyond the Standard Model parameter space scans by augmenting an Evolutionary Strategy with Novelty Detection.","Our approach leverages the power of Evolutionary Strategies, previously shown to quickly converge to the valid regions of the parameter space, with a \\emph{novelty reward} to continue exploration once converged.","Taking the $Z_3$ 3HDM as our Physics case, we show how our methodology allows us to quickly explore highly constrained multidimensional parameter spaces, providing up to eight orders of magnitude higher sampling efficiency when compared with pure random sampling and up to four orders of magnitude when compared to random sampling around the alignment limit.","In turn, this enables us to explore regions of the parameter space that have been hitherto overlooked, leading to the possibility of novel phenomenological realisations of the $Z_3$ 3HDM that had not been considered before."],"url":"http://arxiv.org/abs/2402.07661v1","category":"hep-ph"}
{"created":"2024-02-12 13:34:33","title":"Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models","abstract":"Difficult-to-treat depression (DTD) has been proposed as a broader and more clinically comprehensive perspective on a person's depressive disorder where despite treatment, they continue to experience significant burden. We sought to develop a Large Language Model (LLM)-based tool capable of interrogating routinely-collected, narrative (free-text) electronic health record (EHR) data to locate published prognostic factors that capture the clinical syndrome of DTD. In this work, we use LLM-generated synthetic data (GPT3.5) and a Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction model. The resulting model is then able to extract and label spans related to a variety of relevant positive and negative factors in real clinical data (i.e. spans of text that increase or decrease the likelihood of a patient matching the DTD syndrome). We show it is possible to obtain good overall performance (0.70 F1 across polarity) on real clinical data on a set of as many as 20 different factors, and high performance (0.85 F1 with 0.95 precision) on a subset of important DTD factors such as history of abuse, family history of affective disorder, illness severity and suicidality by training the model exclusively on synthetic data. Our results show promise for future healthcare applications especially in applications where traditionally, highly confidential medical data and human-expert annotation would normally be required.","sentences":["Difficult-to-treat depression (DTD) has been proposed as a broader and more clinically comprehensive perspective on a person's depressive disorder where despite treatment, they continue to experience significant burden.","We sought to develop a Large Language Model (LLM)-based tool capable of interrogating routinely-collected, narrative (free-text) electronic health record (EHR) data to locate published prognostic factors that capture the clinical syndrome of DTD.","In this work, we use LLM-generated synthetic data (GPT3.5) and a Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction model.","The resulting model is then able to extract and label spans related to a variety of relevant positive and negative factors in real clinical data (i.e. spans of text that increase or decrease the likelihood of a patient matching the DTD syndrome).","We show it is possible to obtain good overall performance (0.70 F1 across polarity) on real clinical data on a set of as many as 20 different factors, and high performance (0.85 F1 with 0.95 precision) on a subset of important DTD factors such as history of abuse, family history of affective disorder, illness severity and suicidality by training the model exclusively on synthetic data.","Our results show promise for future healthcare applications especially in applications where traditionally, highly confidential medical data and human-expert annotation would normally be required."],"url":"http://arxiv.org/abs/2402.07645v1","category":"cs.CL"}
{"created":"2024-02-12 13:27:22","title":"Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data","abstract":"The ability to generate sentiment-controlled feedback in response to multimodal inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative) sentiment. A sentiment classification accuracy of 77.23% has been achieved, 18.82% higher than the accuracy without using the controllability. Moreover, the system incorporates a similarity module for assessing feedback relevance through rank-based metrics. It implements an interpretability technique to analyze the contribution of textual and visual features during the generation of uncontrolled and controlled feedback.","sentences":["The ability to generate sentiment-controlled feedback in response to multimodal inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses.","This capability has profound applications in healthcare, marketing, and education.","To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system.","The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs.","It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback.","The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments.","The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative) sentiment.","A sentiment classification accuracy of 77.23% has been achieved, 18.82% higher than the accuracy without using the controllability.","Moreover, the system incorporates a similarity module for assessing feedback relevance through rank-based metrics.","It implements an interpretability technique to analyze the contribution of textual and visual features during the generation of uncontrolled and controlled feedback."],"url":"http://arxiv.org/abs/2402.07640v1","category":"cs.MM"}
{"created":"2024-02-12 13:24:32","title":"Tighter Bounds on the Information Bottleneck with Application to Deep Learning","abstract":"Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks. This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs.","sentences":["Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters.","The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space.","The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable.","Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks.","This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs.","These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs."],"url":"http://arxiv.org/abs/2402.07639v1","category":"cs.LG"}
{"created":"2024-02-12 13:16:47","title":"Complete Instances Mining for Weakly Supervised Instance Segmentation","abstract":"Weakly supervised instance segmentation (WSIS) using only image-level labels is a challenging task due to the difficulty of aligning coarse annotations with the finer task. However, with the advancement of deep neural networks (DNNs), WSIS has garnered significant attention. Following a proposal-based paradigm, we encounter a redundant segmentation problem resulting from a single instance being represented by multiple proposals. For example, we feed a picture of a dog and proposals into the network and expect to output only one proposal containing a dog, but the network outputs multiple proposals. To address this problem, we propose a novel approach for WSIS that focuses on the online refinement of complete instances through the use of MaskIoU heads to predict the integrity scores of proposals and a Complete Instances Mining (CIM) strategy to explicitly model the redundant segmentation problem and generate refined pseudo labels. Our approach allows the network to become aware of multiple instances and complete instances, and we further improve its robustness through the incorporation of an Anti-noise strategy. Empirical evaluations on the PASCAL VOC 2012 and MS COCO datasets demonstrate that our method achieves state-of-the-art performance with a notable margin. Our implementation will be made available at https://github.com/ZechengLi19/CIM.","sentences":["Weakly supervised instance segmentation (WSIS) using only image-level labels is a challenging task due to the difficulty of aligning coarse annotations with the finer task.","However, with the advancement of deep neural networks (DNNs), WSIS has garnered significant attention.","Following a proposal-based paradigm, we encounter a redundant segmentation problem resulting from a single instance being represented by multiple proposals.","For example, we feed a picture of a dog and proposals into the network and expect to output only one proposal containing a dog, but the network outputs multiple proposals.","To address this problem, we propose a novel approach for WSIS that focuses on the online refinement of complete instances through the use of MaskIoU heads to predict the integrity scores of proposals and a Complete Instances Mining (CIM) strategy to explicitly model the redundant segmentation problem and generate refined pseudo labels.","Our approach allows the network to become aware of multiple instances and complete instances, and we further improve its robustness through the incorporation of an Anti-noise strategy.","Empirical evaluations on the PASCAL VOC 2012 and MS COCO datasets demonstrate that our method achieves state-of-the-art performance with a notable margin.","Our implementation will be made available at https://github.com/ZechengLi19/CIM."],"url":"http://arxiv.org/abs/2402.07633v1","category":"cs.CV"}
{"created":"2024-02-12 13:16:30","title":"Overconfident and Unconfident AI Hinder Human-AI Collaboration","abstract":"As artificial intelligence (AI) advances, human-AI collaboration has become increasingly prevalent across both professional and everyday settings. In such collaboration, AI can express its confidence level about its performance, serving as a crucial indicator for humans to evaluate AI's suggestions. However, AI may exhibit overconfidence or underconfidence--its expressed confidence is higher or lower than its actual performance--which may lead humans to mistakenly evaluate AI advice. Our study investigates the influences of AI's overconfidence and underconfidence on human trust, their acceptance of AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI confidence levels and performance feedback facilitates better recognition of AI confidence misalignments. However, participants tend to withhold their trust as perceiving such misalignments, leading to a rejection of AI suggestions and subsequently poorer performance in collaborative tasks. Conversely, without such information, participants struggle to identify misalignments, resulting in either the neglect of correct AI advice or the following of incorrect AI suggestions, adversely affecting collaboration. This study offers valuable insights for enhancing human-AI collaboration by underscoring the importance of aligning AI's expressed confidence with its actual performance and the necessity of calibrating human trust towards AI confidence.","sentences":["As artificial intelligence (AI) advances, human-AI collaboration has become increasingly prevalent across both professional and everyday settings.","In such collaboration, AI can express its confidence level about its performance, serving as a crucial indicator for humans to evaluate AI's suggestions.","However, AI may exhibit overconfidence or underconfidence--its expressed confidence is higher or lower than its actual performance--which may lead humans to mistakenly evaluate AI advice.","Our study investigates the influences of AI's overconfidence and underconfidence on human trust, their acceptance of AI suggestions, and collaboration outcomes.","Our study reveal that disclosing AI confidence levels and performance feedback facilitates better recognition of AI confidence misalignments.","However, participants tend to withhold their trust as perceiving such misalignments, leading to a rejection of AI suggestions and subsequently poorer performance in collaborative tasks.","Conversely, without such information, participants struggle to identify misalignments, resulting in either the neglect of correct AI advice or the following of incorrect AI suggestions, adversely affecting collaboration.","This study offers valuable insights for enhancing human-AI collaboration by underscoring the importance of aligning AI's expressed confidence with its actual performance and the necessity of calibrating human trust towards AI confidence."],"url":"http://arxiv.org/abs/2402.07632v1","category":"cs.AI"}
{"created":"2024-02-12 13:13:04","title":"G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering","abstract":"Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and resists hallucination. (Our codes and datasets are available at: https://github.com/XiaoxinHe/G-Retriever.)","sentences":["Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface.","In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph.","While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs.","In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning.","Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks.","Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can be fine-tuned to enhance graph understanding via soft prompting.","To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem.","Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and resists hallucination.","(Our codes and datasets are available at: https://github.com/XiaoxinHe/G-Retriever.)"],"url":"http://arxiv.org/abs/2402.07630v1","category":"cs.LG"}
{"created":"2024-02-12 13:09:21","title":"AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts","abstract":"To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing models' mathematical reasoning capabilities. The AutoMathText dataset is available at https://huggingface.co/datasets/math-ai/AutoMathText. The code is available at https://github.com/yifanzhang-pro/AutoMathText.","sentences":["To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection.","Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data.","To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works.","Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing models' mathematical reasoning capabilities.","The AutoMathText dataset is available at https://huggingface.co/datasets/math-ai/AutoMathText.","The code is available at https://github.com/yifanzhang-pro/AutoMathText."],"url":"http://arxiv.org/abs/2402.07625v1","category":"cs.CL"}
{"created":"2024-02-12 12:52:47","title":"Developing a Multi-variate Prediction Model For COVID-19 From Crowd-sourced Respiratory Voice Data","abstract":"COVID-19 has affected more than 223 countries worldwide and in the Post-COVID Era, there is a pressing need for non-invasive, low-cost, and highly scalable solutions to detect COVID-19. We develop a deep learning model to identify COVID-19 from voice recording data. The novelty of this work is in the development of deep learning models for COVID-19 identification from only voice recordings. We use the Cambridge COVID-19 Sound database which contains 893 speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app. Voice features including Mel-spectrograms and Mel-frequency cepstral coefficients (MFCC) and CNN Encoder features are extracted. Based on the voice data, we develop deep learning classification models to detect COVID-19 cases. These models include Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power to baseline machine learning models. HuBERT achieves the highest accuracy of 86\\% and the highest AUC of 0.93. The results achieved with the proposed models suggest promising results in COVID-19 diagnosis from voice recordings when compared to the results obtained from the state-of-the-art.","sentences":["COVID-19 has affected more than 223 countries worldwide and in the Post-COVID Era, there is a pressing need for non-invasive, low-cost, and highly scalable solutions to detect COVID-19.","We develop a deep learning model to identify COVID-19 from voice recording data.","The novelty of this work is in the development of deep learning models for COVID-19 identification from only voice recordings.","We use the Cambridge COVID-19 Sound database which contains 893 speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app.","Voice features including Mel-spectrograms and Mel-frequency cepstral coefficients (MFCC) and CNN Encoder features are extracted.","Based on the voice data, we develop deep learning classification models to detect COVID-19 cases.","These models include Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) and Hidden-Unit BERT (HuBERT).","We compare their predictive power to baseline machine learning models.","HuBERT achieves the highest accuracy of 86\\% and the highest AUC of 0.93.","The results achieved with the proposed models suggest promising results in COVID-19 diagnosis from voice recordings when compared to the results obtained from the state-of-the-art."],"url":"http://arxiv.org/abs/2402.07619v1","category":"cs.SD"}
{"created":"2024-02-12 12:48:02","title":"Anchor-based Large Language Models","abstract":"Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM significantly improves computational efficiency and resource utilization, demonstrating the potential of the anchor-based attention approach in the context of LLMs for real-time inference in practical applications.","sentences":["Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation.","However, the substantial size and parameter volume of these LLMs require massive GPU memory.","This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing.","This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy.","This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency.","Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference.","Despite a minor compromise in accuracy, the AnLLM significantly improves computational efficiency and resource utilization, demonstrating the potential of the anchor-based attention approach in the context of LLMs for real-time inference in practical applications."],"url":"http://arxiv.org/abs/2402.07616v1","category":"cs.CL"}
{"created":"2024-02-12 12:30:42","title":"Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping","abstract":"Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced few-shot ability to boost zero or one-shot performance. Based on easy-to-hard training recipe, we propose SOFT+ which further boost self-alignment's performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across various classification and generation tasks, highlighting the potential of bootstrapping self-alignment on continually enhancing model alignment performance.","sentences":["Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability.","However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models.","This gives rise to a key query: What if we do multi-time bootstrapping self-alignment?","Does this strategy enhance model performance or lead to rapid degradation?","In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models.","Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning.","To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model.","Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced few-shot ability to boost zero or one-shot performance.","Based on easy-to-hard training recipe, we propose SOFT+ which further boost self-alignment's performance.","Our experiments demonstrate the efficiency of SOFT (SOFT+) across various classification and generation tasks, highlighting the potential of bootstrapping self-alignment on continually enhancing model alignment performance."],"url":"http://arxiv.org/abs/2402.07610v1","category":"cs.CL"}
{"created":"2024-02-12 11:10:12","title":"Goal-Oriented and Semantic Communication in 6G AI-Native Networks: The 6G-GOALS Approach","abstract":"Recent advances in AI technologies have notably expanded device intelligence, fostering federation and cooperation among distributed AI agents. These advancements impose new requirements on future 6G mobile network architectures. To meet these demands, it is essential to transcend classical boundaries and integrate communication, computation, control, and intelligence. This paper presents the 6G-GOALS approach to goal-oriented and semantic communications for AI-Native 6G Networks. The proposed approach incorporates semantic, pragmatic, and goal-oriented communication into AI-native technologies, aiming to facilitate information exchange between intelligent agents in a more relevant, effective, and timely manner, thereby optimizing bandwidth, latency, energy, and electromagnetic field (EMF) radiation. The focus is on distilling data to its most relevant form and terse representation, aligning with the source's intent or the destination's objectives and context, or serving a specific goal. 6G-GOALS builds on three fundamental pillars: i) AI-enhanced semantic data representation, sensing, compression, and communication, ii) foundational AI reasoning and causal semantic data representation, contextual relevance, and value for goal-oriented effectiveness, and iii) sustainability enabled by more efficient wireless services. Finally, we illustrate two proof-of-concepts implementing semantic, goal-oriented, and pragmatic communication principles in near-future use cases. Our study covers the project's vision, methodologies, and potential impact.","sentences":["Recent advances in AI technologies have notably expanded device intelligence, fostering federation and cooperation among distributed AI agents.","These advancements impose new requirements on future 6G mobile network architectures.","To meet these demands, it is essential to transcend classical boundaries and integrate communication, computation, control, and intelligence.","This paper presents the 6G-GOALS approach to goal-oriented and semantic communications for AI-Native 6G Networks.","The proposed approach incorporates semantic, pragmatic, and goal-oriented communication into AI-native technologies, aiming to facilitate information exchange between intelligent agents in a more relevant, effective, and timely manner, thereby optimizing bandwidth, latency, energy, and electromagnetic field (EMF) radiation.","The focus is on distilling data to its most relevant form and terse representation, aligning with the source's intent or the destination's objectives and context, or serving a specific goal.","6G-GOALS builds on three fundamental pillars: i) AI-enhanced semantic data representation, sensing, compression, and communication, ii) foundational AI reasoning and causal semantic data representation, contextual relevance, and value for goal-oriented effectiveness, and iii) sustainability enabled by more efficient wireless services.","Finally, we illustrate two proof-of-concepts implementing semantic, goal-oriented, and pragmatic communication principles in near-future use cases.","Our study covers the project's vision, methodologies, and potential impact."],"url":"http://arxiv.org/abs/2402.07573v1","category":"eess.SP"}
{"created":"2024-02-12 11:04:14","title":"Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction","abstract":"We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate time series forecasting.","sentences":["We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting.","GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains.","In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude.","GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner.","Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines.","Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate time series forecasting."],"url":"http://arxiv.org/abs/2402.07570v1","category":"cs.LG"}
{"created":"2024-02-12 10:19:17","title":"Ensuring trustworthy and ethical behaviour in intelligent logical agents","abstract":"Autonomous Intelligent Agents are employed in many applications upon which the life and welfare of living beings and vital social functions may depend. Therefore, agents should be trustworthy. A priori certification techniques (i.e., techniques applied prior to system's deployment) can be useful, but are not sufficient for agents that evolve, and thus modify their epistemic and belief state, and for open Multi-Agent Systems, where heterogeneous agents can join or leave the system at any stage of its operation. In this paper, we propose/refine/extend dynamic (runtime) logic-based self-checking techniques, devised in order to be able to ensure agents' trustworthy and ethical behaviour.","sentences":["Autonomous Intelligent Agents are employed in many applications upon which the life and welfare of living beings and vital social functions may depend.","Therefore, agents should be trustworthy.","A priori certification techniques (i.e., techniques applied prior to system's deployment) can be useful, but are not sufficient for agents that evolve, and thus modify their epistemic and belief state, and for open Multi-Agent Systems, where heterogeneous agents can join or leave the system at any stage of its operation.","In this paper, we propose/refine/extend dynamic (runtime) logic-based self-checking techniques, devised in order to be able to ensure agents' trustworthy and ethical behaviour."],"url":"http://arxiv.org/abs/2402.07547v1","category":"cs.MA"}
{"created":"2024-02-12 10:11:50","title":"Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models","abstract":"Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model. In essence, our findings suggest that fine-tuning with explanations significantly bolsters the performance of large language models.","sentences":["Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models.","Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase.","In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers.","We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach.","Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length.","Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations.","Lastly, we argue that despite the challenging nature of adding explanations, samples that contain explanations not only reduce the volume of data required for training but also promote a more effective generalization by the model.","In essence, our findings suggest that fine-tuning with explanations significantly bolsters the performance of large language models."],"url":"http://arxiv.org/abs/2402.07543v1","category":"cs.CL"}
{"created":"2024-02-12 10:09:16","title":"PKG API: A Tool for Personal Knowledge Graph Management","abstract":"Personal knowledge graphs (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control. Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce. This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs. Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API. To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance.","sentences":["Personal knowledge graphs (PKGs) offer individuals a way to store and consolidate their fragmented personal data in a central place, improving service personalization while maintaining full user control.","Despite their potential, practical PKG implementations with user-friendly interfaces remain scarce.","This work addresses this gap by proposing a complete solution to represent, manage, and interface with PKGs.","Our approach includes (1) a user-facing PKG Client, enabling end-users to administer their personal data easily via natural language statements, and (2) a service-oriented PKG API.","To tackle the complexity of representing these statements within a PKG, we present an RDF-based PKG vocabulary that supports this, along with properties for access rights and provenance."],"url":"http://arxiv.org/abs/2402.07540v1","category":"cs.HC"}
{"created":"2024-02-12 10:04:07","title":"BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection","abstract":"Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the multi-stage structure contributing 17.6% to the improvement. Additionally, it outperforms ChatGPT-4 by 42.07%. Our Code is publicly available: https://github.com/Neviim96/BreakGPT","sentences":["Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange.","However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors.","Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar.","The reason is that the unique data and specific knowledge are required in breakout detection.","To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection.","Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications.","Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the multi-stage structure contributing 17.6% to the improvement.","Additionally, it outperforms ChatGPT-4 by 42.07%.","Our Code is publicly available: https://github.com/Neviim96/BreakGPT"],"url":"http://arxiv.org/abs/2402.07536v1","category":"cs.AI"}
{"created":"2024-02-12 10:00:10","title":"Semantic Data for Humanities and Social Sciences (SDHSS): an Ecosystem of CIDOC CRM Extensions for Research Data Production and Reuse","abstract":"Given the challenge of giant knowledge graphs created by major eco-nomic actors, which could virtually replace research in the Humani-ties and Social Sciences (HSS) in responding to public concerns, thequestion arises of how to increase the value of research data throughtheir publication and networking, applying the FAIR principles. Bothan epistemological and a semantic analysis show that the most rel-evant part of research data is factual information, understood as arepresentation of the objects observed by the scientific disciplines,their properties and their relationships.This rich universe of information will be made understandable andtherefore reusable through the application of foundational ontologiesand a methodology based on the distinction between different levelsof abstraction, allowing the collective development of one or moreshared and reusable domain ontologies. This vision is being carriedout around the CIDOC CRM, as core ontology, and Semantic Datafor Humanities and Social Sciences (SDHSS), as a high-level exten-sion of it, as well as an ecosystem of sub-domain extensions that canbe easily managed through the ontome.net application. This willresult in an interoperability that is semantically richer than the sim-ple alignment of ontologies and less costly in terms of resources, andabove all adapted to the scientific and humanistic project of the HSS.","sentences":["Given the challenge of giant knowledge graphs created by major eco-nomic actors, which could virtually replace research in the Humani-ties and Social Sciences (HSS) in responding to public concerns, thequestion arises of how to increase the value of research data throughtheir publication and networking, applying the FAIR principles.","Bothan epistemological and a semantic analysis show that the most rel-evant part of research data is factual information, understood as arepresentation of the objects observed by the scientific disciplines,their properties and their relationships.","This rich universe of information will be made understandable andtherefore reusable through the application of foundational ontologiesand a methodology based on the distinction between different levelsof abstraction, allowing the collective development of one or moreshared and reusable domain ontologies.","This vision is being carriedout around the CIDOC CRM, as core ontology, and Semantic Datafor Humanities and Social Sciences (SDHSS), as a high-level exten-sion of it, as well as an ecosystem of sub-domain extensions that canbe easily managed through the ontome.net application.","This willresult in an interoperability that is semantically richer than the sim-ple alignment of ontologies and less costly in terms of resources, andabove all adapted to the scientific and humanistic project of the HSS."],"url":"http://arxiv.org/abs/2402.07531v1","category":"cs.IT"}
{"created":"2024-02-12 09:43:17","title":"A step towards the integration of machine learning and small area estimation","abstract":"The use of machine-learning techniques has grown in numerous research areas. Currently, it is also widely used in statistics, including the official statistics for data collection (e.g. satellite imagery, web scraping and text mining, data cleaning, integration and imputation) but also for data analysis. However, the usage of these methods in survey sampling including small area estimation is still very limited. Therefore, we propose a predictor supported by these algorithms which can be used to predict any population or subpopulation characteristics based on cross-sectional and longitudinal data. Machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means that they have very good properties in case of strong departures from the classic assumptions. Therefore, we analyse the performance of our proposal under a different set-up, in our opinion of greater importance in real-life surveys. We study only small departures from the assumed model, to show that our proposal is a good alternative in this case as well, even in comparison with optimal methods under the model. What is more, we propose the method of the accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods, where the accuracy is measured as in survey sampling practice. The solution of this problem is indicated in the literature as one of the key issues in integration of these approaches. The simulation studies are based on a real, longitudinal dataset, freely available from the Polish Local Data Bank, where the prediction problem of subpopulation characteristics in the last period, with \"borrowing strength\" from other subpopulations and time periods, is considered.","sentences":["The use of machine-learning techniques has grown in numerous research areas.","Currently, it is also widely used in statistics, including the official statistics for data collection (e.g. satellite imagery, web scraping and text mining, data cleaning, integration and imputation) but also for data analysis.","However, the usage of these methods in survey sampling including small area estimation is still very limited.","Therefore, we propose a predictor supported by these algorithms which can be used to predict any population or subpopulation characteristics based on cross-sectional and longitudinal data.","Machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means that they have very good properties in case of strong departures from the classic assumptions.","Therefore, we analyse the performance of our proposal under a different set-up, in our opinion of greater importance in real-life surveys.","We study only small departures from the assumed model, to show that our proposal is a good alternative in this case as well, even in comparison with optimal methods under the model.","What is more, we propose the method of the accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods, where the accuracy is measured as in survey sampling practice.","The solution of this problem is indicated in the literature as one of the key issues in integration of these approaches.","The simulation studies are based on a real, longitudinal dataset, freely available from the Polish Local Data Bank, where the prediction problem of subpopulation characteristics in the last period, with \"borrowing strength\" from other subpopulations and time periods, is considered."],"url":"http://arxiv.org/abs/2402.07521v1","category":"stat.ME"}
{"created":"2024-02-12 09:38:42","title":"Physics-informed machine learning as a kernel method","abstract":"Physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models. In this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency. We prove that for linear differential priors, the problem can be formulated as a kernel regression task. Taking advantage of kernel theory, we derive convergence rates for the minimizer of the regularized risk and show that it converges at least at the Sobolev minimax rate. However, faster rates can be achieved, depending on the physical error. This principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators.","sentences":["Physics-informed machine learning combines the expressiveness of data-based approaches with the interpretability of physical models.","In this context, we consider a general regression problem where the empirical risk is regularized by a partial differential equation that quantifies the physical inconsistency.","We prove that for linear differential priors, the problem can be formulated as a kernel regression task.","Taking advantage of kernel theory, we derive convergence rates for the minimizer of the regularized risk and show that it converges at least at the Sobolev minimax rate.","However, faster rates can be achieved, depending on the physical error.","This principle is illustrated with a one-dimensional example, supporting the claim that regularizing the empirical risk with physical information can be beneficial to the statistical performance of estimators."],"url":"http://arxiv.org/abs/2402.07514v1","category":"cs.AI"}
{"created":"2024-02-12 09:35:13","title":"The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese","abstract":"In the field of spoken language understanding, systems like Whisper and Multilingual Massive Speech (MMS) have shown state-of-the-art performances. This study is dedicated to a comprehensive exploration of the Whisper and MMS systems, with a focus on assessing biases in automatic speech recognition (ASR) inherent to casual conversation speech specific to the Portuguese language. Our investigation encompasses various categories, including gender, age, skin tone color, and geo-location. Alongside traditional ASR evaluation metrics such as Word Error Rate (WER), we have incorporated p-value statistical significance for gender bias analysis. Furthermore, we extensively examine the impact of data distribution and empirically show that oversampling techniques alleviate such stereotypical biases. This research represents a pioneering effort in quantifying biases in the Portuguese language context through the application of MMS and Whisper, contributing to a better understanding of ASR systems' performance in multilingual settings.","sentences":["In the field of spoken language understanding, systems like Whisper and Multilingual Massive Speech (MMS) have shown state-of-the-art performances.","This study is dedicated to a comprehensive exploration of the Whisper and MMS systems, with a focus on assessing biases in automatic speech recognition (ASR) inherent to casual conversation speech specific to the Portuguese language.","Our investigation encompasses various categories, including gender, age, skin tone color, and geo-location.","Alongside traditional ASR evaluation metrics such as Word Error Rate (WER), we have incorporated p-value statistical significance for gender bias analysis.","Furthermore, we extensively examine the impact of data distribution and empirically show that oversampling techniques alleviate such stereotypical biases.","This research represents a pioneering effort in quantifying biases in the Portuguese language context through the application of MMS and Whisper, contributing to a better understanding of ASR systems' performance in multilingual settings."],"url":"http://arxiv.org/abs/2402.07513v1","category":"cs.CL"}
{"created":"2024-02-12 09:31:21","title":"Secret Collusion Among Generative AI Agents","abstract":"Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.","sentences":["Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks.","This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination.","Modern steganographic techniques could render such dynamics hard to detect.","In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature.","We study incentives for the use of steganography, and propose a variety of mitigation measures.","Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion.","We provide extensive empirical results across a range of contemporary LLMs.","While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities.","We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models."],"url":"http://arxiv.org/abs/2402.07510v1","category":"cs.AI"}
{"created":"2024-02-12 09:28:16","title":"Clustering Dynamics for Improved Speed Prediction Deriving from Topographical GPS Registrations","abstract":"A persistent challenge in the field of Intelligent Transportation Systems is to extract accurate traffic insights from geographic regions with scarce or no data coverage. To this end, we propose solutions for speed prediction using sparse GPS data points and their associated topographical and road design features. Our goal is to investigate whether we can use similarities in the terrain and infrastructure to train a machine learning model that can predict speed in regions where we lack transportation data. For this we create a Temporally Orientated Speed Dictionary Centered on Topographically Clustered Roads, which helps us to provide speed correlations to selected feature configurations. Our results show qualitative and quantitative improvement over new and standard regression methods. The presented framework provides a fresh perspective on devising strategies for missing data traffic analysis.","sentences":["A persistent challenge in the field of Intelligent Transportation Systems is to extract accurate traffic insights from geographic regions with scarce or no data coverage.","To this end, we propose solutions for speed prediction using sparse GPS data points and their associated topographical and road design features.","Our goal is to investigate whether we can use similarities in the terrain and infrastructure to train a machine learning model that can predict speed in regions where we lack transportation data.","For this we create a Temporally Orientated Speed Dictionary Centered on Topographically Clustered Roads, which helps us to provide speed correlations to selected feature configurations.","Our results show qualitative and quantitative improvement over new and standard regression methods.","The presented framework provides a fresh perspective on devising strategies for missing data traffic analysis."],"url":"http://arxiv.org/abs/2402.07507v1","category":"cs.AI"}
{"created":"2024-02-12 18:56:53","title":"MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO","abstract":"Low-light conditions and occluded scenarios impede object detection in real-world Internet of Things (IoT) applications like autonomous vehicles and security systems. While advanced machine learning models strive for accuracy, their computational demands clash with the limitations of resource-constrained devices, hampering real-time performance. In our current research, we tackle this challenge, by introducing \"YOLO Phantom\", one of the smallest YOLO models ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block, achieving comparable accuracy to the latest YOLOv8n model while simultaneously reducing both parameters and model size by 43%, resulting in a significant 19% reduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages transfer learning on our multimodal RGB-infrared dataset to address low-light and occlusion issues, equipping it with robust vision under adverse conditions. Its real-world efficacy is demonstrated on an IoT platform with advanced low-light and RGB cameras, seamlessly connecting to an AWS-based notification endpoint for efficient real-time object detection. Benchmarks reveal a substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB detection, respectively, compared to the baseline YOLOv8n model. For community contribution, both the code and the multimodal dataset are available on GitHub.","sentences":["Low-light conditions","and","occluded scenarios impede object detection in real-world Internet of Things (IoT) applications like autonomous vehicles and security systems.","While advanced machine learning models strive for accuracy, their computational demands clash with the limitations of resource-constrained devices, hampering real-time performance.","In our current research, we tackle this challenge, by introducing \"YOLO Phantom\", one of the smallest YOLO models ever conceived.","YOLO Phantom utilizes the novel Phantom Convolution block, achieving comparable accuracy to the latest YOLOv8n model while simultaneously reducing both parameters and model size by 43%, resulting in a significant 19% reduction in Giga Floating Point Operations (GFLOPs).","YOLO Phantom leverages transfer learning on our multimodal RGB-infrared dataset to address low-light and occlusion issues, equipping it with robust vision under adverse conditions.","Its real-world efficacy is demonstrated on an IoT platform with advanced low-light and RGB cameras, seamlessly connecting to an AWS-based notification endpoint for efficient real-time object detection.","Benchmarks reveal a substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB detection, respectively, compared to the baseline YOLOv8n model.","For community contribution, both the code and the multimodal dataset are available on GitHub."],"url":"http://arxiv.org/abs/2402.07894v1","category":"cs.CV"}
{"created":"2024-02-12 18:56:13","title":"The TESS-Keck Survey XXI: 13 New Planets and Homogeneous Properties for 21 Subgiant Systems","abstract":"We present a dedicated transit and radial velocity survey of planets orbiting subgiant stars observed by the TESS Mission. Using $\\sim$$16$ nights on Keck/HIRES, we confirm and characterize $12$ new transiting planets -- $\\rm TOI-329\\,b$, $\\rm HD\\,39688\\,b$ ($\\rm TOI-480$), $\\rm TOI-603\\,b$, $\\rm TOI-1199\\,b$, $\\rm TOI-1294\\,b$, $\\rm TOI-1439\\,b$, $\\rm TOI-1605\\,b$, $\\rm TOI-1828\\,b$, $\\rm HD\\,148193\\,b$ ($\\rm TOI-1836$), $\\rm TOI-1885\\,b$, $\\rm HD\\,83342\\,b$ ($\\rm TOI-1898$), $\\rm TOI-2019\\,b$ -- and provide updated properties for 9 previously confirmed TESS subgiant systems ($\\rm TOI-197$, $\\rm TOI-954$, $\\rm TOI-1181$, $\\rm TOI-1296$, $\\rm TOI-1298$, $\\rm TOI-1601$, $\\rm TOI-1736$, $\\rm TOI-1842$, $\\rm TOI-2145$). We also report the discovery of an outer, non-transiting planet, $\\rm TOI-1294\\,c$ ($P=160.1\\pm2.5$ days, $M_{\\mathrm{p}}=148.3^{+18.2}_{-16.4} \\,M_{\\oplus}$), and three additional stars with long-term RV trends. We find that at least $19\\pm8\\%$ of subgiants in our sample of $21$ stars have outer companions, comparable to main-sequence stars. We perform a homogeneous analysis of the stars and planets in the sample, with median uncertainties of $3\\%$, $8\\%$ and $15\\%$ for planet radii, masses and ages, doubling the number of known planets orbiting subgiant stars with bulk densities measured to better than $10\\%$. We observe a dearth of giant planets around evolved stars with short orbital periods, consistent with tidal dissipation theories that predict the rapid inspiral of planets as their host stars leave the main sequence. We note the possible evidence for two distinct classes of hot Jupiter populations, indicating multiple formation channels to explain the observed distributions around evolved stars. Finally, continued RV monitoring of planets in this sample will provide a more comprehensive understanding of demographics for evolved planetary systems.","sentences":["We present a dedicated transit and radial velocity survey of planets orbiting subgiant stars observed by the TESS Mission.","Using $\\sim$$16$ nights on Keck/HIRES, we confirm and characterize $12$ new transiting planets -- $\\rm TOI-329\\,b$, $\\rm HD\\,39688\\,b$ ($\\rm TOI-480$), $\\rm TOI-603\\,b$, $\\rm TOI-1199\\,b$, $\\rm TOI-1294\\,b$, $\\rm TOI-1439\\,b$, $\\rm TOI-1605\\,b$, $\\rm TOI-1828\\,b$, $\\rm HD\\,148193\\,b$ ($\\rm TOI-1836$), $\\rm TOI-1885\\,b$, $\\rm HD\\,83342\\,b$ ($\\rm TOI-1898$), $\\rm TOI-2019\\,b$ -- and provide updated properties for 9 previously confirmed TESS subgiant systems ($\\rm TOI-197$, $\\rm TOI-954$, $\\rm TOI-1181$, $\\rm TOI-1296$, $\\rm TOI-1298$, $\\rm TOI-1601$, $\\rm TOI-1736$, $\\rm TOI-1842$, $\\rm TOI-2145$).","We also report the discovery of an outer, non-transiting planet, $\\rm TOI-1294\\,c$ ($P=160.1\\pm2.5$ days, $M_{\\mathrm{p}}=148.3^{+18.2}_{-16.4} \\,M_{\\oplus}$), and three additional stars with long-term RV trends.","We find that at least $19\\pm8\\%$ of subgiants in our sample of $21$ stars have outer companions, comparable to main-sequence stars.","We perform a homogeneous analysis of the stars and planets in the sample, with median uncertainties of $3\\%$, $8\\%$ and $15\\%$ for planet radii, masses and ages, doubling the number of known planets orbiting subgiant stars with bulk densities measured to better than $10\\%$. We observe a dearth of giant planets around evolved stars with short orbital periods, consistent with tidal dissipation theories that predict the rapid inspiral of planets as their host stars leave the main sequence.","We note the possible evidence for two distinct classes of hot Jupiter populations, indicating multiple formation channels to explain the observed distributions around evolved stars.","Finally, continued RV monitoring of planets in this sample will provide a more comprehensive understanding of demographics for evolved planetary systems."],"url":"http://arxiv.org/abs/2402.07893v1","category":"astro-ph.EP"}
{"created":"2024-02-12 18:50:41","title":"Hybrid acousto-optical swing-up preparation of exciton and biexciton states in a quantum dot","abstract":"Recent years brought the idea of hybrid systems, in which quantum degrees of freedom, due to controlled couplings, allow the transfer of quantum information and may lead to the emergence of new generation devices. Due to the universal coupling with all solid-state systems and compatibility with miniaturization, acoustic fields will play an important role in interfacing such components. Optically active quantum dots (QDs) are at the forefront of systems for applications in quantum technologies and their multiple available interfaces make them a great component of hybrid systems. QDs generate polarization-entangled photon pairs, however deterministic and high-fidelity preparation of the state is needed. All resonant schemes need filtering to distinguish emitted photons from the excitation pulse, which limits the photon yield significantly. Thus, non-resonant excitation methods are needed like the recently proposed and successful swing-up scheme. Here, we propose a hybrid acousto-optical version of this non-resonant scheme to prepare exciton and biexciton states. We show that using acoustic modulation allows selectively exciting either exciton or biexciton states with just one mode of vibration and one optical pulse or vice versa: acoustic pulse during detuned optical driving. Thus, either of the fields can act as a trigger controlling the evolution. Further, we evaluate the impact of phonon decoherence at finite temperatures for two types of application-relevant QDs, InAs/GaAs and GaAs/AlGaAs, and find that for GaAs QDs exciton preparation can be almost decoherence-free even at liquid nitrogen temperatures already with currently available acoustic modulation frequencies. This approach may pave the way for generating entanglement between an emitter and a quantum acoustic mode when using the acoustic mode as a trigger for the transitions.","sentences":["Recent years brought the idea of hybrid systems, in which quantum degrees of freedom, due to controlled couplings, allow the transfer of quantum information and may lead to the emergence of new generation devices.","Due to the universal coupling with all solid-state systems and compatibility with miniaturization, acoustic fields will play an important role in interfacing such components.","Optically active quantum dots (QDs) are at the forefront of systems for applications in quantum technologies and their multiple available interfaces make them a great component of hybrid systems.","QDs generate polarization-entangled photon pairs, however deterministic and high-fidelity preparation of the state is needed.","All resonant schemes need filtering to distinguish emitted photons from the excitation pulse, which limits the photon yield significantly.","Thus, non-resonant excitation methods are needed like the recently proposed and successful swing-up scheme.","Here, we propose a hybrid acousto-optical version of this non-resonant scheme to prepare exciton and biexciton states.","We show that using acoustic modulation allows selectively exciting either exciton or biexciton states with just one mode of vibration and one optical pulse or vice versa: acoustic pulse during detuned optical driving.","Thus, either of the fields can act as a trigger controlling the evolution.","Further, we evaluate the impact of phonon decoherence at finite temperatures for two types of application-relevant QDs, InAs/GaAs and GaAs/AlGaAs, and find that for GaAs QDs exciton preparation can be almost decoherence-free even at liquid nitrogen temperatures already with currently available acoustic modulation frequencies.","This approach may pave the way for generating entanglement between an emitter and a quantum acoustic mode when using the acoustic mode as a trigger for the transitions."],"url":"http://arxiv.org/abs/2402.07887v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-12 18:50:07","title":"Distributed Anomaly Detection in Modern Power Systems: A Penalty-based Mitigation Approach","abstract":"The evolving landscape of electric power networks, influenced by the integration of distributed energy resources require the development of novel power system monitoring and control architectures. This paper develops algorithm to monitor and detect anomalies of different parts of a power system that cannot be measured directly, by applying neighboring measurements and a dynamic probing technique in a distributed fashion. Additionally, the proposed method accurately assesses the severity of the anomaly. A decision-making algorithm is introduced to effectively penalize anomalous agents, ensuring vigilant oversight of the entire power system's functioning. Simulation results show the efficacy of algorithms in distributed anomaly detection and mitigation.","sentences":["The evolving landscape of electric power networks, influenced by the integration of distributed energy resources require the development of novel power system monitoring and control architectures.","This paper develops algorithm to monitor and detect anomalies of different parts of a power system that cannot be measured directly, by applying neighboring measurements and a dynamic probing technique in a distributed fashion.","Additionally, the proposed method accurately assesses the severity of the anomaly.","A decision-making algorithm is introduced to effectively penalize anomalous agents, ensuring vigilant oversight of the entire power system's functioning.","Simulation results show the efficacy of algorithms in distributed anomaly detection and mitigation."],"url":"http://arxiv.org/abs/2402.07884v1","category":"eess.SY"}
{"created":"2024-02-12 18:17:01","title":"An approximation algorithm for Maximum DiCut vs. Cut","abstract":"Goemans and Williamson designed a 0.878-approximation algorithm for Max-Cut in undirected graphs [JACM'95]. Khot, Kindler, Mosel, and O'Donnel showed that the approximation ratio of the Goemans-Williamson algorithm is optimal assuming Khot's Unique Games Conjecture [SICOMP'07]. In the problem of maximum cuts in directed graphs (Max-DiCut), in which we seek as many edges going from one particular side of the cut to the other, the situation is more complicated but the recent work of Brakensiek, Huang, Potechin, and Zwick showed that their 0.874-approximation algorithm is tight under the Unique Games Conjecture (up to a small delta)[FOCS'23].   We consider a promise version of the problem and design an SDP-based algorithm which, if given a directed graph G that has a directed cut of value rho, finds an undirected cut in G (ignoring edge directions) with value at least \\rho.","sentences":["Goemans and Williamson designed a 0.878-approximation algorithm for Max-Cut in undirected graphs [JACM'95].","Khot, Kindler, Mosel, and O'Donnel showed that the approximation ratio of the Goemans-Williamson algorithm is optimal assuming","Khot's Unique Games Conjecture","[SICOMP'07].","In the problem of maximum cuts in directed graphs (Max-DiCut), in which we seek as many edges going from one particular side of the cut to the other, the situation is more complicated but the recent work of Brakensiek, Huang, Potechin, and Zwick showed that their 0.874-approximation algorithm is tight under the Unique Games Conjecture (up to a small delta)[FOCS'23].   ","We consider a promise version of the problem and design an SDP-based algorithm which, if given a directed graph G that has a directed cut of value rho, finds an undirected cut in G (ignoring edge directions) with value at least \\rho."],"url":"http://arxiv.org/abs/2402.07863v1","category":"cs.DS"}
{"created":"2024-02-12 18:12:52","title":"TOI-1199\\:b and TOI-1273\\:b: Two new transiting hot Saturns detected and characterized with SOPHIE and TESS","abstract":"We report the characterization of two planet candidates detected by the Transiting Exoplanet Survey Satellite (TESS), TOI-1199\\:b and TOI-1273\\:b, with periods of 3.7 and 4.6\\,days, respectively. Follow-up observations for both targets, which include several ground-based light curves, confirmed the transit events. High-precision radial velocities from the SOPHIE spectrograph revealed signals at the expected frequencies and phases of the transiting candidates and allowed mass determinations with a precision of 8.4\\% and 6.7\\% for TOI-1199\\:b and TOI-1273\\:b, respectively. The planetary and orbital parameters were derived from a joint analysis of the radial velocities and photometric data. We find that the planets have masses of 0.239$\\,\\pm\\,$0.020\\,M$_{\\mathrm{J}}$ \\ and 0.222$\\,\\pm\\,$0.015\\,M$_{\\mathrm{J}}$ \\ and radii of 0.938$\\,\\pm\\,$0.025\\,R$_{\\mathrm{J}}$ \\ and 0.99$\\,\\pm\\,$0.22\\,R$_{\\mathrm{J}}$,\\ respectively. The grazing transit of TOI-1273\\:b translates to a larger uncertainty in its radius, and hence also in its bulk density, compared to TOI-1199\\:b. The inferred bulk densities of 0.358$\\,\\pm\\,$0.041\\,g\\,cm$^{-3}$ \\ and 0.28$\\,\\pm\\,$0.11\\,g\\,cm$^{-3}$ \\ are among the lowest known for exoplanets in this mass range, which, considering the brightness of the host stars ($V$$\\approx$11\\,mag), render them particularly amenable to atmospheric characterization via the transit spectroscopy technique. The better constraints on the parameters of TOI-1199\\:b provide a transmission spectroscopy metric of 134\\,$\\pm$\\,17, making it the better suited of the two planets for atmospheric studies.","sentences":["We report the characterization of two planet candidates detected by the Transiting Exoplanet Survey Satellite (TESS), TOI-1199\\:b and TOI-1273\\:b, with periods of 3.7 and 4.6\\,days, respectively.","Follow-up observations for both targets, which include several ground-based light curves, confirmed the transit events.","High-precision radial velocities from the SOPHIE spectrograph revealed signals at the expected frequencies and phases of the transiting candidates and allowed mass determinations with a precision of 8.4\\% and 6.7\\% for TOI-1199\\:b and TOI-1273\\:b, respectively.","The planetary and orbital parameters were derived from a joint analysis of the radial velocities and photometric data.","We find that the planets have masses of 0.239$\\,\\pm\\,$0.020\\,M$_{\\mathrm{J}}$ \\ and 0.222$\\,\\pm\\,$0.015\\,M$_{\\mathrm{J}}$ \\ and radii of 0.938$\\,\\pm\\,$0.025\\,R$_{\\mathrm{J}}$ \\ and 0.99$\\,\\pm\\,$0.22\\,R$_{\\mathrm{J}}$,\\ respectively.","The grazing transit of TOI-1273\\:b translates to a larger uncertainty in its radius, and hence also in its bulk density, compared to TOI-1199\\:b.","The inferred bulk densities of 0.358$\\,\\pm\\,$0.041\\,g\\,cm$^{-3}$ \\ and 0.28$\\,\\pm\\,$0.11\\,g\\,cm$^{-3}$ \\ are among the lowest known for exoplanets in this mass range, which, considering the brightness of the host stars ($V$$\\approx$11\\,mag), render them particularly amenable to atmospheric characterization via the transit spectroscopy technique.","The better constraints on the parameters of TOI-1199\\:b provide a transmission spectroscopy metric of 134\\,$\\pm$\\,17, making it the better suited of the two planets for atmospheric studies."],"url":"http://arxiv.org/abs/2402.07861v1","category":"astro-ph.EP"}
{"created":"2024-02-12 18:05:03","title":"Multiscale Neuroimaging Features for the Identification of Medication Class and Non-Responders in Mood Disorder Treatment","abstract":"In the clinical treatment of mood disorders, the complex behavioral symptoms presented by patients and variability of patient response to particular medication classes can create difficulties in providing fast and reliable treatment when standard diagnostic and prescription methods are used. Increasingly, the incorporation of physiological information such as neuroimaging scans and derivatives into the clinical process promises to alleviate some of the uncertainty surrounding this process. Particularly, if neural features can help to identify patients who may not respond to standard courses of anti-depressants or mood stabilizers, clinicians may elect to avoid lengthy and side-effect-laden treatments and seek out a different, more effective course that might otherwise not have been under consideration. Previously, approaches for the derivation of relevant neuroimaging features work at only one scale in the data, potentially limiting the depth of information available for clinical decision support. In this work, we show that the utilization of multi spatial scale neuroimaging features - particularly resting state functional networks and functional network connectivity measures - provide a rich and robust basis for the identification of relevant medication class and non-responders in the treatment of mood disorders. We demonstrate that the generated features, along with a novel approach for fast and automated feature selection, can support high accuracy rates in the identification of medication class and non-responders as well as the identification of novel, multi-scale biomarkers.","sentences":["In the clinical treatment of mood disorders, the complex behavioral symptoms presented by patients and variability of patient response to particular medication classes can create difficulties in providing fast and reliable treatment when standard diagnostic and prescription methods are used.","Increasingly, the incorporation of physiological information such as neuroimaging scans and derivatives into the clinical process promises to alleviate some of the uncertainty surrounding this process.","Particularly, if neural features can help to identify patients who may not respond to standard courses of anti-depressants or mood stabilizers, clinicians may elect to avoid lengthy and side-effect-laden treatments and seek out a different, more effective course that might otherwise not have been under consideration.","Previously, approaches for the derivation of relevant neuroimaging features work at only one scale in the data, potentially limiting the depth of information available for clinical decision support.","In this work, we show that the utilization of multi spatial scale neuroimaging features - particularly resting state functional networks and functional network connectivity measures - provide a rich and robust basis for the identification of relevant medication class and non-responders in the treatment of mood disorders.","We demonstrate that the generated features, along with a novel approach for fast and automated feature selection, can support high accuracy rates in the identification of medication class and non-responders as well as the identification of novel, multi-scale biomarkers."],"url":"http://arxiv.org/abs/2402.07858v1","category":"cs.LG"}
{"created":"2024-02-12 18:00:37","title":"The Physical Properties of Low Redshift FeLoBAL Quasars. IV. Optical-Near IR Spectral Energy Distributions and Near-IR Variability Properties","abstract":"We present the optical-near infrared spectral energy distributions (SED) and near infrared variability properties of 30 low-redshift iron low-ionization Broad Absorption Line quasars (FeLoBALQs) and matched samples of LoBALQs and unabsorbed quasars. Significant correlations between the SED properties and accretion rate indicators found among the unabsorbed comparison sample objects suggest an intrinsic origin for SED differences. A range of reddening likely mutes these correlations among the FeLoBAL quasars. The restframe optical-band reddening is correlated with the location of the outflow, suggesting a link between the outflows and the presence of dust. We analyzed WISE variability and provide a correction for photometry uncertainties in an appendix. We found an anticorrelation between the variability amplitude and inferred continuum emission region size, and suggest that as the origin of the anticorrelation between variability amplitude and luminosity typically observed in quasars. We found that the LoBALQ optical emission line and other parameters are more similar to those of the unabsorbed continuum sample objects than the FeLoBALQs. Thus, FeLoBAL quasars are a special population of objects. We interpret the results using an accretion-rate scenario for FeLoBAL quasars. The high accretion rate FeLoBAL quasars are radiating powerfully enough to drive a thick, high-velocity outflow. Quasars with intermediate accretion rates may have an outflow, but it is not sufficiently thick to include FeII absorption. Low accretion rate FeLoBAL outflows originate in absorption in a failing torus, no longer optically thick enough to reprocess radiation into the near-IR.","sentences":["We present the optical-near infrared spectral energy distributions (SED) and near infrared variability properties of 30 low-redshift iron low-ionization Broad Absorption Line quasars (FeLoBALQs) and matched samples of LoBALQs and unabsorbed quasars.","Significant correlations between the SED properties and accretion rate indicators found among the unabsorbed comparison sample objects suggest an intrinsic origin for SED differences.","A range of reddening likely mutes these correlations among the FeLoBAL quasars.","The restframe optical-band reddening is correlated with the location of the outflow, suggesting a link between the outflows and the presence of dust.","We analyzed WISE variability and provide a correction for photometry uncertainties in an appendix.","We found an anticorrelation between the variability amplitude and inferred continuum emission region size, and suggest that as the origin of the anticorrelation between variability amplitude and luminosity typically observed in quasars.","We found that the LoBALQ optical emission line and other parameters are more similar to those of the unabsorbed continuum sample objects than the FeLoBALQs.","Thus, FeLoBAL quasars are a special population of objects.","We interpret the results using an accretion-rate scenario for FeLoBAL quasars.","The high accretion rate FeLoBAL quasars are radiating powerfully enough to drive a thick, high-velocity outflow.","Quasars with intermediate accretion rates may have an outflow, but it is not sufficiently thick to include FeII absorption.","Low accretion rate FeLoBAL outflows originate in absorption in a failing torus, no longer optically thick enough to reprocess radiation into the near-IR."],"url":"http://arxiv.org/abs/2402.07855v1","category":"astro-ph.GA"}
{"created":"2024-02-12 17:59:26","title":"The Complexity of Algebraic Algorithms for LWE","abstract":"Arora & Ge introduced a noise-free polynomial system to compute the secret of a Learning With Errors (LWE) instance via linearization. Albrecht et al. later utilized the Arora-Ge polynomial model to study the complexity of Gr\\\"obner basis computations on LWE polynomial systems under the assumption of semi-regularity. In this paper we revisit the Arora-Ge polynomial and prove that it satisfies a genericity condition recently introduced by Caminata & Gorla, called being in generic coordinates. For polynomial systems in generic coordinates one can always estimate the complexity of DRL Gr\\\"obner basis computations in terms of the Castelnuovo-Mumford regularity and henceforth also via the Macaulay bound.   Moreover, we generalize the Gr\\\"obner basis algorithm of Semaev & Tenti to arbitrary polynomial systems with a finite degree of regularity. In particular, existence of this algorithm yields another approach to estimate the complexity of DRL Gr\\\"obner basis computations in terms of the degree of regularity. In practice, the degree of regularity of LWE polynomial systems is not known, though one can always estimate the lowest achievable degree of regularity. Consequently, from a designer's worst case perspective this approach yields sub-exponential complexity estimates for general, binary secret, and binary error LWE.   In recent works by Dachman-Soled et al. the hardness of LWE in the presence of side information was analyzed. Utilizing their framework we discuss how hints can be incorporated into LWE polynomial systems and how they affect the complexity of Gr\\\"obner basis computations.","sentences":["Arora & Ge introduced a noise-free polynomial system to compute the secret of a Learning With Errors (LWE) instance via linearization.","Albrecht et al. later utilized the Arora-Ge polynomial model to study the complexity of Gr\\\"obner basis computations on LWE polynomial systems under the assumption of semi-regularity.","In this paper we revisit the Arora-Ge polynomial and prove that it satisfies a genericity condition recently introduced by Caminata & Gorla, called being in generic coordinates.","For polynomial systems in generic coordinates one can always estimate the complexity of DRL Gr\\\"obner basis computations in terms of the Castelnuovo-Mumford regularity and henceforth also via the Macaulay bound.   ","Moreover, we generalize the Gr\\\"obner basis algorithm of Semaev & Tenti to arbitrary polynomial systems with a finite degree of regularity.","In particular, existence of this algorithm yields another approach to estimate the complexity of DRL Gr\\\"obner basis computations in terms of the degree of regularity.","In practice, the degree of regularity of LWE polynomial systems is not known, though one can always estimate the lowest achievable degree of regularity.","Consequently, from a designer's worst case perspective this approach yields sub-exponential complexity estimates for general, binary secret, and binary error LWE.   ","In recent works by Dachman-Soled et al.","the hardness of LWE in the presence of side information was analyzed.","Utilizing their framework we discuss how hints can be incorporated into LWE polynomial systems and how they affect the complexity of Gr\\\"obner basis computations."],"url":"http://arxiv.org/abs/2402.07852v1","category":"cs.CR"}
{"created":"2024-02-12 17:57:54","title":"Triply Periodic Helical Weaves","abstract":"Weaving typically involves forming a sufrace by interlacing fibers into a mechanically stable arrangement, effectively making a two-dimensional object out of one-dimensional objects. Moorish Fretwork involves interweaving solid helical elements into mechanically stable two-dimensional arrangements by exploiting the helices' screw symmetry. A three-dimensional extension of this idea was demonstrated by Alexandru Usineviciu at Bridges in 2015. Here we expand the idea further by considering cases informed by invariant cylindrical rod packing, and cases related to geodesics of the gyroid. Simulations and/or physical models of nineteen triply periodic arrangements of interwoven helices are shown, with physical models demonstrated for eight.","sentences":["Weaving typically involves forming a sufrace by interlacing fibers into a mechanically stable arrangement, effectively making a two-dimensional object out of one-dimensional objects.","Moorish Fretwork involves interweaving solid helical elements into mechanically stable two-dimensional arrangements by exploiting the helices' screw symmetry.","A three-dimensional extension of this idea was demonstrated by Alexandru Usineviciu at Bridges in 2015.","Here we expand the idea further by considering cases informed by invariant cylindrical rod packing, and cases related to geodesics of the gyroid.","Simulations and/or physical models of nineteen triply periodic arrangements of interwoven helices are shown, with physical models demonstrated for eight."],"url":"http://arxiv.org/abs/2402.07849v1","category":"math.GT"}
{"created":"2024-02-12 17:52:51","title":"Stabilizer entropy of quantum tetrahedra","abstract":"How complex is the structure of quantum geometry? In several approaches, the spacetime atoms are obtained by the SU(2) intertwiner called quantum tetrahedron. The complexity of this construction has a concrete consequence in recent efforts to simulate such models and toward experimental demonstrations of quantum gravity effects. There are, therefore, both a computational and an experimental complexity inherent to this class of models. In this paper, we study this complexity under the lens of stabilizer entropy (SE). We calculate the SE of the gauge-invariant basis states and its average in the SU(2) gauge invariant subspace. We find that the states of definite volume are singled out by the (near) maximal SE and give precise bounds to the verification protocols for experimental demonstrations on available quantum computers.","sentences":["How complex is the structure of quantum geometry?","In several approaches, the spacetime atoms are obtained by the SU(2) intertwiner called quantum tetrahedron.","The complexity of this construction has a concrete consequence in recent efforts to simulate such models and toward experimental demonstrations of quantum gravity effects.","There are, therefore, both a computational and an experimental complexity inherent to this class of models.","In this paper, we study this complexity under the lens of stabilizer entropy (SE).","We calculate the SE of the gauge-invariant basis states and its average in the SU(2) gauge invariant subspace.","We find that the states of definite volume are singled out by the (near) maximal SE and give precise bounds to the verification protocols for experimental demonstrations on available quantum computers."],"url":"http://arxiv.org/abs/2402.07843v1","category":"hep-th"}
{"created":"2024-02-12 17:52:47","title":"Exact lower bound of the uncertainty principle product for the harmonic oscillator with position-momentum coupling","abstract":"We show that the uncertainty principle product for the position and momentum operators for a system described by the Hamiltonian $ \\hat H= \\frac{\\hat{p}^2}{2m} +\\frac{1}{2} m \\omega^2 \\hat{x}^2+\\frac{\\mu}{2}(\\hat x \\hat p+ \\hat p \\hat x)$ where $\\mu<\\omega$ reads $\\Delta x \\Delta p\\ge\\frac{\\hbar \\omega}{2\\sqrt{\\omega^2-\\mu^2}}$. All the values bellow this lower bound are thus quantum-mechanically forbidden. We construct the annihilation and creation operators for this system and we calculate the expectation values of the operators $\\hat p$ and $\\hat x$ with respect to the corresponding coherent states.","sentences":["We show that the uncertainty principle product for the position and momentum operators for a system described by the Hamiltonian $ \\hat H= \\frac{\\hat{p}^2}{2m} +\\frac{1}{2} m \\omega^2 \\hat{x}^2+\\frac{\\mu}{2}(\\hat x \\hat p+ \\hat p \\hat x)$ where $\\mu<\\omega$ reads $\\Delta x \\Delta p\\ge\\frac{\\hbar \\omega}{2\\sqrt{\\omega^2-\\mu^2}}$. All the values bellow this lower bound are thus quantum-mechanically forbidden.","We construct the annihilation and creation operators for this system and we calculate the expectation values of the operators $\\hat p$ and $\\hat x$ with respect to the corresponding coherent states."],"url":"http://arxiv.org/abs/2402.07842v1","category":"quant-ph"}
{"created":"2024-02-12 17:50:56","title":"Towards Meta-Pruning via Optimal Transport","abstract":"Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We benchmark our results for various networks on commonly used datasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that the proposed Intra-Fusion approach invigorates exploration into a fresh alternative to the predominant compression approaches. Our code is available here: https://github.com/alexandertheus/Intra-Fusion.","sentences":["Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts.","This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm.","Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure.","Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation.","Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   ","Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance.","We benchmark our results for various networks on commonly used datasets such as CIFAR-10, CIFAR-100, and ImageNet.","More broadly, we hope that the proposed Intra-Fusion approach invigorates exploration into a fresh alternative to the predominant compression approaches.","Our code is available here: https://github.com/alexandertheus/Intra-Fusion."],"url":"http://arxiv.org/abs/2402.07839v2","category":"cs.CV"}
{"created":"2024-02-12 17:28:32","title":"Random optimization problems at fixed temperatures","abstract":"This article considers a class of disordered mean-field combinatorial optimization problems. We focus on the Gibbs measure, where the inverse temperature does not vary with the size of the graph and the edge weights are sampled from a general distribution under mild assumptions. Our results consist of the Law of Large Numbers and Central Limit Theorems for the log-partition function, the weight of a typical configuration, and the Gibbs average in both quenched and annealed forms. We also derive quenched Poisson convergence for the size of the intersection of two independent samples, yielding replica symmetry of the model. Applications cover popular models from the literature, such as the Minimal Matching Problem, Traveling Salesman Problem, and Minimal Spanning Tree Problem, on a sequence of deterministic and random dense block graphs of increasing size.","sentences":["This article considers a class of disordered mean-field combinatorial optimization problems.","We focus on the Gibbs measure, where the inverse temperature does not vary with the size of the graph and the edge weights are sampled from a general distribution under mild assumptions.","Our results consist of the Law of Large Numbers and Central Limit Theorems for the log-partition function, the weight of a typical configuration, and the Gibbs average in both quenched and annealed forms.","We also derive quenched Poisson convergence for the size of the intersection of two independent samples, yielding replica symmetry of the model.","Applications cover popular models from the literature, such as the Minimal Matching Problem, Traveling Salesman Problem, and Minimal Spanning Tree Problem, on a sequence of deterministic and random dense block graphs of increasing size."],"url":"http://arxiv.org/abs/2402.07825v1","category":"math.PR"}
{"created":"2024-02-12 17:27:32","title":"XZ-type Tanner-graph-recursive-expansion code","abstract":"Quantum stabilizer codes face the problem of low coding rate. In this letter, we propose a new class of quantum stabilizer codes called XZ-type Tanner-graph-recursive-expansion code. Though this code still have zero asymptotic coding rate, its coding rate tends to zero extremely slowly with the growth of code length. Under the same code length, its coding rate is much higher than that of surface code. We prove that the code distance of XZ-type Tanner-graph-recursive-expansion code is $O(log(N))$. Moreover, the code capacity noise threshold is around 0.078, which is obtained by fully decoupled belief propagation decoder. This letter shows that the idea of recursively expanding Tanner graph might have potential to construct quantum codes with better performance.","sentences":["Quantum stabilizer codes face the problem of low coding rate.","In this letter, we propose a new class of quantum stabilizer codes called XZ-type Tanner-graph-recursive-expansion code.","Though this code still have zero asymptotic coding rate, its coding rate tends to zero extremely slowly with the growth of code length.","Under the same code length, its coding rate is much higher than that of surface code.","We prove that the code distance of XZ-type Tanner-graph-recursive-expansion code is $O(log(N))$. Moreover, the code capacity noise threshold is around 0.078, which is obtained by fully decoupled belief propagation decoder.","This letter shows that the idea of recursively expanding Tanner graph might have potential to construct quantum codes with better performance."],"url":"http://arxiv.org/abs/2402.07823v1","category":"quant-ph"}
{"created":"2024-02-12 17:25:23","title":"On Computationally Efficient Multi-Class Calibration","abstract":"Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$? Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.   Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for all downstream decision makers who want to use the predictor for binary classification problems of the form: does the label belong to a subset $T \\subseteq [k]$: e.g. is this an image of an animal? It ensures that the probabilities predicted by summing the probabilities assigned to labels in $T$ are close to some perfectly calibrated binary predictor for that task. We also show that natural strengthenings of our definition are computationally hard to achieve: they run into information theoretic barriers or computational intractability. Underlying both our upper and lower bounds is a tight connection that we prove between multi-class calibration and the well-studied problem of agnostic learning in the (standard) binary prediction setting.","sentences":["Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels.","In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$?","Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.   ","Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for all downstream decision makers who want to use the predictor for binary classification problems of the form: does the label belong to a subset $T \\subseteq","[k]$: e.g. is this an image of an animal?","It ensures that the probabilities predicted by summing the probabilities assigned to labels in $T$ are close to some perfectly calibrated binary predictor for that task.","We also show that natural strengthenings of our definition are computationally hard to achieve: they run into information theoretic barriers or computational intractability.","Underlying both our upper and lower bounds is a tight connection that we prove between multi-class calibration and the well-studied problem of agnostic learning in the (standard) binary prediction setting."],"url":"http://arxiv.org/abs/2402.07821v1","category":"cs.LG"}
{"created":"2024-02-12 17:24:44","title":"First-Order Phase Transition in Perovskites Pr$_{0.67}$Sr$_{0.33}$MnO$_{3}$ - Magneto-Caloric Properties -- Effect of Multi-Spin Interaction","abstract":"We show by extensive Monte Carlo simulations that we need a multi-spin interaction in addition to pairwise interactions in order to reproduce the temperature dependence of the experimental magnetization observed in the perovskite compound Pr$_{0.67}$Sr$_{0.33}$MnO$_{3}$. The multi-spin interaction is introduced in the Hamiltonian as follows: each spin interacts simultaneously with its four nearest-neighbors. It does not have the reversal invariance as in a pairwise interaction where reversing the directions of two spins leaves the interaction energy invariant. As a consequence, it competes with the pairwise interactions between magnetic ions. The multi-spin interaction allows the sample magnetization $M$ to increase, to decrease or to have a plateau with increasing $T$. In this paper we show that $M$ increases with increasing $T$ before making a vertical fall at the transition temperature $T_C$, in contrast to the usual decrease of $M$ with increasing $T$ in most of magnetic systems. This result is in an excellent agreement with the experimental data observed in Pr$_{0.67}$Sr$_{0.33}$MnO$_{3}$. Furthermore, we show by the energy histogram taken at $T_C$ that the transition is clearly of first order. We also calculate the magnetic entropy change $|\\Delta S_m|$ and the Relative Cooling Power (RCP) by using the set of curves of $M$ obtained under an applied magnetic field $H$ varying from 0 to 5 Tesla across the transition temperature region. We obtain a good agreement with experiments on $|\\Delta S_m|$ and the values of RCP. This perovskite compound has a good potential in refrigeration application due to its high RCP.","sentences":["We show by extensive Monte Carlo simulations that we need a multi-spin interaction in addition to pairwise interactions in order to reproduce the temperature dependence of the experimental magnetization observed in the perovskite compound Pr$_{0.67}$Sr$_{0.33}$MnO$_{3}$.","The multi-spin interaction is introduced in the Hamiltonian as follows: each spin interacts simultaneously with its four nearest-neighbors.","It does not have the reversal invariance as in a pairwise interaction where reversing the directions of two spins leaves the interaction energy invariant.","As a consequence, it competes with the pairwise interactions between magnetic ions.","The multi-spin interaction allows the sample magnetization $M$ to increase, to decrease or to have a plateau with increasing $T$. In this paper we show that $M$ increases with increasing $T$ before making a vertical fall at the transition temperature $T_C$, in contrast to the usual decrease of $M$ with increasing $T$ in most of magnetic systems.","This result is in an excellent agreement with the experimental data observed in Pr$_{0.67}$Sr$_{0.33}$MnO$_{3}$.","Furthermore, we show by the energy histogram taken at $T_C$ that the transition is clearly of first order.","We also calculate the magnetic entropy change $|\\Delta S_m|$ and the Relative Cooling Power (RCP) by using the set of curves of $M$ obtained under an applied magnetic field $H$ varying from 0 to 5 Tesla across the transition temperature region.","We obtain a good agreement with experiments on $|\\Delta S_m|$ and the values of RCP.","This perovskite compound has a good potential in refrigeration application due to its high RCP."],"url":"http://arxiv.org/abs/2402.07820v1","category":"cond-mat.str-el"}
{"created":"2024-02-12 17:13:02","title":"Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation","abstract":"Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation. This problem can be ill-posed, however, since many different source distributions might produce the same distribution of data-consistent simulations. To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution, i.e., prioritizes retaining as much uncertainty as possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein distance to measure the discrepancy between the dataset and simulations - and thus suitable for simulators with intractable likelihoods. We benchmark our method on several tasks, and show that it can recover source distributions with substantially higher entropy without sacrificing the fidelity of the simulations. Finally, to demonstrate the utility of our approach, we infer source distributions for parameters of the Hodgkin-Huxley neuron model from experimental datasets with thousands of measurements. In summary, we propose a principled framework for inferring unique source distributions of scientific simulator parameters while retaining as much uncertainty as possible.","sentences":["Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation.","This problem can be ill-posed, however, since many different source distributions might produce the same distribution of data-consistent simulations.","To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution, i.e., prioritizes retaining as much uncertainty as possible.","Our method is purely sample-based - leveraging the Sliced-Wasserstein distance to measure the discrepancy between the dataset and simulations - and thus suitable for simulators with intractable likelihoods.","We benchmark our method on several tasks, and show that it can recover source distributions with substantially higher entropy without sacrificing the fidelity of the simulations.","Finally, to demonstrate the utility of our approach, we infer source distributions for parameters of the Hodgkin-Huxley neuron model from experimental datasets with thousands of measurements.","In summary, we propose a principled framework for inferring unique source distributions of scientific simulator parameters while retaining as much uncertainty as possible."],"url":"http://arxiv.org/abs/2402.07808v1","category":"cs.LG"}
{"created":"2024-02-12 16:59:06","title":"Tuning-Free Stochastic Optimization","abstract":"Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of \"tuning-free\" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently well-behaved. For the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of SGD that matches the best-known high-probability convergence rate for tuned SGD at only an additional polylogarithmic cost. However, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned SGD with high probability.","sentences":["Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive.","This creates a need for algorithms that can tune themselves on-the-fly.","We formalize the notion of \"tuning-free\" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters.","We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD).","When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms.","We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible.","We discuss conditions under which tuning-free optimization is possible even over unbounded domains.","In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently well-behaved.","For the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of SGD that matches the best-known high-probability convergence rate for tuned SGD at only an additional polylogarithmic cost.","However, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned SGD with high probability."],"url":"http://arxiv.org/abs/2402.07793v1","category":"math.OC"}
{"created":"2024-02-12 16:55:19","title":"From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration","abstract":"The assessment of binary classifier performance traditionally centers on discriminative ability using metrics, such as accuracy. However, these metrics often disregard the model's inherent uncertainty, especially when dealing with sensitive decision-making domains, such as finance or healthcare. Given that model-predicted scores are commonly seen as event probabilities, calibration is crucial for accurate interpretation. In our study, we analyze the sensitivity of various calibration measures to score distortions and introduce a refined metric, the Local Calibration Score. Comparing recalibration methods, we advocate for local regressions, emphasizing their dual role as effective recalibration tools and facilitators of smoother visualizations. We apply these findings in a real-world scenario using Random Forest classifier and regressor to predict credit default while simultaneously measuring calibration during performance optimization.","sentences":["The assessment of binary classifier performance traditionally centers on discriminative ability using metrics, such as accuracy.","However, these metrics often disregard the model's inherent uncertainty, especially when dealing with sensitive decision-making domains, such as finance or healthcare.","Given that model-predicted scores are commonly seen as event probabilities, calibration is crucial for accurate interpretation.","In our study, we analyze the sensitivity of various calibration measures to score distortions and introduce a refined metric, the Local Calibration Score.","Comparing recalibration methods, we advocate for local regressions, emphasizing their dual role as effective recalibration tools and facilitators of smoother visualizations.","We apply these findings in a real-world scenario using Random Forest classifier and regressor to predict credit default while simultaneously measuring calibration during performance optimization."],"url":"http://arxiv.org/abs/2402.07790v1","category":"cs.LG"}
{"created":"2024-02-12 18:58:01","title":"A holographic mobile-based application for practicing pronunciation of basic English vocabulary for Spanish speaking children","abstract":"This paper describes a holographic mobile-based application designed to help Spanish-speaking children to practice the pronunciation of basic English vocabulary words. The mastery of vocabulary is a fundamental step when learning a language but is often perceived as boring. Producing the correct pronunciation is frequently regarded as the most difficult and complex skill for new learners of English. In order to address these problems this research takes advantage of the power of multi-channel stimuli (sound, image and interaction) in a mobilebased hologram application in order to motivate students and improve their experience of practicing. We adapted the prize-winning HolograFX game and developed a new mobile application to help practice English pronunciation. A 3D holographic robot that acts as a virtual teacher interacts via voice with the children. To test the tool we carried out an experiment with 70 Spanish pre-school children divided into three classes, the control group using traditional methods such as images in books and on the blackboard, and two experimental groups using our drills and practice software. One experimental group used the mobile application without the holographic game and the other experimental group used the application with the holographic game. We performed pre-test and post-test performance assessments, a satisfaction survey and emotion analysis. The results are very promising. They show that the use of the holographic mobile-based application had a significant impact on the children's motivation. It also improved their performance compared to traditional methods used in the classroom.","sentences":["This paper describes a holographic mobile-based application designed to help Spanish-speaking children to practice the pronunciation of basic English vocabulary words.","The mastery of vocabulary is a fundamental step when learning a language but is often perceived as boring.","Producing the correct pronunciation is frequently regarded as the most difficult and complex skill for new learners of English.","In order to address these problems this research takes advantage of the power of multi-channel stimuli (sound, image and interaction) in a mobilebased hologram application in order to motivate students and improve their experience of practicing.","We adapted the prize-winning HolograFX game and developed a new mobile application to help practice English pronunciation.","A 3D holographic robot that acts as a virtual teacher interacts via voice with the children.","To test the tool we carried out an experiment with 70 Spanish pre-school children divided into three classes, the control group using traditional methods such as images in books and on the blackboard, and two experimental groups using our drills and practice software.","One experimental group used the mobile application without the holographic game and the other experimental group used the application with the holographic game.","We performed pre-test and post-test performance assessments, a satisfaction survey and emotion analysis.","The results are very promising.","They show that the use of the holographic mobile-based application had a significant impact on the children's motivation.","It also improved their performance compared to traditional methods used in the classroom."],"url":"http://arxiv.org/abs/2402.07897v1","category":"cs.HC"}
{"created":"2024-02-12 18:23:11","title":"Virtual Channel Purification","abstract":"Quantum error mitigation is a key approach for extracting target state properties on state-of-the-art noisy machines and early fault-tolerant devices. Using the ideas from flag fault tolerance and virtual state purification, we develop the virtual channel purification (VCP) protocol, which consumes similar qubit and gate resources as virtual state purification but offers up to exponentially stronger error suppression with increased system size and more noisy operation copies. Furthermore, VCP removes most of the assumptions required in virtual state purification. Essentially, VCP is the first quantum error mitigation protocol that does not require specific knowledge about the noise models, the target quantum state, and the target problem while still offering rigorous performance guarantees for practical noise regimes. Further connections are made between VCP and quantum error correction to produce one of the first protocols that combine quantum error correction and quantum error mitigation beyond concatenation. We can remove all noise in the channel while paying only the same sampling cost as low-order purification, reaching beyond the standard bias-variance trade-off in quantum error mitigation. Our protocol can also be adapted to key tasks in quantum networks like channel capacity activation and entanglement distribution.","sentences":["Quantum error mitigation is a key approach for extracting target state properties on state-of-the-art noisy machines and early fault-tolerant devices.","Using the ideas from flag fault tolerance and virtual state purification, we develop the virtual channel purification (VCP) protocol, which consumes similar qubit and gate resources as virtual state purification but offers up to exponentially stronger error suppression with increased system size and more noisy operation copies.","Furthermore, VCP removes most of the assumptions required in virtual state purification.","Essentially, VCP is the first quantum error mitigation protocol that does not require specific knowledge about the noise models, the target quantum state, and the target problem while still offering rigorous performance guarantees for practical noise regimes.","Further connections are made between VCP and quantum error correction to produce one of the first protocols that combine quantum error correction and quantum error mitigation beyond concatenation.","We can remove all noise in the channel while paying only the same sampling cost as low-order purification, reaching beyond the standard bias-variance trade-off in quantum error mitigation.","Our protocol can also be adapted to key tasks in quantum networks like channel capacity activation and entanglement distribution."],"url":"http://arxiv.org/abs/2402.07866v1","category":"quant-ph"}
{"created":"2024-02-12 17:22:42","title":"Injecting Wiktionary to improve token-level contextual representations using contrastive learning","abstract":"While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b). In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary. We also test how dimensionality reduction impacts the resulting contextual word embeddings. We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set). We achieve new SoTA result on the original WiC test set. We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements. We also observe improvements, although modest, for the semantic frame induction task. Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist.","sentences":["While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019).","Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b).","In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary.","We also test how dimensionality reduction impacts the resulting contextual word embeddings.","We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set).","We achieve new SoTA result on the original WiC test set.","We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements.","We also observe improvements, although modest, for the semantic frame induction task.","Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist."],"url":"http://arxiv.org/abs/2402.07817v1","category":"cs.CL"}
{"created":"2024-02-12 17:06:01","title":"Flux qubit-based detector of microwave photons","abstract":"A theory of detection of microwave photons with a flux qubit-based detector is presented. We consider semiclassical approximation with the electromagnetic field being in a coherent state. Flux qubit is considered as a multilevel quantum system (qudit). By solving the Lindblad equation, we describe the time evolution of occupations of the qudit's levels for readout and reset stages of detection. When considering the reset stage, the time evolution is described by multiple avoided-level crossings, thus providing a multilevel Landau-Zener-Stuckelberg-Majorana (LZSM) problem. In addition to numerical calculations, we present an approximate analytical solution for the description of the reset stage dynamics based on the adiabatic-impulse approximation and rate equation approach. Our theory may be useful for the theoretical description of driven-dissipative dynamics of qudits, including applications such as single-photon detection.","sentences":["A theory of detection of microwave photons with a flux qubit-based detector is presented.","We consider semiclassical approximation with the electromagnetic field being in a coherent state.","Flux qubit is considered as a multilevel quantum system (qudit).","By solving the Lindblad equation, we describe the time evolution of occupations of the qudit's levels for readout and reset stages of detection.","When considering the reset stage, the time evolution is described by multiple avoided-level crossings, thus providing a multilevel Landau-Zener-Stuckelberg-Majorana (LZSM) problem.","In addition to numerical calculations, we present an approximate analytical solution for the description of the reset stage dynamics based on the adiabatic-impulse approximation and rate equation approach.","Our theory may be useful for the theoretical description of driven-dissipative dynamics of qudits, including applications such as single-photon detection."],"url":"http://arxiv.org/abs/2402.07801v1","category":"quant-ph"}
{"created":"2024-02-12 16:50:35","title":"\"Layer-by-layer\" Unsupervised Clustering of Statistically Relevant Fluctuations in Noisy Time-series Data of Complex Dynamical Systems","abstract":"Complex systems are typically characterized by intricate internal dynamics that are often hard to elucidate. Ideally, this requires methods that allow to detect and classify in unsupervised way the microscopic dynamical events occurring in the system. However, decoupling statistically relevant fluctuations from the internal noise remains most often non-trivial. Here we describe \"Onion Clustering\": a simple, iterative unsupervised clustering method that efficiently detects and classifies statistically relevant fluctuations in noisy time-series data. We demonstrate its efficiency by analyzing simulation and experimental trajectories of various systems with complex internal dynamics, ranging from the atomic- to the microscopic-scale, in- and out-of-equilibrium. The method is based on an iterative detect-classify-archive approach. In similar way as peeling the external (evident) layer of an onion reveals the internal hidden ones, the method performs a first detection and classification of the most populated dynamical environment in the system and of its characteristic noise. The signal of such dynamical cluster is then removed from the time-series data and the remaining part, cleared-out from its noise, is analyzed again. At every iteration, the detection of hidden dynamical sub-domains is facilitated by an increasing (and adaptive) relevance-to-noise ratio. The process iterates until no new dynamical domains can be uncovered, revealing, as an output, the number of clusters that can be effectively distinguished/classified in statistically robust way as a function of the time-resolution of the analysis. Onion Clustering is general and benefits from clear-cut physical interpretability. We expect that it will help analyzing a variety of complex dynamical systems and time-series data.","sentences":["Complex systems are typically characterized by intricate internal dynamics that are often hard to elucidate.","Ideally, this requires methods that allow to detect and classify in unsupervised way the microscopic dynamical events occurring in the system.","However, decoupling statistically relevant fluctuations from the internal noise remains most often non-trivial.","Here we describe \"Onion Clustering\": a simple, iterative unsupervised clustering method that efficiently detects and classifies statistically relevant fluctuations in noisy time-series data.","We demonstrate its efficiency by analyzing simulation and experimental trajectories of various systems with complex internal dynamics, ranging from the atomic- to the microscopic-scale, in- and out-of-equilibrium.","The method is based on an iterative detect-classify-archive approach.","In similar way as peeling the external (evident) layer of an onion reveals the internal hidden ones, the method performs a first detection and classification of the most populated dynamical environment in the system and of its characteristic noise.","The signal of such dynamical cluster is then removed from the time-series data and the remaining part, cleared-out from its noise, is analyzed again.","At every iteration, the detection of hidden dynamical sub-domains is facilitated by an increasing (and adaptive) relevance-to-noise ratio.","The process iterates until no new dynamical domains can be uncovered, revealing, as an output, the number of clusters that can be effectively distinguished/classified in statistically robust way as a function of the time-resolution of the analysis.","Onion Clustering is general and benefits from clear-cut physical interpretability.","We expect that it will help analyzing a variety of complex dynamical systems and time-series data."],"url":"http://arxiv.org/abs/2402.07786v2","category":"physics.data-an"}
{"created":"2024-02-12 15:57:31","title":"Task-conditioned adaptation of visual features in multi-task policy learning","abstract":"Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy. In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations.","sentences":["Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules.","An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task.","Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning.","We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks.","We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations.","To this end, we propose a new optimization-based estimator.","We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy.","In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations."],"url":"http://arxiv.org/abs/2402.07739v1","category":"cs.CV"}
{"created":"2024-02-12 15:52:27","title":"Universal link predictor by In-context Learning","abstract":"Link prediction is a crucial task in graph machine learning, where the goal is to infer missing or future links within a graph. Traditional approaches leverage heuristic methods based on widely observed connectivity patterns, offering broad applicability and generalizability without the need for model training. Despite their utility, these methods are limited by their reliance on human-derived heuristics and lack the adaptability of data-driven approaches. Conversely, parametric link predictors excel in automatically learning the connectivity patterns from data and achieving state-of-the-art but fail short to directly transfer across different graphs. Instead, it requires the cost of extensive training and hyperparameter optimization to adapt to the target graph. In this work, we introduce the Universal Link Predictor (UniLP), a novel model that combines the generalizability of heuristic approaches with the pattern learning capabilities of parametric models. UniLP is designed to autonomously identify connectivity patterns across diverse graphs, ready for immediate application to any unseen graph dataset without targeted training. We address the challenge of conflicting connectivity patterns-arising from the unique distributions of different graphs-through the implementation of In-context Learning (ICL). This approach allows UniLP to dynamically adjust to various target graphs based on contextual demonstrations, thereby avoiding negative transfer. Through rigorous experimentation, we demonstrate UniLP's effectiveness in adapting to new, unseen graphs at test time, showcasing its ability to perform comparably or even outperform parametric models that have been finetuned for specific datasets. Our findings highlight UniLP's potential to set a new standard in link prediction, combining the strengths of heuristic and parametric methods in a single, versatile framework.","sentences":["Link prediction is a crucial task in graph machine learning, where the goal is to infer missing or future links within a graph.","Traditional approaches leverage heuristic methods based on widely observed connectivity patterns, offering broad applicability and generalizability without the need for model training.","Despite their utility, these methods are limited by their reliance on human-derived heuristics and lack the adaptability of data-driven approaches.","Conversely, parametric link predictors excel in automatically learning the connectivity patterns from data and achieving state-of-the-art but fail short to directly transfer across different graphs.","Instead, it requires the cost of extensive training and hyperparameter optimization to adapt to the target graph.","In this work, we introduce the Universal Link Predictor (UniLP), a novel model that combines the generalizability of heuristic approaches with the pattern learning capabilities of parametric models.","UniLP is designed to autonomously identify connectivity patterns across diverse graphs, ready for immediate application to any unseen graph dataset without targeted training.","We address the challenge of conflicting connectivity patterns-arising from the unique distributions of different graphs-through the implementation of In-context Learning (ICL).","This approach allows UniLP to dynamically adjust to various target graphs based on contextual demonstrations, thereby avoiding negative transfer.","Through rigorous experimentation, we demonstrate UniLP's effectiveness in adapting to new, unseen graphs at test time, showcasing its ability to perform comparably or even outperform parametric models that have been finetuned for specific datasets.","Our findings highlight UniLP's potential to set a new standard in link prediction, combining the strengths of heuristic and parametric methods in a single, versatile framework."],"url":"http://arxiv.org/abs/2402.07738v1","category":"cs.LG"}
{"created":"2024-02-12 15:39:21","title":"Distributed Observer Design over Directed Switching Topologies","abstract":"The distributed observer design problem holds significant importance in cases in which the output information of a system is decentralized across different subsystems. Each subsystem has a local observer and access to one part of the measurement outputs and information exchanged through communication networks. This paper focuses on the design of distributed observer with jointly connected directed switching networks. The problem presents challenges due to passive switching modes and the open-loop unboundedness that results from local observability. To overcome these challenges, we develop a network transformation mapping method whereby each local observer can classify itself into an independent subgraph based on independent judgment. Next, an observable decomposition and reorganization method is developed for the digraph case to ensure that each subgraph possesses independent dynamic properties. Asymptotic omniscience is then proven using a developed recursive proof method. This paper includes many previous results as special cases, because most are only suitable for undirected switching topologies or fast-switching cases. An adaptive coupling gain design is proposed to simplify the calculation and verification of conditions that guarantee asymptotic omniscience. Finally, simulation results with the power system show the validity of the developed theory.","sentences":["The distributed observer design problem holds significant importance in cases in which the output information of a system is decentralized across different subsystems.","Each subsystem has a local observer and access to one part of the measurement outputs and information exchanged through communication networks.","This paper focuses on the design of distributed observer with jointly connected directed switching networks.","The problem presents challenges due to passive switching modes and the open-loop unboundedness that results from local observability.","To overcome these challenges, we develop a network transformation mapping method whereby each local observer can classify itself into an independent subgraph based on independent judgment.","Next, an observable decomposition and reorganization method is developed for the digraph case to ensure that each subgraph possesses independent dynamic properties.","Asymptotic omniscience is then proven using a developed recursive proof method.","This paper includes many previous results as special cases, because most are only suitable for undirected switching topologies or fast-switching cases.","An adaptive coupling gain design is proposed to simplify the calculation and verification of conditions that guarantee asymptotic omniscience.","Finally, simulation results with the power system show the validity of the developed theory."],"url":"http://arxiv.org/abs/2402.07727v1","category":"math.DS"}
{"created":"2024-02-12 15:34:56","title":"LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation","abstract":"Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.","sentences":["Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources.","But it still faces challenges of resource consumption when scaling up to larger models.","Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem.","However, these efforts only analyzed parameter features to evaluate their importance.","Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model.","To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output.","We retain LoRA for important layers and the LoRA of the other layers share the same parameters.","Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop."],"url":"http://arxiv.org/abs/2402.07721v1","category":"cs.LG"}
{"created":"2024-02-12 14:10:38","title":"A hybrid memetic-ANS optimization algorithm for the home health care and home care routing and re","abstract":"This paper addresses a realistic home health care and home care (HHC\\&HC) problem which has become increasingly complex in the face of demographic aging and post-COVID-19 disruptions. The HHC\\&HC sector, as the essential component of modern health care systems, faces unique challenges in efficiently scheduling and routing caregivers to meet the rising demand for home-based care services. Traditional approaches often fall short in addressing the dynamic nature of care requests, especially in accommodating new, same-day service requests without compromising scheduled visits. To tackle these issues, We define the problem as an HHC\\&HC routing and rescheduling problem with rejection of new customers (HHC\\&HCRRP-RNC), focusing on rescheduling for a single HHC\\&HC caregiver in response to new customer requests within a single period. This problem is a variant of both the single-machine reschedule problem and the orienteering problem with mandatory visits (OPMV), where certain nodes must be visited while others are optional. A mixed integer linear programming (MILP) model is developed to cater to two groups of customers: pre-scheduled existing customers and same-day service new customers. The model emphasized maintaining minimal disruptions to the original schedule for existing customers as a constraint, highlighting the balance between adhering to scheduled visits and accommodating new customers. A hybrid memetic-Adaptive Neighborhood Search (ANS) optimization algorithm is proposed to tackle the model. This approach aims to minimize operational costs and opportunity costs while enhancing service quality and patient satisfaction. Through computational experiments, our proposed algorithm demonstrates notable performance, offering significant improvements in both efficiency and robustness within the problem domain.","sentences":["This paper addresses a realistic home health care and home care (HHC\\&HC) problem which has become increasingly complex in the face of demographic aging and post-COVID-19 disruptions.","The HHC\\&HC sector, as the essential component of modern health care systems, faces unique challenges in efficiently scheduling and routing caregivers to meet the rising demand for home-based care services.","Traditional approaches often fall short in addressing the dynamic nature of care requests, especially in accommodating new, same-day service requests without compromising scheduled visits.","To tackle these issues, We define the problem as an HHC\\&HC routing and rescheduling problem with rejection of new customers (HHC\\&HCRRP-RNC), focusing on rescheduling for a single HHC\\&HC caregiver in response to new customer requests within a single period.","This problem is a variant of both the single-machine reschedule problem and the orienteering problem with mandatory visits (OPMV), where certain nodes must be visited while others are optional.","A mixed integer linear programming (MILP) model is developed to cater to two groups of customers: pre-scheduled existing customers and same-day service new customers.","The model emphasized maintaining minimal disruptions to the original schedule for existing customers as a constraint, highlighting the balance between adhering to scheduled visits and accommodating new customers.","A hybrid memetic-Adaptive Neighborhood Search (ANS) optimization algorithm is proposed to tackle the model.","This approach aims to minimize operational costs and opportunity costs while enhancing service quality and patient satisfaction.","Through computational experiments, our proposed algorithm demonstrates notable performance, offering significant improvements in both efficiency and robustness within the problem domain."],"url":"http://arxiv.org/abs/2402.07662v1","category":"math.OC"}
{"created":"2024-02-12 13:50:07","title":"Well-posedness for the NLS hierarchy","abstract":"We prove well-posedness for higher-order equations in the so-called NLS hierarchy (also known as part of the AKNS hierarchy) in almost critical Fourier-Lebesgue spaces and in modulation spaces. We show the $j$th equation in the hierarchy is locally well-posed for initial data in $\\hat H^s_r(\\mathbb{R})$ for $s \\ge \\frac{j-1}{r'}$ and $1 < r \\le 2$ and also in $M^s_{2, p}(\\mathbb{R})$ for $s = \\frac{j-1}{2}$ and $2 \\le p < \\infty$. Supplementing our results with corresponding ill-posedness results in Fourier-Lebesgue spaces shows optimality. Using the conserved quantities derived in Koch-Tataru (2018) we argue that the hierarchy equations are globally well-posed for data in $H^s(\\mathbb{R})$ for $s \\ge \\frac{j-1}{2}$.   Our arguments are based on the Fourier restriction norm method in Bourgain spaces adapted to our data spaces and bi- & trilinear refinements of Strichartz estimates.","sentences":["We prove well-posedness for higher-order equations in the so-called NLS hierarchy (also known as part of the AKNS hierarchy) in almost critical Fourier-Lebesgue spaces and in modulation spaces.","We show the $j$th equation in the hierarchy is locally well-posed for initial data in $\\hat H^s_r(\\mathbb{R})$ for $s \\ge \\frac{j-1}{r'}$ and $1 < r \\le 2$ and also in $M^s_{2, p}(\\mathbb{R})$ for $s = \\frac{j-1}{2}$ and $2 \\le p < \\infty$. Supplementing our results with corresponding ill-posedness results in Fourier-Lebesgue spaces shows optimality.","Using the conserved quantities derived in Koch-Tataru (2018)","we argue that the hierarchy equations are globally well-posed for data in $H^s(\\mathbb{R})$ for $s \\ge \\frac{j-1}{2}$.   ","Our arguments are based on the Fourier restriction norm method in Bourgain spaces adapted to our data spaces and bi- & trilinear refinements of Strichartz estimates."],"url":"http://arxiv.org/abs/2402.07652v1","category":"math.AP"}
{"created":"2024-02-12 13:46:30","title":"Spin orbit resonance cascade via core shell model. Application to Mercury and Ganymede","abstract":"We discuss a model describing the spin orbit resonance cascade. We assume that the primary has a two-layer (core-shell) structure: it is composed by a thin solid crust and an inner and heavier solid core that are interacting due to the presence of a fluid interface. We assume two sources of dissipation: a viscous one, depending on the relative angular velocity between core and crust and a tidal one, smaller than the first, due to the viscoelastic structure of the core. We show how these two sources of dissipation are needful for the capture in spin-orbit resonance. The crust and the core fall in resonance with different time scales if the viscous coupling between them is big enough. Finally, the tidal dissipation of the viscoelastic core, decreasing the eccentricity, brings the system out of the resonance in a third very long time scale. This mechanism of entry and exit from resonance ends in the $1:1$ stable state.","sentences":["We discuss a model describing the spin orbit resonance cascade.","We assume that the primary has a two-layer (core-shell) structure: it is composed by a thin solid crust and an inner and heavier solid core that are interacting due to the presence of a fluid interface.","We assume two sources of dissipation: a viscous one, depending on the relative angular velocity between core and crust and a tidal one, smaller than the first, due to the viscoelastic structure of the core.","We show how these two sources of dissipation are needful for the capture in spin-orbit resonance.","The crust and the core fall in resonance with different time scales if the viscous coupling between them is big enough.","Finally, the tidal dissipation of the viscoelastic core, decreasing the eccentricity, brings the system out of the resonance in a third very long time scale.","This mechanism of entry and exit from resonance ends in the $1:1$ stable state."],"url":"http://arxiv.org/abs/2402.07650v1","category":"math-ph"}
{"created":"2024-02-12 13:42:11","title":"GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants","abstract":"We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures.","sentences":["We tackle the challenge of building real-world multimodal assistants for complex real-world tasks.","We describe the practicalities and challenges of developing and deploying","GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge.","Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency.","OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner.","For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns.","For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency.","Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge.","These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures."],"url":"http://arxiv.org/abs/2402.07647v1","category":"cs.IR"}
{"created":"2024-02-12 13:15:08","title":"Higher-order Connection Laplacians for Directed Simplicial Complexes","abstract":"Higher-order networks encode the many-body interactions existing in complex systems, such as the brain, protein complexes, and social interactions. Simplicial complexes are higher-order networks that allow a comprehensive investigation of the interplay between topology and dynamics. However, simplicial complexes have the limitation that they only capture undirected higher-order interactions while in real-world scenarios, often there is a need to introduce the direction of simplices, extending the popular notion of direction of edges. On graphs and networks the Magnetic Laplacian, a special case of Connection Laplacian, is becoming a popular operator to treat edge directionality. Here we tackle the challenge of treating directional simplicial complexes by formulating Higher-order Connection Laplacians taking into account the configurations induced by the simplices' directions. Specifically, we define all the Connection Laplacians of directed simplicial complexes of dimension two and we discuss the induced higher-order diffusion dynamics by considering instructive synthetic examples of simplicial complexes. The proposed higher-order diffusion processes can be adopted in real scenarios when we want to consider higher-order diffusion displaying non-trivial frustration effects due to conflicting directionalities of the incident simplices.","sentences":["Higher-order networks encode the many-body interactions existing in complex systems, such as the brain, protein complexes, and social interactions.","Simplicial complexes are higher-order networks that allow a comprehensive investigation of the interplay between topology and dynamics.","However, simplicial complexes have the limitation that they only capture undirected higher-order interactions while in real-world scenarios, often there is a need to introduce the direction of simplices, extending the popular notion of direction of edges.","On graphs and networks the Magnetic Laplacian, a special case of Connection Laplacian, is becoming a popular operator to treat edge directionality.","Here we tackle the challenge of treating directional simplicial complexes by formulating Higher-order Connection Laplacians taking into account the configurations induced by the simplices' directions.","Specifically, we define all the Connection Laplacians of directed simplicial complexes of dimension two","and we discuss the induced higher-order diffusion dynamics by considering instructive synthetic examples of simplicial complexes.","The proposed higher-order diffusion processes can be adopted in real scenarios when we want to consider higher-order diffusion displaying non-trivial frustration effects due to conflicting directionalities of the incident simplices."],"url":"http://arxiv.org/abs/2402.07631v1","category":"cs.SI"}
{"created":"2024-02-12 13:11:40","title":"On implicit and explicit representations for 1D distributed port-Hamiltonian systems","abstract":"First, two examples of 1D distributed port-Hamiltonian systems with dissipation, given in explicit (descriptor) form, are considered: the Dzekster model for the seepage of underground water and a nanorod model with non-local viscous damping. Implicit representations in Stokes-Lagrange subspaces are formulated. These formulations lead to modified Hamiltonian functions with spatial differential operators. The associated power balance equations are derived, together with the new boundary ports. Second, the port-Hamiltonian formulations for the Timoshenko and the Euler-Bernoulli beams are recalled, the latter being a flow-constrained version of the former. Implicit representations of these models in Stokes-Lagrange subspaces and corresponding power balance equations are derived. Bijective transformations are proposed between the explicit and implicit representations. It is proven these transformations commute with the flow-constraint projection operator.","sentences":["First, two examples of 1D distributed port-Hamiltonian systems with dissipation, given in explicit (descriptor) form, are considered: the Dzekster model for the seepage of underground water and a nanorod model with non-local viscous damping.","Implicit representations in Stokes-Lagrange subspaces are formulated.","These formulations lead to modified Hamiltonian functions with spatial differential operators.","The associated power balance equations are derived, together with the new boundary ports.","Second, the port-Hamiltonian formulations for the Timoshenko and the Euler-Bernoulli beams are recalled, the latter being a flow-constrained version of the former.","Implicit representations of these models in Stokes-Lagrange subspaces and corresponding power balance equations are derived.","Bijective transformations are proposed between the explicit and implicit representations.","It is proven these transformations commute with the flow-constraint projection operator."],"url":"http://arxiv.org/abs/2402.07628v1","category":"math.DS"}
{"created":"2024-02-12 12:59:40","title":"Fluctuation-dissipation relation in cosmic microwave background","abstract":"We study the fluctuation-dissipation relation for sound waves in the cosmic microwave background (CMB), employing effective field theory (EFT) for fluctuating hydrodynamics. Treating sound waves as the linear response to thermal radiation, we establish the fluctuation-dissipation relation within a cosmological framework. While dissipation is elucidated in established linear cosmological perturbation theory, the standard Boltzmann theory overlooks the associated noise, possibly contributing to inconsistencies in Lambda Cold Dark Matter ($\\Lambda$CDM) cosmology. This paper employs EFT for fluctuating hydrodynamics in cosmological perturbation theory, deriving sound wave noise. Notably, the long-time limit of the noise spectrum is independent of viscosity details, resembling a Brownian motion bounded in a harmonic potential. The net energy transfer between the sound wave system and the radiation environment reaches a balance within Hubble time, suggesting the thermal equilibrium of the sound waves themselves. The induced density power spectrum is characterized as white noise dependent on the inverse of the entropy density, which is negligibly small on the CMB scale. The energy density of the entire sound wave system scales as $a^{-4}$, akin to radiation. While the numerical factor is not determined in the present calculation, the back reaction of the sound wave system to the background radiation may not be negligible, serving as a potential source for various fitting issues in $\\Lambda$CDM cosmology.","sentences":["We study the fluctuation-dissipation relation for sound waves in the cosmic microwave background (CMB), employing effective field theory (EFT) for fluctuating hydrodynamics.","Treating sound waves as the linear response to thermal radiation, we establish the fluctuation-dissipation relation within a cosmological framework.","While dissipation is elucidated in established linear cosmological perturbation theory, the standard Boltzmann theory overlooks the associated noise, possibly contributing to inconsistencies in Lambda Cold Dark Matter ($\\Lambda$CDM) cosmology.","This paper employs EFT for fluctuating hydrodynamics in cosmological perturbation theory, deriving sound wave noise.","Notably, the long-time limit of the noise spectrum is independent of viscosity details, resembling a Brownian motion bounded in a harmonic potential.","The net energy transfer between the sound wave system and the radiation environment reaches a balance within Hubble time, suggesting the thermal equilibrium of the sound waves themselves.","The induced density power spectrum is characterized as white noise dependent on the inverse of the entropy density, which is negligibly small on the CMB scale.","The energy density of the entire sound wave system scales as $a^{-4}$, akin to radiation.","While the numerical factor is not determined in the present calculation, the back reaction of the sound wave system to the background radiation may not be negligible, serving as a potential source for various fitting issues in $\\Lambda$CDM cosmology."],"url":"http://arxiv.org/abs/2402.07623v1","category":"hep-th"}
{"created":"2024-02-12 12:02:13","title":"Resistive switching acceleration induced by thermal confinement","abstract":"Enhancing the switching speed of oxide-based memristive devices at a low voltage level is crucial for their use as non-volatile memory and their integration into emerging computing paradigms such as neuromorphic computing. Efforts to accelerate the switching speed often result in an energy tradeoff, leading to an increase of the minimum working voltage. In our study, we present an innovative solution: the introduction of a low thermal conductivity layer placed within the active electrode, which impedes the dissipation of heat generated during the switching process. The result is a notable acceleration in the switching speed of the memristive model system SrTiO$_{3}$ by a remarkable factor of 10$^{3}$, while preserving the integrity of the switching layer and the interfaces with the electrodes, rendering it adaptable to various filamentary memristive systems. The incorporation of HfO$_{2}$ or TaO$_{x}$ as heat-blocking layers not only streamlines the fabrication process, but also ensures compatibility with complementary metal-oxide-semiconductor technology.","sentences":["Enhancing the switching speed of oxide-based memristive devices at a low voltage level is crucial for their use as non-volatile memory and their integration into emerging computing paradigms such as neuromorphic computing.","Efforts to accelerate the switching speed often result in an energy tradeoff, leading to an increase of the minimum working voltage.","In our study, we present an innovative solution: the introduction of a low thermal conductivity layer placed within the active electrode, which impedes the dissipation of heat generated during the switching process.","The result is a notable acceleration in the switching speed of the memristive model system SrTiO$_{3}$ by a remarkable factor of 10$^{3}$, while preserving the integrity of the switching layer and the interfaces with the electrodes, rendering it adaptable to various filamentary memristive systems.","The incorporation of HfO$_{2}$ or TaO$_{x}$ as heat-blocking layers not only streamlines the fabrication process, but also ensures compatibility with complementary metal-oxide-semiconductor technology."],"url":"http://arxiv.org/abs/2402.07603v1","category":"physics.app-ph"}
{"created":"2024-02-12 11:58:41","title":"Interactive singing melody extraction based on active adaptation","abstract":"Extraction of predominant pitch from polyphonic audio is one of the fundamental tasks in the field of music information retrieval and computational musicology. To accomplish this task using machine learning, a large amount of labeled audio data is required to train the model. However, a classical model pre-trained on data from one domain (source), e.g., songs of a particular singer or genre, may not perform comparatively well in extracting melody from other domains (target). The performance of such models can be boosted by adapting the model using very little annotated data from the target domain. In this work, we propose an efficient interactive melody adaptation method. Our method selects the regions in the target audio that require human annotation using a confidence criterion based on normalized true class probability. The annotations are used by the model to adapt itself to the target domain using meta-learning. Our method also provides a novel meta-learning approach that handles class imbalance, i.e., a few representative samples from a few classes are available for adaptation in the target domain. Experimental results show that the proposed method outperforms other adaptive melody extraction baselines. The proposed method is model-agnostic and hence can be applied to other non-adaptive melody extraction models to boost their performance. Also, we released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio files of about 6.86 hours of duration intended for singing melody extraction tasks.","sentences":["Extraction of predominant pitch from polyphonic audio is one of the fundamental tasks in the field of music information retrieval and computational musicology.","To accomplish this task using machine learning, a large amount of labeled audio data is required to train the model.","However, a classical model pre-trained on data from one domain (source), e.g., songs of a particular singer or genre, may not perform comparatively well in extracting melody from other domains (target).","The performance of such models can be boosted by adapting the model using very little annotated data from the target domain.","In this work, we propose an efficient interactive melody adaptation method.","Our method selects the regions in the target audio that require human annotation using a confidence criterion based on normalized true class probability.","The annotations are used by the model to adapt itself to the target domain using meta-learning.","Our method also provides a novel meta-learning approach that handles class imbalance, i.e., a few representative samples from a few classes are available for adaptation in the target domain.","Experimental results show that the proposed method outperforms other adaptive melody extraction baselines.","The proposed method is model-agnostic and hence can be applied to other non-adaptive melody extraction models to boost their performance.","Also, we released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio files of about 6.86 hours of duration intended for singing melody extraction tasks."],"url":"http://arxiv.org/abs/2402.07599v1","category":"eess.AS"}
{"created":"2024-02-12 11:52:21","title":"Sheet Music Transformer: End-To-End Optical Music Recognition Beyond Monophonic Transcription","abstract":"State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date, primarily been carried out using monophonic transcription techniques to handle complex score layouts, such as polyphony, often by resorting to simplifications or specific adaptations. Despite their efficacy, these approaches imply challenges related to scalability and limitations. This paper presents the Sheet Music Transformer, the first end-to-end OMR model designed to transcribe complex musical scores without relying solely on monophonic strategies. Our model employs a Transformer-based image-to-sequence framework that predicts score transcriptions in a standard digital music encoding format from input images. Our model has been tested on two polyphonic music datasets and has proven capable of handling these intricate music structures effectively. The experimental outcomes not only indicate the competence of the model, but also show that it is better than the state-of-the-art methods, thus contributing to advancements in end-to-end OMR transcription.","sentences":["State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date, primarily been carried out using monophonic transcription techniques to handle complex score layouts, such as polyphony, often by resorting to simplifications or specific adaptations.","Despite their efficacy, these approaches imply challenges related to scalability and limitations.","This paper presents the Sheet Music Transformer, the first end-to-end OMR model designed to transcribe complex musical scores without relying solely on monophonic strategies.","Our model employs a Transformer-based image-to-sequence framework that predicts score transcriptions in a standard digital music encoding format from input images.","Our model has been tested on two polyphonic music datasets and has proven capable of handling these intricate music structures effectively.","The experimental outcomes not only indicate the competence of the model, but also show that it is better than the state-of-the-art methods, thus contributing to advancements in end-to-end OMR transcription."],"url":"http://arxiv.org/abs/2402.07596v1","category":"cs.CV"}
{"created":"2024-02-12 11:35:25","title":"Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning","abstract":"In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable. Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness. One of the significant contributions of our research is the formalization and introduction of the problem of group-specific concept drift and its distributed counterpart, shedding light on its critical importance in the realm of fairness. In addition, leveraging insights from prior research, we adapt an existing distributed concept drift adaptation algorithm to tackle group-specific distributed concept drift which utilizes a multi-model approach, a local group-specific drift detection mechanism, and continuous clustering of models over time. The findings from our experiments highlight the importance of addressing group-specific concept drift and its distributed counterpart to advance fairness in machine learning.","sentences":["In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes.","However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard.","Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable.","Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness.","One of the significant contributions of our research is the formalization and introduction of the problem of group-specific concept drift and its distributed counterpart, shedding light on its critical importance in the realm of fairness.","In addition, leveraging insights from prior research, we adapt an existing distributed concept drift adaptation algorithm to tackle group-specific distributed concept drift which utilizes a multi-model approach, a local group-specific drift detection mechanism, and continuous clustering of models over time.","The findings from our experiments highlight the importance of addressing group-specific concept drift and its distributed counterpart to advance fairness in machine learning."],"url":"http://arxiv.org/abs/2402.07586v1","category":"cs.LG"}
{"created":"2024-02-12 11:12:27","title":"Love numbers and Love symmetries for $p$-form and gravitational perturbations of higher-dimensional spherically symmetric black holes","abstract":"The static Love numbers of four-dimensional asymptotically flat, isolated, general-relativistic black holes are known to be identically vanishing. The Love symmetry proposal suggests that such vanishings are addressed by selection rules following from the emergence of an enhanced $\\text{SL}(2,\\mathbb{R})$ (``Love'') symmetry in the near-zone region; more specifically, it is the fact that the black hole perturbations belong to a highest-weight representation of this near-zone $\\text{SL}(2,\\mathbb{R})$ symmetry, rather than the existence of the Love symmetry itself, that outputs the vanishings of the corresponding Love numbers. In higher spacetime dimensions, some towers of magic zeroes with regards to the black hole response problem have also been reported for scalar, electromagnetic and gravitational perturbations of the Schwarzschild-Tangherlini black hole. Here, we extend these results by supplementing with $p$-form perturbations of the Schwarzschild-Tangherlini black hole. We furthermore analytically extract the static Love numbers and the leading order dissipation numbers associated with spin-$0$ scalar and spin-$2$ tensor-type tidal perturbations of the higher-dimensional Reissner-Nordstr\\\"om black hole. We find that Love symmetries exist and that the vanishings of the static Love numbers are captured by representation theory arguments even for these higher spin perturbations of the higher-dimensional spherically symmetric black holes of General Relativity. Interestingly, these near-zone $\\text{SL}(2,\\mathbb{R})$ structures acquire extensions to Witt algebras. Our setup allows to also study the $p$-form response problem of a static spherically symmetric black hole in a generic theory of gravity. We perform explicit computations for some black holes in the presence of string-theoretic corrections and investigate under what geometric conditions Love symmetries emerge in the near-zone.","sentences":["The static Love numbers of four-dimensional asymptotically flat, isolated, general-relativistic black holes are known to be identically vanishing.","The Love symmetry proposal suggests that such vanishings are addressed by selection rules following from the emergence of an enhanced $\\text{SL}(2,\\mathbb{R})$ (``Love'') symmetry in the near-zone region; more specifically, it is the fact that the black hole perturbations belong to a highest-weight representation of this near-zone $\\text{SL}(2,\\mathbb{R})$ symmetry, rather than the existence of the Love symmetry itself, that outputs the vanishings of the corresponding Love numbers.","In higher spacetime dimensions, some towers of magic zeroes with regards to the black hole response problem have also been reported for scalar, electromagnetic and gravitational perturbations of the Schwarzschild-Tangherlini black hole.","Here, we extend these results by supplementing with $p$-form perturbations of the Schwarzschild-Tangherlini black hole.","We furthermore analytically extract the static Love numbers and the leading order dissipation numbers associated with spin-$0$ scalar and spin-$2$ tensor-type tidal perturbations of the higher-dimensional Reissner-Nordstr\\\"om black hole.","We find that Love symmetries exist and that the vanishings of the static Love numbers are captured by representation theory arguments even for these higher spin perturbations of the higher-dimensional spherically symmetric black holes of General Relativity.","Interestingly, these near-zone $\\text{SL}(2,\\mathbb{R})$ structures acquire extensions to Witt algebras.","Our setup allows to also study the $p$-form response problem of a static spherically symmetric black hole in a generic theory of gravity.","We perform explicit computations for some black holes in the presence of string-theoretic corrections and investigate under what geometric conditions Love symmetries emerge in the near-zone."],"url":"http://arxiv.org/abs/2402.07574v1","category":"hep-th"}
{"created":"2024-02-12 10:39:17","title":"Digital Twins Below the Surface: Enhancing Underwater Teleoperation","abstract":"Subsea exploration, inspection, and intervention operations heavily rely on remotely operated vehicles (ROVs). However, the inherent complexity of the underwater environment presents significant challenges to the operators of these vehicles. This paper delves into the challenges associated with navigation and maneuvering tasks in the teleoperation of ROVs, such as reduced situational awareness and heightened teleoperator workload. To address these challenges, we introduce an underwater Digital Twin (DT) system designed to enhance underwater teleoperation, enable autonomous navigation, support system monitoring, and facilitate system testing through simulation. Our approach involves a dynamic representation of the underwater robot and its environment using desktop virtual reality, as well as the integration of mapping, localization, path planning and simulation capabilities within the DT system. Our research demonstrates the system's adaptability, versatility and feasibility, highlighting significant challenges and, in turn, improving the teleoperators' situational awareness and reducing their workload.","sentences":["Subsea exploration, inspection, and intervention operations heavily rely on remotely operated vehicles (ROVs).","However, the inherent complexity of the underwater environment presents significant challenges to the operators of these vehicles.","This paper delves into the challenges associated with navigation and maneuvering tasks in the teleoperation of ROVs, such as reduced situational awareness and heightened teleoperator workload.","To address these challenges, we introduce an underwater Digital Twin (DT) system designed to enhance underwater teleoperation, enable autonomous navigation, support system monitoring, and facilitate system testing through simulation.","Our approach involves a dynamic representation of the underwater robot and its environment using desktop virtual reality, as well as the integration of mapping, localization, path planning and simulation capabilities within the DT system.","Our research demonstrates the system's adaptability, versatility and feasibility, highlighting significant challenges and, in turn, improving the teleoperators' situational awareness and reducing their workload."],"url":"http://arxiv.org/abs/2402.07556v1","category":"cs.RO"}
{"created":"2024-02-12 10:37:54","title":"Thermodynamically consistent modelling of viscoelastic solids under finite strain","abstract":"The present article is concerned with modelling the viscoelastic behavior of Polydimethylsiloxane (PDMS) in large-strain regime. Starting from the basic principles of thermodynamics, an incremental variational formulation is derived. Within this model, the free energy density and dissipation function determine elastic and viscous properties of the solid. The main contribution of this paper is the estimation of the parameters in the proposed phenomenological model from measurements conducted on PDMS samples. This non-linear material model simplifies to a Prony-series representation in frequency domain in case of small deformations. The coefficients of this Prony-series are detected from dynamical temperature mechanical analysis measurements. Time-temperature superposition allows to combine measurements at different temperatures, such that a sufficiently large frequency range is available for subsequent fitting of Prony-parameters. A set of material parameters is thus provided. The incremental variational formulation directly lends itself to finite element discretization, where an efficient and stable choice of elements is proposed for radially symmetric problems. This formulation allows to verify the proposed model against experimental data gained in ball-drop experiments.","sentences":["The present article is concerned with modelling the viscoelastic behavior of Polydimethylsiloxane (PDMS) in large-strain regime.","Starting from the basic principles of thermodynamics, an incremental variational formulation is derived.","Within this model, the free energy density and dissipation function determine elastic and viscous properties of the solid.","The main contribution of this paper is the estimation of the parameters in the proposed phenomenological model from measurements conducted on PDMS samples.","This non-linear material model simplifies to a Prony-series representation in frequency domain in case of small deformations.","The coefficients of this Prony-series are detected from dynamical temperature mechanical analysis measurements.","Time-temperature superposition allows to combine measurements at different temperatures, such that a sufficiently large frequency range is available for subsequent fitting of Prony-parameters.","A set of material parameters is thus provided.","The incremental variational formulation directly lends itself to finite element discretization, where an efficient and stable choice of elements is proposed for radially symmetric problems.","This formulation allows to verify the proposed model against experimental data gained in ball-drop experiments."],"url":"http://arxiv.org/abs/2402.07555v1","category":"physics.app-ph"}
{"created":"2024-02-12 09:41:00","title":"MAFIA: Multi-Adapter Fused Inclusive LanguAge Models","abstract":"Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task. In this work, we aim to modularly debias a pretrained language model across multiple dimensions. Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of our approach.","sentences":["Pretrained Language Models (PLMs) are widely used in NLP for various tasks.","Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases.","However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion.","Moreover, the methods typically involve finetuning the full model to maintain the performance on the downstream task.","In this work, we aim to modularly debias a pretrained language model across multiple dimensions.","Previous works extensively explored debiasing PLMs using limited US-centric counterfactual data augmentation (CDA).","We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way.","We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously.","An extensive evaluation on multiple tasks and languages demonstrates the efficacy of our approach."],"url":"http://arxiv.org/abs/2402.07519v1","category":"cs.CL"}
{"created":"2024-02-12 09:31:43","title":"Global subelliptic estimates for geometric Kramers-Fokker-Planck operators on closed manifolds","abstract":"In this article we reconsider the proof of subelliptic estimates for Geometric Kramers-Fokker-Planck operators, a class which includes Bismut's hypoelliptic Laplacian, when the base manifold is closed (no boundary). The method is significantly different from the ones proposed by Bismut-Lebeau in [BiLe] and Lebeau in [Leb1] and [Leb2]. As a new result we are able to prove maximal subelliptic estimates with some control of the constants in the two asymptotic regimes of high (b $\\rightarrow$ 0) and low (b $\\rightarrow$ +$\\infty$) friction. After a dyadic partition in the momentum variable, the analysis is essentially local in the position variable, contrary to the microlocal reduction techniques of the previous works. In particular this method will be easier to adapt on manifolds with boundaries. A byproduct of our analysis is the introduction of a very convenient double exponent Sobolev scale associated with globally defined differential operators. Applications of this convenient parameter dependent functional analysis to accurate spectral problems, in particular for Bismut's hypoelliptic Laplacian with all its specific geometry, is deferred to subsequent works.","sentences":["In this article we reconsider the proof of subelliptic estimates for Geometric Kramers-Fokker-Planck operators, a class which includes Bismut's hypoelliptic Laplacian, when the base manifold is closed (no boundary).","The method is significantly different from the ones proposed by Bismut-Lebeau in [BiLe] and Lebeau in [Leb1] and [Leb2].","As a new result we are able to prove maximal subelliptic estimates with some control of the constants in the two asymptotic regimes of high (b $\\rightarrow$ 0) and low (b $\\rightarrow$ +$\\infty$) friction.","After a dyadic partition in the momentum variable, the analysis is essentially local in the position variable, contrary to the microlocal reduction techniques of the previous works.","In particular this method will be easier to adapt on manifolds with boundaries.","A byproduct of our analysis is the introduction of a very convenient double exponent Sobolev scale associated with globally defined differential operators.","Applications of this convenient parameter dependent functional analysis to accurate spectral problems, in particular for Bismut's hypoelliptic Laplacian with all its specific geometry, is deferred to subsequent works."],"url":"http://arxiv.org/abs/2402.07511v1","category":"math.AP"}
{"created":"2024-02-12 08:47:53","title":"An elementary approach to mixing and dissipation enhancement by transport noise","abstract":"We investigate the mixing properties of solutions to the stochastic transport equation $d u= \\circ d W \\cdot\\nabla u$, where the driving noise $W(t,x)$ is white in time, colored and divergence-free in space. Furthermore, we prove the dissipation enhancement in the presence of a small viscous term. Applying our results, we also derive the mixing properties for a regularized stochastic 2D Euler equation.","sentences":["We investigate the mixing properties of solutions to the stochastic transport equation $d u= \\circ d","W \\cdot\\nabla u$, where the driving noise $W(t,x)$ is white in time, colored and divergence-free in space.","Furthermore, we prove the dissipation enhancement in the presence of a small viscous term.","Applying our results, we also derive the mixing properties for a regularized stochastic 2D Euler equation."],"url":"http://arxiv.org/abs/2402.07484v1","category":"math.PR"}
{"created":"2024-02-12 08:14:03","title":"Pushing The Limit of LLM Capacity for Text Classification","abstract":"The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 1.36% on average. Further evaluation experiments show a clear surpassing of RGPT over human classification.","sentences":["The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks.","In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs?","To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners.","The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them.","Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners.","Through a comprehensive empirical comparison, we show that RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 1.36% on average.","Further evaluation experiments show a clear surpassing of RGPT over human classification."],"url":"http://arxiv.org/abs/2402.07470v1","category":"cs.CL"}
{"created":"2024-02-12 07:20:05","title":"Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs","abstract":"Consider the domain of multiclass classification within the adversarial online setting. What is the price of relying on bandit feedback as opposed to full information? To what extent can an adaptive adversary amplify the loss compared to an oblivious one? To what extent can a randomized learner reduce the loss compared to a deterministic one? We study these questions in the mistake bound model and provide nearly tight answers.   We demonstrate that the optimal mistake bound under bandit feedback is at most $O(k)$ times higher than the optimal mistake bound in the full information case, where $k$ represents the number of labels. This bound is tight and provides an answer to an open question previously posed and studied by Daniely and Helbertal ['13] and by Long ['17, '20], who focused on deterministic learners.   Moreover, we present nearly optimal bounds of $\\tilde{\\Theta}(k)$ on the gap between randomized and deterministic learners, as well as between adaptive and oblivious adversaries in the bandit feedback setting. This stands in contrast to the full information scenario, where adaptive and oblivious adversaries are equivalent, and the gap in mistake bounds between randomized and deterministic learners is a constant multiplicative factor of $2$.   In addition, our results imply that in some cases the optimal randomized mistake bound is approximately the square-root of its deterministic parallel. Previous results show that this is essentially the smallest it can get.","sentences":["Consider the domain of multiclass classification within the adversarial online setting.","What is the price of relying on bandit feedback as opposed to full information?","To what extent can an adaptive adversary amplify the loss compared to an oblivious one?","To what extent can a randomized learner reduce the loss compared to a deterministic one?","We study these questions in the mistake bound model and provide nearly tight answers.   ","We demonstrate that the optimal mistake bound under bandit feedback is at most $O(k)$ times higher than the optimal mistake bound in the full information case, where $k$ represents the number of labels.","This bound is tight and provides an answer to an open question previously posed and studied by Daniely and Helbertal ['13] and by Long ['17, '20], who focused on deterministic learners.   ","Moreover, we present nearly optimal bounds of $\\tilde{\\Theta}(k)$ on the gap between randomized and deterministic learners, as well as between adaptive and oblivious adversaries in the bandit feedback setting.","This stands in contrast to the full information scenario, where adaptive and oblivious adversaries are equivalent, and the gap in mistake bounds between randomized and deterministic learners is a constant multiplicative factor of $2$.   In addition, our results imply that in some cases the optimal randomized mistake bound is approximately the square-root of its deterministic parallel.","Previous results show that this is essentially the smallest it can get."],"url":"http://arxiv.org/abs/2402.07453v1","category":"cs.LG"}
{"created":"2024-02-12 06:43:52","title":"Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT","abstract":"Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scaling to documents up to 32K tokens long. We describe a pretraining data mixture which allows this encoder to process both short and long context sequences, and a finetuning approach that adapts this base model to retrieval with only single-sample batches. Finally, we validate the M2-BERT retrieval encoder on LoCoV1, finding that it outperforms competitive baselines by up to 23.3 points, despite containing 5-90x fewer parameters.","sentences":["Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text.","Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints.","To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective.","We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scaling to documents up to 32K tokens long.","We describe a pretraining data mixture which allows this encoder to process both short and long context sequences, and a finetuning approach that adapts this base model to retrieval with only single-sample batches.","Finally, we validate the M2-BERT retrieval encoder on LoCoV1, finding that it outperforms competitive baselines by up to 23.3 points, despite containing 5-90x fewer parameters."],"url":"http://arxiv.org/abs/2402.07440v1","category":"cs.IR"}
{"created":"2024-02-12 06:02:24","title":"Debiasing Recommendation with Personal Popularity","abstract":"Global popularity (GP) bias is the phenomenon that popular items are recommended much more frequently than they should be, which goes against the goal of providing personalized recommendations and harms user experience and recommendation accuracy. Many methods have been proposed to reduce GP bias but they fail to notice the fundamental problem of GP, i.e., it considers popularity from a \\textit{global} perspective of \\textit{all users} and uses a single set of popular items, and thus cannot capture the interests of individual users. As such, we propose a user-aware version of item popularity named \\textit{personal popularity} (PP), which identifies different popular items for each user by considering the users that share similar interests. As PP models the preferences of individual users, it naturally helps to produce personalized recommendations and mitigate GP bias. To integrate PP into recommendation, we design a general \\textit{personal popularity aware counterfactual} (PPAC) framework, which adapts easily to existing recommendation models. In particular, PPAC recognizes that PP and GP have both direct and indirect effects on recommendations and controls direct effects with counterfactual inference techniques for unbiased recommendations. All codes and datasets are available at \\url{https://github.com/Stevenn9981/PPAC}.","sentences":["Global popularity (GP) bias is the phenomenon that popular items are recommended much more frequently than they should be, which goes against the goal of providing personalized recommendations and harms user experience and recommendation accuracy.","Many methods have been proposed to reduce GP bias but they fail to notice the fundamental problem of GP, i.e., it considers popularity from a \\textit{global} perspective of \\textit{all users} and uses a single set of popular items, and thus cannot capture the interests of individual users.","As such, we propose a user-aware version of item popularity named \\textit{personal popularity} (PP), which identifies different popular items for each user by considering the users that share similar interests.","As PP models the preferences of individual users, it naturally helps to produce personalized recommendations and mitigate GP bias.","To integrate PP into recommendation, we design a general \\textit{personal popularity aware counterfactual} (PPAC) framework, which adapts easily to existing recommendation models.","In particular, PPAC recognizes that PP and GP have both direct and indirect effects on recommendations and controls direct effects with counterfactual inference techniques for unbiased recommendations.","All codes and datasets are available at \\url{https://github.com/Stevenn9981/PPAC}."],"url":"http://arxiv.org/abs/2402.07425v1","category":"cs.IR"}
{"created":"2024-02-12 05:46:10","title":"SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation","abstract":"This work explores the zero-shot adaptation capability of semantic skills, semantically interpretable experts' behavior patterns, in cross-domain settings, where a user input in interleaved multi-modal snippets can prompt a new long-horizon task for different domains. In these cross-domain settings, we present a semantic skill translator framework SemTra which utilizes a set of multi-modal models to extract skills from the snippets, and leverages the reasoning capabilities of a pretrained language model to adapt these extracted skills to the target domain. The framework employs a two-level hierarchy for adaptation: task adaptation and skill adaptation. During task adaptation, seq-to-seq translation by the language model transforms the extracted skills into a semantic skill sequence, which is tailored to fit the cross-domain contexts. Skill adaptation focuses on optimizing each semantic skill for the target domain context, through parametric instantiations that are facilitated by language prompting and contrastive learning-based context inferences. This hierarchical adaptation empowers the framework to not only infer a complex task specification in one-shot from the interleaved multi-modal snippets, but also adapt it to new domains with zero-shot learning abilities. We evaluate our framework with Meta-World, Franka Kitchen, RLBench, and CARLA environments. The results clarify the framework's superiority in performing long-horizon tasks and adapting to different domains, showing its broad applicability in practical use cases, such as cognitive robots interpreting abstract instructions and autonomous vehicles operating under varied configurations.","sentences":["This work explores the zero-shot adaptation capability of semantic skills, semantically interpretable experts' behavior patterns, in cross-domain settings, where a user input in interleaved multi-modal snippets can prompt a new long-horizon task for different domains.","In these cross-domain settings, we present a semantic skill translator framework SemTra which utilizes a set of multi-modal models to extract skills from the snippets, and leverages the reasoning capabilities of a pretrained language model to adapt these extracted skills to the target domain.","The framework employs a two-level hierarchy for adaptation: task adaptation and skill adaptation.","During task adaptation, seq-to-seq translation by the language model transforms the extracted skills into a semantic skill sequence, which is tailored to fit the cross-domain contexts.","Skill adaptation focuses on optimizing each semantic skill for the target domain context, through parametric instantiations that are facilitated by language prompting and contrastive learning-based context inferences.","This hierarchical adaptation empowers the framework to not only infer a complex task specification in one-shot from the interleaved multi-modal snippets, but also adapt it to new domains with zero-shot learning abilities.","We evaluate our framework with Meta-World, Franka Kitchen, RLBench, and CARLA environments.","The results clarify the framework's superiority in performing long-horizon tasks and adapting to different domains, showing its broad applicability in practical use cases, such as cognitive robots interpreting abstract instructions and autonomous vehicles operating under varied configurations."],"url":"http://arxiv.org/abs/2402.07418v1","category":"cs.AI"}
{"created":"2024-02-12 02:09:08","title":"Real-World Atmospheric Turbulence Correction via Domain Adaptation","abstract":"Atmospheric turbulence, a common phenomenon in daily life, is primarily caused by the uneven heating of the Earth's surface. This phenomenon results in distorted and blurred acquired images or videos and can significantly impact downstream vision tasks, particularly those that rely on capturing clear, stable images or videos from outdoor environments, such as accurately detecting or recognizing objects. Therefore, people have proposed ways to simulate atmospheric turbulence and designed effective deep learning-based methods to remove the atmospheric turbulence effect. However, these synthesized turbulent images can not cover all the range of real-world turbulence effects. Though the models have achieved great performance for synthetic scenarios, there always exists a performance drop when applied to real-world cases. Moreover, reducing real-world turbulence is a more challenging task as there are no clean ground truth counterparts provided to the models during training. In this paper, we propose a real-world atmospheric turbulence mitigation model under a domain adaptation framework, which links the supervised simulated atmospheric turbulence correction with the unsupervised real-world atmospheric turbulence correction. We will show our proposed method enhances performance in real-world atmospheric turbulence scenarios, improving both image quality and downstream vision tasks.","sentences":["Atmospheric turbulence, a common phenomenon in daily life, is primarily caused by the uneven heating of the Earth's surface.","This phenomenon results in distorted and blurred acquired images or videos and can significantly impact downstream vision tasks, particularly those that rely on capturing clear, stable images or videos from outdoor environments, such as accurately detecting or recognizing objects.","Therefore, people have proposed ways to simulate atmospheric turbulence and designed effective deep learning-based methods to remove the atmospheric turbulence effect.","However, these synthesized turbulent images can not cover all the range of real-world turbulence effects.","Though the models have achieved great performance for synthetic scenarios, there always exists a performance drop when applied to real-world cases.","Moreover, reducing real-world turbulence is a more challenging task as there are no clean ground truth counterparts provided to the models during training.","In this paper, we propose a real-world atmospheric turbulence mitigation model under a domain adaptation framework, which links the supervised simulated atmospheric turbulence correction with the unsupervised real-world atmospheric turbulence correction.","We will show our proposed method enhances performance in real-world atmospheric turbulence scenarios, improving both image quality and downstream vision tasks."],"url":"http://arxiv.org/abs/2402.07371v1","category":"cs.CV"}
{"created":"2024-02-12 01:17:09","title":"Regression Trees for Fast and Adaptive Prediction Intervals","abstract":"Predictive models make mistakes. Hence, there is a need to quantify the uncertainty associated with their predictions. Conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions. New conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation. Although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model. This paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees. Our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage. We create this partition by training regression trees and Random Forests on conformity scores. Our proposal is versatile, as it applies to various conformity scores and prediction settings and demonstrates superior scalability and performance compared to established baselines in simulated and real-world datasets. We provide a Python package clover that implements our methods using the standard scikit-learn interface.","sentences":["Predictive models make mistakes.","Hence, there is a need to quantify the uncertainty associated with their predictions.","Conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions.","New conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation.","Although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model.","This paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees.","Our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage.","We create this partition by training regression trees and Random Forests on conformity scores.","Our proposal is versatile, as it applies to various conformity scores and prediction settings and demonstrates superior scalability and performance compared to established baselines in simulated and real-world datasets.","We provide a Python package clover that implements our methods using the standard scikit-learn interface."],"url":"http://arxiv.org/abs/2402.07357v2","category":"stat.ML"}
{"created":"2024-02-12 00:19:09","title":"Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization","abstract":"Adapting to a priori unknown noise level is a very important but challenging problem in sequential decision-making as efficient exploration typically requires knowledge of the noise level, which is often loosely specified. We report significant progress in addressing this issue in linear bandits in two respects. First, we propose a novel confidence set that is `semi-adaptive' to the unknown sub-Gaussian parameter $\\sigma_*^2$ in the sense that the (normalized) confidence width scales with $\\sqrt{d\\sigma_*^2 + \\sigma_0^2}$ where $d$ is the dimension and $\\sigma_0^2$ is the specified sub-Gaussian parameter (known) that can be much larger than $\\sigma_*^2$. This is a significant improvement over $\\sqrt{d\\sigma_0^2}$ of the standard confidence set of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that this leads to an improved regret bound in linear bandits. Second, for bounded rewards, we propose a novel variance-adaptive confidence set that has a much improved numerical performance upon prior art. We then apply this confidence set to develop, as we claim, the first practical variance-adaptive linear bandit algorithm via an optimistic approach, which is enabled by our novel regret analysis technique. Both of our confidence sets rely critically on `regret equality' from online learning. Our empirical evaluation in Bayesian optimization tasks shows that our algorithms demonstrate better or comparable performance compared to existing methods.","sentences":["Adapting to a priori unknown noise level is a very important but challenging problem in sequential decision-making as efficient exploration typically requires knowledge of the noise level, which is often loosely specified.","We report significant progress in addressing this issue in linear bandits in two respects.","First, we propose a novel confidence set that is `semi-adaptive' to the unknown sub-Gaussian parameter $\\sigma_*^2$ in the sense that the (normalized) confidence width scales with $\\sqrt{d\\sigma_*^2 + \\sigma_0^2}$ where $d$ is the dimension and $\\sigma_0^2$ is the specified sub-Gaussian parameter (known) that can be much larger than $\\sigma_*^2$. This is a significant improvement over $\\sqrt{d\\sigma_0^2}$ of the standard confidence set of Abbasi-Yadkori et al. (2011), especially when $d$ is large.","We show that this leads to an improved regret bound in linear bandits.","Second, for bounded rewards, we propose a novel variance-adaptive confidence set that has a much improved numerical performance upon prior art.","We then apply this confidence set to develop, as we claim, the first practical variance-adaptive linear bandit algorithm via an optimistic approach, which is enabled by our novel regret analysis technique.","Both of our confidence sets rely critically on `regret equality' from online learning.","Our empirical evaluation in Bayesian optimization tasks shows that our algorithms demonstrate better or comparable performance compared to existing methods."],"url":"http://arxiv.org/abs/2402.07341v1","category":"stat.ML"}
{"created":"2024-02-11 23:39:42","title":"Deep Learning for Medical Image Segmentation with Imprecise Annotation","abstract":"Medical image segmentation (MIS) plays an instrumental role in medical image analysis, where considerable efforts have been devoted to automating the process. Currently, mainstream MIS approaches are based on deep neural networks (DNNs) which are typically trained on a dataset that contains annotation masks produced by doctors. However, in the medical domain, the annotation masks generated by different doctors can inherently vary because a doctor may unnecessarily produce precise and unique annotations to meet the goal of diagnosis. Therefore, the DNN model trained on the data annotated by certain doctors, often just a single doctor, could undesirably favour those doctors who annotate the training data, leading to the unsatisfaction of a new doctor who will use the trained model. To address this issue, this work investigates the utilization of multi-expert annotation to enhance the adaptability of the model to a new doctor and we conduct a pilot study on the MRI brain segmentation task. Experimental results demonstrate that the model trained on a dataset with multi-expert annotation can efficiently cater for a new doctor, after lightweight fine-tuning on just a few annotations from the new doctor.","sentences":["Medical image segmentation (MIS) plays an instrumental role in medical image analysis, where considerable efforts have been devoted to automating the process.","Currently, mainstream MIS approaches are based on deep neural networks (DNNs) which are typically trained on a dataset that contains annotation masks produced by doctors.","However, in the medical domain, the annotation masks generated by different doctors can inherently vary because a doctor may unnecessarily produce precise and unique annotations to meet the goal of diagnosis.","Therefore, the DNN model trained on the data annotated by certain doctors, often just a single doctor, could undesirably favour those doctors who annotate the training data, leading to the unsatisfaction of a new doctor who will use the trained model.","To address this issue, this work investigates the utilization of multi-expert annotation to enhance the adaptability of the model to a new doctor and we conduct a pilot study on the MRI brain segmentation task.","Experimental results demonstrate that the model trained on a dataset with multi-expert annotation can efficiently cater for a new doctor, after lightweight fine-tuning on just a few annotations from the new doctor."],"url":"http://arxiv.org/abs/2402.07330v1","category":"cs.CV"}
{"created":"2024-02-11 23:14:56","title":"Adaptive Voronoi-based Column Selection Methods for Interpretable Dimensionality Reduction","abstract":"In data analysis, there continues to be a need for interpretable dimensionality reduction methods whereby instrinic meaning associated with the data is retained in the reduced space. Standard approaches such as Principal Component Analysis (PCA) and the Singular Value Decomposition (SVD) fail at this task. A popular alternative is the CUR decomposition. In an SVD-like manner, the CUR decomposition approximates a matrix $A \\in \\mathbb{R}^{m \\times n}$ as $A \\approx CUR$, where $C$ and $R$ are matrices whose columns and rows are selected from the original matrix \\cite{goreinov1997theory}, \\cite{mahoney2009cur}. The difficulty in constructing a CUR decomposition is in determining which columns and rows to select when forming $C$ and $R$. Current column/row selection algorithms, particularly those that rely on an SVD, become infeasible as the size of the data becomes large \\cite{dong2021simpler}. We address this problem by reducing the column/row selection problem to a collection of smaller sub-problems. The basic idea is to first partition the rows/columns of a matrix, and then apply an existing selection algorithm on each piece; for illustration purposes we use the Discrete Empirical Interpolation Method (\\textsf{DEIM}) \\cite{sorensen2016deim}. For the first task, we consider two existing algorithms that construct a Voronoi Tessellation (VT) of the rows and columns of a given matrix. We then extend these methods to automatically adapt to the data. The result is four data-driven row/column selection methods that are well-suited for parallelization, and compatible with nearly any existing column/row selection strategy. Theory and numerical examples show the design to be competitive with the original \\textsf{DEIM} routine.","sentences":["In data analysis, there continues to be a need for interpretable dimensionality reduction methods whereby instrinic meaning associated with the data is retained in the reduced space.","Standard approaches such as Principal Component Analysis (PCA) and the Singular Value Decomposition (SVD) fail at this task.","A popular alternative is the CUR decomposition.","In an SVD-like manner, the CUR decomposition approximates a matrix $A \\in \\mathbb{R}^{m \\times n}$ as $A \\approx CUR$, where $C$ and $R$ are matrices whose columns and rows are selected from the original matrix \\cite{goreinov1997theory}, \\cite{mahoney2009cur}.","The difficulty in constructing a CUR decomposition is in determining which columns and rows to select when forming $C$ and $R$. Current column/row selection algorithms, particularly those that rely on an SVD, become infeasible as the size of the data becomes large \\cite{dong2021simpler}.","We address this problem by reducing the column/row selection problem to a collection of smaller sub-problems.","The basic idea is to first partition the rows/columns of a matrix, and then apply an existing selection algorithm on each piece; for illustration purposes we use the Discrete Empirical Interpolation Method (\\textsf{DEIM}) \\cite{sorensen2016deim}.","For the first task, we consider two existing algorithms that construct a Voronoi Tessellation (VT) of the rows and columns of a given matrix.","We then extend these methods to automatically adapt to the data.","The result is four data-driven row/column selection methods that are well-suited for parallelization, and compatible with nearly any existing column/row selection strategy.","Theory and numerical examples show the design to be competitive with the original \\textsf{DEIM} routine."],"url":"http://arxiv.org/abs/2402.07325v1","category":"math.NA"}
{"created":"2024-02-11 22:59:19","title":"Lessons Learned from Mining the Hugging Face Repository","abstract":"The rapidly evolving fields of Machine Learning (ML) and Artificial Intelligence have witnessed the emergence of platforms like Hugging Face (HF) as central hubs for model development and sharing. This experience report synthesizes insights from two comprehensive studies conducted on HF, focusing on carbon emissions and the evolutionary and maintenance aspects of ML models. Our objective is to provide a practical guide for future researchers embarking on mining software repository studies within the HF ecosystem to enhance the quality of these studies. We delve into the intricacies of the replication package used in our studies, highlighting the pivotal tools and methodologies that facilitated our analysis. Furthermore, we propose a nuanced stratified sampling strategy tailored for the diverse HF Hub dataset, ensuring a representative and comprehensive analytical approach. The report also introduces preliminary guidelines, transitioning from repository mining to cohort studies, to establish causality in repository mining studies, particularly within the ML model of HF context. This transition is inspired by existing frameworks and is adapted to suit the unique characteristics of the HF model ecosystem. Our report serves as a guiding framework for researchers, contributing to the responsible and sustainable advancement of ML, and fostering a deeper understanding of the broader implications of ML models.","sentences":["The rapidly evolving fields of Machine Learning (ML) and Artificial Intelligence have witnessed the emergence of platforms like Hugging Face (HF) as central hubs for model development and sharing.","This experience report synthesizes insights from two comprehensive studies conducted on HF, focusing on carbon emissions and the evolutionary and maintenance aspects of ML models.","Our objective is to provide a practical guide for future researchers embarking on mining software repository studies within the HF ecosystem to enhance the quality of these studies.","We delve into the intricacies of the replication package used in our studies, highlighting the pivotal tools and methodologies that facilitated our analysis.","Furthermore, we propose a nuanced stratified sampling strategy tailored for the diverse HF Hub dataset, ensuring a representative and comprehensive analytical approach.","The report also introduces preliminary guidelines, transitioning from repository mining to cohort studies, to establish causality in repository mining studies, particularly within the ML model of HF context.","This transition is inspired by existing frameworks and is adapted to suit the unique characteristics of the HF model ecosystem.","Our report serves as a guiding framework for researchers, contributing to the responsible and sustainable advancement of ML, and fostering a deeper understanding of the broader implications of ML models."],"url":"http://arxiv.org/abs/2402.07323v1","category":"cs.SE"}
{"created":"2024-02-11 19:12:51","title":"Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study","abstract":"Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured. A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events. This paper evaluates a diverse array of machine learning-based anomaly detection algorithms through a comprehensive benchmark study. The paper contributes significantly by conducting an unbiased comparison of various anomaly detection algorithms, spanning classical machine learning including various tree-based approaches to deep learning and outlier detection methods. The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios. The paper dispels the deep learning myth, demonstrating that though powerful, deep learning is not a universal solution in this case. We observed that recently proposed tree-based evolutionary algorithms outperform in many scenarios. We noticed that tree-based approaches catch a singleton anomaly in a dataset where deep learning methods fail. On the other hand, classical SVM performs the best on datasets with more than 10% anomalies, implying that such scenarios can be best modeled as a classification problem rather than anomaly detection. To our knowledge, such a study on a large number of state-of-the-art algorithms using diverse data sets, with the objective of guiding researchers and practitioners in making informed algorithmic choices, has not been attempted earlier.","sentences":["Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured.","A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events.","This paper evaluates a diverse array of machine learning-based anomaly detection algorithms through a comprehensive benchmark study.","The paper contributes significantly by conducting an unbiased comparison of various anomaly detection algorithms, spanning classical machine learning including various tree-based approaches to deep learning and outlier detection methods.","The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios.","The paper dispels the deep learning myth, demonstrating that though powerful, deep learning is not a universal solution in this case.","We observed that recently proposed tree-based evolutionary algorithms outperform in many scenarios.","We noticed that tree-based approaches catch a singleton anomaly in a dataset where deep learning methods fail.","On the other hand, classical SVM performs the best on datasets with more than 10% anomalies, implying that such scenarios can be best modeled as a classification problem rather than anomaly detection.","To our knowledge, such a study on a large number of state-of-the-art algorithms using diverse data sets, with the objective of guiding researchers and practitioners in making informed algorithmic choices, has not been attempted earlier."],"url":"http://arxiv.org/abs/2402.07281v1","category":"cs.LG"}
{"created":"2024-02-11 18:32:54","title":"Adaptive Finite Element Methods","abstract":"This is a survey on the theory of adaptive finite element methods (AFEMs), which are fundamental in modern computational science and engineering but whose mathematical assessment is a formidable challenge. We present a self-contained and up-to-date discussion of AFEMs for linear second order elliptic PDEs and dimension d>1, with emphasis on foundational issues. After a brief review of functional analysis and basic finite element theory, including piecewise polynomial approximation in graded meshes, we present the core material for coercive problems. We start with a novel a posteriori error analysis applicable to rough data, which delivers estimators fully equivalent to the solution error. They are used in the design and study of three AFEMs depending on the structure of data. We prove linear convergence of these algorithms and rate-optimality provided the solution and data belong to suitable approximation classes. We also address the relation between approximation and regularity classes. We finally extend this theory to discontinuous Galerkin methods as prototypes of non-conforming AFEMs and beyond coercive problems to inf-sup stable AFEMs.","sentences":["This is a survey on the theory of adaptive finite element methods (AFEMs), which are fundamental in modern computational science and engineering but whose mathematical assessment is a formidable challenge.","We present a self-contained and up-to-date discussion of AFEMs for linear second order elliptic PDEs and dimension d>1, with emphasis on foundational issues.","After a brief review of functional analysis and basic finite element theory, including piecewise polynomial approximation in graded meshes, we present the core material for coercive problems.","We start with a novel a posteriori error analysis applicable to rough data, which delivers estimators fully equivalent to the solution error.","They are used in the design and study of three AFEMs depending on the structure of data.","We prove linear convergence of these algorithms and rate-optimality provided the solution and data belong to suitable approximation classes.","We also address the relation between approximation and regularity classes.","We finally extend this theory to discontinuous Galerkin methods as prototypes of non-conforming AFEMs and beyond coercive problems to inf-sup stable AFEMs."],"url":"http://arxiv.org/abs/2402.07273v1","category":"math.NA"}
{"created":"2024-02-11 18:05:04","title":"Optimization of laser pumping for the generation of multi-pulse structures in fiber lasers","abstract":"We address the challenge of configuring a fiber laser cavity to enable efficient access to multi-pulse structures such as dissipative soliton molecules. We theoretically compare multi-pulsing routes in the parameter space of the laser. By using a two-dimensional parameter space, we experimentally demonstrate an important reduction in the laser pumping power required to form soliton molecules.","sentences":["We address the challenge of configuring a fiber laser cavity to enable efficient access to multi-pulse structures such as dissipative soliton molecules.","We theoretically compare multi-pulsing routes in the parameter space of the laser.","By using a two-dimensional parameter space, we experimentally demonstrate an important reduction in the laser pumping power required to form soliton molecules."],"url":"http://arxiv.org/abs/2402.07260v1","category":"physics.optics"}
{"created":"2024-02-11 17:40:49","title":"Eco-evolutionary dynamics of adapting pathogens and host immunity","abstract":"As pathogens spread in a population of hosts, immunity is built up and the pool of susceptible individuals is depleted. This generates selective pressure, to which many human RNA viruses, such as influenza virus or SARS-CoV-2, respond with rapid antigenic evolution and frequent emergence of immune evasive variants. However, the host's immune systems adapt and older immune responses wane, such that escape variants only enjoy a growth advantage for a limited time. If variant growth dynamics and reshaping of host-immunity operate on comparable time scales, viral adaptation is determined by eco-evolutionary interactions that are not captured by models of rapid evolution in a fixed environment. Here, we use a Susceptible/Infected model to describe the interaction between an evolving viral population in a dynamic but immunologically diverse host population. We show that depending on strain cross-immunity, heterogeneity of the host population, and durability of immune responses, escape variants initially grow exponentially, but lose their growth advantage before reaching high frequencies. Their subsequent dynamics follows an anomalous random walk determined by future escape variants and results in variant trajectories that are unpredictable. This model can explain the apparent contradiction between the clearly adaptive nature of antigenic evolution and the quasi-neutral dynamics of high frequency variants observed for influenza viruses.","sentences":["As pathogens spread in a population of hosts, immunity is built up and the pool of susceptible individuals is depleted.","This generates selective pressure, to which many human RNA viruses, such as influenza virus or SARS-CoV-2, respond with rapid antigenic evolution and frequent emergence of immune evasive variants.","However, the host's immune systems adapt and older immune responses wane, such that escape variants only enjoy a growth advantage for a limited time.","If variant growth dynamics and reshaping of host-immunity operate on comparable time scales, viral adaptation is determined by eco-evolutionary interactions that are not captured by models of rapid evolution in a fixed environment.","Here, we use a Susceptible/Infected model to describe the interaction between an evolving viral population in a dynamic but immunologically diverse host population.","We show that depending on strain cross-immunity, heterogeneity of the host population, and durability of immune responses, escape variants initially grow exponentially, but lose their growth advantage before reaching high frequencies.","Their subsequent dynamics follows an anomalous random walk determined by future escape variants and results in variant trajectories that are unpredictable.","This model can explain the apparent contradiction between the clearly adaptive nature of antigenic evolution and the quasi-neutral dynamics of high frequency variants observed for influenza viruses."],"url":"http://arxiv.org/abs/2402.07252v1","category":"q-bio.PE"}
{"created":"2024-02-12 18:58:58","title":"A systematic investigation of learnability from single child linguistic input","abstract":"Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines). We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child's linguistic input.","sentences":["Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability.","However, a significant gap exists between the training data for these models and the linguistic input a child receives.","LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a).","Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input.","Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset.","Here, to examine the robustness of learnability from single-child input, we systematically train six different model architectures on five datasets (3 single-child and 2 baselines).","We find that the models trained on single-child datasets showed consistent results that matched with previous work, underscoring the robustness of forming meaningful syntactic and semantic representations from a subset of a child's linguistic input."],"url":"http://arxiv.org/abs/2402.07899v1","category":"cs.CL"}
{"created":"2024-02-12 18:55:43","title":"Consistent eccentricities for gravitational wave astronomy: Resolving discrepancies between astrophysical simulations and waveform models","abstract":"Detecting imprints of orbital eccentricity in gravitational wave signals promises to shed light on the formation mechanisms of binary black holes. To constrain formation mechanisms, distributions of eccentricity derived from numerical simulations of astrophysical formation channels are compared to the estimates of eccentricity inferred from GW signals. We report that the definition of eccentricity typically used in astrophysical simulations is inconsistent with the one used while modeling GW signals, with the differences mainly arising due to the choice of reference frequency used in both cases. We also posit a prescription for calculating eccentricity from astrophysical simulations by evolving ordinary differential equations obtained from post-Newtonian theory, and using the dominant ($\\ell = m =2$) mode's frequency as the reference frequency; this ensures consistency in the definitions. On comparing the existing eccentricities of binaries present in the Cluster Monte Carlo catalog of globular cluster simulations with the eccentricities calculated using the prescription presented here, we find a significant discrepancy at $e \\gtrsim 0.2$; this discrepancy becomes worse with increasing eccentricity. We note the implications this discrepancy has for existing studies, and recommend that care be taken when comparing data-driven constraints on eccentricity to expectations from astrophysical formation channels.","sentences":["Detecting imprints of orbital eccentricity in gravitational wave signals promises to shed light on the formation mechanisms of binary black holes.","To constrain formation mechanisms, distributions of eccentricity derived from numerical simulations of astrophysical formation channels are compared to the estimates of eccentricity inferred from GW signals.","We report that the definition of eccentricity typically used in astrophysical simulations is inconsistent with the one used while modeling GW signals, with the differences mainly arising due to the choice of reference frequency used in both cases.","We also posit a prescription for calculating eccentricity from astrophysical simulations by evolving ordinary differential equations obtained from post-Newtonian theory, and using the dominant ($\\ell = m =2$) mode's frequency as the reference frequency; this ensures consistency in the definitions.","On comparing the existing eccentricities of binaries present in the Cluster Monte Carlo catalog of globular cluster simulations with the eccentricities calculated using the prescription presented here, we find a significant discrepancy at $e \\gtrsim 0.2$; this discrepancy becomes worse with increasing eccentricity.","We note the implications this discrepancy has for existing studies, and recommend that care be taken when comparing data-driven constraints on eccentricity to expectations from astrophysical formation channels."],"url":"http://arxiv.org/abs/2402.07892v1","category":"astro-ph.HE"}
{"created":"2024-02-12 18:36:10","title":"Tanaka rigidity of graph Lie algebras","abstract":"We give sufficient conditions on a labeled direct graph to determine whether the Tanaka prolongation of its associated Lie algebra is infinite-dimensional. In the case that all directed edges are labeled differently, the corresponding graph Lie algebra is of infinite type if and only if the graph has a vertex of degree one.","sentences":["We give sufficient conditions on a labeled direct graph to determine whether the Tanaka prolongation of its associated Lie algebra is infinite-dimensional.","In the case that all directed edges are labeled differently, the corresponding graph Lie algebra is of infinite type if and only if the graph has a vertex of degree one."],"url":"http://arxiv.org/abs/2402.07873v1","category":"math.DG"}
{"created":"2024-02-12 18:04:49","title":"Differentiation of simplicial manifolds I","abstract":"In this paper we expand a technique for differentiating higher Lie groupoids that was initially outlined in previous work and has been elaborated in current research. We extend this technique to the setting of general simplicial manifolds. The concept underlying this extension is that even though simplicial manifolds might not possess the additional criteria that characterize higher Lie groupoids, they do satisfy \"local Kan conditions\", which suffices to perform the constructions presented in the aforementioned study.","sentences":["In this paper we expand a technique for differentiating higher Lie groupoids that was initially outlined in previous work and has been elaborated in current research.","We extend this technique to the setting of general simplicial manifolds.","The concept underlying this extension is that even though simplicial manifolds might not possess the additional criteria that characterize higher Lie groupoids, they do satisfy \"local Kan conditions\", which suffices to perform the constructions presented in the aforementioned study."],"url":"http://arxiv.org/abs/2402.07857v1","category":"math.DG"}
{"created":"2024-02-12 18:04:45","title":"The Phase Transition of $4D$ Yang-Mills Charged GB AdS Black Hole with Cloud of Strings","abstract":"In this paper, we present an exact spherically symmetric and Yang-Mills charged AdS black hole solution in the context of $4D$ Einstein-Gauss-Bonnet (EGB) gravity in the presence of a cloud of strings. Thermodynamics of this solution is studied. The critical behavior, the types of phase transitions in canonical ensemble, the Joule-Thomson expansion, the Clapeyron equation and the critical exponents shall be investigated.","sentences":["In this paper, we present an exact spherically symmetric and Yang-Mills charged AdS black hole solution in the context of $4D$ Einstein-Gauss-Bonnet (EGB) gravity in the presence of a cloud of strings.","Thermodynamics of this solution is studied.","The critical behavior, the types of phase transitions in canonical ensemble, the Joule-Thomson expansion, the Clapeyron equation and the critical exponents shall be investigated."],"url":"http://arxiv.org/abs/2402.07856v1","category":"hep-th"}
{"created":"2024-02-12 17:59:20","title":"Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts","abstract":"In this draft we consider the problem of forecasting rainfall across India during the four monsoon months, one day as well as three days in advance. We train neural networks using historical daily gridded precipitation data for India obtained from IMD for the time period $1901- 2022$, at a spatial resolution of $1^{\\circ} \\times 1^{\\circ}$. This is compared with the numerical weather prediction (NWP) forecasts obtained from NCEP (National Centre for Environmental Prediction) available for the period 2011-2022. We conduct a detailed country wide analysis and separately analyze some of the most populated cities in India. Our conclusion is that forecasts obtained by applying deep learning to historical rainfall data are more accurate compared to NWP forecasts as well as predictions based on persistence. On average, compared to our predictions, forecasts from NCEP-NWP model have about 34% higher error for a single day prediction, and over 68% higher error for a three day prediction. Similarly, persistence estimates report a 29% higher error in a single day forecast, and over 54% error in a three day forecast. We further observe that data up to 20 days in the past is useful in reducing errors of one and three day forecasts, when a transformer based learning architecture, and to a lesser extent when an LSTM is used. A key conclusion suggested by our preliminary analysis is that NWP forecasts can be substantially improved upon through more and diverse data relevant to monsoon prediction combined with carefully selected neural network architecture.","sentences":["In this draft we consider the problem of forecasting rainfall across India during the four monsoon months, one day as well as three days in advance.","We train neural networks using historical daily gridded precipitation data for India obtained from IMD for the time period $1901- 2022$, at a spatial resolution of $1^{\\circ} \\times 1^{\\circ}$.","This is compared with the numerical weather prediction (NWP) forecasts obtained from NCEP (National Centre for Environmental Prediction) available for the period 2011-2022.","We conduct a detailed country wide analysis and separately analyze some of the most populated cities in India.","Our conclusion is that forecasts obtained by applying deep learning to historical rainfall data are more accurate compared to NWP forecasts as well as predictions based on persistence.","On average, compared to our predictions, forecasts from NCEP-NWP model have about 34% higher error for a single day prediction, and over 68% higher error for a three day prediction.","Similarly, persistence estimates report a 29% higher error in a single day forecast, and over 54% error in a three day forecast.","We further observe that data up to 20 days in the past is useful in reducing errors of one and three day forecasts, when a transformer based learning architecture, and to a lesser extent when an LSTM is used.","A key conclusion suggested by our preliminary analysis is that NWP forecasts can be substantially improved upon through more and diverse data relevant to monsoon prediction combined with carefully selected neural network architecture."],"url":"http://arxiv.org/abs/2402.07851v1","category":"cs.LG"}
{"created":"2024-02-12 17:56:55","title":"Canonical Lifts in Multisymplectic De Donder-Weyl Hamiltonian Field Theories","abstract":"In this paper, we define canonical lifts of vector fields to the multisymplectic multimomentum bundles of De Donder-Weyl Hamiltonian first-order field theories and to the appropriate premultisymplectic embedded constraint submanifolds on which singular field theories are studied. These new canonical lifts are used to study the so-called natural Noether symmetries present in both regular and singular Hamiltonian field theories along with their associated conserved quantities obtained from Noether's theorem. The Klein-Gordon field, Einstein-Cartan gravity in $3+1$ dimensions, and the Polyakov bosonic string are analyzed in depth as applications of these concepts; as a peripheral result obtained in the analysis of the bosonic string, we provide a new geometrical interpretation of the well-known Virasoro constraint.","sentences":["In this paper, we define canonical lifts of vector fields to the multisymplectic multimomentum bundles of De Donder-Weyl Hamiltonian first-order field theories and to the appropriate premultisymplectic embedded constraint submanifolds on which singular field theories are studied.","These new canonical lifts are used to study the so-called natural Noether symmetries present in both regular and singular Hamiltonian field theories along with their associated conserved quantities obtained from Noether's theorem.","The Klein-Gordon field, Einstein-Cartan gravity in $3+1$ dimensions, and the Polyakov bosonic string are analyzed in depth as applications of these concepts; as a peripheral result obtained in the analysis of the bosonic string, we provide a new geometrical interpretation of the well-known Virasoro constraint."],"url":"http://arxiv.org/abs/2402.07847v1","category":"math-ph"}
{"created":"2024-02-12 17:46:58","title":"Quantile Least Squares: A Flexible Approach for Robust Estimation and Validation of Location-Scale Families","abstract":"In this paper, the problem of robust estimation and validation of location-scale families is revisited. The proposed methods exploit the joint asymptotic normality of sample quantiles (of i.i.d random variables) to construct the ordinary and generalized least squares estimators of location and scale parameters. These quantile least squares (QLS) estimators are easy to compute because they have explicit expressions, their robustness is achieved by excluding extreme quantiles from the least-squares estimation, and efficiency is boosted by using as many non-extreme quantiles as practically relevant. The influence functions of the QLS estimators are specified and plotted for several location-scale families. They closely resemble the shapes of some well-known influence functions yet those shapes emerge automatically (i.e., do not need to be specified). The joint asymptotic normality of the proposed estimators is established, and their finite-sample properties are explored using simulations. Also, computational costs of these estimators, as well as those of MLE, are evaluated for sample sizes n = 10^6, 10^7, 10^8, 10^9. For model validation, two goodness-of-fit tests are constructed and their performance is studied using simulations and real data. In particular, for the daily stock returns of Google over the last four years, both tests strongly support the logistic distribution assumption and reject other bell-shaped competitors.","sentences":["In this paper, the problem of robust estimation and validation of location-scale families is revisited.","The proposed methods exploit the joint asymptotic normality of sample quantiles (of i.i.d random variables) to construct the ordinary and generalized least squares estimators of location and scale parameters.","These quantile least squares (QLS) estimators are easy to compute because they have explicit expressions, their robustness is achieved by excluding extreme quantiles from the least-squares estimation, and efficiency is boosted by using as many non-extreme quantiles as practically relevant.","The influence functions of the QLS estimators are specified and plotted for several location-scale families.","They closely resemble the shapes of some well-known influence functions yet those shapes emerge automatically (i.e., do not need to be specified).","The joint asymptotic normality of the proposed estimators is established, and their finite-sample properties are explored using simulations.","Also, computational costs of these estimators, as well as those of MLE, are evaluated for sample sizes n = 10^6, 10^7, 10^8, 10^9.","For model validation, two goodness-of-fit tests are constructed and their performance is studied using simulations and real data.","In particular, for the daily stock returns of Google over the last four years, both tests strongly support the logistic distribution assumption and reject other bell-shaped competitors."],"url":"http://arxiv.org/abs/2402.07837v1","category":"stat.ME"}
{"created":"2024-02-12 17:45:40","title":"Generalizing across Temporal Domains with Koopman Operators","abstract":"In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging. This problem becomes further complicated when considering evolving dynamics between domains. While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking. In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds. Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By employing Koopman Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations between evolving domains. Through empirical evaluations conducted on synthetic and real-world datasets, we validate the effectiveness of our proposed approach.","sentences":["In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging.","This problem becomes further complicated when considering evolving dynamics between domains.","While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking.","In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds.","Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets).","By employing Koopman Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations between evolving domains.","Through empirical evaluations conducted on synthetic and real-world datasets, we validate the effectiveness of our proposed approach."],"url":"http://arxiv.org/abs/2402.07834v1","category":"cs.LG"}
{"created":"2024-02-12 17:32:06","title":"Schr\u00f6dinger type equations with singular coefficients and lower order terms","abstract":"In this paper we investigate the well-posedness of the Cauchy problem for a Schr\\\"odinger operator with singular lower order terms. We allow distributional coefficients and we approach this problem via the regularising methods at the core of the theory of very weak solutions. We prove that a very weak solution exists and it is unique modulo negligible perturbations. Very weak solutions converge to classical solutions when the equation coefficients are regular enough.","sentences":["In this paper we investigate the well-posedness of the Cauchy problem for a Schr\\\"odinger operator with singular lower order terms.","We allow distributional coefficients and we approach this problem via the regularising methods at the core of the theory of very weak solutions.","We prove that a very weak solution exists and it is unique modulo negligible perturbations.","Very weak solutions converge to classical solutions when the equation coefficients are regular enough."],"url":"http://arxiv.org/abs/2402.07826v1","category":"math.AP"}
{"created":"2024-02-12 17:15:58","title":"Small separators, upper bounds for $l^\\infty$-widths, and systolic geometry","abstract":"We investigate the dependence on the dimension in the inequalities that relate the volume of a closed submanifold $M^n\\subset \\mathbb{R}^N$ with its $l^\\infty$-width $W^{l^\\infty}_{n-1}(M^n)$ defined as the infimum over all continuous maps $\\phi:M^n\\longrightarrow K^{n-1}\\subset\\mathbb{R}^N$ of $sup_{x\\in M^n}\\Vert \\phi(x)-x\\Vert_{l^\\infty}$. We prove that $W^{l^\\infty}_{n-1}(M^n)\\leq const\\ \\sqrt{n}\\ vol(M^n)^{\\frac{1}{n}}$, and if the codimension $N-n$ is equal to $1$, then $W^{l^\\infty}_{n-1}(M^n)\\leq 3\\ vol(M^n)^{\\frac{1}{n}}$.   As a corollary, we prove that if $M^n\\subset \\mathbb{R}^N$ is essential, then there exists a non-contractible closed curve on $M^n$ contained in a cube in $\\mathbb{R}^N$ with side length $const\\ \\sqrt{n}\\ vol^{\\frac{1}{n}}(M^n)$ with sides parallel to the coordinate axes. If the codimension is $1$, then the side length of the cube is $12\\cdot vol^{\\frac{1}{n}}(M^n)$.","sentences":["We investigate the dependence on the dimension in the inequalities that relate the volume of a closed submanifold $M^n\\subset \\mathbb{R}^N$ with its $l^\\infty$-width $W^{l^\\infty}_{n-1}(M^n)$ defined as the infimum over all continuous maps $\\phi:M^n\\longrightarrow K^{n-1}\\subset\\mathbb{R}^N$ of $sup_{x\\in M^n}\\Vert \\phi(x)-x\\Vert_{l^\\infty}$.","We prove that $W^{l^\\infty}_{n-1}(M^n)\\leq const\\ \\sqrt{n}\\ vol(M^n)^{\\frac{1}{n}}$, and if the codimension $N-n$ is equal to $1$, then $W^{l^\\infty}_{n-1}(M^n)\\leq 3\\ vol(M^n)^{\\frac{1}{n}}$.   As a corollary, we prove that if $M^n\\subset \\mathbb{R}^N$ is essential, then there exists a non-contractible closed curve on $M^n$ contained in a cube in $\\mathbb{R}^N$ with side length $const\\ \\sqrt{n}\\ vol^{\\frac{1}{n}}(M^n)$ with sides parallel to the coordinate axes.","If the codimension is $1$, then the side length of the cube is $12\\cdot vol^{\\frac{1}{n}}(M^n)$."],"url":"http://arxiv.org/abs/2402.07810v1","category":"math.DG"}
{"created":"2024-02-12 17:15:19","title":"Quantum walks, the discrete wave equation and Chebyshev polynomials","abstract":"A quantum walk is the quantum analogue of a random walk. While it is relatively well understood how quantum walks can speed up random walk hitting times, it is a long-standing open question to what extent quantum walks can speed up the spreading or mixing rate of random walks on graphs. In this expository paper, inspired by a blog post by Terence Tao, we describe a particular perspective on this question that derives quantum walks from the discrete wave equation on graphs. This yields a description of the quantum walk dynamics as simply applying a Chebyshev polynomial to the random walk transition matrix. This perspective decouples the problem from its quantum origin, and highlights connections to earlier (non-quantum) work and the use of Chebyshev polynomials in random walk theory as in the Varopoulos-Carne bound. We illustrate the approach by proving a weak limit of the quantum walk dynamics on the lattice. This gives a different proof of the quadratically improved spreading behavior of quantum walks on lattices.","sentences":["A quantum walk is the quantum analogue of a random walk.","While it is relatively well understood how quantum walks can speed up random walk hitting times, it is a long-standing open question to what extent quantum walks can speed up the spreading or mixing rate of random walks on graphs.","In this expository paper, inspired by a blog post by Terence Tao, we describe a particular perspective on this question that derives quantum walks from the discrete wave equation on graphs.","This yields a description of the quantum walk dynamics as simply applying a Chebyshev polynomial to the random walk transition matrix.","This perspective decouples the problem from its quantum origin, and highlights connections to earlier (non-quantum) work and the use of Chebyshev polynomials in random walk theory as in the Varopoulos-Carne bound.","We illustrate the approach by proving a weak limit of the quantum walk dynamics on the lattice.","This gives a different proof of the quadratically improved spreading behavior of quantum walks on lattices."],"url":"http://arxiv.org/abs/2402.07809v1","category":"quant-ph"}
{"created":"2024-02-12 17:01:12","title":"Estimation of non-uniform blur using a patch-based regression convolutional neural network (CNN)","abstract":"The non-uniform blur of atmospheric turbulence can be modeled as a superposition of linear motion blur kernels at a patch level. We propose a regression convolutional neural network (CNN) to predict angle and length of a linear motion blur kernel for varying sized patches. We analyze the robustness of the network for different patch sizes and the performance of the network in regions where the characteristics of the blur are transitioning. Alternating patch sizes per epoch in training, we find coefficient of determination scores across a range of patch sizes of $R^2>0.78$ for length and $R^2>0.94$ for angle prediction. We find that blur predictions in regions overlapping two blur characteristics transition between the two characteristics as overlap changes. These results validate the use of such a network for prediction of non-uniform blur characteristics at a patch level.","sentences":["The non-uniform blur of atmospheric turbulence can be modeled as a superposition of linear motion blur kernels at a patch level.","We propose a regression convolutional neural network (CNN) to predict angle and length of a linear motion blur kernel for varying sized patches.","We analyze the robustness of the network for different patch sizes and the performance of the network in regions where the characteristics of the blur are transitioning.","Alternating patch sizes per epoch in training, we find coefficient of determination scores across a range of patch sizes of $R^2>0.78$ for length and $R^2>0.94$ for angle prediction.","We find that blur predictions in regions overlapping two blur characteristics transition between the two characteristics as overlap changes.","These results validate the use of such a network for prediction of non-uniform blur characteristics at a patch level."],"url":"http://arxiv.org/abs/2402.07796v1","category":"eess.IV"}
{"created":"2024-02-12 16:59:39","title":"Finite and infinite order differential properties of the reduced Mittag--Leffler polynomials","abstract":"This paper deals with the Mittag-Leffler polynomials (MLP) by extracting their essence which consists of real polynomials with fine properties. They are orthogonal on the real line instead of the imaginary axes for MLP. Beside recurrence relations and zeros, we will point to the closed form of its Fourier transform. The most important contribution consists of the new differential properties, especially the finite and infinite differential equation.","sentences":["This paper deals with the Mittag-Leffler polynomials (MLP) by extracting their essence which consists of real polynomials with fine properties.","They are orthogonal on the real line instead of the imaginary axes for MLP.","Beside recurrence relations and zeros, we will point to the closed form of its Fourier transform.","The most important contribution consists of the new differential properties, especially the finite and infinite differential equation."],"url":"http://arxiv.org/abs/2402.07795v1","category":"math.CA"}
{"created":"2024-02-12 16:54:55","title":"Instability of periodic waves for the Korteweg-de Vries-Burgers equation with monostable source","abstract":"In this paper, it is proved that the KdV-Burgers equation with a monostable source term of Fisher-KPP type has small-amplitude periodic traveling wave solutions with finite fundamental period. These solutions emerge from a supercritical local Hopf bifurcation around a critical value of the wave speed. Moreover, it is shown that these periodic waves are spectrally unstable as solutions to the PDE, that is, the Floquet (continuous) spectrum of the linearization around each periodic wave intersects the unstable half plane of complex values with positive real part. To that end, classical perturbation theory for linear operators is applied in order to prove that the spectrum of the linearized operator around the wave can be approximated by that of a constant coefficient operator around the zero solution, which intersects the unstable complex half plane.","sentences":["In this paper, it is proved that the KdV-Burgers equation with a monostable source term of Fisher-KPP type has small-amplitude periodic traveling wave solutions with finite fundamental period.","These solutions emerge from a supercritical local Hopf bifurcation around a critical value of the wave speed.","Moreover, it is shown that these periodic waves are spectrally unstable as solutions to the PDE, that is, the Floquet (continuous) spectrum of the linearization around each periodic wave intersects the unstable half plane of complex values with positive real part.","To that end, classical perturbation theory for linear operators is applied in order to prove that the spectrum of the linearized operator around the wave can be approximated by that of a constant coefficient operator around the zero solution, which intersects the unstable complex half plane."],"url":"http://arxiv.org/abs/2402.07789v1","category":"math.AP"}
{"created":"2024-02-12 16:47:13","title":"Solving parameter-dependent semi-algebraic systems","abstract":"We consider systems of polynomial equations and inequalities in $\\mathbb{Q}[\\boldsymbol{y}][\\boldsymbol{x}]$ where $\\boldsymbol{x} = (x_1, \\ldots, x_n)$ and $\\boldsymbol{y} = (y_1, \\ldots,y_t)$. The $\\boldsymbol{y}$ indeterminates are considered as parameters and we assume that when specialising them generically, the set of common complex solutions, to the obtained equations, is finite. We consider the problem of real root classification for such parameter-dependent problems, i.e. identifying the possible number of real solutions depending on the values of the parameters and computing a description of the regions of the space of parameters over which the number of real roots remains invariant.   We design an algorithm for solving this problem. The formulas it outputs enjoy a determinantal structure. Under genericity assumptions, we show that its arithmetic complexity is polynomial in both the maximum degree $d$ and the number $s$ of the input inequalities and exponential in $nt+t^2$. The output formulas consist of polynomials of degree bounded by $(2s+n)d^{n+1}$. This is the first algorithm with such a singly exponential complexity. We report on practical experiments showing that a first implementation of this algorithm can tackle examples which were previously out of reach.","sentences":["We consider systems of polynomial equations and inequalities in $\\mathbb{Q}[\\boldsymbol{y}][\\boldsymbol{x}]$ where $\\boldsymbol{x} = (x_1, \\ldots, x_n)$ and $\\boldsymbol{y} = (y_1, \\ldots,y_t)$. The $\\boldsymbol{y}$ indeterminates are considered as parameters and we assume that when specialising them generically, the set of common complex solutions, to the obtained equations, is finite.","We consider the problem of real root classification for such parameter-dependent problems, i.e. identifying the possible number of real solutions depending on the values of the parameters and computing a description of the regions of the space of parameters over which the number of real roots remains invariant.   ","We design an algorithm for solving this problem.","The formulas it outputs enjoy a determinantal structure.","Under genericity assumptions, we show that its arithmetic complexity is polynomial in both the maximum degree $d$ and the number $s$ of the input inequalities and exponential in $nt+t^2$. The output formulas consist of polynomials of degree bounded by $(2s+n)d^{n+1}$. This is the first algorithm with such a singly exponential complexity.","We report on practical experiments showing that a first implementation of this algorithm can tackle examples which were previously out of reach."],"url":"http://arxiv.org/abs/2402.07782v1","category":"cs.SC"}
{"created":"2024-02-12 16:44:09","title":"On the Asymptotic Behavior in Time of the Kinetic Energy in a Rigid Body-Liquid Problem","abstract":"We give sufficient conditions on the initial data for the decay in time of the kinetic energy, $E$, of solutions to the system of equations describing the motion of a rigid body in a Navier-Stokes liquid. More precisely, assuming the initial data ``small\" in appropriate norm, we show that if, in addition, the initial velocity field of the liquid, $v_0$, is in $L^q$, $q\\in(1,2)$, then $E(t)$ vanishes as $t\\to\\infty$ with a specific order of decay. The order remains, however, unspecified if $v_0\\in L^2$.","sentences":["We give sufficient conditions on the initial data for the decay in time of the kinetic energy, $E$, of solutions to the system of equations describing the motion of a rigid body in a Navier-Stokes liquid.","More precisely, assuming the initial data ``small\" in appropriate norm, we show that if, in addition, the initial velocity field of the liquid, $v_0$, is in $L^q$, $q\\in(1,2)$, then $E(t)$ vanishes as $t\\to\\infty$ with a specific order of decay.","The order remains, however, unspecified if $v_0\\in L^2$."],"url":"http://arxiv.org/abs/2402.07780v1","category":"math.AP"}
{"created":"2024-02-12 16:33:56","title":"Relativistic corrections to prompt double charmonium hadroproduction near threshold","abstract":"We calculate the relativistic corrections to prompt $J/\\psi$ pair and $J/\\psi+\\psi(2S)$ hadroproduction through the color-singlet channel within the framework of nonrelativistic QCD (NRQCD) factorization. The short-distance coefficients are obtained by matching full-QCD and NRQCD calculations at the partonic level, in which both squared amplitude and phase space are expanded in $v^2$. We find that such an expansion of the phase space spoils the convergence of NRQCD factorization near the production threshold. To fix this problem, we propose to modify the matching between full QCD and NRQCD by adopting the physical phase space. In this modified approach, the theoretical uncertainties due to the choice of charm quark mass $m_c$ are largely reduced and the overall agreement of our predictions with LHC data is significantly improved, both as for total and differential cross sections.","sentences":["We calculate the relativistic corrections to prompt $J/\\psi$ pair and $J/\\psi+\\psi(2S)$ hadroproduction through the color-singlet channel within the framework of nonrelativistic QCD (NRQCD) factorization.","The short-distance coefficients are obtained by matching full-QCD and NRQCD calculations at the partonic level, in which both squared amplitude and phase space are expanded in $v^2$. We find that such an expansion of the phase space spoils the convergence of NRQCD factorization near the production threshold.","To fix this problem, we propose to modify the matching between full QCD and NRQCD by adopting the physical phase space.","In this modified approach, the theoretical uncertainties due to the choice of charm quark mass $m_c$ are largely reduced and the overall agreement of our predictions with LHC data is significantly improved, both as for total and differential cross sections."],"url":"http://arxiv.org/abs/2402.07773v1","category":"hep-ph"}
{"created":"2024-02-12 16:28:57","title":"Multi-level Optimal Control with Neural Surrogate Models","abstract":"Optimal actuator and control design is studied as a multi-level optimisation problem, where the actuator design is evaluated based on the performance of the associated optimal closed loop. The evaluation of the optimal closed loop for a given actuator realisation is a computationally demanding task, for which the use of a neural network surrogate is proposed. The use of neural network surrogates to replace the lower level of the optimisation hierarchy enables the use of fast gradient-based and gradient-free consensus-based optimisation methods to determine the optimal actuator design. The effectiveness of the proposed surrogate models and optimisation methods is assessed in a test related to optimal actuator location for heat control.","sentences":["Optimal actuator and control design is studied as a multi-level optimisation problem, where the actuator design is evaluated based on the performance of the associated optimal closed loop.","The evaluation of the optimal closed loop for a given actuator realisation is a computationally demanding task, for which the use of a neural network surrogate is proposed.","The use of neural network surrogates to replace the lower level of the optimisation hierarchy enables the use of fast gradient-based and gradient-free consensus-based optimisation methods to determine the optimal actuator design.","The effectiveness of the proposed surrogate models and optimisation methods is assessed in a test related to optimal actuator location for heat control."],"url":"http://arxiv.org/abs/2402.07763v1","category":"math.OC"}
{"created":"2024-02-12 16:24:41","title":"Global existence for the Willmore flow with boundary via Simon's Li-Yau inequality","abstract":"It is well-known that the Willmore flow of closed spherical immersions exists globally in time and converges if the initial datum has Willmore energy below $8\\pi$ - exactly the Li-Yau energy threshold below which all closed immersions are embedded. Extending the Li-Yau inequality for closed surfaces via Simon's monotonicity formula also for surfaces with boundary, given Dirichlet boundary conditions, one obtains an energy threshold $C_{\\mathrm{LY}}$ below which surfaces with this boundary are embedded.   With a new argument, using the Li-Yau inequality and tools from geometric measure theory, we show that the Willmore flow with Dirichlet boundary data starting in cylindrical surfaces of revolution exists globally in time if the energy of the initial datum is below $C_{\\mathrm{LY}}$. Moreover, given Dirichlet boundary data, we also obtain the existence of a Willmore minimizer in the class of cylindrical surfaces of revolution if the corresponding infimum lies below $C_{\\mathrm{LY}}$ which improves previous results for the stationary problem.","sentences":["It is well-known that the Willmore flow of closed spherical immersions exists globally in time and converges if the initial datum has Willmore energy below $8\\pi$ - exactly the Li-Yau energy threshold below which all closed immersions are embedded.","Extending the Li-Yau inequality for closed surfaces via Simon's monotonicity formula also for surfaces with boundary, given Dirichlet boundary conditions, one obtains an energy threshold $C_{\\mathrm{LY}}$ below which surfaces with this boundary are embedded.   ","With a new argument, using the Li-Yau inequality and tools from geometric measure theory, we show that the Willmore flow with Dirichlet boundary data starting in cylindrical surfaces of revolution exists globally in time if the energy of the initial datum is below $C_{\\mathrm{LY}}$. Moreover, given Dirichlet boundary data, we also obtain the existence of a Willmore minimizer in the class of cylindrical surfaces of revolution if the corresponding infimum lies below $C_{\\mathrm{LY}}$ which improves previous results for the stationary problem."],"url":"http://arxiv.org/abs/2402.07755v1","category":"math.AP"}
{"created":"2024-02-12 16:15:28","title":"Minimally Interactive Segmentation of Soft-Tissue Tumors on CT and MRI using Deep Learning","abstract":"Segmentations are crucial in medical imaging to obtain morphological, volumetric, and radiomics biomarkers. Manual segmentation is accurate but not feasible in the radiologist's clinical workflow, while automatic segmentation generally obtains sub-par performance. We therefore developed a minimally interactive deep learning-based segmentation method for soft-tissue tumors (STTs) on CT and MRI. The method requires the user to click six points near the tumor's extreme boundaries. These six points are transformed into a distance map and serve, with the image, as input for a Convolutional Neural Network. For training and validation, a multicenter dataset containing 514 patients and nine STT types in seven anatomical locations was used, resulting in a Dice Similarity Coefficient (DSC) of 0.85$\\pm$0.11 (mean $\\pm$ standard deviation (SD)) for CT and 0.84$\\pm$0.12 for T1-weighted MRI, when compared to manual segmentations made by expert radiologists. Next, the method was externally validated on a dataset including five unseen STT phenotypes in extremities, achieving 0.81$\\pm$0.08 for CT, 0.84$\\pm$0.09 for T1-weighted MRI, and 0.88\\pm0.08 for previously unseen T2-weighted fat-saturated (FS) MRI. In conclusion, our minimally interactive segmentation method effectively segments different types of STTs on CT and MRI, with robust generalization to previously unseen phenotypes and imaging modalities.","sentences":["Segmentations are crucial in medical imaging to obtain morphological, volumetric, and radiomics biomarkers.","Manual segmentation is accurate but not feasible in the radiologist's clinical workflow, while automatic segmentation generally obtains sub-par performance.","We therefore developed a minimally interactive deep learning-based segmentation method for soft-tissue tumors (STTs) on CT and MRI.","The method requires the user to click six points near the tumor's extreme boundaries.","These six points are transformed into a distance map and serve, with the image, as input for a Convolutional Neural Network.","For training and validation, a multicenter dataset containing 514 patients and nine STT types in seven anatomical locations was used, resulting in a Dice Similarity Coefficient (DSC) of 0.85$\\pm$0.11 (mean $\\pm$ standard deviation (SD)) for CT and 0.84$\\pm$0.12 for T1-weighted MRI, when compared to manual segmentations made by expert radiologists.","Next, the method was externally validated on a dataset including five unseen STT phenotypes in extremities, achieving 0.81$\\pm$0.08 for CT, 0.84$\\pm$0.09 for T1-weighted MRI, and 0.88\\pm0.08 for previously unseen T2-weighted fat-saturated (FS) MRI.","In conclusion, our minimally interactive segmentation method effectively segments different types of STTs on CT and MRI, with robust generalization to previously unseen phenotypes and imaging modalities."],"url":"http://arxiv.org/abs/2402.07746v1","category":"eess.IV"}
{"created":"2024-02-12 15:52:09","title":"Liftable Point-Line Configurations: Defining Equations and Irreducibility of Associated Matroid and Circuit Varieties","abstract":"We study point-line configurations through the lens of projective geometry and matroid theory. Our focus is on their realisation spaces, where we introduce the concepts of liftable and quasi-liftable configurations, exploring cases in which an $n$-tuple of collinear points can be lifted to a non-degenerate realisation of a point-line configuration. We show that forest configurations are liftable and characterise the realisation space of liftable configurations as the solution set of certain linear systems of equations. Moreover, we study the Zariski closure of the realisation spaces of liftable and quasi-liftable configurations, known as matroid varieties, and establish their irreducibility. Additionally, we compute an irreducible decomposition for their corresponding circuit varieties. Applying these liftability properties, we present a procedure generate some of the defining equations of the associated matroid varieties. As corollaries, we provide a geometric representation for the defining equations of two specific examples: the quadrilateral set and the $3\\times4$ grid. While the polynomials for the latter were previously computed using specialised algorithms tailored for this configuration, the geometric interpretation of these generators was missing. We compute a minimal generating set for the corresponding ideals.","sentences":["We study point-line configurations through the lens of projective geometry and matroid theory.","Our focus is on their realisation spaces, where we introduce the concepts of liftable and quasi-liftable configurations, exploring cases in which an $n$-tuple of collinear points can be lifted to a non-degenerate realisation of a point-line configuration.","We show that forest configurations are liftable and characterise the realisation space of liftable configurations as the solution set of certain linear systems of equations.","Moreover, we study the Zariski closure of the realisation spaces of liftable and quasi-liftable configurations, known as matroid varieties, and establish their irreducibility.","Additionally, we compute an irreducible decomposition for their corresponding circuit varieties.","Applying these liftability properties, we present a procedure generate some of the defining equations of the associated matroid varieties.","As corollaries, we provide a geometric representation for the defining equations of two specific examples: the quadrilateral set and the $3\\times4$ grid.","While the polynomials for the latter were previously computed using specialised algorithms tailored for this configuration, the geometric interpretation of these generators was missing.","We compute a minimal generating set for the corresponding ideals."],"url":"http://arxiv.org/abs/2402.07737v1","category":"math.CO"}
{"created":"2024-02-12 15:49:19","title":"Multimodal Learned Sparse Retrieval for Image Suggestion","abstract":"Learned Sparse Retrieval (LSR) is a group of neural methods designed to encode queries and documents into sparse lexical vectors. These vectors can be efficiently indexed and retrieved using an inverted index. While LSR has shown promise in text retrieval, its potential in multi-modal retrieval remains largely unexplored. Motivated by this, in this work, we explore the application of LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse Retrieval (MLSR). We conduct experiments using several MLSR model configurations and evaluate the performance on the image suggestion task. We find that solving the task solely based on the image content is challenging. Enriching the image content with its caption improves the model performance significantly, implying the importance of image captions to provide fine-grained concepts and context information of images. Our approach presents a practical and effective solution for training LSR retrieval models in multi-modal settings.","sentences":["Learned Sparse Retrieval (LSR) is a group of neural methods designed to encode queries and documents into sparse lexical vectors.","These vectors can be efficiently indexed and retrieved using an inverted index.","While LSR has shown promise in text retrieval, its potential in multi-modal retrieval remains largely unexplored.","Motivated by this, in this work, we explore the application of LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse Retrieval (MLSR).","We conduct experiments using several MLSR model configurations and evaluate the performance on the image suggestion task.","We find that solving the task solely based on the image content is challenging.","Enriching the image content with its caption improves the model performance significantly, implying the importance of image captions to provide fine-grained concepts and context information of images.","Our approach presents a practical and effective solution for training LSR retrieval models in multi-modal settings."],"url":"http://arxiv.org/abs/2402.07736v1","category":"cs.IR"}
{"created":"2024-02-12 15:48:58","title":"Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism","abstract":"In statistics and machine learning, detecting dependencies in datasets is a central challenge. We propose a novel neural network model for supervised graph structure learning, i.e., the process of learning a mapping between observational data and their underlying dependence structure. The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. By leveraging structural equation models and employing randomly generated multivariate Chebyshev polynomials for the simulation of training data, our method demonstrates robust generalizability across both linear and various types of non-linear dependencies. We introduce a novel bilinear attention mechanism (BAM) for explicit processing of dependency information, which operates on the level of covariance matrices of transformed data and respects the geometry of the manifold of symmetric positive definite matrices. Empirical evaluation demonstrates the robustness of our method in detecting a wide range of dependencies, excelling in undirected graph estimation and proving competitive in completed partially directed acyclic graph estimation through a novel two-step approach.","sentences":["In statistics and machine learning, detecting dependencies in datasets is a central challenge.","We propose a novel neural network model for supervised graph structure learning, i.e., the process of learning a mapping between observational data and their underlying dependence structure.","The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference.","By leveraging structural equation models and employing randomly generated multivariate Chebyshev polynomials for the simulation of training data, our method demonstrates robust generalizability across both linear and various types of non-linear dependencies.","We introduce a novel bilinear attention mechanism (BAM) for explicit processing of dependency information, which operates on the level of covariance matrices of transformed data and respects the geometry of the manifold of symmetric positive definite matrices.","Empirical evaluation demonstrates the robustness of our method in detecting a wide range of dependencies, excelling in undirected graph estimation and proving competitive in completed partially directed acyclic graph estimation through a novel two-step approach."],"url":"http://arxiv.org/abs/2402.07735v2","category":"stat.ML"}
{"created":"2024-02-12 15:46:17","title":"Approximate Analytical Solutions for the Circular Restricted Three-Body Problem Including Non-Hamiltonian Solar Radiation Pressure","abstract":"The circular restricted three-body problem (CR3BP) with solar radiation pressure (SRP) has often been analyzed with assumptions made on a spacecraft's attitude, such that the problem remains Hamiltonian. These assumptions are unsatisfactorily limiting for a starshade mission since the starshade's attitude will inherently vary from the configuration that corresponds to Hamiltonian dynamics. This paper presents the derivation of the equations of motion for CR3BP with SRP that permit the application of the Lindstedt-Poincare method, such that approximate solutions are produced, which may serve as invaluable trajectory design tools. Examples of periodic orbits and manifolds corresponding to three sets of attitude angles are shown and the accuracy of their seventh-order approximations is considered.","sentences":["The circular restricted three-body problem (CR3BP) with solar radiation pressure (SRP) has often been analyzed with assumptions made on a spacecraft's attitude, such that the problem remains Hamiltonian.","These assumptions are unsatisfactorily limiting for a starshade mission since the starshade's attitude will inherently vary from the configuration that corresponds to Hamiltonian dynamics.","This paper presents the derivation of the equations of motion for CR3BP with SRP that permit the application of the Lindstedt-Poincare method, such that approximate solutions are produced, which may serve as invaluable trajectory design tools.","Examples of periodic orbits and manifolds corresponding to three sets of attitude angles are shown and the accuracy of their seventh-order approximations is considered."],"url":"http://arxiv.org/abs/2402.07734v1","category":"math.DS"}
{"created":"2024-02-12 15:39:05","title":"Unsupervised Sign Language Translation and Generation","abstract":"Motivated by the success of unsupervised neural machine translation (UNMT), we introduce an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences. We propose a sliding window method to address the issues of aligning variable-length text with video sequences. To our knowledge, USLNet is the first unsupervised sign language translation and generation model capable of generating both natural language text and sign language video in a unified manner. Experimental results on the BBC-Oxford Sign Language dataset (BOBSL) and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet achieves competitive results compared to supervised baseline models, indicating its effectiveness in sign language translation and generation.","sentences":["Motivated by the success of unsupervised neural machine translation (UNMT), we introduce an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data.","USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure.","Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences.","We propose a sliding window method to address the issues of aligning variable-length text with video sequences.","To our knowledge, USLNet is the first unsupervised sign language translation and generation model capable of generating both natural language text and sign language video in a unified manner.","Experimental results on the BBC-Oxford Sign Language dataset (BOBSL) and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet achieves competitive results compared to supervised baseline models, indicating its effectiveness in sign language translation and generation."],"url":"http://arxiv.org/abs/2402.07726v1","category":"cs.CL"}
{"created":"2024-02-12 15:35:32","title":"Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation","abstract":"Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms. To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE). In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to prior art. Our results further identify a phase transition phenomenon, which suggests that heavy tails can be either beneficial or harmful depending on the problem structure. We support our theory with experiments conducted in a variety of settings.","sentences":["Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years.","While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms.","Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms.","To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE).","In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to prior art.","Our results further identify a phase transition phenomenon, which suggests that heavy tails can be either beneficial or harmful depending on the problem structure.","We support our theory with experiments conducted in a variety of settings."],"url":"http://arxiv.org/abs/2402.07723v1","category":"stat.ML"}
{"created":"2024-02-12 15:35:24","title":"Path Integral Monte Carlo Study of a Doubly-Dipolar Bose Gas","abstract":"By combining first-principles path integral Monte Carlo methods and mean-field techniques, we explore the properties of cylindrically trapped doubly-dipolar Bose gases. We first verify the emergence of a pancake quantum droplet at low temperatures, validating previously mean-field calculations. In a regime of small doubly-dipolar interactions, first-principles calculations agree with the generalized Gross-Pitaevskii equation. Such an accordance disappears in a large interaction limit. Here the path integral Monte Carlo estimates the strong doubly-dipolar regime with accuracy. On the contrary, the Gross-Pitaevskii equation does not seize quantum fluctuations in full. We also provide a complete description of the system's quantum behavior in a wide range of parameters. When the system forms a droplet, the superfluid fraction exhibits an anisotropic behavior if compared to the usual Bose gas regime. Interestingly, we observe that the transition temperature from thermal gas to droplet results higher than that of the thermal gas to a Bose-Einstein condensate, indicating the robustness of the droplet against thermal fluctuations. Further, we investigate the anisotropic behavior of the superfluid fraction during the structural transition from a pancake to a cigar-shaped droplet by varying the ratio between electric and magnetic dipole interaction strengths. Our findings furnish evidence that the stability of doubly-dipolar Bose-Einstein condensates can be detected in experiments by means of dysprosium atoms.","sentences":["By combining first-principles path integral Monte Carlo methods and mean-field techniques, we explore the properties of cylindrically trapped doubly-dipolar Bose gases.","We first verify the emergence of a pancake quantum droplet at low temperatures, validating previously mean-field calculations.","In a regime of small doubly-dipolar interactions, first-principles calculations agree with the generalized Gross-Pitaevskii equation.","Such an accordance disappears in a large interaction limit.","Here the path integral Monte Carlo estimates the strong doubly-dipolar regime with accuracy.","On the contrary, the Gross-Pitaevskii equation does not seize quantum fluctuations in full.","We also provide a complete description of the system's quantum behavior in a wide range of parameters.","When the system forms a droplet, the superfluid fraction exhibits an anisotropic behavior if compared to the usual Bose gas regime.","Interestingly, we observe that the transition temperature from thermal gas to droplet results higher than that of the thermal gas to a Bose-Einstein condensate, indicating the robustness of the droplet against thermal fluctuations.","Further, we investigate the anisotropic behavior of the superfluid fraction during the structural transition from a pancake to a cigar-shaped droplet by varying the ratio between electric and magnetic dipole interaction strengths.","Our findings furnish evidence that the stability of doubly-dipolar Bose-Einstein condensates can be detected in experiments by means of dysprosium atoms."],"url":"http://arxiv.org/abs/2402.07722v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-12 15:32:38","title":"Efficient reductions between some statistical models","abstract":"We study the problem of approximately transforming a sample from a source statistical model to a sample from a target statistical model without knowing the parameters of the source model, and construct several computationally efficient such reductions between statistical experiments. In particular, we provide computationally efficient procedures that approximately reduce uniform, Erlang, and Laplace location models to general target families. We illustrate our methodology by establishing nonasymptotic reductions between some canonical high-dimensional problems, spanning mixtures of experts, phase retrieval, and signal denoising. Notably, the reductions are structure preserving and can accommodate missing data. We also point to a possible application in transforming one differentially private mechanism to another.","sentences":["We study the problem of approximately transforming a sample from a source statistical model to a sample from a target statistical model without knowing the parameters of the source model, and construct several computationally efficient such reductions between statistical experiments.","In particular, we provide computationally efficient procedures that approximately reduce uniform, Erlang, and Laplace location models to general target families.","We illustrate our methodology by establishing nonasymptotic reductions between some canonical high-dimensional problems, spanning mixtures of experts, phase retrieval, and signal denoising.","Notably, the reductions are structure preserving and can accommodate missing data.","We also point to a possible application in transforming one differentially private mechanism to another."],"url":"http://arxiv.org/abs/2402.07717v1","category":"math.ST"}
{"created":"2024-02-12 15:06:56","title":"Optimal consumption and investment under relative performance criteria with Epstein-Zin utility","abstract":"We consider the strategic interaction of traders in a continuous-time financial market with Epstein-Zin-type recursive intertemporal preferences and performance concerns. We derive explicitly an equilibrium for the finite player and the mean-field version of the game, based on a study of geometric backward stochastic differential equations of Bernoulli type that describe the best replies of traders. Our results show that Epstein-Zin preferences can lead to substantially different equilibrium behavior.","sentences":["We consider the strategic interaction of traders in a continuous-time financial market with Epstein-Zin-type recursive intertemporal preferences and performance concerns.","We derive explicitly an equilibrium for the finite player and the mean-field version of the game, based on a study of geometric backward stochastic differential equations of Bernoulli type that describe the best replies of traders.","Our results show that Epstein-Zin preferences can lead to substantially different equilibrium behavior."],"url":"http://arxiv.org/abs/2402.07698v1","category":"math.OC"}
{"created":"2024-02-12 18:48:50","title":"Equivalence of cost concentration and gradient vanishing for quantum circuits: an elementary proof in the Riemannian formulation","abstract":"The optimization of quantum circuits can be hampered by a decay of average gradient amplitudes with the system size. When the decay is exponential, this is called the barren plateau problem. Considering explicit circuit parametrizations (in terms of rotation angles), it has been shown in Arrasmith et al., Quantum Sci. Technol. 7, 045015 (2022) that barren plateaus are equivalent to an exponential decay of the cost-function variance. We show that the issue becomes particularly simple in the (parametrization-free) Riemannian formulation of such optimization problems. An elementary derivation shows that the single-gate variance of the cost function is strictly equal to half the variance of the Riemannian single-gate gradient, where we sample variable gates according to the uniform Haar measure. The total variances of the cost function and its gradient are both bounded from above by the sum of single-gate variances and, conversely, bound single-gate variances from above. So, decays of gradients and cost-function variations go hand in hand, and barren plateau problems cannot be resolved by avoiding gradient-based in favor of gradient-free optimization methods.","sentences":["The optimization of quantum circuits can be hampered by a decay of average gradient amplitudes with the system size.","When the decay is exponential, this is called the barren plateau problem.","Considering explicit circuit parametrizations (in terms of rotation angles), it has been shown in Arrasmith et al., Quantum Sci. Technol. 7, 045015 (2022) that barren plateaus are equivalent to an exponential decay of the cost-function variance.","We show that the issue becomes particularly simple in the (parametrization-free) Riemannian formulation of such optimization problems.","An elementary derivation shows that the single-gate variance of the cost function is strictly equal to half the variance of the Riemannian single-gate gradient, where we sample variable gates according to the uniform Haar measure.","The total variances of the cost function and its gradient are both bounded from above by the sum of single-gate variances and, conversely, bound single-gate variances from above.","So, decays of gradients and cost-function variations go hand in hand, and barren plateau problems cannot be resolved by avoiding gradient-based in favor of gradient-free optimization methods."],"url":"http://arxiv.org/abs/2402.07883v1","category":"quant-ph"}
{"created":"2024-02-12 18:33:47","title":"PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs","abstract":"Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.","sentences":["Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding.","This opens the door to richer interaction with the world, for example robotic control.","However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories.","How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   ","In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering.","In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories).","The VLM then selects the best ones for the task.","These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer.","We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization.","We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities.","Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains.","Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo."],"url":"http://arxiv.org/abs/2402.07872v1","category":"cs.RO"}
{"created":"2024-02-12 18:29:17","title":"Nesting Particle Filters for Experimental Design in Dynamical Systems","abstract":"In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization. This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.","sentences":["In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization.","We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization.","This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance.","Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies."],"url":"http://arxiv.org/abs/2402.07868v1","category":"cs.LG"}
{"created":"2024-02-12 18:00:23","title":"Introducing Novel Planar Micromixers with Pillars and Gaps and Studying the Impact of Various Geometric Parameters on the Efficiency of Micromixers","abstract":"Chemical bioreactions play a significant role in many of the microfluidic devices, and their applications in biomedical science have seen substantial growth. Given that effective mixing is vital for initiating biochemical reactions in many applications, micromixers have become increasingly prevalent for high-throughput assays. In this study, numerical study is conducted to examine the fluid flow and mass transfer characteristics in novel micromixers featuring an array of pillars. The study explores the effects of pillar array design on mixing performance and pressure drop, drawing from principles such as contraction-expansion and split-recombine. Two configurations of pillar arrays are introduced, each undergoing investigation regarding parameters such as pillar diameter, gap size between pillar groups, distance between pillars, and vertical shift in pillar groups. Subsequently, optimal micromixers are identified, exhibiting mixing efficiency exceeding 99.7% at moderate Reynolds number (Re = 1), a level typically challenging for micromixers to attain high mixing efficiency. Notably, the pressure drop remains low at 1102 Pa. Furthermore, the variations in mixing index over time and across different positions along the channel are examined. Both configurations demonstrate short mixing lengths and times. The combination of rapid mixing, low pressure drop, and short mixing length positions the novel micromixers as highly promising for microfluidic applications.","sentences":["Chemical bioreactions play a significant role in many of the microfluidic devices, and their applications in biomedical science have seen substantial growth.","Given that effective mixing is vital for initiating biochemical reactions in many applications, micromixers have become increasingly prevalent for high-throughput assays.","In this study, numerical study is conducted to examine the fluid flow and mass transfer characteristics in novel micromixers featuring an array of pillars.","The study explores the effects of pillar array design on mixing performance and pressure drop, drawing from principles such as contraction-expansion and split-recombine.","Two configurations of pillar arrays are introduced, each undergoing investigation regarding parameters such as pillar diameter, gap size between pillar groups, distance between pillars, and vertical shift in pillar groups.","Subsequently, optimal micromixers are identified, exhibiting mixing efficiency exceeding 99.7% at moderate Reynolds number (Re = 1), a level typically challenging for micromixers to attain high mixing efficiency.","Notably, the pressure drop remains low at 1102 Pa.","Furthermore, the variations in mixing index over time and across different positions along the channel are examined.","Both configurations demonstrate short mixing lengths and times.","The combination of rapid mixing, low pressure drop, and short mixing length positions the novel micromixers as highly promising for microfluidic applications."],"url":"http://arxiv.org/abs/2402.07854v1","category":"physics.flu-dyn"}
{"created":"2024-02-12 17:34:13","title":"Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model","abstract":"Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101","sentences":["Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages.","What does it take to broaden access to breakthroughs beyond first-class citizen languages?","Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced.","Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages.","We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance.","Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.","We open-source our instruction datasets and our model at https://hf.co/CohereForAI/aya-101"],"url":"http://arxiv.org/abs/2402.07827v1","category":"cs.CL"}
{"created":"2024-02-12 17:12:09","title":"A comparison of six linear and non-linear mixed models for longitudinal data: application to late-life cognitive trajectories","abstract":"Longitudinal characterization of cognitive change in late-life has received increasing attention to better understand age-related cognitive aging and cognitive changes reflecting pathology-related and mortality-related processes. Several mixed-effects models have been proposed to accommodate the non-linearity of cognitive decline and assess the putative influence of covariates on it. In this work, we examine the standard linear mixed model (LMM) with a linear function of time and five alternative models capturing non-linearity of change over time, including the LMM with a quadratic term, LMM with splines, the functional mixed model, the piecewise linear mixed model and the sigmoidal mixed model. We first theoretically describe the models. Next, using data from deceased participants from two prospective cohorts with annual cognitive testing, we compared the interpretation of the models by investigating the association of education on cognitive change before death. Finally, we performed a simulation study to empirically evaluate the models and provide practical recommendations. In particular, models were challenged by increasing follow-up spacing, increasing missing data, and decreasing sample size. With the exception of the LMM with a quadratic term, the fit of all models was generally adequate to capture non-linearity of cognitive change and models were relatively robust. Although spline-based models do not have interpretable nonlinearity parameters, their convergence was easier to achieve and they allow for graphical interpretation. In contrast the piecewise and the sigmoidal models, with interpretable non-linear parameters may require more data to achieve convergence.","sentences":["Longitudinal characterization of cognitive change in late-life has received increasing attention to better understand age-related cognitive aging and cognitive changes reflecting pathology-related and mortality-related processes.","Several mixed-effects models have been proposed to accommodate the non-linearity of cognitive decline and assess the putative influence of covariates on it.","In this work, we examine the standard linear mixed model (LMM) with a linear function of time and five alternative models capturing non-linearity of change over time, including the LMM with a quadratic term, LMM with splines, the functional mixed model, the piecewise linear mixed model and the sigmoidal mixed model.","We first theoretically describe the models.","Next, using data from deceased participants from two prospective cohorts with annual cognitive testing, we compared the interpretation of the models by investigating the association of education on cognitive change before death.","Finally, we performed a simulation study to empirically evaluate the models and provide practical recommendations.","In particular, models were challenged by increasing follow-up spacing, increasing missing data, and decreasing sample size.","With the exception of the LMM with a quadratic term, the fit of all models was generally adequate to capture non-linearity of cognitive change and models were relatively robust.","Although spline-based models do not have interpretable nonlinearity parameters, their convergence was easier to achieve and they allow for graphical interpretation.","In contrast the piecewise and the sigmoidal models, with interpretable non-linear parameters may require more data to achieve convergence."],"url":"http://arxiv.org/abs/2402.07806v1","category":"stat.AP"}
{"created":"2024-02-12 16:47:08","title":"IR-Aware ECO Timing Optimization Using Reinforcement Learning","abstract":"Engineering change orders (ECOs) in late stages make minimal design fixes to recover from timing shifts due to excessive IR drops. This paper integrates IR-drop-aware timing analysis and ECO timing optimization using reinforcement learning (RL). The method operates after physical design and power grid synthesis, and rectifies IR-drop-induced timing degradation through gate sizing. It incorporates the Lagrangian relaxation (LR) technique into a novel RL framework, which trains a relational graph convolutional network (R-GCN) agent to sequentially size gates to fix timing violations. The R-GCN agent outperforms a classical LR-only algorithm: in an open 45nm technology, it (a) moves the Pareto front of the delay-area tradeoff curve to the left and (b) saves runtime over the classical method by running fast inference using trained models at iso-quality. The RL model is transferable across timing specifications, and transferable to unseen designs with zero-shot learning or fine tuning.","sentences":["Engineering change orders (ECOs) in late stages make minimal design fixes to recover from timing shifts due to excessive IR drops.","This paper integrates IR-drop-aware timing analysis and ECO timing optimization using reinforcement learning (RL).","The method operates after physical design and power grid synthesis, and rectifies IR-drop-induced timing degradation through gate sizing.","It incorporates the Lagrangian relaxation (LR) technique into a novel RL framework, which trains a relational graph convolutional network (R-GCN) agent to sequentially size gates to fix timing violations.","The R-GCN agent outperforms a classical LR-only algorithm: in an open 45nm technology, it (a) moves the Pareto front of the delay-area tradeoff curve to the left and (b) saves runtime over the classical method by running fast inference using trained models at iso-quality.","The RL model is transferable across timing specifications, and transferable to unseen designs with zero-shot learning or fine tuning."],"url":"http://arxiv.org/abs/2402.07781v1","category":"cs.AR"}
{"created":"2024-02-12 16:33:23","title":"Insights into $(k,\u03c1)$-shortcutting algorithms","abstract":"A graph is called a $(k,\\rho)$-graph iff every node can reach $\\rho$ of its nearest neighbors in at most k hops. This property proved useful in the analysis and design of parallel shortest-path algorithms. Any graph can be transformed into a $(k,\\rho)$-graph by adding shortcuts. Formally, the $(k,\\rho)$-Minimum-Shortcut problem asks to find an appropriate shortcut set of minimal cardinality.   We show that the $(k,\\rho)$-Minimum-Shortcut problem is NP-complete in the practical regime of $k \\ge 3$ and $\\rho = \\Theta(n^\\epsilon)$ for $\\epsilon > 0$. With a related construction, we bound the approximation factor of known $(k,\\rho)$-Minimum-Shortcut problem heuristics from below and propose algorithmic countermeasures improving the approximation quality. Further, we describe an integer linear problem (ILP) solving the $(k,\\rho)$-Minimum-Shortcut problem optimally. Finally, we compare the practical performance and quality of all algorithms in an empirical campaign.","sentences":["A graph is called a $(k,\\rho)$-graph iff every node can reach $\\rho$ of its nearest neighbors in at most k hops.","This property proved useful in the analysis and design of parallel shortest-path algorithms.","Any graph can be transformed into a $(k,\\rho)$-graph by adding shortcuts.","Formally, the $(k,\\rho)$-Minimum-Shortcut problem asks to find an appropriate shortcut set of minimal cardinality.   ","We show that the $(k,\\rho)$-Minimum-Shortcut problem is NP-complete in the practical regime of $k \\ge 3$ and $\\rho = \\Theta(n^\\epsilon)$ for $\\epsilon > 0$. With a related construction, we bound the approximation factor of known $(k,\\rho)$-Minimum-Shortcut problem heuristics from below and propose algorithmic countermeasures improving the approximation quality.","Further, we describe an integer linear problem (ILP) solving the $(k,\\rho)$-Minimum-Shortcut problem optimally.","Finally, we compare the practical performance and quality of all algorithms in an empirical campaign."],"url":"http://arxiv.org/abs/2402.07771v1","category":"cs.DS"}
{"created":"2024-02-12 16:32:14","title":"Observations of the new meteor shower from comet 46P/Wirtanen","abstract":"A new meteor shower $\\lambda$-Sculptorids produced by the comet 46P/Wirtanen was forecast for December 12, 2023. The predicted activity was highly uncertain, but generally considered to be low. Observations in Australia, New Zealand, and Oceania were solicited to help constrain the size distribution of meteoroids in the shower. This work aims to characterize the new meteor shower, by comparing the observed and predicted radiants and orbits, and to provide a calibration for future predictions. Global Meteor Network video cameras were used to observe the meteor shower. Multi-station observations were used to compute trajectories and orbits, while single-station observations were used to measure the flux profile. A total of 23 $\\lambda$-Sculptorid orbits have been measured. The shower peaked at a zenithal hourly rate (ZHR) of $0.65^{+0.24}_{-0.20}$ meteors per hour at $\\lambda_{\\odot} = 259.988^{\\circ} \\pm 0.042^{\\circ}$. Due to the low in-atmosphere speed of 15~km s$^{-1}$, the mean mass of observed meteoroids was 0.5~g ($\\sim10$~mm diameter), an order of magnitude higher than predicted. The dynamical simulations of the meteoroid stream can only produce such large meteoroids arriving at Earth in 2023 with correct radiants when a very low meteoroid density of $\\sim 100$~kg~m$^{-3}$ is assumed. However, this assumption cannot reproduce the activity profile. It may be reproduced by considering higher density meteoroids in a larger ecliptic plane-crossing time window ($\\Delta T$ = 20 days) and trails ejected prior to 1908, but then the observed radiant structure is not reproduced.","sentences":["A new meteor shower $\\lambda$-Sculptorids produced by the comet 46P/Wirtanen was forecast for December 12, 2023.","The predicted activity was highly uncertain, but generally considered to be low.","Observations in Australia, New Zealand, and Oceania were solicited to help constrain the size distribution of meteoroids in the shower.","This work aims to characterize the new meteor shower, by comparing the observed and predicted radiants and orbits, and to provide a calibration for future predictions.","Global Meteor Network video cameras were used to observe the meteor shower.","Multi-station observations were used to compute trajectories and orbits, while single-station observations were used to measure the flux profile.","A total of 23 $\\lambda$-Sculptorid orbits have been measured.","The shower peaked at a zenithal hourly rate (ZHR) of $0.65^{+0.24}_{-0.20}$ meteors per hour at $\\lambda_{\\odot} = 259.988^{\\circ} \\pm 0.042^{\\circ}$. Due to the low in-atmosphere speed of 15~km s$^{-1}$, the mean mass of observed meteoroids was 0.5~g ($\\sim10$~mm diameter), an order of magnitude higher than predicted.","The dynamical simulations of the meteoroid stream can only produce such large meteoroids arriving at Earth in 2023 with correct radiants when a very low meteoroid density of $\\sim 100$~kg~m$^{-3}$ is assumed.","However, this assumption cannot reproduce the activity profile.","It may be reproduced by considering higher density meteoroids in a larger ecliptic plane-crossing time window ($\\Delta T$ = 20 days) and trails ejected prior to 1908, but then the observed radiant structure is not reproduced."],"url":"http://arxiv.org/abs/2402.07769v1","category":"astro-ph.EP"}
{"created":"2024-02-12 16:30:03","title":"Multi-Facility Location Models Incorporating Multipurpose Shopping Trips","abstract":"This paper continues to develop and explore the impact of multipurpose trips on retail location. We develop the model of locating multiple competing facilities of a chain where several competing facilities exist in the area. There may be some existing facilities of the same chain as well. The addition of multiple new outlets can cause cannibalization of existing sales, but this effect is mitigated by selecting good locations, and the total market share captured by the chain increases. The introduction of multipurpose trips enhances the total market share of the location decision maker. Since in reality many customers combine a visit to more than one facility in one shopping trip, the model predicts the expected market share captured more accurately. Therefore, the selected locations for new facilities are more accurate as well.","sentences":["This paper continues to develop and explore the impact of multipurpose trips on retail location.","We develop the model of locating multiple competing facilities of a chain where several competing facilities exist in the area.","There may be some existing facilities of the same chain as well.","The addition of multiple new outlets can cause cannibalization of existing sales, but this effect is mitigated by selecting good locations, and the total market share captured by the chain increases.","The introduction of multipurpose trips enhances the total market share of the location decision maker.","Since in reality many customers combine a visit to more than one facility in one shopping trip, the model predicts the expected market share captured more accurately.","Therefore, the selected locations for new facilities are more accurate as well."],"url":"http://arxiv.org/abs/2402.07765v1","category":"math.OC"}
{"created":"2024-02-12 16:28:52","title":"Scalable Structure Learning for Sparse Context-Specific Causal Systems","abstract":"Several approaches to graphically representing context-specific relations among jointly distributed categorical variables have been proposed, along with structure learning algorithms. While existing optimization-based methods have limited scalability due to the large number of context-specific models, the constraint-based methods are more prone to error than even constraint-based DAG learning algorithms since more relations must be tested. We present a hybrid algorithm for learning context-specific models that scales to hundreds of variables while testing no more constraints than standard DAG learning algorithms. Scalable learning is achieved through a combination of an order-based MCMC algorithm and sparsity assumptions analogous to those typically invoked for DAG models. To implement the method, we solve a special case of an open problem recently posed by Alon and Balogh. The method is shown to perform well on synthetic data and real world examples, in terms of both accuracy and scalability.","sentences":["Several approaches to graphically representing context-specific relations among jointly distributed categorical variables have been proposed, along with structure learning algorithms.","While existing optimization-based methods have limited scalability due to the large number of context-specific models, the constraint-based methods are more prone to error than even constraint-based DAG learning algorithms since more relations must be tested.","We present a hybrid algorithm for learning context-specific models that scales to hundreds of variables while testing no more constraints than standard DAG learning algorithms.","Scalable learning is achieved through a combination of an order-based MCMC algorithm and sparsity assumptions analogous to those typically invoked for DAG models.","To implement the method, we solve a special case of an open problem recently posed by Alon and Balogh.","The method is shown to perform well on synthetic data and real world examples, in terms of both accuracy and scalability."],"url":"http://arxiv.org/abs/2402.07762v1","category":"stat.ML"}
{"created":"2024-02-12 16:21:50","title":"Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains","abstract":"Tackling multi-agent learning problems efficiently is a challenging task in continuous action domains. While value-based algorithms excel in sample efficiency when applied to discrete action domains, they are usually inefficient when dealing with continuous actions. Policy-based algorithms, on the other hand, attempt to address this challenge by leveraging critic networks for guiding the learning process and stabilizing the gradient estimation. The limitations in the estimation of true return and falling into local optima in these methods result in inefficient and often sub-optimal policies. In this paper, we diverge from the trend of further enhancing critic networks, and focus on improving the effectiveness of value-based methods in multi-agent continuous domains by concurrently evaluating numerous actions. We propose a novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired from the idea of Q-Functionals, that enables agents to transform their states into basis functions. Our algorithm fosters collaboration among agents by mixing their action-values. We evaluate the efficacy of our algorithm in six cooperative multi-agent scenarios. Our empirical findings reveal that MQF outperforms four variants of Deep Deterministic Policy Gradient through rapid action evaluation and increased sample efficiency.","sentences":["Tackling multi-agent learning problems efficiently is a challenging task in continuous action domains.","While value-based algorithms excel in sample efficiency when applied to discrete action domains, they are usually inefficient when dealing with continuous actions.","Policy-based algorithms, on the other hand, attempt to address this challenge by leveraging critic networks for guiding the learning process and stabilizing the gradient estimation.","The limitations in the estimation of true return and falling into local optima in these methods result in inefficient and often sub-optimal policies.","In this paper, we diverge from the trend of further enhancing critic networks, and focus on improving the effectiveness of value-based methods in multi-agent continuous domains by concurrently evaluating numerous actions.","We propose a novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired from the idea of Q-Functionals, that enables agents to transform their states into basis functions.","Our algorithm fosters collaboration among agents by mixing their action-values.","We evaluate the efficacy of our algorithm in six cooperative multi-agent scenarios.","Our empirical findings reveal that MQF outperforms four variants of Deep Deterministic Policy Gradient through rapid action evaluation and increased sample efficiency."],"url":"http://arxiv.org/abs/2402.07752v1","category":"cs.MA"}
{"created":"2024-02-12 16:17:40","title":"Optimal score estimation via empirical Bayes smoothing","abstract":"We study the problem of estimating the score function of an unknown probability distribution $\\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions. Assuming that $\\rho^*$ is subgaussian and has a Lipschitz-continuous score function $s^*$, we establish the optimal rate of $\\tilde \\Theta(n^{-\\frac{2}{d+4}})$ for this estimation problem under the loss function $\\|\\hat s - s^*\\|^2_{L^2(\\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound. We also discuss the implication of our theory on the sample complexity of score-based generative models.","sentences":["We study the problem of estimating the score function of an unknown probability distribution $\\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions.","Assuming that $\\rho^*$ is subgaussian and has a Lipschitz-continuous score function $s^*$, we establish the optimal rate of $\\tilde \\Theta(n^{-\\frac{2}{d+4}})$ for this estimation problem under the loss function $\\|\\hat s - s^*\\|^2_{L^2(\\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound.","We also discuss the implication of our theory on the sample complexity of score-based generative models."],"url":"http://arxiv.org/abs/2402.07747v1","category":"math.ST"}
{"created":"2024-02-12 16:15:25","title":"Predictive Churn with the Set of Good Models","abstract":"Machine learning models in modern mass-market applications are often updated over time. One of the foremost challenges faced is that, despite increasing overall performance, these updates may flip specific model predictions in unpredictable ways. In practice, researchers quantify the number of unstable predictions between models pre and post update -- i.e., predictive churn. In this paper, we study this effect through the lens of predictive multiplicity -- i.e., the prevalence of conflicting predictions over the set of near-optimal models (the Rashomon set). We show how traditional measures of predictive multiplicity can be used to examine expected churn over this set of prospective models -- i.e., the set of models that may be used to replace a baseline model in deployment. We present theoretical results on the expected churn between models within the Rashomon set from different perspectives. And we characterize expected churn over model updates via the Rashomon set, pairing our analysis with empirical results on real-world datasets -- showing how our approach can be used to better anticipate, reduce, and avoid churn in consumer-facing applications. Further, we show that our approach is useful even for models enhanced with uncertainty awareness.","sentences":["Machine learning models in modern mass-market applications are often updated over time.","One of the foremost challenges faced is that, despite increasing overall performance, these updates may flip specific model predictions in unpredictable ways.","In practice, researchers quantify the number of unstable predictions between models pre and post update -- i.e., predictive churn.","In this paper, we study this effect through the lens of predictive multiplicity -- i.e., the prevalence of conflicting predictions over the set of near-optimal models (the Rashomon set).","We show how traditional measures of predictive multiplicity can be used to examine expected churn over this set of prospective models -- i.e., the set of models that may be used to replace a baseline model in deployment.","We present theoretical results on the expected churn between models within the Rashomon set from different perspectives.","And we characterize expected churn over model updates via the Rashomon set, pairing our analysis with empirical results on real-world datasets -- showing how our approach can be used to better anticipate, reduce, and avoid churn in consumer-facing applications.","Further, we show that our approach is useful even for models enhanced with uncertainty awareness."],"url":"http://arxiv.org/abs/2402.07745v1","category":"cs.LG"}
{"created":"2024-02-12 15:38:31","title":"$\u03c3(500)$ resonance pole positions as function of $m_\u03c0$: analysis with a unitary coupled-channel model","abstract":"Resonance pole positions of the $f_0(500)$ alias $\\sigma(500)$ meson are computed and plotted as a continuous function of pion mass in the framework of a unitary and analytic coupled-channel model for scalar mesons as dynamical $q\\bar{q}$ states. The $\\sigma$ is described with a light and a strange $q\\bar{q}$ seed, mixing with each other mainly through the common $\\pi\\pi$, $K\\bar{K}$, and $\\eta\\eta$ meson-meson channels. The few model parameters are fitted to experimental $S$-wave $\\pi\\pi$ phase shifts up to 1 GeV, yielding, in the case of the physical pion mass, resonance poles at $(460-i222)$ MeV for the $\\sigma(500)$ and $(978-i37)$ MeV for the $f_0(980)$. Resonance, bound-state, and virtual-state pole trajectories are shown as a function of $m_\\pi$ running from 139.57 MeV to 1 GeV. These are compared to recent lattice QCD computations that use interpolating fields corresponding to the model's channels, i.e., for a few discrete $m_\\pi $values.","sentences":["Resonance pole positions of the $f_0(500)$ alias $\\sigma(500)$ meson are computed and plotted as a continuous function of pion mass in the framework of a unitary and analytic coupled-channel model for scalar mesons as dynamical $q\\bar{q}$ states.","The $\\sigma$ is described with a light and a strange $q\\bar{q}$ seed, mixing with each other mainly through the common $\\pi\\pi$, $K\\bar{K}$, and $\\eta\\eta$ meson-meson channels.","The few model parameters are fitted to experimental $S$-wave $\\pi\\pi$ phase shifts up to 1 GeV, yielding, in the case of the physical pion mass, resonance poles at $(460-i222)$ MeV for the $\\sigma(500)$ and $(978-i37)$ MeV for the $f_0(980)$. Resonance, bound-state, and virtual-state pole trajectories are shown as a function of $m_\\pi$ running from 139.57 MeV to 1 GeV.","These are compared to recent lattice QCD computations that use interpolating fields corresponding to the model's channels, i.e., for a few discrete $m_\\pi $values."],"url":"http://arxiv.org/abs/2402.07725v1","category":"hep-ph"}
{"created":"2024-02-12 15:34:04","title":"Interaction-Based Driving Scenario Classification and Labeling","abstract":"Scenario data play a vital role in autonomous driving related researches, and it is essential to obtain refined descriptions and labels to extract and index scenarios with different types of interactions. However, existing methods cannot cope well with the problem of scenario classification and comparison with vehicle interactions as the core. In this paper, we propose a framework for interaction-based refined scenario classification and labeling. Based on the summarized basic types of vehicle interactions, we slice scenario data stream into a series of scenario segments via spatiotemporal scenario evolution tree. The scenario segment statistics of many published scenario datasets are further analyzed. We also propose the scenario metric Graph-DTW based on Graph Computation Tree and Dynamic Time Warping to conduct refined scenario comparison and labeling. The extreme interactive scenarios and corner cases can be efficiently filtered and extracted. Moreover, testing examples on trajectory prediction model demonstrate the effectiveness and advantages of scenario labeling and the proposed metric. The overall framework can provide solid support for the usage and indexing of scenario data.","sentences":["Scenario data play a vital role in autonomous driving related researches, and it is essential to obtain refined descriptions and labels to extract and index scenarios with different types of interactions.","However, existing methods cannot cope well with the problem of scenario classification and comparison with vehicle interactions as the core.","In this paper, we propose a framework for interaction-based refined scenario classification and labeling.","Based on the summarized basic types of vehicle interactions, we slice scenario data stream into a series of scenario segments via spatiotemporal scenario evolution tree.","The scenario segment statistics of many published scenario datasets are further analyzed.","We also propose the scenario metric Graph-DTW based on Graph Computation Tree and Dynamic Time Warping to conduct refined scenario comparison and labeling.","The extreme interactive scenarios and corner cases can be efficiently filtered and extracted.","Moreover, testing examples on trajectory prediction model demonstrate the effectiveness and advantages of scenario labeling and the proposed metric.","The overall framework can provide solid support for the usage and indexing of scenario data."],"url":"http://arxiv.org/abs/2402.07720v1","category":"cs.SE"}
{"created":"2024-02-12 15:32:48","title":"Local Centrality Minimization with Quality Guarantees","abstract":"Centrality measures, quantifying the importance of vertices or edges, play a fundamental role in network analysis. To date, triggered by some positive approximability results, a large body of work has been devoted to studying centrality maximization, where the goal is to maximize the centrality score of a target vertex by manipulating the structure of a given network. On the other hand, due to the lack of such results, only very little attention has been paid to centrality minimization, despite its practical usefulness.   In this study, we introduce a novel optimization model for local centrality minimization, where the manipulation is allowed only around the target vertex. We prove the NP-hardness of our model and that the most intuitive greedy algorithm has a quite limited performance in terms of approximation ratio. Then we design two effective approximation algorithms: The first algorithm is a highly-scalable algorithm that has an approximation ratio unachievable by the greedy algorithm, while the second algorithm is a bicriteria approximation algorithm that solves a continuous relaxation based on the Lov\\'asz extension, using a projected subgradient method. To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization. Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms: Our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is rather strong against adversarial instances.","sentences":["Centrality measures, quantifying the importance of vertices or edges, play a fundamental role in network analysis.","To date, triggered by some positive approximability results, a large body of work has been devoted to studying centrality maximization, where the goal is to maximize the centrality score of a target vertex by manipulating the structure of a given network.","On the other hand, due to the lack of such results, only very little attention has been paid to centrality minimization, despite its practical usefulness.   ","In this study, we introduce a novel optimization model for local centrality minimization, where the manipulation is allowed only around the target vertex.","We prove the NP-hardness of our model and that the most intuitive greedy algorithm has a quite limited performance in terms of approximation ratio.","Then we design two effective approximation algorithms: The first algorithm is a highly-scalable algorithm that has an approximation ratio unachievable by the greedy algorithm, while the second algorithm is a bicriteria approximation algorithm that solves a continuous relaxation based on the Lov\\'asz extension, using a projected subgradient method.","To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization.","Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms: Our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is rather strong against adversarial instances."],"url":"http://arxiv.org/abs/2402.07718v1","category":"cs.SI"}
{"created":"2024-02-12 15:24:44","title":"Near optimal constructions of frameproof codes","abstract":"Frameproof codes are a class of secure codes that were originally introduced in the pioneering work of Boneh and Shaw in the context of digital fingerprinting. They can be used to enhance the security and credibility of digital content. Let $M_{c,l}(q)$ denote the largest cardinality of a $q$-ary $c$-frameproof code with length $l$. Based on an intriguing observation that relates $M_{c,l}(q)$ to the renowned Erd\\H{o}s Matching Conjecture in extremal set theory, in 2003, Blackburn posed an open problem on the precise value of the limit $R_{c,l}=\\lim_{q\\rightarrow\\infty}\\frac{M_{c,l}(q)}{q^{\\lceil l/c \\rceil}}$. By combining several ideas from the probabilistic method, we present a lower bound for $M_{c,l}(q)$, which, together with an upper bound of Blackburn, completely determines $R_{c,l}$ for {\\it all} fixed $c,l$, and resolves the above open problem in the full generality. We also present an improved upper bound for $M_{c,l}(q)$.","sentences":["Frameproof codes are a class of secure codes that were originally introduced in the pioneering work of Boneh and Shaw in the context of digital fingerprinting.","They can be used to enhance the security and credibility of digital content.","Let $M_{c,l}(q)$ denote the largest cardinality of a $q$-ary $c$-frameproof code with length $l$. Based on an intriguing observation that relates $M_{c,l}(q)$ to the renowned Erd\\H{o}s","Matching Conjecture in extremal set theory, in 2003, Blackburn posed an open problem on the precise value of the limit $R_{c,l}=\\lim_{q\\rightarrow\\infty}\\frac{M_{c,l}(q)}{q^{\\lceil l/c \\rceil}}$. By combining several ideas from the probabilistic method, we present a lower bound for $M_{c,l}(q)$, which, together with an upper bound of Blackburn, completely determines $R_{c,l}$ for {\\it all} fixed $c,l$, and resolves the above open problem in the full generality.","We also present an improved upper bound for $M_{c,l}(q)$."],"url":"http://arxiv.org/abs/2402.07711v1","category":"cs.IT"}
{"created":"2024-02-12 15:00:51","title":"LFOC+: A Fair OS-level Cache-Clustering Policy for Commodity Multicore Systems","abstract":"Commodity multicore systems are increasingly adopting hardware support that enables the system software to partition the last-level cache (LLC). This support makes it possible for the operating system (OS) or the Virtual Machine Monitor (VMM) to mitigate shared-resource contention effects on multicores by assigning different co-running applications to various cache partitions. Recently cache-clustering (or partition-sharing) strategies have emerged as a way to improve system throughput and fairness on new platforms with cache-partitioning support. As opposed to strict cache-partitioning, which allocates separate cache partitions to each application, cache-clustering allows partitions to be shared by a group of applications.   In this article we propose LFOC+, a fairness-aware OS-level cache-clustering policy for commodity multicore systems. LFOC+ tries to mimic the behavior of the optimal cache-clustering solution for fairness, which we could obtain for different workload scenarios by using a simulation tool. Our dynamic cache-clustering strategy continuously gathers data from performance monitoring counters to classify applications at runtime based on the degree of cache sensitivity and contentiousness, and effectively separates cache-sensitive applications from aggressor programs to improve fairness, while providing acceptable system throughput.   We implemented LFOC+ in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of four previously proposed cache-clustering policies. Our experimental analysis reveals that LFOC+ constitutes a lightweight OS-level policy and improves fairness relative to two other state-of-the-art fairness-aware strategies --Dunn and LFOC--, by up to 22\\% and up to 20.6\\%, respectively, and by 9\\% and 4.9\\% on average.","sentences":["Commodity multicore systems are increasingly adopting hardware support that enables the system software to partition the last-level cache (LLC).","This support makes it possible for the operating system (OS) or the Virtual Machine Monitor (VMM) to mitigate shared-resource contention effects on multicores by assigning different co-running applications to various cache partitions.","Recently cache-clustering (or partition-sharing) strategies have emerged as a way to improve system throughput and fairness on new platforms with cache-partitioning support.","As opposed to strict cache-partitioning, which allocates separate cache partitions to each application, cache-clustering allows partitions to be shared by a group of applications.   ","In this article we propose LFOC+, a fairness-aware OS-level cache-clustering policy for commodity multicore systems.","LFOC+ tries to mimic the behavior of the optimal cache-clustering solution for fairness, which we could obtain for different workload scenarios by using a simulation tool.","Our dynamic cache-clustering strategy continuously gathers data from performance monitoring counters to classify applications at runtime based on the degree of cache sensitivity and contentiousness, and effectively separates cache-sensitive applications from aggressor programs to improve fairness, while providing acceptable system throughput.   ","We implemented LFOC+ in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of four previously proposed cache-clustering policies.","Our experimental analysis reveals that LFOC+ constitutes a lightweight OS-level policy and improves fairness relative to two other state-of-the-art fairness-aware strategies --Dunn and LFOC--, by up to 22\\% and up to 20.6\\%, respectively, and by 9\\% and 4.9\\% on average."],"url":"http://arxiv.org/abs/2402.07693v1","category":"cs.AR"}
{"created":"2024-02-12 14:59:40","title":"Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints","abstract":"Bayesian optimization has been successfully applied to optimize black-box functions where the number of evaluations is severely limited. However, in many real-world applications, it is hard or impossible to know in advance which designs are feasible due to some physical or system limitations. These issues lead to an even more challenging problem of optimizing an unknown function with unknown constraints. In this paper, we observe that in such scenarios optimal solution typically lies on the boundary between feasible and infeasible regions of the design space, making it considerably more difficult than that with interior optima. Inspired by this observation, we propose BE-CBO, a new Bayesian optimization method that efficiently explores the boundary between feasible and infeasible designs. To identify the boundary, we learn the constraints with an ensemble of neural networks that outperform the standard Gaussian Processes for capturing complex boundaries. Our method demonstrates superior performance against state-of-the-art methods through comprehensive experiments on synthetic and real-world benchmarks.","sentences":["Bayesian optimization has been successfully applied to optimize black-box functions where the number of evaluations is severely limited.","However, in many real-world applications, it is hard or impossible to know in advance which designs are feasible due to some physical or system limitations.","These issues lead to an even more challenging problem of optimizing an unknown function with unknown constraints.","In this paper, we observe that in such scenarios optimal solution typically lies on the boundary between feasible and infeasible regions of the design space, making it considerably more difficult than that with interior optima.","Inspired by this observation, we propose BE-CBO, a new Bayesian optimization method that efficiently explores the boundary between feasible and infeasible designs.","To identify the boundary, we learn the constraints with an ensemble of neural networks that outperform the standard Gaussian Processes for capturing complex boundaries.","Our method demonstrates superior performance against state-of-the-art methods through comprehensive experiments on synthetic and real-world benchmarks."],"url":"http://arxiv.org/abs/2402.07692v1","category":"cs.LG"}
{"created":"2024-02-12 14:52:30","title":"Global well-posedness and asymptotic behavior for the Euler-alignment system with pressure","abstract":"We study the Cauchy problem of the compressible Euler system with strongly singular velocity alignment. We establish a global well-posedness theory for the system with small smooth initial data. Additionally, we derive asymptotic emergent behaviors for the system, providing time decay estimates with optimal decay rates. Notably, the optimal decay rate we obtain does not align with the corresponding fractional heat equation within our considered range, where the parameter $\\alpha\\in(0,1)$. This highlights the distinct feature of the alignment operator.","sentences":["We study the Cauchy problem of the compressible Euler system with strongly singular velocity alignment.","We establish a global well-posedness theory for the system with small smooth initial data.","Additionally, we derive asymptotic emergent behaviors for the system, providing time decay estimates with optimal decay rates.","Notably, the optimal decay rate we obtain does not align with the corresponding fractional heat equation within our considered range, where the parameter $\\alpha\\in(0,1)$. This highlights the distinct feature of the alignment operator."],"url":"http://arxiv.org/abs/2402.07686v1","category":"math.AP"}
{"created":"2024-02-12 14:43:40","title":"Two Choices are Enough for P-LCPs, USOs, and Colorful Tangents","abstract":"We provide polynomial-time reductions between three search problems from three distinct areas: the P-matrix linear complementarity problem (P-LCP), finding the sink of a unique sink orientation (USO), and a variant of the $\\alpha$-Ham Sandwich problem. For all three settings, we show that \"two choices are enough\", meaning that the general non-binary version of the problem can be reduced in polynomial time to the binary version. This specifically means that generalized P-LCPs are equivalent to P-LCPs, and grid USOs are equivalent to cube USOs. These results are obtained by showing that both the P-LCP and our $\\alpha$-Ham Sandwich variant are equivalent to a new problem we introduce, P-Lin-Bellman. This problem can be seen as a new tool for formulating problems as P-LCPs.","sentences":["We provide polynomial-time reductions between three search problems from three distinct areas: the P-matrix linear complementarity problem (P-LCP), finding the sink of a unique sink orientation (USO), and a variant of the $\\alpha$-Ham Sandwich problem.","For all three settings, we show that \"two choices are enough\", meaning that the general non-binary version of the problem can be reduced in polynomial time to the binary version.","This specifically means that generalized P-LCPs are equivalent to P-LCPs, and grid USOs are equivalent to cube USOs.","These results are obtained by showing that both the P-LCP and our $\\alpha$-Ham Sandwich variant are equivalent to a new problem we introduce, P-Lin-Bellman.","This problem can be seen as a new tool for formulating problems as P-LCPs."],"url":"http://arxiv.org/abs/2402.07683v1","category":"cs.CC"}
{"created":"2024-02-12 13:52:34","title":"Impact of spatial transformations on landscape features of CEC2022 basic benchmark problems","abstract":"When benchmarking optimization heuristics, we need to take care to avoid an algorithm exploiting biases in the construction of the used problems. One way in which this might be done is by providing different versions of each problem but with transformations applied to ensure the algorithms are equipped with mechanisms for successfully tackling a range of problems. In this paper, we investigate several of these problem transformations and show how they influence the low-level landscape features of a set of 5 problems from the CEC2022 benchmark suite. Our results highlight that even relatively small transformations can significantly alter the measured landscape features. This poses a wider question of what properties we want to preserve when creating problem transformations, and how to fairly measure them.","sentences":["When benchmarking optimization heuristics, we need to take care to avoid an algorithm exploiting biases in the construction of the used problems.","One way in which this might be done is by providing different versions of each problem but with transformations applied to ensure the algorithms are equipped with mechanisms for successfully tackling a range of problems.","In this paper, we investigate several of these problem transformations and show how they influence the low-level landscape features of a set of 5 problems from the CEC2022 benchmark suite.","Our results highlight that even relatively small transformations can significantly alter the measured landscape features.","This poses a wider question of what properties we want to preserve when creating problem transformations, and how to fairly measure them."],"url":"http://arxiv.org/abs/2402.07654v1","category":"cs.NE"}
{"created":"2024-02-12 13:32:04","title":"A Lattice-Reduction Aided Vector Perturbation Precoder Relying on Quantum Annealing","abstract":"Quantum annealing (QA) is proposed for vector perturbation precoding (VPP) in multiple input multiple output (MIMO) communications systems. The mathematical framework of VPP is presented, outlining the problem formulation and the benefits of lattice reduction algorithms. Lattice reduction aided quantum vector perturbation (LRAQVP) is designed by harnessing physical quantum hardware, and the optimization of hardware parameters is discussed. We observe a 5dB gain over lattice reduction zero forcing precoding (LRZFP), which behaves similarly to a quantum annealing algorithm operating without a lattice reduction stage. The proposed algorithm is also shown to approach the performance of a sphere encoder, which exhibits an exponentially escalating complexity.","sentences":["Quantum annealing (QA) is proposed for vector perturbation precoding (VPP) in multiple input multiple output (MIMO) communications systems.","The mathematical framework of VPP is presented, outlining the problem formulation and the benefits of lattice reduction algorithms.","Lattice reduction aided quantum vector perturbation (LRAQVP) is designed by harnessing physical quantum hardware, and the optimization of hardware parameters is discussed.","We observe a 5dB gain over lattice reduction zero forcing precoding (LRZFP), which behaves similarly to a quantum annealing algorithm operating without a lattice reduction stage.","The proposed algorithm is also shown to approach the performance of a sphere encoder, which exhibits an exponentially escalating complexity."],"url":"http://arxiv.org/abs/2402.07643v1","category":"cs.IT"}
{"created":"2024-02-12 13:11:29","title":"Unveiling the GeI2-Assisted Oriented Growth of Perovskite Crystallite for High-Performance Flexible Sn Perovskite Solar Cells","abstract":"Tin perovskites are emerging as promising alternatives to their lead-based counterparts for high-performance and flexible perovskite solar cells (PSCs). However, their rapid crystallization often leads to inadequate film quality and poor device performance. In this study, the role of GeI2 as an additive is investigated for controlling the nucleation and crystallization processes of formamidium tin triiodide (FASnI3). The findings reveal the preferential formation of a Ge-rich layer at the bottom of the perovskite film upon the introduction of GeI2. It is proposed that the initial formation of the Ge-complex acts as a crystallization regulator, promoting oriented growth of subsequent FASnI3 crystals and enhancing overall crystallinity. Through the incorporation of an optimal amount of GeI2, flexible Sn PSCs with an efficiency of 10.8% were achieved. Furthermore, it was observed that the GeI2 additive ensures a remarkable shelf-life for the devices, with the rigid cells retaining 91% of their initial performance after more than 13,800 hours of storage in an N2 gas environment. This study elucidates the mechanistic role of GeI2 in regulating the nucleation and crystallization process of tin perovskites, providing valuable insights into the significance of additive engineering for the development of high-performance flexible tin PSCs.","sentences":["Tin perovskites are emerging as promising alternatives to their lead-based counterparts for high-performance and flexible perovskite solar cells (PSCs).","However, their rapid crystallization often leads to inadequate film quality and poor device performance.","In this study, the role of GeI2 as an additive is investigated for controlling the nucleation and crystallization processes of formamidium tin triiodide (FASnI3).","The findings reveal the preferential formation of a Ge-rich layer at the bottom of the perovskite film upon the introduction of GeI2.","It is proposed that the initial formation of the Ge-complex acts as a crystallization regulator, promoting oriented growth of subsequent FASnI3 crystals and enhancing overall crystallinity.","Through the incorporation of an optimal amount of GeI2, flexible Sn PSCs with an efficiency of 10.8% were achieved.","Furthermore, it was observed that the GeI2 additive ensures a remarkable shelf-life for the devices, with the rigid cells retaining 91% of their initial performance after more than 13,800 hours of storage in an N2 gas environment.","This study elucidates the mechanistic role of GeI2 in regulating the nucleation and crystallization process of tin perovskites, providing valuable insights into the significance of additive engineering for the development of high-performance flexible tin PSCs."],"url":"http://arxiv.org/abs/2402.07627v1","category":"physics.app-ph"}
{"created":"2024-02-12 12:48:03","title":"Optimized noise-assisted simulation of the Lindblad equation with time-dependent coefficients on a noisy quantum processor","abstract":"Noise in quantum devices is generally considered detrimental to computational accuracy. However, the recent proposal of noise-assisted simulation has demonstrated that noise can be an asset in digital quantum simulations of open systems on Noisy Intermediate-Scale Quantum (NISQ) devices. In this context, we introduce an optimized decoherence rate control scheme that can significantly reduce computational requirements by multiple orders of magnitude, in comparison to the original noise-assisted simulation. We further extend this approach to encompass Lindblad equations with time-dependent coefficients, using only quantum error characterization and mitigation techniques. This extension allows for the perturbative simulation of non-Markovian dynamics on NISQ devices, eliminating the need for ancilla qubits or mid-circuit measurements. Our contributions are validated through numerical experiments on an emulated IBMQ device. Overall, our work offers valuable optimizations that bring current quantum processors closer to effectively simulating realistic open systems.","sentences":["Noise in quantum devices is generally considered detrimental to computational accuracy.","However, the recent proposal of noise-assisted simulation has demonstrated that noise can be an asset in digital quantum simulations of open systems on Noisy Intermediate-Scale Quantum (NISQ) devices.","In this context, we introduce an optimized decoherence rate control scheme that can significantly reduce computational requirements by multiple orders of magnitude, in comparison to the original noise-assisted simulation.","We further extend this approach to encompass Lindblad equations with time-dependent coefficients, using only quantum error characterization and mitigation techniques.","This extension allows for the perturbative simulation of non-Markovian dynamics on NISQ devices, eliminating the need for ancilla qubits or mid-circuit measurements.","Our contributions are validated through numerical experiments on an emulated IBMQ device.","Overall, our work offers valuable optimizations that bring current quantum processors closer to effectively simulating realistic open systems."],"url":"http://arxiv.org/abs/2402.07617v1","category":"quant-ph"}
{"created":"2024-02-12 12:42:48","title":"Riemannian trust-region methods for strict saddle functions with complexity guarantees","abstract":"The difficulty of minimizing a nonconvex function is in part explained by the presence of saddle points. This slows down optimization algorithms and impacts worst-case complexity guarantees. However, many nonconvex problems of interest possess a favorable structure for optimization, in the sense that saddle points can be escaped efficiently by appropriate algorithms. This strict saddle property has been extensively used in data science to derive good properties for first-order algorithms, such as convergence to second-order critical points. However, the analysis and the design of second-order algorithms in the strict saddle setting have received significantly less attention. In this paper, we consider second-order trust-region methods for a class of strict saddle functions defined on Riemannian manifolds. These functions exhibit (geodesic) strong convexity around minimizers and negative curvature at saddle points. We show that the standard trust-region method with exact subproblem minimization finds an approximate local minimizer in a number of iterations that depends logarithmically on the accuracy parameter, which significantly improves known results for general nonconvex optimization. We also propose an inexact variant of the algorithm that explicitly leverages the strict saddle property to compute the most appropriate step at every iteration. Our bounds for the inexact variant also improve over the general nonconvex case, and illustrate the benefit of using strict saddle properties within optimization algorithms. Keywords: Riemannian optimization, strict saddle function, second-order method, complexity guarantees.","sentences":["The difficulty of minimizing a nonconvex function is in part explained by the presence of saddle points.","This slows down optimization algorithms and impacts worst-case complexity guarantees.","However, many nonconvex problems of interest possess a favorable structure for optimization, in the sense that saddle points can be escaped efficiently by appropriate algorithms.","This strict saddle property has been extensively used in data science to derive good properties for first-order algorithms, such as convergence to second-order critical points.","However, the analysis and the design of second-order algorithms in the strict saddle setting have received significantly less attention.","In this paper, we consider second-order trust-region methods for a class of strict saddle functions defined on Riemannian manifolds.","These functions exhibit (geodesic) strong convexity around minimizers and negative curvature at saddle points.","We show that the standard trust-region method with exact subproblem minimization finds an approximate local minimizer in a number of iterations that depends logarithmically on the accuracy parameter, which significantly improves known results for general nonconvex optimization.","We also propose an inexact variant of the algorithm that explicitly leverages the strict saddle property to compute the most appropriate step at every iteration.","Our bounds for the inexact variant also improve over the general nonconvex case, and illustrate the benefit of using strict saddle properties within optimization algorithms.","Keywords: Riemannian optimization, strict saddle function, second-order method, complexity guarantees."],"url":"http://arxiv.org/abs/2402.07614v1","category":"math.OC"}
{"created":"2024-02-12 12:38:20","title":"Global optimality under amenable symmetry constraints","abstract":"We ask whether there exists a function or measure that (1) minimizes a given convex functional or risk and (2) satisfies a symmetry property specified by an amenable group of transformations. Examples of such symmetry properties are invariance, equivariance, or quasi-invariance. Our results draw on old ideas of Stein and Le Cam and on approximate group averages that appear in ergodic theorems for amenable groups. A class of convex sets known as orbitopes in convex analysis emerges as crucial, and we establish properties of such orbitopes in nonparametric settings. We also show how a simple device called a cocycle can be used to reduce different forms of symmetry to a single problem. As applications, we obtain results on invariant kernel mean embeddings and a Monge-Kantorovich theorem on optimality of transport plans under symmetry constraints. We also explain connections to the Hunt-Stein theorem on invariant tests.","sentences":["We ask whether there exists a function or measure that (1) minimizes a given convex functional or risk and (2) satisfies a symmetry property specified by an amenable group of transformations.","Examples of such symmetry properties are invariance, equivariance, or quasi-invariance.","Our results draw on old ideas of Stein and Le Cam and on approximate group averages that appear in ergodic theorems for amenable groups.","A class of convex sets known as orbitopes in convex analysis emerges as crucial, and we establish properties of such orbitopes in nonparametric settings.","We also show how a simple device called a cocycle can be used to reduce different forms of symmetry to a single problem.","As applications, we obtain results on invariant kernel mean embeddings and a Monge-Kantorovich theorem on optimality of transport plans under symmetry constraints.","We also explain connections to the Hunt-Stein theorem on invariant tests."],"url":"http://arxiv.org/abs/2402.07613v1","category":"math.ST"}
{"created":"2024-02-12 12:16:17","title":"Variational post-selection for ground states and thermal states simulation","abstract":"Variational quantum algorithms (VQAs), as one of the most promising routes in the noisy intermediate-scale quantum (NISQ) era, offer various potential applications while also confront severe challenges due to near-term quantum hardware restrictions. In this work, we propose a framework to enhance the expressiveness of variational quantum ansatz by incorporating variational post-selection techniques. These techniques apply variational modules and neural network post-processing on ancilla qubits, which are compatible with the current generation of quantum devices. Equipped with variational post-selection, we demonstrate that the accuracy of the variational ground state and thermal state preparation for both quantum spin and molecule systems is substantially improved. Notably, in the case of estimating the local properties of a thermalized quantum system, we present a scalable approach that outperforms previous methods through the combination of neural post-selection and a new optimization objective.","sentences":["Variational quantum algorithms (VQAs), as one of the most promising routes in the noisy intermediate-scale quantum (NISQ) era, offer various potential applications while also confront severe challenges due to near-term quantum hardware restrictions.","In this work, we propose a framework to enhance the expressiveness of variational quantum ansatz by incorporating variational post-selection techniques.","These techniques apply variational modules and neural network post-processing on ancilla qubits, which are compatible with the current generation of quantum devices.","Equipped with variational post-selection, we demonstrate that the accuracy of the variational ground state and thermal state preparation for both quantum spin and molecule systems is substantially improved.","Notably, in the case of estimating the local properties of a thermalized quantum system, we present a scalable approach that outperforms previous methods through the combination of neural post-selection and a new optimization objective."],"url":"http://arxiv.org/abs/2402.07605v1","category":"quant-ph"}
{"created":"2024-02-12 11:59:47","title":"Topic-aware Most Influential Community Search in Social Networks","abstract":"Community search is a problem aimed at searching for densely connected subgraphs within a network based on query conditions, which has recently attracted significant attention. However, most previous community search studies have overlooked the coexistence relationship among attributes. They typically assign a single attribute to each node or edge (e.g.,only considering influence scores or keywords), which is difficult for users to obtain a comprehensive and beneficial information. Additionally, most of them also ignored the uncertainty in the attribute graph. Therefore, in this paper, we introduce two novel community models, namely topic-based interaction graph and $(k,l,\\eta)$-influential community. The former is a directed ucertain graph generated by the query topic distribution provided by users, while the latter is used for solving the topic-aware most influential community search problem in social networks. Furthermore, we propose an online search algorithm which computes the influence value of each vertex by considering the topic-aware information diffusion process on interaction graphs. And then, we use a peeling-pruning strategy to iteratively find the topic-aware most $(k,l,\\eta)$-influential community. To further speed up the search performance, we devise two lightweight index structures which efficiently support the search for the topic-aware most influential community within an optimal time. We also propose three optimization methods to improve the space and time costs of the index-based approach.","sentences":["Community search is a problem aimed at searching for densely connected subgraphs within a network based on query conditions, which has recently attracted significant attention.","However, most previous community search studies have overlooked the coexistence relationship among attributes.","They typically assign a single attribute to each node or edge (e.g.,only considering influence scores or keywords), which is difficult for users to obtain a comprehensive and beneficial information.","Additionally, most of them also ignored the uncertainty in the attribute graph.","Therefore, in this paper, we introduce two novel community models, namely topic-based interaction graph and $(k,l,\\eta)$-influential community.","The former is a directed ucertain graph generated by the query topic distribution provided by users, while the latter is used for solving the topic-aware most influential community search problem in social networks.","Furthermore, we propose an online search algorithm which computes the influence value of each vertex by considering the topic-aware information diffusion process on interaction graphs.","And then, we use a peeling-pruning strategy to iteratively find the topic-aware most $(k,l,\\eta)$-influential community.","To further speed up the search performance, we devise two lightweight index structures which efficiently support the search for the topic-aware most influential community within an optimal time.","We also propose three optimization methods to improve the space and time costs of the index-based approach."],"url":"http://arxiv.org/abs/2402.07601v1","category":"cs.SI"}
{"created":"2024-02-12 11:59:05","title":"Optical Routing with Binary Optimisation and Quantum Annealing","abstract":"A challenge for scalability of demand-responsive, elastic optical Dense Wavelength Division Multiplexing (DWDM) and Flexgrid networks is the computational complexity of allocating many optical routes on large networks. We demonstrate that demand satisfaction problems in communication networks can be formulated as quadratic unconstrained binary optimisation (QUBO) problems, and solved using a hybrid quantum annealer. Efficient encodings are developed which solve both unicast and multicast multicommodity-flow problems, while also adhering to individual requirements for maximum latency and resilience for each route. We present several QUBO formulations and analyse the qubit scaling. We demonstrate solutions using a hybrid solver, D-Wave Quantum Advantage QPU. Progress in generating optimal solutions with efficient use of computational resources will be beneficial to telecoms operators, enabling them to run dynamic optical network infrastructures which use resources efficiently, are resilient to local faults and cyber-attacks, and can be elastically responsive to demands.","sentences":["A challenge for scalability of demand-responsive, elastic optical Dense Wavelength Division Multiplexing (DWDM) and Flexgrid networks is the computational complexity of allocating many optical routes on large networks.","We demonstrate that demand satisfaction problems in communication networks can be formulated as quadratic unconstrained binary optimisation (QUBO) problems, and solved using a hybrid quantum annealer.","Efficient encodings are developed which solve both unicast and multicast multicommodity-flow problems, while also adhering to individual requirements for maximum latency and resilience for each route.","We present several QUBO formulations and analyse the qubit scaling.","We demonstrate solutions using a hybrid solver, D-Wave Quantum Advantage QPU.","Progress in generating optimal solutions with efficient use of computational resources will be beneficial to telecoms operators, enabling them to run dynamic optical network infrastructures which use resources efficiently, are resilient to local faults and cyber-attacks, and can be elastically responsive to demands."],"url":"http://arxiv.org/abs/2402.07600v1","category":"cs.NI"}
{"created":"2024-02-12 11:58:18","title":"Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model","abstract":"We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023). Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners.","sentences":["We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023).","Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest.","We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners."],"url":"http://arxiv.org/abs/2402.07598v1","category":"cs.LG"}
{"created":"2024-02-12 11:48:00","title":"Inverse source problems for coupled parabolic systems from measurements of one internal component","abstract":"This paper is devoted to the study of inverse source problems for coupled systems of heat equations with constant or spatial--dependent coupling terms and whose internal measurements involve a reduced number of observed states. The analysis is developed for two kind of systems: the first one consists of parabolic equations with zero order coupling terms (or the so-called non-self-adjoint matrix potential) and whose possibly space-dependent coefficients. The second one consists of parabolic equations with coupling in the diffusion matrix. In all configurations the source is decomposed in separate variables, where the temporal part is known and scalar, whereas the spatial dependence is an unknown vector field. This work builds on previous methodologies for the recovery of source in scalar equations and Stokes fluids, thus expanding the field to include coupled systems of second order parabolic equations. Numerical algorithms through the finite element method in 1D and 2D are performed. Several examples showing that the algorithms make possible to recover space-dependent sources.","sentences":["This paper is devoted to the study of inverse source problems for coupled systems of heat equations with constant or spatial--dependent coupling terms and whose internal measurements involve a reduced number of observed states.","The analysis is developed for two kind of systems: the first one consists of parabolic equations with zero order coupling terms (or the so-called non-self-adjoint matrix potential) and whose possibly space-dependent coefficients.","The second one consists of parabolic equations with coupling in the diffusion matrix.","In all configurations the source is decomposed in separate variables, where the temporal part is known and scalar, whereas the spatial dependence is an unknown vector field.","This work builds on previous methodologies for the recovery of source in scalar equations and Stokes fluids, thus expanding the field to include coupled systems of second order parabolic equations.","Numerical algorithms through the finite element method in 1D and 2D are performed.","Several examples showing that the algorithms make possible to recover space-dependent sources."],"url":"http://arxiv.org/abs/2402.07593v1","category":"math.OC"}
{"created":"2024-02-12 11:44:52","title":"Superconducting single-photon detector integrated in DBR with optical microconnector for MM or SM fiber","abstract":"This paper presents the development of a superconducting nanowire single-photon detector (SNSPD) integrated into a distributed Bragg reflector (DBR) with a design center wavelength of 830 nm and a width of 200 nm. This SNSPD is made of a superconducting niobium nitride (NbN) thin film that is produced using plasma-enhanced atomic layer deposition (PEALD). The DBR is made of 15 alternating layers of silicon nitride and silicon oxide that are produced through plasma-enhanced chemical vapor deposition (PECVD). The reflection efficiency of the mirror is 90% at a wavelength of 830 nm. For sufficient optical coupling, an optical micro-connector optimized for multimode or single-mode optical fibers with a diameter of 128 {\\mu}m was formed using two-photon polymerization techniques. The niobium nitride film was deposited onto the DBR surface in-situ in two separate reactors connected by a vacuum transfer. The in-situ technique of deposition of a superconducting niobium nitride film and a distributed Bragg reflector has allowed achieving a detection efficiency of 90% at a wavelength of 830 nm and a dark count rate of 10 s-1 at a temperature of 2.5 K. Additionally, the detector jitter was 50 ps.","sentences":["This paper presents the development of a superconducting nanowire single-photon detector (SNSPD) integrated into a distributed Bragg reflector (DBR) with a design center wavelength of 830 nm and a width of 200 nm.","This SNSPD is made of a superconducting niobium nitride (NbN) thin film that is produced using plasma-enhanced atomic layer deposition (PEALD).","The DBR is made of 15 alternating layers of silicon nitride and silicon oxide that are produced through plasma-enhanced chemical vapor deposition (PECVD).","The reflection efficiency of the mirror is 90% at a wavelength of 830 nm.","For sufficient optical coupling, an optical micro-connector optimized for multimode or single-mode optical fibers with a diameter of 128 {\\mu}m was formed using two-photon polymerization techniques.","The niobium nitride film was deposited onto the DBR surface in-situ in two separate reactors connected by a vacuum transfer.","The in-situ technique of deposition of a superconducting niobium nitride film and a distributed Bragg reflector has allowed achieving a detection efficiency of 90% at a wavelength of 830 nm and a dark count rate of 10 s-1 at a temperature of 2.5 K. Additionally, the detector jitter was 50 ps."],"url":"http://arxiv.org/abs/2402.07592v1","category":"cond-mat.supr-con"}
{"created":"2024-02-12 11:34:42","title":"Privacy-Optimized Randomized Response for Sharing Multi-Attribute Data","abstract":"With the increasing amount of data in society, privacy concerns in data sharing have become widely recognized. Particularly, protecting personal attribute information is essential for a wide range of aims from crowdsourcing to realizing personalized medicine. Although various differentially private methods based on randomized response have been proposed for single attribute information or specific analysis purposes such as frequency estimation, there is a lack of studies on the mechanism for sharing individuals' multiple categorical information itself. The existing randomized response for sharing multi-attribute data uses the Kronecker product to perturb each attribute information in turn according to the respective privacy level but achieves only a weak privacy level for the entire dataset. Therefore, in this study, we propose a privacy-optimized randomized response that guarantees the strongest privacy in sharing multi-attribute data. Furthermore, we present an efficient heuristic algorithm for constructing a near-optimal mechanism. The time complexity of our algorithm is O(k^2), where k is the number of attributes, and it can be performed in about 1 second even for large datasets with k = 1,000. The experimental results demonstrate that both of our methods provide significantly stronger privacy guarantees for the entire dataset than the existing method. In addition, we show an analysis example using genome statistics to confirm that our methods can achieve less than half the output error compared with that of the existing method. Overall, this study is an important step toward trustworthy sharing and analysis of multi-attribute data. The Python implementation of our experiments and supplemental results are available at https://github.com/ay0408/Optimized-RR.","sentences":["With the increasing amount of data in society, privacy concerns in data sharing have become widely recognized.","Particularly, protecting personal attribute information is essential for a wide range of aims from crowdsourcing to realizing personalized medicine.","Although various differentially private methods based on randomized response have been proposed for single attribute information or specific analysis purposes such as frequency estimation, there is a lack of studies on the mechanism for sharing individuals' multiple categorical information itself.","The existing randomized response for sharing multi-attribute data uses the Kronecker product to perturb each attribute information in turn according to the respective privacy level but achieves only a weak privacy level for the entire dataset.","Therefore, in this study, we propose a privacy-optimized randomized response that guarantees the strongest privacy in sharing multi-attribute data.","Furthermore, we present an efficient heuristic algorithm for constructing a near-optimal mechanism.","The time complexity of our algorithm is O(k^2), where k is the number of attributes, and it can be performed in about 1 second even for large datasets with k = 1,000.","The experimental results demonstrate that both of our methods provide significantly stronger privacy guarantees for the entire dataset than the existing method.","In addition, we show an analysis example using genome statistics to confirm that our methods can achieve less than half the output error compared with that of the existing method.","Overall, this study is an important step toward trustworthy sharing and analysis of multi-attribute data.","The Python implementation of our experiments and supplemental results are available at https://github.com/ay0408/Optimized-RR."],"url":"http://arxiv.org/abs/2402.07584v1","category":"cs.CR"}
{"created":"2024-02-12 11:30:15","title":"Passive detection of a random signal common to multi-sensor reference and surveillance arrays","abstract":"This paper addresses the passive detection of a common rank-one subspace signal received in two multi-sensor arrays. We consider the case of a one-antenna transmitter sending a common Gaussian signal, independent Gaussian noises with arbitrary spatial covariance, and known channel subspaces. The detector derived in this paper is a generalized likelihood ratio (GLR) test. For all but one of the unknown parameters, it is possible to find closed-form maximum likelihood (ML) estimator functions. We can further compress the likelihood to only an unknown vector whose ML estimate requires maximizing a product of ratios in quadratic forms, which is carried out using a trust-region algorithm. We propose two approximations of the GLR that do not require any numerical optimization: one based on a sample-based estimator of the unknown parameter whose ML estimate cannot be obtained in closed-form, and one derived under low-SNR conditions. Notably, all the detectors are scale-invariant, and the approximations are functions of beamformed data. However, they are not GLRTs for data that has been pre-processed with a beamformer, a point that is elaborated in the paper. These detectors outperform previously published correlation detectors on simulated data, in many cases quite significantly. Moreover, performance results quantify the performance gains over detectors that assume only the dimension of the subspace to be known.","sentences":["This paper addresses the passive detection of a common rank-one subspace signal received in two multi-sensor arrays.","We consider the case of a one-antenna transmitter sending a common Gaussian signal, independent Gaussian noises with arbitrary spatial covariance, and known channel subspaces.","The detector derived in this paper is a generalized likelihood ratio (GLR) test.","For all but one of the unknown parameters, it is possible to find closed-form maximum likelihood (ML) estimator functions.","We can further compress the likelihood to only an unknown vector whose ML estimate requires maximizing a product of ratios in quadratic forms, which is carried out using a trust-region algorithm.","We propose two approximations of the GLR that do not require any numerical optimization: one based on a sample-based estimator of the unknown parameter whose ML estimate cannot be obtained in closed-form, and one derived under low-SNR conditions.","Notably, all the detectors are scale-invariant, and the approximations are functions of beamformed data.","However, they are not GLRTs for data that has been pre-processed with a beamformer, a point that is elaborated in the paper.","These detectors outperform previously published correlation detectors on simulated data, in many cases quite significantly.","Moreover, performance results quantify the performance gains over detectors that assume only the dimension of the subspace to be known."],"url":"http://arxiv.org/abs/2402.07583v1","category":"eess.SP"}
{"created":"2024-02-12 11:28:56","title":"Quantum speed limit for Kirkwood-Dirac quasiprobabilities","abstract":"What is the minimal time until a quantum system can exhibit genuine quantum features? To answer this question we derive quantum speed limits for two-time correlation functions arising from statistics of measurements. Generally, these two-time correlators are described by quasiprobabilities, if the initial quantum state of the system does not commute with the measurement observables. Our quantum speed limits are derived from the Heisenberg-Robertson uncertainty relation, and set the minimal time at which a quasiprobability can become non-positive, which is evidence for the onset of non-classical traits in the system dynamics. As an illustrative example, we apply these results to a conditional quantum gate, by determining the optimal condition giving rise to non-classicality at maximum speed. Our analysis also hints at boosted power extraction in genuinely non-classical dynamics.","sentences":["What is the minimal time until a quantum system can exhibit genuine quantum features?","To answer this question we derive quantum speed limits for two-time correlation functions arising from statistics of measurements.","Generally, these two-time correlators are described by quasiprobabilities, if the initial quantum state of the system does not commute with the measurement observables.","Our quantum speed limits are derived from the Heisenberg-Robertson uncertainty relation, and set the minimal time at which a quasiprobability can become non-positive, which is evidence for the onset of non-classical traits in the system dynamics.","As an illustrative example, we apply these results to a conditional quantum gate, by determining the optimal condition giving rise to non-classicality at maximum speed.","Our analysis also hints at boosted power extraction in genuinely non-classical dynamics."],"url":"http://arxiv.org/abs/2402.07582v1","category":"quant-ph"}
{"created":"2024-02-12 11:20:22","title":"LFOC: A Lightweight Fairness-Oriented Cache Clustering Policy for Commodity Multicores","abstract":"Multicore processors constitute the main architecture choice for modern computing systems in different market segments. Despite their benefits, the contention that naturally appears when multiple applications compete for the use of shared resources among cores, such as the last-level cache (LLC), may lead to substantial performance degradation. This may have a negative impact on key system aspects such as throughput and fairness. Assigning the various applications in the workload to separate LLC partitions with possibly different sizes, has been proven effective to mitigate shared-resource contention effects.   In this article we propose LFOC, a clustering-based cache partitioning scheme that strives to deliver fairness while providing acceptable system throughput. LFOC leverages the Intel Cache Allocation Technology (CAT), which enables the system software to divide the LLC into different partitions. To accomplish its goals, LFOC tries to mimic the behavior of the optimal cache-clustering solution, which we could approximate by means of a simulator in different scenarios. To this end, LFOC effectively identifies streaming aggressor programs and cache sensitive applications, which are then assigned to separate cache partitions.   We implemented LFOC in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of two state-of-the-art policies that optimize fairness and throughput, respectively. Our experimental analysis reveals that LFOC is able to bring a higher reduction in unfairness by leveraging a lightweight algorithm suitable for adoption in a real OS.","sentences":["Multicore processors constitute the main architecture choice for modern computing systems in different market segments.","Despite their benefits, the contention that naturally appears when multiple applications compete for the use of shared resources among cores, such as the last-level cache (LLC), may lead to substantial performance degradation.","This may have a negative impact on key system aspects such as throughput and fairness.","Assigning the various applications in the workload to separate LLC partitions with possibly different sizes, has been proven effective to mitigate shared-resource contention effects.   ","In this article we propose LFOC, a clustering-based cache partitioning scheme that strives to deliver fairness while providing acceptable system throughput.","LFOC leverages the Intel Cache Allocation Technology (CAT), which enables the system software to divide the LLC into different partitions.","To accomplish its goals, LFOC tries to mimic the behavior of the optimal cache-clustering solution, which we could approximate by means of a simulator in different scenarios.","To this end, LFOC effectively identifies streaming aggressor programs and cache sensitive applications, which are then assigned to separate cache partitions.   ","We implemented LFOC in the Linux kernel and evaluated it on a real system featuring an Intel Skylake processor, where we compare its effectiveness to that of two state-of-the-art policies that optimize fairness and throughput, respectively.","Our experimental analysis reveals that LFOC is able to bring a higher reduction in unfairness by leveraging a lightweight algorithm suitable for adoption in a real OS."],"url":"http://arxiv.org/abs/2402.07578v1","category":"cs.DC"}
{"created":"2024-02-12 11:18:32","title":"Topic Modeling as Multi-Objective Contrastive Optimization","abstract":"Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive learning objective that contrasts pairs of input documents. However, document-level contrastive learning might capture low-level mutual information, such as word ratio, which disturbs topic modeling. Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive loss which attempts to learn topic representations that generalize among input documents. To address these issues, we first introduce a novel contrastive learning method oriented towards sets of topic vectors to capture useful semantics that are shared among a set of input documents. Secondly, we explicitly cast contrastive topic modeling as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that balances the trade-off between the ELBO and the contrastive objective. Extensive experiments demonstrate that our framework consistently produces higher-performing neural topic models in terms of topic coherence, topic diversity, and downstream performance.","sentences":["Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive learning objective that contrasts pairs of input documents.","However, document-level contrastive learning might capture low-level mutual information, such as word ratio, which disturbs topic modeling.","Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive loss which attempts to learn topic representations that generalize among input documents.","To address these issues, we first introduce a novel contrastive learning method oriented towards sets of topic vectors to capture useful semantics that are shared among a set of input documents.","Secondly, we explicitly cast contrastive topic modeling as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that balances the trade-off between the ELBO and the contrastive objective.","Extensive experiments demonstrate that our framework consistently produces higher-performing neural topic models in terms of topic coherence, topic diversity, and downstream performance."],"url":"http://arxiv.org/abs/2402.07577v1","category":"cs.CL"}
{"created":"2024-02-12 11:14:47","title":"Flux-tunable Kitaev chain in a quantum dot array","abstract":"Connecting quantum dots through Andreev bound states in a semiconductor-superconductor hybrid provides a platform to create a Kitaev chain. Interestingly, in a double quantum dot, a pair of poor man's Majorana zero modes can emerge when the system is fine-tuned to a sweet spot, where superconducting and normal couplings are equal in magnitude. Control of the Andreev bound states is crucial for achieving this, usually implemented by varying its chemical potential. In this work, we propose using Andreev bound states in a narrow Josephson junction to mediate both types of couplings, with the ratio tunable by the phase difference across the junction. Now a minimal Kitaev chain can be easily tuned into the strong coupling regime by varying the phase and junction asymmetry, even without changing the dot-hybrid coupling strength. Furthermore, we identify an optimal sweet spot at $\\pi$ phase, enhancing the excitation gap and robustness against phase fluctuations. Our proposal introduces a new device platform and a new tuning method for realizing quantum-dot-based Kitaev chains.","sentences":["Connecting quantum dots through Andreev bound states in a semiconductor-superconductor hybrid provides a platform to create a Kitaev chain.","Interestingly, in a double quantum dot, a pair of poor man's Majorana zero modes can emerge when the system is fine-tuned to a sweet spot, where superconducting and normal couplings are equal in magnitude.","Control of the Andreev bound states is crucial for achieving this, usually implemented by varying its chemical potential.","In this work, we propose using Andreev bound states in a narrow Josephson junction to mediate both types of couplings, with the ratio tunable by the phase difference across the junction.","Now a minimal Kitaev chain can be easily tuned into the strong coupling regime by varying the phase and junction asymmetry, even without changing the dot-hybrid coupling strength.","Furthermore, we identify an optimal sweet spot at $\\pi$ phase, enhancing the excitation gap and robustness against phase fluctuations.","Our proposal introduces a new device platform and a new tuning method for realizing quantum-dot-based Kitaev chains."],"url":"http://arxiv.org/abs/2402.07575v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-12 10:56:47","title":"Joint User and Beam Selection in Millimeter Wave Networks","abstract":"We study the problem of selecting a user equipment (UE) and a beam for each access point (AP) for concurrent transmissions in a millimeter wave (mmWave) network, such that the sum of weighted rates of UEs is maximized. We prove that this problem is NP-complete. We propose two algorithms -- Markov Chain Monte Carlo (MCMC) based and local interaction game (LIG) based UE and beam selection -- and prove that both of them asymptotically achieve the optimal solution. Also, we propose two fast greedy algorithms -- NGUB1 and NGUB2 -- for UE and beam selection. Through extensive simulations, we show that our proposed greedy algorithms outperform the most relevant algorithms proposed in prior work and perform close to the asymptotically optimal algorithms.","sentences":["We study the problem of selecting a user equipment (UE) and a beam for each access point (AP) for concurrent transmissions in a millimeter wave (mmWave) network, such that the sum of weighted rates of UEs is maximized.","We prove that this problem is NP-complete.","We propose two algorithms -- Markov Chain Monte Carlo (MCMC) based and local interaction game (LIG) based UE and beam selection -- and prove that both of them asymptotically achieve the optimal solution.","Also, we propose two fast greedy algorithms -- NGUB1 and NGUB2 -- for UE and beam selection.","Through extensive simulations, we show that our proposed greedy algorithms outperform the most relevant algorithms proposed in prior work and perform close to the asymptotically optimal algorithms."],"url":"http://arxiv.org/abs/2402.07563v1","category":"eess.SY"}
{"created":"2024-02-12 10:54:57","title":"A Reinforcement Learning Approach to the Design of Quantum Chains for Optimal Energy Transfer","abstract":"We propose a bottom-up approach, based on Reinforcement Learning, to the design of a chain achieving efficient excitation-transfer performances. We assume distance-dependent interactions among particles arranged in a chain under tight-binding conditions. Starting from two particles and a localised excitation, we gradually increase the number of constitutents of the system so as to improve the transfer probability. We formulate the problem of finding the optimal locations and numbers of particles as a Markov Decision Process: we use Proximal Policy Optimization to find the optimal chain-building policies and the optimal chain configurations under different scenarios. We consider both the case in which the target is a sink connected to the end of the chain and the case in which the target is the right-most particle in the chain. We address the problem of disorder in the chain induced by particle positioning errors. We are able to achieve extremely high excitation transfer in all cases, with different chain configurations and properties depending on the specific conditions.","sentences":["We propose a bottom-up approach, based on Reinforcement Learning, to the design of a chain achieving efficient excitation-transfer performances.","We assume distance-dependent interactions among particles arranged in a chain under tight-binding conditions.","Starting from two particles and a localised excitation, we gradually increase the number of constitutents of the system so as to improve the transfer probability.","We formulate the problem of finding the optimal locations and numbers of particles as a Markov Decision Process: we use Proximal Policy Optimization to find the optimal chain-building policies and the optimal chain configurations under different scenarios.","We consider both the case in which the target is a sink connected to the end of the chain and the case in which the target is the right-most particle in the chain.","We address the problem of disorder in the chain induced by particle positioning errors.","We are able to achieve extremely high excitation transfer in all cases, with different chain configurations and properties depending on the specific conditions."],"url":"http://arxiv.org/abs/2402.07561v1","category":"quant-ph"}
{"created":"2024-02-12 10:54:05","title":"Stabilization of control systems associated with a strongly continuous group","abstract":"This paper is devoted to the stabilization of a linear control system $y' = A y + B u$ and its suitable non-linear variants where $(A, \\cD(A))$ is an infinitesimal generator of a strongly continuous {\\it group} in a Hilbert space $\\mH$, and $B$ defined in a Hilbert space $\\mU$ is an admissible control operator with respect to the semigroup generated by $A$. Let $\\lambda \\in \\mR$ and assume that, for some {\\it positive} symmetric, invertible $Q = Q(\\lambda) \\in \\cL(\\mH)$, for some {\\it non-negative}, symmetric $R = R(\\lambda) \\in \\cL(\\mH)$, and for some {\\it non-negative}, symmetric $W = W(\\lambda) \\in \\cL(\\mU)$, it holds $$ A Q + Q A^* - B W B^* + Q R Q + 2 \\lambda Q = 0 $$ in the sense that $$ \\langle Qx, A^*y \\rangle_{\\mH} + \\langle A^*x, Q y \\rangle_{\\mH} - \\langle W B^*x, B^*y \\rangle_{\\mU} + \\langle R Qx, Q y \\rangle_{\\mH} + 2 \\lambda \\langle Q x, y \\rangle_{\\mH}= 0 \\quad \\forall \\, x, y \\in \\cD(A^*), $$ where $A^*$ is the adjoint of $A$ and $\\cD(A^*)$ is its domain. We present a new method to study the stabilization of such a system and its suitable nonlinear variants. Both the stabilization using dynamic feedback controls and the stabilization using static feedback controls in a weak sense are investigated. To our knowledge, the stabilization by dynamic feedback controls is new even in the linear setting. The nonlinear case is out of reach previously when $B$ is unbounded for both types of stabilization. Consequently, we derive that if the control system is exactly controllable in some positive time, then it is rapidly stabilizable.","sentences":["This paper is devoted to the stabilization of a linear control system $y' =","A y + B u$ and its suitable non-linear variants where $(A, \\cD(A))$ is an infinitesimal generator of a strongly continuous {\\it group} in a Hilbert space $\\mH$, and $B$ defined in a Hilbert space $\\mU$ is an admissible control operator with respect to the semigroup generated by $A$.","Let $\\lambda \\in \\mR$ and assume that, for some {\\it positive} symmetric, invertible $Q = Q(\\lambda) \\in \\cL(\\mH)$, for some {\\it non-negative}, symmetric $R = R(\\lambda) \\in \\cL(\\mH)$, and for some {\\it non-negative}, symmetric $W = W(\\lambda) \\in \\cL(\\mU)$, it holds $$ A Q + Q A^* - B W B^* + Q R Q + 2 \\lambda Q = 0 $$ in the sense that $$ \\langle Qx, A^*y \\rangle_{\\mH} + \\langle A^*x, Q y \\rangle_{\\mH} - \\langle W B^*x, B^*y \\rangle_{\\mU} +","\\langle R Qx, Q y \\rangle_{\\mH} + 2 \\lambda \\langle Q x, y \\rangle_{\\mH}= 0","\\quad \\forall \\, x, y \\in \\cD(A^*), $$ where $A^*$ is the adjoint of $A$ and $\\cD(A^*)$ is its domain.","We present a new method to study the stabilization of such a system and its suitable nonlinear variants.","Both the stabilization using dynamic feedback controls and the stabilization using static feedback controls in a weak sense are investigated.","To our knowledge, the stabilization by dynamic feedback controls is new even in the linear setting.","The nonlinear case is out of reach previously when $B$ is unbounded for both types of stabilization.","Consequently, we derive that if the control system is exactly controllable in some positive time, then it is rapidly stabilizable."],"url":"http://arxiv.org/abs/2402.07560v1","category":"math.OC"}
{"created":"2024-02-12 10:35:16","title":"Highly efficient channeling of single photons into guided modes of optical nanocapillary fibers","abstract":"We report numerically the efficient channeling of single photons from a single quantum emitter into guided modes of optical nanocapillary fibers (NCFs). The NCF is formed of a liquid core optical nanofiber with inner and outer diameters. We optimize the inner and outer diameters of the NCF filled with water medium by placing a single dipole source (SDS) inside. The maximum channeling efficiency of 52% is found when the radially polarized SDS is placed at the center of the NCF filled with the water medium. The optimum inner and outer diameters of the NCF are 100 nm and 360 nm for the emission wavelength of 620 nm, respectively. Additionally, we investigate the SDS position dependence inside the NCF considering experimental ambiguity in placing a single quantum emitter inside the NCF. We found that the channeling efficiency remains almost constant for the water medium at the optimum condition. The present platform may open a novel route for generating single photons in quantum technologies and detecting single cells in bio-sensing.","sentences":["We report numerically the efficient channeling of single photons from a single quantum emitter into guided modes of optical nanocapillary fibers (NCFs).","The NCF is formed of a liquid core optical nanofiber with inner and outer diameters.","We optimize the inner and outer diameters of the NCF filled with water medium by placing a single dipole source (SDS) inside.","The maximum channeling efficiency of 52% is found when the radially polarized SDS is placed at the center of the NCF filled with the water medium.","The optimum inner and outer diameters of the NCF are 100 nm and 360 nm for the emission wavelength of 620 nm, respectively.","Additionally, we investigate the SDS position dependence inside the NCF considering experimental ambiguity in placing a single quantum emitter inside the NCF.","We found that the channeling efficiency remains almost constant for the water medium at the optimum condition.","The present platform may open a novel route for generating single photons in quantum technologies and detecting single cells in bio-sensing."],"url":"http://arxiv.org/abs/2402.07552v1","category":"quant-ph"}
{"created":"2024-02-12 10:30:45","title":"A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing","abstract":"Analog In-Memory Computing (AIMC) is an emerging technology for fast and energy-efficient Deep Learning (DL) inference. However, a certain amount of digital post-processing is required to deal with circuit mismatches and non-idealities associated with the memory devices. Efficient near-memory digital logic is critical to retain the high area/energy efficiency and low latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic with limited parallelization capability and high latency. To overcome these limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on fixed-point arithmetic. It achieves competitive accuracy and higher computing throughput than previous approaches while minimizing the area overhead. Moreover, the NMPU supports standard DL activation steps, such as ReLU and Batch Normalization. We perform a physical implementation of the NMPU design in a 14 nm CMOS technology and provide detailed performance, power, and area assessments. We validate the efficacy of the NMPU by using data from an AIMC chip and demonstrate that a simulated AIMC system with the proposed NMPU outperforms existing FP16-based implementations, providing 139$\\times$ speed-up, 7.8$\\times$ smaller area, and a competitive power consumption. Additionally, our approach achieves an inference accuracy of 86.65 %/65.06 %, with an accuracy drop of just 0.12 %/0.4 % compared to the FP16 baseline when benchmarked with ResNet9/ResNet32 networks trained on the CIFAR10/CIFAR100 datasets, respectively.","sentences":["Analog In-Memory Computing (AIMC) is an emerging technology for fast and energy-efficient Deep Learning (DL) inference.","However, a certain amount of digital post-processing is required to deal with circuit mismatches and non-idealities associated with the memory devices.","Efficient near-memory digital logic is critical to retain the high area/energy efficiency and low latency of AIMC.","Existing systems adopt Floating Point 16 (FP16) arithmetic with limited parallelization capability and high latency.","To overcome these limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on fixed-point arithmetic.","It achieves competitive accuracy and higher computing throughput than previous approaches while minimizing the area overhead.","Moreover, the NMPU supports standard DL activation steps, such as ReLU and Batch Normalization.","We perform a physical implementation of the NMPU design in a 14 nm CMOS technology and provide detailed performance, power, and area assessments.","We validate the efficacy of the NMPU by using data from an AIMC chip and demonstrate that a simulated AIMC system with the proposed NMPU outperforms existing FP16-based implementations, providing 139$\\times$ speed-up, 7.8$\\times$ smaller area, and a competitive power consumption.","Additionally, our approach achieves an inference accuracy of 86.65 %/65.06","%, with an accuracy drop of just 0.12 %/0.4","% compared to the FP16 baseline when benchmarked with ResNet9/ResNet32 networks trained on the CIFAR10/CIFAR100 datasets, respectively."],"url":"http://arxiv.org/abs/2402.07549v1","category":"cs.AR"}
{"created":"2024-02-12 10:16:55","title":"Agro-ecological control of a pest-host system: optimizing the harvest","abstract":"We delve into the interactions between a prey-predator and a vector-borne epidemic system, driven by agro-ecological motivations. This system involves an ODE, two reaction--diffusion PDEs and one reaction--diffusion--advection PDE. It has no complete variational or monotonic structure and features spatially heterogeneous coefficients. Our initial focus is to examine the continuity of a quantity known as ''harvest'', which depends on the time-integral of infected vectors. We analyze its asymptotic behaviour as the domain becomes homogeneous. Then we tackle a non-standard optimal control problem related to the linearized harvest and conduct an analysis to establish the existence, uniqueness, and properties of optimizers. Finally, we refine the location of optimizers under specific initial conditions.","sentences":["We delve into the interactions between a prey-predator and a vector-borne epidemic system, driven by agro-ecological motivations.","This system involves an ODE, two reaction--diffusion PDEs and one reaction--diffusion--advection PDE.","It has no complete variational or monotonic structure and features spatially heterogeneous coefficients.","Our initial focus is to examine the continuity of a quantity known as ''harvest'', which depends on the time-integral of infected vectors.","We analyze its asymptotic behaviour as the domain becomes homogeneous.","Then we tackle a non-standard optimal control problem related to the linearized harvest and conduct an analysis to establish the existence, uniqueness, and properties of optimizers.","Finally, we refine the location of optimizers under specific initial conditions."],"url":"http://arxiv.org/abs/2402.07546v1","category":"math.AP"}
{"created":"2024-02-12 09:57:47","title":"Accelerating Distributed Deep Learning using Lossless Homomorphic Compression","abstract":"As deep neural networks (DNNs) grow in complexity and size, the resultant increase in communication overhead during distributed training has become a significant bottleneck, challenging the scalability of distributed training systems. Existing solutions, while aiming to mitigate this bottleneck through worker-level compression and in-network aggregation, fall short due to their inability to efficiently reconcile the trade-offs between compression effectiveness and computational overhead, hindering overall performance and scalability. In this paper, we introduce a novel compression algorithm that effectively merges worker-level compression with in-network aggregation. Our solution is both homomorphic, allowing for efficient in-network aggregation without CPU/GPU processing, and lossless, ensuring no compromise on training accuracy. Theoretically optimal in compression and computational efficiency, our approach is empirically validated across diverse DNN models such as NCF, LSTM, VGG19, and BERT-base, showing up to a 6.33$\\times$ improvement in aggregation throughput and a 3.74$\\times$ increase in per-iteration training speed.","sentences":["As deep neural networks (DNNs) grow in complexity and size, the resultant increase in communication overhead during distributed training has become a significant bottleneck, challenging the scalability of distributed training systems.","Existing solutions, while aiming to mitigate this bottleneck through worker-level compression and in-network aggregation, fall short due to their inability to efficiently reconcile the trade-offs between compression effectiveness and computational overhead, hindering overall performance and scalability.","In this paper, we introduce a novel compression algorithm that effectively merges worker-level compression with in-network aggregation.","Our solution is both homomorphic, allowing for efficient in-network aggregation without CPU/GPU processing, and lossless, ensuring no compromise on training accuracy.","Theoretically optimal in compression and computational efficiency, our approach is empirically validated across diverse DNN models such as NCF, LSTM, VGG19, and BERT-base, showing up to a 6.33$\\times$ improvement in aggregation throughput and a 3.74$\\times$ increase in per-iteration training speed."],"url":"http://arxiv.org/abs/2402.07529v1","category":"cs.DC"}
{"created":"2024-02-12 09:47:50","title":"Reinforcement learning based demand charge minimization using energy storage","abstract":"Utilities have introduced demand charges to encourage customers to reduce their demand peaks, since a high peak may cause very high costs for both the utility and the consumer. We herein study the bill minimization problem for customers equipped with an energy storage device and a self-owned renewable energy production. A model-free reinforcement learning algorithm is carefully designed to reduce both the energy charge and the demand charge of the consumer. The proposed algorithm does not need forecasting models for the energy demand and the renewable energy production. The resulting controller can be used online, and progressively improved with newly gathered data. The algorithm is validated on real data from an office building of IFPEN Solaize site. Numerical results show that our algorithm can reduce electricity bills with both daily and monthly demand charges.","sentences":["Utilities have introduced demand charges to encourage customers to reduce their demand peaks, since a high peak may cause very high costs for both the utility and the consumer.","We herein study the bill minimization problem for customers equipped with an energy storage device and a self-owned renewable energy production.","A model-free reinforcement learning algorithm is carefully designed to reduce both the energy charge and the demand charge of the consumer.","The proposed algorithm does not need forecasting models for the energy demand and the renewable energy production.","The resulting controller can be used online, and progressively improved with newly gathered data.","The algorithm is validated on real data from an office building of IFPEN Solaize site.","Numerical results show that our algorithm can reduce electricity bills with both daily and monthly demand charges."],"url":"http://arxiv.org/abs/2402.07525v1","category":"math.OC"}
{"created":"2024-02-12 09:08:12","title":"Efficient reduction of Feynman integrals on supercomputers","abstract":"Feynman integral reduction by means of integration-by-parts identities is a major power gadget in a theorist toolbox indispensable for calculation of multiloop quantum effects relevant for particle phenomenology and formal theory alike. An algorithmic approach consists of solving a large sparse non-square system of homogeneous linear equations with polynomial coefficients. While an analytical way of doing this is legitimate and was pursued for decades, it undoubtedly has its limitations when applied in complicated circumstances. Thus, a complementary framework based on modular arithmetic becomes critical on the way to conquer the current `what is possible' frontier. This calls for use of supercomputers to address the reduction problem. In order to properly utilize these computational resources, one has to efficiently optimize the technique for this purpose. Presently, we discuss and implement various methods which allow us to significantly improve performance of Feynman integral reduction within the FIRE environment.","sentences":["Feynman integral reduction by means of integration-by-parts identities is a major power gadget in a theorist toolbox indispensable for calculation of multiloop quantum effects relevant for particle phenomenology and formal theory alike.","An algorithmic approach consists of solving a large sparse non-square system of homogeneous linear equations with polynomial coefficients.","While an analytical way of doing this is legitimate and was pursued for decades, it undoubtedly has its limitations when applied in complicated circumstances.","Thus, a complementary framework based on modular arithmetic becomes critical on the way to conquer the current `what is possible' frontier.","This calls for use of supercomputers to address the reduction problem.","In order to properly utilize these computational resources, one has to efficiently optimize the technique for this purpose.","Presently, we discuss and implement various methods which allow us to significantly improve performance of Feynman integral reduction within the FIRE environment."],"url":"http://arxiv.org/abs/2402.07499v1","category":"hep-ph"}
{"created":"2024-02-12 09:00:06","title":"Ultra-fast Waveguide MUTC Photodiodes over 220 GHz","abstract":"We present InP-based evanescently-coupled waveguide modified uni-traveling carrier photodiodes (MUTC-PDs) exhibiting a breakthrough in bandwidth. The optimization of carrier transport and optical coupling is achieved through a detailed discussion on the design of the cliff layer and waveguide layer. Addressing the parasitic capacitance challenge, we introduce benzocyclobutene (BCB) beneath the PD electrodes, effectively overcoming the bandwidth bottleneck associated with the RC time constant. Devices with sizes of 2 * 7 um2 and 2 * 10 um2 achieve 3-dB bandwidths over 220 GHz, along with external responsivities of 0.161 A/W and 0.237 A/W, respectively. Notably, the RF output power reaches a peak of -1.69 dBm at 215 GHz for 2 * 15 um2 PDs.","sentences":["We present InP-based evanescently-coupled waveguide modified uni-traveling carrier photodiodes (MUTC-PDs) exhibiting a breakthrough in bandwidth.","The optimization of carrier transport and optical coupling is achieved through a detailed discussion on the design of the cliff layer and waveguide layer.","Addressing the parasitic capacitance challenge, we introduce benzocyclobutene (BCB) beneath the PD electrodes, effectively overcoming the bandwidth bottleneck associated with the RC time constant.","Devices with sizes of 2 * 7 um2 and 2 * 10 um2 achieve 3-dB bandwidths over 220 GHz, along with external responsivities of 0.161 A/W and 0.237 A/W, respectively.","Notably, the RF output power reaches a peak of -1.69 dBm at 215 GHz for 2 * 15 um2 PDs."],"url":"http://arxiv.org/abs/2402.07491v1","category":"physics.app-ph"}
{"created":"2024-02-12 08:12:59","title":"Transient growth of a wake vortex and its initiation via inertial particles","abstract":"The transient dynamics of a wake vortex, modelled by a strong swirling $q$-vortex, are examined with an emphasis on exploring optimal transient growth constructed by continuous eigenmodes associated with continuous spectra. The pivotal contribution of the viscous critical-layer eigenmodes (Lee & Marcus, J. Fluid Mech., vol. 967) amongst the entire eigenmode families to optimal perturbations is numerically confirmed, based on a spectral collocation method for a radially unbounded domain that ensures correct analyticity and far-field behaviour. The consistence of the numerical method against numerical sensitivity provides reliability of results as well as flexibility for tuning. Both axisymmetric and helical perturbation cases with axial wavenumbers of order unity or less are considered in the study through both linearised theory and non-linear simulations, yielding results that align with literature on both energy growth curves and optimal perturbation structures. Additionally, the initiation process of transient growth is discussed to reveal its practicability. Inspired by ice crystals in contrails, the role of backward influences of inertial particles on the carrier vortex flow, especially via particle drag, is underscored. In the pursuit of optimal transient growth, the particles are initially distributed at the periphery of the vortex core to disturb the vortex. Two-way coupled vortex-particle simulations conclusively demonstrate clear indications of optimal transient growth during continual vortex-particle interactions, reinforcing the robustness and significance of the transient growth process in the original non-linear vortex system over finite time periods.","sentences":["The transient dynamics of a wake vortex, modelled by a strong swirling $q$-vortex, are examined with an emphasis on exploring optimal transient growth constructed by continuous eigenmodes associated with continuous spectra.","The pivotal contribution of the viscous critical-layer eigenmodes (Lee & Marcus, J. Fluid Mech., vol. 967) amongst the entire eigenmode families to optimal perturbations is numerically confirmed, based on a spectral collocation method for a radially unbounded domain that ensures correct analyticity and far-field behaviour.","The consistence of the numerical method against numerical sensitivity provides reliability of results as well as flexibility for tuning.","Both axisymmetric and helical perturbation cases with axial wavenumbers of order unity or less are considered in the study through both linearised theory and non-linear simulations, yielding results that align with literature on both energy growth curves and optimal perturbation structures.","Additionally, the initiation process of transient growth is discussed to reveal its practicability.","Inspired by ice crystals in contrails, the role of backward influences of inertial particles on the carrier vortex flow, especially via particle drag, is underscored.","In the pursuit of optimal transient growth, the particles are initially distributed at the periphery of the vortex core to disturb the vortex.","Two-way coupled vortex-particle simulations conclusively demonstrate clear indications of optimal transient growth during continual vortex-particle interactions, reinforcing the robustness and significance of the transient growth process in the original non-linear vortex system over finite time periods."],"url":"http://arxiv.org/abs/2402.07469v1","category":"physics.flu-dyn"}
{"created":"2024-02-12 07:52:55","title":"PyDMD: A Python package for robust dynamic mode decomposition","abstract":"The dynamic mode decomposition (DMD) is a simple and powerful data-driven modeling technique that is capable of revealing coherent spatiotemporal patterns from data. The method's linear algebra-based formulation additionally allows for a variety of optimizations and extensions that make the algorithm practical and viable for real-world data analysis. As a result, DMD has grown to become a leading method for dynamical system analysis across multiple scientific disciplines. PyDMD is a Python package that implements DMD and several of its major variants. In this work, we expand the PyDMD package to include a number of cutting-edge DMD methods and tools specifically designed to handle dynamics that are noisy, multiscale, parameterized, prohibitively high-dimensional, or even strongly nonlinear. We provide a complete overview of the features available in PyDMD as of version 1.0, along with a brief overview of the theory behind the DMD algorithm, information for developers, tips regarding practical DMD usage, and introductory coding examples. All code is available at https://github.com/PyDMD/PyDMD .","sentences":["The dynamic mode decomposition (DMD) is a simple and powerful data-driven modeling technique that is capable of revealing coherent spatiotemporal patterns from data.","The method's linear algebra-based formulation additionally allows for a variety of optimizations and extensions that make the algorithm practical and viable for real-world data analysis.","As a result, DMD has grown to become a leading method for dynamical system analysis across multiple scientific disciplines.","PyDMD is a Python package that implements DMD and several of its major variants.","In this work, we expand the PyDMD package to include a number of cutting-edge DMD methods and tools specifically designed to handle dynamics that are noisy, multiscale, parameterized, prohibitively high-dimensional, or even strongly nonlinear.","We provide a complete overview of the features available in PyDMD as of version 1.0, along with a brief overview of the theory behind the DMD algorithm, information for developers, tips regarding practical DMD usage, and introductory coding examples.","All code is available at https://github.com/PyDMD/PyDMD ."],"url":"http://arxiv.org/abs/2402.07463v1","category":"stat.CO"}
{"created":"2024-02-12 07:49:48","title":"A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?","abstract":"The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences. This problem requires a method to define and regulate safe and optimal limits of AI behaviors. In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful. By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors. We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips. Our approach may be used to help create an evolving database of 'values' based on the hedonic calculus of repeatable behaviors with decreasing marginal utility. This positions HALO as a promising solution for the value-loading problem, which involves embedding human-aligned values into an AI system, and the weak-to-strong generalization problem, which explores whether weak models can supervise stronger models as they become more intelligent. Hence, HALO opens several research avenues that may lead to the development of a computational value system that allows an AI algorithm to learn whether the decisions it makes are right or wrong.","sentences":["The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences.","This problem requires a method to define and regulate safe and optimal limits of AI behaviors.","In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI.","Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful.","By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors.","We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips.","Our approach may be used to help create an evolving database of 'values' based on the hedonic calculus of repeatable behaviors with decreasing marginal utility.","This positions HALO as a promising solution for the value-loading problem, which involves embedding human-aligned values into an AI system, and the weak-to-strong generalization problem, which explores whether weak models can supervise stronger models as they become more intelligent.","Hence, HALO opens several research avenues that may lead to the development of a computational value system that allows an AI algorithm to learn whether the decisions it makes are right or wrong."],"url":"http://arxiv.org/abs/2402.07462v2","category":"cs.AI"}
