{"created":"2024-04-25 17:59:59","title":"Double Copy of 3D Chern-Simons Theory and 6D Kodaira-Spencer Gravity","abstract":"We apply an algebraic double copy construction of gravity from gauge theory to three-dimensional (3D) Chern-Simons theory. The kinematic algebra ${\\cal K}$ is the 3D de Rham complex of forms equipped, for a choice of metric, with a graded Lie algebra that is equivalent to the Schouten-Nijenhuis bracket on polyvector fields. The double copied gravity is defined on a subspace of ${\\cal K}\\otimes \\bar{\\cal K}$ and yields a topological double field theory for a generalized metric perturbation and two 2-forms. This local and gauge invariant theory is non-Lagrangian but can be rendered Lagrangian by abandoning locality. Upon fixing a gauge this reduces to the double copy of Chern-Simons theory previously proposed by Ben-Shahar and Johansson. Furthermore, using complex coordinates in $\\mathbb{C}^3$ this theory is related to six-dimensional (6D) Kodaira-Spencer gravity in that truncating the two 2-forms and one equation yields the Kodaira-Spencer equations on a 3D real slice of $\\mathbb{C}^3$. The full 6D Kodaira-Spencer theory can instead be obtained as a consistent truncation of a chiral double copy.","sentences":["We apply an algebraic double copy construction of gravity from gauge theory to three-dimensional (3D) Chern-Simons theory.","The kinematic algebra ${\\cal K}$ is the 3D de Rham complex of forms equipped, for a choice of metric, with a graded Lie algebra that is equivalent to the Schouten-Nijenhuis bracket on polyvector fields.","The double copied gravity is defined on a subspace of ${\\cal K}\\otimes \\bar{\\cal K}$ and yields a topological double field theory for a generalized metric perturbation and two 2-forms.","This local and gauge invariant theory is non-Lagrangian but can be rendered Lagrangian by abandoning locality.","Upon fixing a gauge this reduces to the double copy of Chern-Simons theory previously proposed by Ben-Shahar and Johansson.","Furthermore, using complex coordinates in $\\mathbb{C}^3$ this theory is related to six-dimensional (6D) Kodaira-Spencer gravity in that truncating the two 2-forms and one equation yields the Kodaira-Spencer equations on a 3D real slice of $\\mathbb{C}^3$. The full 6D Kodaira-Spencer theory can instead be obtained as a consistent truncation of a chiral double copy."],"url":"http://arxiv.org/abs/2404.16830v1","category":"hep-th"}
{"created":"2024-04-25 17:59:59","title":"The Third Monocular Depth Estimation Challenge","abstract":"This paper discusses the results of the third edition of the Monocular Depth Estimation Challenge (MDEC). The challenge focuses on zero-shot generalization to the challenging SYNS-Patches dataset, featuring complex scenes in natural and indoor settings. As with the previous edition, methods can use any form of supervision, i.e. supervised or self-supervised. The challenge received a total of 19 submissions outperforming the baseline on the test set: 10 among them submitted a report describing their approach, highlighting a diffused use of foundational models such as Depth Anything at the core of their method. The challenge winners drastically improved 3D F-Score performance, from 17.51% to 23.72%.","sentences":["This paper discusses the results of the third edition of the Monocular Depth Estimation Challenge (MDEC).","The challenge focuses on zero-shot generalization to the challenging SYNS-Patches dataset, featuring complex scenes in natural and indoor settings.","As with the previous edition, methods can use any form of supervision, i.e. supervised or self-supervised.","The challenge received a total of 19 submissions outperforming the baseline on the test set: 10 among them submitted a report describing their approach, highlighting a diffused use of foundational models such as Depth Anything at the core of their method.","The challenge winners drastically improved 3D F-Score performance, from 17.51% to 23.72%."],"url":"http://arxiv.org/abs/2404.16831v1","category":"cs.CV"}
{"created":"2024-04-25 17:59:58","title":"Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting 3D Objects with Realistic Materials","abstract":"Physically realistic materials are pivotal in augmenting the realism of 3D assets across various applications and lighting conditions. However, existing 3D assets and generative models often lack authentic material properties. Manual assignment of materials using graphic software is a tedious and time-consuming task. In this paper, we exploit advancements in Multimodal Large Language Models (MLLMs), particularly GPT-4V, to present a novel approach, Make-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and describe materials, allowing the construction of a detailed material library. 2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V precisely identifies and aligns materials with the corresponding components of 3D objects. 3) The correctly matched materials are then meticulously applied as reference for the new SVBRDF material generation according to the original diffuse map, significantly enhancing their visual authenticity. Make-it-Real offers a streamlined integration into the 3D content creation workflow, showcasing its utility as an essential tool for developers of 3D assets.","sentences":["Physically realistic materials are pivotal in augmenting the realism of 3D assets across various applications and lighting conditions.","However, existing 3D assets and generative models often lack authentic material properties.","Manual assignment of materials using graphic software is a tedious and time-consuming task.","In this paper, we exploit advancements in Multimodal Large Language Models (MLLMs), particularly GPT-4V, to present a novel approach, Make-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and describe materials, allowing the construction of a detailed material library.","2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V precisely identifies and aligns materials with the corresponding components of 3D objects.","3)","The correctly matched materials are then meticulously applied as reference for the new SVBRDF material generation according to the original diffuse map, significantly enhancing their visual authenticity.","Make-it-Real offers a streamlined integration into the 3D content creation workflow, showcasing its utility as an essential tool for developers of 3D assets."],"url":"http://arxiv.org/abs/2404.16829v1","category":"cs.CV"}
{"created":"2024-04-25 17:59:56","title":"Made to Order: Discovering monotonic temporal changes via self-supervised video ordering","abstract":"Our objective is to discover and localize monotonic temporal changes in a sequence of images. To achieve this, we exploit a simple proxy task of ordering a shuffled image sequence, with `time' serving as a supervisory signal since only changes that are monotonic with time can give rise to the correct ordering. We also introduce a flexible transformer-based model for general-purpose ordering of image sequences of arbitrary length with built-in attribution maps. After training, the model successfully discovers and localizes monotonic changes while ignoring cyclic and stochastic ones. We demonstrate applications of the model in multiple video settings covering different scene and object types, discovering both object-level and environmental changes in unseen sequences. We also demonstrate that the attention-based attribution maps function as effective prompts for segmenting the changing regions, and that the learned representations can be used for downstream applications. Finally, we show that the model achieves the state of the art on standard benchmarks for ordering a set of images.","sentences":["Our objective is to discover and localize monotonic temporal changes in a sequence of images.","To achieve this, we exploit a simple proxy task of ordering a shuffled image sequence, with `time' serving as a supervisory signal since only changes that are monotonic with time can give rise to the correct ordering.","We also introduce a flexible transformer-based model for general-purpose ordering of image sequences of arbitrary length with built-in attribution maps.","After training, the model successfully discovers and localizes monotonic changes while ignoring cyclic and stochastic ones.","We demonstrate applications of the model in multiple video settings covering different scene and object types, discovering both object-level and environmental changes in unseen sequences.","We also demonstrate that the attention-based attribution maps function as effective prompts for segmenting the changing regions, and that the learned representations can be used for downstream applications.","Finally, we show that the model achieves the state of the art on standard benchmarks for ordering a set of images."],"url":"http://arxiv.org/abs/2404.16828v1","category":"cs.CV"}
{"created":"2024-04-25 17:59:47","title":"Successive Convexification for Trajectory Optimization with Continuous-Time Constraint Satisfaction","abstract":"We present successive convexification, a real-time-capable solution method for nonconvex trajectory optimization, with continuous-time constraint satisfaction and guaranteed convergence, that only requires first-order information. The proposed framework combines several key methods to solve a large class of nonlinear optimal control problems: (i) exterior penalty-based reformulation of the path constraints; (ii) generalized time-dilation; (iii) multiple-shooting discretization; (iv) $\\ell_1$ exact penalization of the nonconvex constraints; and (v) the prox-linear method, a sequential convex programming (SCP) algorithm for convex-composite minimization. The reformulation of the path constraints enables continuous-time constraint satisfaction even on sparse discretization grids and obviates the need for mesh refinement heuristics. Through the prox-linear method, we guarantee convergence of the solution method to stationary points of the penalized problem and guarantee that the converged solutions that are feasible with respect to the discretized and control-parameterized optimal control problem are also Karush-Kuhn-Tucker (KKT) points. Furthermore, we highlight the specialization of this property to global minimizers of convex optimal control problems, wherein the reformulated path constraints cannot be represented by canonical cones, i.e., in the form required by existing convex optimization solvers. In addition to theoretical analysis, we demonstrate the effectiveness and real-time capability of the proposed framework with numerical examples based on popular optimal control applications: dynamic obstacle avoidance and rocket landing.","sentences":["We present successive convexification, a real-time-capable solution method for nonconvex trajectory optimization, with continuous-time constraint satisfaction and guaranteed convergence, that only requires first-order information.","The proposed framework combines several key methods to solve a large class of nonlinear optimal control problems: (i) exterior penalty-based reformulation of the path constraints; (ii) generalized time-dilation; (iii) multiple-shooting discretization; (iv) $\\ell_1$ exact penalization of the nonconvex constraints; and (v) the prox-linear method, a sequential convex programming (SCP) algorithm for convex-composite minimization.","The reformulation of the path constraints enables continuous-time constraint satisfaction even on sparse discretization grids and obviates the need for mesh refinement heuristics.","Through the prox-linear method, we guarantee convergence of the solution method to stationary points of the penalized problem and guarantee that the converged solutions that are feasible with respect to the discretized and control-parameterized optimal control problem are also Karush-Kuhn-Tucker (KKT) points.","Furthermore, we highlight the specialization of this property to global minimizers of convex optimal control problems, wherein the reformulated path constraints cannot be represented by canonical cones, i.e., in the form required by existing convex optimization solvers.","In addition to theoretical analysis, we demonstrate the effectiveness and real-time capability of the proposed framework with numerical examples based on popular optimal control applications: dynamic obstacle avoidance and rocket landing."],"url":"http://arxiv.org/abs/2404.16826v1","category":"math.OC"}
{"created":"2024-04-25 17:59:45","title":"V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation Localization and Copyright Protection","abstract":"AI-generated video has revolutionized short video production, filmmaking, and personalized media, making video local editing an essential tool. However, this progress also blurs the line between reality and fiction, posing challenges in multimedia forensics. To solve this urgent issue, V2A-Mark is proposed to address the limitations of current video tampering forensics, such as poor generalizability, singular function, and single modality focus. Combining the fragility of video-into-video steganography with deep robust watermarking, our method can embed invisible visual-audio localization watermarks and copyright watermarks into the original video frames and audio, enabling precise manipulation localization and copyright protection. We also design a temporal alignment and fusion module and degradation prompt learning to enhance the localization accuracy and decoding robustness. Meanwhile, we introduce a sample-level audio localization method and a cross-modal copyright extraction mechanism to couple the information of audio and video frames. The effectiveness of V2A-Mark has been verified on a visual-audio tampering dataset, emphasizing its superiority in localization precision and copyright accuracy, crucial for the sustainable development of video editing in the AIGC video era.","sentences":["AI-generated video has revolutionized short video production, filmmaking, and personalized media, making video local editing an essential tool.","However, this progress also blurs the line between reality and fiction, posing challenges in multimedia forensics.","To solve this urgent issue, V2A-Mark is proposed to address the limitations of current video tampering forensics, such as poor generalizability, singular function, and single modality focus.","Combining the fragility of video-into-video steganography with deep robust watermarking, our method can embed invisible visual-audio localization watermarks and copyright watermarks into the original video frames and audio, enabling precise manipulation localization and copyright protection.","We also design a temporal alignment and fusion module and degradation prompt learning to enhance the localization accuracy and decoding robustness.","Meanwhile, we introduce a sample-level audio localization method and a cross-modal copyright extraction mechanism to couple the information of audio and video frames.","The effectiveness of V2A-Mark has been verified on a visual-audio tampering dataset, emphasizing its superiority in localization precision and copyright accuracy, crucial for the sustainable development of video editing in the AIGC video era."],"url":"http://arxiv.org/abs/2404.16824v1","category":"cs.CV"}
{"created":"2024-04-25 17:59:41","title":"Learning Visuotactile Skills with Two Multifingered Hands","abstract":"Aiming to replicate human-like dexterity, perceptual experiences, and motion patterns, we explore learning from human demonstrations using a bimanual system with multifingered hands and visuotactile data. Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing. To tackle the first challenge, we develop HATO, a low-cost hands-arms teleoperation system that leverages off-the-shelf electronics, complemented with a software suite that enables efficient data collection; the comprehensive software suite also supports multimodal data processing, scalable policy learning, and smooth policy deployment. To tackle the latter challenge, we introduce a novel hardware adaptation by repurposing two prosthetic hands equipped with touch sensors for research. Using visuotactile data collected from our system, we learn skills to complete long-horizon, high-precision tasks which are difficult to achieve without multifingered dexterity and touch feedback. Furthermore, we empirically investigate the effects of dataset size, sensing modality, and visual input preprocessing on policy learning. Our results mark a promising step forward in bimanual multifingered manipulation from visuotactile data. Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .","sentences":["Aiming to replicate human-like dexterity, perceptual experiences, and motion patterns, we explore learning from human demonstrations using a bimanual system with multifingered hands and visuotactile data.","Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing.","To tackle the first challenge, we develop HATO, a low-cost hands-arms teleoperation system that leverages off-the-shelf electronics, complemented with a software suite that enables efficient data collection; the comprehensive software suite also supports multimodal data processing, scalable policy learning, and smooth policy deployment.","To tackle the latter challenge, we introduce a novel hardware adaptation by repurposing two prosthetic hands equipped with touch sensors for research.","Using visuotactile data collected from our system, we learn skills to complete long-horizon, high-precision tasks which are difficult to achieve without multifingered dexterity and touch feedback.","Furthermore, we empirically investigate the effects of dataset size, sensing modality, and visual input preprocessing on policy learning.","Our results mark a promising step forward in bimanual multifingered manipulation from visuotactile data.","Videos, code, and datasets can be found at https://toruowo.github.io/hato/ ."],"url":"http://arxiv.org/abs/2404.16823v1","category":"cs.RO"}
{"created":"2024-04-25 17:59:40","title":"Cosmological probes of Dark Radiation from Neutrino Mixing","abstract":"Models of stepped dark radiation have recently been found to have an important impact on the anisotropies of the cosmic microwave background, aiding in easing the Hubble tension. In this work, we study models with a sector of dark radiation with a step in its abundance, which thermalizes after big bang nucleosynthesis by mixing with the standard model neutrinos. For this, we extend an earlier work which has focused on the background evolution only until the dark sector thermalizes by deriving the full background and perturbation equations of the model and implementing them in an Einstein-Boltzmann solving code. We expound on the behavior of this model, discussing the wide range of parameters that result in interesting and viable cosmologies that dynamically generate dark radiation during a range of epochs. We find that for the strongly self-coupled regime, there is no large cosmological impact for a tight prior on the mass, whereas larger mass ranges allow a smooth interpolation between a behavior close to the $\\Lambda$CDM cosmological standard model and close to an additional component of strongly self-interacting dark radiation. In the weakly self-coupled regime we find that we can accommodate a parameter space relevant for the neutrino anomalies as well as one relevant to easing the Hubble tension.","sentences":["Models of stepped dark radiation have recently been found to have an important impact on the anisotropies of the cosmic microwave background, aiding in easing the Hubble tension.","In this work, we study models with a sector of dark radiation with a step in its abundance, which thermalizes after big bang nucleosynthesis by mixing with the standard model neutrinos.","For this, we extend an earlier work which has focused on the background evolution only until the dark sector thermalizes by deriving the full background and perturbation equations of the model and implementing them in an Einstein-Boltzmann solving code.","We expound on the behavior of this model, discussing the wide range of parameters that result in interesting and viable cosmologies that dynamically generate dark radiation during a range of epochs.","We find that for the strongly self-coupled regime, there is no large cosmological impact for a tight prior on the mass, whereas larger mass ranges allow a smooth interpolation between a behavior close to the $\\Lambda$CDM cosmological standard model and close to an additional component of strongly self-interacting dark radiation.","In the weakly self-coupled regime we find that we can accommodate a parameter space relevant for the neutrino anomalies as well as one relevant to easing the Hubble tension."],"url":"http://arxiv.org/abs/2404.16822v1","category":"astro-ph.CO"}
{"created":"2024-04-25 17:58:43","title":"Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and Human Ratings","abstract":"While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. While previous work has evaluated T2I alignment by proposing metrics, benchmarks, and templates for collecting human judgements, the quality of these components is not systematically measured. Human-rated prompt sets are generally small and the reliability of the ratings -- and thereby the prompt set used to compare models -- is not evaluated. We address this gap by performing an extensive study evaluating auto-eval metrics and human templates. We provide three main contributions: (1) We introduce a comprehensive skills-based benchmark that can discriminate models across different human templates. This skills-based benchmark categorises prompts into sub-skills, allowing a practitioner to pinpoint not only which skills are challenging, but at what level of complexity a skill becomes challenging. (2) We gather human ratings across four templates and four T2I models for a total of >100K annotations. This allows us to understand where differences arise due to inherent ambiguity in the prompt and where they arise due to differences in metric and model quality. (3) Finally, we introduce a new QA-based auto-eval metric that is better correlated with human ratings than existing metrics for our new dataset, across different human templates, and on TIFA160.","sentences":["While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt.","While previous work has evaluated T2I alignment by proposing metrics, benchmarks, and templates for collecting human judgements, the quality of these components is not systematically measured.","Human-rated prompt sets are generally small and the reliability of the ratings -- and thereby the prompt set used to compare models -- is not evaluated.","We address this gap by performing an extensive study evaluating auto-eval metrics and human templates.","We provide three main contributions: (1) We introduce a comprehensive skills-based benchmark that can discriminate models across different human templates.","This skills-based benchmark categorises prompts into sub-skills, allowing a practitioner to pinpoint not only which skills are challenging, but at what level of complexity a skill becomes challenging.","(2) We gather human ratings across four templates and four T2I models for a total of >100K annotations.","This allows us to understand where differences arise due to inherent ambiguity in the prompt and where they arise due to differences in metric and model quality.","(3) Finally, we introduce a new QA-based auto-eval metric that is better correlated with human ratings than existing metrics for our new dataset, across different human templates, and on TIFA160."],"url":"http://arxiv.org/abs/2404.16820v1","category":"cs.CV"}
{"created":"2024-04-25 17:57:58","title":"Modified scattering for the cubic Schr\u00f6dinger equation on Diophantine waveguides","abstract":"We consider the cubic Schr\\\"odinger equation posed on product spaces subject to a generic Diophantine condition. Our analysis shows that the small-amplitude solutions undergo modified scattering to an effective dynamics governed by some interactions that do not amplify the Sobolev norms. This is in sharp contrast with the infinite energy cascade scenario observed by Hani--Pausader--Tzvetkov--Visciglia in the absence of Diophantine conditions.","sentences":["We consider the cubic Schr\\\"odinger equation posed on product spaces subject to a generic Diophantine condition.","Our analysis shows that the small-amplitude solutions undergo modified scattering to an effective dynamics governed by some interactions that do not amplify the Sobolev norms.","This is in sharp contrast with the infinite energy cascade scenario observed by Hani--Pausader--Tzvetkov--Visciglia in the absence of Diophantine conditions."],"url":"http://arxiv.org/abs/2404.16817v1","category":"math.AP"}
{"created":"2024-04-25 17:57:36","title":"IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages","abstract":"As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench","sentences":["As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world.","India is a linguistically diverse country of 1.4 Billion people.","To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families.","IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering.","IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time.","We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings.","The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models.","IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench"],"url":"http://arxiv.org/abs/2404.16816v1","category":"cs.CL"}
{"created":"2024-04-25 17:56:34","title":"Atmospheric Retrievals of the Phase-resolved Spectra of Irradiated Brown Dwarfs WD-0137B and EPIC-2122B","abstract":"We present an atmospheric retrieval analysis of HST/WFC3/G141 spectroscopic phase curve observations of two brown dwarfs, WD-0137B and EPIC-2122B, in ultra-short period orbits around white dwarf hosts. These systems are analogous to hot and ultra-hot Jupiter systems, enabling a unique and high-precision comparison to exoplanet systems. We use the PETRA retrieval suite to test various analysis setups, including joint-phase retrievals, multiple temperature structures, and non-uniform abundances. We find that WD-0137B has a dayside that closely resembles that of other ultra-hot Jupiters with inverted temperature structures and H$^-$ opacity, but quickly transitions to a mostly non-inverted temperature structure on the nightside. Meanwhile, EPIC-2122B's atmosphere remains inverted at all constrained longitudes, with dominant H$^-$ opacity. Retrievals with multiple temperature profiles and non-uniform vertical abundances were generally not statistically justified for this dataset, but retrievals with dayside-dilution factors were found to be justified. Retrieving all phases simultaneously with a linear combination of a dayside and nightside atmosphere was found to be an adequate representation of the entire phase-curve once a longitudinal temperature gradient free parameter was included in the retrieval. Comparing to global circulation models, we attribute behavior in the 1D retrievals to the inclined viewing geometry of the systems, which results in always-visible irradiated and inverted portions of the atmosphere \"contaminating\" spectra measured from the nightside hemisphere. This study sheds light on the similarities between these irradiated brown dwarf systems and hot and ultra-hot Jupiters, but also their unique differences, including the influence of the inclined viewing geometry.","sentences":["We present an atmospheric retrieval analysis of HST/WFC3/G141 spectroscopic phase curve observations of two brown dwarfs, WD-0137B and EPIC-2122B, in ultra-short period orbits around white dwarf hosts.","These systems are analogous to hot and ultra-hot Jupiter systems, enabling a unique and high-precision comparison to exoplanet systems.","We use the PETRA retrieval suite to test various analysis setups, including joint-phase retrievals, multiple temperature structures, and non-uniform abundances.","We find that WD-0137B has a dayside that closely resembles that of other ultra-hot Jupiters with inverted temperature structures and H$^-$ opacity, but quickly transitions to a mostly non-inverted temperature structure on the nightside.","Meanwhile, EPIC-2122B's atmosphere remains inverted at all constrained longitudes, with dominant H$^-$ opacity.","Retrievals with multiple temperature profiles and non-uniform vertical abundances were generally not statistically justified for this dataset, but retrievals with dayside-dilution factors were found to be justified.","Retrieving all phases simultaneously with a linear combination of a dayside and nightside atmosphere was found to be an adequate representation of the entire phase-curve once a longitudinal temperature gradient free parameter was included in the retrieval.","Comparing to global circulation models, we attribute behavior in the 1D retrievals to the inclined viewing geometry of the systems, which results in always-visible irradiated and inverted portions of the atmosphere \"contaminating\" spectra measured from the nightside hemisphere.","This study sheds light on the similarities between these irradiated brown dwarf systems and hot and ultra-hot Jupiters, but also their unique differences, including the influence of the inclined viewing geometry."],"url":"http://arxiv.org/abs/2404.16813v1","category":"astro-ph.SR"}
{"created":"2024-04-25 17:55:14","title":"Make Your LLM Fully Utilize the Context","abstract":"While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU). Github Link: https://github.com/microsoft/FILM.","sentences":["While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge.","We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information.","Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle.","Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments.","Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle).","To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval).","The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window.","Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU).","Github Link: https://github.com/microsoft/FILM."],"url":"http://arxiv.org/abs/2404.16811v1","category":"cs.CL"}
{"created":"2024-04-25 17:52:39","title":"Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning","abstract":"Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity. Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators.","sentences":["Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences.","Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts.","Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning.","However, the diversity aspect in LLM outputs has not been systematically studied before.","To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality.","Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity.","Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators."],"url":"http://arxiv.org/abs/2404.16807v1","category":"cs.CL"}
{"created":"2024-04-25 17:51:10","title":"AAPL: Adding Attributes to Prompt Learning for Vision-Language Models","abstract":"Recent advances in large pre-trained vision-language models have demonstrated remarkable performance on zero-shot downstream tasks. Building upon this, recent studies, such as CoOp and CoCoOp, have proposed the use of prompt learning, where context within a prompt is replaced with learnable vectors, leading to significant improvements over manually crafted prompts. However, the performance improvement for unseen classes is still marginal, and to tackle this problem, data augmentation has been frequently used in traditional zero-shot learning techniques. Through our experiments, we have identified important issues in CoOp and CoCoOp: the context learned through traditional image augmentation is biased toward seen classes, negatively impacting generalization to unseen classes. To address this problem, we propose adversarial token embedding to disentangle low-level visual augmentation features from high-level class information when inducing bias in learnable prompts. Through our novel mechanism called \"Adding Attributes to Prompt Learning\", AAPL, we guide the learnable context to effectively extract text features by focusing on high-level features for unseen classes. We have conducted experiments across 11 datasets, and overall, AAPL shows favorable performances compared to the existing methods in few-shot learning, zero-shot learning, cross-dataset, and domain generalization tasks.","sentences":["Recent advances in large pre-trained vision-language models have demonstrated remarkable performance on zero-shot downstream tasks.","Building upon this, recent studies, such as CoOp and CoCoOp, have proposed the use of prompt learning, where context within a prompt is replaced with learnable vectors, leading to significant improvements over manually crafted prompts.","However, the performance improvement for unseen classes is still marginal, and to tackle this problem, data augmentation has been frequently used in traditional zero-shot learning techniques.","Through our experiments, we have identified important issues in CoOp and CoCoOp: the context learned through traditional image augmentation is biased toward seen classes, negatively impacting generalization to unseen classes.","To address this problem, we propose adversarial token embedding to disentangle low-level visual augmentation features from high-level class information when inducing bias in learnable prompts.","Through our novel mechanism called \"Adding Attributes to Prompt Learning\", AAPL, we guide the learnable context to effectively extract text features by focusing on high-level features for unseen classes.","We have conducted experiments across 11 datasets, and overall, AAPL shows favorable performances compared to the existing methods in few-shot learning, zero-shot learning, cross-dataset, and domain generalization tasks."],"url":"http://arxiv.org/abs/2404.16804v1","category":"cs.CV"}
{"created":"2024-04-25 17:46:50","title":"Complementary asymptotic analysis for a minimal random walk","abstract":"We discuss a complementary asymptotic analysis of the so called minimal random walk. More precisely, we present a version of the almost sure central limit theorem as well as a generalization of the recently proposed quadratic strong laws. In addition, alternative demonstrations of the functional limit theorems will be supplied based on a P\\'olya urn scheme instead of a martingale approach.","sentences":["We discuss a complementary asymptotic analysis of the so called minimal random walk.","More precisely, we present a version of the almost sure central limit theorem as well as a generalization of the recently proposed quadratic strong laws.","In addition, alternative demonstrations of the functional limit theorems will be supplied based on a P\\'olya urn scheme instead of a martingale approach."],"url":"http://arxiv.org/abs/2404.16800v1","category":"math.PR"}
{"created":"2024-04-25 17:46:36","title":"Model-free inference of memory in conformational dynamics of a multi-domain protein","abstract":"Single-molecule experiments provide insight into the motion (conformational dynamics) of individual protein molecules. Usually, a well-defined but coarse-grained intramolecular coordinate is measured and subsequently analysed with the help of Hidden Markov Models (HMMs) to deduce the kinetics of protein conformational changes. Such approaches rely on the assumption that the microscopic dynamics of the protein evolve according to a Markov-jump process on some network. However, the manifestation and extent of memory in the dynamics of the observable strongly depends on the chosen underlying Markov model, which is generally not known and therefore can lead to misinterpretations. Here, we combine extensive single-molecule plasmon ruler experiments on the heat shock protein Hsp90, computer simulations, and theory to infer and quantify memory in a model-free fashion. Our analysis is based on the bare definition of non-Markovian behaviour and does not require any underlying model. In the case of Hsp90 probed by a plasmon ruler, the Markov assumption is found to be clearly and conclusively violated on timescales up to roughly 50 s, which corresponds roughly to $\\sim$50% of the inferred correlation time of the signal. The extent of memory is striking and reaches biologically relevant timescales. This implies that memory effects penetrate even the slowest observed motions. We provide clear and reproducible guidelines on how to test for the presence and duration of memory in experimental single-molecule data.","sentences":["Single-molecule experiments provide insight into the motion (conformational dynamics) of individual protein molecules.","Usually, a well-defined but coarse-grained intramolecular coordinate is measured and subsequently analysed with the help of Hidden Markov Models (HMMs) to deduce the kinetics of protein conformational changes.","Such approaches rely on the assumption that the microscopic dynamics of the protein evolve according to a Markov-jump process on some network.","However, the manifestation and extent of memory in the dynamics of the observable strongly depends on the chosen underlying Markov model, which is generally not known and therefore can lead to misinterpretations.","Here, we combine extensive single-molecule plasmon ruler experiments on the heat shock protein Hsp90, computer simulations, and theory to infer and quantify memory in a model-free fashion.","Our analysis is based on the bare definition of non-Markovian behaviour and does not require any underlying model.","In the case of Hsp90 probed by a plasmon ruler, the Markov assumption is found to be clearly and conclusively violated on timescales up to roughly 50 s, which corresponds roughly to $\\sim$50% of the inferred correlation time of the signal.","The extent of memory is striking and reaches biologically relevant timescales.","This implies that memory effects penetrate even the slowest observed motions.","We provide clear and reproducible guidelines on how to test for the presence and duration of memory in experimental single-molecule data."],"url":"http://arxiv.org/abs/2404.16799v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 17:43:29","title":"Spherical bispectrum expansion and quadratic estimators","abstract":"We describe a general expansion of spherical (full-sky) bispectra into a set of orthogonal modes. For squeezed shapes, the basis separates physically-distinct signals and is dominated by the lowest moments. In terms of reduced bispectra, we identify a set of discrete polynomials that are pairwise orthogonal with respect to the relevant Wigner 3j symbol, and reduce to Chebyshev polynomials in the flat-sky (high-momentum) limit for both parity-even and parity-odd cases. For squeezed shapes, the flat-sky limit is equivalent to previous moment expansions used for CMB bispectra and quadratic estimators, but in general reduces to a distinct expansion in the angular dependence of triangles at fixed total side length (momentum). We use the full-sky expansion to construct a tower of orthogonal CMB lensing quadratic estimators and construct estimators that are immune to foregrounds like point sources or noise inhomogeneities. In parity-even combinations (such as the lensing gradient mode from $TT$, or the lensing curl mode from $EB$) the leading two modes can be identified with information from the magnification and shear respectively, whereas the parity-odd combinations are shear-only. Although not directly separable, we show that these estimators can nonetheless be evaluated numerically sufficiently easily.","sentences":["We describe a general expansion of spherical (full-sky) bispectra into a set of orthogonal modes.","For squeezed shapes, the basis separates physically-distinct signals and is dominated by the lowest moments.","In terms of reduced bispectra, we identify a set of discrete polynomials that are pairwise orthogonal with respect to the relevant Wigner 3j symbol, and reduce to Chebyshev polynomials in the flat-sky (high-momentum) limit for both parity-even and parity-odd cases.","For squeezed shapes, the flat-sky limit is equivalent to previous moment expansions used for CMB bispectra and quadratic estimators, but in general reduces to a distinct expansion in the angular dependence of triangles at fixed total side length (momentum).","We use the full-sky expansion to construct a tower of orthogonal CMB lensing quadratic estimators and construct estimators that are immune to foregrounds like point sources or noise inhomogeneities.","In parity-even combinations (such as the lensing gradient mode from $TT$, or the lensing curl mode from $EB$) the leading two modes can be identified with information from the magnification and shear respectively, whereas the parity-odd combinations are shear-only.","Although not directly separable, we show that these estimators can nonetheless be evaluated numerically sufficiently easily."],"url":"http://arxiv.org/abs/2404.16797v1","category":"astro-ph.CO"}
{"created":"2024-04-25 17:42:32","title":"SAGBI and Gr\u00f6bner Bases Detection","abstract":"We introduce a detection algorithm for SAGBI basis in polynomial rings, analogous to a Gr\\\"obner basis detection algorithm previously proposed by Gritzmann and Sturmfels. We also present two accompanying software packages named SagbiGbDetection for Macaulay2 and Julia. Both packages allow the user to find one or more term orders for which a set of input polynomials form either Gr\\\"obner basis for the ideal they generate or a SAGBI basis for the subalgebra. Additionally, we investigate the computational complexity of homogeneous SAGBI detection and apply our implementation to several novel examples.","sentences":["We introduce a detection algorithm for SAGBI basis in polynomial rings, analogous to a Gr\\\"obner basis detection algorithm previously proposed by Gritzmann and Sturmfels.","We also present two accompanying software packages named SagbiGbDetection for Macaulay2 and Julia.","Both packages allow the user to find one or more term orders for which a set of input polynomials form either Gr\\\"obner basis for the ideal they generate or a SAGBI basis for the subalgebra.","Additionally, we investigate the computational complexity of homogeneous SAGBI detection and apply our implementation to several novel examples."],"url":"http://arxiv.org/abs/2404.16796v1","category":"math.AC"}
{"created":"2024-04-25 17:39:50","title":"Extreme points of general transportation polytopes","abstract":"Transportation matrices are $m\\times n$ non-negative matrices whose row sums and row columns are equal to, or dominated above with given integral vectors $R$ and $C$. Those matrices belong to a convex polytope whose extreme points have been previously characterized. In this article, a more general set of non-negative transportation matrices is considered, whose row sums are bounded by two integral non-negative vectors $R_{min}$ and $R_{max}$ and column sums are bounded by two integral non-negative vectors $C_{min}$ and $C_{max}$. It is shown that this set is also a convex polytope whose extreme points are then fully characterized.","sentences":["Transportation matrices are $m\\times n$ non-negative matrices whose row sums and row columns are equal to, or dominated above with given integral vectors $R$ and $C$.","Those matrices belong to a convex polytope whose extreme points have been previously characterized.","In this article, a more general set of non-negative transportation matrices is considered, whose row sums are bounded by two integral non-negative vectors $R_{min}$ and $R_{max}$ and column sums are bounded by two integral non-negative vectors $C_{min}$ and $C_{max}$. It is shown that this set is also a convex polytope whose extreme points are then fully characterized."],"url":"http://arxiv.org/abs/2404.16791v1","category":"math.CO"}
{"created":"2024-04-25 17:39:50","title":"Weak-to-Strong Extrapolation Expedites Alignment","abstract":"Although the capabilities of large language models (LLMs) ideally scale up with increasing data and compute, they are inevitably constrained by limited resources in reality. Suppose we have a moderately trained LLM (e.g., trained to align with human preference) in hand, can we further exploit its potential and cheaply acquire a stronger model? In this paper, we propose a simple method called ExPO to boost LLMs' alignment with human preference. ExPO assumes that a medium-aligned model can be interpolated between a less-aligned (weaker) model, e.g., the initial SFT model, and a better-aligned (stronger) one, thereby directly obtaining this stronger model by extrapolating from the weights of the former two relatively weaker models. On the AlpacaEval 2.0 benchmark, we show that ExPO pushes models trained with less preference data (e.g., 10% or 20%) to reach and even surpass the fully-trained one, without any additional training. Furthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and exhibits decent scalability across model sizes from 7B to 70B. Our work demonstrates the efficacy of model extrapolation in exploiting LLMs' capabilities, suggesting a promising direction that deserves future exploration.","sentences":["Although the capabilities of large language models (LLMs) ideally scale up with increasing data and compute, they are inevitably constrained by limited resources in reality.","Suppose we have a moderately trained LLM (e.g., trained to align with human preference) in hand, can we further exploit its potential and cheaply acquire a stronger model?","In this paper, we propose a simple method called ExPO to boost LLMs' alignment with human preference.","ExPO assumes that a medium-aligned model can be interpolated between a less-aligned (weaker) model, e.g., the initial SFT model, and a better-aligned (stronger) one, thereby directly obtaining this stronger model by extrapolating from the weights of the former two relatively weaker models.","On the AlpacaEval 2.0 benchmark, we show that ExPO pushes models trained with less preference data (e.g., 10% or 20%) to reach and even surpass the fully-trained one, without any additional training.","Furthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and exhibits decent scalability across model sizes from 7B to 70B.","Our work demonstrates the efficacy of model extrapolation in exploiting LLMs' capabilities, suggesting a promising direction that deserves future exploration."],"url":"http://arxiv.org/abs/2404.16792v1","category":"cs.LG"}
{"created":"2024-04-25 17:39:35","title":"SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension","abstract":"Comprehending text-rich visual content is paramount for the practical application of Multimodal Large Language Models (MLLMs), since text-rich scenarios are ubiquitous in the real world, which are characterized by the presence of extensive texts embedded within images. Recently, the advent of MLLMs with impressive versatility has raised the bar for what we can expect from MLLMs. However, their proficiency in text-rich scenarios has yet to be comprehensively and objectively assessed, since current MLLM benchmarks primarily focus on evaluating general visual comprehension. In this work, we introduce SEED-Bench-2-Plus, a benchmark specifically designed for evaluating \\textbf{text-rich visual comprehension} of MLLMs. Our benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs, each of which covers a wide spectrum of text-rich scenarios in the real world. These categories, due to their inherent complexity and diversity, effectively simulate real-world text-rich environments. We further conduct a thorough evaluation involving 34 prominent MLLMs (including GPT-4V, Gemini-Pro-Vision and Claude-3-Opus) and emphasize the current limitations of MLLMs in text-rich visual comprehension. We hope that our work can serve as a valuable addition to existing MLLM benchmarks, providing insightful observations and inspiring further research in the area of text-rich visual comprehension with MLLMs. The dataset and evaluation code can be accessed at https://github.com/AILab-CVC/SEED-Bench.","sentences":["Comprehending text-rich visual content is paramount for the practical application of Multimodal Large Language Models (MLLMs), since text-rich scenarios are ubiquitous in the real world, which are characterized by the presence of extensive texts embedded within images.","Recently, the advent of MLLMs with impressive versatility has raised the bar for what we can expect from MLLMs.","However, their proficiency in text-rich scenarios has yet to be comprehensively and objectively assessed, since current MLLM benchmarks primarily focus on evaluating general visual comprehension.","In this work, we introduce SEED-Bench-2-Plus, a benchmark specifically designed for evaluating \\textbf{text-rich visual comprehension} of MLLMs.","Our benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs, each of which covers a wide spectrum of text-rich scenarios in the real world.","These categories, due to their inherent complexity and diversity, effectively simulate real-world text-rich environments.","We further conduct a thorough evaluation involving 34 prominent MLLMs (including GPT-4V, Gemini-Pro-Vision and Claude-3-Opus) and emphasize the current limitations of MLLMs in text-rich visual comprehension.","We hope that our work can serve as a valuable addition to existing MLLM benchmarks, providing insightful observations and inspiring further research in the area of text-rich visual comprehension with MLLMs.","The dataset and evaluation code can be accessed at https://github.com/AILab-CVC/SEED-Bench."],"url":"http://arxiv.org/abs/2404.16790v1","category":"cs.CV"}
{"created":"2024-04-25 17:38:57","title":"Continual Learning of Large Language Models: A Comprehensive Survey","abstract":"The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as \"catastrophic forgetting\". While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs. In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5). Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.","sentences":["The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications.","One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences.","Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as \"catastrophic forgetting\".","While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs.","In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL.","This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3).","We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4).","Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5).","Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6).","The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey."],"url":"http://arxiv.org/abs/2404.16789v1","category":"cs.LG"}
{"created":"2024-04-25 17:32:55","title":"Harnessing Inferior Solutions For Superior Outcomes: Obtaining Robust Solutions From Quantum Algorithms","abstract":"In the rapidly advancing domain of quantum optimization, the confluence of quantum algorithms such as Quantum Annealing (QA) and the Quantum Approximate Optimization Algorithm (QAOA) with robust optimization methodologies presents a cutting-edge frontier. Although it seems natural to apply quantum algorithms when facing uncertainty, this has barely been approached.   In this paper we adapt the aforementioned quantum optimization techniques to tackle robust optimization problems. By leveraging the inherent stochasticity of quantum annealing and adjusting the parameters and evaluation functions within QAOA, we present two innovative methods for obtaining robust optimal solutions. These heuristics are applied on two use cases within the energy sector: the unit commitment problem, which is central to the scheduling of power plant operations, and the optimization of charging electric vehicles (EVs) including electricity from photovoltaic (PV) to minimize costs. These examples highlight not only the potential of quantum optimization methods to enhance decision-making in energy management but also the practical relevance of the young field of quantum computing in general. Through careful adaptation of quantum algorithms, we lay the foundation for exploring ways to achieve more reliable and efficient solutions in complex optimization scenarios that occur in the real-world.","sentences":["In the rapidly advancing domain of quantum optimization, the confluence of quantum algorithms such as Quantum Annealing (QA) and the Quantum Approximate Optimization Algorithm (QAOA) with robust optimization methodologies presents a cutting-edge frontier.","Although it seems natural to apply quantum algorithms when facing uncertainty, this has barely been approached.   ","In this paper we adapt the aforementioned quantum optimization techniques to tackle robust optimization problems.","By leveraging the inherent stochasticity of quantum annealing and adjusting the parameters and evaluation functions within QAOA, we present two innovative methods for obtaining robust optimal solutions.","These heuristics are applied on two use cases within the energy sector: the unit commitment problem, which is central to the scheduling of power plant operations, and the optimization of charging electric vehicles (EVs) including electricity from photovoltaic (PV) to minimize costs.","These examples highlight not only the potential of quantum optimization methods to enhance decision-making in energy management but also the practical relevance of the young field of quantum computing in general.","Through careful adaptation of quantum algorithms, we lay the foundation for exploring ways to achieve more reliable and efficient solutions in complex optimization scenarios that occur in the real-world."],"url":"http://arxiv.org/abs/2404.16784v1","category":"quant-ph"}
{"created":"2024-04-25 17:31:31","title":"Dual-isometric Projected Entangled Pair States","abstract":"Efficient characterization of higher dimensional many-body physical states presents significant challenges. In this paper, we propose a new class of Project Entangled Pair State (PEPS) that incorporates two isometric conditions. This new class facilitates the efficient calculation of general local observables and certain two-point correlation functions, which have been previously shown to be intractable for general PEPS, or PEPS with only a single isometric constraint. Despite incorporating two isometric conditions, our class preserves the rich physical structure while enhancing the analytical capabilities. It features a large set of tunable parameters, with only a subleading correction compared to that of general PEPS. Furthermore, we analytically demonstrate that this class can encode universal quantum computations and can represent a transition from topological to trivial order.","sentences":["Efficient characterization of higher dimensional many-body physical states presents significant challenges.","In this paper, we propose a new class of Project Entangled Pair State (PEPS) that incorporates two isometric conditions.","This new class facilitates the efficient calculation of general local observables and certain two-point correlation functions, which have been previously shown to be intractable for general PEPS, or PEPS with only a single isometric constraint.","Despite incorporating two isometric conditions, our class preserves the rich physical structure while enhancing the analytical capabilities.","It features a large set of tunable parameters, with only a subleading correction compared to that of general PEPS.","Furthermore, we analytically demonstrate that this class can encode universal quantum computations and can represent a transition from topological to trivial order."],"url":"http://arxiv.org/abs/2404.16783v1","category":"quant-ph"}
{"created":"2024-04-25 17:30:59","title":"Gromov-Witten Invariants and Mirror Symmetry for Non-Fano Varieties Using Scattering Diagrams","abstract":"Gromov-Witten invariants arise in the topological A-model as counts of worldsheet instantons. On the A-side, these invariants can be computed for a Fano or semi-Fano toric variety using generating functions associated to the toric divisors. On the B-side, the same invariants can be computed from the periods of the mirror. We utilize scattering diagrams (aka wall structures) in the Gross-Siebert mirror symmetry program to extend the calculation of Gromov-Witten invariants to non-Fano toric varieties. Following the work of Carl-Pumperla-Siebert, we compute corrected mirror superpotentials $\\vartheta_1(\\mathbb{F}_m)$ and their periods for the Hirzebruch surfaces $\\mathbb{F}_m$ with $m \\ge 2$.","sentences":["Gromov-Witten invariants arise in the topological A-model as counts of worldsheet instantons.","On the A-side, these invariants can be computed for a Fano or semi-Fano toric variety using generating functions associated to the toric divisors.","On the B-side, the same invariants can be computed from the periods of the mirror.","We utilize scattering diagrams (aka wall structures) in the Gross-Siebert mirror symmetry program to extend the calculation of Gromov-Witten invariants to non-Fano toric varieties.","Following the work of Carl-Pumperla-Siebert, we compute corrected mirror superpotentials $\\vartheta_1(\\mathbb{F}_m)$ and their periods for the Hirzebruch surfaces $\\mathbb{F}_m$ with $m \\ge 2$."],"url":"http://arxiv.org/abs/2404.16782v1","category":"math.AG"}
{"created":"2024-04-25 17:30:37","title":"Rapid thermalization of dissipative many-body dynamics of commuting Hamiltonians","abstract":"Quantum systems typically reach thermal equilibrium rather quickly when coupled to a thermal environment. The usual way of bounding the speed of this process is by estimating the spectral gap of the dissipative generator. However the gap, by itself, does not always yield a reasonable estimate for the thermalization time in many-body systems: without further structure, a uniform lower bound on it only constrains the thermalization time to grow polynomially with system size.   Here, instead, we show that for a large class of geometrically-2-local models of Davies generators with commuting Hamiltonians, the thermalization time is much shorter than one would na\\\"ively estimate from the gap: at most logarithmic in the system size. This yields the so-called rapid mixing of dissipative dynamics. The result is particularly relevant for 1D systems, for which we prove rapid thermalization with a system size independent decay rate only from a positive gap in the generator. We also prove that systems in hypercubic lattices of any dimension, and exponential graphs, such as trees, have rapid mixing at high enough temperatures. We do this by introducing a novel notion of clustering which we call \"strong local indistinguishability\" based on a max-relative entropy, and then proving that it implies a lower bound on the modified logarithmic Sobolev inequality (MLSI) for nearest neighbour commuting models.   This has consequences for the rate of thermalization towards Gibbs states, and also for their relevant Wasserstein distances and transportation cost inequalities. Along the way, we show that several measures of decay of correlations on Gibbs states of commuting Hamiltonians are equivalent, a result of independent interest. At the technical level, we also show a direct relation between properties of Davies and Schmidt dynamics, that allows to transfer results of thermalization between both.","sentences":["Quantum systems typically reach thermal equilibrium rather quickly when coupled to a thermal environment.","The usual way of bounding the speed of this process is by estimating the spectral gap of the dissipative generator.","However the gap, by itself, does not always yield a reasonable estimate for the thermalization time in many-body systems: without further structure, a uniform lower bound on it only constrains the thermalization time to grow polynomially with system size.   ","Here, instead, we show that for a large class of geometrically-2-local models of Davies generators with commuting Hamiltonians, the thermalization time is much shorter than one would na\\\"ively estimate from the gap: at most logarithmic in the system size.","This yields the so-called rapid mixing of dissipative dynamics.","The result is particularly relevant for 1D systems, for which we prove rapid thermalization with a system size independent decay rate only from a positive gap in the generator.","We also prove that systems in hypercubic lattices of any dimension, and exponential graphs, such as trees, have rapid mixing at high enough temperatures.","We do this by introducing a novel notion of clustering which we call \"strong local indistinguishability\" based on a max-relative entropy, and then proving that it implies a lower bound on the modified logarithmic Sobolev inequality (MLSI) for nearest neighbour commuting models.   ","This has consequences for the rate of thermalization towards Gibbs states, and also for their relevant Wasserstein distances and transportation cost inequalities.","Along the way, we show that several measures of decay of correlations on Gibbs states of commuting Hamiltonians are equivalent, a result of independent interest.","At the technical level, we also show a direct relation between properties of Davies and Schmidt dynamics, that allows to transfer results of thermalization between both."],"url":"http://arxiv.org/abs/2404.16780v1","category":"quant-ph"}
{"created":"2024-04-25 17:28:33","title":"DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks","abstract":"The success of many RL techniques heavily relies on human-engineered dense rewards, which typically demand substantial domain expertise and extensive trial and error. In our work, we propose DrS (Dense reward learning from Stages), a novel approach for learning reusable dense rewards for multi-stage tasks in a data-driven manner. By leveraging the stage structures of the task, DrS learns a high-quality dense reward from sparse rewards and demonstrations if given. The learned rewards can be \\textit{reused} in unseen tasks, thus reducing the human effort for reward engineering. Extensive experiments on three physical robot manipulation task families with 1000+ task variants demonstrate that our learned rewards can be reused in unseen tasks, resulting in improved performance and sample efficiency of RL algorithms. The learned rewards even achieve comparable performance to human-engineered rewards on some tasks. See our project page (https://sites.google.com/view/iclr24drs) for more details.","sentences":["The success of many RL techniques heavily relies on human-engineered dense rewards, which typically demand substantial domain expertise and extensive trial and error.","In our work, we propose DrS (Dense reward learning from Stages), a novel approach for learning reusable dense rewards for multi-stage tasks in a data-driven manner.","By leveraging the stage structures of the task, DrS learns a high-quality dense reward from sparse rewards and demonstrations if given.","The learned rewards can be \\textit{reused} in unseen tasks, thus reducing the human effort for reward engineering.","Extensive experiments on three physical robot manipulation task families with 1000+ task variants demonstrate that our learned rewards can be reused in unseen tasks, resulting in improved performance and sample efficiency of RL algorithms.","The learned rewards even achieve comparable performance to human-engineered rewards on some tasks.","See our project page (https://sites.google.com/view/iclr24drs) for more details."],"url":"http://arxiv.org/abs/2404.16779v1","category":"cs.LG"}
{"created":"2024-04-25 17:27:39","title":"Unifying Asynchronous Logics for Hyperproperties","abstract":"We introduce and investigate a powerful hyper logical framework in the linear-time setting, we call generalized HyperLTL with stuttering and contexts (GHyperLTL_SC for short). GHyperLTL_SC unifies known asynchronous extensions of HyperLTL and the well-known extension KLTL of LTL with knowledge modalities under both the synchronous and asynchronous perfect recall semantics. As a main contribution, we individuate a meaningful fragment of GHyperLTL_SC, we call simple GHyperLTL_SC, with a decidable model-checking problem, which is more expressive than HyperLTL and known fragments of asynchronous extensions of HyperLTL with a decidable model-checking problem. Simple GHyperLTL_SC subsumes KLTL under the synchronous semantics and the one-agent fragment of KLTL under the asynchronous semantics, and to the best of our knowledge, it represents the unique hyper logic with a decidable model-checking problem which can express powerful non-regular trace properties when interpreted on singleton sets of traces. We justify the relevance of simple GHyperLTL_SC by showing that it can express diagnosability properties, interesting classes of information-flow security policies, both in the synchronous and asynchronous settings, and bounded termination (more in general, global promptness in the style of Prompt LTL).","sentences":["We introduce and investigate a powerful hyper logical framework in the linear-time setting, we call generalized HyperLTL with stuttering and contexts (GHyperLTL_SC for short).","GHyperLTL_SC unifies known asynchronous extensions of HyperLTL and the well-known extension KLTL of LTL with knowledge modalities under both the synchronous and asynchronous perfect recall semantics.","As a main contribution, we individuate a meaningful fragment of GHyperLTL_SC, we call simple GHyperLTL_SC, with a decidable model-checking problem, which is more expressive than HyperLTL and known fragments of asynchronous extensions of HyperLTL with a decidable model-checking problem.","Simple GHyperLTL_SC subsumes KLTL under the synchronous semantics and the one-agent fragment of KLTL under the asynchronous semantics, and to the best of our knowledge, it represents the unique hyper logic with a decidable model-checking problem which can express powerful non-regular trace properties when interpreted on singleton sets of traces.","We justify the relevance of simple GHyperLTL_SC by showing that it can express diagnosability properties, interesting classes of information-flow security policies, both in the synchronous and asynchronous settings, and bounded termination (more in general, global promptness in the style of Prompt LTL)."],"url":"http://arxiv.org/abs/2404.16778v1","category":"cs.LO"}
{"created":"2024-04-25 17:26:24","title":"Imaginary Stark Skin Effect","abstract":"The non-Hermitian skin effect (NHSE) is a unique phenomenon in non-Hermitian systems. However, studies on NHSE in systems without translational symmetry remain largely unexplored. Here, we unveil a new class of NHSE, dubbed \"imaginary Stark skin effect\" (ISSE), in a one-dimensional lossy lattice with a spatially increasing loss rate. The energy spectrum of this model exhibits a T-shaped feature, with approximately half of the eigenstates localized at the left boundary. These skin modes exhibit peculiar behaviors, expressed as a single stable exponential decay wave within the bulk region. We use the transfer matrix method to analyze the formation of the ISSE in this model. According to the eigen-decomposition of the transfer matrix, the wave function is divided into two parts, one of which dominates the behavior of the skin modes in the bulk. Our findings provide insights into the NHSE in systems without translational symmetry and contribute to the understanding of non-Hermitian systems in general.","sentences":["The non-Hermitian skin effect (NHSE) is a unique phenomenon in non-Hermitian systems.","However, studies on NHSE in systems without translational symmetry remain largely unexplored.","Here, we unveil a new class of NHSE, dubbed \"imaginary Stark skin effect\" (ISSE), in a one-dimensional lossy lattice with a spatially increasing loss rate.","The energy spectrum of this model exhibits a T-shaped feature, with approximately half of the eigenstates localized at the left boundary.","These skin modes exhibit peculiar behaviors, expressed as a single stable exponential decay wave within the bulk region.","We use the transfer matrix method to analyze the formation of the ISSE in this model.","According to the eigen-decomposition of the transfer matrix, the wave function is divided into two parts, one of which dominates the behavior of the skin modes in the bulk.","Our findings provide insights into the NHSE in systems without translational symmetry and contribute to the understanding of non-Hermitian systems in general."],"url":"http://arxiv.org/abs/2404.16774v1","category":"quant-ph"}
{"created":"2024-04-25 17:23:43","title":"ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving","abstract":"Diffusion-based technologies have made significant strides, particularly in personalized and customized facialgeneration. However, existing methods face challenges in achieving high-fidelity and detailed identity (ID)consistency, primarily due to insufficient fine-grained control over facial areas and the lack of a comprehensive strategy for ID preservation by fully considering intricate facial details and the overall face. To address these limitations, we introduce ConsistentID, an innovative method crafted for diverseidentity-preserving portrait generation under fine-grained multimodal facial prompts, utilizing only a single reference image. ConsistentID comprises two key components: a multimodal facial prompt generator that combines facial features, corresponding facial descriptions and the overall facial context to enhance precision in facial details, and an ID-preservation network optimized through the facial attention localization strategy, aimed at preserving ID consistency in facial regions. Together, these components significantly enhance the accuracy of ID preservation by introducing fine-grained multimodal ID information from facial regions. To facilitate training of ConsistentID, we present a fine-grained portrait dataset, FGID, with over 500,000 facial images, offering greater diversity and comprehensiveness than existing public facial datasets. % such as LAION-Face, CelebA, FFHQ, and SFHQ. Experimental results substantiate that our ConsistentID achieves exceptional precision and diversity in personalized facial generation, surpassing existing methods in the MyStyle dataset. Furthermore, while ConsistentID introduces more multimodal ID information, it maintains a fast inference speed during generation.","sentences":["Diffusion-based technologies have made significant strides, particularly in personalized and customized facialgeneration.","However, existing methods face challenges in achieving high-fidelity and detailed identity (ID)consistency, primarily due to insufficient fine-grained control over facial areas and the lack of a comprehensive strategy for ID preservation by fully considering intricate facial details and the overall face.","To address these limitations, we introduce ConsistentID, an innovative method crafted for diverseidentity-preserving portrait generation under fine-grained multimodal facial prompts, utilizing only a single reference image.","ConsistentID comprises two key components: a multimodal facial prompt generator that combines facial features, corresponding facial descriptions and the overall facial context to enhance precision in facial details, and an ID-preservation network optimized through the facial attention localization strategy, aimed at preserving ID consistency in facial regions.","Together, these components significantly enhance the accuracy of ID preservation by introducing fine-grained multimodal ID information from facial regions.","To facilitate training of ConsistentID, we present a fine-grained portrait dataset, FGID, with over 500,000 facial images, offering greater diversity and comprehensiveness than existing public facial datasets.","% such as LAION-Face, CelebA, FFHQ, and SFHQ.","Experimental results substantiate that our ConsistentID achieves exceptional precision and diversity in personalized facial generation, surpassing existing methods in the MyStyle dataset.","Furthermore, while ConsistentID introduces more multimodal ID information, it maintains a fast inference speed during generation."],"url":"http://arxiv.org/abs/2404.16771v1","category":"cs.CV"}
{"created":"2024-04-25 17:22:43","title":"Redefining Safety for Autonomous Vehicles","abstract":"Existing definitions and associated conceptual frameworks for computer-based system safety should be revisited in light of real-world experiences from deploying autonomous vehicles. Current terminology used by industry safety standards emphasizes mitigation of risk from specifically identified hazards, and carries assumptions based on human-supervised vehicle operation. Operation without a human driver dramatically increases the scope of safety concerns, especially due to operation in an open world environment, a requirement to self-enforce operational limits, participation in an ad hoc sociotechnical system of systems, and a requirement to conform to both legal and ethical constraints. Existing standards and terminology only partially address these new challenges. We propose updated definitions for core system safety concepts that encompass these additional considerations as a starting point for evolving safe-ty approaches to address these additional safety challenges. These results might additionally inform framing safety terminology for other autonomous system applications.","sentences":["Existing definitions and associated conceptual frameworks for computer-based system safety should be revisited in light of real-world experiences from deploying autonomous vehicles.","Current terminology used by industry safety standards emphasizes mitigation of risk from specifically identified hazards, and carries assumptions based on human-supervised vehicle operation.","Operation without a human driver dramatically increases the scope of safety concerns, especially due to operation in an open world environment, a requirement to self-enforce operational limits, participation in an ad hoc sociotechnical system of systems, and a requirement to conform to both legal and ethical constraints.","Existing standards and terminology only partially address these new challenges.","We propose updated definitions for core system safety concepts that encompass these additional considerations as a starting point for evolving safe-ty approaches to address these additional safety challenges.","These results might additionally inform framing safety terminology for other autonomous system applications."],"url":"http://arxiv.org/abs/2404.16768v1","category":"cs.RO"}
{"created":"2024-04-25 17:20:45","title":"REBEL: Reinforcement Learning via Regressing Relative Rewards","abstract":"While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping) and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative rewards via a direct policy parameterization between two completions to a prompt, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally tractable than PPO.","sentences":["While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications including the fine-tuning of generative models.","Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping) and is notorious for its sensitivity to the precise implementation of these components.","In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like.","We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative rewards via a direct policy parameterization between two completions to a prompt, enabling strikingly lightweight implementation.","In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature.","REBEL can also cleanly incorporate offline data and handle the intransitive preferences we frequently see in practice.","Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally tractable than PPO."],"url":"http://arxiv.org/abs/2404.16767v1","category":"cs.LG"}
{"created":"2024-04-25 17:19:36","title":"Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model","abstract":"While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely \"superficial\". We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts. This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs.","sentences":["While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely \"superficial\".","We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation.","Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training.","Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings.","Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts.","This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs."],"url":"http://arxiv.org/abs/2404.16766v1","category":"cs.CL"}
{"created":"2024-04-25 17:18:54","title":"The asymptotic spectrum distance, graph limits, and the Shannon capacity","abstract":"Determining the Shannon capacity of graphs is a long-standing open problem in information theory, graph theory and combinatorial optimization. Over decades, a wide range of upper and lower bound methods have been developed to analyze this problem. However, despite tremendous effort, even small instances of the problem have remained open.   In recent years, a new dual characterization of the Shannon capacity of graphs, asymptotic spectrum duality, has unified and extended known upper bound methods and structural theorems. In this paper, building on asymptotic spectrum duality, we develop a new theory of graph distance, that we call asymptotic spectrum distance, and corresponding limits (reminiscent of, but different from, the celebrated theory of cut-norm, graphons and flag algebras). We propose a graph limit approach to the Shannon capacity problem: to determine the Shannon capacity of a graph, construct a sequence of easier to analyse graphs converging to it.   (1) We give a very general construction of non-trivial converging sequences of graphs (in a family of circulant graphs).   (2) We construct Cauchy sequences of finite graphs that do not converge to any finite graph, but do converge to an infinite graph. We establish strong connections between convergence questions of finite graphs and the asymptotic properties of Borsuk-like infinite graphs on the circle.   (3) We observe that all best-known lower bound constructions for Shannon capacity of small odd cycles can be obtained from a \"finite\" version of the graph limit approach. We develop computational and theoretical aspects of this approach and use these to obtain a new Shannon capacity lower bound for the fifteen-cycle.   The theory of asymptotic spectrum distance applies not only to Shannon capacity of graphs; indeed, we will develop it for a general class of mathematical objects and their asymptotic properties.","sentences":["Determining the Shannon capacity of graphs is a long-standing open problem in information theory, graph theory and combinatorial optimization.","Over decades, a wide range of upper and lower bound methods have been developed to analyze this problem.","However, despite tremendous effort, even small instances of the problem have remained open.   ","In recent years, a new dual characterization of the Shannon capacity of graphs, asymptotic spectrum duality, has unified and extended known upper bound methods and structural theorems.","In this paper, building on asymptotic spectrum duality, we develop a new theory of graph distance, that we call asymptotic spectrum distance, and corresponding limits (reminiscent of, but different from, the celebrated theory of cut-norm, graphons and flag algebras).","We propose a graph limit approach to the Shannon capacity problem: to determine the Shannon capacity of a graph, construct a sequence of easier to analyse graphs converging to it.   ","(1) We give a very general construction of non-trivial converging sequences of graphs (in a family of circulant graphs).   ","(2) We construct Cauchy sequences of finite graphs that do not converge to any finite graph, but do converge to an infinite graph.","We establish strong connections between convergence questions of finite graphs and the asymptotic properties of Borsuk-like infinite graphs on the circle.   ","(3) We observe that all best-known lower bound constructions for Shannon capacity of small odd cycles can be obtained from a \"finite\" version of the graph limit approach.","We develop computational and theoretical aspects of this approach and use these to obtain a new Shannon capacity lower bound for the fifteen-cycle.   ","The theory of asymptotic spectrum distance applies not only to Shannon capacity of graphs; indeed, we will develop it for a general class of mathematical objects and their asymptotic properties."],"url":"http://arxiv.org/abs/2404.16763v1","category":"math.CO"}
{"created":"2024-04-25 17:16:37","title":"Beyond Boolean networks, a multi-valued approach","abstract":"Boolean networks can be viewed as functions on the set of binary strings of a given length, described via logical rules. They were introduced as dynamic models into biology, in particular as logical models of intracellular regulatory networks involving genes, proteins, and metabolites. Since genes can have several modes of action, depending on their expression levels, binary variables are often not sufficiently rich, requiring the use of multi-valued networks instead. The steady state analysis of Boolean networks is computationally complex, and increasing the number of variable values beyond $2$ adds substantially to this complexity, and no general methods are available beyond simulation. The main contribution of this paper is to give an algorithm to compute the steady states of a multi-valued network that has a complexity that, in many cases, is essentially the same as that for the case of binary values. Our approach is based on a representation of multi-valued networks using multi-valued logic functions, providing a biologically intuitive representation of the network. Furthermore, it uses tools to compute lattice points in rational polytopes, tapping a rich area of algebraic combinatorics as a source for combinatorial algorithms for Boolean network analysis. An implementation of the algorithm is provided.","sentences":["Boolean networks can be viewed as functions on the set of binary strings of a given length, described via logical rules.","They were introduced as dynamic models into biology, in particular as logical models of intracellular regulatory networks involving genes, proteins, and metabolites.","Since genes can have several modes of action, depending on their expression levels, binary variables are often not sufficiently rich, requiring the use of multi-valued networks instead.","The steady state analysis of Boolean networks is computationally complex, and increasing the number of variable values beyond $2$ adds substantially to this complexity, and no general methods are available beyond simulation.","The main contribution of this paper is to give an algorithm to compute the steady states of a multi-valued network that has a complexity that, in many cases, is essentially the same as that for the case of binary values.","Our approach is based on a representation of multi-valued networks using multi-valued logic functions, providing a biologically intuitive representation of the network.","Furthermore, it uses tools to compute lattice points in rational polytopes, tapping a rich area of algebraic combinatorics as a source for combinatorial algorithms for Boolean network analysis.","An implementation of the algorithm is provided."],"url":"http://arxiv.org/abs/2404.16760v1","category":"math.DS"}
{"created":"2024-04-25 17:16:07","title":"Revealing the regularities of electron correlation energies associated with valence electrons in atoms in the first three rows of the periodic table","abstract":"Electronic correlation is a complex many-body effect and the correlation energy depends on the specific electronic structure and spatial distribution of electrons in each atom and molecule. Although the total correlation energy in an atom can be decomposed into different components such as inter-orbital and intra-orbital pair-correlation energies (PCE), it is generally believed that the PCEs in different atoms cannot be the same. In this work, we investigate the correlation energies of the atoms in the first three rows of the periodic table (He to Ar). It is found that when the correlation energy is defined as the difference between the exact ground-state energy and the unrestricted Hartree-Fock (UHF) energy, the inter- and intra-orbital PECs associated with the valence electrons of the atoms in the same row of the periodic table have the same values. These PCEs are not entangled and their values depend only on the electron orbitals. For two specific orbitals, the inter-orbital correlation energy is the same between two electrons of parallel spins or anti-parallel spins. We also show that the effects of orbital relaxation on the correlation energy are surprisingly small.","sentences":["Electronic correlation is a complex many-body effect and the correlation energy depends on the specific electronic structure and spatial distribution of electrons in each atom and molecule.","Although the total correlation energy in an atom can be decomposed into different components such as inter-orbital and intra-orbital pair-correlation energies (PCE), it is generally believed that the PCEs in different atoms cannot be the same.","In this work, we investigate the correlation energies of the atoms in the first three rows of the periodic table (He to Ar).","It is found that when the correlation energy is defined as the difference between the exact ground-state energy and the unrestricted Hartree-Fock (UHF) energy, the inter- and intra-orbital PECs associated with the valence electrons of the atoms in the same row of the periodic table have the same values.","These PCEs are not entangled and their values depend only on the electron orbitals.","For two specific orbitals, the inter-orbital correlation energy is the same between two electrons of parallel spins or anti-parallel spins.","We also show that the effects of orbital relaxation on the correlation energy are surprisingly small."],"url":"http://arxiv.org/abs/2404.16759v1","category":"physics.chem-ph"}
{"created":"2024-04-25 17:12:09","title":"Concentration inequalities for Poisson $U$-statistics","abstract":"In this article we obtain concentration inequalities for Poisson $U$-statistics $F_m(f,\\eta)$ of order $m\\ge 1$ with kernels $f$ under general assumptions on $f$ and the intensity measure $\\gamma \\Lambda$ of underlying Poisson point process $\\eta$. The main result are new concentration bounds of the form \\[   \\mathbb{P}(|F_m ( f , \\eta) -\\mathbb{E} F_m ( f , \\eta)| \\ge t)\\leq 2\\exp(-I(\\gamma,t)), \\]   where $I(\\gamma,t)$ satisfies $I(\\gamma,t)=\\Theta(t^{1\\over m}\\log t)$ as $t\\to\\infty$ and $\\gamma$ is fixed. The function $I(\\gamma,t)$ is given explicitly in terms of parameters of the assumptions satisfied by $f$ and $\\Lambda$. One of the key ingredients of the proof are fine bounds for the centred moments of $F_m(f,\\eta)$. We discuss the optimality of obtained bounds and consider a number of applications related to Gilbert graphs and Poisson hyperplane processes in constant curvature spaces.","sentences":["In this article we obtain concentration inequalities for Poisson $U$-statistics $F_m(f,\\eta)$ of order $m\\ge 1$ with kernels $f$ under general assumptions on $f$ and the intensity measure $\\gamma \\Lambda$ of underlying Poisson point process $\\eta$.","The main result are new concentration bounds of the form \\[   \\mathbb{P}(|F_m ( f , \\eta) -\\mathbb{E","} F_m ( f , \\eta)| \\ge t)\\leq 2\\exp(-I(\\gamma,t)), \\]   where $I(\\gamma,t)$ satisfies $I(\\gamma,t)=\\Theta(t^{1\\over m}\\log t)$ as $t\\to\\infty$ and $\\gamma$ is fixed.","The function $I(\\gamma,t)$ is given explicitly in terms of parameters of the assumptions satisfied by $f$ and $\\Lambda$. One of the key ingredients of the proof are fine bounds for the centred moments of $F_m(f,\\eta)$. We discuss the optimality of obtained bounds and consider a number of applications related to Gilbert graphs and Poisson hyperplane processes in constant curvature spaces."],"url":"http://arxiv.org/abs/2404.16756v1","category":"math.PR"}
{"created":"2024-04-25 17:11:37","title":"RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT Analysis","abstract":"Developing generalist foundation model has recently attracted tremendous attention among researchers in the field of AI for Medicine (AI4Medicine). A pivotal insight in developing these models is their reliance on dataset scaling, which emphasizes the requirements on developing open-source medical image datasets that incorporate diverse supervision signals across various imaging modalities. In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE. Specifically, we leverage the latest powerful universal segmentation and large language models, to extend the original datasets (over 25,692 non-contrast 3D chest CT volume and reports from 20,000 patients) from the following aspects: (i) organ-level segmentation masks covering 197 categories, which provide intermediate reasoning visual clues for interpretation; (ii) 665 K multi-granularity grounded reports, where each sentence of the report is linked to the corresponding anatomical region of CT volume in the form of a segmentation mask; (iii) 1.3 M grounded VQA pairs, where questions and answers are all linked with reference segmentation masks, enabling models to associate visual evidence with textual explanations. All grounded reports and VQA pairs in the validation set have gone through manual verification to ensure dataset quality. We believe that RadGenome-Chest CT can significantly advance the development of multimodal medical foundation models, by training to generate texts based on given segmentation regions, which is unattainable with previous relevant datasets. We will release all segmentation masks, grounded reports, and VQA pairs to facilitate further research and development in this field.","sentences":["Developing generalist foundation model has recently attracted tremendous attention among researchers in the field of AI for Medicine (AI4Medicine).","A pivotal insight in developing these models is their reliance on dataset scaling, which emphasizes the requirements on developing open-source medical image datasets that incorporate diverse supervision signals across various imaging modalities.","In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE.","Specifically, we leverage the latest powerful universal segmentation and large language models, to extend the original datasets (over 25,692 non-contrast 3D chest CT volume and reports from 20,000 patients) from the following aspects: (i) organ-level segmentation masks covering 197 categories, which provide intermediate reasoning visual clues for interpretation; (ii) 665 K multi-granularity grounded reports, where each sentence of the report is linked to the corresponding anatomical region of CT volume in the form of a segmentation mask; (iii) 1.3 M grounded VQA pairs, where questions and answers are all linked with reference segmentation masks, enabling models to associate visual evidence with textual explanations.","All grounded reports and VQA pairs in the validation set have gone through manual verification to ensure dataset quality.","We believe that RadGenome-Chest CT can significantly advance the development of multimodal medical foundation models, by training to generate texts based on given segmentation regions, which is unattainable with previous relevant datasets.","We will release all segmentation masks, grounded reports, and VQA pairs to facilitate further research and development in this field."],"url":"http://arxiv.org/abs/2404.16754v1","category":"cs.CV"}
{"created":"2024-04-25 17:05:38","title":"TELA: Text to Layer-wise 3D Clothed Human Generation","abstract":"This paper addresses the task of 3D clothed human generation from textural descriptions. Previous works usually encode the human body and clothes as a holistic model and generate the whole model in a single-stage optimization, which makes them struggle for clothing editing and meanwhile lose fine-grained control over the whole generation process. To solve this, we propose a layer-wise clothed human representation combined with a progressive optimization strategy, which produces clothing-disentangled 3D human models while providing control capacity for the generation process. The basic idea is progressively generating a minimal-clothed human body and layer-wise clothes. During clothing generation, a novel stratified compositional rendering method is proposed to fuse multi-layer human models, and a new loss function is utilized to help decouple the clothing model from the human body. The proposed method achieves high-quality disentanglement, which thereby provides an effective way for 3D garment generation. Extensive experiments demonstrate that our approach achieves state-of-the-art 3D clothed human generation while also supporting cloth editing applications such as virtual try-on. Project page: http://jtdong.com/tela_layer/","sentences":["This paper addresses the task of 3D clothed human generation from textural descriptions.","Previous works usually encode the human body and clothes as a holistic model and generate the whole model in a single-stage optimization, which makes them struggle for clothing editing and meanwhile lose fine-grained control over the whole generation process.","To solve this, we propose a layer-wise clothed human representation combined with a progressive optimization strategy, which produces clothing-disentangled 3D human models while providing control capacity for the generation process.","The basic idea is progressively generating a minimal-clothed human body and layer-wise clothes.","During clothing generation, a novel stratified compositional rendering method is proposed to fuse multi-layer human models, and a new loss function is utilized to help decouple the clothing model from the human body.","The proposed method achieves high-quality disentanglement, which thereby provides an effective way for 3D garment generation.","Extensive experiments demonstrate that our approach achieves state-of-the-art 3D clothed human generation while also supporting cloth editing applications such as virtual try-on.","Project page: http://jtdong.com/tela_layer/"],"url":"http://arxiv.org/abs/2404.16748v1","category":"cs.CV"}
{"created":"2024-04-25 17:00:13","title":"Statistical Inference for Covariate-Adjusted and Interpretable Generalized Factor Model with Application to Testing Fairness","abstract":"In the era of data explosion, statisticians have been developing interpretable and computationally efficient statistical methods to measure latent factors (e.g., skills, abilities, and personalities) using large-scale assessment data. In addition to understanding the latent information, the covariate effect on responses controlling for latent factors is also of great scientific interest and has wide applications, such as evaluating the fairness of educational testing, where the covariate effect reflects whether a test question is biased toward certain individual characteristics (e.g., gender and race) taking into account their latent abilities. However, the large sample size, substantial covariate dimension, and great test length pose challenges to developing efficient methods and drawing valid inferences. Moreover, to accommodate the commonly encountered discrete types of responses, nonlinear latent factor models are often assumed, bringing further complexity to the problem. To address these challenges, we consider a covariate-adjusted generalized factor model and develop novel and interpretable conditions to address the identifiability issue. Based on the identifiability conditions, we propose a joint maximum likelihood estimation method and establish estimation consistency and asymptotic normality results for the covariate effects under a practical yet challenging asymptotic regime. Furthermore, we derive estimation and inference results for latent factors and the factor loadings. We illustrate the finite sample performance of the proposed method through extensive numerical studies and an application to an educational assessment dataset obtained from the Programme for International Student Assessment (PISA).","sentences":["In the era of data explosion, statisticians have been developing interpretable and computationally efficient statistical methods to measure latent factors (e.g., skills, abilities, and personalities) using large-scale assessment data.","In addition to understanding the latent information, the covariate effect on responses controlling for latent factors is also of great scientific interest and has wide applications, such as evaluating the fairness of educational testing, where the covariate effect reflects whether a test question is biased toward certain individual characteristics (e.g., gender and race) taking into account their latent abilities.","However, the large sample size, substantial covariate dimension, and great test length pose challenges to developing efficient methods and drawing valid inferences.","Moreover, to accommodate the commonly encountered discrete types of responses, nonlinear latent factor models are often assumed, bringing further complexity to the problem.","To address these challenges, we consider a covariate-adjusted generalized factor model and develop novel and interpretable conditions to address the identifiability issue.","Based on the identifiability conditions, we propose a joint maximum likelihood estimation method and establish estimation consistency and asymptotic normality results for the covariate effects under a practical yet challenging asymptotic regime.","Furthermore, we derive estimation and inference results for latent factors and the factor loadings.","We illustrate the finite sample performance of the proposed method through extensive numerical studies and an application to an educational assessment dataset obtained from the Programme for International Student Assessment (PISA)."],"url":"http://arxiv.org/abs/2404.16745v1","category":"stat.ME"}
{"created":"2024-04-25 16:57:05","title":"Automatic Speech Recognition System-Independent Word Error Rate Estimatio","abstract":"Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.","sentences":["Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems.","In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript.","Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent).","These are also domain-dependent and inflexible in real-world applications.","In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed.","In contrast to prior work, the WER estimators are trained using data that simulates ASR system output.","Hypotheses are generated using phonetically similar or linguistically more likely alternative words.","In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data.","On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME.","The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset."],"url":"http://arxiv.org/abs/2404.16743v1","category":"cs.CL"}
{"created":"2024-04-25 16:49:10","title":"CBRW: A Novel Approach for Cancelable Biometric Template Generation based on","abstract":"Cancelable Biometric is a challenging research field in which security of an original biometric image is ensured by transforming the original biometric into another irreversible domain. Several approaches have been suggested in literature for generating cancelable biometric templates. In this paper, two novel and simple cancelable biometric template generation methods based on Random Walk (CBRW) have been proposed. By employing random walk and other steps given in the proposed two algorithms viz. CBRW-BitXOR and CBRW-BitCMP, the original biometric is transformed into a cancellable template. The performance of the proposed methods is compared with other state-of-the-art methods. Experiments have been performed on eight publicly available gray and color datasets i.e. CP (ear) (gray and color), UTIRIS (iris) (gray and color), ORL (face) (gray), IIT Delhi (iris) (gray and color), and AR (face) (color). Performance of the generated templates is measured in terms of Correlation Coefficient (Cr), Root Mean Square Error (RMSE), Peak Signal to Noise Ratio (PSNR), Structural Similarity (SSIM), Mean Absolute Error (MAE), Number of Pixel Change Rate (NPCR), and Unified Average Changing Intensity (UACI). By experimental results, it has been proved that proposed methods are superior than other state-of-the-art methods in qualitative as well as quantitative analysis. Furthermore, CBRW performs better on both gray as well as color images.","sentences":["Cancelable Biometric is a challenging research field in which security of an original biometric image is ensured by transforming the original biometric into another irreversible domain.","Several approaches have been suggested in literature for generating cancelable biometric templates.","In this paper, two novel and simple cancelable biometric template generation methods based on Random Walk (CBRW) have been proposed.","By employing random walk and other steps given in the proposed two algorithms viz.","CBRW-BitXOR and CBRW-BitCMP, the original biometric is transformed into a cancellable template.","The performance of the proposed methods is compared with other state-of-the-art methods.","Experiments have been performed on eight publicly available gray and color datasets i.e. CP (ear) (gray and color), UTIRIS (iris) (gray and color), ORL (face) (gray), IIT Delhi (iris) (gray and color), and AR (face) (color).","Performance of the generated templates is measured in terms of Correlation Coefficient (Cr), Root Mean Square Error (RMSE), Peak Signal to Noise Ratio (PSNR), Structural Similarity (SSIM), Mean Absolute Error (MAE), Number of Pixel Change Rate (NPCR), and Unified Average Changing Intensity (UACI).","By experimental results, it has been proved that proposed methods are superior than other state-of-the-art methods in qualitative as well as quantitative analysis.","Furthermore, CBRW performs better on both gray as well as color images."],"url":"http://arxiv.org/abs/2404.16739v1","category":"cs.CV"}
{"created":"2024-04-25 16:44:45","title":"Lifts of quantum CSS codes","abstract":"We propose a notion of lift for quantum CSS codes, inspired by the geometrical construction of Freedman and Hastings. It is based on the existence of a canonical complex associated to any CSS code, that we introduce under the name of Tanner cone-complex, and over which we generate covering spaces. As a first application, we describe the classification of lifts of hypergraph product codes (HPC) and demonstrate the equivalence with the lifted product code (LPC) of Panteleev and Kalachev, including when the linear codes, factors of the HPC, are Tanner codes. As a second application, we report several new non-product constructions of quantum CSS codes, and we apply the prescription to generate their lifts which, for certain selected covering maps, are codes with improved relative parameters compared to the initial one.","sentences":["We propose a notion of lift for quantum CSS codes, inspired by the geometrical construction of Freedman and Hastings.","It is based on the existence of a canonical complex associated to any CSS code, that we introduce under the name of Tanner cone-complex, and over which we generate covering spaces.","As a first application, we describe the classification of lifts of hypergraph product codes (HPC) and demonstrate the equivalence with the lifted product code (LPC) of Panteleev and Kalachev, including when the linear codes, factors of the HPC, are Tanner codes.","As a second application, we report several new non-product constructions of quantum CSS codes, and we apply the prescription to generate their lifts which, for certain selected covering maps, are codes with improved relative parameters compared to the initial one."],"url":"http://arxiv.org/abs/2404.16736v1","category":"quant-ph"}
{"created":"2024-04-25 16:42:42","title":"Diagram model for the Okada algebra and monoid","abstract":"It is well known that the Young lattice is the Bratelli diagram of the symmetric groups expressing how irreducible representations restrict from $S_N$ to $S_{N-1}$. In 1988, Stanley discovered a similar lattice called the Young-Fibonacci lattice which was realized as the Bratelli diagram of a family of algebras by Okada in 1994. In this paper, we realize the Okada algebra and its associated monoid using a labeled version of Temperley-Lieb arc-diagrams. We prove in full generality that the dimension of the Okada algebra is $n!$. In particular, we interpret a natural bijection between permutations and labeled arc-diagrams as an instance of Fomin's Robinson-Schensted correspondence for the Young-Fibonacci lattice. We prove that the Okada monoid is aperiodic and describe its Green relations. Lifting those results to the algebra allows us to construct a cellular basis of the Okada algebra. }","sentences":["It is well known that the Young lattice is the Bratelli diagram of the symmetric groups expressing how irreducible representations restrict from $S_N$ to $S_{N-1}$. In 1988, Stanley discovered a similar lattice called the Young-Fibonacci lattice which was realized as the Bratelli diagram of a family of algebras by Okada in 1994.","In this paper, we realize the Okada algebra and its associated monoid using a labeled version of Temperley-Lieb arc-diagrams.","We prove in full generality that the dimension of the Okada algebra is $n!$. In particular, we interpret a natural bijection between permutations and labeled arc-diagrams as an instance of Fomin's Robinson-Schensted correspondence for the Young-Fibonacci lattice.","We prove that the Okada monoid is aperiodic and describe its Green relations.","Lifting those results to the algebra allows us to construct a cellular basis of the Okada algebra. }"],"url":"http://arxiv.org/abs/2404.16733v1","category":"math.RT"}
{"created":"2024-04-25 16:40:31","title":"Tidal reconstruction of neutron star mergers from their late inspiral","abstract":"We investigate the measurement correlation between the effective spin and the effective tidal deformability in gravitational wave signals from binary neutron star mergers. We exploit the fact that the tidal effects in a binary system are prominent when the components are closer during the late-inspiral. Thus, we indicate to a computationally efficient strategy of extracting the tidal information compressed within seconds before the merger. We report our observations for \\texttt{GW170817} and explore the suitability of our approach for the upcoming observation scenarios. Fast and accurate measurements of the tidal deformability parameters can be used to inform astronomers in prioritizing the electromagnetic follow-up efforts for such sources.","sentences":["We investigate the measurement correlation between the effective spin and the effective tidal deformability in gravitational wave signals from binary neutron star mergers.","We exploit the fact that the tidal effects in a binary system are prominent when the components are closer during the late-inspiral.","Thus, we indicate to a computationally efficient strategy of extracting the tidal information compressed within seconds before the merger.","We report our observations for \\texttt{GW170817} and explore the suitability of our approach for the upcoming observation scenarios.","Fast and accurate measurements of the tidal deformability parameters can be used to inform astronomers in prioritizing the electromagnetic follow-up efforts for such sources."],"url":"http://arxiv.org/abs/2404.16729v1","category":"gr-qc"}
{"created":"2024-04-25 16:38:52","title":"Approximation Algorithms for Hop Constrained and Buy-at-Bulk Network Design via Hop Constrained Oblivious Routing","abstract":"We consider two-cost network design models in which edges of the input graph have an associated cost and length. We build upon recent advances in hop-constrained oblivious routing to obtain two sets of results.   We address multicommodity buy-at-bulk network design in the nonuniform setting. Existing poly-logarithmic approximations are based on the junction tree approach [CHKS09,KN11]. We obtain a new polylogarithmic approximation via a natural LP relaxation. This establishes an upper bound on its integrality gap and affirmatively answers an open question raised in [CHKS09]. The rounding is based on recent results in hop-constrained oblivious routing [GHZ21], and this technique yields a polylogarithmic approximation in more general settings such as set connectivity. Our algorithm for buy-at-bulk network design is based on an LP-based reduction to hop constrained network design for which we obtain LP-based bicriteria approximation algorithms.   We also consider a fault-tolerant version of hop constrained network design where one wants to design a low-cost network to guarantee short paths between a given set of source-sink pairs even when k-1 edges can fail. This model has been considered in network design [GL17,GML18,AJL20] but no approximation algorithms were known. We obtain polylogarithmic bicriteria approximation algorithms for the single-source setting for any fixed k. We build upon the single-source algorithm and the junction-tree approach to obtain an approximation algorithm for the multicommodity setting when at most one edge can fail.","sentences":["We consider two-cost network design models in which edges of the input graph have an associated cost and length.","We build upon recent advances in hop-constrained oblivious routing to obtain two sets of results.   ","We address multicommodity buy-at-bulk network design in the nonuniform setting.","Existing poly-logarithmic approximations are based on the junction tree approach [CHKS09,KN11].","We obtain a new polylogarithmic approximation via a natural LP relaxation.","This establishes an upper bound on its integrality gap and affirmatively answers an open question raised in [CHKS09].","The rounding is based on recent results in hop-constrained oblivious routing [GHZ21], and this technique yields a polylogarithmic approximation in more general settings such as set connectivity.","Our algorithm for buy-at-bulk network design is based on an LP-based reduction to hop constrained network design for which we obtain LP-based bicriteria approximation algorithms.   ","We also consider a fault-tolerant version of hop constrained network design where one wants to design a low-cost network to guarantee short paths between a given set of source-sink pairs even when k-1 edges can fail.","This model has been considered in network design [GL17,GML18,AJL20] but no approximation algorithms were known.","We obtain polylogarithmic bicriteria approximation algorithms for the single-source setting for any fixed k. We build upon the single-source algorithm and the junction-tree approach to obtain an approximation algorithm for the multicommodity setting when at most one edge can fail."],"url":"http://arxiv.org/abs/2404.16725v1","category":"cs.DS"}
{"created":"2024-04-25 16:36:54","title":"Constrained Level Planarity is FPT with Respect to the Vertex Cover Number","abstract":"The problem Level Planarity asks for a crossing-free drawing of a graph in the plane such that vertices are placed at prescribed y-coordinates (called levels) and such that every edge is realized as a y-monotone curve. In the variant Constrained Level Planarity, each level y is equipped with a partial order <_y on its vertices and in the desired drawing the left-to-right order of vertices on level y has to be a linear extension of <_y. Constrained Level Planarity is known to be a remarkably difficult problem: previous results by Klemz and Rote [ACM Trans. Alg. 2019] and by Br\\\"uckner and Rutter [SODA 2017] imply that it remains NP-hard even when restricted to graphs whose tree-depth and feedback vertex set number are bounded by a constant and even when the instances are additionally required to be either proper, meaning that each edge spans two consecutive levels, or ordered, meaning that all given partial orders are total orders. In particular, these results rule out the existence of FPT-time (even XP-time) algorithms with respect to these and related graph parameters (unless P=NP). However, the parameterized complexity of Constrained Level Planarity with respect to the vertex cover number of the input graph remained open.   In this paper, we show that Constrained Level Planarity can be solved in FPT-time when parameterized by the vertex cover number. In view of the previous intractability statements, our result is best-possible in several regards: a speed-up to polynomial time or a generalization to the aforementioned smaller graph parameters is not possible, even if restricting to proper or ordered instances.","sentences":["The problem Level Planarity asks for a crossing-free drawing of a graph in the plane such that vertices are placed at prescribed y-coordinates (called levels) and such that every edge is realized as a y-monotone curve.","In the variant Constrained Level Planarity, each level y is equipped with a partial order <_y on its vertices and in the desired drawing the left-to-right order of vertices on level y has to be a linear extension of <_y. Constrained Level Planarity is known to be a remarkably difficult problem: previous results by Klemz and Rote [ACM Trans.","Alg. 2019]","and by Br\\\"uckner and Rutter [SODA 2017] imply that it remains NP-hard even when restricted to graphs whose tree-depth and feedback vertex set number are bounded by a constant and even when the instances are additionally required to be either proper, meaning that each edge spans two consecutive levels, or ordered, meaning that all given partial orders are total orders.","In particular, these results rule out the existence of FPT-time (even XP-time) algorithms with respect to these and related graph parameters (unless P=NP).","However, the parameterized complexity of Constrained Level Planarity with respect to the vertex cover number of the input graph remained open.   ","In this paper, we show that Constrained Level Planarity can be solved in FPT-time when parameterized by the vertex cover number.","In view of the previous intractability statements, our result is best-possible in several regards: a speed-up to polynomial time or a generalization to the aforementioned smaller graph parameters is not possible, even if restricting to proper or ordered instances."],"url":"http://arxiv.org/abs/2404.16723v1","category":"cs.DS"}
{"created":"2024-04-25 16:33:19","title":"Distilling Privileged Information for Dubins Traveling Salesman Problems with Neighborhoods","abstract":"This paper presents a novel learning approach for Dubins Traveling Salesman Problems(DTSP) with Neighborhood (DTSPN) to quickly produce a tour of a non-holonomic vehicle passing through neighborhoods of given task points. The method involves two learning phases: initially, a model-free reinforcement learning approach leverages privileged information to distill knowledge from expert trajectories generated by the LinKernighan heuristic (LKH) algorithm. Subsequently, a supervised learning phase trains an adaptation network to solve problems independently of privileged information. Before the first learning phase, a parameter initialization technique using the demonstration data was also devised to enhance training efficiency. The proposed learning method produces a solution about 50 times faster than LKH and substantially outperforms other imitation learning and RL with demonstration schemes, most of which fail to sense all the task points.","sentences":["This paper presents a novel learning approach for Dubins Traveling Salesman Problems(DTSP) with Neighborhood (DTSPN) to quickly produce a tour of a non-holonomic vehicle passing through neighborhoods of given task points.","The method involves two learning phases: initially, a model-free reinforcement learning approach leverages privileged information to distill knowledge from expert trajectories generated by the LinKernighan heuristic (LKH) algorithm.","Subsequently, a supervised learning phase trains an adaptation network to solve problems independently of privileged information.","Before the first learning phase, a parameter initialization technique using the demonstration data was also devised to enhance training efficiency.","The proposed learning method produces a solution about 50 times faster than LKH and substantially outperforms other imitation learning and RL with demonstration schemes, most of which fail to sense all the task points."],"url":"http://arxiv.org/abs/2404.16721v1","category":"cs.AI"}
{"created":"2024-04-25 16:31:59","title":"Simulations of gravitational collapse in null coordinates: III. Hyperbolicity","abstract":"We investigate the well-posedness of the characteristic initial-boundary value problem for the Einstein equations in Bondi-like coordinates (including Bondi, double-null and affine). We propose a definition of strong hyperbolicity of a system of partial differential equations of any order, and show that the Einstein equations in Bondi-like coordinates in their second-order form used in numerical relativity do not meet it, in agreement with results of Giannakopoulos et al for specific first-order reductions. In the principal part, frozen coefficient approximation that one uses to examine hyperbolicity, we explicitly construct the general solution to identify the solutions that obstruct strong hyperbolicity. Independently, we present a first-order symmetric hyperbolic formulation of the Einstein equations in Bondi gauge, linearised about Schwarzschild, thus completing work by Frittelli. This establishes an energy norm ($L^2$ in the metric perturbations and selected first and second derivatives), in which the initial-boundary value problem, with initial data on an outgoing null cone and boundary data on a timelike cylinder or an ingoing null cone, is well-posed, thus verifying a conjecture by Giannakopoulos et al. Unfortunately, our method does not extend to the pure initial-value problem on a null cone with regular vertex.","sentences":["We investigate the well-posedness of the characteristic initial-boundary value problem for the Einstein equations in Bondi-like coordinates (including Bondi, double-null and affine).","We propose a definition of strong hyperbolicity of a system of partial differential equations of any order, and show that the Einstein equations in Bondi-like coordinates in their second-order form used in numerical relativity do not meet it, in agreement with results of Giannakopoulos et al for specific first-order reductions.","In the principal part, frozen coefficient approximation that one uses to examine hyperbolicity, we explicitly construct the general solution to identify the solutions that obstruct strong hyperbolicity.","Independently, we present a first-order symmetric hyperbolic formulation of the Einstein equations in Bondi gauge, linearised about Schwarzschild, thus completing work by Frittelli.","This establishes an energy norm ($L^2$ in the metric perturbations and selected first and second derivatives), in which the initial-boundary value problem, with initial data on an outgoing null cone and boundary data on a timelike cylinder or an ingoing null cone, is well-posed, thus verifying a conjecture by Giannakopoulos et al.","Unfortunately, our method does not extend to the pure initial-value problem on a null cone with regular vertex."],"url":"http://arxiv.org/abs/2404.16720v1","category":"gr-qc"}
{"created":"2024-04-25 16:30:42","title":"Bulk flows, general relativity and the fundamental role of the \"peculiar\" flux","abstract":"Recent surveys have been reporting bulk peculiar flows considerably faster than expected. Bulk flows are moving matter and matter in motion implies nonzero energy flux. In relativity, as opposed to Newtonian physics, matter fluxes gravitate as well, since they also contribute to the energy-momentum tensor. The gravitational input of the \"peculiar\" flux survives at the linear level and it can drastically change our understanding of the way bulk flows have evolved in time. By default, the Newtonian analysis of peculiar motions bypasses the relativistic flux-contribution to the gravitational field. The problem is that there are also studies, with an otherwise relativistic profile, which inadvertently do the same. As result, these treatments reduce to Newtonian and they misleadingly reproduce the slow Newtonian growth-rate of linear peculiar velocities. In contrast, by accounting for the flux effects, the proper relativistic analysis arrives at a considerably stronger growth. We show that it is the flux input of the moving matter to gravity that separates the relativistic studies of peculiar motions from the rest and, in so doing, it could provide an answer to the bulk-flow question.","sentences":["Recent surveys have been reporting bulk peculiar flows considerably faster than expected.","Bulk flows are moving matter and matter in motion implies nonzero energy flux.","In relativity, as opposed to Newtonian physics, matter fluxes gravitate as well, since they also contribute to the energy-momentum tensor.","The gravitational input of the \"peculiar\" flux survives at the linear level and it can drastically change our understanding of the way bulk flows have evolved in time.","By default, the Newtonian analysis of peculiar motions bypasses the relativistic flux-contribution to the gravitational field.","The problem is that there are also studies, with an otherwise relativistic profile, which inadvertently do the same.","As result, these treatments reduce to Newtonian and they misleadingly reproduce the slow Newtonian growth-rate of linear peculiar velocities.","In contrast, by accounting for the flux effects, the proper relativistic analysis arrives at a considerably stronger growth.","We show that it is the flux input of the moving matter to gravity that separates the relativistic studies of peculiar motions from the rest and, in so doing, it could provide an answer to the bulk-flow question."],"url":"http://arxiv.org/abs/2404.16719v1","category":"gr-qc"}
{"created":"2024-04-25 16:30:30","title":"Features Fusion for Dual-View Mammography Mass Detection","abstract":"Detection of malignant lesions on mammography images is extremely important for early breast cancer diagnosis. In clinical practice, images are acquired from two different angles, and radiologists can fully utilize information from both views, simultaneously locating the same lesion. However, for automatic detection approaches such information fusion remains a challenge. In this paper, we propose a new model called MAMM-Net, which allows the processing of both mammography views simultaneously by sharing information not only on an object level, as seen in existing works, but also on a feature level. MAMM-Net's key component is the Fusion Layer, based on deformable attention and designed to increase detection precision while keeping high recall. Our experiments show superior performance on the public DDSM dataset compared to the previous state-of-the-art model, while introducing new helpful features such as lesion annotation on pixel-level and classification of lesions malignancy.","sentences":["Detection of malignant lesions on mammography images is extremely important for early breast cancer diagnosis.","In clinical practice, images are acquired from two different angles, and radiologists can fully utilize information from both views, simultaneously locating the same lesion.","However, for automatic detection approaches such information fusion remains a challenge.","In this paper, we propose a new model called MAMM-Net, which allows the processing of both mammography views simultaneously by sharing information not only on an object level, as seen in existing works, but also on a feature level.","MAMM-Net's key component is the Fusion Layer, based on deformable attention and designed to increase detection precision while keeping high recall.","Our experiments show superior performance on the public DDSM dataset compared to the previous state-of-the-art model, while introducing new helpful features such as lesion annotation on pixel-level and classification of lesions malignancy."],"url":"http://arxiv.org/abs/2404.16718v1","category":"eess.IV"}
{"created":"2024-04-25 16:29:06","title":"Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class","abstract":"Vision-language models enable open-world classification of objects without the need for any retraining. While this zero-shot paradigm marks a significant advance, even today's best models exhibit skewed performance when objects are dissimilar from their typical depiction. Real world objects such as pears appear in a variety of forms -- from diced to whole, on a table or in a bowl -- yet standard VLM classifiers map all instances of a class to a \\it{single vector based on the class label}. We argue that to represent this rich diversity within a class, zero-shot classification should move beyond a single vector. We propose a method to encode and account for diversity within a class using inferred attributes, still in the zero-shot setting without retraining. We find our method consistently outperforms standard zero-shot classification over a large suite of datasets encompassing hierarchies, diverse object states, and real-world geographic diversity, as well finer-grained datasets where intra-class diversity may be less prevalent. Importantly, our method is inherently interpretable, offering faithful explanations for each inference to facilitate model debugging and enhance transparency. We also find our method scales efficiently to a large number of attributes to account for diversity -- leading to more accurate predictions for atypical instances. Finally, we characterize a principled trade-off between overall and worst class accuracy, which can be tuned via a hyperparameter of our method. We hope this work spurs further research into the promise of zero-shot classification beyond a single class vector for capturing diversity in the world, and building transparent AI systems without compromising performance.","sentences":["Vision-language models enable open-world classification of objects without the need for any retraining.","While this zero-shot paradigm marks a significant advance, even today's best models exhibit skewed performance when objects are dissimilar from their typical depiction.","Real world objects such as pears appear in a variety of forms -- from diced to whole, on a table or in a bowl -- yet standard VLM classifiers map all instances of a class to a \\it{single vector based on the class label}.","We argue that to represent this rich diversity within a class, zero-shot classification should move beyond a single vector.","We propose a method to encode and account for diversity within a class using inferred attributes, still in the zero-shot setting without retraining.","We find our method consistently outperforms standard zero-shot classification over a large suite of datasets encompassing hierarchies, diverse object states, and real-world geographic diversity, as well finer-grained datasets where intra-class diversity may be less prevalent.","Importantly, our method is inherently interpretable, offering faithful explanations for each inference to facilitate model debugging and enhance transparency.","We also find our method scales efficiently to a large number of attributes to account for diversity -- leading to more accurate predictions for atypical instances.","Finally, we characterize a principled trade-off between overall and worst class accuracy, which can be tuned via a hyperparameter of our method.","We hope this work spurs further research into the promise of zero-shot classification beyond a single class vector for capturing diversity in the world, and building transparent AI systems without compromising performance."],"url":"http://arxiv.org/abs/2404.16717v1","category":"cs.CV"}
{"created":"2024-04-25 16:22:44","title":"Geometry of paraquaternionic contact structures","abstract":"We introduce the notion of paraquaternionic contact structures (pqc structures), which turns out to be a generalization of the para 3-Sasakian geometry. We derive a distinguished linear connection preserving the pqc structure. Its torsion tensor is expressed explicitly in terms of the structure tensors. We define pqc-Einstein manifolds and show that para 3-Sasakian spaces are precisely pqc manifolds, which are pqc-Einstein. We introduce the paraquaternionic Heisenberg qroup and show that it is the flat model of the pqc geometry.","sentences":["We introduce the notion of paraquaternionic contact structures (pqc structures), which turns out to be a generalization of the para 3-Sasakian geometry.","We derive a distinguished linear connection preserving the pqc structure.","Its torsion tensor is expressed explicitly in terms of the structure tensors.","We define pqc-Einstein manifolds and show that para 3-Sasakian spaces are precisely pqc manifolds, which are pqc-Einstein.","We introduce the paraquaternionic Heisenberg qroup and show that it is the flat model of the pqc geometry."],"url":"http://arxiv.org/abs/2404.16713v1","category":"math.DG"}
{"created":"2024-04-25 16:20:23","title":"Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding","abstract":"We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.","sentences":["We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs).","First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit.","Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model.","Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model.","Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages.","We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task.","We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task."],"url":"http://arxiv.org/abs/2404.16710v1","category":"cs.CL"}
{"created":"2024-04-25 17:59:19","title":"How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites","abstract":"In this report, we introduce InternVL 1.5, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448$\\times$448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks. We evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.","sentences":["In this report, we introduce InternVL 1.5, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding.","We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs.","(2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448$\\times$448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input.","(3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks.","We evaluate InternVL 1.5 through a series of benchmarks and comparative studies.","Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks.","Code has been released at https://github.com/OpenGVLab/InternVL."],"url":"http://arxiv.org/abs/2404.16821v1","category":"cs.CV"}
{"created":"2024-04-25 16:40:15","title":"Learning-Based Efficient Approximation of Data-enabled Predictive Control","abstract":"Data-Enabled Predictive Control (DeePC) bypasses the need for system identification by directly leveraging raw data to formulate optimal control policies. However, the size of the optimization problem in DeePC grows linearly with respect to the data size, which prohibits its application due to high computational costs. In this paper, we propose an efficient approximation of DeePC, whose size is invariant with respect to the amount of data collected, via differentiable convex programming. Specifically, the optimization problem in DeePC is decomposed into two parts: a control objective and a scoring function that evaluates the likelihood of a guessed I/O sequence, the latter of which is approximated with a size-invariant learned optimization problem. The proposed method is validated through numerical simulations on a quadruple tank system, illustrating that the learned controller can reduce the computational time of DeePC by 5x while maintaining its control performance.","sentences":["Data-Enabled Predictive Control (DeePC) bypasses the need for system identification by directly leveraging raw data to formulate optimal control policies.","However, the size of the optimization problem in DeePC grows linearly with respect to the data size, which prohibits its application due to high computational costs.","In this paper, we propose an efficient approximation of DeePC, whose size is invariant with respect to the amount of data collected, via differentiable convex programming.","Specifically, the optimization problem in DeePC is decomposed into two parts: a control objective and a scoring function that evaluates the likelihood of a guessed I/O sequence, the latter of which is approximated with a size-invariant learned optimization problem.","The proposed method is validated through numerical simulations on a quadruple tank system, illustrating that the learned controller can reduce the computational time of DeePC by 5x while maintaining its control performance."],"url":"http://arxiv.org/abs/2404.16727v1","category":"eess.SY"}
