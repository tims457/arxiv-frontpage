{"created":"2024-03-12 17:58:38","title":"OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation","abstract":"Open-sourced, user-friendly tools form the bedrock of scientific advancement across disciplines. The widespread adoption of data-driven learning has led to remarkable progress in multi-fingered dexterity, bimanual manipulation, and applications ranging from logistics to home robotics. However, existing data collection platforms are often proprietary, costly, or tailored to specific robotic morphologies. We present OPEN TEACH, a new teleoperation system leveraging VR headsets to immerse users in mixed reality for intuitive robot control. Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH enables real-time control of various robots, including multi-fingered hands and bimanual arms, through an easy-to-use app. Using natural hand gestures and movements, users can manipulate robots at up to 90Hz with smooth visual feedback and interface widgets offering closeup environment views. We demonstrate the versatility of OPEN TEACH across 38 tasks on different robots. A comprehensive user study indicates significant improvement in teleoperation capability over the AnyTeleop framework. Further experiments exhibit that the collected data is compatible with policy learning on 10 dexterous and contact-rich manipulation tasks. Currently supporting Franka, xArm, Jaco, and Allegro platforms, OPEN TEACH is fully open-sourced to promote broader adoption. Videos are available at https://open-teach.github.io/.","sentences":["Open-sourced, user-friendly tools form the bedrock of scientific advancement across disciplines.","The widespread adoption of data-driven learning has led to remarkable progress in multi-fingered dexterity, bimanual manipulation, and applications ranging from logistics to home robotics.","However, existing data collection platforms are often proprietary, costly, or tailored to specific robotic morphologies.","We present OPEN TEACH, a new teleoperation system leveraging VR headsets to immerse users in mixed reality for intuitive robot control.","Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH enables real-time control of various robots, including multi-fingered hands and bimanual arms, through an easy-to-use app.","Using natural hand gestures and movements, users can manipulate robots at up to 90Hz with smooth visual feedback and interface widgets offering closeup environment views.","We demonstrate the versatility of OPEN TEACH across 38 tasks on different robots.","A comprehensive user study indicates significant improvement in teleoperation capability over the AnyTeleop framework.","Further experiments exhibit that the collected data is compatible with policy learning on 10 dexterous and contact-rich manipulation tasks.","Currently supporting Franka, xArm, Jaco, and Allegro platforms, OPEN TEACH is fully open-sourced to promote broader adoption.","Videos are available at https://open-teach.github.io/."],"url":"http://arxiv.org/abs/2403.07870v1","category":"cs.RO"}
{"created":"2024-03-12 17:58:01","title":"TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation","abstract":"A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world. We demonstrate the quality of the demonstrations collected with TeleMoMa by training imitation learning policies for mobile manipulation tasks involving synchronized whole-body motion. Finally, we also show that TeleMoMa's teleoperation channel enables teleoperation on site, looking at the robot, or remote, sending commands and observations through a computer network, and perform user studies to evaluate how easy it is for novice users to learn to collect demonstrations with different combinations of human interfaces enabled by our system. We hope TeleMoMa becomes a helpful tool for the community enabling researchers to collect whole-body mobile manipulation demonstrations. For more information and video results, https://robin-lab.cs.utexas.edu/telemoma-web.","sentences":["A critical bottleneck limiting imitation learning in robotics is the lack of data.","This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces.","In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators.","TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof.","In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations.","We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world.","We demonstrate the quality of the demonstrations collected with TeleMoMa by training imitation learning policies for mobile manipulation tasks involving synchronized whole-body motion.","Finally, we also show that TeleMoMa's teleoperation channel enables teleoperation on site, looking at the robot, or remote, sending commands and observations through a computer network, and perform user studies to evaluate how easy it is for novice users to learn to collect demonstrations with different combinations of human interfaces enabled by our system.","We hope TeleMoMa becomes a helpful tool for the community enabling researchers to collect whole-body mobile manipulation demonstrations.","For more information and video results, https://robin-lab.cs.utexas.edu/telemoma-web."],"url":"http://arxiv.org/abs/2403.07869v1","category":"cs.RO"}
{"created":"2024-03-12 17:55:38","title":"Exploring Safety Generalization Challenges of Large Language Models via Code","abstract":"The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages. These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs.","sentences":["The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse.","While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains.","This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs.","Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\\% of the time.","Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages.","These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs."],"url":"http://arxiv.org/abs/2403.07865v1","category":"cs.CL"}
{"created":"2024-03-12 17:24:26","title":"MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric","abstract":"Vision-language pre-trained models have achieved impressive performance on various downstream tasks. However, their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance. Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks. In this paper, we first propose the Module-wise Pruning Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks. Using the MoPE metric, we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly reducing pre-training costs while maintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning from width to depth yields highly competitive task-specific models. Extensive experiments in two stages demonstrate the effectiveness of the MoPE metric, and MoPE-CLIP outperforms previous state-of-the-art VLP compression methods.","sentences":["Vision-language pre-trained models have achieved impressive performance on various downstream tasks.","However, their large model sizes hinder their utilization on platforms with limited computational resources.","We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance.","Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks.","In this paper, we first propose the Module-wise Pruning Error (MoPE) metric, accurately assessing CLIP module importance by performance decline on cross-modal tasks.","Using the MoPE metric, we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages.","For pre-training, MoPE-CLIP effectively leverages knowledge from the teacher model, significantly reducing pre-training costs while maintaining strong zero-shot capabilities.","For fine-tuning, consecutive pruning from width to depth yields highly competitive task-specific models.","Extensive experiments in two stages demonstrate the effectiveness of the MoPE metric, and MoPE-CLIP outperforms previous state-of-the-art VLP compression methods."],"url":"http://arxiv.org/abs/2403.07839v1","category":"cs.CV"}
{"created":"2024-03-12 17:17:20","title":"When Eye-Tracking Meets Machine Learning: A Systematic Review on Applications in Medical Image Analysis","abstract":"Eye-gaze tracking research offers significant promise in enhancing various healthcare-related tasks, above all in medical image analysis and interpretation. Eye tracking, a technology that monitors and records the movement of the eyes, provides valuable insights into human visual attention patterns. This technology can transform how healthcare professionals and medical specialists engage with and analyze diagnostic images, offering a more insightful and efficient approach to medical diagnostics. Hence, extracting meaningful features and insights from medical images by leveraging eye-gaze data improves our understanding of how radiologists and other medical experts monitor, interpret, and understand images for diagnostic purposes. Eye-tracking data, with intricate human visual attention patterns embedded, provides a bridge to integrating artificial intelligence (AI) development and human cognition. This integration allows novel methods to incorporate domain knowledge into machine learning (ML) and deep learning (DL) approaches to enhance their alignment with human-like perception and decision-making. Moreover, extensive collections of eye-tracking data have also enabled novel ML/DL methods to analyze human visual patterns, paving the way to a better understanding of human vision, attention, and cognition. This systematic review investigates eye-gaze tracking applications and methodologies for enhancing ML/DL algorithms for medical image analysis in depth.","sentences":["Eye-gaze tracking research offers significant promise in enhancing various healthcare-related tasks, above all in medical image analysis and interpretation.","Eye tracking, a technology that monitors and records the movement of the eyes, provides valuable insights into human visual attention patterns.","This technology can transform how healthcare professionals and medical specialists engage with and analyze diagnostic images, offering a more insightful and efficient approach to medical diagnostics.","Hence, extracting meaningful features and insights from medical images by leveraging eye-gaze data improves our understanding of how radiologists and other medical experts monitor, interpret, and understand images for diagnostic purposes.","Eye-tracking data, with intricate human visual attention patterns embedded, provides a bridge to integrating artificial intelligence (AI) development and human cognition.","This integration allows novel methods to incorporate domain knowledge into machine learning (ML) and deep learning (DL) approaches to enhance their alignment with human-like perception and decision-making.","Moreover, extensive collections of eye-tracking data have also enabled novel ML/DL methods to analyze human visual patterns, paving the way to a better understanding of human vision, attention, and cognition.","This systematic review investigates eye-gaze tracking applications and methodologies for enhancing ML/DL algorithms for medical image analysis in depth."],"url":"http://arxiv.org/abs/2403.07834v1","category":"eess.IV"}
{"created":"2024-03-12 17:13:43","title":"Utilizing Load Shifting for Optimal Compressor Sequencing in Industrial Refrigeration","abstract":"The ubiquity and energy needs of industrial refrigeration has prompted several research studies investigating various control opportunities for reducing energy demand. This work focuses on one such opportunity, termed compressor sequencing, which entails intelligently selecting the operational state of the compressors to service the required refrigeration load with the least possible work. We first study the static compressor sequencing problem and observe that deriving the optimal compressor operational state is computationally challenging and can vary dramatically based on the refrigeration load. Thus we introduce load shifting in conjunction with compressor sequencing, which entails strategically precooling the facility to allow for more efficient compressor operation. Interestingly, we show that load shifting not only provides benefits in computing the optimal compressor operational state, but also can lead to significant energy savings. Our results are based on and compared to real-world sensor data from an operating industrial refrigeration site of Butterball LLC located in Huntsville, AR, which demonstrated that without load shifting, even optimal compressor operation results in compressors often running at intermediate capacity levels, which can lead to inefficiencies. Through collected data, we demonstrate that a load shifting approach for compressor sequencing has the potential to reduce energy use of the compressors up to 20% compared to optimal sequencing without load shifting.","sentences":["The ubiquity and energy needs of industrial refrigeration has prompted several research studies investigating various control opportunities for reducing energy demand.","This work focuses on one such opportunity, termed compressor sequencing, which entails intelligently selecting the operational state of the compressors to service the required refrigeration load with the least possible work.","We first study the static compressor sequencing problem and observe that deriving the optimal compressor operational state is computationally challenging and can vary dramatically based on the refrigeration load.","Thus we introduce load shifting in conjunction with compressor sequencing, which entails strategically precooling the facility to allow for more efficient compressor operation.","Interestingly, we show that load shifting not only provides benefits in computing the optimal compressor operational state, but also can lead to significant energy savings.","Our results are based on and compared to real-world sensor data from an operating industrial refrigeration site of Butterball LLC located in Huntsville, AR, which demonstrated that without load shifting, even optimal compressor operation results in compressors often running at intermediate capacity levels, which can lead to inefficiencies.","Through collected data, we demonstrate that a load shifting approach for compressor sequencing has the potential to reduce energy use of the compressors up to 20% compared to optimal sequencing without load shifting."],"url":"http://arxiv.org/abs/2403.07831v1","category":"eess.SY"}
{"created":"2024-03-12 17:11:50","title":"Parity questions in critical planar Brownian loop-soups (or \"where did the free planar bosons go?\")","abstract":"The critical two-dimensional Brownian loop-soup is an infinite collection of non-interacting Brownian loops in a planar domain that possesses some combinatorial features related to the notion of indistinguishability of bosons. The properly renormalized occupation time field of this collection of loops is known to be distributed like the properly defined square of a Gaussian free field. In the present paper, we investigate aspects of the question about how much information these fields provide about the loop-soup. Among other things, we show that the exact set of points that are actually visited by some loops in the loop-soup is not determined by these fields. We further prove that given the fields, a dense family of special points will each have a conditional probability 1/2 of being part of the loop-soup. We also exhibit another instance where the possible decompositions (given the field) into individual loops and excursions can be grouped into two clearly different groups, each having a conditional probability 1/2 of occurring.","sentences":["The critical two-dimensional Brownian loop-soup is an infinite collection of non-interacting Brownian loops in a planar domain that possesses some combinatorial features related to the notion of indistinguishability of bosons.","The properly renormalized occupation time field of this collection of loops is known to be distributed like the properly defined square of a Gaussian free field.","In the present paper, we investigate aspects of the question about how much information these fields provide about the loop-soup.","Among other things, we show that the exact set of points that are actually visited by some loops in the loop-soup is not determined by these fields.","We further prove that given the fields, a dense family of special points will each have a conditional probability 1/2 of being part of the loop-soup.","We also exhibit another instance where the possible decompositions (given the field) into individual loops and excursions can be grouped into two clearly different groups, each having a conditional probability 1/2 of occurring."],"url":"http://arxiv.org/abs/2403.07830v1","category":"math.PR"}
{"created":"2024-03-12 16:57:56","title":"Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling","abstract":"Echocardiography (echo) is the first imaging modality used when assessing cardiac function. The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process. However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.). To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets. A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled. Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data. In this paper we show that training naively with such a loss function and multiple diverse datasets can lead to a form of shortcut learning, where the model associates label presence with domain characteristics, leading to a drop in performance. To address this problem, we propose a novel label dropout scheme to break the link between domain characteristics and the presence or absence of labels. We demonstrate that label dropout improves echo segmentation Dice score by 62% and 25% on two cardiac structures when training using multiple diverse partially labelled datasets.","sentences":["Echocardiography (echo) is the first imaging modality used when assessing cardiac function.","The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process.","However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.).","To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets.","A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled.","Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data.","In this paper we show that training naively with such a loss function and multiple diverse datasets can lead to a form of shortcut learning, where the model associates label presence with domain characteristics, leading to a drop in performance.","To address this problem, we propose a novel label dropout scheme to break the link between domain characteristics and the presence or absence of labels.","We demonstrate that label dropout improves echo segmentation Dice score by 62% and 25% on two cardiac structures when training using multiple diverse partially labelled datasets."],"url":"http://arxiv.org/abs/2403.07818v1","category":"cs.CV"}
{"created":"2024-03-12 16:54:58","title":"Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM","abstract":"We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.","sentences":["We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge.","Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost.","After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing.","BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously.","Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff."],"url":"http://arxiv.org/abs/2403.07816v1","category":"cs.CL"}
{"created":"2024-03-12 16:53:54","title":"Chronos: Learning the Language of Time Series","abstract":"We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained specifically on them. Our results demonstrate that Chronos models can leverage time series data from diverse domains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines.","sentences":["We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models.","Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss.","We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization.","In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained specifically on them.","Our results demonstrate that Chronos models can leverage time series data from diverse domains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrained models as a viable tool to greatly simplify forecasting pipelines."],"url":"http://arxiv.org/abs/2403.07815v1","category":"cs.LG"}
{"created":"2024-03-12 16:42:44","title":"Beyond Memorization: The Challenge of Random Memory Access in Language Models","abstract":"Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering. The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access.","sentences":["Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks.","However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive.","In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly.","Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content.","We find that techniques including recitation and permutation improve the random memory access capability of LMs.","Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering.","The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access."],"url":"http://arxiv.org/abs/2403.07805v2","category":"cs.CL"}
{"created":"2024-03-12 16:34:07","title":"Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data","abstract":"Mechanisms for generating differentially private synthetic data based on marginals and graphical models have been successful in a wide range of settings. However, one limitation of these methods is their inability to incorporate public data. Initializing a data generating model by pre-training on public data has shown to improve the quality of synthetic data, but this technique is not applicable when model structure is not determined a priori. We develop the mechanism jam-pgm, which expands the adaptive measurements framework to jointly select between measuring public data and private data. This technique allows for public data to be included in a graphical-model-based mechanism. We show that jam-pgm is able to outperform both publicly assisted and non publicly assisted synthetic data generation mechanisms even when the public data distribution is biased.","sentences":["Mechanisms for generating differentially private synthetic data based on marginals and graphical models have been successful in a wide range of settings.","However, one limitation of these methods is their inability to incorporate public data.","Initializing a data generating model by pre-training on public data has shown to improve the quality of synthetic data, but this technique is not applicable when model structure is not determined a priori.","We develop the mechanism jam-pgm, which expands the adaptive measurements framework to jointly select between measuring public data and private data.","This technique allows for public data to be included in a graphical-model-based mechanism.","We show that jam-pgm is able to outperform both publicly assisted and non publicly assisted synthetic data generation mechanisms even when the public data distribution is biased."],"url":"http://arxiv.org/abs/2403.07797v1","category":"cs.LG"}
{"created":"2024-03-12 16:27:25","title":"RobotCycle: Assessing Cycling Safety in Urban Environments","abstract":"This paper introduces RobotCycle, a novel ongoing project that leverages Autonomous Vehicle (AV) research to investigate how cycling infrastructure influences cyclist behaviour and safety during real-world journeys. The project's requirements were defined in collaboration with key stakeholders (i.e. city planners, cyclists, and policymakers), informing the design of risk and safety metrics and the data collection criteria. We propose a data-driven approach relying on a novel, rich dataset of diverse traffic scenes captured through a custom-designed wearable sensing unit. We extract road-user trajectories and analyse deviations suggesting risk or potentially hazardous interactions in correlation with infrastructural elements in the environment. Driving profiles and trajectory patterns are associated with local road segments, driving conditions, and road-user interactions to predict traffic behaviour and identify critical scenarios. Moreover, leveraging advancements in AV research, the project extracts detailed 3D maps, traffic flow patterns, and trajectory models to provide an in-depth assessment and analysis of the behaviour of all traffic agents. This data can then inform the design of cyclist-friendly road infrastructure, improving road safety and cyclability, as it provides valuable insights for enhancing cyclist protection and promoting sustainable urban mobility.","sentences":["This paper introduces RobotCycle, a novel ongoing project that leverages Autonomous Vehicle (AV) research to investigate how cycling infrastructure influences cyclist behaviour and safety during real-world journeys.","The project's requirements were defined in collaboration with key stakeholders (i.e. city planners, cyclists, and policymakers), informing the design of risk and safety metrics and the data collection criteria.","We propose a data-driven approach relying on a novel, rich dataset of diverse traffic scenes captured through a custom-designed wearable sensing unit.","We extract road-user trajectories and analyse deviations suggesting risk or potentially hazardous interactions in correlation with infrastructural elements in the environment.","Driving profiles and trajectory patterns are associated with local road segments, driving conditions, and road-user interactions to predict traffic behaviour and identify critical scenarios.","Moreover, leveraging advancements in AV research, the project extracts detailed 3D maps, traffic flow patterns, and trajectory models to provide an in-depth assessment and analysis of the behaviour of all traffic agents.","This data can then inform the design of cyclist-friendly road infrastructure, improving road safety and cyclability, as it provides valuable insights for enhancing cyclist protection and promoting sustainable urban mobility."],"url":"http://arxiv.org/abs/2403.07789v1","category":"cs.RO"}
{"created":"2024-03-12 16:23:49","title":"DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation","abstract":"Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism to refine and further improve robot performance. Through extensive evaluation across six dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods for dexterous manipulation. More details can be found at https://dex-cap.github.io","sentences":["Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks.","Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies.","To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data.","DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment.","Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands.","Beyond learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism to refine and further improve robot performance.","Through extensive evaluation across six dexterous manipulation tasks, our approach not only demonstrates superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods for dexterous manipulation.","More details can be found at https://dex-cap.github.io"],"url":"http://arxiv.org/abs/2403.07788v1","category":"cs.RO"}
{"created":"2024-03-12 16:20:27","title":"Generative deep learning-enabled ultra-large field-of-view lens-free imaging","abstract":"Advancements in high-throughput biomedical applications necessitate real-time, large field-of-view (FOV) imaging capabilities. Conventional lens-free imaging (LFI) systems, while addressing the limitations of physical lenses, have been constrained by dynamic, hard-to-model optical fields, resulting in a limited one-shot FOV of approximately 20 $mm^2$. This restriction has been a major bottleneck in applications like live-cell imaging and automation of microfluidic systems for biomedical research. Here, we present a deep-learning(DL)-based imaging framework -- GenLFI -- leveraging generative artificial intelligence (AI) for holographic image reconstruction. We demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$, surpassing the current LFI system by more than 20-fold, and even larger than the world's largest confocal microscope by 1.76 times. The resolution is at the sub-pixel level of 5.52 $\\mu m$, without the need for a shifting light source. The unsupervised learning-based reconstruction does not require optical field modeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics and 3D cell models) in complex optical fields possible. This GenLFI framework unlocks the potential of LFI systems, offering a robust tool to tackle new frontiers in high-throughput biomedical applications such as drug discovery.","sentences":["Advancements in high-throughput biomedical applications necessitate real-time, large field-of-view (FOV) imaging capabilities.","Conventional lens-free imaging (LFI) systems, while addressing the limitations of physical lenses, have been constrained by dynamic, hard-to-model optical fields, resulting in a limited one-shot FOV of approximately 20 $mm^2$. This restriction has been a major bottleneck in applications like live-cell imaging and automation of microfluidic systems for biomedical research.","Here, we present a deep-learning(DL)-based imaging framework -- GenLFI -- leveraging generative artificial intelligence (AI) for holographic image reconstruction.","We demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$, surpassing the current LFI system by more than 20-fold, and even larger than the world's largest confocal microscope by 1.76 times.","The resolution is at the sub-pixel level of 5.52 $\\mu m$, without the need for a shifting light source.","The unsupervised learning-based reconstruction does not require optical field modeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics and 3D cell models) in complex optical fields possible.","This GenLFI framework unlocks the potential of LFI systems, offering a robust tool to tackle new frontiers in high-throughput biomedical applications such as drug discovery."],"url":"http://arxiv.org/abs/2403.07786v1","category":"physics.optics"}
{"created":"2024-03-12 15:56:10","title":"Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations","abstract":"This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct prototyping that considers behavioral elements, driven by strategies that stimulate the generation of knowledge based on the use case proposed in the scenario (role-play) business, using a discussion approach between agents (guided conversation). We demonstrate the potential of developing agents useful for organizational strategies, based on multi-agent system theories (SMA) and innovative uses based on large language models (LLM based), offering a differentiated and adaptable experiment to different applications, complexities, domains, and capabilities from LLM.","sentences":["This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration.","Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving.","It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance.","In our approach we employ agents developed from large language models (LLM), each with distinct prototyping that considers behavioral elements, driven by strategies that stimulate the generation of knowledge based on the use case proposed in the scenario (role-play) business, using a discussion approach between agents (guided conversation).","We demonstrate the potential of developing agents useful for organizational strategies, based on multi-agent system theories (SMA) and innovative uses based on large language models (LLM based), offering a differentiated and adaptable experiment to different applications, complexities, domains, and capabilities from LLM."],"url":"http://arxiv.org/abs/2403.07769v1","category":"cs.AI"}
{"created":"2024-03-12 15:51:38","title":"Emerging Technologies for 6G Non-Terrestrial-Networks: From Academia to Industrial Applications","abstract":"Terrestrial networks form the fundamental infrastructure of modern communication systems, serving more than 4 billion users globally. However, terrestrial networks are facing a wide range of challenges, from coverage and reliability to interference and congestion. As the demands of the 6G era are expected to be much higher, it is crucial to address these challenges to ensure a robust and efficient communication infrastructure for the future. To address these problems, Non-terrestrial Network (NTN) has emerged to be a promising solution. NTNs are communication networks that leverage airborne (e.g., unmanned aerial vehicles) and spaceborne vehicles (e.g., satellites) to facilitate ultra-reliable communications and connectivity with high data rates and low latency over expansive regions. This article aims to provide a comprehensive survey on the utilization of network slicing, Artificial Intelligence/Machine Learning (AI/ML), and Open Radio Access Network (ORAN) to address diverse challenges of NTNs from the perspectives of both academia and industry. Particularly, we first provide an in-depth tutorial on NTN and the key enabling technologies including network slicing, AI/ML, and ORAN. Then, we provide a comprehensive survey on how network slicing and AI/ML have been leveraged to overcome the challenges that NTNs are facing. Moreover, we present how ORAN can be utilized for NTNs. Finally, we highlight important challenges, open issues, and future research directions of NTN in the 6G era.","sentences":["Terrestrial networks form the fundamental infrastructure of modern communication systems, serving more than 4 billion users globally.","However, terrestrial networks are facing a wide range of challenges, from coverage and reliability to interference and congestion.","As the demands of the 6G era are expected to be much higher, it is crucial to address these challenges to ensure a robust and efficient communication infrastructure for the future.","To address these problems, Non-terrestrial Network (NTN) has emerged to be a promising solution.","NTNs are communication networks that leverage airborne (e.g., unmanned aerial vehicles) and spaceborne vehicles (e.g., satellites) to facilitate ultra-reliable communications and connectivity with high data rates and low latency over expansive regions.","This article aims to provide a comprehensive survey on the utilization of network slicing, Artificial Intelligence/Machine Learning (AI/ML), and Open Radio Access Network (ORAN) to address diverse challenges of NTNs from the perspectives of both academia and industry.","Particularly, we first provide an in-depth tutorial on NTN and the key enabling technologies including network slicing, AI/ML, and ORAN.","Then, we provide a comprehensive survey on how network slicing and AI/ML have been leveraged to overcome the challenges that NTNs are facing.","Moreover, we present how ORAN can be utilized for NTNs.","Finally, we highlight important challenges, open issues, and future research directions of NTN in the 6G era."],"url":"http://arxiv.org/abs/2403.07763v1","category":"cs.NI"}
{"created":"2024-03-12 15:36:42","title":"Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings","abstract":"The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel space. This research introduces a promising technique for generating large-scale, customizable image datasets, leading to enhanced VLM performance and wider applicability across various domains, all with improved data efficiency and resource utilization.","sentences":["The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs).","We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training.","Our method employs pretraining a text-to-image model to synthesize image embeddings starting from captions generated by an LLM.","These synthetic pairs are then used to train a VLM.","Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data.","In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset.","Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel space.","This research introduces a promising technique for generating large-scale, customizable image datasets, leading to enhanced VLM performance and wider applicability across various domains, all with improved data efficiency and resource utilization."],"url":"http://arxiv.org/abs/2403.07750v1","category":"cs.CV"}
{"created":"2024-03-12 15:33:09","title":"Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph","abstract":"We investigate two fundamental problems in mobile computing: exploration and rendezvous, with two distinct mobile agents in an unknown graph. The agents can read and write information on whiteboards that are located at all nodes. They both move along one adjacent edge at every time-step. In the exploration problem, both agents start from the same node of the graph and must traverse all of its edges. We show that a simple variant of depth-first search achieves collective exploration in $m$ synchronous time-steps, where $m$ is the number of edges of the graph. This improves the competitive ratio of collective graph exploration. In the rendezvous problem, the agents start from different nodes of the graph and must meet as fast as possible. We introduce an algorithm guaranteeing rendezvous in at most $\\frac{3}{2}m$ time-steps. This improves over the so-called `wait for Mommy' algorithm which requires $2m$ time-steps. All our guarantees are derived from a more general asynchronous setting in which the speeds of the agents are controlled by an adversary at all times. Our guarantees also generalize to weighted graphs, if the number of edges $m$ is replaced by the sum of all edge lengths.","sentences":["We investigate two fundamental problems in mobile computing: exploration and rendezvous, with two distinct mobile agents in an unknown graph.","The agents can read and write information on whiteboards that are located at all nodes.","They both move along one adjacent edge at every time-step.","In the exploration problem, both agents start from the same node of the graph and must traverse all of its edges.","We show that a simple variant of depth-first search achieves collective exploration in $m$ synchronous time-steps, where $m$ is the number of edges of the graph.","This improves the competitive ratio of collective graph exploration.","In the rendezvous problem, the agents start from different nodes of the graph and must meet as fast as possible.","We introduce an algorithm guaranteeing rendezvous in at most $\\frac{3}{2}m$ time-steps.","This improves over the so-called `wait for Mommy' algorithm which requires $2m$ time-steps.","All our guarantees are derived from a more general asynchronous setting in which the speeds of the agents are controlled by an adversary at all times.","Our guarantees also generalize to weighted graphs, if the number of edges $m$ is replaced by the sum of all edge lengths."],"url":"http://arxiv.org/abs/2403.07748v1","category":"cs.MA"}
{"created":"2024-03-12 15:32:39","title":"FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models","abstract":"To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathematical reasoning capability of Chinese LLMs. We also carry out an in-depth analysis on the evaluation process and methods that have been overlooked previously. These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities. The dataset will be publicly available soon.","sentences":["To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels.","In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs.","FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs.","All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems.","We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathematical reasoning capability of Chinese LLMs.","We also carry out an in-depth analysis on the evaluation process and methods that have been overlooked previously.","These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities.","The dataset will be publicly available soon."],"url":"http://arxiv.org/abs/2403.07747v1","category":"cs.CL"}
{"created":"2024-03-12 15:28:21","title":"Probabilistic Easy Variational Causal Effect","abstract":"Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the total variation and the flux of $g$, we develop a point of view in causal inference capable of dealing with a broad domain of causal problems. Indeed, we focus on a function, called Probabilistic Easy Variational Causal Effect (PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect to continuously and interventionally changing the values of $X$ while keeping the value of $Z$ constant. PEACE is a function of $d\\ge 0$, which is a degree managing the strengths of probability density values $f(x|z)$. On the other hand, we generalize the above idea for the discrete case and show its compatibility with the continuous case. Further, we investigate some properties of PEACE using measure theoretical concepts. Furthermore, we provide some identifiability criteria and several examples showing the generic capability of PEACE. We note that PEACE can deal with the causal problems for which micro-level or just macro-level changes in the value of the input variables are important. Finally, PEACE is stable under small changes in $\\partial g_{in}/\\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is obtained from $g$ by removing all functional relationships defining $X$ and $Z$.","sentences":["Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$.","In this paper, on the one hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the total variation and the flux of $g$, we develop a point of view in causal inference capable of dealing with a broad domain of causal problems.","Indeed, we focus on a function, called Probabilistic Easy Variational Causal Effect (PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect to continuously and interventionally changing the values of $X$ while keeping the value of $Z$ constant.","PEACE is a function of $d\\ge 0$, which is a degree managing the strengths of probability density values $f(x|z)$. On the other hand, we generalize the above idea for the discrete case and show its compatibility with the continuous case.","Further, we investigate some properties of PEACE using measure theoretical concepts.","Furthermore, we provide some identifiability criteria and several examples showing the generic capability of PEACE.","We note that PEACE can deal with the causal problems for which micro-level or just macro-level changes in the value of the input variables are important.","Finally, PEACE is stable under small changes in $\\partial g_{in}/\\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is obtained from $g$ by removing all functional relationships defining $X$ and $Z$."],"url":"http://arxiv.org/abs/2403.07745v1","category":"stat.ML"}
{"created":"2024-03-12 15:22:05","title":"Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs","abstract":"Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probability distribution to improve the sensitivity of the MoE. We developed DL pipelines using two MoEs and two multiclass models of state-of-the-art deep convolutional neural networks (DCNNs) and vision transformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed simpler multiclass models and were tested on datasets from different hospitals and cancer types, where MoE using DCNNs yielded the best results. The proposed MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining less computational cost for inference than MoE using ViTs. This best performance of MoEs comes with relatively higher computational trade-offs than multiclass models. The proposed artifact detection pipeline will not only ensure reliable CPATH predictions but may also provide quality control.","sentences":["Histopathology is a gold standard for cancer diagnosis under a microscopic examination.","However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs).","Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions.","Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis.","In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs.","First, we train independent binary DL models as experts to capture particular artifact morphology.","Then, we ensemble their predictions using a fusion mechanism.","We apply probabilistic thresholding over the final probability distribution to improve the sensitivity of the MoE. We developed DL pipelines using two MoEs and two multiclass models of state-of-the-art deep convolutional neural networks (DCNNs) and vision transformers (ViTs).","DCNNs-based MoE and ViTs-based MoE schemes outperformed simpler multiclass models and were tested on datasets from different hospitals and cancer types, where MoE using DCNNs yielded the best results.","The proposed MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining less computational cost for inference than MoE using ViTs.","This best performance of MoEs comes with relatively higher computational trade-offs than multiclass models.","The proposed artifact detection pipeline will not only ensure reliable CPATH predictions but may also provide quality control."],"url":"http://arxiv.org/abs/2403.07743v2","category":"eess.IV"}
{"created":"2024-03-12 15:19:25","title":"Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation","abstract":"The estimation of 6D object poses is a fundamental task in many computer vision applications. Particularly, in high risk scenarios such as human-robot interaction, industrial inspection, and automation, reliable pose estimates are crucial. In the last years, increasingly accurate and robust deep-learning-based approaches for 6D object pose estimation have been proposed. Many top-performing methods are not end-to-end trainable but consist of multiple stages. In the context of deep uncertainty quantification, deep ensembles are considered as state of the art since they have been proven to produce well-calibrated and robust uncertainty estimates. However, deep ensembles can only be applied to methods that can be trained end-to-end. In this work, we propose a method to quantify the uncertainty of multi-stage 6D object pose estimation approaches with deep ensembles. For the implementation, we choose SurfEmb as representative, since it is one of the top-performing 6D object pose estimation approaches in the BOP Challenge 2022. We apply established metrics and concepts for deep uncertainty quantification to evaluate the results. Furthermore, we propose a novel uncertainty calibration score for regression tasks to quantify the quality of the estimated uncertainty.","sentences":["The estimation of 6D object poses is a fundamental task in many computer vision applications.","Particularly, in high risk scenarios such as human-robot interaction, industrial inspection, and automation, reliable pose estimates are crucial.","In the last years, increasingly accurate and robust deep-learning-based approaches for 6D object pose estimation have been proposed.","Many top-performing methods are not end-to-end trainable but consist of multiple stages.","In the context of deep uncertainty quantification, deep ensembles are considered as state of the art since they have been proven to produce well-calibrated and robust uncertainty estimates.","However, deep ensembles can only be applied to methods that can be trained end-to-end.","In this work, we propose a method to quantify the uncertainty of multi-stage 6D object pose estimation approaches with deep ensembles.","For the implementation, we choose SurfEmb as representative, since it is one of the top-performing 6D object pose estimation approaches in the BOP Challenge 2022.","We apply established metrics and concepts for deep uncertainty quantification to evaluate the results.","Furthermore, we propose a novel uncertainty calibration score for regression tasks to quantify the quality of the estimated uncertainty."],"url":"http://arxiv.org/abs/2403.07741v1","category":"cs.CV"}
{"created":"2024-03-12 15:11:47","title":"Performance Analysis of Matrix Multiplication for Deep Learning on the Edge","abstract":"The devices designed for the Internet-of-Things encompass a large variety of distinct processor architectures, forming a highly heterogeneous zoo. In order to tackle this, we employ a simulator to estimate the performance of the matrix-matrix multiplication (GEMM) kernel on processors designed to operate at the edge. Our simulator adheres to the modern implementations of GEMM, advocated by GotoBLAS2, BLIS, OpenBLAS, etc., to carefully account for the amount of data transfers across the memory hierarchy of different algorithmic variants of the kernel. %Armed with this tool, A small collection of experiments provide the necessary data to calibrate the simulator and deliver highly accurate estimations of the execution time for a given processor architecture.","sentences":["The devices designed for the Internet-of-Things encompass a large variety of distinct processor architectures, forming a highly heterogeneous zoo.","In order to tackle this, we employ a simulator to estimate the performance of the matrix-matrix multiplication (GEMM) kernel on processors designed to operate at the edge.","Our simulator adheres to the modern implementations of GEMM, advocated by GotoBLAS2, BLIS, OpenBLAS, etc., to carefully account for the amount of data transfers across the memory hierarchy of different algorithmic variants of the kernel.","%Armed with this tool, A small collection of experiments provide the necessary data to calibrate the simulator and deliver highly accurate estimations of the execution time for a given processor architecture."],"url":"http://arxiv.org/abs/2403.07731v1","category":"cs.AR"}
{"created":"2024-03-12 15:06:22","title":"SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes","abstract":"This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate. Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical. The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling.   The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task. We observe a number of key trends in how this approach was tackled -- many participants rely on a handful of model, and often rely either on synthetic data for fine-tuning or zero-shot prompting strategies. While a majority of the teams did outperform our proposed baseline system, the performances of top-scoring systems are still consistent with a random handling of the more challenging items.","sentences":["This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate.","Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical.","The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling.   ","The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task.","We observe a number of key trends in how this approach was tackled -- many participants rely on a handful of model, and often rely either on synthetic data for fine-tuning or zero-shot prompting strategies.","While a majority of the teams did outperform our proposed baseline system, the performances of top-scoring systems are still consistent with a random handling of the more challenging items."],"url":"http://arxiv.org/abs/2403.07726v1","category":"cs.CL"}
{"created":"2024-03-12 15:01:27","title":"Balancing Fairness and Accuracy in Data-Restricted Binary Classification","abstract":"Applications that deal with sensitive information may have restrictions placed on the data available to a machine learning (ML) classifier. For example, in some applications, a classifier may not have direct access to sensitive attributes, affecting its ability to produce accurate and fair decisions. This paper proposes a framework that models the trade-off between accuracy and fairness under four practical scenarios that dictate the type of data available for analysis. Prior works examine this trade-off by analyzing the outputs of a scoring function that has been trained to implicitly learn the underlying distribution of the feature vector, class label, and sensitive attribute of a dataset. In contrast, our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself. This approach enables us to formulate multiple convex optimization problems, which allow us to answer the question: How is the accuracy of a Bayesian classifier affected in different data restricting scenarios when constrained to be fair? Analysis is performed on a set of fairness definitions that include group and individual fairness. Experiments on three datasets demonstrate the utility of the proposed framework as a tool for quantifying the trade-offs among different fairness notions and their distributional dependencies.","sentences":["Applications that deal with sensitive information may have restrictions placed on the data available to a machine learning (ML) classifier.","For example, in some applications, a classifier may not have direct access to sensitive attributes, affecting its ability to produce accurate and fair decisions.","This paper proposes a framework that models the trade-off between accuracy and fairness under four practical scenarios that dictate the type of data available for analysis.","Prior works examine this trade-off by analyzing the outputs of a scoring function that has been trained to implicitly learn the underlying distribution of the feature vector, class label, and sensitive attribute of a dataset.","In contrast, our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself.","This approach enables us to formulate multiple convex optimization problems, which allow us to answer the question: How is the accuracy of a Bayesian classifier affected in different data restricting scenarios when constrained to be fair?","Analysis is performed on a set of fairness definitions that include group and individual fairness.","Experiments on three datasets demonstrate the utility of the proposed framework as a tool for quantifying the trade-offs among different fairness notions and their distributional dependencies."],"url":"http://arxiv.org/abs/2403.07724v1","category":"cs.LG"}
{"created":"2024-03-12 14:58:52","title":"Multi-modal Auto-regressive Modeling via Visual Words","abstract":"Large Language Models (LLMs), benefiting from the auto-regressive modelling approach performed on massive unannotated texts corpora, demonstrates powerful perceptual and reasoning capabilities. However, as for extending auto-regressive modelling to multi-modal scenarios to build Large Multi-modal Models (LMMs), there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification. In this paper, we successfully perform multi-modal auto-regressive modeling with a unified objective for the first time. Specifically, we propose the concept of visual words, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling. We further explore the distribution of visual features in the semantic space within LMM and the possibility of using text embeddings to represent visual information. Experimental results and ablation studies on 5 VQA tasks and 4 benchmark toolkits validate the powerful performance of our proposed approach.","sentences":["Large Language Models (LLMs), benefiting from the auto-regressive modelling approach performed on massive unannotated texts corpora, demonstrates powerful perceptual and reasoning capabilities.","However, as for extending auto-regressive modelling to multi-modal scenarios to build Large Multi-modal Models (LMMs), there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification.","In this paper, we successfully perform multi-modal auto-regressive modeling with a unified objective for the first time.","Specifically, we propose the concept of visual words, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling.","We further explore the distribution of visual features in the semantic space within LMM and the possibility of using text embeddings to represent visual information.","Experimental results and ablation studies on 5 VQA tasks and 4 benchmark toolkits validate the powerful performance of our proposed approach."],"url":"http://arxiv.org/abs/2403.07720v1","category":"cs.CV"}
{"created":"2024-03-12 14:58:45","title":"WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?","abstract":"We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.","sentences":["We study the use of large language model-based agents for interacting with software via web browsers.","Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems.","To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform.","We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations.","Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation.","Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field."],"url":"http://arxiv.org/abs/2403.07718v1","category":"cs.LG"}
{"created":"2024-03-12 14:53:56","title":"SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces","abstract":"Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.","sentences":["Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation.","Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features.","However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence.","This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models.","To overcome this challenge, we propose leveraging state-space models (SSMs).","SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length.","In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation.","In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150.","In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models.","Our codes are available at https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models."],"url":"http://arxiv.org/abs/2403.07711v1","category":"cs.CV"}
{"created":"2024-03-12 14:53:37","title":"Focusing Surface Acoustic Waves with a Plasmonic Hypersonic Lens","abstract":"Plasmonic nanoantennas have proven to be efficient transducers of electromagnetic to mechanical energy and vice versa. The sudden thermal expansion of these structures after an ultrafast optical pulsed excitation leads to the emission of hypersonic acoustic waves to the supporting substrate, which can be detected by another antenna that acts as a high-sensitive mechanical probe due to the strong modulation of its optical response. Sophisticated fabrication techniques, together with the implementation of numerical simulations, have allowed the engineering of nanostructures for the controlled directional generation and detection of high-frequency acoustic phonons at the nanoscale, with many potential applications in telecommunications, sensing, mechanical switching, and energy transport. Here, we propose and experimentally demonstrate a nanoscale acoustic lens comprised of 11 gold nanodisks whose collective oscillation gives rise to an interference pattern that results in a diffraction-limited surface acoustic beam of about 340 nm width, with an amplitude contrast of 60%. Via spatially decoupled pump-probe experiments, we were able to map the radiated acoustic energy in the proximity of the focal area, obtaining a very good agreement with the continuum elastic theory.","sentences":["Plasmonic nanoantennas have proven to be efficient transducers of electromagnetic to mechanical energy and vice versa.","The sudden thermal expansion of these structures after an ultrafast optical pulsed excitation leads to the emission of hypersonic acoustic waves to the supporting substrate, which can be detected by another antenna that acts as a high-sensitive mechanical probe due to the strong modulation of its optical response.","Sophisticated fabrication techniques, together with the implementation of numerical simulations, have allowed the engineering of nanostructures for the controlled directional generation and detection of high-frequency acoustic phonons at the nanoscale, with many potential applications in telecommunications, sensing, mechanical switching, and energy transport.","Here, we propose and experimentally demonstrate a nanoscale acoustic lens comprised of 11 gold nanodisks whose collective oscillation gives rise to an interference pattern that results in a diffraction-limited surface acoustic beam of about 340 nm width, with an amplitude contrast of 60%.","Via spatially decoupled pump-probe experiments, we were able to map the radiated acoustic energy in the proximity of the focal area, obtaining a very good agreement with the continuum elastic theory."],"url":"http://arxiv.org/abs/2403.07710v1","category":"physics.optics"}
{"created":"2024-03-12 14:51:57","title":"Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards","abstract":"Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \\textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and reduce variance in PPO. We show empirically contrastive rewards can improve RLHF substantially, evaluated by both GPTs and humans, and our method consistently outperforms strong baselines.","sentences":["Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences.","Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile.","In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \\textit{contrastive rewards}.","%Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step.","We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and reduce variance in PPO.","We show empirically contrastive rewards can improve RLHF substantially, evaluated by both GPTs and humans, and our method consistently outperforms strong baselines."],"url":"http://arxiv.org/abs/2403.07708v1","category":"cs.CL"}
{"created":"2024-03-12 14:49:19","title":"Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning","abstract":"In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution.","sentences":["In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential.","The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution.","However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method.","To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution.","We evaluated the proposed method on continuous control benchmark tasks in MuJoCo.","It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution."],"url":"http://arxiv.org/abs/2403.07704v1","category":"cs.LG"}
{"created":"2024-03-12 14:46:03","title":"CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers","abstract":"In this paper, we introduce VoteCut, an innovative method for unsupervised object discovery that leverages feature representations from multiple self-supervised models. VoteCut employs normalized-cut based graph partitioning, clustering and a pixel voting approach. Additionally, We present CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels, generated by VoteCut, and a novel soft target loss to refine segmentation accuracy. Through rigorous evaluations across multiple datasets and several unsupervised setups, our methods demonstrate significant improvements in comparison to previous state-of-the-art models. Our ablation studies further highlight the contributions of each component, revealing the robustness and efficacy of our approach. Collectively, VoteCut and CuVLER pave the way for future advancements in image segmentation.","sentences":["In this paper, we introduce VoteCut, an innovative method for unsupervised object discovery that leverages feature representations from multiple self-supervised models.","VoteCut employs normalized-cut based graph partitioning, clustering and a pixel voting approach.","Additionally, We present CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels, generated by VoteCut, and a novel soft target loss to refine segmentation accuracy.","Through rigorous evaluations across multiple datasets and several unsupervised setups, our methods demonstrate significant improvements in comparison to previous state-of-the-art models.","Our ablation studies further highlight the contributions of each component, revealing the robustness and efficacy of our approach.","Collectively, VoteCut and CuVLER pave the way for future advancements in image segmentation."],"url":"http://arxiv.org/abs/2403.07700v1","category":"cs.CV"}
{"created":"2024-03-12 14:37:03","title":"Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization","abstract":"As more than 70$\\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated data. After training, a large amount of synthetic data can be obtained by decoding the new representation obtained from the combination of different sample representations and filtering based on confusion degree and sentiment classification. Experiments have proved that our framework can effectively alleviate emotional bias same as using only large models, but more economically.","sentences":["As more than 70$\\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts.","To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset.","However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs.","Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization.","In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model.","Then, a disentangle reconstruction model is trained based on the generated data.","After training, a large amount of synthetic data can be obtained by decoding the new representation obtained from the combination of different sample representations and filtering based on confusion degree and sentiment classification.","Experiments have proved that our framework can effectively alleviate emotional bias same as using only large models, but more economically."],"url":"http://arxiv.org/abs/2403.07693v1","category":"cs.CL"}
{"created":"2024-03-12 14:34:08","title":"Reference-free Monolithic Preference Optimization with Odds Ratio","abstract":"While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20% on $\\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12. We release code and model checkpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B).","sentences":["While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence.","In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT.","Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase.","We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20% on $\\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12.","We release code and model checkpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B)."],"url":"http://arxiv.org/abs/2403.07691v1","category":"cs.CL"}
{"created":"2024-03-12 14:28:06","title":"Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons","abstract":"When training deep neural networks, the phenomenon of $\\textit{dying neurons}$ $\\unicode{x2013}$units that become inactive or saturated, output zero during training$\\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 and ImageNet datasets demonstrate that DemP surpasses existing structured pruning techniques, showcasing superior accuracy-sparsity tradeoffs and training speedups. These findings suggest a novel perspective on dying neurons as a valuable resource for efficient model compression and optimization.","sentences":["When training deep neural networks, the phenomenon of $\\textit{dying neurons}$ $\\unicode{x2013}$units that become inactive or saturated, output zero during training$\\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios.","In this paper, we reassess this phenomenon, focusing on sparsity and pruning.","By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms.","We introduce $\\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity.","Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability.","Experiments on CIFAR10","and ImageNet datasets demonstrate that DemP surpasses existing structured pruning techniques, showcasing superior accuracy-sparsity tradeoffs and training speedups.","These findings suggest a novel perspective on dying neurons as a valuable resource for efficient model compression and optimization."],"url":"http://arxiv.org/abs/2403.07688v1","category":"cs.LG"}
{"created":"2024-03-12 14:27:17","title":"Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model Performance and Annotation Cost","abstract":"Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs. The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual_diversity_budget.","sentences":["Current foundation models have shown impressive performance across various tasks.","However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process.","Most of this data comes from Western countries, leading to poor results for underrepresented countries.","To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck.","In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs.","Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models.","Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs.","The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual_diversity_budget."],"url":"http://arxiv.org/abs/2403.07687v1","category":"cs.CV"}
{"created":"2024-03-12 14:18:19","title":"Adapting LoRaWAN to the Open-RAN Architecture","abstract":"This article proposes O-LoRaWAN, an adaptation of the LoRaWAN architecture into a modular network architecture based on the Open RAN (O-RAN) principles. In our vision, standardization of the network components and interfaces will enable the reuse of network functions, and thus, foster an accelerated tailoring of the network functions to the changing application demands. LoRaWAN shares similarities to cellular networks and becomes an interesting candidate for a transformation to the O-RAN standard. In the article we draw several transition strategies, these include the reorganization of the LoRa gateway functions into Radio and Distributed Units; enhancing network performance with RAN Intelligent Controllers exploiting the network data; and the standardization of the management and orchestration of network components. Key for that adaptation are the O-RAN interfaces. Along the article, we analyze them and suggest protocol extensions or adjustments for compatibility and interoperability between network components, advocating for the design of extensible protocols","sentences":["This article proposes O-LoRaWAN, an adaptation of the LoRaWAN architecture into a modular network architecture based on the Open RAN (O-RAN) principles.","In our vision, standardization of the network components and interfaces will enable the reuse of network functions, and thus, foster an accelerated tailoring of the network functions to the changing application demands.","LoRaWAN shares similarities to cellular networks and becomes an interesting candidate for a transformation to the O-RAN standard.","In the article we draw several transition strategies, these include the reorganization of the LoRa gateway functions into Radio and Distributed Units; enhancing network performance with RAN Intelligent Controllers exploiting the network data; and the standardization of the management and orchestration of network components.","Key for that adaptation are the O-RAN interfaces.","Along the article, we analyze them and suggest protocol extensions or adjustments for compatibility and interoperability between network components, advocating for the design of extensible protocols"],"url":"http://arxiv.org/abs/2403.07680v1","category":"cs.NI"}
{"created":"2024-03-12 13:54:25","title":"Enabling self-identification in intelligent agent: insights from computational psychoanalysis","abstract":"Building upon prior framework of computational Lacanian psychoanalysis with the theory of active inference, this paper aims to further explore the concept of self-identification and its potential applications. Beginning with two classic paradigms in psychology, mirror self-recognition and rubber hand illusion, we suggest that imaginary identification is characterized by an integrated body schema with minimal free energy. Next, we briefly survey three dimensions of symbolic identification (sociological, psychoanalytic, and linguistical) and corresponding active inference accounts. To provide intuition, we respectively employ a convolutional neural network (CNN) and a multi-layer perceptron (MLP) supervised by ChatGPT to showcase optimization of free energy during motor skill and language mastery underlying identification formation. We then introduce Lacan's Graph II of desire, unifying imaginary and symbolic identification, and propose an illustrative model called FreeAgent. In concluding remarks, we discuss some key issues in the potential of computational Lacanian psychoanalysis to advance mental health and artificial intelligence, including digital twin mind, large language models as avatars of the Lacanian Other, and the feasibility of human-level artificial general intelligence with self-awareness in the context of post-structuralism.","sentences":["Building upon prior framework of computational Lacanian psychoanalysis with the theory of active inference, this paper aims to further explore the concept of self-identification and its potential applications.","Beginning with two classic paradigms in psychology, mirror self-recognition and rubber hand illusion, we suggest that imaginary identification is characterized by an integrated body schema with minimal free energy.","Next, we briefly survey three dimensions of symbolic identification (sociological, psychoanalytic, and linguistical) and corresponding active inference accounts.","To provide intuition, we respectively employ a convolutional neural network (CNN) and a multi-layer perceptron (MLP) supervised by ChatGPT to showcase optimization of free energy during motor skill and language mastery underlying identification formation.","We then introduce Lacan's Graph II of desire, unifying imaginary and symbolic identification, and propose an illustrative model called FreeAgent.","In concluding remarks, we discuss some key issues in the potential of computational Lacanian psychoanalysis to advance mental health and artificial intelligence, including digital twin mind, large language models as avatars of the Lacanian Other, and the feasibility of human-level artificial general intelligence with self-awareness in the context of post-structuralism."],"url":"http://arxiv.org/abs/2403.07664v1","category":"q-bio.NC"}
{"created":"2024-03-12 13:47:50","title":"Scalable Spatiotemporal Prediction with Bayesian Neural Fields","abstract":"Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent. We evaluate BayesNF against prominent statistical and machine-learning baselines, showing considerable improvements on diverse prediction problems from climate and public health datasets that contain tens to hundreds of thousands of measurements. The paper is accompanied with an open-source software package (https://github.com/google/bayesnf) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform.","sentences":["Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting.","As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems.","This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography.","BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification.","By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent.","We evaluate BayesNF against prominent statistical and machine-learning baselines, showing considerable improvements on diverse prediction problems from climate and public health datasets that contain tens to hundreds of thousands of measurements.","The paper is accompanied with an open-source software package (https://github.com/google/bayesnf) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform."],"url":"http://arxiv.org/abs/2403.07657v1","category":"cs.LG"}
{"created":"2024-03-12 13:31:14","title":"Characterization of Large Language Model Development in the Datacenter","abstract":"Large Language Models (LLMs) have presented impressive performance across several transformative tasks. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. In this paper, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures. Our analysis summarizes hurdles we encountered and uncovers potential opportunities to optimize systems tailored for LLMs. Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through LLM-involved failure diagnosis and automatic recovery. (2) decoupled scheduling for evaluation, which achieves timely performance feedback via trial decomposition and scheduling optimization.","sentences":["Large Language Models (LLMs) have presented impressive performance across several transformative tasks.","However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization.","In this paper, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme.","Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures.","Our analysis summarizes hurdles we encountered and uncovers potential opportunities to optimize systems tailored for LLMs.","Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through LLM-involved failure diagnosis and automatic recovery.","(2) decoupled scheduling for evaluation, which achieves timely performance feedback via trial decomposition and scheduling optimization."],"url":"http://arxiv.org/abs/2403.07648v1","category":"cs.DC"}
{"created":"2024-03-12 13:11:58","title":"Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation","abstract":"Recent weakly supervised semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM). In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics. Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances. The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias. Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions. With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension. The core of this method is to accurately capture intra-class variations in object features through context-aware prototypes, facilitating the adaptation to the semantic attributes of various instances. We design feature distribution alignment to optimize prototype awareness, aligning instance feature distributions with dense features. In addition, a unified training framework is proposed to combine label-guided classification supervision and prototypes-guided self-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show that CPAL significantly improves off-the-shelf methods and achieves state-of-the-art performance. The project is available at https://github.com/Barrett-python/CPAL.","sentences":["Recent weakly supervised semantic segmentation (WSSS) methods strive to incorporate contextual knowledge to improve the completeness of class activation maps (CAM).","In this work, we argue that the knowledge bias between instances and contexts affects the capability of the prototype to sufficiently understand instance semantics.","Inspired by prototype learning theory, we propose leveraging prototype awareness to capture diverse and fine-grained feature attributes of instances.","The hypothesis is that contextual prototypes might erroneously activate similar and frequently co-occurring object categories due to this knowledge bias.","Therefore, we propose to enhance the prototype representation ability by mitigating the bias to better capture spatial coverage in semantic object regions.","With this goal, we present a Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic context to enrich instance comprehension.","The core of this method is to accurately capture intra-class variations in object features through context-aware prototypes, facilitating the adaptation to the semantic attributes of various instances.","We design feature distribution alignment to optimize prototype awareness, aligning instance feature distributions with dense features.","In addition, a unified training framework is proposed to combine label-guided classification supervision and prototypes-guided self-supervision.","Experimental results on PASCAL VOC 2012 and MS COCO 2014 show that CPAL significantly improves off-the-shelf methods and achieves state-of-the-art performance.","The project is available at https://github.com/Barrett-python/CPAL."],"url":"http://arxiv.org/abs/2403.07630v1","category":"cs.CV"}
{"created":"2024-03-12 13:05:51","title":"Multiple Latent Space Mapping for Compressed Dark Image Enhancement","abstract":"Dark image enhancement aims at converting dark images to normal-light images. Existing dark image enhancement methods take uncompressed dark images as inputs and achieve great performance. However, in practice, dark images are often compressed before storage or transmission over the Internet. Current methods get poor performance when processing compressed dark images. Artifacts hidden in the dark regions are amplified by current methods, which results in uncomfortable visual effects for observers. Based on this observation, this study aims at enhancing compressed dark images while avoiding compression artifacts amplification. Since texture details intertwine with compression artifacts in compressed dark images, detail enhancement and blocking artifacts suppression contradict each other in image space. Therefore, we handle the task in latent space. To this end, we propose a novel latent mapping network based on variational auto-encoder (VAE). Firstly, different from previous VAE-based methods with single-resolution features only, we exploit multiple latent spaces with multi-resolution features, to reduce the detail blur and improve image fidelity. Specifically, we train two multi-level VAEs to project compressed dark images and normal-light images into their latent spaces respectively. Secondly, we leverage a latent mapping network to transform features from compressed dark space to normal-light space. Specifically, since the degradation models of darkness and compression are different from each other, the latent mapping process is divided mapping into enlightening branch and deblocking branch. Comprehensive experiments demonstrate that the proposed method achieves state-of-the-art performance in compressed dark image enhancement.","sentences":["Dark image enhancement aims at converting dark images to normal-light images.","Existing dark image enhancement methods take uncompressed dark images as inputs and achieve great performance.","However, in practice, dark images are often compressed before storage or transmission over the Internet.","Current methods get poor performance when processing compressed dark images.","Artifacts hidden in the dark regions are amplified by current methods, which results in uncomfortable visual effects for observers.","Based on this observation, this study aims at enhancing compressed dark images while avoiding compression artifacts amplification.","Since texture details intertwine with compression artifacts in compressed dark images, detail enhancement and blocking artifacts suppression contradict each other in image space.","Therefore, we handle the task in latent space.","To this end, we propose a novel latent mapping network based on variational auto-encoder (VAE).","Firstly, different from previous VAE-based methods with single-resolution features only, we exploit multiple latent spaces with multi-resolution features, to reduce the detail blur and improve image fidelity.","Specifically, we train two multi-level VAEs to project compressed dark images and normal-light images into their latent spaces respectively.","Secondly, we leverage a latent mapping network to transform features from compressed dark space to normal-light space.","Specifically, since the degradation models of darkness and compression are different from each other, the latent mapping process is divided mapping into enlightening branch and deblocking branch.","Comprehensive experiments demonstrate that the proposed method achieves state-of-the-art performance in compressed dark image enhancement."],"url":"http://arxiv.org/abs/2403.07622v1","category":"cs.CV"}
{"created":"2024-03-12 13:04:37","title":"Smartphone region-wise image indoor localization using deep learning for indoor tourist attraction","abstract":"Smart indoor tourist attractions, such as smart museums and aquariums, usually require a significant investment in indoor localization devices. The smartphone Global Positional Systems use is unsuitable for scenarios where dense materials such as concrete and metal block weaken the GPS signals, which is the most common scenario in an indoor tourist attraction. Deep learning makes it possible to perform region-wise indoor localization using smartphone images. This approach does not require any investment in infrastructure, reducing the cost and time to turn museums and aquariums into smart museums or smart aquariums. This paper proposes using deep learning algorithms to classify locations using smartphone camera images for indoor tourism attractions. We evaluate our proposal in a real-world scenario in Brazil. We extensively collect images from ten different smartphones to classify biome-themed fish tanks inside the Pantanal Biopark, creating a new dataset of 3654 images. We tested seven state-of-the-art neural networks, three being transformer-based, achieving precision around 90% on average and recall and f-score around 89% on average. The results indicate good feasibility of the proposal in a most indoor tourist attractions.","sentences":["Smart indoor tourist attractions, such as smart museums and aquariums, usually require a significant investment in indoor localization devices.","The smartphone Global Positional Systems use is unsuitable for scenarios where dense materials such as concrete and metal block weaken the GPS signals, which is the most common scenario in an indoor tourist attraction.","Deep learning makes it possible to perform region-wise indoor localization using smartphone images.","This approach does not require any investment in infrastructure, reducing the cost and time to turn museums and aquariums into smart museums or smart aquariums.","This paper proposes using deep learning algorithms to classify locations using smartphone camera images for indoor tourism attractions.","We evaluate our proposal in a real-world scenario in Brazil.","We extensively collect images from ten different smartphones to classify biome-themed fish tanks inside the Pantanal Biopark, creating a new dataset of 3654 images.","We tested seven state-of-the-art neural networks, three being transformer-based, achieving precision around 90% on average and recall and f-score around 89% on average.","The results indicate good feasibility of the proposal in a most indoor tourist attractions."],"url":"http://arxiv.org/abs/2403.07621v1","category":"cs.CV"}
{"created":"2024-03-12 12:49:47","title":"Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning","abstract":"Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model. This capability enables data holders to adhere strictly to data protection regulations. However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage. In response, this paper introduces a novel class of machine unlearning algorithms. First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning. In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model. The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of data deletion on model efficacy. Through a detailed experimental evaluation, we showcase the effectiveness of proposed unlearning methods. Experimental results highlight that the partial amnesiac unlearning not only preserves model efficacy but also eliminates the necessity for brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover, employing layer-wise partial updates in label-flipping and optimization-based unlearning techniques demonstrates superiority in preserving model efficacy compared to their naive counterparts.","sentences":["Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model.","This capability enables data holders to adhere strictly to data protection regulations.","However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage.","In response, this paper introduces a novel class of machine unlearning algorithms.","First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning.","In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model.","The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of data deletion on model efficacy.","Through a detailed experimental evaluation, we showcase the effectiveness of proposed unlearning methods.","Experimental results highlight that the partial amnesiac unlearning not only preserves model efficacy but also eliminates the necessity for brief post fine-tuning, unlike conventional amnesiac unlearning.","Moreover, employing layer-wise partial updates in label-flipping and optimization-based unlearning techniques demonstrates superiority in preserving model efficacy compared to their naive counterparts."],"url":"http://arxiv.org/abs/2403.07611v1","category":"cs.LG"}
{"created":"2024-03-12 12:49:47","title":"Minimal cellular automaton model with heterogeneous cell sizes predicts epithelial colony growth","abstract":"Regulation of cell proliferation is a crucial aspect of tissue development and homeostasis and plays a major role in morphogenesis, wound healing, and tumor invasion. A phenomenon of such regulation is contact inhibition, which describes the dramatic slowing of proliferation, cell migration and individual cell growth when multiple cells are in contact with each other. While many physiological, molecular and genetic factors are known, the mechanism of contact inhibition is still not fully understood. In particular, the relevance of cellular signaling due to interfacial contact for contact inhibition is still debated. Cellular automata (CA) have been employed in the past as numerically efficient mathematical models to study the dynamics of cell ensembles, but they are not suitable to explore the origins of contact inhibition as such agent-based models assume fixed cell sizes. We develop a minimal, data-driven model to simulate the dynamics of planar cell cultures by extending a probabilistic CA to incorporate size changes of individual cells during growth and cell division. We successfully apply this model to previous in-vitro experiments on contact inhibition in epithelial tissue: After a systematic calibration of the model parameters to measurements of single-cell dynamics, our CA model quantitatively reproduces independent measurements of emergent, culture-wide features, like colony size, cell density and collective cell migration. In particular, the dynamics of the CA model also exhibit the transition from a low-density confluent regime to a stationary postconfluent regime with a rapid decrease in cell size and motion. This implies that the volume exclusion principle, a mechanical constraint which is the only inter-cellular interaction incorporated in the model, paired with a size-dependent proliferation rate is sufficient to generate the observed contact inhibition.","sentences":["Regulation of cell proliferation is a crucial aspect of tissue development and homeostasis and plays a major role in morphogenesis, wound healing, and tumor invasion.","A phenomenon of such regulation is contact inhibition, which describes the dramatic slowing of proliferation, cell migration and individual cell growth when multiple cells are in contact with each other.","While many physiological, molecular and genetic factors are known, the mechanism of contact inhibition is still not fully understood.","In particular, the relevance of cellular signaling due to interfacial contact for contact inhibition is still debated.","Cellular automata (CA) have been employed in the past as numerically efficient mathematical models to study the dynamics of cell ensembles, but they are not suitable to explore the origins of contact inhibition as such agent-based models assume fixed cell sizes.","We develop a minimal, data-driven model to simulate the dynamics of planar cell cultures by extending a probabilistic CA to incorporate size changes of individual cells during growth and cell division.","We successfully apply this model to previous in-vitro experiments on contact inhibition in epithelial tissue: After a systematic calibration of the model parameters to measurements of single-cell dynamics, our CA model quantitatively reproduces independent measurements of emergent, culture-wide features, like colony size, cell density and collective cell migration.","In particular, the dynamics of the CA model also exhibit the transition from a low-density confluent regime to a stationary postconfluent regime with a rapid decrease in cell size and motion.","This implies that the volume exclusion principle, a mechanical constraint which is the only inter-cellular interaction incorporated in the model, paired with a size-dependent proliferation rate is sufficient to generate the observed contact inhibition."],"url":"http://arxiv.org/abs/2403.07612v1","category":"q-bio.CB"}
{"created":"2024-03-12 12:47:32","title":"Couler: Unified Machine Learning Workflow Optimization in Cloud","abstract":"Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.   In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud. Our main insight lies in the ability to generate an ML workflow using natural language (NL) descriptions. We integrate Large Language Models (LLMs) into workflow generation, and provide a unified programming interface for various workflow engines. This approach alleviates the need to understand various workflow engines' APIs. Moreover, Couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling large workflow auto-parallelization and automatic hyperparameters tuning. These enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training. Couler is extensively deployed in real-world production scenarios at Ant Group, handling approximately 22k workflows daily, and has successfully improved the CPU/Memory utilization by more than 15% and the workflow completion rate by around 17%.","sentences":["Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations.","Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming.","Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs.","Currently, numerous workflow engines are available (with over ten being widely recognized).","This variety poses a challenge for end-users in terms of mastering different engine APIs.","While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.   ","In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud.","Our main insight lies in the ability to generate an ML workflow using natural language (NL) descriptions.","We integrate Large Language Models (LLMs) into workflow generation, and provide a unified programming interface for various workflow engines.","This approach alleviates the need to understand various workflow engines' APIs.","Moreover, Couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling large workflow auto-parallelization and automatic hyperparameters tuning.","These enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training.","Couler is extensively deployed in real-world production scenarios at Ant Group, handling approximately 22k workflows daily, and has successfully improved the CPU/Memory utilization by more than 15% and the workflow completion rate by around 17%."],"url":"http://arxiv.org/abs/2403.07608v1","category":"cs.DB"}
{"created":"2024-03-12 12:46:24","title":"Bilocal holography and locality in the bulk","abstract":"Bilocal holography provides a constructive approach to the vector model/higher spin gravity duality. It has two ingredients: a change of field variables and a change of space time coordinates. The change of field variables ensures that the loop expansion parameter becomes ${1\\over N}$. The change of coordinates solves the Clebsch-Gordan problem of moving from the tensor product basis (in which the collective bilocal field is written) to the direct sum basis (appropriate for the description of the gravity fields). We argue that the change of space time coordinates can be deduced by requiring that operators constructed in the bilocal collective field theory are dual to local operators in the AdS bulk.","sentences":["Bilocal holography provides a constructive approach to the vector model/higher spin gravity duality.","It has two ingredients: a change of field variables and a change of space time coordinates.","The change of field variables ensures that the loop expansion parameter becomes ${1\\over N}$. The change of coordinates solves the Clebsch-Gordan problem of moving from the tensor product basis (in which the collective bilocal field is written) to the direct sum basis (appropriate for the description of the gravity fields).","We argue that the change of space time coordinates can be deduced by requiring that operators constructed in the bilocal collective field theory are dual to local operators in the AdS bulk."],"url":"http://arxiv.org/abs/2403.07606v1","category":"hep-th"}
{"created":"2024-03-12 12:44:34","title":"Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation","abstract":"In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.","sentences":["In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality.","However, producing good negative prompts is manual and tedious.","To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning.","Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set.","Furthermore, with NegOpt we can preferentially optimize the metrics most important to us.","Finally, we construct Negative Prompts DB, a dataset of negative prompts."],"url":"http://arxiv.org/abs/2403.07605v1","category":"cs.CV"}
{"created":"2024-03-12 12:35:12","title":"Mondrian: On-Device High-Performance Video Analytics with Compressive Packed Inference","abstract":"In this paper, we present Mondrian, an edge system that enables high-performance object detection on high-resolution video streams. Many lightweight models and system optimization techniques have been proposed for resource-constrained devices, but they do not fully utilize the potential of the accelerators over dynamic, high-resolution videos. To enable such capability, we devise a novel Compressive Packed Inference to minimize per-pixel processing costs by selectively determining the necessary pixels to process and combining them to maximize processing parallelism. In particular, our system quickly extracts ROIs and dynamically shrinks them, reflecting the effect of the fast-changing characteristics of objects and scenes. It then intelligently combines such scaled ROIs into large canvases to maximize the utilization of inference accelerators such as GPU. Evaluation across various datasets, models, and devices shows Mondrian outperforms state-of-the-art baselines (e.g., input rescaling, ROI extractions, ROI extractions+batching) by 15.0-19.7% higher accuracy, leading to $\\times$6.65 higher throughput than frame-wise inference for processing various 1080p video streams. We will release the code after the paper review.","sentences":["In this paper, we present Mondrian, an edge system that enables high-performance object detection on high-resolution video streams.","Many lightweight models and system optimization techniques have been proposed for resource-constrained devices, but they do not fully utilize the potential of the accelerators over dynamic, high-resolution videos.","To enable such capability, we devise a novel Compressive Packed Inference to minimize per-pixel processing costs by selectively determining the necessary pixels to process and combining them to maximize processing parallelism.","In particular, our system quickly extracts ROIs and dynamically shrinks them, reflecting the effect of the fast-changing characteristics of objects and scenes.","It then intelligently combines such scaled ROIs into large canvases to maximize the utilization of inference accelerators such as GPU.","Evaluation across various datasets, models, and devices shows Mondrian outperforms state-of-the-art baselines (e.g., input rescaling, ROI extractions, ROI extractions+batching) by 15.0-19.7% higher accuracy, leading to $\\times$6.65 higher throughput than frame-wise inference for processing various 1080p video streams.","We will release the code after the paper review."],"url":"http://arxiv.org/abs/2403.07598v1","category":"cs.CV"}
{"created":"2024-03-12 12:34:02","title":"Generalized paths and cycles in semicomplete multipartite digraphs","abstract":"It is well-known and easy to show that even the following version of the directed travelling salesman problem is NP-complete: Given a strongly connected complete digraph $D=(V,A)$, a cost function $w: A\\rightarrow \\{0,1\\}$ and a natural number $K$; decide whether $D$ has a directed Hamiltonian cycle of cost at most $K$. We study the following variant of this problem for $\\{0,1\\}$-weighted semicomplete digraphs where the set of arcs which have cost 1 form a collection of vertex-disjoint complete digraphs. A digraph is \\textbf{semicomplete multipartite} if it can be obtained from a semicomplete digraph $D$ by choosing a collection of vertex-disjoint subsets $X_1,\\ldots{},X_c$ of $V(D)$ and then deleting all arcs both of whose end-vertices lie inside some $X_i$. Let $D$ be a semicomplete digraph with a cost function $w$ as above, where $w(a)=1$ precisely when $a$ is an arc inside one of the subsets $X_1,\\ldots{},X_c$ and let $D^*$ be the corresponding \\smd{} that we obtain by deleting all arcs inside the $X_i$'s. Then every cycle $C$ of $D$ corresponds to a {\\bf generalized cycle} $C^g$ of $D^*$ which is either the cycle $C$ itself if $w(C)=0$ or a collection of two or more paths that we obtain by deleting all arcs of cost 1 on $C$. Similarly we can define a {\\bf generalized path} $P^g$ in a semicomplete multipartite digraph. The purpose of this paper is to study structural and algorithmic properties of generalized paths and cycles in semicomplete multipartite digraphs. This allows us to identify classes of directed $\\{0,1\\}$-weighted TSP instances that can be solved in polynomial time as well as others for which we can get very close to the optimum in polynomial time. Along with these results we also show that two natural questions about properties of cycles meeting all partite sets in semicomplete multipartite digraphs are NP-complete.","sentences":["It is well-known and easy to show that even the following version of the directed travelling salesman problem is NP-complete: Given a strongly connected complete digraph $D=(V,A)$, a cost function $w: A\\rightarrow \\{0,1\\}$ and a natural number $K$; decide whether $D$ has a directed Hamiltonian cycle of cost at most $K$. We study the following variant of this problem for $\\{0,1\\}$-weighted semicomplete digraphs where the set of arcs which have cost 1 form a collection of vertex-disjoint complete digraphs.","A digraph is \\textbf{semicomplete multipartite} if it can be obtained from a semicomplete digraph $D$ by choosing a collection of vertex-disjoint subsets $X_1,\\ldots{},X_c$ of $V(D)$ and then deleting all arcs both of whose end-vertices lie inside some $X_i$. Let $D$ be a semicomplete digraph with a cost function $w$ as above, where $w(a)=1$ precisely when $a$ is an arc inside one of the subsets $X_1,\\ldots{},X_c$ and let $D^*$ be the corresponding \\smd{} that we obtain by deleting all arcs inside the $X_i$'s.","Then every cycle $C$ of $D$ corresponds to a {\\bf generalized cycle} $C^g$ of $D^*$ which is either the cycle $C$ itself if $w(C)=0$ or a collection of two or more paths that we obtain by deleting all arcs of cost 1 on $C$.","Similarly we can define a {\\bf generalized path} $P^g$ in a semicomplete multipartite digraph.","The purpose of this paper is to study structural and algorithmic properties of generalized paths and cycles in semicomplete multipartite digraphs.","This allows us to identify classes of directed $\\{0,1\\}$-weighted TSP instances that can be solved in polynomial time as well as others for which we can get very close to the optimum in polynomial time.","Along with these results we also show that two natural questions about properties of cycles meeting all partite sets in semicomplete multipartite digraphs are NP-complete."],"url":"http://arxiv.org/abs/2403.07597v1","category":"math.CO"}
{"created":"2024-03-12 12:18:20","title":"Perennial Semantic Data Terms of Use for Decentralized Web","abstract":"In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives policies for output data. This constitutes a ``perennial'' DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles. Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine. It ensures seamless integration with other semantic tools for enhanced interoperability. We have successfully integrated this language into the Solid framework, and conducted performance benchmark. We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability.","sentences":["In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations.","Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods'.","However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods.","This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore.","This compromises user autonomy and impedes detection of data misuse.","We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner.","Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations.","Automated reasoning verifies compliance, and also derives policies for output data.","This constitutes a ``perennial'' DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles.","Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine.","It ensures seamless integration with other semantic tools for enhanced interoperability.","We have successfully integrated this language into the Solid framework, and conducted performance benchmark.","We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability."],"url":"http://arxiv.org/abs/2403.07587v1","category":"cs.AI"}
{"created":"2024-03-12 12:16:40","title":"Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments","abstract":"As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial. For their widespread open-world application, it is important to explore Federated Learning (FL) settings where individual robots can learn about their unique environments while also learning from each others' experiences. In this paper, we present a novel FL benchmark that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others. Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel Federated Continual Learning (FCL) benchmark that adapts FL-based methods to use state-of-the-art Continual Learning (CL) methods to continually learn socially appropriate agent behaviours under different contextual settings. Federated Averaging (FedAvg) of weights emerges as a robust FL strategy while rehearsal-based FCL enables incrementally learning the social appropriateness of robot actions, across contextual splits.","sentences":["As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial.","For their widespread open-world application, it is important to explore Federated Learning (FL) settings where individual robots can learn about their unique environments while also learning from each others' experiences.","In this paper, we present a novel FL benchmark that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others.","Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel Federated Continual Learning (FCL) benchmark that adapts FL-based methods to use state-of-the-art Continual Learning (CL) methods to continually learn socially appropriate agent behaviours under different contextual settings.","Federated Averaging (FedAvg) of weights emerges as a robust FL strategy while rehearsal-based FCL enables incrementally learning the social appropriateness of robot actions, across contextual splits."],"url":"http://arxiv.org/abs/2403.07586v1","category":"cs.LG"}
{"created":"2024-03-12 12:15:57","title":"Communication Optimization for Distributed Training: Architecture, Advances, and Opportunities","abstract":"The past few years have witnessed the flourishing of large-scale deep neural network models with ever-growing parameter numbers. Training such large-scale models typically requires massive memory and computing resources that exceed those of a single GPU, necessitating distributed training. As GPU performance has rapidly evolved in recent years, computation time has shrunk, thereby increasing the proportion of communication in the overall training time. Therefore, optimizing communication for distributed training has become an urgent issue. In this article, we briefly introduce the general architecture of distributed deep neural network training and analyze relationships among Parallelization Strategy, Collective Communication Library, and Network from the perspective of communication optimization, which forms a three-layer paradigm. We then review current representative research advances with this three-layer paradigm. We find that layers in the current three-layer paradigm are relatively independent, but there is a rich design space for cross-layer collaborative optimization in distributed training scenarios. Therefore, we further advocate a communication-efficient five-layer paradigm underlining opportunities for collaboration designs and look forward to the perspectives of \"Vertical\", \"Horizontal\", \"Intra-Inter\" and \"Host-Net\" collaboration designs. We hope this article can shed some light on future research on communication optimization for distributed training.","sentences":["The past few years have witnessed the flourishing of large-scale deep neural network models with ever-growing parameter numbers.","Training such large-scale models typically requires massive memory and computing resources that exceed those of a single GPU, necessitating distributed training.","As GPU performance has rapidly evolved in recent years, computation time has shrunk, thereby increasing the proportion of communication in the overall training time.","Therefore, optimizing communication for distributed training has become an urgent issue.","In this article, we briefly introduce the general architecture of distributed deep neural network training and analyze relationships among Parallelization Strategy, Collective Communication Library, and Network from the perspective of communication optimization, which forms a three-layer paradigm.","We then review current representative research advances with this three-layer paradigm.","We find that layers in the current three-layer paradigm are relatively independent, but there is a rich design space for cross-layer collaborative optimization in distributed training scenarios.","Therefore, we further advocate a communication-efficient five-layer paradigm underlining opportunities for collaboration designs and look forward to the perspectives of \"Vertical\", \"Horizontal\", \"Intra-Inter\" and \"Host-Net\" collaboration designs.","We hope this article can shed some light on future research on communication optimization for distributed training."],"url":"http://arxiv.org/abs/2403.07585v1","category":"cs.DC"}
{"created":"2024-03-12 12:10:18","title":"LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model","abstract":"Personality detection aims to detect one's personality traits underlying in social media posts. One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires. Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance. In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them. In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even when the LLM fails in this task. Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, and linguistic, which are critical for personality detection. By using contrastive learning to pull them together in the embedding space, the post encoder can better capture the psycho-linguistic information within the post representations, thus improving personality detection. Furthermore, we utilize the LLM to enrich the information of personality labels for enhancing the detection performance. Experimental results on the benchmark datasets demonstrate that our model outperforms the state-of-the-art methods on personality detection.","sentences":["Personality detection aims to detect one's personality traits underlying in social media posts.","One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires.","Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels.","This leads to inferior quality of post features and consequently affects the performance.","In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them.","In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even when the LLM fails in this task.","Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, and linguistic, which are critical for personality detection.","By using contrastive learning to pull them together in the embedding space, the post encoder can better capture the psycho-linguistic information within the post representations, thus improving personality detection.","Furthermore, we utilize the LLM to enrich the information of personality labels for enhancing the detection performance.","Experimental results on the benchmark datasets demonstrate that our model outperforms the state-of-the-art methods on personality detection."],"url":"http://arxiv.org/abs/2403.07581v1","category":"cs.CL"}
{"created":"2024-03-12 12:03:16","title":"Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)","abstract":"In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent requirements. ACNC encompasses two primary functionalities: state recognition and context detection. Given the intricate nature of the user-service-computing-network space, the paper employs dimension reduction to generate live, holistic, abstract system states in a hierarchical structure. To address the challenges posed by dynamic changes, Continual Learning (CL) is employed, classifying the system state into contexts controlled by dedicated ML agents, enabling them to operate efficiently. These two functionalities are intricately linked within a closed loop overseen by the End-to-End (E2E) orchestrator to allocate resources. The paper introduces the components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow, details a numerical analysis for efficiency assessment, and concludes with discussions on relevant challenges and potential avenues for future research.","sentences":["In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites.","The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration.","While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources.","Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent requirements.","ACNC encompasses two primary functionalities: state recognition and context detection.","Given the intricate nature of the user-service-computing-network space, the paper employs dimension reduction to generate live, holistic, abstract system states in a hierarchical structure.","To address the challenges posed by dynamic changes, Continual Learning (CL) is employed, classifying the system state into contexts controlled by dedicated ML agents, enabling them to operate efficiently.","These two functionalities are intricately linked within a closed loop overseen by the End-to-End (E2E) orchestrator to allocate resources.","The paper introduces the components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow, details a numerical analysis for efficiency assessment, and concludes with discussions on relevant challenges and potential avenues for future research."],"url":"http://arxiv.org/abs/2403.07573v1","category":"cs.NI"}
{"created":"2024-03-12 11:53:00","title":"An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning","abstract":"Blood Glucose (BG) control involves keeping an individual's BG within a healthy range through extracorporeal insulin injections is an important task for people with type 1 diabetes. However,traditional patient self-management is cumbersome and risky. Recent research has been devoted to exploring individualized and automated BG control approaches, among which Deep Reinforcement Learning (DRL) shows potential as an emerging approach. In this paper, we use an exponential decay model of drug concentration to convert the formalization of the BG control problem, which takes into account the delay and prolongedness of drug effects, from a PAE-POMDP (Prolonged Action Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a novel multi-step DRL-based algorithm to solve the problem. The Prioritized Experience Replay (PER) sampling method is also used in it. Compared to single-step bootstrapped updates, multi-step learning is more efficient and reduces the influence from biasing targets. Our proposed method converges faster and achieves higher cumulative rewards compared to the benchmark in the same training environment, and improves the time-in-range (TIR), the percentage of time the patient's BG is within the target range, in the evaluation phase. Our work validates the effectiveness of multi-step reinforcement learning in BG control, which may help to explore the optimal glycemic control measure and improve the survival of diabetic patients.","sentences":["Blood Glucose (BG) control involves keeping an individual's BG within a healthy range through extracorporeal insulin injections is an important task for people with type 1 diabetes.","However,traditional patient self-management is cumbersome and risky.","Recent research has been devoted to exploring individualized and automated BG control approaches, among which Deep Reinforcement Learning (DRL) shows potential as an emerging approach.","In this paper, we use an exponential decay model of drug concentration to convert the formalization of the BG control problem, which takes into account the delay and prolongedness of drug effects, from a PAE-POMDP (Prolonged Action Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a novel multi-step DRL-based algorithm to solve the problem.","The Prioritized Experience Replay (PER) sampling method is also used in it.","Compared to single-step bootstrapped updates, multi-step learning is more efficient and reduces the influence from biasing targets.","Our proposed method converges faster and achieves higher cumulative rewards compared to the benchmark in the same training environment, and improves the time-in-range (TIR), the percentage of time the patient's BG is within the target range, in the evaluation phase.","Our work validates the effectiveness of multi-step reinforcement learning in BG control, which may help to explore the optimal glycemic control measure and improve the survival of diabetic patients."],"url":"http://arxiv.org/abs/2403.07566v1","category":"cs.AI"}
{"created":"2024-03-12 11:51:59","title":"RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model","abstract":"The intelligent interpretation of buildings plays a significant role in urban planning and management, macroeconomic analysis, population dynamics, etc. Remote sensing image building interpretation primarily encompasses building extraction and change detection. However, current methodologies often treat these two tasks as separate entities, thereby failing to leverage shared knowledge. Moreover, the complexity and diversity of remote sensing image scenes pose additional challenges, as most algorithms are designed to model individual small datasets, thus lacking cross-scene generalization. In this paper, we propose a comprehensive remote sensing image building understanding model, termed RSBuilding, developed from the perspective of the foundation model. RSBuilding is designed to enhance cross-scene generalization and task universality. Specifically, we extract image features based on the prior knowledge of the foundation model and devise a multi-level feature sampler to augment scale information. To unify task representation and integrate image spatiotemporal clues, we introduce a cross-attention decoder with task prompts. Addressing the current shortage of datasets that incorporate annotations for both tasks, we have developed a federated training strategy to facilitate smooth model convergence even when supervision for some tasks is missing, thereby bolstering the complementarity of different tasks. Our model was trained on a dataset comprising up to 245,000 images and validated on multiple building extraction and change detection datasets. The experimental results substantiate that RSBuilding can concurrently handle two structurally distinct tasks and exhibits robust zero-shot generalization capabilities.","sentences":["The intelligent interpretation of buildings plays a significant role in urban planning and management, macroeconomic analysis, population dynamics, etc.","Remote sensing image building interpretation primarily encompasses building extraction and change detection.","However, current methodologies often treat these two tasks as separate entities, thereby failing to leverage shared knowledge.","Moreover, the complexity and diversity of remote sensing image scenes pose additional challenges, as most algorithms are designed to model individual small datasets, thus lacking cross-scene generalization.","In this paper, we propose a comprehensive remote sensing image building understanding model, termed RSBuilding, developed from the perspective of the foundation model.","RSBuilding is designed to enhance cross-scene generalization and task universality.","Specifically, we extract image features based on the prior knowledge of the foundation model and devise a multi-level feature sampler to augment scale information.","To unify task representation and integrate image spatiotemporal clues, we introduce a cross-attention decoder with task prompts.","Addressing the current shortage of datasets that incorporate annotations for both tasks, we have developed a federated training strategy to facilitate smooth model convergence even when supervision for some tasks is missing, thereby bolstering the complementarity of different tasks.","Our model was trained on a dataset comprising up to 245,000 images and validated on multiple building extraction and change detection datasets.","The experimental results substantiate that RSBuilding can concurrently handle two structurally distinct tasks and exhibits robust zero-shot generalization capabilities."],"url":"http://arxiv.org/abs/2403.07564v1","category":"cs.CV"}
{"created":"2024-03-12 11:49:00","title":"Maximum Defective Clique Computation: Improved Time Complexities and Practical Performance","abstract":"The concept of $k$-defective clique, a relaxation of clique by allowing up-to $k$ missing edges, has been receiving increasing interests recently. Although the problem of finding the maximum $k$-defective clique is NP-hard, several practical algorithms have been recently proposed in the literature, with kDC being the state of the art. kDC not only runs the fastest in practice, but also achieves the best time complexity. Specifically, it runs in $O^*(\\gamma_k^n)$ time when ignoring polynomial factors; here, $\\gamma_k$ is a constant that is smaller than two and only depends on $k$, and $n$ is the number of vertices in the input graph $G$. In this paper, we propose the kDC-Two algorithm to improve the time complexity as well as practical performance. kDC-Two runs in $O^*( (\\alpha\\Delta)^{k+2} \\gamma_{k-1}^\\alpha)$ time when the maximum $k$-defective clique size $\\omega_k(G)$ is at least $k+2$, and in $O^*(\\gamma_{k-1}^n)$ time otherwise, where $\\alpha$ and $\\Delta$ are the degeneracy and maximum degree of $G$, respectively. In addition, with slight modification, kDC-Two also runs in $O^*( (\\alpha\\Delta)^{k+2} (k+1)^{\\alpha+k+1-\\omega_k(G)})$ time by using the degeneracy gap $\\alpha+k+1-\\omega_k(G)$ parameterization; this is better than $O^*( (\\alpha\\Delta)^{k+2}\\gamma_{k-1}^\\alpha)$ when $\\omega_k(G)$ is close to the degeneracy-based upper bound $\\alpha+k+1$. Finally, to further improve the practical performance, we propose a new degree-sequence-based reduction rule that can be efficiently applied, and theoretically demonstrate its effectiveness compared with those proposed in the literature. Extensive empirical studies on three benchmark graph collections show that our algorithm outperforms the existing fastest algorithm by several orders of magnitude.","sentences":["The concept of $k$-defective clique, a relaxation of clique by allowing up-to $k$ missing edges, has been receiving increasing interests recently.","Although the problem of finding the maximum $k$-defective clique is NP-hard, several practical algorithms have been recently proposed in the literature, with kDC being the state of the art.","kDC","not only runs the fastest in practice, but also achieves the best time complexity.","Specifically, it runs in $O^*(\\gamma_k^n)$ time when ignoring polynomial factors; here, $\\gamma_k$ is a constant that is smaller than two and only depends on $k$, and $n$ is the number of vertices in the input graph $G$.","In this paper, we propose the kDC-Two algorithm to improve the time complexity as well as practical performance.","kDC-Two runs in $O^*( (\\alpha\\Delta)^{k+2} \\gamma_{k-1}^\\alpha)$ time when the maximum $k$-defective clique size $\\omega_k(G)$ is at least $k+2$, and in $O^*(\\gamma_{k-1}^n)$ time otherwise, where $\\alpha$ and $\\Delta$ are the degeneracy and maximum degree of $G$, respectively.","In addition, with slight modification, kDC-Two also runs in $O^*( (\\alpha\\Delta)^{k+2} (k+1)^{\\alpha+k+1-\\omega_k(G)})$ time by using the degeneracy gap $\\alpha+k+1-\\omega_k(G)$ parameterization; this is better than $O^*( (\\alpha\\Delta)^{k+2}\\gamma_{k-1}^\\alpha)$ when $\\omega_k(G)$ is close to the degeneracy-based upper bound $\\alpha+k+1$. Finally, to further improve the practical performance, we propose a new degree-sequence-based reduction rule that can be efficiently applied, and theoretically demonstrate its effectiveness compared with those proposed in the literature.","Extensive empirical studies on three benchmark graph collections show that our algorithm outperforms the existing fastest algorithm by several orders of magnitude."],"url":"http://arxiv.org/abs/2403.07561v1","category":"cs.DS"}
{"created":"2024-03-12 17:52:35","title":"Low coordinate degree algorithms I: Universality of computational thresholds for hypothesis testing","abstract":"We study when low coordinate degree functions (LCDF) -- linear combinations of functions depending on small subsets of entries of a vector -- can hypothesis test between high-dimensional probability measures. These functions are a generalization, proposed in Hopkins' 2018 thesis but seldom studied since, of low degree polynomials (LDP), a class widely used in recent literature as a proxy for all efficient algorithms for tasks in statistics and optimization. Instead of the orthogonal polynomial decompositions used in LDP calculations, our analysis of LCDF is based on the Efron-Stein or ANOVA decomposition, making it much more broadly applicable. By way of illustration, we prove channel universality for the success of LCDF in testing for the presence of sufficiently \"dilute\" random signals through noisy channels: the efficacy of LCDF depends on the channel only through the scalar Fisher information for a class of channels including nearly arbitrary additive i.i.d. noise and nearly arbitrary exponential families. As applications, we extend lower bounds against LDP for spiked matrix and tensor models under additive Gaussian noise to lower bounds against LCDF under general noisy channels. We also give a simple and unified treatment of the effect of censoring models by erasing observations at random and of quantizing models by taking the sign of the observations. These results are the first computational lower bounds against any large class of algorithms for all of these models when the channel is not one of a few special cases, and thereby give the first substantial evidence for the universality of several statistical-to-computational gaps.","sentences":["We study when low coordinate degree functions (LCDF) -- linear combinations of functions depending on small subsets of entries of a vector -- can hypothesis test between high-dimensional probability measures.","These functions are a generalization, proposed in Hopkins' 2018 thesis but seldom studied since, of low degree polynomials (LDP), a class widely used in recent literature as a proxy for all efficient algorithms for tasks in statistics and optimization.","Instead of the orthogonal polynomial decompositions used in LDP calculations, our analysis of LCDF is based on the Efron-Stein or ANOVA decomposition, making it much more broadly applicable.","By way of illustration, we prove channel universality for the success of LCDF in testing for the presence of sufficiently \"dilute\" random signals through noisy channels: the efficacy of LCDF depends on the channel only through the scalar Fisher information for a class of channels including nearly arbitrary additive i.i.d. noise and nearly arbitrary exponential families.","As applications, we extend lower bounds against LDP for spiked matrix and tensor models under additive Gaussian noise to lower bounds against LCDF under general noisy channels.","We also give a simple and unified treatment of the effect of censoring models by erasing observations at random and of quantizing models by taking the sign of the observations.","These results are the first computational lower bounds against any large class of algorithms for all of these models when the channel is not one of a few special cases, and thereby give the first substantial evidence for the universality of several statistical-to-computational gaps."],"url":"http://arxiv.org/abs/2403.07862v1","category":"math.ST"}
{"created":"2024-03-12 17:48:11","title":"Accelerating Biclique Counting on GPU","abstract":"Counting (p,q)-bicliques in bipartite graphs poses a foundational challenge with broad applications, from densest subgraph discovery in algorithmic research to personalized content recommendation in practical scenarios. Despite its significance, current leading (p,q)-biclique counting algorithms fall short, particularly when faced with larger graph sizes and clique scales. Fortunately, the problem's inherent structure, allowing for the independent counting of each biclique starting from every vertex, combined with a substantial set intersections, makes it highly amenable to parallelization. Recent successes in GPU-accelerated algorithms across various domains motivate our exploration into harnessing the parallelism power of GPUs to efficiently address the (p,q)-biclique counting challenge. We introduce GBC (GPU-based Biclique Counting), a novel approach designed to enable efficient and scalable (p,q)-biclique counting on GPUs. To address major bottleneck arising from redundant comparisons in set intersections (occupying an average of 90% of the runtime), we introduce a novel data structure that hashes adjacency lists into truncated bitmaps to enable efficient set intersection on GPUs via bit-wise AND operations. Our innovative hybrid DFS-BFS exploration strategy further enhances thread utilization and effectively manages memory constraints. A composite load balancing strategy, integrating pre-runtime and runtime workload allocation, ensures equitable distribution among threads. Additionally, we employ vertex reordering and graph partitioning strategies for improved compactness and scalability. Experimental evaluations on eight real-life and two synthetic datasets demonstrate that GBC outperforms state-of-the-art algorithms by a substantial margin. In particular, GBC achieves an average speedup of 497.8x, with the largest instance achieving a remarkable 1217.7x speedup when p = q = 8.","sentences":["Counting (p,q)-bicliques in bipartite graphs poses a foundational challenge with broad applications, from densest subgraph discovery in algorithmic research to personalized content recommendation in practical scenarios.","Despite its significance, current leading (p,q)-biclique counting algorithms fall short, particularly when faced with larger graph sizes and clique scales.","Fortunately, the problem's inherent structure, allowing for the independent counting of each biclique starting from every vertex, combined with a substantial set intersections, makes it highly amenable to parallelization.","Recent successes in GPU-accelerated algorithms across various domains motivate our exploration into harnessing the parallelism power of GPUs to efficiently address the (p,q)-biclique counting challenge.","We introduce GBC (GPU-based Biclique Counting), a novel approach designed to enable efficient and scalable (p,q)-biclique counting on GPUs.","To address major bottleneck arising from redundant comparisons in set intersections (occupying an average of 90% of the runtime), we introduce a novel data structure that hashes adjacency lists into truncated bitmaps to enable efficient set intersection on GPUs via bit-wise AND operations.","Our innovative hybrid DFS-BFS exploration strategy further enhances thread utilization and effectively manages memory constraints.","A composite load balancing strategy, integrating pre-runtime and runtime workload allocation, ensures equitable distribution among threads.","Additionally, we employ vertex reordering and graph partitioning strategies for improved compactness and scalability.","Experimental evaluations on eight real-life and two synthetic datasets demonstrate that GBC outperforms state-of-the-art algorithms by a substantial margin.","In particular, GBC achieves an average speedup of 497.8x, with the largest instance achieving a remarkable 1217.7x speedup when p = q = 8."],"url":"http://arxiv.org/abs/2403.07858v1","category":"cs.DC"}
{"created":"2024-03-12 17:44:45","title":"Distilling the Knowledge in Data Pruning","abstract":"With the increasing size of datasets used for training neural networks, data pruning becomes an attractive field of research. However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data. By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. We first establish a theoretical motivation for employing self-distillation to improve training on pruned data. Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is comparable or superior to sophisticated pruning methods across all pruning regimes. On ImageNet for example, we achieve superior accuracy despite training on a random subset of only 50% of the data. Additionally, we demonstrate a crucial connection between the pruning factor and the optimal knowledge distillation weight. This helps mitigate the impact of samples with noisy labels and low-quality images retained by typical pruning algorithms. Finally, we make an intriguing observation: when using lower pruning fractions, larger teachers lead to accuracy degradation, while surprisingly, employing teachers with a smaller capacity than the student's may improve results. Our code will be made available.","sentences":["With the increasing size of datasets used for training neural networks, data pruning becomes an attractive field of research.","However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes.","In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset.","That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data.","By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions.","We first establish a theoretical motivation for employing self-distillation to improve training on pruned data.","Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is comparable or superior to sophisticated pruning methods across all pruning regimes.","On ImageNet for example, we achieve superior accuracy despite training on a random subset of only 50% of the data.","Additionally, we demonstrate a crucial connection between the pruning factor and the optimal knowledge distillation weight.","This helps mitigate the impact of samples with noisy labels and low-quality images retained by typical pruning algorithms.","Finally, we make an intriguing observation: when using lower pruning fractions, larger teachers lead to accuracy degradation, while surprisingly, employing teachers with a smaller capacity than the student's may improve results.","Our code will be made available."],"url":"http://arxiv.org/abs/2403.07854v1","category":"cs.CV"}
{"created":"2024-03-12 17:21:10","title":"Topological Protection of Optical Skyrmions through Complex Media","abstract":"Recent experimental realizations of optical Skyrmions through the techniques of structured light have opened the doors to a completely new way of representing data in electromagnetic fields, namely its topology. Apart from potentially enhancing the bandwidth of optical systems, the intrinsically discrete nature of the topological number allows Skyrmions to naturally interface with the digital world. However, investigations into the topological protection of optical Skyrmions through various media remain limited to date. Here, we rigorously define the optical Skyrmion and establish a framework that can be used to analyze the effects of complex media on the topology of Skyrmion fields. Using this framework, we provide simple criteria for spatially varying retarders, diattenuators, depolarizers, and combinations of the former under which topological protection is guaranteed. We then present experimental results validating the robustness of the Skyrmion number against corrupting media and discuss ways of extending the optical Skyrmion to more general settings. We believe that the work presented in this paper provides a theoretical underpinning for the use of Skyrmions in practical applications ranging from optical communications to photonic computing.","sentences":["Recent experimental realizations of optical Skyrmions through the techniques of structured light have opened the doors to a completely new way of representing data in electromagnetic fields, namely its topology.","Apart from potentially enhancing the bandwidth of optical systems, the intrinsically discrete nature of the topological number allows Skyrmions to naturally interface with the digital world.","However, investigations into the topological protection of optical Skyrmions through various media remain limited to date.","Here, we rigorously define the optical Skyrmion and establish a framework that can be used to analyze the effects of complex media on the topology of Skyrmion fields.","Using this framework, we provide simple criteria for spatially varying retarders, diattenuators, depolarizers, and combinations of the former under which topological protection is guaranteed.","We then present experimental results validating the robustness of the Skyrmion number against corrupting media and discuss ways of extending the optical Skyrmion to more general settings.","We believe that the work presented in this paper provides a theoretical underpinning for the use of Skyrmions in practical applications ranging from optical communications to photonic computing."],"url":"http://arxiv.org/abs/2403.07837v1","category":"physics.optics"}
{"created":"2024-03-12 17:11:36","title":"Convex cones, assessment functions, balanced attributes","abstract":"We investigate a class of polyhedral convex cones, with $R^k_+$ (the nonegative orthant in $\\mathbb{R}^k$) as a special case. We start with the observation that for convex cones contained in $\\mathbb{R}^k$, the respective cone efficiency is inconsistent with the Pareto efficiency, the latter being deeply rooted in economics, the decision theory, and the multiobjective optimization theory. Despite that, we argue that convex cones contained in $\\mathbb{R}^k$ and the respective cone efficiency are also relevant to these domains.   To demonstrate this, we interpret polyhedral convex cones of the investigated class in terms of assessment functions, i.e., functions that aggregate multiple numerical attributes into single numbers.   Further, we observe that all assessment functions in the current use share the same limitation; that is, they do not take explicitly into account attribute proportionality. In consequence, the issue of {\\em attribute balance} (meaning {\\it the balance of attribute values}) escapes them. In contrast, assessment functions defined by polyhedral convex cones of the investigated class, contained in $\\mathbb{R}^k$, enforce the attribute balance. However, enforcing the attribute balance is, in general, inconsistent with the well-established paradigm of Pareto efficiency. We give a practical example where such inconsistency is meaningful.","sentences":["We investigate a class of polyhedral convex cones, with $R^k_+$ (the nonegative orthant in $\\mathbb{R}^k$) as a special case.","We start with the observation that for convex cones contained in $\\mathbb{R}^k$, the respective cone efficiency is inconsistent with the Pareto efficiency, the latter being deeply rooted in economics, the decision theory, and the multiobjective optimization theory.","Despite that, we argue that convex cones contained in $\\mathbb{R}^k$ and the respective cone efficiency are also relevant to these domains.   ","To demonstrate this, we interpret polyhedral convex cones of the investigated class in terms of assessment functions, i.e., functions that aggregate multiple numerical attributes into single numbers.   ","Further, we observe that all assessment functions in the current use share the same limitation; that is, they do not take explicitly into account attribute proportionality.","In consequence, the issue of {\\em attribute balance} (meaning {\\it the balance of attribute values}) escapes them.","In contrast, assessment functions defined by polyhedral convex cones of the investigated class, contained in $\\mathbb{R}^k$, enforce the attribute balance.","However, enforcing the attribute balance is, in general, inconsistent with the well-established paradigm of Pareto efficiency.","We give a practical example where such inconsistency is meaningful."],"url":"http://arxiv.org/abs/2403.07829v1","category":"math.OC"}
{"created":"2024-03-12 17:05:27","title":"Computational modelling of complex multiphase behavior of environmentally-friendly materials for sustainable technological solutions","abstract":"This research introduces a detailed computational framework designed to analyze and forecast the complex multiphase characteristics of eco-friendly lead-free piezoelectric materials, which are essential for developing sustainable technological advancements. Lead-free piezoelectric materials have a significant thermo-electromechanical response, although their electromechanical characteristics vary throughout different phases of the material. Lead-free piezoelectric materials undergo phase changes, including rhombohedral (R3c), orthorhombic (Pnma), tetragonal (P4bm), and cubic (Cc) phases, when the temperature changes. These phases are determined by the symmetry and alignment of the ferroelectric domains. Furthermore, multiple phases exist simultaneously under certain temperature, electrical, and mechanical conditions, resulting in the material displaying intricate multiphase behavior. Studying such behaviour is crucial for evaluating the performance of these materials. The computational approach in this research relies on Landau-Ginzburg-Devonshire theory to model micro-domain phase changes in the material. This research will enhance our comprehension of the significance of complex multiphase behaviour in creating environment-friendly and sustainable technological solutions.","sentences":["This research introduces a detailed computational framework designed to analyze and forecast the complex multiphase characteristics of eco-friendly lead-free piezoelectric materials, which are essential for developing sustainable technological advancements.","Lead-free piezoelectric materials have a significant thermo-electromechanical response, although their electromechanical characteristics vary throughout different phases of the material.","Lead-free piezoelectric materials undergo phase changes, including rhombohedral (R3c), orthorhombic (Pnma), tetragonal (P4bm), and cubic (Cc) phases, when the temperature changes.","These phases are determined by the symmetry and alignment of the ferroelectric domains.","Furthermore, multiple phases exist simultaneously under certain temperature, electrical, and mechanical conditions, resulting in the material displaying intricate multiphase behavior.","Studying such behaviour is crucial for evaluating the performance of these materials.","The computational approach in this research relies on Landau-Ginzburg-Devonshire theory to model micro-domain phase changes in the material.","This research will enhance our comprehension of the significance of complex multiphase behaviour in creating environment-friendly and sustainable technological solutions."],"url":"http://arxiv.org/abs/2403.07826v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 17:02:10","title":"The Variant of Designated Verifier Signature Scheme with Message Recovery","abstract":"In this work, we introduce a strong Designated Verifier Signature (DVS) scheme that incorporates a message recovery mechanism inspired by the concept of the Universal Designated Verifier Signature (UDVS) scheme. It is worth noting that Saeednia's strong designated verifier signature scheme fails to guarantee the privacy of the signature, making it unsuitable for certain applications such as medical record certificates or voting systems. To overcome this limitation, we extend Lee's strong designated verifier signature with a message recovery scheme to develop a universal designated verifier signature scheme. This universal designated verifier scheme is crafted to safeguard the privacy of signature holders, ensuring that only designated verifiers can authenticate the true signer and recover the messages.","sentences":["In this work, we introduce a strong Designated Verifier Signature (DVS) scheme that incorporates a message recovery mechanism inspired by the concept of the Universal Designated Verifier Signature (UDVS) scheme.","It is worth noting that Saeednia's strong designated verifier signature scheme fails to guarantee the privacy of the signature, making it unsuitable for certain applications such as medical record certificates or voting systems.","To overcome this limitation, we extend Lee's strong designated verifier signature with a message recovery scheme to develop a universal designated verifier signature scheme.","This universal designated verifier scheme is crafted to safeguard the privacy of signature holders, ensuring that only designated verifiers can authenticate the true signer and recover the messages."],"url":"http://arxiv.org/abs/2403.07820v1","category":"cs.CR"}
{"created":"2024-03-12 16:41:35","title":"Variational structures for the Fokker--Planck equation with general Dirichlet boundary conditions","abstract":"We prove convergence of a modified Jordan--Kinderlehrer--Otto scheme to a solution to the Fokker--Planck equation in $\\Omega \\Subset \\mathbb{R}^d$ with spatially nonconstant Dirichlet boundary conditions. We work under mild assumptions on the domain, on the drift, and on the initial datum. In the special case where $\\Omega$ is an interval in $\\mathbb{R}^1$, we prove that such a solution is a gradient flow -- curve of maximal slope -- within a suitable space of measures, endowed with a modified Wasserstein distance. Our discrete scheme and modified distance draw inspiration from contributions by A. Figalli and N. Gigli [J. Math. Pures Appl. 94, (2010), pp. 107--130], and J. Morales [J. Math. Pures Appl. 112, (2018), pp. 41--88] on an optimal-transport approach to evolution equations with Dirichlet boundary conditions.","sentences":["We prove convergence of a modified Jordan--Kinderlehrer--Otto scheme to a solution to the Fokker--Planck equation in $\\Omega \\Subset \\mathbb{R}^d$ with spatially nonconstant Dirichlet boundary conditions.","We work under mild assumptions on the domain, on the drift, and on the initial datum.","In the special case where $\\Omega$ is an interval in $\\mathbb{R}^1$, we prove that such a solution is a gradient flow -- curve of maximal slope -- within a suitable space of measures, endowed with a modified Wasserstein distance.","Our discrete scheme and modified distance draw inspiration from contributions by A. Figalli and N. Gigli [J. Math.","Pures Appl.","94, (2010), pp.","107--130], and J. Morales [J. Math.","Pures Appl.","112, (2018), pp.","41--88] on an optimal-transport approach to evolution equations with Dirichlet boundary conditions."],"url":"http://arxiv.org/abs/2403.07803v1","category":"math.AP"}
{"created":"2024-03-12 16:36:27","title":"BraSyn 2023 challenge: Missing MRI synthesis and the effect of different learning objectives","abstract":"This work is addressing the Brain Magnetic Resonance Image Synthesis for Tumor Segmentation (BraSyn) challenge which was hosted as part of the Brain Tumor Segmentation challenge (BraTS) 2023. In this challenge researchers are invited to work on synthesizing a missing magnetic resonance image sequence given other available sequences to facilitate tumor segmentation pipelines trained on complete sets of image sequences. This problem can be addressed using deep learning in the framework of paired images-to-image translation. In this work, we proposed to investigate the effectiveness of a commonly-used deep learning framework such as Pix2Pix trained under supervision of different image-quality loss functions. Our results indicate that using different loss functions significantly affects the synthesis quality. We systematically study the impact of different loss functions in the multi-sequence MR image synthesis setting of the BraSyn challenge. Furthermore, we show how image synthesis performance can be optimized by beneficially combining different learning objectives.","sentences":["This work is addressing the Brain Magnetic Resonance Image Synthesis for Tumor Segmentation (BraSyn) challenge which was hosted as part of the Brain Tumor Segmentation challenge (BraTS) 2023.","In this challenge researchers are invited to work on synthesizing a missing magnetic resonance image sequence given other available sequences to facilitate tumor segmentation pipelines trained on complete sets of image sequences.","This problem can be addressed using deep learning in the framework of paired images-to-image translation.","In this work, we proposed to investigate the effectiveness of a commonly-used deep learning framework such as Pix2Pix trained under supervision of different image-quality loss functions.","Our results indicate that using different loss functions significantly affects the synthesis quality.","We systematically study the impact of different loss functions in the multi-sequence MR image synthesis setting of the BraSyn challenge.","Furthermore, we show how image synthesis performance can be optimized by beneficially combining different learning objectives."],"url":"http://arxiv.org/abs/2403.07800v1","category":"eess.IV"}
{"created":"2024-03-12 16:35:34","title":"Equitable Pricing in Auctions","abstract":"We study how pricing affects the division of surplus among buyers in auctions for multiple units. Our equity objective may be important, e.g., for competition concerns in downstream markets, complementing the long-standing debate on revenue and efficiency. We study a canonical model of auctions for multiple indivisible units with unit demand buyers and valuations with a private and a common component and consider all pricing rules that are a mixture (i.e., a convex combination) of pay-as-bid and uniform pricing. We propose the winners' empirical variance (WEV), the expected empirical variance of surplus among the winners, as a metric for surplus equity. We show that, for a range of private-common value proportions, a strictly interior mix of pay-as-bid and uniform pricing minimizes WEV. From an equity perspective, auctions with a higher private value component benefit from more price discrimination, whereas only auctions with a sufficiently high common value justify a more uniform pricing rule. We provide a criterion under which strictly mixed pricing dominates uniform pricing, a partial ranking of different mixed pricing formats, and bounds on the WEV-minimizing pricing under the assumption of log-concave signal distributions. In numerical experiments, we further illustrate the WEV-minimal pricing as a function of the private-common-value mix.","sentences":["We study how pricing affects the division of surplus among buyers in auctions for multiple units.","Our equity objective may be important, e.g., for competition concerns in downstream markets, complementing the long-standing debate on revenue and efficiency.","We study a canonical model of auctions for multiple indivisible units with unit demand buyers and valuations with a private and a common component and consider all pricing rules that are a mixture (i.e., a convex combination) of pay-as-bid and uniform pricing.","We propose the winners' empirical variance (WEV), the expected empirical variance of surplus among the winners, as a metric for surplus equity.","We show that, for a range of private-common value proportions, a strictly interior mix of pay-as-bid and uniform pricing minimizes WEV.","From an equity perspective, auctions with a higher private value component benefit from more price discrimination, whereas only auctions with a sufficiently high common value justify a more uniform pricing rule.","We provide a criterion under which strictly mixed pricing dominates uniform pricing, a partial ranking of different mixed pricing formats, and bounds on the WEV-minimizing pricing under the assumption of log-concave signal distributions.","In numerical experiments, we further illustrate the WEV-minimal pricing as a function of the private-common-value mix."],"url":"http://arxiv.org/abs/2403.07799v1","category":"econ.TH"}
{"created":"2024-03-12 16:31:37","title":"Search for new bosons with ytterbium isotope shifts","abstract":"The Standard Model of particle physics describes the properties of elementary particles and their interactions remarkably well, but in particular does not account for dark matter. Isotope-shift spectroscopy is a sensitive probe of fifth forces and new particles that illuminate the dark matter sector. This method sets bounds on new bosons that couple neutrons and electrons with masses in the keV/c2 to MeV/c2 range. With increasing spectroscopic precision, such searches are limited by uncertainties of isotope masses and the understanding of nuclear structure. Here, we report on high-precision mass-ratio and isotope-shift measurements of the ytterbium isotopes $^{168,170,172,174,176}$Yb that exceed previous measurements by up to two orders of magnitude. From these measurements, we extract higher-order changes in the nuclear charge distribution along the Yb isotope chain and use these to benchmark novel ab initio calculations. Our measurements set new bounds on the existence of the proposed boson.","sentences":["The Standard Model of particle physics describes the properties of elementary particles and their interactions remarkably well, but in particular does not account for dark matter.","Isotope-shift spectroscopy is a sensitive probe of fifth forces and new particles that illuminate the dark matter sector.","This method sets bounds on new bosons that couple neutrons and electrons with masses in the keV/c2 to MeV/c2 range.","With increasing spectroscopic precision, such searches are limited by uncertainties of isotope masses and the understanding of nuclear structure.","Here, we report on high-precision mass-ratio and isotope-shift measurements of the ytterbium isotopes $^{168,170,172,174,176}$Yb that exceed previous measurements by up to two orders of magnitude.","From these measurements, we extract higher-order changes in the nuclear charge distribution along the Yb isotope chain and use these to benchmark novel ab initio calculations.","Our measurements set new bounds on the existence of the proposed boson."],"url":"http://arxiv.org/abs/2403.07792v1","category":"physics.atom-ph"}
{"created":"2024-03-12 16:14:27","title":"Multi-period stochastic covering location problems: Modeling framework and solution approach","abstract":"This paper introduces a very general discrete covering location model that accounts for uncertainty and time-dependent aspects. A MILP formulation is proposed for the problem. Afterwards, it is observed that most of the models existing in the literature related with covering location can be considered as particular cases of this formulation. In order to tackle large instances of this problem a Lagrangian relaxation based heuristic is developed. A computational study is addressed to check the potentials and limits of the formulation and some variants proposed for the problem, as well as to evaluate the heuristic. Finally, different measures to report the relevance of considering a multi-period stochastic setting are studied.","sentences":["This paper introduces a very general discrete covering location model that accounts for uncertainty and time-dependent aspects.","A MILP formulation is proposed for the problem.","Afterwards, it is observed that most of the models existing in the literature related with covering location can be considered as particular cases of this formulation.","In order to tackle large instances of this problem a Lagrangian relaxation based heuristic is developed.","A computational study is addressed to check the potentials and limits of the formulation and some variants proposed for the problem, as well as to evaluate the heuristic.","Finally, different measures to report the relevance of considering a multi-period stochastic setting are studied."],"url":"http://arxiv.org/abs/2403.07785v1","category":"math.OC"}
{"created":"2024-03-12 16:08:47","title":"FairRR: Pre-Processing for Group Fairness through Randomized Response","abstract":"The increasing usage of machine learning models in consequential decision-making processes has spurred research into the fairness of these systems. While significant work has been done to study group fairness in the in-processing and post-processing setting, there has been little that theoretically connects these results to the pre-processing domain. This paper proposes that achieving group fairness in downstream models can be formulated as finding the optimal design matrix in which to modify a response variable in a Randomized Response framework. We show that measures of group fairness can be directly controlled for with optimal model utility, proposing a pre-processing algorithm called FairRR that yields excellent downstream model utility and fairness.","sentences":["The increasing usage of machine learning models in consequential decision-making processes has spurred research into the fairness of these systems.","While significant work has been done to study group fairness in the in-processing and post-processing setting, there has been little that theoretically connects these results to the pre-processing domain.","This paper proposes that achieving group fairness in downstream models can be formulated as finding the optimal design matrix in which to modify a response variable in a Randomized Response framework.","We show that measures of group fairness can be directly controlled for with optimal model utility, proposing a pre-processing algorithm called FairRR that yields excellent downstream model utility and fairness."],"url":"http://arxiv.org/abs/2403.07780v1","category":"stat.ML"}
{"created":"2024-03-12 16:01:18","title":"Fragmentation of Dense Rotation-Dominated Structures Fed by Collapsing Gravomagneto-Sheetlets and Origin of Misaligned 100 au-Scale Binaries and Multiple Systems","abstract":"The majority of stars are in binary/multiple systems. How such systems form in turbulent, magnetized cores of molecular clouds in the presence of non-ideal MHD effects remains relatively under-explored. Through ATHENA++-based non-ideal MHD AMR simulations with ambipolar diffusion, we show that the collapsing protostellar envelope is dominated by dense gravomagneto-sheetlets, a turbulence-warped version of the classic pseuodisk produced by anisotropic magnetic resistance to the gravitational collapse, in agreement with previous simulations of turbulent, magnetized single-star formation. The sheetlets feed mass, magnetic fields, and angular momentum to a Dense ROtation-Dominated (DROD) structure, which fragments into binary/multiple systems. This DROD fragmentation scenario is a more dynamic variant of the traditional disk fragmentation scenario for binary/multiple formation, with dense spiral filaments created by inhomogeneous feeding from the highly structured larger-scale sheetlets rather than the need for angular momentum transport, which is dominated by magnetic braking. Collisions between the dense spiraling filaments play a key role in pushing the local magnetic Toomre parameter $Q_\\mathrm{m}$ below unity, leading to gravitational collapse and stellar companion formation provided that the local material is sufficiently demagnetized, with a plasma-$\\beta$ of order unity or more. This mechanism can naturally produce {\\it in situ} misaligned systems on the 100-au scale, often detected with high-resolution ALMA observations. Our simulations also highlight the importance of non-ideal MHD effects, which affect whether fragmentation occurs and, if so, the masses and orbital parameters of the stellar companions formed.","sentences":["The majority of stars are in binary/multiple systems.","How such systems form in turbulent, magnetized cores of molecular clouds in the presence of non-ideal MHD effects remains relatively under-explored.","Through ATHENA++-based non-ideal MHD AMR simulations with ambipolar diffusion, we show that the collapsing protostellar envelope is dominated by dense gravomagneto-sheetlets, a turbulence-warped version of the classic pseuodisk produced by anisotropic magnetic resistance to the gravitational collapse, in agreement with previous simulations of turbulent, magnetized single-star formation.","The sheetlets feed mass, magnetic fields, and angular momentum to a Dense ROtation-Dominated (DROD) structure, which fragments into binary/multiple systems.","This DROD fragmentation scenario is a more dynamic variant of the traditional disk fragmentation scenario for binary/multiple formation, with dense spiral filaments created by inhomogeneous feeding from the highly structured larger-scale sheetlets rather than the need for angular momentum transport, which is dominated by magnetic braking.","Collisions between the dense spiraling filaments play a key role in pushing the local magnetic Toomre parameter $Q_\\mathrm{m}$ below unity, leading to gravitational collapse and stellar companion formation provided that the local material is sufficiently demagnetized, with a plasma-$\\beta$ of order unity or more.","This mechanism can naturally produce {\\it in situ} misaligned systems on the 100-au scale, often detected with high-resolution ALMA observations.","Our simulations also highlight the importance of non-ideal MHD effects, which affect whether fragmentation occurs and, if so, the masses and orbital parameters of the stellar companions formed."],"url":"http://arxiv.org/abs/2403.07777v1","category":"astro-ph.SR"}
{"created":"2024-03-12 15:58:53","title":"Privacy Guarantees in Posterior Sampling under Contamination","abstract":"In recent years, differential privacy has been adopted by tech-companies and governmental agencies as the standard for measuring privacy in algorithms. We study the level of differential privacy in Bayesian posterior sampling setups. As opposed to the common privatization approach of injecting Laplace/Gaussian noise into the output, Huber's contamination model is considered, where we replace at random the data points with samples from a heavy-tailed distribution. We derived bounds for the differential privacy level $(\\epsilon,\\delta)$ for our approach while lifting the common restriction on assuming bounded observation and parameter space seen in the existing literature. We further consider the effect of sample size on privacy level and the convergence rate of $(\\epsilon,\\delta)$ to zero. Asymptotically, the contamination approach is fully private at no cost of information loss. We also provide some examples depicting inference models that our setup is applicable to with a theoretical estimation of convergence rate.","sentences":["In recent years, differential privacy has been adopted by tech-companies and governmental agencies as the standard for measuring privacy in algorithms.","We study the level of differential privacy in Bayesian posterior sampling setups.","As opposed to the common privatization approach of injecting Laplace/Gaussian noise into the output, Huber's contamination model is considered, where we replace at random the data points with samples from a heavy-tailed distribution.","We derived bounds for the differential privacy level $(\\epsilon,\\delta)$ for our approach while lifting the common restriction on assuming bounded observation and parameter space seen in the existing literature.","We further consider the effect of sample size on privacy level and the convergence rate of $(\\epsilon,\\delta)$ to zero.","Asymptotically, the contamination approach is fully private at no cost of information loss.","We also provide some examples depicting inference models that our setup is applicable to with a theoretical estimation of convergence rate."],"url":"http://arxiv.org/abs/2403.07772v1","category":"math.ST"}
{"created":"2024-03-12 15:53:14","title":"Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model","abstract":"Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields.","sentences":["Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios.","In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces.","Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details.","It also employs content and structural control modules to preserve the content and structural information of the source image.","With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image.","After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image.","Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on.","Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields."],"url":"http://arxiv.org/abs/2403.07764v1","category":"cs.CV"}
{"created":"2024-03-12 15:51:10","title":"Supporting Annotators with Affordances for Efficiently Labeling Conversational Data","abstract":"Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time consuming and expensive. To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling. We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels. We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet. Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet.","sentences":["Without well-labeled ground truth data, machine learning-based systems would not be as ubiquitous as they are today, but these systems rely on substantial amounts of correctly labeled data.","Unfortunately, crowdsourced labeling is time consuming and expensive.","To address the concerns of effort and tedium, we designed CAL, a novel interface to aid in data labeling.","We made several key design decisions for CAL, which include preventing inapt labels from being selected, guiding users in selecting an appropriate label when they need assistance, incorporating labeling documentation into the interface, and providing an efficient means to view previous labels.","We implemented a production-quality implementation of CAL and report a user-study evaluation that compares CAL to a standard spreadsheet.","Key findings of our study include users using CAL reported lower cognitive load, did not increase task time, users rated CAL to be easier to use, and users preferred CAL over the spreadsheet."],"url":"http://arxiv.org/abs/2403.07762v1","category":"cs.HC"}
{"created":"2024-03-12 15:45:58","title":"Models of Accidental Dark Matter with a Fundamental Scalar","abstract":"We consider models of accidental dark matter, namely models in which the dark matter is a composite state that is stable thanks to an accidental symmetry of the theory. The fundamental constituents are vectorlike fermions, taken to be fragments of representations of the grand unifying gauge group $SU(5)$, as well as a scalar singlet. All the new fields are charged under a new confining gauge group, which we take to be $SU(N)$, leading to models with complex dark matter. We analyse the models in the context of $SU(5)$ grand unification with a non-standard approach recently proposed in the literature. The advantage of including the scalar mainly resides in the fact that it allows to break several undesired accidental symmetries leading to a larger set of viable models with respect to previous literature, in which only fermions (or only scalars) were considered. Moreover these models present distinct novelties, namely dark states with non-zero baryon and lepton number and the existence of composite hybrid states of fermions and scalars. We identify phenomena that are specific to the inclusion of the scalar and discuss possibilities to test this setup.","sentences":["We consider models of accidental dark matter, namely models in which the dark matter is a composite state that is stable thanks to an accidental symmetry of the theory.","The fundamental constituents are vectorlike fermions, taken to be fragments of representations of the grand unifying gauge group $SU(5)$, as well as a scalar singlet.","All the new fields are charged under a new confining gauge group, which we take to be $SU(N)$, leading to models with complex dark matter.","We analyse the models in the context of $SU(5)$ grand unification with a non-standard approach recently proposed in the literature.","The advantage of including the scalar mainly resides in the fact that it allows to break several undesired accidental symmetries leading to a larger set of viable models with respect to previous literature, in which only fermions (or only scalars) were considered.","Moreover these models present distinct novelties, namely dark states with non-zero baryon and lepton number and the existence of composite hybrid states of fermions and scalars.","We identify phenomena that are specific to the inclusion of the scalar and discuss possibilities to test this setup."],"url":"http://arxiv.org/abs/2403.07759v1","category":"hep-ph"}
{"created":"2024-03-12 15:44:12","title":"FAUST XI: Enhancement of the complex organic material in the shocked matter surrounding the [BHB2007] 11 protobinary system","abstract":"iCOMs are species commonly found in the interstellar medium. They are believed to be crucial seed species for the build-up of chemical complexity in star forming regions as well as our own Solar System. Thus, understanding how their abundances evolve during the star formation process and whether it enriches the emerging planetary system is of paramount importance. We use data from the ALMA Large Program FAUST to study the compact line emission towards the [BHB2007] 11 proto-binary system (sources A and B), where a complex structure of filaments connecting the two sources with a larger circumbinary disk has previously been detected. More than 45 CH3OCHO lines are clearly detected, as well as 8 CH3OCH3 transitions , 1 H2CCO transition and 4 t-HCOOH transitions. We compute the abundance ratios with respect to CH3OH for CH3OCHO, CH3OCH3, H2CCO, t-HCOOH (as well as an upper limit for CH3CHO) through a radiative transfer analysis. We also report the upper limits on the column densities of nitrogen bearing iCOMs, N(C2H5CN) and N(C2H3CN). The emission from the detected iCOMs and their precursors is compact and encompasses both protostars, which are separated by only 0.2\" (~ 28 au). The integrated intensities tend to align with the Southern filament, revealed by the high spatial resolution observations of the dust emission at 1.3 mm. A PV and 2D analysis are performed on the strongest and uncontaminated CH3OCH3 transition and show three different spatial and velocity regions, two of them being close to 11B (Southern filament) and the third one near 11A. All our observations suggest that the detected methanol, as well as the other iCOMs, are generated by the shocked gas from the incoming filaments streaming towards [BHB2007] 11A and 11B, respectively, making this source one of the few where chemical enrichment of the gas caused by the streaming material is observed.","sentences":["iCOMs are species commonly found in the interstellar medium.","They are believed to be crucial seed species for the build-up of chemical complexity in star forming regions as well as our own Solar System.","Thus, understanding how their abundances evolve during the star formation process and whether it enriches the emerging planetary system is of paramount importance.","We use data from the ALMA Large Program FAUST to study the compact line emission towards the [BHB2007] 11 proto-binary system (sources A and B), where a complex structure of filaments connecting the two sources with a larger circumbinary disk has previously been detected.","More than 45 CH3OCHO lines are clearly detected, as well as 8 CH3OCH3 transitions , 1 H2CCO transition and 4 t-HCOOH transitions.","We compute the abundance ratios with respect to CH3OH for CH3OCHO, CH3OCH3, H2CCO, t-HCOOH (as well as an upper limit for CH3CHO) through a radiative transfer analysis.","We also report the upper limits on the column densities of nitrogen bearing iCOMs, N(C2H5CN) and N(C2H3CN).","The emission from the detected iCOMs and their precursors is compact and encompasses both protostars, which are separated by only 0.2\" (~ 28 au).","The integrated intensities tend to align with the Southern filament, revealed by the high spatial resolution observations of the dust emission at 1.3 mm.","A PV and 2D analysis are performed on the strongest and uncontaminated CH3OCH3 transition and show three different spatial and velocity regions, two of them being close to 11B (Southern filament) and the third one near 11A. All our observations suggest that the detected methanol, as well as the other iCOMs, are generated by the shocked gas from the incoming filaments streaming towards [BHB2007] 11A and 11B, respectively, making this source one of the few where chemical enrichment of the gas caused by the streaming material is observed."],"url":"http://arxiv.org/abs/2403.07757v1","category":"astro-ph.SR"}
{"created":"2024-03-12 15:39:56","title":"Vision-based Vehicle Re-identification in Bridge Scenario using Flock Similarity","abstract":"Due to the needs of road traffic flow monitoring and public safety management, video surveillance cameras are widely distributed in urban roads. However, the information captured directly by each camera is siloed, making it difficult to use it effectively. Vehicle re-identification refers to finding a vehicle that appears under one camera in another camera, which can correlate the information captured by multiple cameras. While license plate recognition plays an important role in some applications, there are some scenarios where re-identification method based on vehicle appearance are more suitable. The main challenge is that the data of vehicle appearance has the characteristics of high inter-class similarity and large intra-class differences. Therefore, it is difficult to accurately distinguish between different vehicles by relying only on vehicle appearance information. At this time, it is often necessary to introduce some extra information, such as spatio-temporal information. Nevertheless, the relative position of the vehicles rarely changes when passing through two adjacent cameras in the bridge scenario. In this paper, we present a vehicle re-identification method based on flock similarity, which improves the accuracy of vehicle re-identification by utilizing vehicle information adjacent to the target vehicle. When the relative position of the vehicles remains unchanged and flock size is appropriate, we obtain an average relative improvement of 204% on VeRi dataset in our experiments. Then, the effect of the magnitude of the relative position change of the vehicles as they pass through two cameras is discussed. We present two metrics that can be used to quantify the difference and establish a connection between them. Although this assumption is based on the bridge scenario, it is often true in other scenarios due to driving safety and camera location.","sentences":["Due to the needs of road traffic flow monitoring and public safety management, video surveillance cameras are widely distributed in urban roads.","However, the information captured directly by each camera is siloed, making it difficult to use it effectively.","Vehicle re-identification refers to finding a vehicle that appears under one camera in another camera, which can correlate the information captured by multiple cameras.","While license plate recognition plays an important role in some applications, there are some scenarios where re-identification method based on vehicle appearance are more suitable.","The main challenge is that the data of vehicle appearance has the characteristics of high inter-class similarity and large intra-class differences.","Therefore, it is difficult to accurately distinguish between different vehicles by relying only on vehicle appearance information.","At this time, it is often necessary to introduce some extra information, such as spatio-temporal information.","Nevertheless, the relative position of the vehicles rarely changes when passing through two adjacent cameras in the bridge scenario.","In this paper, we present a vehicle re-identification method based on flock similarity, which improves the accuracy of vehicle re-identification by utilizing vehicle information adjacent to the target vehicle.","When the relative position of the vehicles remains unchanged and flock size is appropriate, we obtain an average relative improvement of 204% on VeRi dataset in our experiments.","Then, the effect of the magnitude of the relative position change of the vehicles as they pass through two cameras is discussed.","We present two metrics that can be used to quantify the difference and establish a connection between them.","Although this assumption is based on the bridge scenario, it is often true in other scenarios due to driving safety and camera location."],"url":"http://arxiv.org/abs/2403.07752v1","category":"cs.CV"}
{"created":"2024-03-12 17:50:11","title":"Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation","abstract":"Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images. As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts. A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation. In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models. Our pipeline is compatible with various language models and generative vision models, accommodating different structures. Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality. Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge. Code is available at https://github.com/ShihaoZhaoZSH/LaVi-Bridge.","sentences":["Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models.","These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images.","As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts.","A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation.","In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation.","By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models.","Our pipeline is compatible with various language models and generative vision models, accommodating different structures.","Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality.","Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge.","Code is available at https://github.com/ShihaoZhaoZSH/LaVi-Bridge."],"url":"http://arxiv.org/abs/2403.07860v1","category":"cs.CV"}
{"created":"2024-03-12 17:14:12","title":"DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for Adaptive and Minimally Deforming Grasp Policies","abstract":"Large language models (LLMs) can provide rich physical descriptions of most worldly objects, allowing robots to achieve more informed and capable grasping. We leverage LLMs' common sense physical reasoning and code-writing abilities to infer an object's physical characteristics--mass $m$, friction coefficient $\\mu$, and spring constant $k$--from a semantic description, and then translate those characteristics into an executable adaptive grasp policy. Using a current-controllable, two-finger gripper with a built-in depth camera, we demonstrate that LLM-generated, physically-grounded grasp policies outperform traditional grasp policies on a custom benchmark of 12 delicate and deformable items including food, produce, toys, and other everyday items, spanning two orders of magnitude in mass and required pick-up force. We also demonstrate how compliance feedback from DeliGrasp policies can aid in downstream tasks such as measuring produce ripeness. Our code and videos are available at: https://deligrasp.github.io","sentences":["Large language models (LLMs) can provide rich physical descriptions of most worldly objects, allowing robots to achieve more informed and capable grasping.","We leverage LLMs' common sense physical reasoning and code-writing abilities to infer an object's physical characteristics--mass $m$, friction coefficient $\\mu$, and spring constant $k$--from a semantic description, and then translate those characteristics into an executable adaptive grasp policy.","Using a current-controllable, two-finger gripper with a built-in depth camera, we demonstrate that LLM-generated, physically-grounded grasp policies outperform traditional grasp policies on a custom benchmark of 12 delicate and deformable items including food, produce, toys, and other everyday items, spanning two orders of magnitude in mass and required pick-up force.","We also demonstrate how compliance feedback from DeliGrasp policies can aid in downstream tasks such as measuring produce ripeness.","Our code and videos are available at: https://deligrasp.github.io"],"url":"http://arxiv.org/abs/2403.07832v1","category":"cs.RO"}
{"created":"2024-03-12 17:04:28","title":"The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing","abstract":"Large Language Models have revolutionized numerous tasks with their remarkable efficacy.However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space. This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing. Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect. Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods. However, our proposed methods, GORA and SORA, effectively identify and alleviate this issue, respectively, contributing to the advancement of LLM editing techniques.","sentences":["Large Language Models have revolutionized numerous tasks with their remarkable efficacy.","However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space.","This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.","This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing.","Furthermore, we introduce the Selective Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate this ripple effect.","Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods.","However, our proposed methods, GORA and SORA, effectively identify and alleviate this issue, respectively, contributing to the advancement of LLM editing techniques."],"url":"http://arxiv.org/abs/2403.07825v1","category":"cs.CL"}
{"created":"2024-03-12 16:52:40","title":"Implications of tristability on localization phenomena: a necking bifurcation's tale","abstract":"We analyze the implication of tristability on localization phenomena in one-dimensional extended dissipative systems. In this context, localized states appear due to the interaction and locking of front waves connecting different extended states. In the tristable regime investigated here two extended uniform states coexist with one periodic Turing pattern. This scenario leads to the transition from the standard-homoclinic-snaking-related localized states associated with uniform-pattern bistability to the collapsed-homoclinic-snaking-related states which arise in a uniform-bistable configuration. We find that this transition is mediated by the emergence of hybrid states through codimension-two necking bifurcations. To perform this study we use bifurcation analysis on a non-variational mean-field model describing the spatiotemporal dynamics of light pulses in passive Kerr cavities.","sentences":["We analyze the implication of tristability on localization phenomena in one-dimensional extended dissipative systems.","In this context, localized states appear due to the interaction and locking of front waves connecting different extended states.","In the tristable regime investigated here two extended uniform states coexist with one periodic Turing pattern.","This scenario leads to the transition from the standard-homoclinic-snaking-related localized states associated with uniform-pattern bistability to the collapsed-homoclinic-snaking-related states which arise in a uniform-bistable configuration.","We find that this transition is mediated by the emergence of hybrid states through codimension-two necking bifurcations.","To perform this study we use bifurcation analysis on a non-variational mean-field model describing the spatiotemporal dynamics of light pulses in passive Kerr cavities."],"url":"http://arxiv.org/abs/2403.07814v1","category":"nlin.PS"}
{"created":"2024-03-12 16:46:29","title":"Supporting Error Chains in Static Analysis for Precise Evaluation Results and Enhanced Usability","abstract":"Context: Static analyses are well-established to aid in understanding bugs or vulnerabilities during the development process or in large-scale studies. A low false-positive rate is essential for the adaption in practice and for precise results of empirical studies. Unfortunately, static analyses tend to report where a vulnerability manifests rather than the fix location. This can cause presumed false positives or imprecise results. Method: To address this problem, we designed an adaption of an existing static analysis algorithm that can distinguish between a manifestation and fix location, and reports error chains. An error chain represents at least two interconnected errors that occur successively, thus building the connection between the fix and manifestation location. We used our tool CogniCryptSUBS for a case study on 471 GitHub repositories, a performance benchmark to compare different analysis configurations, and conducted an expert interview. Result: We found that 50 % of the projects with a report had at least one error chain. Our runtime benchmark demonstrated that our improvement caused only a minimal runtime overhead of less than 4 %. The results of our expert interview indicate that with our adapted version participants require fewer executions of the analysis. Conclusion: Our results indicate that error chains occur frequently in real-world projects, and ignoring them can lead to imprecise evaluation results. The runtime benchmark indicates that our tool is a feasible and efficient solution for detecting error chains in real-world projects. Further, our results gave a hint that the usability of static analyses may benefit from supporting error chains.","sentences":["Context: Static analyses are well-established to aid in understanding bugs or vulnerabilities during the development process or in large-scale studies.","A low false-positive rate is essential for the adaption in practice and for precise results of empirical studies.","Unfortunately, static analyses tend to report where a vulnerability manifests rather than the fix location.","This can cause presumed false positives or imprecise results.","Method: To address this problem, we designed an adaption of an existing static analysis algorithm that can distinguish between a manifestation and fix location, and reports error chains.","An error chain represents at least two interconnected errors that occur successively, thus building the connection between the fix and manifestation location.","We used our tool CogniCryptSUBS for a case study on 471 GitHub repositories, a performance benchmark to compare different analysis configurations, and conducted an expert interview.","Result:","We found that 50 % of the projects with a report had at least one error chain.","Our runtime benchmark demonstrated that our improvement caused only a minimal runtime overhead of less than 4 %.","The results of our expert interview indicate that with our adapted version participants require fewer executions of the analysis.","Conclusion: Our results indicate that error chains occur frequently in real-world projects, and ignoring them can lead to imprecise evaluation results.","The runtime benchmark indicates that our tool is a feasible and efficient solution for detecting error chains in real-world projects.","Further, our results gave a hint that the usability of static analyses may benefit from supporting error chains."],"url":"http://arxiv.org/abs/2403.07808v1","category":"cs.SE"}
{"created":"2024-03-12 16:41:31","title":"Boosting keyword spotting through on-device learnable user speech characteristics","abstract":"Keyword spotting systems for always-on TinyML-constrained applications require on-site tuning to boost the accuracy of offline trained classifiers when deployed in unseen inference conditions. Adapting to the speech peculiarities of target users requires many in-domain samples, often unavailable in real-world scenarios. Furthermore, current on-device learning techniques rely on computationally intensive and memory-hungry backbone update schemes, unfit for always-on, battery-powered devices. In this work, we propose a novel on-device learning architecture, composed of a pretrained backbone and a user-aware embedding learning the user's speech characteristics. The so-generated features are fused and used to classify the input utterance. For domain shifts generated by unseen speakers, we measure error rate reductions of up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google Speech Commands dataset, through the inexpensive update of the user projections. We moreover demonstrate the few-shot learning capabilities of our proposed architecture in sample- and class-scarce learning conditions. With 23.7 kparameters and 1 MFLOP per epoch required for on-device training, our system is feasible for TinyML applications aimed at battery-powered microcontrollers.","sentences":["Keyword spotting systems for always-on TinyML-constrained applications require on-site tuning to boost the accuracy of offline trained classifiers when deployed in unseen inference conditions.","Adapting to the speech peculiarities of target users requires many in-domain samples, often unavailable in real-world scenarios.","Furthermore, current on-device learning techniques rely on computationally intensive and memory-hungry backbone update schemes, unfit for always-on, battery-powered devices.","In this work, we propose a novel on-device learning architecture, composed of a pretrained backbone and a user-aware embedding learning the user's speech characteristics.","The so-generated features are fused and used to classify the input utterance.","For domain shifts generated by unseen speakers, we measure error rate reductions of up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google Speech Commands dataset, through the inexpensive update of the user projections.","We moreover demonstrate the few-shot learning capabilities of our proposed architecture in sample- and class-scarce learning conditions.","With 23.7 kparameters and 1 MFLOP per epoch required for on-device training, our system is feasible for TinyML applications aimed at battery-powered microcontrollers."],"url":"http://arxiv.org/abs/2403.07802v1","category":"cs.SD"}
{"created":"2024-03-12 16:35:32","title":"A Fourier Transform Framework for Domain Adaptation","abstract":"By using unsupervised domain adaptation (UDA), knowledge can be transferred from a label-rich source domain to a target domain that contains relevant information but lacks labels. Many existing UDA algorithms suffer from directly using raw images as input, resulting in models that overly focus on redundant information and exhibit poor generalization capability. To address this issue, we attempt to improve the performance of unsupervised domain adaptation by employing the Fourier method (FTF).Specifically, FTF is inspired by the amplitude of Fourier spectra, which primarily preserves low-level statistical information. In FTF, we effectively incorporate low-level information from the target domain into the source domain by fusing the amplitudes of both domains in the Fourier domain. Additionally, we observe that extracting features from batches of images can eliminate redundant information while retaining class-specific features relevant to the task. Building upon this observation, we apply the Fourier Transform at the data stream level for the first time. To further align multiple sources of data, we introduce the concept of correlation alignment. To evaluate the effectiveness of our FTF method, we conducted evaluations on four benchmark datasets for domain adaptation, including Office-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our results demonstrate superior performance.","sentences":["By using unsupervised domain adaptation (UDA), knowledge can be transferred from a label-rich source domain to a target domain that contains relevant information but lacks labels.","Many existing UDA algorithms suffer from directly using raw images as input, resulting in models that overly focus on redundant information and exhibit poor generalization capability.","To address this issue, we attempt to improve the performance of unsupervised domain adaptation by employing the Fourier method (FTF).Specifically, FTF is inspired by the amplitude of Fourier spectra, which primarily preserves low-level statistical information.","In FTF, we effectively incorporate low-level information from the target domain into the source domain by fusing the amplitudes of both domains in the Fourier domain.","Additionally, we observe that extracting features from batches of images can eliminate redundant information while retaining class-specific features relevant to the task.","Building upon this observation, we apply the Fourier Transform at the data stream level for the first time.","To further align multiple sources of data, we introduce the concept of correlation alignment.","To evaluate the effectiveness of our FTF method, we conducted evaluations on four benchmark datasets for domain adaptation, including Office-31, Office-Home, ImageCLEF-DA, and Office-Caltech.","Our results demonstrate superior performance."],"url":"http://arxiv.org/abs/2403.07798v1","category":"cs.CV"}
{"created":"2024-03-12 16:22:31","title":"Transparent boundary condition and its effectively local approximation for the Schr\u00f6dinger equation on a rectangular computational domain","abstract":"The transparent boundary condition for the free Schr\\\"{o}dinger equation on a rectangular computational domain requires implementation of an operator of the form $\\sqrt{\\partial_t-i\\triangle_{\\Gamma}}$ where $\\triangle_{\\Gamma}$ is the Laplace-Beltrami operator. It is known that this operator is nonlocal in time as well as space which poses a significant challenge in developing an efficient numerical method of solution. The computational complexity of the existing methods scale with the number of time-steps which can be attributed to the nonlocal nature of the boundary operator. In this work, we report an effectively local approximation for the boundary operator such that the resulting complexity remains independent of number of time-steps. At the heart of this algorithm is a Pad\\'e approximant based rational approximation of certain fractional operators that handles corners of the domain adequately. For the spatial discretization, we use a Legendre-Galerkin spectral method with a new boundary adapted basis which ensures that the resulting linear system is banded. A compatible boundary-lifting procedure is also presented which accommodates the segments as well as the corners on the boundary. The proposed novel scheme can be implemented within the framework of any one-step time marching schemes. In particular, we demonstrate these ideas for two one-step methods, namely, the backward-differentiation formula of order 1 (BDF1) and the trapezoidal rule (TR). For the sake of comparison, we also present a convolution quadrature based scheme conforming to the one-step methods which is computationally expensive but serves as a golden standard. Finally, several numerical tests are presented to demonstrate the effectiveness of our novel method as well as to verify the order of convergence empirically.","sentences":["The transparent boundary condition for the free Schr\\\"{o}dinger equation on a rectangular computational domain requires implementation of an operator of the form $\\sqrt{\\partial_t-i\\triangle_{\\Gamma}}$ where $\\triangle_{\\Gamma}$ is the Laplace-Beltrami operator.","It is known that this operator is nonlocal in time as well as space which poses a significant challenge in developing an efficient numerical method of solution.","The computational complexity of the existing methods scale with the number of time-steps which can be attributed to the nonlocal nature of the boundary operator.","In this work, we report an effectively local approximation for the boundary operator such that the resulting complexity remains independent of number of time-steps.","At the heart of this algorithm is a Pad\\'e approximant based rational approximation of certain fractional operators that handles corners of the domain adequately.","For the spatial discretization, we use a Legendre-Galerkin spectral method with a new boundary adapted basis which ensures that the resulting linear system is banded.","A compatible boundary-lifting procedure is also presented which accommodates the segments as well as the corners on the boundary.","The proposed novel scheme can be implemented within the framework of any one-step time marching schemes.","In particular, we demonstrate these ideas for two one-step methods, namely, the backward-differentiation formula of order 1 (BDF1) and the trapezoidal rule (TR).","For the sake of comparison, we also present a convolution quadrature based scheme conforming to the one-step methods which is computationally expensive but serves as a golden standard.","Finally, several numerical tests are presented to demonstrate the effectiveness of our novel method as well as to verify the order of convergence empirically."],"url":"http://arxiv.org/abs/2403.07787v1","category":"math.NA"}
{"created":"2024-03-12 15:51:03","title":"Elliptical Halbach magnet and gradient modules for low-field portable MRI","abstract":"Objective. To develop methods to design the complete magnetic system for a truly portable MRI scanner for neurological and musculoskeletal (MSK) applications, optimized for field homogeneity, field of view (FoV) and gradient performance compared to existing low-weight configurations. Approach. We explore optimal elliptic-bore Halbach configurations based on discrete arrays of permanent magnets. In this way, we seek to improve the field homogeneity and remove constraints to the extent of the gradient coils typical of Halbach magnets. Specifically, we have optimized a tightly-packed distribution of magnetic Nd$_2$Fe$_14$B cubes with differential evolution algorithms, and a second array of shimming magnets with interior point and differential evolution methods. We have also designed and constructed an elliptical set of gradient coils that extend over the whole magnet length, maximizing the distance between the lobe centers. These are optimized with a target field method minimizing a cost function that considers also heat dissipation. Main result. We have employed the new toolbox to build the main magnet and gradient modules for a portable MRI scanner designed for point-of-care and residential use. The elliptical Halbach bore has semi-axes of 10 & 14 cm and the magnet generates a field of 87 mT homogeneous down to 5,700 ppm (parts per million) in a 20 cm diameter FoV, it weighs 216 kg and has a width of 65 cm and a height of 72 cm. Gradient efficiencies go up to around 0.8 mT/m/A, for a maximum of 12 mT/m with in 0.5 ms with 15 A & 15 V amplifier. The distance between lobes is 28 cm, significantly increased with respect to other Halbach-based scanners. Heat dissipation is around 25 W at maximum power, and gradient deviations from linearity are below 20% in a 20 cm sphere.","sentences":["Objective.","To develop methods to design the complete magnetic system for a truly portable MRI scanner for neurological and musculoskeletal (MSK) applications, optimized for field homogeneity, field of view (FoV) and gradient performance compared to existing low-weight configurations.","Approach.","We explore optimal elliptic-bore Halbach configurations based on discrete arrays of permanent magnets.","In this way, we seek to improve the field homogeneity and remove constraints to the extent of the gradient coils typical of Halbach magnets.","Specifically, we have optimized a tightly-packed distribution of magnetic Nd$_2$Fe$_14$B cubes with differential evolution algorithms, and a second array of shimming magnets with interior point and differential evolution methods.","We have also designed and constructed an elliptical set of gradient coils that extend over the whole magnet length, maximizing the distance between the lobe centers.","These are optimized with a target field method minimizing a cost function that considers also heat dissipation.","Main result.","We have employed the new toolbox to build the main magnet and gradient modules for a portable MRI scanner designed for point-of-care and residential use.","The elliptical Halbach bore has semi-axes of 10 & 14 cm and the magnet generates a field of 87 mT homogeneous down to 5,700 ppm (parts per million) in a 20 cm diameter FoV, it weighs 216 kg and has a width of 65 cm and a height of 72 cm.","Gradient efficiencies go up to around 0.8 mT/m/A, for a maximum of 12 mT/m with in 0.5 ms with 15 A & 15 V amplifier.","The distance between lobes is 28 cm, significantly increased with respect to other Halbach-based scanners.","Heat dissipation is around 25 W at maximum power, and gradient deviations from linearity are below 20% in a 20 cm sphere."],"url":"http://arxiv.org/abs/2403.07761v1","category":"physics.med-ph"}
{"created":"2024-03-12 15:42:05","title":"An Optimal Sequence Reconstruction Algorithm for Reed-Solomon Codes","abstract":"The sequence reconstruction problem, introduced by Levenshtein in 2001, considers a scenario where the sender transmits a codeword from some codebook, and the receiver obtains $N$ noisy outputs of the codeword. We study the problem of efficient reconstruction using $N$ outputs that are each corrupted by at most $t$ substitutions. Specifically, for the ubiquitous Reed-Solomon codes, we adapt the Koetter-Vardy soft-decoding algorithm, presenting a reconstruction algorithm capable of correcting beyond Johnson radius. Furthermore, the algorithm uses $\\mathcal{O}(nN)$ field operations, where $n$ is the codeword length.","sentences":["The sequence reconstruction problem, introduced by Levenshtein in 2001, considers a scenario where the sender transmits a codeword from some codebook, and the receiver obtains $N$ noisy outputs of the codeword.","We study the problem of efficient reconstruction using $N$ outputs that are each corrupted by at most $t$ substitutions.","Specifically, for the ubiquitous Reed-Solomon codes, we adapt the Koetter-Vardy soft-decoding algorithm, presenting a reconstruction algorithm capable of correcting beyond Johnson radius.","Furthermore, the algorithm uses $\\mathcal{O}(nN)$ field operations, where $n$ is the codeword length."],"url":"http://arxiv.org/abs/2403.07754v1","category":"cs.IT"}
{"created":"2024-03-12 15:27:35","title":"Harnessing two-photon dissipation for enhanced quantum measurement and control","abstract":"Dissipation engineering offers a powerful tool for quantum technologies. Recently, new superconducting devices demonstrated an engineered two-photon dissipation rate exceeding all other relevant timescales. In particular, they have proven most useful to prevent transitions between the logical states $|\\pm\\alpha\\rangle$ of a cat qubit. Here, we present three key applications of strong two-photon dissipation for quantum measurement and control, beyond cat qubit stabilization. Firstly, we demonstrate its efficacy in overcoming limitations encountered in Wigner tomography at high photon numbers. Secondly, we showcase its potential for realizing universal gates on cat qubits, exploiting the coherent mapping between cat qubit states and superpositions of 0 and 1 photons. Finally, we harness the transient dynamics of a cat state under two-photon dissipation to prepare squeezed cat states with a squeezing factor exceeding 3.8 dB.","sentences":["Dissipation engineering offers a powerful tool for quantum technologies.","Recently, new superconducting devices demonstrated an engineered two-photon dissipation rate exceeding all other relevant timescales.","In particular, they have proven most useful to prevent transitions between the logical states $|\\pm\\alpha\\rangle$ of a cat qubit.","Here, we present three key applications of strong two-photon dissipation for quantum measurement and control, beyond cat qubit stabilization.","Firstly, we demonstrate its efficacy in overcoming limitations encountered in Wigner tomography at high photon numbers.","Secondly, we showcase its potential for realizing universal gates on cat qubits, exploiting the coherent mapping between cat qubit states and superpositions of 0 and 1 photons.","Finally, we harness the transient dynamics of a cat state under two-photon dissipation to prepare squeezed cat states with a squeezing factor exceeding 3.8 dB."],"url":"http://arxiv.org/abs/2403.07744v1","category":"quant-ph"}
{"created":"2024-03-12 15:07:20","title":"CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control","abstract":"We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for popular online selection rules. We proved that CAS can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes. For the decision-driven selection rule, including most online multiple-testing procedures, CAS can exactly control the real-time FCR below the target level without any distributional assumptions. For the online selection with symmetric thresholds, we establish the error bound for the control gap of FCR under mild distributional assumptions. To account for the distribution shift in online data, we also embed CAS into some recent dynamic conformal prediction methods and examine the long-run FCR control. Numerical results on both synthetic and real data corroborate that CAS can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings.","sentences":["We study the problem of post-selection predictive inference in an online fashion.","To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks.","Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error.","We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals.","If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label.","We provide tractable constructions for the calibration set for popular online selection rules.","We proved that CAS can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes.","For the decision-driven selection rule, including most online multiple-testing procedures, CAS can exactly control the real-time FCR below the target level without any distributional assumptions.","For the online selection with symmetric thresholds, we establish the error bound for the control gap of FCR under mild distributional assumptions.","To account for the distribution shift in online data, we also embed CAS into some recent dynamic conformal prediction methods and examine the long-run FCR control.","Numerical results on both synthetic and real data corroborate that CAS can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings."],"url":"http://arxiv.org/abs/2403.07728v1","category":"stat.ML"}
{"created":"2024-03-12 15:06:24","title":"Tidal dissipation of binaries in asteroid pairs","abstract":"Tidal dissipation in a celestial body can be used to probe its internal structure. Tides govern the orbital evolution of binary systems and therefore constraints on the interior of binary system members can be derived by knowing the age and tidal state of the binary system. For asteroids, age estimates are challenging due to a lack of direct observation of their surface. However, the age of asteroid pairs formed by rotational fission of a parent body can be derived from dynamical modeling, and as such can be used to constrain the age of binary systems existing within asteroid pairs.We study 13 binary asteroid systems existing in asteroid pairs by modeling their tidal locking and eccentricity damping timescales from tidal dissipation in the primaries and secondaries. We consider the impact of thermal torques on these timescales from the YORP and BYORP effects. The resulting constraints on the tidal dissipation ratio Q/k2 are compared to monolithic and rubble pile asteroid theories, showing that all secondaries are consistent with rubble piles with regolith layers greater than 3m and suggest that Q/k2 for rubble piles increases with radius. A particular case is the first bound secondary of asteroid (3749) Balam, whose Q/k2 is constrained to be between 2.7x10^4 and 1.4x10^6, consistent with a rubble-pile with a regolith thickness between 30m and 100m.","sentences":["Tidal dissipation in a celestial body can be used to probe its internal structure.","Tides govern the orbital evolution of binary systems and therefore constraints on the interior of binary system members can be derived by knowing the age and tidal state of the binary system.","For asteroids, age estimates are challenging due to a lack of direct observation of their surface.","However, the age of asteroid pairs formed by rotational fission of a parent body can be derived from dynamical modeling, and as such can be used to constrain the age of binary systems existing within asteroid pairs.","We study 13 binary asteroid systems existing in asteroid pairs by modeling their tidal locking and eccentricity damping timescales from tidal dissipation in the primaries and secondaries.","We consider the impact of thermal torques on these timescales from the YORP and BYORP effects.","The resulting constraints on the tidal dissipation ratio Q/k2 are compared to monolithic and rubble pile asteroid theories, showing that all secondaries are consistent with rubble piles with regolith layers greater than 3m and suggest that Q/k2 for rubble piles increases with radius.","A particular case is the first bound secondary of asteroid (3749)","Balam, whose Q/k2 is constrained to be between 2.7x10^4 and 1.4x10^6, consistent with a rubble-pile with a regolith thickness between 30m and 100m."],"url":"http://arxiv.org/abs/2403.07727v1","category":"astro-ph.EP"}
{"created":"2024-03-12 14:58:57","title":"Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion","abstract":"How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning. Modern contrastive learning and generative models improved the performance of fMRI-based visual decoding and reconstruction. However, the high cost and low temporal resolution of fMRI limit their applications in brain-computer interfaces (BCIs), prompting a high need for EEG-based visual reconstruction. In this study, we present an EEG-based visual reconstruction framework. It consists of a plug-and-play EEG encoder called the Adaptive Thinking Mapper (ATM), which is aligned with image embeddings, and a two-stage EEG guidance image generator that first transforms EEG features into image priors and then reconstructs the visual stimuli with a pre-trained image generator. Our approach allows EEG embeddings to achieve superior performance in image classification and retrieval tasks. Our two-stage image generation strategy vividly reconstructs images seen by humans. Furthermore, we analyzed the impact of signals from different time windows and brain regions on decoding and reconstruction. The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality. We report that EEG-based visual decoding achieves SOTA performance, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications. The code of ATM is available at https://github.com/dongyangli-del/EEG_Image_decode.","sentences":["How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning.","Modern contrastive learning and generative models improved the performance of fMRI-based visual decoding and reconstruction.","However, the high cost and low temporal resolution of fMRI limit their applications in brain-computer interfaces (BCIs), prompting a high need for EEG-based visual reconstruction.","In this study, we present an EEG-based visual reconstruction framework.","It consists of a plug-and-play EEG encoder called the Adaptive Thinking Mapper (ATM), which is aligned with image embeddings, and a two-stage EEG guidance image generator that first transforms EEG features into image priors and then reconstructs the visual stimuli with a pre-trained image generator.","Our approach allows EEG embeddings to achieve superior performance in image classification and retrieval tasks.","Our two-stage image generation strategy vividly reconstructs images seen by humans.","Furthermore, we analyzed the impact of signals from different time windows and brain regions on decoding and reconstruction.","The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality.","We report that EEG-based visual decoding achieves SOTA performance, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications.","The code of ATM is available at https://github.com/dongyangli-del/EEG_Image_decode."],"url":"http://arxiv.org/abs/2403.07721v2","category":"cs.HC"}
{"created":"2024-03-12 14:21:30","title":"Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal","abstract":"Real-world vision tasks frequently suffer from the appearance of unexpected adverse weather conditions, including rain, haze, snow, and raindrops. In the last decade, convolutional neural networks and vision transformers have yielded outstanding results in single-weather video removal. However, due to the absence of appropriate adaptation, most of them fail to generalize to other weather conditions. Although ViWS-Net is proposed to remove adverse weather conditions in videos with a single set of pre-trained weights, it is seriously blinded by seen weather at train-time and degenerates when coming to unseen weather during test-time. In this work, we introduce test-time adaptation into adverse weather removal in videos, and propose the first framework that integrates test-time adaptation into the iterative diffusion reverse process. Specifically, we devise a diffusion-based network with a novel temporal noise model to efficiently explore frame-correlated information in degraded video clips at training stage. During inference stage, we introduce a proxy task named Diffusion Tubelet Self-Calibration to learn the primer distribution of test video stream and optimize the model by approximating the temporal noise model for online adaptation. Experimental results, on benchmark datasets, demonstrate that our Test-Time Adaptation method with Diffusion-based network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring videos degraded by seen weather conditions. Its generalizable capability is also validated with unseen weather conditions in both synthesized and real-world videos.","sentences":["Real-world vision tasks frequently suffer from the appearance of unexpected adverse weather conditions, including rain, haze, snow, and raindrops.","In the last decade, convolutional neural networks and vision transformers have yielded outstanding results in single-weather video removal.","However, due to the absence of appropriate adaptation, most of them fail to generalize to other weather conditions.","Although ViWS-Net is proposed to remove adverse weather conditions in videos with a single set of pre-trained weights, it is seriously blinded by seen weather at train-time and degenerates when coming to unseen weather during test-time.","In this work, we introduce test-time adaptation into adverse weather removal in videos, and propose the first framework that integrates test-time adaptation into the iterative diffusion reverse process.","Specifically, we devise a diffusion-based network with a novel temporal noise model to efficiently explore frame-correlated information in degraded video clips at training stage.","During inference stage, we introduce a proxy task named Diffusion Tubelet Self-Calibration to learn the primer distribution of test video stream and optimize the model by approximating the temporal noise model for online adaptation.","Experimental results, on benchmark datasets, demonstrate that our Test-Time Adaptation method with Diffusion-based network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring videos degraded by seen weather conditions.","Its generalizable capability is also validated with unseen weather conditions in both synthesized and real-world videos."],"url":"http://arxiv.org/abs/2403.07684v1","category":"cs.CV"}
{"created":"2024-03-12 14:12:59","title":"MoralBERT: Detecting Moral Values in Social Discourse","abstract":"Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements. Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews. Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content. Here, we design a range of language representation models fine-tuned to capture exactly the moral nuances in text, called MoralBERT. We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics. This approach broadens linguistic diversity and potentially enhances the models' ability to comprehend morality in various contexts. We also explore a domain adaptation technique and compare it to the standard fine-tuned BERT model, using two different frameworks for moral prediction: single-label and multi-label. We compare in-domain approaches with conventional models relying on lexicon-based techniques, as well as a Machine Learning classifier with Word2Vec representation. Our results showed that in-domain prediction models significantly outperformed traditional models. While the single-label setting reaches a higher accuracy than previously achieved for the task when using BERT pretrained models. Experiments in an out-of-domain setting, instead, suggest that further work is needed for existing domain adaptation techniques to generalise between different social media platforms, especially for the multi-label task. The investigations and outcomes from this study pave the way for further exploration, enabling a more profound comprehension of moral narratives about controversial social issues.","sentences":["Morality plays a fundamental role in how we perceive information while greatly influencing our decisions and judgements.","Controversial topics, including vaccination, abortion, racism, and sexuality, often elicit opinions and attitudes that are not solely based on evidence but rather reflect moral worldviews.","Recent advances in natural language processing have demonstrated that moral values can be gauged in human-generated textual content.","Here, we design a range of language representation models fine-tuned to capture exactly the moral nuances in text, called MoralBERT.","We leverage annotated moral data from three distinct sources: Twitter, Reddit, and Facebook user-generated content covering various socially relevant topics.","This approach broadens linguistic diversity and potentially enhances the models' ability to comprehend morality in various contexts.","We also explore a domain adaptation technique and compare it to the standard fine-tuned BERT model, using two different frameworks for moral prediction: single-label and multi-label.","We compare in-domain approaches with conventional models relying on lexicon-based techniques, as well as a Machine Learning classifier with Word2Vec representation.","Our results showed that in-domain prediction models significantly outperformed traditional models.","While the single-label setting reaches a higher accuracy than previously achieved for the task when using BERT pretrained models.","Experiments in an out-of-domain setting, instead, suggest that further work is needed for existing domain adaptation techniques to generalise between different social media platforms, especially for the multi-label task.","The investigations and outcomes from this study pave the way for further exploration, enabling a more profound comprehension of moral narratives about controversial social issues."],"url":"http://arxiv.org/abs/2403.07678v1","category":"cs.CL"}
{"created":"2024-03-12 13:32:46","title":"A phase-resolved Fermi-LAT analysis of the mode-changing pulsar PSR J2021+4026 shows hints of a multipolar magnetosphere","abstract":"The goal of our work is to study the mode changes of the radio-quiet gamma-ray pulsar PSR J2021+4026 with improved detail. By accurately characterizing variations in the gamma-ray spectrum and pulse profile, we aim to relate the Fermi-LAT observations to theoretical models and interpret the mode changes in terms of variations in the structure of a multipolar dissipative magnetosphere. We continually monitored the rotational evolution and the gamma-ray flux of PSR J2021+4026 using more than 13 years of Fermi-LAT data with a binned likelihood approach. We clearly detect the previous mode changes and confirm a more recent mode change that occurred around June 2020. We investigated the features of the phase-resolved spectrum and pulse profile, and we inferred the macroscopic conductivity, the electric field parallel to the magnetic field, and the curvature radiation cutoff energy. These physical quantities are related to the spin-down rate and the gamma-ray flux and therefore are relevant to the theoretical interpretation of the mode changes. We computed the relative variations in the best-fit parameters, finding typical flux changes between 13% and 20%. Correlations appear between the gamma-ray flux and the spectral parameters, as the peak of the spectrum shifts by about 10% toward lower energies when the flux decreases. The analysis of the pulse profile reveals that the pulsed fraction of the light curve is larger when the flux is low. We introduced a simple magnetosphere model that combines a dipole field with a strong quadrupole component. We simulated magnetic field configurations to determine the positions of the polar caps for different sets of parameters, and we conclude that some configurations could explain the observed multiwavelength variability.","sentences":["The goal of our work is to study the mode changes of the radio-quiet gamma-ray pulsar PSR J2021+4026 with improved detail.","By accurately characterizing variations in the gamma-ray spectrum and pulse profile, we aim to relate the Fermi-LAT observations to theoretical models and interpret the mode changes in terms of variations in the structure of a multipolar dissipative magnetosphere.","We continually monitored the rotational evolution and the gamma-ray flux of PSR J2021+4026 using more than 13 years of Fermi-LAT data with a binned likelihood approach.","We clearly detect the previous mode changes and confirm a more recent mode change that occurred around June 2020.","We investigated the features of the phase-resolved spectrum and pulse profile, and we inferred the macroscopic conductivity, the electric field parallel to the magnetic field, and the curvature radiation cutoff energy.","These physical quantities are related to the spin-down rate and the gamma-ray flux and therefore are relevant to the theoretical interpretation of the mode changes.","We computed the relative variations in the best-fit parameters, finding typical flux changes between 13% and 20%.","Correlations appear between the gamma-ray flux and the spectral parameters, as the peak of the spectrum shifts by about 10% toward lower energies when the flux decreases.","The analysis of the pulse profile reveals that the pulsed fraction of the light curve is larger when the flux is low.","We introduced a simple magnetosphere model that combines a dipole field with a strong quadrupole component.","We simulated magnetic field configurations to determine the positions of the polar caps for different sets of parameters, and we conclude that some configurations could explain the observed multiwavelength variability."],"url":"http://arxiv.org/abs/2403.07649v1","category":"astro-ph.HE"}
{"created":"2024-03-12 13:28:01","title":"Discrete Laplacian thermostat for flocks and swarms: the fully conserved Inertial Spin Model","abstract":"Experiments on bird flocks and midge swarms reveal that these natural systems are well described by an active theory in which conservation laws play a crucial role. By building a symplectic structure that couples the particles' velocities to the generator of their internal rotations (spin), the Inertial Spin Model (ISM) reinstates a second-order temporal dynamics that captures many phenomenological traits of flocks and swarms. The reversible structure of the ISM predicts that the total spin is a constant of motion, the central conservation law responsible for all the novel dynamical features of the model. However, fluctuations and dissipation introduced in the original model to make it relax, violate the spin conservation law, so that the ISM aligns with the biophysical phenomenology only within finite-size regimes, beyond which the overdamped dynamics characteristic of the Vicsek model takes over. Here, we introduce a novel version of the ISM, in which the irreversible terms needed to relax the dynamics strictly respect the conservation of the spin. We perform a numerical investigation of the fully conservative model, exploring both the fixed-network case, which belongs to the equilibrium class of Model G, and the active case, characterized by self-propulsion of the agents and an out-of-equilibrium reshuffling of the underlying interaction network. Our simulations not only capture the correct spin wave phenomenology of the ordered phase, but they also yield dynamical critical exponents in the near-ordering phase that agree very well with the theoretical predictions.","sentences":["Experiments on bird flocks and midge swarms reveal that these natural systems are well described by an active theory in which conservation laws play a crucial role.","By building a symplectic structure that couples the particles' velocities to the generator of their internal rotations (spin), the Inertial Spin Model (ISM) reinstates a second-order temporal dynamics that captures many phenomenological traits of flocks and swarms.","The reversible structure of the ISM predicts that the total spin is a constant of motion, the central conservation law responsible for all the novel dynamical features of the model.","However, fluctuations and dissipation introduced in the original model to make it relax, violate the spin conservation law, so that the ISM aligns with the biophysical phenomenology only within finite-size regimes, beyond which the overdamped dynamics characteristic of the Vicsek model takes over.","Here, we introduce a novel version of the ISM, in which the irreversible terms needed to relax the dynamics strictly respect the conservation of the spin.","We perform a numerical investigation of the fully conservative model, exploring both the fixed-network case, which belongs to the equilibrium class of Model G, and the active case, characterized by self-propulsion of the agents and an out-of-equilibrium reshuffling of the underlying interaction network.","Our simulations not only capture the correct spin wave phenomenology of the ordered phase, but they also yield dynamical critical exponents in the near-ordering phase that agree very well with the theoretical predictions."],"url":"http://arxiv.org/abs/2403.07644v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-12 13:23:31","title":"Online Adaptation of Sampling-Based Motion Planning with Inaccurate Models","abstract":"Robotic manipulation relies on analytical or learned models to simulate the system dynamics. These models are often inaccurate and based on offline information, so that the robot planner is unable to cope with mismatches between the expected and the actual behavior of the system (e.g., the presence of an unexpected obstacle). In these situations, the robot should use information gathered online to correct its planning strategy and adapt to the actual system response. We propose a sampling-based motion planning approach that uses an estimate of the model error and online observations to correct the planning strategy at each new replanning. Our approach adapts the cost function and the sampling bias of a kinodynamic motion planner when the outcome of the executed transitions is different from the expected one (e.g., when the robot unexpectedly collides with an obstacle) so that future trajectories will avoid unreliable motions. To infer the properties of a new transition, we introduce the notion of context-awareness, i.e., we store local environment information for each executed transition and avoid new transitions with context similar to previous unreliable ones. This is helpful for leveraging online information even if the simulated transitions are far (in the state-and-action space) from the executed ones. Simulation and experimental results show that the proposed approach increases the success rate in execution and reduces the number of replannings needed to reach the goal.","sentences":["Robotic manipulation relies on analytical or learned models to simulate the system dynamics.","These models are often inaccurate and based on offline information, so that the robot planner is unable to cope with mismatches between the expected and the actual behavior of the system (e.g., the presence of an unexpected obstacle).","In these situations, the robot should use information gathered online to correct its planning strategy and adapt to the actual system response.","We propose a sampling-based motion planning approach that uses an estimate of the model error and online observations to correct the planning strategy at each new replanning.","Our approach adapts the cost function and the sampling bias of a kinodynamic motion planner when the outcome of the executed transitions is different from the expected one (e.g., when the robot unexpectedly collides with an obstacle) so that future trajectories will avoid unreliable motions.","To infer the properties of a new transition, we introduce the notion of context-awareness, i.e., we store local environment information for each executed transition and avoid new transitions with context similar to previous unreliable ones.","This is helpful for leveraging online information even if the simulated transitions are far (in the state-and-action space) from the executed ones.","Simulation and experimental results show that the proposed approach increases the success rate in execution and reduces the number of replannings needed to reach the goal."],"url":"http://arxiv.org/abs/2403.07638v1","category":"cs.RO"}
{"created":"2024-03-12 13:09:15","title":"generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation","abstract":"Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data. Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods. Additionally, we demonstrate the applicability of our approach in a qualitative user study. Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases.","sentences":["Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation.","However, the considered output candidates of the underlying search algorithm are under-explored and under-explained.","We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs.","To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities.","Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data.","Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods.","Additionally, we demonstrate the applicability of our approach in a qualitative user study.","Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases."],"url":"http://arxiv.org/abs/2403.07627v1","category":"cs.HC"}
{"created":"2024-03-12 13:08:33","title":"Trade-offs and thermodynamics of energy-relay proofreading","abstract":"Biological processes that are able to discriminate between different molecules consume energy and dissipate heat. They operate at different levels of fidelity and speed, and as a consequence there exist fundamental trade-offs between these quantities and the entropy production rate. Usually, the energy source required to operate in a high-fidelity regime comes from the consumption of external energetic molecules, e.g., GTP hydrolysis in protein translation . In this work, we study trade-offs between several kinetic and thermodynamic observables for Hopfield's energy-relay mechanism, which does not consume external molecules and is able to operate in depleted regions, at the cost of a higher error rate. The trade-offs are obtained both analytically and numerically via Pareto optimal fronts. We find that the scheme is able to operate in three distinct regimes: an energy relay regime, a mixed relay-Michaelis-Menten regime, and a Michaelis-Menten regime, depending on the kinetic and energetic parameters that tune transitions between states. The mixed regime features a dynamical phase transition in the error-entropy production Pareto trade-off, while the pure energy relay regime contains a region where this type of proofreading energetically outperforms standard kinetic proofreading.","sentences":["Biological processes that are able to discriminate between different molecules consume energy and dissipate heat.","They operate at different levels of fidelity and speed, and as a consequence there exist fundamental trade-offs between these quantities and the entropy production rate.","Usually, the energy source required to operate in a high-fidelity regime comes from the consumption of external energetic molecules, e.g., GTP hydrolysis in protein translation .","In this work, we study trade-offs between several kinetic and thermodynamic observables for Hopfield's energy-relay mechanism, which does not consume external molecules and is able to operate in depleted regions, at the cost of a higher error rate.","The trade-offs are obtained both analytically and numerically via Pareto optimal fronts.","We find that the scheme is able to operate in three distinct regimes: an energy relay regime, a mixed relay-Michaelis-Menten regime, and a Michaelis-Menten regime, depending on the kinetic and energetic parameters that tune transitions between states.","The mixed regime features a dynamical phase transition in the error-entropy production Pareto trade-off, while the pure energy relay regime contains a region where this type of proofreading energetically outperforms standard kinetic proofreading."],"url":"http://arxiv.org/abs/2403.07626v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-12 12:40:08","title":"Unified Source-Free Domain Adaptation","abstract":"In the pursuit of transferring a source model to a target domain without access to the source training data, Source-Free Domain Adaptation (SFDA) has been extensively explored across various scenarios, including closed-set, open-set, partial-set, and generalized settings. Existing methods, focusing on specific scenarios, not only address only a subset of challenges but also necessitate prior knowledge of the target domain, significantly limiting their practical utility and deployability. In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner. To tackle this unified SFDA problem, we propose a novel approach called Latent Causal Factors Discovery (LCFD). In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate LCFD from a causality perspective. The objective is to uncover the causal relationships between latent variables and model decisions, enhancing the reliability and robustness of the learned model against domain shifts. To integrate extensive world knowledge, we leverage a pre-trained vision-language model such as CLIP. This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees. Extensive experiments demonstrate that LCFD can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free out-of-distribution generalization.Our code and data are available at https://github.com/tntek/source-free-domain-adaptation.","sentences":["In the pursuit of transferring a source model to a target domain without access to the source training data, Source-Free Domain Adaptation (SFDA) has been extensively explored across various scenarios, including closed-set, open-set, partial-set, and generalized settings.","Existing methods, focusing on specific scenarios, not only address only a subset of challenges but also necessitate prior knowledge of the target domain, significantly limiting their practical utility and deployability.","In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner.","To tackle this unified SFDA problem, we propose a novel approach called Latent Causal Factors Discovery (LCFD).","In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate LCFD from a causality perspective.","The objective is to uncover the causal relationships between latent variables and model decisions, enhancing the reliability and robustness of the learned model against domain shifts.","To integrate extensive world knowledge, we leverage a pre-trained vision-language model such as CLIP.","This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees.","Extensive experiments demonstrate that LCFD can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free out-of-distribution generalization.","Our code and data are available at https://github.com/tntek/source-free-domain-adaptation."],"url":"http://arxiv.org/abs/2403.07601v1","category":"cs.CV"}
{"created":"2024-03-12 11:48:49","title":"Unleashing Network Potentials for Semantic Scene Completion","abstract":"Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy and semantics from a single-view RGB-D image, and recent SSC methods commonly adopt multi-modal inputs. However, our investigation reveals two limitations: ineffective feature learning from single modalities and overfitting to limited datasets. To address these issues, this paper proposes a novel SSC framework - Adversarial Modality Modulation Network (AMMNet) - with a fresh perspective of optimizing gradient updates. The proposed AMMNet introduces two core modules: a cross-modal modulation enabling the interdependence of gradient flows between modalities, and a customized adversarial training scheme leveraging dynamic gradient competition. Specifically, the cross-modal modulation adaptively re-calibrates the features to better excite representation potentials from each single modality. The adversarial training employs a minimax game of evolving gradients, with customized guidance to strengthen the generator's perception of visual fidelity from both geometric completeness and semantic correctness. Extensive experimental results demonstrate that AMMNet outperforms state-of-the-art SSC methods by a large margin, providing a promising direction for improving the effectiveness and generalization of SSC methods.","sentences":["Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy and semantics from a single-view RGB-D image, and recent SSC methods commonly adopt multi-modal inputs.","However, our investigation reveals two limitations: ineffective feature learning from single modalities and overfitting to limited datasets.","To address these issues, this paper proposes a novel SSC framework - Adversarial Modality Modulation Network (AMMNet) - with a fresh perspective of optimizing gradient updates.","The proposed AMMNet introduces two core modules: a cross-modal modulation enabling the interdependence of gradient flows between modalities, and a customized adversarial training scheme leveraging dynamic gradient competition.","Specifically, the cross-modal modulation adaptively re-calibrates the features to better excite representation potentials from each single modality.","The adversarial training employs a minimax game of evolving gradients, with customized guidance to strengthen the generator's perception of visual fidelity from both geometric completeness and semantic correctness.","Extensive experimental results demonstrate that AMMNet outperforms state-of-the-art SSC methods by a large margin, providing a promising direction for improving the effectiveness and generalization of SSC methods."],"url":"http://arxiv.org/abs/2403.07560v1","category":"cs.CV"}
{"created":"2024-03-12 11:40:44","title":"Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts","abstract":"Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations. To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental results show that TACS can effectively filter information in context and significantly improve the overall quality of LLMs' responses when presented with misleading information.","sentences":["Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations.","To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs.","TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM.","Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context.","Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information.","Experimental results show that TACS can effectively filter information in context and significantly improve the overall quality of LLMs' responses when presented with misleading information."],"url":"http://arxiv.org/abs/2403.07556v1","category":"cs.CL"}
{"created":"2024-03-12 11:39:46","title":"An Adaptive Learning Approach to Multivariate Time Forecasting in Industrial Processes","abstract":"Industrial processes generate a massive amount of monitoring data that can be exploited to uncover hidden time losses in the system, leading to enhanced accuracy of maintenance policies and, consequently, increasing the effectiveness of the equipment. In this work, we propose a method for one-step probabilistic multivariate forecasting of time variables based on a Hidden Markov Model with covariates (IO-HMM). These covariates account for the correlation of the predicted variables with their past values and additional process measurements by means of a discrete model and a continuous model. The probabilities of the former are updated using Bayesian principles, while the parameter estimates for the latter are recursively computed through an adaptive algorithm that also admits a Bayesian interpretation. This approach permits the integration of new samples into the estimation of unknown parameters, computationally improving the efficiency of the process. We evaluate the performance of the method using a real data set obtained from a company of a particular sector; however, it is a versatile technique applicable to any other data set. The results show a consistent improvement over a persistence model, which assumes that future values are the same as current values, and more importantly, over univariate versions of our model.","sentences":["Industrial processes generate a massive amount of monitoring data that can be exploited to uncover hidden time losses in the system, leading to enhanced accuracy of maintenance policies and, consequently, increasing the effectiveness of the equipment.","In this work, we propose a method for one-step probabilistic multivariate forecasting of time variables based on a Hidden Markov Model with covariates (IO-HMM).","These covariates account for the correlation of the predicted variables with their past values and additional process measurements by means of a discrete model and a continuous model.","The probabilities of the former are updated using Bayesian principles, while the parameter estimates for the latter are recursively computed through an adaptive algorithm that also admits a Bayesian interpretation.","This approach permits the integration of new samples into the estimation of unknown parameters, computationally improving the efficiency of the process.","We evaluate the performance of the method using a real data set obtained from a company of a particular sector; however, it is a versatile technique applicable to any other data set.","The results show a consistent improvement over a persistence model, which assumes that future values are the same as current values, and more importantly, over univariate versions of our model."],"url":"http://arxiv.org/abs/2403.07554v1","category":"stat.AP"}
{"created":"2024-03-12 11:37:20","title":"The post--quasi-static approximation: An analytical approach to gravitational collapse","abstract":"A semi--numerical approach proposed many years ago for describing gravitational collapse in the post--quasi--static approximation, is modified in order to avoid the numerical integration of the basic differential equations the approach is based upon. For doing that we have to impose some restrictions on the fluid distribution. More specifically, we shall assume the vanishing complexity factor condition, which allows for analytical integration of the pertinent differential equations and leads to physically interesting models. Instead, we show that neither the homologous nor the quasi--homologous evolution are acceptable since they lead to geodesic fluids, which are unsuitable for being described in the post--quasi--static approximation. Also, we prove that, within this approximation, adiabatic evolution also leads to geodesic fluids and therefore we shall consider exclusively dissipative systems. Besides the vanishing complexity factor condition, additional information is required for a full description of models. We shall propose different strategies for obtaining such an information, which are based on observables quantities (e.g. luminosity and redshift), and/or heuristic mathematical ansatz. To illustrate the method, we present two models. One model is inspired in the well known Schwarzschild interior solution, and another one is inspired in Tolman VI solution.","sentences":["A semi--numerical approach proposed many years ago for describing gravitational collapse in the post--quasi--static approximation, is modified in order to avoid the numerical integration of the basic differential equations the approach is based upon.","For doing that we have to impose some restrictions on the fluid distribution.","More specifically, we shall assume the vanishing complexity factor condition, which allows for analytical integration of the pertinent differential equations and leads to physically interesting models.","Instead, we show that neither the homologous nor the quasi--homologous evolution are acceptable since they lead to geodesic fluids, which are unsuitable for being described in the post--quasi--static approximation.","Also, we prove that, within this approximation, adiabatic evolution also leads to geodesic fluids and therefore we shall consider exclusively dissipative systems.","Besides the vanishing complexity factor condition, additional information is required for a full description of models.","We shall propose different strategies for obtaining such an information, which are based on observables quantities (e.g. luminosity and redshift), and/or heuristic mathematical ansatz.","To illustrate the method, we present two models.","One model is inspired in the well known Schwarzschild interior solution, and another one is inspired in Tolman VI solution."],"url":"http://arxiv.org/abs/2403.07550v1","category":"gr-qc"}
{"created":"2024-03-12 11:32:30","title":"MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki","abstract":"NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled. The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality. In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular machine translation systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters. We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information. The toolkit is publicly available online.","sentences":["NLP in the age of monolithic large language models is approaching its limits in terms of size and information that can be handled.","The trend goes to modularization, a necessary step into the direction of designing smaller sub-networks and components with specialized functionality.","In this paper, we present the MAMMOTH toolkit: a framework designed for training massively multilingual modular machine translation systems at scale, initially derived from OpenNMT-py and then adapted to ensure efficient training across computation clusters.","We showcase its efficiency across clusters of A100 and V100 NVIDIA GPUs, and discuss our design philosophy and plans for future information.","The toolkit is publicly available online."],"url":"http://arxiv.org/abs/2403.07544v1","category":"cs.CL"}
{"created":"2024-03-12 11:29:40","title":"A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions","abstract":"This survey explores the adaptation of visual transformer models in Autonomous Driving, a transition inspired by their success in Natural Language Processing. Surpassing traditional Recurrent Neural Networks in tasks like sequential image processing and outperforming Convolutional Neural Networks in global context capture, as evidenced in complex scene recognition, Transformers are gaining traction in computer vision. These capabilities are crucial in Autonomous Driving for real-time, dynamic visual scene processing. Our survey provides a comprehensive overview of Vision Transformer applications in Autonomous Driving, focusing on foundational concepts such as self-attention, multi-head attention, and encoder-decoder architecture. We cover applications in object detection, segmentation, pedestrian detection, lane detection, and more, comparing their architectural merits and limitations. The survey concludes with future research directions, highlighting the growing role of Vision Transformers in Autonomous Driving.","sentences":["This survey explores the adaptation of visual transformer models in Autonomous Driving, a transition inspired by their success in Natural Language Processing.","Surpassing traditional Recurrent Neural Networks in tasks like sequential image processing and outperforming Convolutional Neural Networks in global context capture, as evidenced in complex scene recognition, Transformers are gaining traction in computer vision.","These capabilities are crucial in Autonomous Driving for real-time, dynamic visual scene processing.","Our survey provides a comprehensive overview of Vision Transformer applications in Autonomous Driving, focusing on foundational concepts such as self-attention, multi-head attention, and encoder-decoder architecture.","We cover applications in object detection, segmentation, pedestrian detection, lane detection, and more, comparing their architectural merits and limitations.","The survey concludes with future research directions, highlighting the growing role of Vision Transformers in Autonomous Driving."],"url":"http://arxiv.org/abs/2403.07542v1","category":"cs.CV"}
{"created":"2024-03-12 11:18:35","title":"Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving","abstract":"Multi-view depth estimation has achieved impressive performance over various benchmarks. However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving. In this work, we propose a new robustness benchmark to evaluate the depth estimation system under various noisy pose settings. Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail when given noisy pose settings. To address this challenge, we propose a single-view and multi-view fused depth estimation system, which adaptively integrates high-confident multi-view and single-view results for both robust and accurate depth estimations. The adaptive fusion module performs fusion by dynamically selecting high-confidence regions between two branches based on a wrapping confidence map. Thus, the system tends to choose the more reliable branch when facing textureless scenes, inaccurate calibration, dynamic objects, and other degradation or challenging conditions. Our method outperforms state-of-the-art multi-view and fusion methods under robustness testing. Furthermore, we achieve state-of-the-art performance on challenging benchmarks (KITTI and DDAD) when given accurate pose estimations. Project website: https://github.com/Junda24/AFNet/.","sentences":["Multi-view depth estimation has achieved impressive performance over various benchmarks.","However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving.","In this work, we propose a new robustness benchmark to evaluate the depth estimation system under various noisy pose settings.","Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail when given noisy pose settings.","To address this challenge, we propose a single-view and multi-view fused depth estimation system, which adaptively integrates high-confident multi-view and single-view results for both robust and accurate depth estimations.","The adaptive fusion module performs fusion by dynamically selecting high-confidence regions between two branches based on a wrapping confidence map.","Thus, the system tends to choose the more reliable branch when facing textureless scenes, inaccurate calibration, dynamic objects, and other degradation or challenging conditions.","Our method outperforms state-of-the-art multi-view and fusion methods under robustness testing.","Furthermore, we achieve state-of-the-art performance on challenging benchmarks (KITTI and DDAD) when given accurate pose estimations.","Project website: https://github.com/Junda24/AFNet/."],"url":"http://arxiv.org/abs/2403.07535v1","category":"cs.CV"}
{"created":"2024-03-12 10:47:29","title":"Spatiotemporal Representation Learning for Short and Long Medical Image Time Series","abstract":"Analyzing temporal developments is crucial for the accurate prognosis of many medical conditions. Temporal changes that occur over short time scales are key to assessing the health of physiological functions, such as the cardiac cycle. Moreover, tracking longer term developments that occur over months or years in evolving processes, such as age-related macular degeneration (AMD), is essential for accurate prognosis. Despite the importance of both short and long term analysis to clinical decision making, they remain understudied in medical deep learning. State of the art methods for spatiotemporal representation learning, developed for short natural videos, prioritize the detection of temporal constants rather than temporal developments. Moreover, they do not account for varying time intervals between acquisitions, which are essential for contextualizing observed changes. To address these issues, we propose two approaches. First, we combine clip-level contrastive learning with a novel temporal embedding to adapt to irregular time series. Second, we propose masking and predicting latent frame representations of the temporal sequence. Our two approaches outperform all prior methods on temporally-dependent tasks including cardiac output estimation and three prognostic AMD tasks. Overall, this enables the automated analysis of temporal patterns which are typically overlooked in applications of deep learning to medicine.","sentences":["Analyzing temporal developments is crucial for the accurate prognosis of many medical conditions.","Temporal changes that occur over short time scales are key to assessing the health of physiological functions, such as the cardiac cycle.","Moreover, tracking longer term developments that occur over months or years in evolving processes, such as age-related macular degeneration (AMD), is essential for accurate prognosis.","Despite the importance of both short and long term analysis to clinical decision making, they remain understudied in medical deep learning.","State of the art methods for spatiotemporal representation learning, developed for short natural videos, prioritize the detection of temporal constants rather than temporal developments.","Moreover, they do not account for varying time intervals between acquisitions, which are essential for contextualizing observed changes.","To address these issues, we propose two approaches.","First, we combine clip-level contrastive learning with a novel temporal embedding to adapt to irregular time series.","Second, we propose masking and predicting latent frame representations of the temporal sequence.","Our two approaches outperform all prior methods on temporally-dependent tasks including cardiac output estimation and three prognostic AMD tasks.","Overall, this enables the automated analysis of temporal patterns which are typically overlooked in applications of deep learning to medicine."],"url":"http://arxiv.org/abs/2403.07513v1","category":"cs.CV"}
{"created":"2024-03-12 10:38:03","title":"Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation","abstract":"The objective of personalization and stylization in text-to-image is to instruct a pre-trained diffusion model to analyze new concepts introduced by users and incorporate them into expected styles. Recently, parameter-efficient fine-tuning (PEFT) approaches have been widely adopted to address this task and have greatly propelled the development of this field. Despite their popularity, existing efficient fine-tuning methods still struggle to achieve effective personalization and stylization in T2I generation. To address this issue, we propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained fine-tuning for different blocks of SD, which can generate images faithful to input prompts and target identity and also with desired style. Extensive experiments demonstrate the effectiveness of the proposed method.","sentences":["The objective of personalization and stylization in text-to-image is to instruct a pre-trained diffusion model to analyze new concepts introduced by users and incorporate them into expected styles.","Recently, parameter-efficient fine-tuning (PEFT) approaches have been widely adopted to address this task and have greatly propelled the development of this field.","Despite their popularity, existing efficient fine-tuning methods still struggle to achieve effective personalization and stylization in T2I generation.","To address this issue, we propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained fine-tuning for different blocks of SD, which can generate images faithful to input prompts and target identity and also with desired style.","Extensive experiments demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.07500v1","category":"cs.CV"}
{"created":"2024-03-12 10:35:40","title":"Tuning diagonal scale matrices for HMC","abstract":"Three approaches for adaptively tuning diagonal scale matrices for HMC are discussed and compared. The common practice of scaling according to estimated marginal standard deviations is taken as a benchmark. Scaling according to the mean log-target gradient (ISG), and a scaling method targeting that the frequency of when the underlying Hamiltonian dynamics crosses the respective medians should be uniform across dimensions, are taken as alternatives. Numerical studies suggest that the ISG method leads in many cases to more efficient sampling than the benchmark, in particular in cases with strong correlations or non-linear dependencies. The ISG method is also easy to implement, computationally cheap and would be relatively simple to include in automatically tuned codes as an alternative to the benchmark practice.","sentences":["Three approaches for adaptively tuning diagonal scale matrices for HMC are discussed and compared.","The common practice of scaling according to estimated marginal standard deviations is taken as a benchmark.","Scaling according to the mean log-target gradient (ISG), and a scaling method targeting that the frequency of when the underlying Hamiltonian dynamics crosses the respective medians should be uniform across dimensions, are taken as alternatives.","Numerical studies suggest that the ISG method leads in many cases to more efficient sampling than the benchmark, in particular in cases with strong correlations or non-linear dependencies.","The ISG method is also easy to implement, computationally cheap and would be relatively simple to include in automatically tuned codes as an alternative to the benchmark practice."],"url":"http://arxiv.org/abs/2403.07495v1","category":"stat.CO"}
{"created":"2024-03-12 10:25:29","title":"Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM","abstract":"Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames. We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame. Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation. See project website https://steve-zeyu-zhang.github.io/MotionMamba/","sentences":["Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging.","Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it.","Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence.","To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs.","Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames.","We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame.","Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation.","See project website https://steve-zeyu-zhang.github.io/MotionMamba/"],"url":"http://arxiv.org/abs/2403.07487v1","category":"cs.CV"}
{"created":"2024-03-12 10:21:21","title":"PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates","abstract":"We introduce a surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit. We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions.   The results show that PMBO outperforms the classic Bayesian optimization and is robust with respect to the choice of its correlation function family and its hyper-parameter setting, which, on the contrary, need to be carefully tuned in classic Bayesian optimization. Remarkably, PMBO performs comparably with state-of-the-art evolutionary algorithms such as the Covariance Matrix Adaptation -- Evolution Strategy (CMA-ES). This finding suggests that PMBO emerges as the pivotal choice among surrogate-based optimization methods when addressing low-dimensional optimization problems. Hereby, the simple nature of polynomials opens the opportunity for interpretation and analysis of the inferred surrogate model, providing a macroscopic perspective on the landscape of the objective function.","sentences":["We introduce a surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO).","The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit.","We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions.   ","The results show that PMBO outperforms the classic Bayesian optimization and is robust with respect to the choice of its correlation function family and its hyper-parameter setting, which, on the contrary, need to be carefully tuned in classic Bayesian optimization.","Remarkably, PMBO performs comparably with state-of-the-art evolutionary algorithms such as the Covariance Matrix Adaptation -- Evolution Strategy (CMA-ES).","This finding suggests that PMBO emerges as the pivotal choice among surrogate-based optimization methods when addressing low-dimensional optimization problems.","Hereby, the simple nature of polynomials opens the opportunity for interpretation and analysis of the inferred surrogate model, providing a macroscopic perspective on the landscape of the objective function."],"url":"http://arxiv.org/abs/2403.07485v1","category":"math.OC"}
{"created":"2024-03-12 10:12:59","title":"Towards Graph Foundation Models for Personalization","abstract":"In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions. In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval. In this paper, we present a graph-based foundation modeling approach tailored to personalization. Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types. To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of nodes that accommodates all item types, and construct the graph using co-interaction signals, which inherently transcend content specificity. To facilitate practical generalization, we further couple the HGNN with an adaptation mechanism based on a two-tower (2T) architecture, which also operates agnostically to content type. This multi-stage approach ensures high scalability; while the HGNN produces general purpose embeddings, the 2T component models in a continuous space the sheer size of user-item interaction data. Our comprehensive approach has been rigorously tested and proven effective in delivering recommendations across a diverse array of products within a real-world, industrial audio streaming platform.","sentences":["In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions.","In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs).","While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval.","In this paper, we present a graph-based foundation modeling approach tailored to personalization.","Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types.","To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of nodes that accommodates all item types, and construct the graph using co-interaction signals, which inherently transcend content specificity.","To facilitate practical generalization, we further couple the HGNN with an adaptation mechanism based on a two-tower (2T) architecture, which also operates agnostically to content type.","This multi-stage approach ensures high scalability; while the HGNN produces general purpose embeddings, the 2T component models in a continuous space the sheer size of user-item interaction data.","Our comprehensive approach has been rigorously tested and proven effective in delivering recommendations across a diverse array of products within a real-world, industrial audio streaming platform."],"url":"http://arxiv.org/abs/2403.07478v1","category":"cs.IR"}
{"created":"2024-03-12 10:09:53","title":"Pedophysics: an open-source python package for soil geophysics","abstract":"This study introduces Pedophysics, an open-source Python package designed to facilitate solutions for users who work in the field of soil assessment using near-surface geophysical electromagnetic techniques. At the core of this software is the ability to translate geophysical data into specific soil properties (and vice-versa) using pedophysical models (PM). Pedophysical modelling techniques offer valuable insights into various realms including precision agriculture, soil health, resource prospecting, nutrient and land management, hydrogeology, and heritage conservation. In developing a tool for pedophysical modelling, some challenges emerged: selecting suitable PMs from the extensive literature, adapting these to specific conditions, and ensuring adequate data availability. While addressing these, we designed an automated workflow that implements robust PMs (selected after a throughout review), apply different modelling approaches based on soil characteristics and targeted properties, and employs pedotransfer functions and assumptions to integrate missing soil data into PMs. The capabilities of Pedophysics extend to handling complex scenarios such as fusing data from different instruments, incorporating continuous monitoring measurements, and soil calibration data. With these solutions, Pedophysics automates the process of deriving targeted soil and geophysical properties with state-of-art accuracy. Hereby, users can rely on Pedophysics to implement specific knowledge about pedophysical modeling. The software promotes global access to advanced soil geophysical solutions by being open-source and encouraging community contributions. Pedophysics is written in pure Python and has minimal dependencies. It can be easily installed from the Python Package Index (PyPI).","sentences":["This study introduces Pedophysics, an open-source Python package designed to facilitate solutions for users who work in the field of soil assessment using near-surface geophysical electromagnetic techniques.","At the core of this software is the ability to translate geophysical data into specific soil properties (and vice-versa) using pedophysical models (PM).","Pedophysical modelling techniques offer valuable insights into various realms including precision agriculture, soil health, resource prospecting, nutrient and land management, hydrogeology, and heritage conservation.","In developing a tool for pedophysical modelling, some challenges emerged: selecting suitable PMs from the extensive literature, adapting these to specific conditions, and ensuring adequate data availability.","While addressing these, we designed an automated workflow that implements robust PMs (selected after a throughout review), apply different modelling approaches based on soil characteristics and targeted properties, and employs pedotransfer functions and assumptions to integrate missing soil data into PMs.","The capabilities of Pedophysics extend to handling complex scenarios such as fusing data from different instruments, incorporating continuous monitoring measurements, and soil calibration data.","With these solutions, Pedophysics automates the process of deriving targeted soil and geophysical properties with state-of-art accuracy.","Hereby, users can rely on Pedophysics to implement specific knowledge about pedophysical modeling.","The software promotes global access to advanced soil geophysical solutions by being open-source and encouraging community contributions.","Pedophysics is written in pure Python and has minimal dependencies.","It can be easily installed from the Python Package Index (PyPI)."],"url":"http://arxiv.org/abs/2403.07473v1","category":"physics.geo-ph"}
{"created":"2024-03-12 09:58:12","title":"A time-adaptive finite element phase-field model suitable for rate-independent fracture mechanics","abstract":"The modeling of cracks is an important topic - both in engineering as well as in mathematics. Since crack propagation is characterized by a free boundary value problem (the geometry of the crack is not known beforehand, but part of the solution), approximations of the underlying sharp-interface problem based on phase-field models are often considered. Focusing on a rate-independent setting, these models are defined by a unidirectional gradient-flow of an energy functional. Since this energy functional is non-convex, the evolution of the variables such as the displacement field and the phase-field variable might be discontinuous in time leading to so-called brutal crack growth. For this reason, solution concepts have to be carefully chosen in order to predict discontinuities that are physically reasonable. One such concept is that of Balanced Viscosity solutions (BV solutions). This concept predicts physically sound energy trajectories that do not jump across energy barriers. The paper deals with a time-adaptive finite element phase-field model for rate-independent fracture which converges to BV solutions. The model is motivated by constraining the pseudo-velocity of the crack tip. The resulting constrained minimization problem is solved by the augmented Lagrangian method. Numerical examples highlight the predictive capabilities of the model and furthermore show the efficiency and the robustness of the final algorithm.","sentences":["The modeling of cracks is an important topic - both in engineering as well as in mathematics.","Since crack propagation is characterized by a free boundary value problem (the geometry of the crack is not known beforehand, but part of the solution), approximations of the underlying sharp-interface problem based on phase-field models are often considered.","Focusing on a rate-independent setting, these models are defined by a unidirectional gradient-flow of an energy functional.","Since this energy functional is non-convex, the evolution of the variables such as the displacement field and the phase-field variable might be discontinuous in time leading to so-called brutal crack growth.","For this reason, solution concepts have to be carefully chosen in order to predict discontinuities that are physically reasonable.","One such concept is that of Balanced Viscosity solutions (BV solutions).","This concept predicts physically sound energy trajectories that do not jump across energy barriers.","The paper deals with a time-adaptive finite element phase-field model for rate-independent fracture which converges to BV solutions.","The model is motivated by constraining the pseudo-velocity of the crack tip.","The resulting constrained minimization problem is solved by the augmented Lagrangian method.","Numerical examples highlight the predictive capabilities of the model and furthermore show the efficiency and the robustness of the final algorithm."],"url":"http://arxiv.org/abs/2403.07461v1","category":"cs.CE"}
{"created":"2024-03-12 09:56:50","title":"Localization-Delocalization Transitions in Non-Hermitian Aharonov-Bohm Cages","abstract":"A unique feature of non-Hermitian systems is the extreme sensitivity of the eigenspectrum to boundary conditions with the emergence of the non-Hermitian skin effect (NHSE). A NHSE originates from the point-gap topology of complex eigenspectrum, where an extensive number of eigenstates are anomalously localized at the boundary driven by nonreciprocal dissipation. Two different approaches to create localization are disorder and flat-band spectrum, and their interplay can lead to the anomalous inverse Anderson localization, where the Bernoulli anti-symmetric disorder induce mobility in a full-flat band system in the presence of Aharonov-Bohm (AB) Cage. In this work, we study the localization-delocalization transitions due to the interplay of the point-gap topology, flat band and correlated disorder in the one-dimensional rhombic lattice, where both its Hermitian and non-Hermitian structures show AB cage in the presence of magnetic flux. Although it remains the coexistence of localization and delocalization for the Hermitian rhombic lattice in the presence of the random anti-symmetric disorder, it surprisingly becomes complete delocalization, accompanied by the emergence of NHSE. To further study the effects from the Bernoulli anti-symmetric disorder, we found the similar NHSE due to the interplay of the point-gap topology, correlated disorder and flat bands. Our anomalous localization-delocalization property can be experimentally tested in the classical physical platform, such as electrical circuit.","sentences":["A unique feature of non-Hermitian systems is the extreme sensitivity of the eigenspectrum to boundary conditions with the emergence of the non-Hermitian skin effect (NHSE).","A NHSE originates from the point-gap topology of complex eigenspectrum, where an extensive number of eigenstates are anomalously localized at the boundary driven by nonreciprocal dissipation.","Two different approaches to create localization are disorder and flat-band spectrum, and their interplay can lead to the anomalous inverse Anderson localization, where the Bernoulli anti-symmetric disorder induce mobility in a full-flat band system in the presence of Aharonov-Bohm (AB) Cage.","In this work, we study the localization-delocalization transitions due to the interplay of the point-gap topology, flat band and correlated disorder in the one-dimensional rhombic lattice, where both its Hermitian and non-Hermitian structures show AB cage in the presence of magnetic flux.","Although it remains the coexistence of localization and delocalization for the Hermitian rhombic lattice in the presence of the random anti-symmetric disorder, it surprisingly becomes complete delocalization, accompanied by the emergence of NHSE.","To further study the effects from the Bernoulli anti-symmetric disorder, we found the similar NHSE due to the interplay of the point-gap topology, correlated disorder and flat bands.","Our anomalous localization-delocalization property can be experimentally tested in the classical physical platform, such as electrical circuit."],"url":"http://arxiv.org/abs/2403.07459v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 09:51:05","title":"A tutorial on multi-view autoencoders using the multi-view-AE library","abstract":"There has been a growing interest in recent years in modelling multiple modalities (or views) of data to for example, understand the relationship between modalities or to generate missing data. Multi-view autoencoders have gained significant traction for their adaptability and versatility in modelling multi-modal data, demonstrating an ability to tailor their approach to suit the characteristics of the data at hand. However, most multi-view autoencoders have inconsistent notation and are often implemented using different coding frameworks. To address this, we present a unified mathematical framework for multi-view autoencoders, consolidating their formulations. Moreover, we offer insights into the motivation and theoretical advantages of each model. To facilitate accessibility and practical use, we extend the documentation and functionality of the previously introduced \\texttt{multi-view-AE} library. This library offers Python implementations of numerous multi-view autoencoder models, presented within a user-friendly framework. Through benchmarking experiments, we evaluate our implementations against previous ones, demonstrating comparable or superior performance. This work aims to establish a cohesive foundation for multi-modal modelling, serving as a valuable educational resource in the field.","sentences":["There has been a growing interest in recent years in modelling multiple modalities (or views) of data to for example, understand the relationship between modalities or to generate missing data.","Multi-view autoencoders have gained significant traction for their adaptability and versatility in modelling multi-modal data, demonstrating an ability to tailor their approach to suit the characteristics of the data at hand.","However, most multi-view autoencoders have inconsistent notation and are often implemented using different coding frameworks.","To address this, we present a unified mathematical framework for multi-view autoencoders, consolidating their formulations.","Moreover, we offer insights into the motivation and theoretical advantages of each model.","To facilitate accessibility and practical use, we extend the documentation and functionality of the previously introduced \\texttt{multi-view-AE} library.","This library offers Python implementations of numerous multi-view autoencoder models, presented within a user-friendly framework.","Through benchmarking experiments, we evaluate our implementations against previous ones, demonstrating comparable or superior performance.","This work aims to establish a cohesive foundation for multi-modal modelling, serving as a valuable educational resource in the field."],"url":"http://arxiv.org/abs/2403.07456v1","category":"cs.LG"}
{"created":"2024-03-12 09:32:41","title":"Proxy Methods for Domain Adaptation","abstract":"We study the problem of domain adaptation under distribution shift, where the shift is due to a change in the distribution of an unobserved, latent variable that confounds both the covariates and the labels. In this setting, neither the covariate shift nor the label shift assumptions apply. Our approach to adaptation employs proximal causal learning, a technique for estimating causal effects in settings where proxies of unobserved confounders are available. We demonstrate that proxy variables allow for adaptation to distribution shift without explicitly recovering or modeling latent variables. We consider two settings, (i) Concept Bottleneck: an additional ''concept'' variable is observed that mediates the relationship between the covariates and labels; (ii) Multi-domain: training data from multiple source domains is available, where each source domain exhibits a different distribution over the latent confounder. We develop a two-stage kernel estimation approach to adapt to complex distribution shifts in both settings. In our experiments, we show that our approach outperforms other methods, notably those which explicitly recover the latent confounder.","sentences":["We study the problem of domain adaptation under distribution shift, where the shift is due to a change in the distribution of an unobserved, latent variable that confounds both the covariates and the labels.","In this setting, neither the covariate shift nor the label shift assumptions apply.","Our approach to adaptation employs proximal causal learning, a technique for estimating causal effects in settings where proxies of unobserved confounders are available.","We demonstrate that proxy variables allow for adaptation to distribution shift without explicitly recovering or modeling latent variables.","We consider two settings, (i) Concept Bottleneck: an additional ''concept'' variable is observed that mediates the relationship between the covariates and labels; (ii) Multi-domain: training data from multiple source domains is available, where each source domain exhibits a different distribution over the latent confounder.","We develop a two-stage kernel estimation approach to adapt to complex distribution shifts in both settings.","In our experiments, we show that our approach outperforms other methods, notably those which explicitly recover the latent confounder."],"url":"http://arxiv.org/abs/2403.07442v1","category":"cs.LG"}
{"created":"2024-03-12 09:32:25","title":"Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning","abstract":"Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameterization method for efficient fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA). MTLoRA aims to dynamically alter its spatial geometric structure by applying a transformation-matrix T to perform linear transformations, such as rotation, scaling, and translation, on the task-specific parameter matrix, generating new matrix feature patterns (eigenvectors) to mimic the fundamental influence of complex geometric structure feature patterns in the brain on functions, thereby enhancing the model's performance in downstream tasks. In Natural Language Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and the results reveal that MTLoRA achieves an overall performance increase of about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks, MTLoRA improves performance by an average of 0.95% and 0.31% in the DART and WebNLG tasks, respectively.","sentences":["Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs.","Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources.","Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity.","We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity.","In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameterization method for efficient fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).","MTLoRA aims to dynamically alter its spatial geometric structure by applying a transformation-matrix T to perform linear transformations, such as rotation, scaling, and translation, on the task-specific parameter matrix, generating new matrix feature patterns (eigenvectors) to mimic the fundamental influence of complex geometric structure feature patterns in the brain on functions, thereby enhancing the model's performance in downstream tasks.","In Natural Language Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and the results reveal that MTLoRA achieves an overall performance increase of about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks, MTLoRA improves performance by an average of 0.95% and 0.31% in the DART and WebNLG tasks, respectively."],"url":"http://arxiv.org/abs/2403.07440v1","category":"cs.CL"}
{"created":"2024-03-12 09:22:52","title":"JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object Detection","abstract":"Event-based moving object detection is a challenging task, where static background and moving object are mixed together. Typically, existing methods mainly align the background events to the same spatial coordinate system via motion compensation to distinguish the moving object. However, they neglect the potential spatial tailing effect of moving object events caused by excessive motion, which may affect the structure integrity of the extracted moving object. We discover that the moving object has a complete columnar structure in the point cloud composed of motion-compensated events along the timestamp. Motivated by this, we propose a novel joint spatio-temporal reasoning method for event-based moving object detection. Specifically, we first compensate the motion of background events using inertial measurement unit. In spatial reasoning stage, we project the compensated events into the same image coordinate, discretize the timestamp of events to obtain a time image that can reflect the motion confidence, and further segment the moving object through adaptive threshold on the time image. In temporal reasoning stage, we construct the events into a point cloud along timestamp, and use RANSAC algorithm to extract the columnar shape in the cloud for peeling off the background. Finally, we fuse the results from the two reasoning stages to extract the final moving object region. This joint spatio-temporal reasoning framework can effectively detect the moving object from motion confidence and geometric structure. Moreover, we conduct extensive experiments on various datasets to verify that the proposed method can improve the moving object detection accuracy by 13\\%.","sentences":["Event-based moving object detection is a challenging task, where static background and moving object are mixed together.","Typically, existing methods mainly align the background events to the same spatial coordinate system via motion compensation to distinguish the moving object.","However, they neglect the potential spatial tailing effect of moving object events caused by excessive motion, which may affect the structure integrity of the extracted moving object.","We discover that the moving object has a complete columnar structure in the point cloud composed of motion-compensated events along the timestamp.","Motivated by this, we propose a novel joint spatio-temporal reasoning method for event-based moving object detection.","Specifically, we first compensate the motion of background events using inertial measurement unit.","In spatial reasoning stage, we project the compensated events into the same image coordinate, discretize the timestamp of events to obtain a time image that can reflect the motion confidence, and further segment the moving object through adaptive threshold on the time image.","In temporal reasoning stage, we construct the events into a point cloud along timestamp, and use RANSAC algorithm to extract the columnar shape in the cloud for peeling off the background.","Finally, we fuse the results from the two reasoning stages to extract the final moving object region.","This joint spatio-temporal reasoning framework can effectively detect the moving object from motion confidence and geometric structure.","Moreover, we conduct extensive experiments on various datasets to verify that the proposed method can improve the moving object detection accuracy by 13\\%."],"url":"http://arxiv.org/abs/2403.07436v1","category":"cs.CV"}
{"created":"2024-03-12 09:17:21","title":"DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated MR Images","abstract":"We propose a new method that employs transfer learning techniques to effectively correct sampling selection errors introduced by sparse annotations during supervised learning for automated tumor segmentation. The practicality of current learning-based automated tissue classification approaches is severely impeded by their dependency on manually segmented training databases that need to be recreated for each scenario of application, site, or acquisition setup. The comprehensive annotation of reference datasets can be highly labor-intensive, complex, and error-prone. The proposed method derives high-quality classifiers for the different tissue classes from sparse and unambiguous annotations and employs domain adaptation techniques for effectively correcting sampling selection errors introduced by the sparse sampling. The new approach is validated on labeled, multi-modal MR images of 19 patients with malignant gliomas and by comparative analysis on the BraTS 2013 challenge data sets. Compared to training on fully labeled data, we reduced the time for labeling and training by a factor greater than 70 and 180 respectively without sacrificing accuracy. This dramatically eases the establishment and constant extension of large annotated databases in various scenarios and imaging setups and thus represents an important step towards practical applicability of learning-based approaches in tissue classification.","sentences":["We propose a new method that employs transfer learning techniques to effectively correct sampling selection errors introduced by sparse annotations during supervised learning for automated tumor segmentation.","The practicality of current learning-based automated tissue classification approaches is severely impeded by their dependency on manually segmented training databases that need to be recreated for each scenario of application, site, or acquisition setup.","The comprehensive annotation of reference datasets can be highly labor-intensive, complex, and error-prone.","The proposed method derives high-quality classifiers for the different tissue classes from sparse and unambiguous annotations and employs domain adaptation techniques for effectively correcting sampling selection errors introduced by the sparse sampling.","The new approach is validated on labeled, multi-modal MR images of 19 patients with malignant gliomas and by comparative analysis on the BraTS 2013 challenge data sets.","Compared to training on fully labeled data, we reduced the time for labeling and training by a factor greater than 70 and 180 respectively without sacrificing accuracy.","This dramatically eases the establishment and constant extension of large annotated databases in various scenarios and imaging setups and thus represents an important step towards practical applicability of learning-based approaches in tissue classification."],"url":"http://arxiv.org/abs/2403.07434v1","category":"eess.IV"}
{"created":"2024-03-12 09:11:02","title":"Input Data Adaptive Learning (IDAL) for Sub-acute Ischemic Stroke Lesion Segmentation","abstract":"In machine learning larger databases are usually associated with higher classification accuracy due to better generalization. This generalization may lead to non-optimal classifiers in some medical applications with highly variable expressions of pathologies. This paper presents a method for learning from a large training base by adaptively selecting optimal training samples for given input data. In this way heterogeneous databases are supported two-fold. First, by being able to deal with sparsely annotated data allows a quick inclusion of new data set and second, by training an input-dependent classifier. The proposed approach is evaluated using the SISS challenge. The proposed algorithm leads to a significant improvement of the classification accuracy.","sentences":["In machine learning larger databases are usually associated with higher classification accuracy due to better generalization.","This generalization may lead to non-optimal classifiers in some medical applications with highly variable expressions of pathologies.","This paper presents a method for learning from a large training base by adaptively selecting optimal training samples for given input data.","In this way heterogeneous databases are supported two-fold.","First, by being able to deal with sparsely annotated data allows a quick inclusion of new data set and second, by training an input-dependent classifier.","The proposed approach is evaluated using the SISS challenge.","The proposed algorithm leads to a significant improvement of the classification accuracy."],"url":"http://arxiv.org/abs/2403.07428v1","category":"eess.IV"}
{"created":"2024-03-12 09:00:58","title":"On the locomotion of the slider within a self-adaptive beam-slider system","abstract":"A beam-slider system is considered whose passive self-adaption relies on an intricate locomotion process involving both frictional and unilateral contact. The system also exploits geometric nonlinearity to achieve broadband efficacy. The dynamics of the system take place on three distinct time scales: On the fast time scale of the harmonic base excitation are the vibrations and the locomotion cycle. On the slow time scale, the slider changes its position along the beam, and the overall vibration level varies. Finally, on an intermediate time scale, strong modulations of the vibration amplitude may take place. In the present work, first, an analytical approximation of the beam's response on the slow time scale is derived as function of the slider position, which is a crucial prerequisite for identifying the main drivers of the slider's locomotion. Then, the most important forms of locomotion are described and approximations of their individual contribution to the overall slider transport are estimated. Finally, the theoretical results are compared against numerical results obtained from an experimentally validated model.","sentences":["A beam-slider system is considered whose passive self-adaption relies on an intricate locomotion process involving both frictional and unilateral contact.","The system also exploits geometric nonlinearity to achieve broadband efficacy.","The dynamics of the system take place on three distinct time scales: On the fast time scale of the harmonic base excitation are the vibrations and the locomotion cycle.","On the slow time scale, the slider changes its position along the beam, and the overall vibration level varies.","Finally, on an intermediate time scale, strong modulations of the vibration amplitude may take place.","In the present work, first, an analytical approximation of the beam's response on the slow time scale is derived as function of the slider position, which is a crucial prerequisite for identifying the main drivers of the slider's locomotion.","Then, the most important forms of locomotion are described and approximations of their individual contribution to the overall slider transport are estimated.","Finally, the theoretical results are compared against numerical results obtained from an experimentally validated model."],"url":"http://arxiv.org/abs/2403.07423v1","category":"eess.SY"}
{"created":"2024-03-12 08:34:05","title":"FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental Learning with Hill-Climbing","abstract":"Exemplar-free class-incremental learning (EFCIL) poses significant challenges, primarily due to catastrophic forgetting, necessitating a delicate balance between stability and plasticity to accurately recognize both new and previous classes. Traditional EFCIL approaches typically skew towards either model plasticity through successive fine-tuning or stability by employing a fixed feature extractor beyond the initial incremental state. Building upon the foundational FeTrIL framework, our research extends into novel experimental domains to examine the efficacy of various oversampling techniques and dynamic optimization strategies across multiple challenging datasets and incremental settings. We specifically explore how oversampling impacts accuracy relative to feature availability and how different optimization methodologies, including dynamic recalibration and feature pool diversification, influence incremental learning outcomes. The results from these comprehensive experiments, conducted on CIFAR100, Tiny-ImageNet, and an ImageNet-Subset, under-score the superior performance of FeTrIL in balancing accuracy for both new and past classes against ten contemporary methods. Notably, our extensions reveal the nuanced impacts of oversampling and optimization on EFCIL, contributing to a more refined understanding of feature-space manipulation for class incremental learning. FeTrIL and its extended analysis in this paper FeTrIL++ pave the way for more adaptable and efficient EFCIL methodologies, promising significant improvements in handling catastrophic forgetting without the need for exemplars.","sentences":["Exemplar-free class-incremental learning (EFCIL) poses significant challenges, primarily due to catastrophic forgetting, necessitating a delicate balance between stability and plasticity to accurately recognize both new and previous classes.","Traditional EFCIL approaches typically skew towards either model plasticity through successive fine-tuning or stability by employing a fixed feature extractor beyond the initial incremental state.","Building upon the foundational FeTrIL framework, our research extends into novel experimental domains to examine the efficacy of various oversampling techniques and dynamic optimization strategies across multiple challenging datasets and incremental settings.","We specifically explore how oversampling impacts accuracy relative to feature availability and how different optimization methodologies, including dynamic recalibration and feature pool diversification, influence incremental learning outcomes.","The results from these comprehensive experiments, conducted on CIFAR100, Tiny-ImageNet, and an ImageNet-Subset, under-score the superior performance of FeTrIL in balancing accuracy for both new and past classes against ten contemporary methods.","Notably, our extensions reveal the nuanced impacts of oversampling and optimization on EFCIL, contributing to a more refined understanding of feature-space manipulation for class incremental learning.","FeTrIL and its extended analysis in this paper FeTrIL++ pave the way for more adaptable and efficient EFCIL methodologies, promising significant improvements in handling catastrophic forgetting without the need for exemplars."],"url":"http://arxiv.org/abs/2403.07406v1","category":"cs.CV"}
{"created":"2024-03-12 08:33:26","title":"Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning","abstract":"Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task-wise Logits Correction (TLC), a simple method that equalizes this bias and improves the network performance for every given compute budget in the class-incremental setting. We assess the accuracy and computational cost of various continual learning techniques enhanced with early-exits and TLC across standard class-incremental learning benchmarks such as 10 split CIFAR100 and ImageNetSubset and show that TLC can achieve the accuracy of the standard methods using less than 70\\% of their computations. Moreover, at full computational budget, our method outperforms the accuracy of the standard counterparts by up to 15 percentage points. Our research underscores the inherent synergy between early-exit networks and continual learning, emphasizing their practical utility in resource-constrained environments.","sentences":["Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention.","These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources.","However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data.","This study aims to explore the continual learning of the early-exit networks.","We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting.","We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources.","Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task-wise Logits Correction (TLC), a simple method that equalizes this bias and improves the network performance for every given compute budget in the class-incremental setting.","We assess the accuracy and computational cost of various continual learning techniques enhanced with early-exits and TLC across standard class-incremental learning benchmarks such as 10 split CIFAR100 and ImageNetSubset and show that TLC can achieve the accuracy of the standard methods using less than 70\\% of their computations.","Moreover, at full computational budget, our method outperforms the accuracy of the standard counterparts by up to 15 percentage points.","Our research underscores the inherent synergy between early-exit networks and continual learning, emphasizing their practical utility in resource-constrained environments."],"url":"http://arxiv.org/abs/2403.07404v1","category":"cs.LG"}
{"created":"2024-03-12 07:27:02","title":"NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning","abstract":"Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps. Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications. Code is available at https://github.com/expectorlin/NavCoT.","sentences":["Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions.","Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability.","However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus.","This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner.","Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps.","Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision.","Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants.","Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset.","We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications.","Code is available at https://github.com/expectorlin/NavCoT."],"url":"http://arxiv.org/abs/2403.07376v1","category":"cs.CV"}
{"created":"2024-03-12 07:06:50","title":"Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery","abstract":"In this paper, we study the problem of Generalized Category Discovery (GCD), which aims to cluster unlabeled data from both known and unknown categories using the knowledge of labeled data from known categories. Current GCD methods rely on only visual cues, which however neglect the multi-modality perceptive nature of human cognitive processes in discovering novel visual categories. To address this, we propose a two-phase TextGCD framework to accomplish multi-modality GCD by exploiting powerful Visual-Language Models. TextGCD mainly includes a retrieval-based text generation (RTG) phase and a cross-modality co-teaching (CCT) phase. First, RTG constructs a visual lexicon using category tags from diverse datasets and attributes from Large Language Models, generating descriptive texts for images in a retrieval manner. Second, CCT leverages disparities between textual and visual modalities to foster mutual learning, thereby enhancing visual GCD. In addition, we design an adaptive class aligning strategy to ensure the alignment of category perceptions between modalities as well as a soft-voting mechanism to integrate multi-modality cues. Experiments on eight datasets show the large superiority of our approach over state-of-the-art methods. Notably, our approach outperforms the best competitor, by 7.7% and 10.8% in All accuracy on ImageNet-1k and CUB, respectively.","sentences":["In this paper, we study the problem of Generalized Category Discovery (GCD), which aims to cluster unlabeled data from both known and unknown categories using the knowledge of labeled data from known categories.","Current GCD methods rely on only visual cues, which however neglect the multi-modality perceptive nature of human cognitive processes in discovering novel visual categories.","To address this, we propose a two-phase TextGCD framework to accomplish multi-modality GCD by exploiting powerful Visual-Language Models.","TextGCD mainly includes a retrieval-based text generation (RTG) phase and a cross-modality co-teaching (CCT) phase.","First, RTG constructs a visual lexicon using category tags from diverse datasets and attributes from Large Language Models, generating descriptive texts for images in a retrieval manner.","Second, CCT leverages disparities between textual and visual modalities to foster mutual learning, thereby enhancing visual GCD.","In addition, we design an adaptive class aligning strategy to ensure the alignment of category perceptions between modalities as well as a soft-voting mechanism to integrate multi-modality cues.","Experiments on eight datasets show the large superiority of our approach over state-of-the-art methods.","Notably, our approach outperforms the best competitor, by 7.7% and 10.8% in All accuracy on ImageNet-1k and CUB, respectively."],"url":"http://arxiv.org/abs/2403.07369v1","category":"cs.CV"}
{"created":"2024-03-12 07:01:57","title":"Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors","abstract":"Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for unseen test data. The primary challenge of TTA is limited access to the entire test dataset during online updates, causing error accumulation. To mitigate it, TTA methods have utilized the model output's entropy as a confidence metric that aims to determine which samples have a lower likelihood of causing error. Through experimental studies, however, we observed the unreliability of entropy as a confidence metric for TTA under biased scenarios and theoretically revealed that it stems from the neglect of the influence of latent disentangled factors of data on predictions. Building upon these findings, we introduce a novel TTA method named Destroy Your Object (DeYO), which leverages a newly proposed confidence metric named Pseudo-Label Probability Difference (PLPD). PLPD quantifies the influence of the shape of an object on prediction by measuring the difference between predictions before and after applying an object-destructive transformation. DeYO consists of sample selection and sample weighting, which employ entropy and PLPD concurrently. For robust adaptation, DeYO prioritizes samples that dominantly incorporate shape information when making predictions. Our extensive experiments demonstrate the consistent superiority of DeYO over baseline methods across various scenarios, including biased and wild. Project page is publicly available at https://whitesnowdrop.github.io/DeYO/.","sentences":["Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for unseen test data.","The primary challenge of TTA is limited access to the entire test dataset during online updates, causing error accumulation.","To mitigate it, TTA methods have utilized the model output's entropy as a confidence metric that aims to determine which samples have a lower likelihood of causing error.","Through experimental studies, however, we observed the unreliability of entropy as a confidence metric for TTA under biased scenarios and theoretically revealed that it stems from the neglect of the influence of latent disentangled factors of data on predictions.","Building upon these findings, we introduce a novel TTA method named Destroy Your Object (DeYO), which leverages a newly proposed confidence metric named Pseudo-Label Probability Difference (PLPD).","PLPD quantifies the influence of the shape of an object on prediction by measuring the difference between predictions before and after applying an object-destructive transformation.","DeYO consists of sample selection and sample weighting, which employ entropy and PLPD concurrently.","For robust adaptation, DeYO prioritizes samples that dominantly incorporate shape information when making predictions.","Our extensive experiments demonstrate the consistent superiority of DeYO over baseline methods across various scenarios, including biased and wild.","Project page is publicly available at https://whitesnowdrop.github.io/DeYO/."],"url":"http://arxiv.org/abs/2403.07366v1","category":"cs.CV"}
{"created":"2024-03-12 06:58:37","title":"Hybrid Kinetics Embedding Framework for Dynamic PET Reconstruction","abstract":"In dynamic positron emission tomography (PET) reconstruction, the importance of leveraging the temporal dependence of the data has been well appreciated. Current deep-learning solutions can be categorized in two groups in the way the temporal dynamics is modeled: data-driven approaches use spatiotemporal neural networks to learn the temporal dynamics of tracer kinetics from data, which relies heavily on data supervision; physics-based approaches leverage \\textit{a priori} tracer kinetic models to focus on inferring their parameters, which relies heavily on the accuracy of the prior kinetic model. In this paper, we marry the strengths of these two approaches in a hybrid kinetics embedding (HyKE-Net) framework for dynamic PET reconstruction. We first introduce a novel \\textit{hybrid} model of tracer kinetics consisting of a physics-based function augmented by a neural component to account for its gap to data-generating tracer kinetics, both identifiable from data. We then embed this hybrid model at the latent space of an encoding-decoding framework to enable both supervised and unsupervised identification of the hybrid kinetics and thereby dynamic PET reconstruction. Through both phantom and real-data experiments, we demonstrate the benefits of HyKE-Net -- especially in unsupervised reconstructions -- over existing physics-based and data-driven baselines as well as its ablated formulations where the embedded tracer kinetics are purely physics-based, purely neural, or hybrid but with a non-adaptable neural component.","sentences":["In dynamic positron emission tomography (PET) reconstruction, the importance of leveraging the temporal dependence of the data has been well appreciated.","Current deep-learning solutions can be categorized in two groups in the way the temporal dynamics is modeled: data-driven approaches use spatiotemporal neural networks to learn the temporal dynamics of tracer kinetics from data, which relies heavily on data supervision; physics-based approaches leverage \\textit{a priori} tracer kinetic models to focus on inferring their parameters, which relies heavily on the accuracy of the prior kinetic model.","In this paper, we marry the strengths of these two approaches in a hybrid kinetics embedding (HyKE-Net) framework for dynamic PET reconstruction.","We first introduce a novel \\textit{hybrid} model of tracer kinetics consisting of a physics-based function augmented by a neural component to account for its gap to data-generating tracer kinetics, both identifiable from data.","We then embed this hybrid model at the latent space of an encoding-decoding framework to enable both supervised and unsupervised identification of the hybrid kinetics and thereby dynamic PET reconstruction.","Through both phantom and real-data experiments, we demonstrate the benefits of HyKE-Net -- especially in unsupervised reconstructions -- over existing physics-based and data-driven baselines as well as its ablated formulations where the embedded tracer kinetics are purely physics-based, purely neural, or hybrid but with a non-adaptable neural component."],"url":"http://arxiv.org/abs/2403.07364v1","category":"eess.IV"}
{"created":"2024-03-12 06:29:54","title":"Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning","abstract":"Continual learning requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed. It is rare, however, that the data and task changes are completely unpredictable. Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it. We show here that the combination of a large language model and an image generation model can similarly provide useful premonitions as to how a continual learning challenge might develop over time. We use the large language model to generate text descriptions of semantically related classes that might potentially appear in the data stream in future. These descriptions are then rendered using Stable Diffusion to generate new labelled image samples. The resulting synthetic dataset is employed for supervised pre-training, but is discarded prior to commencing continual learning, along with the pre-training classification head. We find that the backbone of our pre-trained networks can learn representations useful for the downstream continual learning problem, thus becoming a valuable input to any existing continual learning method. Although there are complexities arising from the domain gap between real and synthetic images, we show that pre-training models in this manner improves multiple Class Incremenal Learning (CIL) methods on fine-grained image classification benchmarks. Supporting code can be found at https://github.com/cl-premonition/premonition.","sentences":["Continual learning requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed.","It is rare, however, that the data and task changes are completely unpredictable.","Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it.","We show here that the combination of a large language model and an image generation model can similarly provide useful premonitions as to how a continual learning challenge might develop over time.","We use the large language model to generate text descriptions of semantically related classes that might potentially appear in the data stream in future.","These descriptions are then rendered using Stable Diffusion to generate new labelled image samples.","The resulting synthetic dataset is employed for supervised pre-training, but is discarded prior to commencing continual learning, along with the pre-training classification head.","We find that the backbone of our pre-trained networks can learn representations useful for the downstream continual learning problem, thus becoming a valuable input to any existing continual learning method.","Although there are complexities arising from the domain gap between real and synthetic images, we show that pre-training models in this manner improves multiple Class Incremenal Learning (CIL) methods on fine-grained image classification benchmarks.","Supporting code can be found at https://github.com/cl-premonition/premonition."],"url":"http://arxiv.org/abs/2403.07356v1","category":"cs.CV"}
{"created":"2024-03-12 05:43:16","title":"D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic Communications","abstract":"Semantic communications (SemCom) have emerged as a new paradigm for supporting sixth-generation applications, where semantic features of data are transmitted using artificial intelligence algorithms to attain high communication efficiencies. Most existing SemCom techniques utilize deep neural networks (DNNs) to implement analog source-channel mappings, which are incompatible with existing digital communication architectures. To address this issue, this paper proposes a novel framework of digital deep joint source-channel coding (D$^2$-JSCC) targeting image transmission in SemCom. The framework features digital source and channel codings that are jointly optimized to reduce the end-to-end (E2E) distortion. First, deep source coding with an adaptive density model is designed to encode semantic features according to their distributions. Second, digital channel coding is employed to protect encoded features against channel distortion. To facilitate their joint design, the E2E distortion is characterized as a function of the source and channel rates via the analysis of the Bayesian model and Lipschitz assumption on the DNNs. Then to minimize the E2E distortion, a two-step algorithm is proposed to control the source-channel rates for a given channel signal-to-noise ratio. Simulation results reveal that the proposed framework outperforms classic deep JSCC and mitigates the cliff and leveling-off effects, which commonly exist for separation-based approaches.","sentences":["Semantic communications (SemCom) have emerged as a new paradigm for supporting sixth-generation applications, where semantic features of data are transmitted using artificial intelligence algorithms to attain high communication efficiencies.","Most existing SemCom techniques utilize deep neural networks (DNNs) to implement analog source-channel mappings, which are incompatible with existing digital communication architectures.","To address this issue, this paper proposes a novel framework of digital deep joint source-channel coding (D$^2$-JSCC) targeting image transmission in SemCom.","The framework features digital source and channel codings that are jointly optimized to reduce the end-to-end (E2E) distortion.","First, deep source coding with an adaptive density model is designed to encode semantic features according to their distributions.","Second, digital channel coding is employed to protect encoded features against channel distortion.","To facilitate their joint design, the E2E distortion is characterized as a function of the source and channel rates via the analysis of the Bayesian model and Lipschitz assumption on the DNNs.","Then to minimize the E2E distortion, a two-step algorithm is proposed to control the source-channel rates for a given channel signal-to-noise ratio.","Simulation results reveal that the proposed framework outperforms classic deep JSCC and mitigates the cliff and leveling-off effects, which commonly exist for separation-based approaches."],"url":"http://arxiv.org/abs/2403.07338v2","category":"cs.IT"}
{"created":"2024-03-12 04:13:45","title":"Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models","abstract":"Large Multimodal Model (LMM) is a hot research topic in the computer vision area and has also demonstrated remarkable potential across multiple disciplinary fields. A recent trend is to further extend and enhance the perception capabilities of LMMs. The current methods follow the paradigm of adapting the visual task outputs to the format of the language model, which is the main component of a LMM. This adaptation leads to convenient development of such LMMs with minimal modifications, however, it overlooks the intrinsic characteristics of diverse visual tasks and hinders the learning of perception capabilities. To address this issue, we propose a novel LMM architecture named Lumen, a Large multimodal model with versatile vision-centric capability enhancement. We decouple the LMM's learning of perception capabilities into task-agnostic and task-specific stages. Lumen first promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks. Thus the output of the task-agnostic stage is a shared representation for all the tasks we address in this paper. Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts. Benefiting from such a decoupled design, our Lumen surpasses existing LMM-based approaches on the COCO detection benchmark with a clear margin and exhibits seamless scalability to additional visual tasks. Furthermore, we also conduct comprehensive ablation studies and generalization evaluations for deeper insights. The code will be released at https://github.com/SxJyJay/Lumen.","sentences":["Large Multimodal Model (LMM) is a hot research topic in the computer vision area and has also demonstrated remarkable potential across multiple disciplinary fields.","A recent trend is to further extend and enhance the perception capabilities of LMMs.","The current methods follow the paradigm of adapting the visual task outputs to the format of the language model, which is the main component of a LMM.","This adaptation leads to convenient development of such LMMs with minimal modifications, however, it overlooks the intrinsic characteristics of diverse visual tasks and hinders the learning of perception capabilities.","To address this issue, we propose a novel LMM architecture named Lumen, a Large multimodal model with versatile vision-centric capability enhancement.","We decouple the LMM's learning of perception capabilities into task-agnostic and task-specific stages.","Lumen first promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks.","Thus the output of the task-agnostic stage is a shared representation for all the tasks we address in this paper.","Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts.","Benefiting from such a decoupled design, our Lumen surpasses existing LMM-based approaches on the COCO detection benchmark with a clear margin and exhibits seamless scalability to additional visual tasks.","Furthermore, we also conduct comprehensive ablation studies and generalization evaluations for deeper insights.","The code will be released at https://github.com/SxJyJay/Lumen."],"url":"http://arxiv.org/abs/2403.07304v1","category":"cs.CV"}
{"created":"2024-03-12 04:10:06","title":"Dynamic U-Net: Adaptively Calibrate Features for Abdominal Multi-organ Segmentation","abstract":"U-Net has been widely used for segmenting abdominal organs, achieving promising performance. However, when it is used for multi-organ segmentation, first, it may be limited in exploiting global long-range contextual information due to the implementation of standard convolutions. Second, the use of spatial-wise downsampling (e.g., max pooling or strided convolutions) in the encoding path may lead to the loss of deformable or discriminative details. Third, features upsampled from the higher level are concatenated with those that persevered via skip connections. However, repeated downsampling and upsampling operations lead to misalignments between them and their concatenation degrades segmentation performance. To address these limitations, we propose Dynamically Calibrated Convolution (DCC), Dynamically Calibrated Downsampling (DCD), and Dynamically Calibrated Upsampling (DCU) modules, respectively. The DCC module can utilize global inter-dependencies between spatial and channel features to calibrate these features adaptively. The DCD module enables networks to adaptively preserve deformable or discriminative features during downsampling. The DCU module can dynamically align and calibrate upsampled features to eliminate misalignments before concatenations. We integrated the proposed modules into a standard U-Net, resulting in a new architecture, termed Dynamic U-Net. This architectural design enables U-Net to dynamically adjust features for different organs. We evaluated Dynamic U-Net in two abdominal multi-organ segmentation benchmarks. Dynamic U-Net achieved statistically improved segmentation accuracy compared with standard U-Net. Our code is available at https://github.com/sotiraslab/DynamicUNet.","sentences":["U-Net has been widely used for segmenting abdominal organs, achieving promising performance.","However, when it is used for multi-organ segmentation, first, it may be limited in exploiting global long-range contextual information due to the implementation of standard convolutions.","Second, the use of spatial-wise downsampling (e.g., max pooling or strided convolutions) in the encoding path may lead to the loss of deformable or discriminative details.","Third, features upsampled from the higher level are concatenated with those that persevered via skip connections.","However, repeated downsampling and upsampling operations lead to misalignments between them and their concatenation degrades segmentation performance.","To address these limitations, we propose Dynamically Calibrated Convolution (DCC), Dynamically Calibrated Downsampling (DCD), and Dynamically Calibrated Upsampling (DCU) modules, respectively.","The DCC module can utilize global inter-dependencies between spatial and channel features to calibrate these features adaptively.","The DCD module enables networks to adaptively preserve deformable or discriminative features during downsampling.","The DCU module can dynamically align and calibrate upsampled features to eliminate misalignments before concatenations.","We integrated the proposed modules into a standard U-Net, resulting in a new architecture, termed Dynamic U-Net.","This architectural design enables U-Net to dynamically adjust features for different organs.","We evaluated Dynamic U-Net in two abdominal multi-organ segmentation benchmarks.","Dynamic U-Net achieved statistically improved segmentation accuracy compared with standard U-Net.","Our code is available at https://github.com/sotiraslab/DynamicUNet."],"url":"http://arxiv.org/abs/2403.07303v1","category":"eess.IV"}
{"created":"2024-03-12 04:07:00","title":"Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller","abstract":"Storytelling aims to generate reasonable and vivid narratives based on an ordered image stream. The fidelity to the image story theme and the divergence of story plots attract readers to keep reading. Previous works iteratively improved the alignment of multiple modalities but ultimately resulted in the generation of simplistic storylines for image streams. In this work, we propose a new pipeline, termed LLaMS, to generate multimodal human-level stories that are embodied in expressiveness and consistency. Specifically, by fully exploiting the commonsense knowledge within the LLM, we first employ a sequence data auto-enhancement strategy to enhance factual content expression and leverage a textual reasoning architecture for expressive story generation and prediction. Secondly, we propose SQ-Adatpter module for story illustration generation which can maintain sequence consistency. Numerical results are conducted through human evaluation to verify the superiority of proposed LLaMS. Evaluations show that LLaMS achieves state-of-the-art storytelling performance and 86% correlation and 100% consistency win rate as compared with previous SOTA methods. Furthermore, ablation experiments are conducted to verify the effectiveness of proposed sequence data enhancement and SQ-Adapter.","sentences":["Storytelling aims to generate reasonable and vivid narratives based on an ordered image stream.","The fidelity to the image story theme and the divergence of story plots attract readers to keep reading.","Previous works iteratively improved the alignment of multiple modalities but ultimately resulted in the generation of simplistic storylines for image streams.","In this work, we propose a new pipeline, termed LLaMS, to generate multimodal human-level stories that are embodied in expressiveness and consistency.","Specifically, by fully exploiting the commonsense knowledge within the LLM, we first employ a sequence data auto-enhancement strategy to enhance factual content expression and leverage a textual reasoning architecture for expressive story generation and prediction.","Secondly, we propose SQ-Adatpter module for story illustration generation which can maintain sequence consistency.","Numerical results are conducted through human evaluation to verify the superiority of proposed LLaMS.","Evaluations show that LLaMS achieves state-of-the-art storytelling performance and 86% correlation and 100% consistency win rate as compared with previous SOTA methods.","Furthermore, ablation experiments are conducted to verify the effectiveness of proposed sequence data enhancement and SQ-Adapter."],"url":"http://arxiv.org/abs/2403.07301v1","category":"cs.CV"}
{"created":"2024-03-12 03:54:24","title":"Stability and Sharp Decay Estimates for 3D MHD Equations with Only Vertical Dissipation Near a Background Magnetic Field","abstract":"This paper is concerned with the stability and large-time behavior of 3D incompressible MHD equations with only vertical dissipation near a background magnetic field. By making full use of the dissipation generated by the background magnetic field, we first establish the global stability of the solutions in $H^3$-norm. Then, the optimal decay rates of the solutions are obtained, which are consistent with the 2D classical heat equation. Moreover, some enhanced decay rates of $(u_1,b_1)$ are also achieved. In other words, the decay estimates of the second or third component of velocity/magnetic field coincide with those of 2D heat kernel, while the first component behaves like the 3D heat kernel. This is mainly due to the divergence-free condition and the anisotropic structure. The results obtained improve the previous ones due to Lin-Wu-Zhu [24,25].","sentences":["This paper is concerned with the stability and large-time behavior of 3D incompressible MHD equations with only vertical dissipation near a background magnetic field.","By making full use of the dissipation generated by the background magnetic field, we first establish the global stability of the solutions in $H^3$-norm.","Then, the optimal decay rates of the solutions are obtained, which are consistent with the 2D classical heat equation.","Moreover, some enhanced decay rates of $(u_1,b_1)$ are also achieved.","In other words, the decay estimates of the second or third component of velocity/magnetic field coincide with those of 2D heat kernel, while the first component behaves like the 3D heat kernel.","This is mainly due to the divergence-free condition and the anisotropic structure.","The results obtained improve the previous ones due to Lin-Wu-Zhu","[24,25]."],"url":"http://arxiv.org/abs/2403.07293v1","category":"math.AP"}
{"created":"2024-03-12 03:44:46","title":"Learning Hierarchical Color Guidance for Depth Map Super-Resolution","abstract":"Color information is the most commonly used prior knowledge for depth map super-resolution (DSR), which can provide high-frequency boundary guidance for detail restoration. However, its role and functionality in DSR have not been fully developed. In this paper, we rethink the utilization of color information and propose a hierarchical color guidance network to achieve DSR. On the one hand, the low-level detail embedding module is designed to supplement high-frequency color information of depth features in a residual mask manner at the low-level stages. On the other hand, the high-level abstract guidance module is proposed to maintain semantic consistency in the reconstruction process by using a semantic mask that encodes the global guidance information. The color information of these two dimensions plays a role in the front and back ends of the attention-based feature projection (AFP) module in a more comprehensive form. Simultaneously, the AFP module integrates the multi-scale content enhancement block and adaptive attention projection block to make full use of multi-scale information and adaptively project critical restoration information in an attention manner for DSR. Compared with the state-of-the-art methods on four benchmark datasets, our method achieves more competitive performance both qualitatively and quantitatively.","sentences":["Color information is the most commonly used prior knowledge for depth map super-resolution (DSR), which can provide high-frequency boundary guidance for detail restoration.","However, its role and functionality in DSR have not been fully developed.","In this paper, we rethink the utilization of color information and propose a hierarchical color guidance network to achieve DSR.","On the one hand, the low-level detail embedding module is designed to supplement high-frequency color information of depth features in a residual mask manner at the low-level stages.","On the other hand, the high-level abstract guidance module is proposed to maintain semantic consistency in the reconstruction process by using a semantic mask that encodes the global guidance information.","The color information of these two dimensions plays a role in the front and back ends of the attention-based feature projection (AFP) module in a more comprehensive form.","Simultaneously, the AFP module integrates the multi-scale content enhancement block and adaptive attention projection block to make full use of multi-scale information and adaptively project critical restoration information in an attention manner for DSR.","Compared with the state-of-the-art methods on four benchmark datasets, our method achieves more competitive performance both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2403.07290v1","category":"cs.CV"}
{"created":"2024-03-12 03:44:40","title":"Rediscovering BCE Loss for Uniform Classification","abstract":"This paper introduces the concept of uniform classification, which employs a unified threshold to classify all samples rather than adaptive threshold classifying each individual sample. We also propose the uniform classification accuracy as a metric to measure the model's performance in uniform classification. Furthermore, begin with a naive loss, we mathematically derive a loss function suitable for the uniform classification, which is the BCE function integrated with a unified bias. We demonstrate the unified threshold could be learned via the bias. The extensive experiments on six classification datasets and three feature extraction models show that, compared to the SoftMax loss, the models trained with the BCE loss not only exhibit higher uniform classification accuracy but also higher sample-wise classification accuracy. In addition, the learned bias from BCE loss is very close to the unified threshold used in the uniform classification. The features extracted by the models trained with BCE loss not only possess uniformity but also demonstrate better intra-class compactness and inter-class distinctiveness, yielding superior performance on open-set tasks such as face recognition.","sentences":["This paper introduces the concept of uniform classification, which employs a unified threshold to classify all samples rather than adaptive threshold classifying each individual sample.","We also propose the uniform classification accuracy as a metric to measure the model's performance in uniform classification.","Furthermore, begin with a naive loss, we mathematically derive a loss function suitable for the uniform classification, which is the BCE function integrated with a unified bias.","We demonstrate the unified threshold could be learned via the bias.","The extensive experiments on six classification datasets and three feature extraction models show that, compared to the SoftMax loss, the models trained with the BCE loss not only exhibit higher uniform classification accuracy but also higher sample-wise classification accuracy.","In addition, the learned bias from BCE loss is very close to the unified threshold used in the uniform classification.","The features extracted by the models trained with BCE loss not only possess uniformity but also demonstrate better intra-class compactness and inter-class distinctiveness, yielding superior performance on open-set tasks such as face recognition."],"url":"http://arxiv.org/abs/2403.07289v1","category":"cs.CV"}
{"created":"2024-03-12 03:34:03","title":"SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object Detection","abstract":"Sparse 3D detectors have received significant attention since the query-based paradigm embraces low latency without explicit dense BEV feature construction. However, these detectors achieve worse performance than their dense counterparts. In this paper, we find the key to bridging the performance gap is to enhance the awareness of rich representations in two modalities. Here, we present a high-performance fully sparse detector for end-to-end multi-modality 3D object detection. The detector, termed SparseLIF, contains three key designs, which are (1) Perspective-Aware Query Generation (PAQG) to generate high-quality 3D queries with perspective priors, (2) RoI-Aware Sampling (RIAS) to further refine prior queries by sampling RoI features from each modality, (3) Uncertainty-Aware Fusion (UAF) to precisely quantify the uncertainty of each sensor modality and adaptively conduct final multi-modality fusion, thus achieving great robustness against sensor noises. By the time of submission (2024/03/08), SparseLIF achieves state-of-the-art performance on the nuScenes dataset, ranking 1st on both validation set and test benchmark, outperforming all state-of-the-art 3D object detectors by a notable margin. The source code will be released upon acceptance.","sentences":["Sparse 3D detectors have received significant attention since the query-based paradigm embraces low latency without explicit dense BEV feature construction.","However, these detectors achieve worse performance than their dense counterparts.","In this paper, we find the key to bridging the performance gap is to enhance the awareness of rich representations in two modalities.","Here, we present a high-performance fully sparse detector for end-to-end multi-modality 3D object detection.","The detector, termed SparseLIF, contains three key designs, which are (1) Perspective-Aware Query Generation (PAQG) to generate high-quality 3D queries with perspective priors, (2) RoI-Aware Sampling (RIAS) to further refine prior queries by sampling RoI features from each modality, (3) Uncertainty-Aware Fusion (UAF) to precisely quantify the uncertainty of each sensor modality and adaptively conduct final multi-modality fusion, thus achieving great robustness against sensor noises.","By the time of submission (2024/03/08), SparseLIF achieves state-of-the-art performance on the nuScenes dataset, ranking 1st on both validation set and test benchmark, outperforming all state-of-the-art 3D object detectors by a notable margin.","The source code will be released upon acceptance."],"url":"http://arxiv.org/abs/2403.07284v1","category":"cs.CV"}
{"created":"2024-03-12 03:30:04","title":"A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism","abstract":"As Large Language Models (LLMs) gain great success in real-world applications, an increasing number of users are seeking to develop and deploy their customized LLMs through cloud services. Nonetheless, in some specific domains, there are still concerns regarding cost and trade-offs between privacy issues and accuracy. In this study, we introduce a cost-effective and self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With carefully designed horizontal and vertical shaking operators, we can achieve comparable accuracy results with SOTA privacy-preserving LLM schemes using Cryptography-based or Differential Privacy-based methods. Experiments also show that with the CypherTalk framework, users can achieve reliable accuracy when using optimized shaking operator settings. To our best knowledge, this is the first work that considers cost, and trade-off between model utility and privacy in LLM scenarios.","sentences":["As Large Language Models (LLMs) gain great success in real-world applications, an increasing number of users are seeking to develop and deploy their customized LLMs through cloud services.","Nonetheless, in some specific domains, there are still concerns regarding cost and trade-offs between privacy issues and accuracy.","In this study, we introduce a cost-effective and self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk.","With carefully designed horizontal and vertical shaking operators, we can achieve comparable accuracy results with SOTA privacy-preserving LLM schemes using Cryptography-based or Differential Privacy-based methods.","Experiments also show that with the CypherTalk framework, users can achieve reliable accuracy when using optimized shaking operator settings.","To our best knowledge, this is the first work that considers cost, and trade-off between model utility and privacy in LLM scenarios."],"url":"http://arxiv.org/abs/2403.07283v1","category":"cs.CR"}
{"created":"2024-03-12 02:45:24","title":"Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction","abstract":"Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving. We consider quantifying such uncertainty for multi-object detection. In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes. One challenge in doing so is that bounding box predictions are conditioned on the object's class label. Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals for the bounding boxes. This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, ensuring their usefulness when maximal safety assurances are required. Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage across sizes. Validating our two-step approach on real-world datasets for 2D bounding box localization, we find that desired coverage levels are satisfied with actionably tight predictive uncertainty intervals.","sentences":["Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving.","We consider quantifying such uncertainty for multi-object detection.","In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes.","One challenge in doing so is that bounding box predictions are conditioned on the object's class label.","Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals for the bounding boxes.","This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, ensuring their usefulness when maximal safety assurances are required.","Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage across sizes.","Validating our two-step approach on real-world datasets for 2D bounding box localization, we find that desired coverage levels are satisfied with actionably tight predictive uncertainty intervals."],"url":"http://arxiv.org/abs/2403.07263v1","category":"cs.CV"}
{"created":"2024-03-12 02:05:06","title":"Dataset Condensation for Time Series Classification via Dual Domain Matching","abstract":"Time series data has been demonstrated to be crucial in various research fields. The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network. Recently, a technique named \\textit{Dataset Condensation} has emerged as a solution to this problem. This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification. However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain. In this paper, we propose a novel framework named Dataset \\textit{\\textbf{Cond}}ensation for \\textit{\\textbf{T}}ime \\textit{\\textbf{S}}eries \\textit{\\textbf{C}}lassification via Dual Domain Matching (\\textbf{CondTSC}) which focuses on the time series classification dataset condensation task. Different from previous methods, our proposed framework aims to generate a condensed dataset that matches the surrogate objectives in both the time and frequency domains. Specifically, CondTSC incorporates multi-view data augmentation, dual domain training, and dual surrogate objectives to enhance the dataset condensation process in the time and frequency domains. Through extensive experiments, we demonstrate the effectiveness of our proposed framework, which outperforms other baselines and learns a condensed synthetic dataset that exhibits desirable characteristics such as conforming to the distribution of the original data.","sentences":["Time series data has been demonstrated to be crucial in various research fields.","The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network.","Recently, a technique named \\textit{Dataset Condensation} has emerged as a solution to this problem.","This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification.","However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain.","In this paper, we propose a novel framework named Dataset \\textit{\\textbf{Cond}}ensation for \\textit{\\textbf{T}}ime \\textit{\\textbf{S}}eries \\textit{\\textbf{C}}lassification via Dual Domain Matching (\\textbf{CondTSC}) which focuses on the time series classification dataset condensation task.","Different from previous methods, our proposed framework aims to generate a condensed dataset that matches the surrogate objectives in both the time and frequency domains.","Specifically, CondTSC incorporates multi-view data augmentation, dual domain training, and dual surrogate objectives to enhance the dataset condensation process in the time and frequency domains.","Through extensive experiments, we demonstrate the effectiveness of our proposed framework, which outperforms other baselines and learns a condensed synthetic dataset that exhibits desirable characteristics such as conforming to the distribution of the original data."],"url":"http://arxiv.org/abs/2403.07245v1","category":"cs.LG"}
{"created":"2024-03-12 01:05:25","title":"It's All About Your Sketch: Democratising Sketch Control in Diffusion Models","abstract":"This paper unravels the potential of sketches for diffusion models, addressing the deceptive promise of direct sketch control in generative AI. We importantly democratise the process, enabling amateur sketches to generate precise images, living up to the commitment of \"what you sketch is what you get\". A pilot study underscores the necessity, revealing that deformities in existing models stem from spatial-conditioning. To rectify this, we propose an abstraction-aware framework, utilising a sketch adapter, adaptive time-step sampling, and discriminative guidance from a pre-trained fine-grained sketch-based image retrieval model, working synergistically to reinforce fine-grained sketch-photo association. Our approach operates seamlessly during inference without the need for textual prompts; a simple, rough sketch akin to what you and I can create suffices! We welcome everyone to examine results presented in the paper and its supplementary. Contributions include democratising sketch control, introducing an abstraction-aware framework, and leveraging discriminative guidance, validated through extensive experiments.","sentences":["This paper unravels the potential of sketches for diffusion models, addressing the deceptive promise of direct sketch control in generative AI.","We importantly democratise the process, enabling amateur sketches to generate precise images, living up to the commitment of \"what you sketch is what you get\".","A pilot study underscores the necessity, revealing that deformities in existing models stem from spatial-conditioning.","To rectify this, we propose an abstraction-aware framework, utilising a sketch adapter, adaptive time-step sampling, and discriminative guidance from a pre-trained fine-grained sketch-based image retrieval model, working synergistically to reinforce fine-grained sketch-photo association.","Our approach operates seamlessly during inference without the need for textual prompts; a simple, rough sketch akin to what you and I can create suffices!","We welcome everyone to examine results presented in the paper and its supplementary.","Contributions include democratising sketch control, introducing an abstraction-aware framework, and leveraging discriminative guidance, validated through extensive experiments."],"url":"http://arxiv.org/abs/2403.07234v1","category":"cs.CV"}
{"created":"2024-03-12 00:08:54","title":"Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control","abstract":"The paper presents a technique using reinforcement learning (RL) to adapt the control gains of a quadcopter controller. Specifically, we employed Proximal Policy Optimization (PPO) to train a policy which adapts the gains of a cascaded feedback controller in-flight. The primary goal of this controller is to minimize tracking error while following a specified trajectory. The paper's key objective is to analyze the effectiveness of the adaptive gain policy and compare it to the performance of a static gain control algorithm, where the Integral Squared Error and Integral Time Squared Error are used as metrics. The results show that the adaptive gain scheme achieves over 40$\\%$ decrease in tracking error as compared to the static gain controller.","sentences":["The paper presents a technique using reinforcement learning (RL) to adapt the control gains of a quadcopter controller.","Specifically, we employed Proximal Policy Optimization (PPO) to train a policy which adapts the gains of a cascaded feedback controller in-flight.","The primary goal of this controller is to minimize tracking error while following a specified trajectory.","The paper's key objective is to analyze the effectiveness of the adaptive gain policy and compare it to the performance of a static gain control algorithm, where the Integral Squared Error and Integral Time Squared Error are used as metrics.","The results show that the adaptive gain scheme achieves over 40$\\%$ decrease in tracking error as compared to the static gain controller."],"url":"http://arxiv.org/abs/2403.07216v1","category":"eess.SY"}
{"created":"2024-03-11 22:47:26","title":"Computing $p$-presentation distances is hard","abstract":"Recently, $p$-presentation distances for $p\\in [1,\\infty]$ were introduced for merge trees and multiparameter persistence modules as more sensitive variations of the respective interleaving distances ($p=\\infty$). It is well-known that computing the interleaving distance is NP-hard in both cases. We extend this result by showing that computing the $p$-presentation distance is NP-hard for all $p\\in [1,\\infty)$ for both merge trees and $t$-parameter persistence modules for any $t\\geq 2$. Though the details differ, both proofs follow the same novel strategy, suggesting that our approach can be adapted to proving the NP-hardness of other distances based on sums or $p$-norms.","sentences":["Recently, $p$-presentation distances for $p\\in [1,\\infty]$ were introduced for merge trees and multiparameter persistence modules as more sensitive variations of the respective interleaving distances ($p=\\infty$).","It is well-known that computing the interleaving distance is NP-hard in both cases.","We extend this result by showing that computing the $p$-presentation distance is NP-hard for all $p\\in [1,\\infty)$ for both merge trees and $t$-parameter persistence modules for any $t\\geq 2$.","Though the details differ, both proofs follow the same novel strategy, suggesting that our approach can be adapted to proving the NP-hardness of other distances based on sums or $p$-norms."],"url":"http://arxiv.org/abs/2403.07200v1","category":"cs.CG"}
{"created":"2024-03-11 22:26:45","title":"Accelerating Interface Adaptation with User-Friendly Priors","abstract":"Robots often need to convey information to human users. For example, robots can leverage visual, auditory, and haptic interfaces to display their intent or express their internal state. In some scenarios there are socially agreed upon conventions for what these signals mean: e.g., a red light indicates an autonomous car is slowing down. But as robots develop new capabilities and seek to convey more complex data, the meaning behind their signals is not always mutually understood: one user might think a flashing light indicates the autonomous car is an aggressive driver, while another user might think the same signal means the autonomous car is defensive. In this paper we enable robots to adapt their interfaces to the current user so that the human's personalized interpretation is aligned with the robot's meaning. We start with an information theoretic end-to-end approach, which automatically tunes the interface policy to optimize the correlation between human and robot. But to ensure that this learning policy is intuitive -- and to accelerate how quickly the interface adapts to the human -- we recognize that humans have priors over how interfaces should function. For instance, humans expect interface signals to be proportional and convex. Our approach biases the robot's interface towards these priors, resulting in signals that are adapted to the current user while still following social expectations. Our simulations and user study results across $15$ participants suggest that these priors improve robot-to-human communication. See videos here: https://youtu.be/Re3OLg57hp8","sentences":["Robots often need to convey information to human users.","For example, robots can leverage visual, auditory, and haptic interfaces to display their intent or express their internal state.","In some scenarios there are socially agreed upon conventions for what these signals mean: e.g., a red light indicates an autonomous car is slowing down.","But as robots develop new capabilities and seek to convey more complex data, the meaning behind their signals is not always mutually understood: one user might think a flashing light indicates the autonomous car is an aggressive driver, while another user might think the same signal means the autonomous car is defensive.","In this paper we enable robots to adapt their interfaces to the current user so that the human's personalized interpretation is aligned with the robot's meaning.","We start with an information theoretic end-to-end approach, which automatically tunes the interface policy to optimize the correlation between human and robot.","But to ensure that this learning policy is intuitive -- and to accelerate how quickly the interface adapts to the human -- we recognize that humans have priors over how interfaces should function.","For instance, humans expect interface signals to be proportional and convex.","Our approach biases the robot's interface towards these priors, resulting in signals that are adapted to the current user while still following social expectations.","Our simulations and user study results across $15$ participants suggest that these priors improve robot-to-human communication.","See videos here: https://youtu.be/Re3OLg57hp8"],"url":"http://arxiv.org/abs/2403.07192v1","category":"cs.RO"}
{"created":"2024-03-11 22:00:39","title":"UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation","abstract":"We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of few-shot transfer to different PDE families, coefficients, and resolutions.","sentences":["We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions.","UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators.","We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning.","By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results.","UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered.","Meanwhile, it is capable of few-shot transfer to different PDE families, coefficients, and resolutions."],"url":"http://arxiv.org/abs/2403.07187v1","category":"cs.LG"}
{"created":"2024-03-11 21:45:48","title":"Study of the Impact of the Big Data Era on Accounting and Auditing","abstract":"Big data revolutionizes accounting and auditing, offering deep insights but also introducing challenges like data privacy and security. With data from IoT, social media, and transactions, traditional practices are evolving. Professionals must adapt to these changes, utilizing AI and machine learning for efficient data analysis and anomaly detection. Key to overcoming these challenges are enhanced analytics tools, continuous learning, and industry collaboration. By addressing these areas, the accounting and auditing fields can harness big data's potential while ensuring accuracy, transparency, and integrity in financial reporting. Keywords: Big Data, Accounting, Audit, Data Privacy, AI, Machine Learning, Transparency.","sentences":["Big data revolutionizes accounting and auditing, offering deep insights but also introducing challenges like data privacy and security.","With data from IoT, social media, and transactions, traditional practices are evolving.","Professionals must adapt to these changes, utilizing AI and machine learning for efficient data analysis and anomaly detection.","Key to overcoming these challenges are enhanced analytics tools, continuous learning, and industry collaboration.","By addressing these areas, the accounting and auditing fields can harness big data's potential while ensuring accuracy, transparency, and integrity in financial reporting.","Keywords: Big Data, Accounting, Audit, Data Privacy, AI, Machine Learning, Transparency."],"url":"http://arxiv.org/abs/2403.07180v1","category":"q-fin.GN"}
{"created":"2024-03-11 21:38:41","title":"Collusive Outcomes Without Collusion","abstract":"We develop a model of algorithmic pricing that shuts down every channel for explicit or implicit collusion while still generating collusive outcomes. We analyze the dynamics of a duopoly market where both firms use pricing algorithms consisting of a parameterized family of model specifications. The firms update both the parameters and the weights on models to adapt endogenously to market outcomes. We show that the market experiences recurrent episodes where both firms set prices at collusive levels. We analytically characterize the dynamics of the model, using large deviation theory to explain the recurrent episodes of collusive outcomes. Our results show that collusive outcomes may be a recurrent feature of algorithmic environments with complementarities and endogenous adaptation, providing a challenge for competition policy.","sentences":["We develop a model of algorithmic pricing that shuts down every channel for explicit or implicit collusion while still generating collusive outcomes.","We analyze the dynamics of a duopoly market where both firms use pricing algorithms consisting of a parameterized family of model specifications.","The firms update both the parameters and the weights on models to adapt endogenously to market outcomes.","We show that the market experiences recurrent episodes where both firms set prices at collusive levels.","We analytically characterize the dynamics of the model, using large deviation theory to explain the recurrent episodes of collusive outcomes.","Our results show that collusive outcomes may be a recurrent feature of algorithmic environments with complementarities and endogenous adaptation, providing a challenge for competition policy."],"url":"http://arxiv.org/abs/2403.07177v1","category":"econ.TH"}
{"created":"2024-03-11 19:12:50","title":"Operator size growth in Lindbladian SYK","abstract":"We investigate the growth of operator size in the Lindbladian SYK model with $q$-body interaction terms and linear jump terms at finite dissipation strength. We compute the operator size as well as its distribution numerically at finite $q$ and analytically at large $q$. With dissipative (productive) jump terms, the size converges to a value smaller (larger) than half the number of Majorana fermions. At weak dissipation, the evolution of operator size displays a quadratic-exponential-plateau behavior. The plateau value is determined by the ratios between the coupling of the interaction and the linear jump term in the large $q$ limit. The operator size distribution remains localized in the finite size region even at late times, contrasting with the unitary case. Moreover, we also derived the time-independent orthogonal basis for operator expansion which exhibits the ``operator size concentration'' at finite dissipation. Finally, we observe that the uncertainty relation for operator size growth is saturated at large $q$, leading to a classical dynamics of the operator size growth with dissipation.","sentences":["We investigate the growth of operator size in the Lindbladian SYK model with $q$-body interaction terms and linear jump terms at finite dissipation strength.","We compute the operator size as well as its distribution numerically at finite $q$ and analytically at large $q$. With dissipative (productive) jump terms, the size converges to a value smaller (larger) than half the number of Majorana fermions.","At weak dissipation, the evolution of operator size displays a quadratic-exponential-plateau behavior.","The plateau value is determined by the ratios between the coupling of the interaction and the linear jump term in the large $q$ limit.","The operator size distribution remains localized in the finite size region even at late times, contrasting with the unitary case.","Moreover, we also derived the time-independent orthogonal basis for operator expansion which exhibits the ``operator size concentration'' at finite dissipation.","Finally, we observe that the uncertainty relation for operator size growth is saturated at large $q$, leading to a classical dynamics of the operator size growth with dissipation."],"url":"http://arxiv.org/abs/2403.07115v1","category":"hep-th"}
{"created":"2024-03-11 18:26:02","title":"SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation","abstract":"Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but low computational devices while leave the parameters containing general information on the high computational devices.","sentences":["Large language models(LLMs) have shown its outperforming ability on various tasks and question answering.","However, LLMs require high computation cost and large memory cost.","At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information.","In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints.","Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency.","Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.","Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but low computational devices while leave the parameters containing general information on the high computational devices."],"url":"http://arxiv.org/abs/2403.07088v1","category":"cs.CL"}
{"created":"2024-03-11 18:00:47","title":"Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models","abstract":"Self-Supervised Learning (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks. However, SSL strategies must be adapted to the type of training data and downstream tasks required. We propose RS3L, a novel simulation-based SSL strategy that employs a method of re-simulation to drive data augmentation for contrastive learning. By intervening in the middle of the simulation process and re-running simulation components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator. Using experiments from high-energy physics, we explore how this strategy may enable the development of a foundation model; we show how R3SL pre-training enables powerful performance in downstream tasks such as discrimination of a variety of objects and uncertainty mitigation. In addition to our results, we make the RS3L dataset publicly available for further studies on how to improve SSL strategies.","sentences":["Self-Supervised Learning (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks.","However, SSL strategies must be adapted to the type of training data and downstream tasks required.","We propose RS3L, a novel simulation-based SSL strategy that employs a method of re-simulation to drive data augmentation for contrastive learning.","By intervening in the middle of the simulation process and re-running simulation components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator.","Using experiments from high-energy physics, we explore how this strategy may enable the development of a foundation model; we show how R3SL pre-training enables powerful performance in downstream tasks such as discrimination of a variety of objects and uncertainty mitigation.","In addition to our results, we make the RS3L dataset publicly available for further studies on how to improve SSL strategies."],"url":"http://arxiv.org/abs/2403.07066v1","category":"hep-ph"}
{"created":"2024-03-11 17:59:41","title":"Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling","abstract":"In this paper, we introduce Attention Prompt Tuning (APT) - a computationally efficient variant of prompt tuning for video-based applications such as action recognition. Prompt tuning approaches involve injecting a set of learnable prompts along with data tokens during fine-tuning while keeping the backbone frozen. This approach greatly reduces the number of learnable parameters compared to full tuning. For image-based downstream tasks, normally a couple of learnable prompts achieve results close to those of full tuning. However, videos, which contain more complex spatiotemporal information, require hundreds of tunable prompts to achieve reasonably good results. This reduces the parameter efficiency observed in images and significantly increases latency and the number of floating-point operations (FLOPs) during inference. To tackle these issues, we directly inject the prompts into the keys and values of the non-local attention mechanism within the transformer block. Additionally, we introduce a novel prompt reparameterization technique to make APT more robust against hyperparameter selection. The proposed APT approach greatly reduces the number of FLOPs and latency while achieving a significant performance boost over the existing parameter-efficient tuning methods on UCF101, HMDB51, and SSv2 datasets for action recognition. The code and pre-trained models are available at https://github.com/wgcban/apt","sentences":["In this paper, we introduce Attention Prompt Tuning (APT) - a computationally efficient variant of prompt tuning for video-based applications such as action recognition.","Prompt tuning approaches involve injecting a set of learnable prompts along with data tokens during fine-tuning while keeping the backbone frozen.","This approach greatly reduces the number of learnable parameters compared to full tuning.","For image-based downstream tasks, normally a couple of learnable prompts achieve results close to those of full tuning.","However, videos, which contain more complex spatiotemporal information, require hundreds of tunable prompts to achieve reasonably good results.","This reduces the parameter efficiency observed in images and significantly increases latency and the number of floating-point operations (FLOPs) during inference.","To tackle these issues, we directly inject the prompts into the keys and values of the non-local attention mechanism within the transformer block.","Additionally, we introduce a novel prompt reparameterization technique to make APT more robust against hyperparameter selection.","The proposed APT approach greatly reduces the number of FLOPs and latency while achieving a significant performance boost over the existing parameter-efficient tuning methods on UCF101, HMDB51, and SSv2 datasets for action recognition.","The code and pre-trained models are available at https://github.com/wgcban/apt"],"url":"http://arxiv.org/abs/2403.06978v1","category":"cs.CV"}
{"created":"2024-03-11 17:59:34","title":"VideoMamba: State Space Model for Efficient Video Understanding","abstract":"Addressing the dual challenges of local redundancy and global dependencies in video understanding, this work innovatively adapts the Mamba to the video domain. The proposed VideoMamba overcomes the limitations of existing 3D convolution neural networks and video transformers. Its linear-complexity operator enables efficient long-term modeling, which is crucial for high-resolution long video understanding. Extensive evaluations reveal VideoMamba's four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel self-distillation technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in multi-modal contexts. Through these distinct advantages, VideoMamba sets a new benchmark for video understanding, offering a scalable and efficient solution for comprehensive video understanding. All the code and models are available at https://github.com/OpenGVLab/VideoMamba.","sentences":["Addressing the dual challenges of local redundancy and global dependencies in video understanding, this work innovatively adapts the Mamba to the video domain.","The proposed VideoMamba overcomes the limitations of existing 3D convolution neural networks and video transformers.","Its linear-complexity operator enables efficient long-term modeling, which is crucial for high-resolution long video understanding.","Extensive evaluations reveal VideoMamba's four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel self-distillation technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in multi-modal contexts.","Through these distinct advantages, VideoMamba sets a new benchmark for video understanding, offering a scalable and efficient solution for comprehensive video understanding.","All the code and models are available at https://github.com/OpenGVLab/VideoMamba."],"url":"http://arxiv.org/abs/2403.06977v2","category":"cs.CV"}
{"created":"2024-03-11 17:59:31","title":"BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion","abstract":"Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of diffusion models (DMs). Despite these advancements, current DM adaptations for inpainting, which involve modifications to the sampling strategy or the development of inpainting-specific DMs, frequently suffer from semantic inconsistencies and reduced image quality. Addressing these challenges, our work introduces a novel paradigm: the division of masked image features and noisy latent into separate branches. This division dramatically diminishes the model's learning load, facilitating a nuanced incorporation of essential masked image information in a hierarchical fashion. Herein, we present BrushNet, a novel plug-and-play dual-branch model engineered to embed pixel-level masked image features into any pre-trained DM, guaranteeing coherent and enhanced image inpainting outcomes. Additionally, we introduce BrushData and BrushBench to facilitate segmentation-based inpainting training and performance assessment. Our extensive experimental analysis demonstrates BrushNet's superior performance over existing models across seven key metrics, including image quality, mask region preservation, and textual coherence.","sentences":["Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of diffusion models (DMs).","Despite these advancements, current DM adaptations for inpainting, which involve modifications to the sampling strategy or the development of inpainting-specific DMs, frequently suffer from semantic inconsistencies and reduced image quality.","Addressing these challenges, our work introduces a novel paradigm: the division of masked image features and noisy latent into separate branches.","This division dramatically diminishes the model's learning load, facilitating a nuanced incorporation of essential masked image information in a hierarchical fashion.","Herein, we present BrushNet, a novel plug-and-play dual-branch model engineered to embed pixel-level masked image features into any pre-trained DM, guaranteeing coherent and enhanced image inpainting outcomes.","Additionally, we introduce BrushData and BrushBench to facilitate segmentation-based inpainting training and performance assessment.","Our extensive experimental analysis demonstrates BrushNet's superior performance over existing models across seven key metrics, including image quality, mask region preservation, and textual coherence."],"url":"http://arxiv.org/abs/2403.06976v1","category":"cs.CV"}
{"created":"2024-03-11 17:57:41","title":"Memory-based Adapters for Online 3D Scene Perception","abstract":"In this paper, we propose a new framework for online 3D scene perception. Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene geometry as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos. To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information. To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability. Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features. Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame. We further propose 3D-to-2D adapter to enhance image features with strong global context. Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply finetuning existing offline models, without any model and task-specific designs. \\href{https://xuxw98.github.io/Online3D/}{Project page}.","sentences":["In this paper, we propose a new framework for online 3D scene perception.","Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene geometry as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos.","To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information.","To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability.","Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features.","Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame.","We further propose 3D-to-2D adapter to enhance image features with strong global context.","Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks.","Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply finetuning existing offline models, without any model and task-specific designs.","\\href{https://xuxw98.github.io/Online3D/}{Project page}."],"url":"http://arxiv.org/abs/2403.06974v1","category":"cs.CV"}
{"created":"2024-03-11 17:35:33","title":"SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data","abstract":"Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models.","sentences":["Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions.","However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects.","In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging.","First, SELMA leverages an LLM's in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts.","Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging.","Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets.","We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation.","Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data.","Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models."],"url":"http://arxiv.org/abs/2403.06952v1","category":"cs.CV"}
{"created":"2024-03-11 17:33:12","title":"Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation","abstract":"Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task. Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities. In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation. Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP's features into language-associated and vision-associated components. Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances. We align features across domains using a modality discriminator. Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS","sentences":["Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task.","Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities.","In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation.","Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP's features into language-associated and vision-associated components.","Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances.","We align features across domains using a modality discriminator.","Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs.","Code: https://github.com/TL-UESTC/UniMoS"],"url":"http://arxiv.org/abs/2403.06946v1","category":"cs.CV"}
{"created":"2024-03-11 17:21:39","title":"Counterfactual Reasoning with Knowledge Graph Embeddings","abstract":"Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories. In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR. We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules. We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained. We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark. Our results indicate that KGEs learn patterns in the graph without explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow these patterns. An evaluation on human-annotated data reveals that KGEs adapted with COULDD are mostly unable to recognize changes to the graph that do not follow learned inference rules. In contrast, ChatGPT mostly outperforms KGEs in detecting plausible changes to the graph but has poor knowledge retention. In summary, CFKGR connects two previously distinct areas, namely KG completion and counterfactual reasoning.","sentences":["Knowledge graph embeddings (KGEs) were originally developed to infer true but missing facts in incomplete knowledge repositories.","In this paper, we link knowledge graph completion and counterfactual reasoning via our new task CFKGR.","We model the original world state as a knowledge graph, hypothetical scenarios as edges added to the graph, and plausible changes to the graph as inferences from logical rules.","We create corresponding benchmark datasets, which contain diverse hypothetical scenarios with plausible changes to the original knowledge graph and facts that should be retained.","We develop COULDD, a general method for adapting existing knowledge graph embeddings given a hypothetical premise, and evaluate it on our benchmark.","Our results indicate that KGEs learn patterns in the graph without explicit training.","We further observe that KGEs adapted with COULDD solidly detect plausible counterfactual changes to the graph that follow these patterns.","An evaluation on human-annotated data reveals that KGEs adapted with COULDD are mostly unable to recognize changes to the graph that do not follow learned inference rules.","In contrast, ChatGPT mostly outperforms KGEs in detecting plausible changes to the graph but has poor knowledge retention.","In summary, CFKGR connects two previously distinct areas, namely KG completion and counterfactual reasoning."],"url":"http://arxiv.org/abs/2403.06936v1","category":"cs.LG"}
{"created":"2024-03-11 17:17:18","title":"Heavy Ball Momentum for Non-Strongly Convex Optimization","abstract":"When considering the minimization of a quadratic or strongly convex function, it is well known that first-order methods involving an inertial term weighted by a constant-in-time parameter are particularly efficient (see Polyak [32], Nesterov [28], and references therein). By setting the inertial parameter according to the condition number of the objective function, these methods guarantee a fast exponential decay of the error. We prove that this type of schemes (which are later called Heavy Ball schemes) is relevant in a relaxed setting, i.e. for composite functions satisfying a quadratic growth condition. In particular, we adapt V-FISTA, introduced by Beck in [10] for strongly convex functions, to this broader class of functions. To the authors' knowledge, the resulting worst-case convergence rates are faster than any other in the literature, including those of FISTA restart schemes. No assumption on the set of minimizers is required and guarantees are also given in the non-optimal case, i.e. when the condition number is not exactly known. This analysis follows the study of the corresponding continuous-time dynamical system (Heavy Ball with friction system), for which new convergence results of the trajectory are shown.","sentences":["When considering the minimization of a quadratic or strongly convex function, it is well known that first-order methods involving an inertial term weighted by a constant-in-time parameter are particularly efficient (see","Polyak","[32], Nesterov","[28], and references therein).","By setting the inertial parameter according to the condition number of the objective function, these methods guarantee a fast exponential decay of the error.","We prove that this type of schemes (which are later called Heavy Ball schemes) is relevant in a relaxed setting, i.e. for composite functions satisfying a quadratic growth condition.","In particular, we adapt V-FISTA, introduced by Beck in [10] for strongly convex functions, to this broader class of functions.","To the authors' knowledge, the resulting worst-case convergence rates are faster than any other in the literature, including those of FISTA restart schemes.","No assumption on the set of minimizers is required and guarantees are also given in the non-optimal case, i.e. when the condition number is not exactly known.","This analysis follows the study of the corresponding continuous-time dynamical system (Heavy Ball with friction system), for which new convergence results of the trajectory are shown."],"url":"http://arxiv.org/abs/2403.06930v1","category":"math.OC"}
{"created":"2024-03-11 17:05:16","title":"Energy dissipation in earthquakes","abstract":"Earthquakes are rupture-like processes that propagate along tectonic faults and cause seismic waves. The propagation speed and final area of the rupture, which determine an earthquake's potential impact, are directly related to the nature and quantity of the energy dissipation involved in the rupture process. Here we present the challenges associated with defining and measuring the energy dissipation in laboratory and natural earthquakes across many scales. We discuss the importance and implications of distinguishing between energy dissipation that occurs close to and far behind the rupture tip and we identify open scientific questions related to a consistent modeling framework for earthquake physics that extends beyond classical Linear Elastic Fracture Mechanics.","sentences":["Earthquakes are rupture-like processes that propagate along tectonic faults and cause seismic waves.","The propagation speed and final area of the rupture, which determine an earthquake's potential impact, are directly related to the nature and quantity of the energy dissipation involved in the rupture process.","Here we present the challenges associated with defining and measuring the energy dissipation in laboratory and natural earthquakes across many scales.","We discuss the importance and implications of distinguishing between energy dissipation that occurs close to and far behind the rupture tip and we identify open scientific questions related to a consistent modeling framework for earthquake physics that extends beyond classical Linear Elastic Fracture Mechanics."],"url":"http://arxiv.org/abs/2403.06916v1","category":"physics.geo-ph"}
{"created":"2024-03-11 16:55:19","title":"Deep adaptative spectral zoom for improved remote heart rate estimation","abstract":"Recent advances in remote heart rate measurement, motivated by data-driven approaches, have notably enhanced accuracy. However, these improvements primarily focus on recovering the rPPG signal, overlooking the implicit challenges of estimating the heart rate (HR) from the derived signal. While many methods employ the Fast Fourier Transform (FFT) for HR estimation, the performance of the FFT is inherently affected by a limited frequency resolution. In contrast, the Chirp-Z Transform (CZT), a generalization form of FFT, can refine the spectrum to the narrow-band range of interest for heart rate, providing improved frequential resolution and, consequently, more accurate estimation. This paper presents the advantages of employing the CZT for remote HR estimation and introduces a novel data-driven adaptive CZT estimator. The objective of our proposed model is to tailor the CZT to match the characteristics of each specific dataset sensor, facilitating a more optimal and accurate estimation of HR from the rPPG signal without compromising generalization across diverse datasets. This is achieved through a Sparse Matrix Optimization (SMO). We validate the effectiveness of our model through exhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE, and UBFC-rPPG employing both intra- and cross-database performance metrics. The results reveal outstanding heart rate estimation capabilities, establishing the proposed approach as a robust and versatile estimator for any rPPG method.","sentences":["Recent advances in remote heart rate measurement, motivated by data-driven approaches, have notably enhanced accuracy.","However, these improvements primarily focus on recovering the rPPG signal, overlooking the implicit challenges of estimating the heart rate (HR) from the derived signal.","While many methods employ the Fast Fourier Transform (FFT) for HR estimation, the performance of the FFT is inherently affected by a limited frequency resolution.","In contrast, the Chirp-Z Transform (CZT), a generalization form of FFT, can refine the spectrum to the narrow-band range of interest for heart rate, providing improved frequential resolution and, consequently, more accurate estimation.","This paper presents the advantages of employing the CZT for remote HR estimation and introduces a novel data-driven adaptive CZT estimator.","The objective of our proposed model is to tailor the CZT to match the characteristics of each specific dataset sensor, facilitating a more optimal and accurate estimation of HR from the rPPG signal without compromising generalization across diverse datasets.","This is achieved through a Sparse Matrix Optimization (SMO).","We validate the effectiveness of our model through exhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE, and UBFC-rPPG employing both intra- and cross-database performance metrics.","The results reveal outstanding heart rate estimation capabilities, establishing the proposed approach as a robust and versatile estimator for any rPPG method."],"url":"http://arxiv.org/abs/2403.06902v1","category":"cs.CV"}
{"created":"2024-03-11 16:54:23","title":"Dynamic Client Clustering, Bandwidth Allocation, and Workload Optimization for Semi-synchronous Federated Learning","abstract":"Federated Learning (FL) revolutionizes collaborative machine learning among Internet of Things (IoT) devices by enabling them to train models collectively while preserving data privacy. FL algorithms fall into two primary categories: synchronous and asynchronous. While synchronous FL efficiently handles straggler devices, it can compromise convergence speed and model accuracy. In contrast, asynchronous FL allows all devices to participate but incurs high communication overhead and potential model staleness. To overcome these limitations, the semi-synchronous FL framework introduces client tiering based on computing and communication latencies. Clients in different tiers upload their local models at distinct frequencies, striking a balance between straggler mitigation and communication costs. Enter the DecantFed algorithm (Dynamic client clustering, bandwidth allocation, and local training for semi-synchronous Federated learning), a dynamic solution that optimizes client clustering, bandwidth allocation, and local training workloads to maximize data sample processing rates. Additionally, DecantFed adapts client learning rates according to their tiers, addressing the model staleness problem. The algorithm's performance shines in extensive simulations using benchmark datasets, including MNIST and CIFAR-10, under independent and identically distributed (IID) and non-IID scenarios. DecantFed outpaces FedAvg and FedProx in terms of convergence speed and delivers a remarkable minimum 28% boost in model accuracy compared to FedProx.","sentences":["Federated Learning (FL) revolutionizes collaborative machine learning among Internet of Things (IoT) devices by enabling them to train models collectively while preserving data privacy.","FL algorithms fall into two primary categories: synchronous and asynchronous.","While synchronous FL efficiently handles straggler devices, it can compromise convergence speed and model accuracy.","In contrast, asynchronous FL allows all devices to participate but incurs high communication overhead and potential model staleness.","To overcome these limitations, the semi-synchronous FL framework introduces client tiering based on computing and communication latencies.","Clients in different tiers upload their local models at distinct frequencies, striking a balance between straggler mitigation and communication costs.","Enter the DecantFed algorithm (Dynamic client clustering, bandwidth allocation, and local training for semi-synchronous Federated learning), a dynamic solution that optimizes client clustering, bandwidth allocation, and local training workloads to maximize data sample processing rates.","Additionally, DecantFed adapts client learning rates according to their tiers, addressing the model staleness problem.","The algorithm's performance shines in extensive simulations using benchmark datasets, including MNIST and CIFAR-10, under independent and identically distributed (IID) and non-IID scenarios.","DecantFed outpaces FedAvg and FedProx in terms of convergence speed and delivers a remarkable minimum 28% boost in model accuracy compared to FedProx."],"url":"http://arxiv.org/abs/2403.06900v1","category":"cs.DC"}
{"created":"2024-03-11 16:47:09","title":"Application of Quantum Tensor Networks for Protein Classification","abstract":"We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems. We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences. We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results. We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve the binary classification task mentioned above. Our top-performing quantum model has achieved a 94% accuracy rate, which is comparable to the performance of a classical model that uses the ESM2 protein language model embeddings. It's noteworthy that the ESM2 model is extremely large, containing 8 million parameters in its smallest configuration, whereas our best quantum model requires only around 800 parameters. We demonstrate that these hybrid models exhibit promising performance, showcasing their potential to compete with classical models of similar complexity.","sentences":["We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems.","We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms.","Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences.","We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results.","We demonstrate two distinct QTNs, inspired by classical recurrent neural networks (RNN) and convolutional neural networks (CNN), to solve the binary classification task mentioned above.","Our top-performing quantum model has achieved a 94% accuracy rate, which is comparable to the performance of a classical model that uses the ESM2 protein language model embeddings.","It's noteworthy that the ESM2 model is extremely large, containing 8 million parameters in its smallest configuration, whereas our best quantum model requires only around 800 parameters.","We demonstrate that these hybrid models exhibit promising performance, showcasing their potential to compete with classical models of similar complexity."],"url":"http://arxiv.org/abs/2403.06890v1","category":"quant-ph"}
{"created":"2024-03-11 16:31:25","title":"SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields for Robotic Inspection","abstract":"We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures. This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals. We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss. We use submapping to scale the system to large-scale environments captured over long trajectories. We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building. Website: https://ori-drs.github.io/projects/silvr/","sentences":["We present a neural-field-based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photo-realistic textures.","This system adapts the state-of-the-art neural radiance field (NeRF) representation to also incorporate lidar data which adds strong geometric constraints on the depth and surface normals.","We exploit the trajectory from a real-time lidar SLAM system to bootstrap a Structure-from-Motion (SfM) procedure to both significantly reduce the computation time and to provide metric scale which is crucial for lidar depth loss.","We use submapping to scale the system to large-scale environments captured over long trajectories.","We demonstrate the reconstruction system with data from a multi-camera, lidar sensor suite onboard a legged robot, hand-held while scanning building scenes for 600 metres, and onboard an aerial robot surveying a multi-storey mock disaster site-building.","Website: https://ori-drs.github.io/projects/silvr/"],"url":"http://arxiv.org/abs/2403.06877v1","category":"cs.RO"}
{"created":"2024-03-11 16:26:35","title":"COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification","abstract":"High-performing out-of-distribution (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models. In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality. We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a supervised model. The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind. COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection. We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority of experiments, e.g., improving detecting ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset. SHAP (feature contribution) analysis shows that different individual OOD measures are essential for various tasks, indicating that multiple OOD measures and combinations are needed to generalize. Additionally, we show that explicitly considering ID images that are incorrectly classified for the original (species) recognition task is important for constructing high-performing OOD detection methods and for practical applicability. The framework can easily be extended or adapted to other tasks and media modalities.","sentences":["High-performing out-of-distribution (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models.","In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality.","We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a supervised model.","The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind.","COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection.","We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority of experiments, e.g., improving detecting ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.","SHAP (feature contribution) analysis shows that different individual OOD measures are essential for various tasks, indicating that multiple OOD measures and combinations are needed to generalize.","Additionally, we show that explicitly considering ID images that are incorrectly classified for the original (species) recognition task is important for constructing high-performing OOD detection methods and for practical applicability.","The framework can easily be extended or adapted to other tasks and media modalities."],"url":"http://arxiv.org/abs/2403.06874v1","category":"cs.CV"}
{"created":"2024-03-11 16:24:08","title":"Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents","abstract":"Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation. We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the hierarchical framework of MESc and compare them with their standalone performance on legal texts. We also study their intra-domain(legal) transfer learning capability and the impact of combining embeddings from their last layers in MESc. We test these methods and their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset. Our approach achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art methods.","sentences":["Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure.","Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation.","We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment prediction.","Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering.","Which we use in another set of transformer encoder layers to learn the inter-chunk representations.","We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the hierarchical framework of MESc and compare them with their standalone performance on legal texts.","We also study their intra-domain(legal) transfer learning capability and the impact of combining embeddings from their last layers in MESc.","We test these methods and their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset.","Our approach achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.06872v1","category":"cs.CL"}
{"created":"2024-03-11 16:23:38","title":"Semantic Residual Prompts for Continual Learning","abstract":"Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a second pool. The retrieved prompts serve to adapt a pre-trained ViT, granting plasticity. In doing so, we also propose a novel residual mechanism to transfer CLIP semantics to the ViT layers. Through extensive analysis on established CL benchmarks, we show that our method significantly outperforms both state-of-the-art CL approaches and the zero-shot CLIP test. Notably, our findings hold true even for datasets with a substantial domain gap w.r.t. the pre-training knowledge of the backbone model, as showcased by experiments on satellite imagery and medical datasets.","sentences":["Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts.","Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values).","However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches.","For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts.","To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism.","Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes.","The second level, instead, uses these prototypes along with the query image as keys to index a second pool.","The retrieved prompts serve to adapt a pre-trained ViT, granting plasticity.","In doing so, we also propose a novel residual mechanism to transfer CLIP semantics to the ViT layers.","Through extensive analysis on established CL benchmarks, we show that our method significantly outperforms both state-of-the-art CL approaches and the zero-shot CLIP test.","Notably, our findings hold true even for datasets with a substantial domain gap w.r.t.","the pre-training knowledge of the backbone model, as showcased by experiments on satellite imagery and medical datasets."],"url":"http://arxiv.org/abs/2403.06870v1","category":"cs.LG"}
{"created":"2024-03-11 16:22:41","title":"Learning with Noisy Foundation Models","abstract":"Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning.","sentences":["Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning.","However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks.","This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks.","Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different.","These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications.","We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently.","We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners.","We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation.","Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning."],"url":"http://arxiv.org/abs/2403.06869v1","category":"cs.LG"}
{"created":"2024-03-12 17:59:02","title":"SIDE-real: Truncated marginal neural ratio estimation for Supernova Ia Dust Extinction with real data","abstract":"We present the first fully simulation-based hierarchical analysis of the light curves of a population of low-redshift type Ia supernovae (SNae Ia). Our hardware-accelerated forward model, released in the Python package slicsim, includes stochastic variations of each SN's spectral flux distribution (based on the pre-trained BayeSN model), extinction from dust in the host and in the Milky Way, redshift, and realistic instrumental noise. By utilising truncated marginal neural ratio estimation (TMNRE), a neural network-enabled simulation-based inference technique, we implicitly marginalise over 4000 latent variables (for a set of $\\approx 100$ SNae Ia) to efficiently infer SN Ia absolute magnitudes and host-galaxy dust properties at the population level while also constraining the parameters of individual objects. Amortisation of the inference procedure allows us to obtain coverage guarantees for our results through Bayesian validation and frequentist calibration. Furthermore, we show a detailed comparison to full likelihood-based inference, implemented through Hamiltonian Monte Carlo, on simulated data and then apply TMNRE to the light curves of 86 SNae Ia from the Carnegie Supernova Project, deriving marginal posteriors in excellent agreement with previous work. Given its ability to accommodate arbitrarily complex extensions to the forward model -- e.g. different populations based on host properties, redshift evolution, complicated photometric redshift estimates, selection effects, and non-Ia contamination -- without significant modifications to the inference procedure, TMNRE has the potential to become the tool of choice for cosmological parameter inference from future, large SN Ia samples.","sentences":["We present the first fully simulation-based hierarchical analysis of the light curves of a population of low-redshift type Ia supernovae (SNae Ia).","Our hardware-accelerated forward model, released in the Python package slicsim, includes stochastic variations of each SN's spectral flux distribution (based on the pre-trained BayeSN model), extinction from dust in the host and in the Milky Way, redshift, and realistic instrumental noise.","By utilising truncated marginal neural ratio estimation (TMNRE), a neural network-enabled simulation-based inference technique, we implicitly marginalise over 4000 latent variables (for a set of $\\approx 100$ SNae Ia) to efficiently infer SN Ia absolute magnitudes and host-galaxy dust properties at the population level while also constraining the parameters of individual objects.","Amortisation of the inference procedure allows us to obtain coverage guarantees for our results through Bayesian validation and frequentist calibration.","Furthermore, we show a detailed comparison to full likelihood-based inference, implemented through Hamiltonian Monte Carlo, on simulated data and then apply TMNRE to the light curves of 86 SNae Ia from the Carnegie Supernova Project, deriving marginal posteriors in excellent agreement with previous work.","Given its ability to accommodate arbitrarily complex extensions to the forward model -- e.g. different populations based on host properties, redshift evolution, complicated photometric redshift estimates, selection effects, and non-Ia contamination -- without significant modifications to the inference procedure, TMNRE has the potential to become the tool of choice for cosmological parameter inference from future, large SN Ia samples."],"url":"http://arxiv.org/abs/2403.07871v1","category":"astro-ph.CO"}
{"created":"2024-03-12 17:44:22","title":"On the One-dimensional Singular Abreu Equations","abstract":"Singular fourth-order Abreu equations have been used to approximate minimizers of convex functionals subject to a convexity constraint in dimensions higher than or equal to two. For Abreu type equations, they often exhibit different solvability phenomena in dimension one and dimensions at least two. We prove the analogues of these results for the variational problem and singular Abreu equations in dimension one, and use the approximation scheme to obtain a characterization of limiting minimizers to the one-dimensional variational problem.","sentences":["Singular fourth-order Abreu equations have been used to approximate minimizers of convex functionals subject to a convexity constraint in dimensions higher than or equal to two.","For Abreu type equations, they often exhibit different solvability phenomena in dimension one and dimensions at least two.","We prove the analogues of these results for the variational problem and singular Abreu equations in dimension one, and use the approximation scheme to obtain a characterization of limiting minimizers to the one-dimensional variational problem."],"url":"http://arxiv.org/abs/2403.07852v1","category":"math.AP"}
{"created":"2024-03-12 17:41:27","title":"Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations","abstract":"We formulate an XAI-based model improvement approach for Graph Neural Networks (GNNs) for node classification, called Explanation Enhanced Graph Learning (EEGL). The goal is to improve predictive performance of GNN using explanations. EEGL is an iterative self-improving algorithm, which starts with a learned \"vanilla\" GNN, and repeatedly uses frequent subgraph mining to find relevant patterns in explanation subgraphs. These patterns are then filtered further to obtain application-dependent features corresponding to the presence of certain subgraphs in the node neighborhoods. Giving an application-dependent algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL) algorithm has previously been posed as an open problem. We present experimental evidence, with synthetic and real-world data, which show that EEGL outperforms related approaches in predictive performance and that it has a node-distinguishing power beyond that of vanilla GNNs. We also analyze EEGL's training dynamics.","sentences":["We formulate an XAI-based model improvement approach for Graph Neural Networks (GNNs) for node classification, called Explanation Enhanced Graph Learning (EEGL).","The goal is to improve predictive performance of GNN using explanations.","EEGL is an iterative self-improving algorithm, which starts with a learned \"vanilla\" GNN, and repeatedly uses frequent subgraph mining to find relevant patterns in explanation subgraphs.","These patterns are then filtered further to obtain application-dependent features corresponding to the presence of certain subgraphs in the node neighborhoods.","Giving an application-dependent algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL) algorithm has previously been posed as an open problem.","We present experimental evidence, with synthetic and real-world data, which show that EEGL outperforms related approaches in predictive performance and that it has a node-distinguishing power beyond that of vanilla GNNs.","We also analyze EEGL's training dynamics."],"url":"http://arxiv.org/abs/2403.07849v1","category":"cs.LG"}
{"created":"2024-03-12 17:37:41","title":"Hyper-density functional theory of soft matter","abstract":"We present a scheme for investigating arbitrary thermal observables in spatially inhomogeneous many-body systems. Extending the equilibrium ensemble yields any given observable as an explicit hyper-density functional. Associated local fluctuation profiles follow from an exact hyper-Ornstein-Zernike equation. Simulation-based supervised machine learning trains neural networks that act as hyper-direct correlation functionals which facilitate efficient and accurate predictions. We exemplify the approach for the cluster statistics of hard rods and square well particles. The theory provides access to complex order parameters, as is impossible in standard density functional theory.","sentences":["We present a scheme for investigating arbitrary thermal observables in spatially inhomogeneous many-body systems.","Extending the equilibrium ensemble yields any given observable as an explicit hyper-density functional.","Associated local fluctuation profiles follow from an exact hyper-Ornstein-Zernike equation.","Simulation-based supervised machine learning trains neural networks that act as hyper-direct correlation functionals which facilitate efficient and accurate predictions.","We exemplify the approach for the cluster statistics of hard rods and square well particles.","The theory provides access to complex order parameters, as is impossible in standard density functional theory."],"url":"http://arxiv.org/abs/2403.07845v1","category":"cond-mat.soft"}
{"created":"2024-03-12 17:27:49","title":"Quantifying and Mitigating Privacy Risks for Tabular Generative Models","abstract":"Synthetic data from generative models emerges as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models. Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder network to encode the tabular data and a latent diffusion model to synthesize the latent tables. Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms. Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data. Specifically, compared to other DP-protected tabular generative models, DP-TLDM improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk.","sentences":["Synthetic data from generative models emerges as the privacy-preserving data-sharing solution.","Such a synthetic data set shall resemble the original data without revealing identifiable private information.","The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models.","Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data.","We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks.","Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder network to encode the tabular data and a latent diffusion model to synthesize the latent tables.","Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms.","Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data.","Specifically, compared to other DP-protected tabular generative models, DP-TLDM improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk."],"url":"http://arxiv.org/abs/2403.07842v1","category":"cs.LG"}
{"created":"2024-03-12 17:26:00","title":"Critical metrics of eigenvalue functionals via Clarke subdifferential","abstract":"We set up a new framework to study critical points of functionals defined as combinations of eigenvalues of operators with respect to a given set of parameters: Riemannian metrics, potentials, etc. Our setting builds upon Clarke's differentiation theory to provide a novel understanding of critical metrics. In particular, we unify and refine previous research carried out on Laplace and Steklov eigenvalues. We also use our theory to tackle original examples such as the conformal GJMS operators, the conformal Laplacian, and the Laplacian with mixed boundary conditions.","sentences":["We set up a new framework to study critical points of functionals defined as combinations of eigenvalues of operators with respect to a given set of parameters: Riemannian metrics, potentials, etc.","Our setting builds upon Clarke's differentiation theory to provide a novel understanding of critical metrics.","In particular, we unify and refine previous research carried out on Laplace and Steklov eigenvalues.","We also use our theory to tackle original examples such as the conformal GJMS operators, the conformal Laplacian, and the Laplacian with mixed boundary conditions."],"url":"http://arxiv.org/abs/2403.07841v1","category":"math.DG"}
{"created":"2024-03-12 17:07:35","title":"Affine Gateaux Differentials and the von Mises Statistical Calculus","abstract":"This paper presents a general study of one-dimensional differentiability for functionals on convex domains that are not necessarily open. The local approximation is carried out by affine functionals, rather than linear ones as in standard Gateaux differentiability. This affine notion of differentiability naturally arises in many applications and, here and there, it appeared in the literature. Our systematic analysis aims to give a general perspective on it.","sentences":["This paper presents a general study of one-dimensional differentiability for functionals on convex domains that are not necessarily open.","The local approximation is carried out by affine functionals, rather than linear ones as in standard Gateaux differentiability.","This affine notion of differentiability naturally arises in many applications and, here and there, it appeared in the literature.","Our systematic analysis aims to give a general perspective on it."],"url":"http://arxiv.org/abs/2403.07827v1","category":"math.FA"}
{"created":"2024-03-12 17:03:40","title":"Preconditioners based on Voronoi quantizers of random variable coefficients for stochastic elliptic partial differential equations","abstract":"A preconditioning strategy is proposed for the iterative solve of large numbers of linear systems with variable matrix and right-hand side which arise during the computation of solution statistics of stochastic elliptic partial differential equations with random variable coefficients sampled by Monte Carlo. Building on the assumption that a truncated Karhunen-Lo\\`{e}ve expansion of a known transform of the random variable coefficient is known, we introduce a compact representation of the random coefficient in the form of a Voronoi quantizer. The number of Voronoi cells, each of which is represented by a centroidal variable coefficient, is set to the prescribed number $P$ of preconditioners. Upon sampling the random variable coefficient, the linear system assembled with a given realization of the coefficient is solved with the preconditioner whose centroidal variable coefficient is the closest to the realization. We consider different ways to define and obtain the centroidal variable coefficients, and we investigate the properties of the induced preconditioning strategies in terms of average number of solver iterations for sequential simulations, and of load balancing for parallel simulations. Another approach, which is based on deterministic grids on the system of stochastic coordinates of the truncated representation of the random variable coefficient, is proposed with a stochastic dimension which increases with the number $P$ of preconditioners. This approach allows to bypass the need for preliminary computations in order to determine the optimal stochastic dimension of the truncated approximation of the random variable coefficient for a given number of preconditioners.","sentences":["A preconditioning strategy is proposed for the iterative solve of large numbers of linear systems with variable matrix and right-hand side which arise during the computation of solution statistics of stochastic elliptic partial differential equations with random variable coefficients sampled by Monte Carlo.","Building on the assumption that a truncated Karhunen-Lo\\`{e}ve expansion of a known transform of the random variable coefficient is known, we introduce a compact representation of the random coefficient in the form of a Voronoi quantizer.","The number of Voronoi cells, each of which is represented by a centroidal variable coefficient, is set to the prescribed number $P$ of preconditioners.","Upon sampling the random variable coefficient, the linear system assembled with a given realization of the coefficient is solved with the preconditioner whose centroidal variable coefficient is the closest to the realization.","We consider different ways to define and obtain the centroidal variable coefficients, and we investigate the properties of the induced preconditioning strategies in terms of average number of solver iterations for sequential simulations, and of load balancing for parallel simulations.","Another approach, which is based on deterministic grids on the system of stochastic coordinates of the truncated representation of the random variable coefficient, is proposed with a stochastic dimension which increases with the number $P$ of preconditioners.","This approach allows to bypass the need for preliminary computations in order to determine the optimal stochastic dimension of the truncated approximation of the random variable coefficient for a given number of preconditioners."],"url":"http://arxiv.org/abs/2403.07824v1","category":"math.NA"}
{"created":"2024-03-12 17:03:10","title":"Time-discretization method for a multi-term time fractional differential equation with delay","abstract":"This paper discusses a multi-term time-fractional delay differential equation in a real Hilbert space. An iterative scheme for a multi-term time-fractional differential equation is established using Rothe's method. The method of semi-discretization is extended to this kind of time fractional problem with delay in the case that the time delay parameter $\\nu >0$ satisfies $\\nu\\leq T$, where $T$ denotes the final time. We apply the accretivity of the operator $A$ in an iterative scheme to establish the existence and regularity of strong solutions to the considered problem. Finally, an example is provided to demonstrate the abstract result.","sentences":["This paper discusses a multi-term time-fractional delay differential equation in a real Hilbert space.","An iterative scheme for a multi-term time-fractional differential equation is established using Rothe's method.","The method of semi-discretization is extended to this kind of time fractional problem with delay in the case that the time delay parameter $\\nu >0$ satisfies $\\nu\\leq T$, where $T$ denotes the final time.","We apply the accretivity of the operator $A$ in an iterative scheme to establish the existence and regularity of strong solutions to the considered problem.","Finally, an example is provided to demonstrate the abstract result."],"url":"http://arxiv.org/abs/2403.07823v1","category":"math.NA"}
{"created":"2024-03-12 17:03:07","title":"Fusing Climate Data Products using a Spatially Varying Autoencoder","abstract":"Autoencoders are powerful machine learning models used to compress information from multiple data sources. However, autoencoders, like all artificial neural networks, are often unidentifiable and uninterpretable. This research focuses on creating an identifiable and interpretable autoencoder that can be used to meld and combine climate data products. The proposed autoencoder utilizes a Bayesian statistical framework, allowing for probabilistic interpretations while also varying spatially to capture useful spatial patterns across the various data products. Constraints are placed on the autoencoder as it learns patterns in the data, creating an interpretable consensus that includes the important features from each input. We demonstrate the utility of the autoencoder by combining information from multiple precipitation products in High Mountain Asia.","sentences":["Autoencoders are powerful machine learning models used to compress information from multiple data sources.","However, autoencoders, like all artificial neural networks, are often unidentifiable and uninterpretable.","This research focuses on creating an identifiable and interpretable autoencoder that can be used to meld and combine climate data products.","The proposed autoencoder utilizes a Bayesian statistical framework, allowing for probabilistic interpretations while also varying spatially to capture useful spatial patterns across the various data products.","Constraints are placed on the autoencoder as it learns patterns in the data, creating an interpretable consensus that includes the important features from each input.","We demonstrate the utility of the autoencoder by combining information from multiple precipitation products in High Mountain Asia."],"url":"http://arxiv.org/abs/2403.07822v1","category":"stat.AP"}
{"created":"2024-03-12 16:46:54","title":"pyvene: A Library for Understanding and Improving PyTorch Models via Interventions","abstract":"Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce $\\textbf{pyvene}$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. $\\textbf{pyvene}$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how $\\textbf{pyvene}$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene.","sentences":["Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability.","To facilitate such research, we introduce $\\textbf{pyvene}$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules.","$\\textbf{pyvene}$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters.","We show how $\\textbf{pyvene}$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others.","We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization.","We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene."],"url":"http://arxiv.org/abs/2403.07809v1","category":"cs.LG"}
{"created":"2024-03-12 16:33:32","title":"Fine-tuning Neural Network Quantum States","abstract":"Recent progress in the design and optimization of Neural Network Quantum States (NNQS) have made them an effective method to investigate ground-state properties of quantum many-body systems. In contrast to the standard approach of training a separate NNQS from scratch at every point of the phase diagram, we demonstrate that the optimization at a highly expressive point of the phase diagram (i.e., close to a phase transition) yields interpretable features that can be reused to accurately describe a wide region across the transition. We demonstrate the feasibility of our approach on different systems in one and two dimensions by initially pretraining a NNQS at a given point of the phase diagram, followed by fine-tuning only the output layer for all other points. Notably, the computational cost of the fine-tuning step is very low compared to the pretraining stage. We argue that the reduced cost of this paradigm has significant potential to advance the exploration of condensed matter systems using NNQS, mirroring the success of fine-tuning in machine learning and natural language processing.","sentences":["Recent progress in the design and optimization of Neural Network Quantum States (NNQS) have made them an effective method to investigate ground-state properties of quantum many-body systems.","In contrast to the standard approach of training a separate NNQS from scratch at every point of the phase diagram, we demonstrate that the optimization at a highly expressive point of the phase diagram (i.e., close to a phase transition) yields interpretable features that can be reused to accurately describe a wide region across the transition.","We demonstrate the feasibility of our approach on different systems in one and two dimensions by initially pretraining a NNQS at a given point of the phase diagram, followed by fine-tuning only the output layer for all other points.","Notably, the computational cost of the fine-tuning step is very low compared to the pretraining stage.","We argue that the reduced cost of this paradigm has significant potential to advance the exploration of condensed matter systems using NNQS, mirroring the success of fine-tuning in machine learning and natural language processing."],"url":"http://arxiv.org/abs/2403.07795v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-12 16:32:25","title":"Optimal regularity for nonlocal elliptic equations and free boundary problems","abstract":"In this article we establish for the first time the $C^s$ boundary regularity of solutions to nonlocal elliptic equations with kernels $K(y)\\asymp |y|^{-n-2s}$. This was known to hold only when $K$ is homogeneous, and it is quite surprising that it holds for general inhomogeneous kernels, too. As an application of our results, we also establish the optimal $C^{1+s}$ regularity of solutions to obstacle problems for general nonlocal operators with kernels $K(y)\\asymp |y|^{-n-2s}$. Again, this was only known when $K$ is homogeneous, and it solves a long-standing open question in the field. A new key idea is to construct a 1D solution as a minimizer of an appropriate nonlocal one-phase free boundary problem, for which we establish optimal $C^s$ regularity and non-degeneracy estimates.","sentences":["In this article we establish for the first time the $C^s$ boundary regularity of solutions to nonlocal elliptic equations with kernels $K(y)\\asymp |y|^{-n-2s}$.","This was known to hold only when $K$ is homogeneous, and it is quite surprising that it holds for general inhomogeneous kernels, too.","As an application of our results, we also establish the optimal $C^{1+s}$ regularity of solutions to obstacle problems for general nonlocal operators with kernels $K(y)\\asymp |y|^{-n-2s}$.","Again, this was only known when $K$ is homogeneous, and it solves a long-standing open question in the field.","A new key idea is to construct a 1D solution as a minimizer of an appropriate nonlocal one-phase free boundary problem, for which we establish optimal $C^s$ regularity and non-degeneracy estimates."],"url":"http://arxiv.org/abs/2403.07793v1","category":"math.AP"}
{"created":"2024-03-12 16:31:19","title":"Stability of the Favorable Falkner-Skan Profiles for the Stationary Prandtl Equations","abstract":"The (favorable) Falkner-Skan boundary layer profiles are a one parameter ($\\beta \\in [0,2]$) family of self-similar solutions to the stationary Prandtl system which describes the flow over a wedge with angle $\\beta \\frac{\\pi}{2}$. The most famous member of this family is the endpoint Blasius profile, $\\beta = 0$, which exhibits pressureless flow over a flat plate. In contrast, the $\\beta > 0$ profiles are physically expected to exhibit a \\textit{favorable pressure gradient}, a common adage in the physics literature. In this work, we prove quantitative scattering estimates as $x \\rightarrow \\infty$ which precisely captures the effect of this favorable gradient through the presence of new ``CK\" (Cauchy-Kovalevskaya) terms that appear in a quasilinear energy cascade.","sentences":["The (favorable) Falkner-Skan boundary layer profiles are a one parameter ($\\beta \\in [0,2]$) family of self-similar solutions to the stationary Prandtl system which describes the flow over a wedge with angle $\\beta \\frac{\\pi}{2}$. The most famous member of this family is the endpoint Blasius profile, $\\beta = 0$, which exhibits pressureless flow over a flat plate.","In contrast, the $\\beta > 0$ profiles are physically expected to exhibit a \\textit{favorable pressure gradient}, a common adage in the physics literature.","In this work, we prove quantitative scattering estimates as $x \\rightarrow \\infty$ which precisely captures the effect of this favorable gradient through the presence of new ``CK\" (Cauchy-Kovalevskaya) terms that appear in a quasilinear energy cascade."],"url":"http://arxiv.org/abs/2403.07791v1","category":"math.AP"}
{"created":"2024-03-12 16:10:01","title":"QCD Equation of State at nonzero baryon density in external magnetic field","abstract":"This paper is devoted to the study of QCD equation of state in external magnetic field and nonzero baryon density. Our study is carried out by means of lattice simulation with 2+1 dynamical staggered quarks at the physical masses. The simulation is conducted at imaginary baryon chemical potential what allowed us to overcome the sign problem. We expand the pressure in the baryon imaginary chemical potential and study three leading nonzero coefficients in this expansion. These coefficients were calculated for the following values of magnetic field: $eB=0.3$, $0.6$, $1.2$ GeV$^2$ with the lattice sizes $8\\times32^3$, $10\\times40^3$, $12\\times48^3$. Using these data we take continuum limit for the coefficients. Our results indicate considerable enhancement of the expansion coefficients by the magnetic field.","sentences":["This paper is devoted to the study of QCD equation of state in external magnetic field and nonzero baryon density.","Our study is carried out by means of lattice simulation with 2+1 dynamical staggered quarks at the physical masses.","The simulation is conducted at imaginary baryon chemical potential what allowed us to overcome the sign problem.","We expand the pressure in the baryon imaginary chemical potential and study three leading nonzero coefficients in this expansion.","These coefficients were calculated for the following values of magnetic field: $eB=0.3$, $0.6$, $1.2$ GeV$^2$ with the lattice sizes $8\\times32^3$, $10\\times40^3$, $12\\times48^3$.","Using these data we take continuum limit for the coefficients.","Our results indicate considerable enhancement of the expansion coefficients by the magnetic field."],"url":"http://arxiv.org/abs/2403.07783v1","category":"hep-lat"}
{"created":"2024-03-12 16:09:50","title":"Minimal Elements of the Causal Boundary with Applications to Spacetime Splitting","abstract":"In 1972, Geroch, Kronheimer, and Penrose introduced what is now called the causal boundary of a spacetime. This boundary is constructed out of Terminal Indecomposable Past sets (TIPs) and their future analogues (TIFs), which are the pasts and futures of inextendible causal curves. The causal boundary is a key tool to understand the global structure of a spacetime. In this paper, we show that in a spacetime with compact Cauchy surfaces, there is always at least one minimal TIP and one minimal TIF, minimal meaning that it does not contain another TIP (resp. TIF) as a proper subset. We then study the implications of the minimal TIP and TIF meeting each other. This condition generalizes some of the ``no observer horizon'' conditions that have been used in the literature to obtain partial solutions of the Bartnik splitting conjecture.","sentences":["In 1972, Geroch, Kronheimer, and Penrose introduced what is now called the causal boundary of a spacetime.","This boundary is constructed out of Terminal Indecomposable Past sets (TIPs) and their future analogues (TIFs), which are the pasts and futures of inextendible causal curves.","The causal boundary is a key tool to understand the global structure of a spacetime.","In this paper, we show that in a spacetime with compact Cauchy surfaces, there is always at least one minimal TIP and one minimal TIF, minimal meaning that it does not contain another TIP (resp.","TIF) as a proper subset.","We then study the implications of the minimal TIP and TIF meeting each other.","This condition generalizes some of the ``no observer horizon'' conditions that have been used in the literature to obtain partial solutions of the Bartnik splitting conjecture."],"url":"http://arxiv.org/abs/2403.07782v1","category":"gr-qc"}
{"created":"2024-03-12 16:09:12","title":"Conservative Black Hole Scattering at Fifth Post-Minkowskian and First Self-Force Order","abstract":"We compute the 5PM order contributions to the scattering angle and impulse of classical black hole scattering in the conservative sector at first self-force order (1SF) using the worldline quantum field theory formalism. This challenging four-loop computation required the use of advanced integration-by-parts and differential equation technology implemented on high-perfomance computing systems. Use of partial fraction identities allowed us to render the complete integrand in a fully planar form. The resulting function space is simpler than expected: in the scattering angle we see only multiple polylogarithms up to weight three, and a total absence of the elliptic integrals that appeared at 4PM order. All checks on our result, both internal - cancellation of dimensional regularization poles, preservation of the on-shell condition - and external - matching the slow-velocity limit with the post-Newtonian (PN) literature up to 5PN order and matching the tail terms to the 4PM loss of energy - are passed.","sentences":["We compute the 5PM order contributions to the scattering angle and impulse of classical black hole scattering in the conservative sector at first self-force order (1SF) using the worldline quantum field theory formalism.","This challenging four-loop computation required the use of advanced integration-by-parts and differential equation technology implemented on high-perfomance computing systems.","Use of partial fraction identities allowed us to render the complete integrand in a fully planar form.","The resulting function space is simpler than expected: in the scattering angle we see only multiple polylogarithms up to weight three, and a total absence of the elliptic integrals that appeared at 4PM order.","All checks on our result, both internal - cancellation of dimensional regularization poles, preservation of the on-shell condition - and external - matching the slow-velocity limit with the post-Newtonian (PN) literature up to 5PN order and matching the tail terms to the 4PM loss of energy - are passed."],"url":"http://arxiv.org/abs/2403.07781v1","category":"hep-th"}
{"created":"2024-03-12 15:45:57","title":"HermEIS: A Parallel Multichannel Approach to Rapid Spectral Characterization of Neural MEAs","abstract":"The promise of increasing channel counts in high density ($> 10^4$) neural Microelectrode Arrays (MEAs) for high resolution recording comes with the curse of developing faster characterization strategies for concurrent acquisition of multichannel electrode integrities over a wide frequency spectrum. To circumvent the latency associated with the current multiplexed technique for impedance acquisition, it is common practice to resort to the single frequency impedance measurement (i.e. $Z_{1 \\text{kHz}}$). This, however, does not offer sufficient spectral impedance information crucial for determining the capacity of electrodes at withstanding slow and fast-changing stimulus and recordings. In this work, we present \\textit{HermEIS}, a novel approach that leverages single cycle in-phase and quadrature signal integrations for reducing the massive data throughput characteristic of such high density acquisition systems. As an initial proof-of-concept, we demonstrate over $6$ decades of impedance bandwidth ($5\\times10^{-2} - 5\\times10^{4}\\text{ Hz}$) in a parallel $4$-channel potentiostatic setup composed of a custom PCB with off-the-shelf electronics working in tandem with an FPGA.","sentences":["The promise of increasing channel counts in high density ($> 10^4$) neural Microelectrode Arrays (MEAs) for high resolution recording comes with the curse of developing faster characterization strategies for concurrent acquisition of multichannel electrode integrities over a wide frequency spectrum.","To circumvent the latency associated with the current multiplexed technique for impedance acquisition, it is common practice to resort to the single frequency impedance measurement (i.e. $Z_{1 \\text{kHz}}$).","This, however, does not offer sufficient spectral impedance information crucial for determining the capacity of electrodes at withstanding slow and fast-changing stimulus and recordings.","In this work, we present \\textit{HermEIS}, a novel approach that leverages single cycle in-phase and quadrature signal integrations for reducing the massive data throughput characteristic of such high density acquisition systems.","As an initial proof-of-concept, we demonstrate over $6$ decades of impedance bandwidth ($5\\times10^{-2} - 5\\times10^{4}\\text{ Hz}$) in a parallel $4$-channel potentiostatic setup composed of a custom PCB with off-the-shelf electronics working in tandem with an FPGA."],"url":"http://arxiv.org/abs/2403.07758v1","category":"eess.SP"}
{"created":"2024-03-12 14:56:14","title":"Nonlocal Stokes equation with relaxation on the divergence free equation","abstract":"In this paper, we consider a new nonlocal approximation to the linear Stokes system with periodic boundary conditions in two and three dimensional spaces . A relaxation term is added to the equation of nonlocal divergence free equation, which is reminiscent to the relaxation of local Stokes equation with small artificial compressibility. Our analysis shows that the well-posedness of the nonlocal system can be established under some mild assumptions on the kernel of nonlocal interactions. Furthermore, the new nonlocal system converges to the conventional, local Stokes system in second order as the horizon parameter of the nonlocal interaction goes to zero. The study provides more theoretical understanding to some numerical methods, such as smoothed particle hydrodynamics, for simulating incompressible viscous flows.","sentences":["In this paper, we consider a new nonlocal approximation to the linear Stokes system with periodic boundary conditions in two and three dimensional spaces .","A relaxation term is added to the equation of nonlocal divergence free equation, which is reminiscent to the relaxation of local Stokes equation with small artificial compressibility.","Our analysis shows that the well-posedness of the nonlocal system can be established under some mild assumptions on the kernel of nonlocal interactions.","Furthermore, the new nonlocal system converges to the conventional, local Stokes system in second order as the horizon parameter of the nonlocal interaction goes to zero.","The study provides more theoretical understanding to some numerical methods, such as smoothed particle hydrodynamics, for simulating incompressible viscous flows."],"url":"http://arxiv.org/abs/2403.07712v1","category":"math.AP"}
{"created":"2024-03-12 14:48:45","title":"Lipschitz maps with prescribed local Lipschitz constants","abstract":"Let $\\Gamma$ be a closed subset of a complete Riemannian manifold $M$ of dimension $\\geq 2$, let $f: M \\to N$ be a Lipschitz map to a complete Riemannian manifold $N$, and let $\\psi$ be a continuous function which dominates the local Lipschitz constant of $f$. We construct a Lipschitz map which agress with $f$ on $\\Gamma$ and whose local Lipschitz constant is $\\psi$.","sentences":["Let $\\Gamma$ be a closed subset of a complete Riemannian manifold $M$ of dimension $\\geq 2$, let $f: M \\to N$ be a Lipschitz map to a complete Riemannian manifold $N$, and let $\\psi$ be a continuous function which dominates the local Lipschitz constant of $f$. We construct a Lipschitz map which agress with $f$ on $\\Gamma$ and whose local Lipschitz constant is $\\psi$."],"url":"http://arxiv.org/abs/2403.07702v1","category":"math.DG"}
{"created":"2024-03-12 14:41:57","title":"The Kazdan-Warner problem on compact K\u00e4hler surfaces","abstract":"In this paper, we investigate a Kazdan-Warner problem on compact K\\\"ahler surfaces with negative Gauduchon degree, which corresponds to prescribing sign-changing Chern scalar curvatures. By the method of our recent paper [J. Funt. Anal. 285 (2023): 109948], we establish a Chen-Li type existence theorem on compact K\\\"ahler surfaces when the candidate curvature function is of negative average. Moreover, we give an alternative proof of Ding-Liu's theorem [Trans. Amer. Math. Soc. 347(1995) 1059-1066] on prescribing sign-changing Gaussian curvatures by using the $\\sup+\\inf$ inequality due to H. Brezis, Y. Y. Li and I. Shafrir.","sentences":["In this paper, we investigate a Kazdan-Warner problem on compact K\\\"ahler surfaces with negative Gauduchon degree, which corresponds to prescribing sign-changing Chern scalar curvatures.","By the method of our recent paper [J. Funt.","Anal. 285 (2023): 109948], we establish a Chen-Li type existence theorem on compact K\\\"ahler surfaces when the candidate curvature function is of negative average.","Moreover, we give an alternative proof of Ding-Liu's theorem [Trans.","Amer.","Math.","Soc.","347(1995) 1059-1066] on prescribing sign-changing Gaussian curvatures by using the $\\sup+\\inf$ inequality due to H. Brezis, Y. Y. Li and I. Shafrir."],"url":"http://arxiv.org/abs/2403.07698v1","category":"math.DG"}
{"created":"2024-03-12 14:40:19","title":"Using Equation of State Constraints to Classify Low-Mass Compact Binary Mergers","abstract":"Compact objects observed via gravitational waves are classified as black holes or neutron stars primarily based on their inferred mass with respect to stellar evolution expectations. However, astrophysical expectations for the lowest mass range, $\\lesssim 1.2 \\,M_\\odot$, are uncertain. If such low-mass compact objects exist, ground-based gravitational wave detectors may observe them in binary mergers. Lacking astrophysical expectations for classifying such observations, we go beyond the mass and explore the role of tidal effects. We evaluate how combined mass and tidal inference can inform whether each binary component is a black hole or a neutron star based on consistency with the supranuclear-density equation of state. Low-mass neutron stars experience a large tidal deformation; its observational identification (or lack thereof) can therefore aid in determining the nature of the binary components. Using simulated data, we find that the presence of a sub-solar mass neutron star (black hole) can be established with odds $\\sim 100:1$ when two neutron stars (black holes) merge and emit gravitational waves at signal-to-noise ratio $\\sim 20$. For the same systems, the absence of a black hole (neutron star) can be established with odds $\\sim 10:1$. For mixed neutron star-black hole binaries, we can establish that the system contains a neutron star with odds $\\gtrsim 5:1$. Establishing the presence of a black hole in mixed neutron star-black hole binaries is more challenging, except for the case of a $\\lesssim 1\\,M_{\\odot}$ black hole with a $\\gtrsim 1\\,M_{\\odot}$ neutron star companion. On the other hand, classifying each individual binary component suffers from an inherent labeling ambiguity.","sentences":["Compact objects observed via gravitational waves are classified as black holes or neutron stars primarily based on their inferred mass with respect to stellar evolution expectations.","However, astrophysical expectations for the lowest mass range, $\\lesssim 1.2 \\,M_\\odot$, are uncertain.","If such low-mass compact objects exist, ground-based gravitational wave detectors may observe them in binary mergers.","Lacking astrophysical expectations for classifying such observations, we go beyond the mass and explore the role of tidal effects.","We evaluate how combined mass and tidal inference can inform whether each binary component is a black hole or a neutron star based on consistency with the supranuclear-density equation of state.","Low-mass neutron stars experience a large tidal deformation; its observational identification (or lack thereof) can therefore aid in determining the nature of the binary components.","Using simulated data, we find that the presence of a sub-solar mass neutron star (black hole) can be established with odds $\\sim 100:1$ when two neutron stars (black holes) merge and emit gravitational waves at signal-to-noise ratio $\\sim 20$.","For the same systems, the absence of a black hole (neutron star) can be established with odds $\\sim 10:1$. For mixed neutron star-black hole binaries, we can establish that the system contains a neutron star with odds $\\gtrsim 5:1$. Establishing the presence of a black hole in mixed neutron star-black hole binaries is more challenging, except for the case of a $\\lesssim 1\\,M_{\\odot}$ black hole with a $\\gtrsim 1\\,M_{\\odot}$ neutron star companion.","On the other hand, classifying each individual binary component suffers from an inherent labeling ambiguity."],"url":"http://arxiv.org/abs/2403.07697v1","category":"astro-ph.HE"}
{"created":"2024-03-12 14:31:38","title":"Probing anomalous $Z\u03b3\u03b3\u03b3$ couplings at a future muon collider","abstract":"The sensitivity to anomalous quartic gauge couplings (AQGCs) of the $\\gamma\\gamma\\gamma Z$ interaction is studied in the $\\mu^+\\mu^- \\rightarrow \\mu^+\\gamma\\gamma \\mu^-$ scattering at a future muon collider with unpolarized beams. The anomalous $\\gamma\\gamma\\gamma Z$ vertex is described by two couplings, $\\zeta_1$ and $\\zeta_2$. The differential and total cross sections are calculated for the center-of-mass energies of 3 TeV, 14 TeV, and 100 TeV. For these values of the collision energy the $95\\%$ C.L. exclusion regions for AQGCs are obtained depending on the systematic error. In particular, for the 14 TeV muon collider with the integrated luminosity $L = 20$ ab$^{-1}$ the best sensitivities are derived to be $\\zeta_1 = 3.1 \\times 10^{-5}$ TeV$^{-4}$ and $\\zeta_2 = 6.5 \\times 10^{-5}$ TeV$^{-4}$. These constraints are three orders of magnitude stronger than the bounds obtained for the 27 TeV HE-LHC with $L = 15$ ab$^{-1}$. At the 100 TeV muon collider with $L = 1000$ ab$^{-1}$ AQGCs can be probed up to $(1.64 \\div 3.4) \\times 10^{-8}$ TeV$^{-4}$. The partial-wave unitarity constraints on couplings $\\zeta_1$, $\\zeta_2$ are evaluated. It is shown that the unitarity is not violated in the region of the AQGCs examined in the present paper.","sentences":["The sensitivity to anomalous quartic gauge couplings (AQGCs) of the $\\gamma\\gamma\\gamma Z$ interaction is studied in the $\\mu^+\\mu^- \\rightarrow \\mu^+\\gamma\\gamma \\mu^-$ scattering at a future muon collider with unpolarized beams.","The anomalous $\\gamma\\gamma\\gamma Z$ vertex is described by two couplings, $\\zeta_1$ and $\\zeta_2$. The differential and total cross sections are calculated for the center-of-mass energies of 3 TeV, 14 TeV, and 100 TeV. For these values of the collision energy the $95\\%$ C.L. exclusion regions for AQGCs are obtained depending on the systematic error.","In particular, for the 14 TeV muon collider with the integrated luminosity $L = 20$ ab$^{-1}$ the best sensitivities are derived to be $\\zeta_1 = 3.1 \\times 10^{-5}$ TeV$^{-4}$ and $\\zeta_2 = 6.5 \\times 10^{-5}$ TeV$^{-4}$. These constraints are three orders of magnitude stronger than the bounds obtained for the 27 TeV HE-LHC with $L = 15$ ab$^{-1}$. At the 100 TeV muon collider with $L = 1000$ ab$^{-1}$ AQGCs can be probed up to $(1.64 \\div 3.4)","\\times 10^{-8}$ TeV$^{-4}$.","The partial-wave unitarity constraints on couplings $\\zeta_1$, $\\zeta_2$ are evaluated.","It is shown that the unitarity is not violated in the region of the AQGCs examined in the present paper."],"url":"http://arxiv.org/abs/2403.07689v1","category":"hep-ph"}
{"created":"2024-03-12 17:57:56","title":"Online Digital Twin-Empowered Content Resale Mechanism in Age of Information-Aware Edge Caching Networks","abstract":"For users requesting popular contents from content providers, edge caching can alleviate backhaul pressure and enhance the quality of experience of users. Recently there is also a growing concern about content freshness that is quantified by age of information (AoI). Therefore, AoI-aware online caching algorithms are required, which is challenging because the content popularity is usually unknown in advance and may vary over time. In this paper, we propose an online digital twin (DT) empowered content resale mechanism in AoI-aware edge caching networks. We aim to design an optimal two-timescale caching strategy to maximize the utility of an edge network service provider (ENSP). The formulated optimization problem is non-convex and NP-hard. To tackle this intractable problem, we propose a DT-assisted Online Caching Algorithm (DT-OCA). In specific, we first decompose our formulated problem into a series of subproblems, each handling a cache period. For each cache period, we use a DT-based prediction method to effectively capture future content popularity, and develop online caching strategy. Competitive ratio analysis and extensive experimental results demonstrate that our algorithm has promising performance, and outperforms other benchmark algorithms. Insightful observations are also found and discussed.","sentences":["For users requesting popular contents from content providers, edge caching can alleviate backhaul pressure and enhance the quality of experience of users.","Recently there is also a growing concern about content freshness that is quantified by age of information (AoI).","Therefore, AoI-aware online caching algorithms are required, which is challenging because the content popularity is usually unknown in advance and may vary over time.","In this paper, we propose an online digital twin (DT) empowered content resale mechanism in AoI-aware edge caching networks.","We aim to design an optimal two-timescale caching strategy to maximize the utility of an edge network service provider (ENSP).","The formulated optimization problem is non-convex and NP-hard.","To tackle this intractable problem, we propose a DT-assisted Online Caching Algorithm (DT-OCA).","In specific, we first decompose our formulated problem into a series of subproblems, each handling a cache period.","For each cache period, we use a DT-based prediction method to effectively capture future content popularity, and develop online caching strategy.","Competitive ratio analysis and extensive experimental results demonstrate that our algorithm has promising performance, and outperforms other benchmark algorithms.","Insightful observations are also found and discussed."],"url":"http://arxiv.org/abs/2403.07868v1","category":"cs.NI"}
{"created":"2024-03-12 17:57:35","title":"The Virtues of Laziness: Multi-Query Kinodynamic Motion Planning with Lazy Methods","abstract":"In this work, we introduce LazyBoE, a multi-query method for kinodynamic motion planning with forward propagation. This algorithm allows for the simultaneous exploration of a robot's state and control spaces, thereby enabling a wider suite of dynamic tasks in real-world applications. Our contributions are three-fold: i) a method for discretizing the state and control spaces to amortize planning times across multiple queries; ii) lazy approaches to collision checking and propagation of control sequences that decrease the cost of physics-based simulation; and iii) LazyBoE, a robust kinodynamic planner that leverages these two contributions to produce dynamically-feasible trajectories. The proposed framework not only reduces planning time but also increases success rate in comparison to previous approaches.","sentences":["In this work, we introduce LazyBoE, a multi-query method for kinodynamic motion planning with forward propagation.","This algorithm allows for the simultaneous exploration of a robot's state and control spaces, thereby enabling a wider suite of dynamic tasks in real-world applications.","Our contributions are three-fold: i) a method for discretizing the state and control spaces to amortize planning times across multiple queries; ii) lazy approaches to collision checking and propagation of control sequences that decrease the cost of physics-based simulation; and iii) LazyBoE, a robust kinodynamic planner that leverages these two contributions to produce dynamically-feasible trajectories.","The proposed framework not only reduces planning time but also increases success rate in comparison to previous approaches."],"url":"http://arxiv.org/abs/2403.07867v1","category":"cs.RO"}
{"created":"2024-03-12 17:44:26","title":"Improving Fairness in Photovoltaic Curtailments via Daily Topology Reconfiguration for Voltage Control in Power Distribution Networks","abstract":"In PV-rich power distribution systems, over-voltage issues are often addressed by curtailing excess generation from PV plants (in addition to reactive power control), raising fairness concerns. Existing fairness-aware control schemes tackle this problem by incorporating fairness objectives into the cost function. However, such schemes result in increased overall curtailments. This paper proposes a solution through daily topology reconfiguration, ensuring that different PV plants face varying grid conditions each day, leading to different curtailment levels and enhancing fairness. We illustrate that implementing this approach enhances overall fairness without significantly increasing overall curtailments. The optimization problem involves two stages. The day-ahead stage optimizes the network topology using day-ahead forecasts of PV generation and demand, minimizing net curtailment and accounting for fairness based on curtailments from prior days. The real-time stage implements the optimized topology and computes active and reactive power setpoints for the PV plants. Day-ahead grid constraints are modeled using LinDistFlow, and real-time control employs a linearized model with a first-order Taylor approximation. The proposed scheme is numerically validated on several benchmark test cases. Results are compared using the Jain Fairness Index, considering fairness and reconfiguration scenarios.","sentences":["In PV-rich power distribution systems, over-voltage issues are often addressed by curtailing excess generation from PV plants (in addition to reactive power control), raising fairness concerns.","Existing fairness-aware control schemes tackle this problem by incorporating fairness objectives into the cost function.","However, such schemes result in increased overall curtailments.","This paper proposes a solution through daily topology reconfiguration, ensuring that different PV plants face varying grid conditions each day, leading to different curtailment levels and enhancing fairness.","We illustrate that implementing this approach enhances overall fairness without significantly increasing overall curtailments.","The optimization problem involves two stages.","The day-ahead stage optimizes the network topology using day-ahead forecasts of PV generation and demand, minimizing net curtailment and accounting for fairness based on curtailments from prior days.","The real-time stage implements the optimized topology and computes active and reactive power setpoints for the PV plants.","Day-ahead grid constraints are modeled using LinDistFlow, and real-time control employs a linearized model with a first-order Taylor approximation.","The proposed scheme is numerically validated on several benchmark test cases.","Results are compared using the Jain Fairness Index, considering fairness and reconfiguration scenarios."],"url":"http://arxiv.org/abs/2403.07853v1","category":"eess.SY"}
{"created":"2024-03-12 17:32:52","title":"A Machine learning and Empirical Bayesian Approach for Predictive Buying in B2B E-commerce","abstract":"In the context of developing nations like India, traditional business to business (B2B) commerce heavily relies on the establishment of robust relationships, trust, and credit arrangements between buyers and sellers. Consequently, ecommerce enterprises frequently. Established in 2016 with a vision to revolutionize trade in India through technology, Udaan is the countrys largest business to business ecommerce platform. Udaan operates across diverse product categories, including lifestyle, electronics, home and employ telecallers to cultivate buyer relationships, streamline order placement procedures, and promote special promotions. The accurate anticipation of buyer order placement behavior emerges as a pivotal factor for attaining sustainable growth, heightening competitiveness, and optimizing the efficiency of these telecallers. To address this challenge, we have employed an ensemble approach comprising XGBoost and a modified version of Poisson Gamma model to predict customer order patterns with precision. This paper provides an in-depth exploration of the strategic fusion of machine learning and an empirical Bayesian approach, bolstered by the judicious selection of pertinent features. This innovative approach has yielded a remarkable 3 times increase in customer order rates, show casing its potential for transformative impact in the ecommerce industry.","sentences":["In the context of developing nations like India, traditional business to business (B2B) commerce heavily relies on the establishment of robust relationships, trust, and credit arrangements between buyers and sellers.","Consequently, ecommerce enterprises frequently.","Established in 2016 with a vision to revolutionize trade in India through technology, Udaan is the countrys largest business to business ecommerce platform.","Udaan operates across diverse product categories, including lifestyle, electronics, home and employ telecallers to cultivate buyer relationships, streamline order placement procedures, and promote special promotions.","The accurate anticipation of buyer order placement behavior emerges as a pivotal factor for attaining sustainable growth, heightening competitiveness, and optimizing the efficiency of these telecallers.","To address this challenge, we have employed an ensemble approach comprising XGBoost and a modified version of Poisson Gamma model to predict customer order patterns with precision.","This paper provides an in-depth exploration of the strategic fusion of machine learning and an empirical Bayesian approach, bolstered by the judicious selection of pertinent features.","This innovative approach has yielded a remarkable 3 times increase in customer order rates, show casing its potential for transformative impact in the ecommerce industry."],"url":"http://arxiv.org/abs/2403.07843v1","category":"cs.LG"}
{"created":"2024-03-12 16:51:16","title":"Mesh Refinement with Early Termination for Dynamic Feasibility Problems","abstract":"We propose a novel early-terminating mesh refinement strategy using an integrated residual method to solve dynamic feasibility problems. As a generalization of direct collocation, the integrated residual method is used to approximate an infinite-dimensional problem into a sequence of finite-dimensional optimization subproblems. Each subproblem in the sequence is a finer approximation of the previous. It is shown that these subproblems need not be solved to a high precision; instead, an early termination procedure can determine when mesh refinement should be performed. The new refinement strategy, applied to an inverted pendulum swing-up problem, outperforms a conventional refinement method by up to a factor of three in function evaluations.","sentences":["We propose a novel early-terminating mesh refinement strategy using an integrated residual method to solve dynamic feasibility problems.","As a generalization of direct collocation, the integrated residual method is used to approximate an infinite-dimensional problem into a sequence of finite-dimensional optimization subproblems.","Each subproblem in the sequence is a finer approximation of the previous.","It is shown that these subproblems need not be solved to a high precision; instead, an early termination procedure can determine when mesh refinement should be performed.","The new refinement strategy, applied to an inverted pendulum swing-up problem, outperforms a conventional refinement method by up to a factor of three in function evaluations."],"url":"http://arxiv.org/abs/2403.07811v1","category":"math.OC"}
{"created":"2024-03-12 16:43:55","title":"A Stochastic GDA Method With Backtracking For Solving Nonconvex (Strongly) Concave Minimax Problems","abstract":"We propose a stochastic GDA (gradient descent ascent) method with backtracking (SGDA-B) to solve nonconvex-(strongly) concave (NCC) minimax problems $\\min_x \\max_y \\sum_{i=1}^N g_i(x_i)+f(x,y)-h(y)$, where $h$ and $g_i$ for $i = 1, \\ldots, N$ are closed, convex functions, $f$ is $L$-smooth and $\\mu$-strongly concave in $y$ for some $\\mu\\geq 0$. We consider two scenarios: (i) the deterministic setting where we assume one can compute $\\nabla f$ exactly, and (ii) the stochastic setting where we have only access to $\\nabla f$ through an unbiased stochastic oracle with a finite variance. While most of the existing methods assume knowledge of the Lipschitz constant $L$, SGDA-B is agnostic to $L$. Moreover, SGDA-B can support random block-coordinate updates. In the deterministic setting, SGDA-B can compute an $\\epsilon$-stationary point within $\\mathcal{O}(L\\kappa^2/\\epsilon^2)$ and $\\mathcal{O}(L^3/\\epsilon^4)$ gradient calls when $\\mu>0$ and $\\mu=0$, respectively, where $\\kappa=L/\\mu$. In the stochastic setting, for any $p \\in (0, 1)$ and $\\epsilon >0$, it can compute an $\\epsilon$-stationary point with high probability, which requires $\\mathcal{O}(L\\kappa^3\\epsilon^{-4}\\log(1/p))$ and $\\tilde{\\mathcal{O}}(L^4\\epsilon^{-7}\\log(1/p))$ stochastic oracle calls, with probability at least $1-p$, when $\\mu>0$ and $\\mu=0$, respectively. To our knowledge, SGDA-B is the first GDA-type method with backtracking to solve NCC minimax problems and achieves the best complexity among the methods that are agnostic to $L$. We also provide numerical results for SGDA-B on a distributionally robust learning problem illustrating the potential performance gains that can be achieved by SGDA-B.","sentences":["We propose a stochastic GDA (gradient descent ascent) method with backtracking (SGDA-B) to solve nonconvex-(strongly) concave (NCC) minimax problems $\\min_x \\max_y \\sum_{i=1}^N g_i(x_i)+f(x,y)-h(y)$, where $h$ and $g_i$ for $i = 1, \\ldots, N$ are closed, convex functions, $f$ is $L$-smooth and $\\mu$-strongly concave in $y$ for some $\\mu\\geq 0$.","We consider two scenarios: (i) the deterministic setting where we assume one can compute $\\nabla f$ exactly, and (ii) the stochastic setting where we have only access to $\\nabla f$ through an unbiased stochastic oracle with a finite variance.","While most of the existing methods assume knowledge of the Lipschitz constant $L$, SGDA-B is agnostic to $L$. Moreover, SGDA-B can support random block-coordinate updates.","In the deterministic setting, SGDA-B can compute an $\\epsilon$-stationary point within $\\mathcal{O}(L\\kappa^2/\\epsilon^2)$ and $\\mathcal{O}(L^3/\\epsilon^4)$ gradient calls when $\\mu>0$ and $\\mu=0$, respectively, where $\\kappa=L/\\mu$.","In the stochastic setting, for any $p \\in (0, 1)$ and $\\epsilon >0$, it can compute an $\\epsilon$-stationary point with high probability, which requires $\\mathcal{O}(L\\kappa^3\\epsilon^{-4}\\log(1/p))$ and $\\tilde{\\mathcal{O}}(L^4\\epsilon^{-7}\\log(1/p))$ stochastic oracle calls, with probability at least $1-p$, when $\\mu>0$ and $\\mu=0$, respectively.","To our knowledge, SGDA-B is the first GDA-type method with backtracking to solve NCC minimax problems and achieves the best complexity among the methods that are agnostic to $L$. We also provide numerical results for SGDA-B on a distributionally robust learning problem illustrating the potential performance gains that can be achieved by SGDA-B."],"url":"http://arxiv.org/abs/2403.07806v1","category":"math.OC"}
{"created":"2024-03-12 15:59:50","title":"The probabilistic p-center problem: Planning service for potential customers","abstract":"This work deals with the probabilistic p-center problem, which aims at minimizing the expected maximum distance between any site with demand and its center, considering that each site has demand with a specific probability. The problem is of interest when emergencies may occur at predefined sites with known probabilities. For this problem we propose and analyze different formulations as well as a Variable Neighborhood Search heuristic. Computational tests are reported, showing the potentials and limits of each formulation, the impact of their enhancements, and the effectiveness of the heuristic.","sentences":["This work deals with the probabilistic p-center problem, which aims at minimizing the expected maximum distance between any site with demand and its center, considering that each site has demand with a specific probability.","The problem is of interest when emergencies may occur at predefined sites with known probabilities.","For this problem we propose and analyze different formulations as well as a Variable Neighborhood Search heuristic.","Computational tests are reported, showing the potentials and limits of each formulation, the impact of their enhancements, and the effectiveness of the heuristic."],"url":"http://arxiv.org/abs/2403.07775v1","category":"math.OC"}
{"created":"2024-03-12 15:43:18","title":"Pediatric vaccine tender scheduling in low- and middle-income countries","abstract":"Effective and efficient scheduling of vaccine distribution can significantly impact vaccine uptake, which is critical to controlling the spread of infectious diseases. Ineffective scheduling can lead to waste, delays, and low vaccine coverage, potentially weakening the efforts to protect the public. Organizations such as UNICEF (United Nations Children's Fund), PAHO (Pan American Health Organization), and GAVI (Gavi, the Vaccine Alliance) coordinate vaccine tenders to ensure that enough supply is available on the international market at the lowest possible prices. Scheduling vaccine tenders over a planning horizon in a way that is equitable, efficient, and accessible is a complex problem that involves trade-offs between multiple objectives while ensuring that vaccine availability, demand, and logistical constraints are met. The current method for scheduling tenders is generally reactive and over short planning horizons. Vaccine tenders are scheduled when supply is insufficient to cover demand. We propose an optimization model to dynamically and proactively generate vaccine tender schedules over long planning horizons. This model helps us address the following research questions: What should the optimal sequencing and scheduling of vaccine tenders be to enhance affordability and profit over long time horizons? What is the optimal tender procurement schedule for single or multiple antigen scenarios? We use several real-life data sources to validate the model and address our research questions. Results from our analysis show when to schedule vaccine tenders, what volumes manufacturers should commit to, and the optimal tender lengths to satisfy demand. We show that vaccine tenders tend towards maximum lengths, generally converge over long time horizons, and are robust to changes in varying conditions.","sentences":["Effective and efficient scheduling of vaccine distribution can significantly impact vaccine uptake, which is critical to controlling the spread of infectious diseases.","Ineffective scheduling can lead to waste, delays, and low vaccine coverage, potentially weakening the efforts to protect the public.","Organizations such as UNICEF (United Nations Children's Fund), PAHO (Pan American Health Organization), and GAVI (Gavi, the Vaccine Alliance) coordinate vaccine tenders to ensure that enough supply is available on the international market at the lowest possible prices.","Scheduling vaccine tenders over a planning horizon in a way that is equitable, efficient, and accessible is a complex problem that involves trade-offs between multiple objectives while ensuring that vaccine availability, demand, and logistical constraints are met.","The current method for scheduling tenders is generally reactive and over short planning horizons.","Vaccine tenders are scheduled when supply is insufficient to cover demand.","We propose an optimization model to dynamically and proactively generate vaccine tender schedules over long planning horizons.","This model helps us address the following research questions: What should the optimal sequencing and scheduling of vaccine tenders be to enhance affordability and profit over long time horizons?","What is the optimal tender procurement schedule for single or multiple antigen scenarios?","We use several real-life data sources to validate the model and address our research questions.","Results from our analysis show when to schedule vaccine tenders, what volumes manufacturers should commit to, and the optimal tender lengths to satisfy demand.","We show that vaccine tenders tend towards maximum lengths, generally converge over long time horizons, and are robust to changes in varying conditions."],"url":"http://arxiv.org/abs/2403.07755v1","category":"math.OC"}
{"created":"2024-03-12 15:41:12","title":"A robust SVM-based approach with feature selection and outliers detection for classification problems","abstract":"This paper proposes a robust classification model, based on support vector machine (SVM), which simultaneously deals with outliers detection and feature selection. The classifier is built considering the ramp loss margin error and it includes a budget constraint to limit the number of selected features. The search of this classifier is modeled using a mixed-integer formulation with big M parameters. Two different approaches (exact and heuristic) are proposed to solve the model. The heuristic approach is validated by comparing the quality of the solutions provided by this approach with the exact approach. In addition, the classifiers obtained with the heuristic method are tested and compared with existing SVM-based models to demonstrate their efficiency.","sentences":["This paper proposes a robust classification model, based on support vector machine (SVM), which simultaneously deals with outliers detection and feature selection.","The classifier is built considering the ramp loss margin error and it includes a budget constraint to limit the number of selected features.","The search of this classifier is modeled using a mixed-integer formulation with big M parameters.","Two different approaches (exact and heuristic) are proposed to solve the model.","The heuristic approach is validated by comparing the quality of the solutions provided by this approach with the exact approach.","In addition, the classifiers obtained with the heuristic method are tested and compared with existing SVM-based models to demonstrate their efficiency."],"url":"http://arxiv.org/abs/2403.07753v1","category":"math.OC"}
{"created":"2024-03-12 15:37:02","title":"Quotients of M-convex sets and M-convex functions","abstract":"We unify the study of quotients of matroids, polymatroids, valuated matroids and strong maps of submodular functions in the framework of Murota's discrete convex analysis. As a main result, we compile a list of ten equivalent characterizations of quotients for M-convex sets, generalizing existing formulations for (poly)matroids and submodular functions. We also initiate the study of quotients of M-convex functions, constructing a hierarchy of four separate characterizations. Our investigations yield new insights into the fundamental operation of induction, as well as the structure of linking sets and linking functions, which are generalizations of linking systems and bimatroids.","sentences":["We unify the study of quotients of matroids, polymatroids, valuated matroids and strong maps of submodular functions in the framework of Murota's discrete convex analysis.","As a main result, we compile a list of ten equivalent characterizations of quotients for M-convex sets, generalizing existing formulations for (poly)matroids and submodular functions.","We also initiate the study of quotients of M-convex functions, constructing a hierarchy of four separate characterizations.","Our investigations yield new insights into the fundamental operation of induction, as well as the structure of linking sets and linking functions, which are generalizations of linking systems and bimatroids."],"url":"http://arxiv.org/abs/2403.07751v1","category":"math.CO"}
{"created":"2024-03-12 15:34:55","title":"Distributed Estimation by Two Agents with Different Feature Spaces","abstract":"We consider the problem of estimation of a function by two agents (and a fusion center) given local data. Data comprises of samples of an independent variable and the corresponding value of a dependent variable. The agents are given a set of features using which they construct suitable function spaces to formulate and solve the estimation problem. The estimated functions are to be uploaded to a fusion space where they are fused to obtain the system estimate of the mapping and then downloaded by the agents to gather knowledge about the other agents estimate of the function. To this end, we present the following: a systematic construction of fusion space given the features of the agents; the derivation of an uploading operator for the agents to upload their estimated functions to a fusion space; an optimization problem in the fusion space to fuse the functions uploaded; the derivation of a downloading operator for the fused function to be downloaded. Through an example on least squares regression, we demonstrate the distributed estimation architecture that has been developed.","sentences":["We consider the problem of estimation of a function by two agents (and a fusion center) given local data.","Data comprises of samples of an independent variable and the corresponding value of a dependent variable.","The agents are given a set of features using which they construct suitable function spaces to formulate and solve the estimation problem.","The estimated functions are to be uploaded to a fusion space where they are fused to obtain the system estimate of the mapping and then downloaded by the agents to gather knowledge about the other agents estimate of the function.","To this end, we present the following: a systematic construction of fusion space given the features of the agents; the derivation of an uploading operator for the agents to upload their estimated functions to a fusion space; an optimization problem in the fusion space to fuse the functions uploaded; the derivation of a downloading operator for the fused function to be downloaded.","Through an example on least squares regression, we demonstrate the distributed estimation architecture that has been developed."],"url":"http://arxiv.org/abs/2403.07749v1","category":"eess.SY"}
{"created":"2024-03-12 15:13:40","title":"Tightening big Ms in integer programming formulations for support vector machines with ramp loss","abstract":"This paper considers various models of support vector machines with ramp loss, these being an efficient and robust tool in supervised classification for the detection of outliers. The exact solution approaches for the resulting optimization problem are of high demand for large datasets. Hence, the goal of this paper is to develop algorithms that provide efficient methodologies to exactly solve these optimization problems. These approaches are based on three strategies for obtaining tightened values of the big M parameters included in the formulation of the problem. Two of them require solving a sequence of continuous problems, while the third uses the Lagrangian relaxation to tighten the bounds. The proposed resolution methods are valid for the l1-norm and l2-norm ramp loss formulations. They were tested and compared with existing solution methods in simulated and real-life datasets, showing the efficiency of the developed methodology.","sentences":["This paper considers various models of support vector machines with ramp loss, these being an efficient and robust tool in supervised classification for the detection of outliers.","The exact solution approaches for the resulting optimization problem are of high demand for large datasets.","Hence, the goal of this paper is to develop algorithms that provide efficient methodologies to exactly solve these optimization problems.","These approaches are based on three strategies for obtaining tightened values of the big M parameters included in the formulation of the problem.","Two of them require solving a sequence of continuous problems, while the third uses the Lagrangian relaxation to tighten the bounds.","The proposed resolution methods are valid for the l1-norm and l2-norm ramp loss formulations.","They were tested and compared with existing solution methods in simulated and real-life datasets, showing the efficiency of the developed methodology."],"url":"http://arxiv.org/abs/2403.07736v1","category":"math.OC"}
{"created":"2024-03-12 15:13:21","title":"The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels","abstract":"Kernel techniques are among the most influential approaches in data science and statistics. Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of $M\\ge 2$ random variables. Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature). Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open. In this work, we prove that the minimax optimal rate of HSIC estimation on $\\mathbb R^d$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $\\mathcal O\\!\\left(n^{-1/2}\\right)$. Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nystr\\\"om-based one) on $\\mathbb R^d$.","sentences":["Kernel techniques are among the most influential approaches in data science and statistics.","Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of $M\\ge 2$ random variables.","Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature).","Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open.","In this work, we prove that the minimax optimal rate of HSIC estimation on $\\mathbb R^d$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $\\mathcal O\\!\\left(n^{-1/2}\\right)$.","Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nystr\\\"om-based one) on $\\mathbb R^d$."],"url":"http://arxiv.org/abs/2403.07735v1","category":"math.ST"}
{"created":"2024-03-12 15:01:17","title":"On the Last-Iterate Convergence of Shuffling Gradient Methods","abstract":"Shuffling gradient methods, which are also known as stochastic gradient descent (SGD) without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objective value even without strong convexity. Our new results either (nearly) match the existing last-iterate lower bounds or are as fast as the previous best upper bounds for the average iterate.","sentences":["Shuffling gradient methods, which are also known as stochastic gradient descent (SGD) without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG).","Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time.","Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric).","However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization).","To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objective value even without strong convexity.","Our new results either (nearly) match the existing last-iterate lower bounds or are as fast as the previous best upper bounds for the average iterate."],"url":"http://arxiv.org/abs/2403.07723v1","category":"cs.LG"}
{"created":"2024-03-12 14:51:51","title":"Tightly Bounded Polynomials via Flexible Discretizations for Dynamic Optimization Problems","abstract":"Polynomials are widely used to represent the trajectories of states and/or inputs. It has been shown that a polynomial can be bounded by its coefficients, when expressed in the Bernstein basis. However, in general, the bounds provided by the Bernstein coefficients are not tight. We propose a method for obtaining numerical solutions to dynamic optimization problems, where a flexible discretization is used to achieve tight polynomial bounds. The proposed method is used to solve a constrained cart-pole swing-up optimal control problem. The flexible discretization eliminates the conservatism of the Bernstein bounds and enables a lower cost, in comparison with non-flexible discretizations. A theoretical result on obtaining tight polynomial bounds with a finite discretization is presented. In some applications with linear dynamics, the non-convexity introduced by the flexible discretization may be a drawback.","sentences":["Polynomials are widely used to represent the trajectories of states and/or inputs.","It has been shown that a polynomial can be bounded by its coefficients, when expressed in the Bernstein basis.","However, in general, the bounds provided by the Bernstein coefficients are not tight.","We propose a method for obtaining numerical solutions to dynamic optimization problems, where a flexible discretization is used to achieve tight polynomial bounds.","The proposed method is used to solve a constrained cart-pole swing-up optimal control problem.","The flexible discretization eliminates the conservatism of the Bernstein bounds and enables a lower cost, in comparison with non-flexible discretizations.","A theoretical result on obtaining tight polynomial bounds with a finite discretization is presented.","In some applications with linear dynamics, the non-convexity introduced by the flexible discretization may be a drawback."],"url":"http://arxiv.org/abs/2403.07707v2","category":"math.OC"}
{"created":"2024-03-12 13:58:01","title":"Optical computing with supercontinuum generation in photonic crystal fibers","abstract":"We introduce a novel photonic neural network using photonic crystal fibers, leveraging femtosecond pulse supercontinuum generation for optical computing. Investigating its efficacy across machine learning tasks, we uncover the crucial impact of nonlinear pulse propagation dynamics on network performance. Our findings show that octave-spanning supercontinuum generation results in loss of dataset variety due to many-to-one mapping, and optimal performance requires balancing optical nonlinearity. This study offers guidance for designing energy-efficient and high-performance photonic neural network architectures by explaining the interplay between nonlinear dynamics and optical computing.","sentences":["We introduce a novel photonic neural network using photonic crystal fibers, leveraging femtosecond pulse supercontinuum generation for optical computing.","Investigating its efficacy across machine learning tasks, we uncover the crucial impact of nonlinear pulse propagation dynamics on network performance.","Our findings show that octave-spanning supercontinuum generation results in loss of dataset variety due to many-to-one mapping, and optimal performance requires balancing optical nonlinearity.","This study offers guidance for designing energy-efficient and high-performance photonic neural network architectures by explaining the interplay between nonlinear dynamics and optical computing."],"url":"http://arxiv.org/abs/2403.07667v1","category":"physics.optics"}
{"created":"2024-03-12 13:57:49","title":"Holistic numerical simulation of a quenching process on a real-size multifilamentary superconducting coil","abstract":"Superconductors play a crucial role in the advancement of high-field electromagnets. Unfortunately, their performance can be compromised by thermomagnetic instabilities, wherein the interplay of rapid magnetic and slow heat diffusion can result in catastrophic flux jumps eventually leading to irreversible damage. This issue has long plagued high-$J_c$ Nb$_3$Sn wires at the core of high-field magnets. In this study, we introduce a groundbreaking large-scale GPU-optimized algorithm aimed at tackling the complex intertwined effects of electromagnetism, heating, and strain acting concomitantly during the quenching process of superconducting coils. We validate our model by conducting comparisons with magnetization measurements obtained from short multifilamentary Nb$_3$Sn wires and further experimental tests conducted on solenoid coils while subject to ramping transport currents. Furthermore, leveraging our developed numerical algorithm, we unveil the dynamic propagation mechanisms underlying thermomagnetic instabilities (including flux jumps and quenches) within the coils. Remarkably, our findings reveal that the velocity field of flux jumps and quenches within the coil is correlated with the amount of Joule heating experienced by each wire over a specific time interval, rather than solely being dependent on instantaneous Joule heating or maximum temperature. These insights have the potential to pave the way for optimizing the design of next-generation superconducting magnets, thereby directly influencing a wide array of technologically relevant and multidisciplinary applications.","sentences":["Superconductors play a crucial role in the advancement of high-field electromagnets.","Unfortunately, their performance can be compromised by thermomagnetic instabilities, wherein the interplay of rapid magnetic and slow heat diffusion can result in catastrophic flux jumps eventually leading to irreversible damage.","This issue has long plagued high-$J_c$ Nb$_3$Sn wires at the core of high-field magnets.","In this study, we introduce a groundbreaking large-scale GPU-optimized algorithm aimed at tackling the complex intertwined effects of electromagnetism, heating, and strain acting concomitantly during the quenching process of superconducting coils.","We validate our model by conducting comparisons with magnetization measurements obtained from short multifilamentary Nb$_3$Sn wires and further experimental tests conducted on solenoid coils while subject to ramping transport currents.","Furthermore, leveraging our developed numerical algorithm, we unveil the dynamic propagation mechanisms underlying thermomagnetic instabilities (including flux jumps and quenches) within the coils.","Remarkably, our findings reveal that the velocity field of flux jumps and quenches within the coil is correlated with the amount of Joule heating experienced by each wire over a specific time interval, rather than solely being dependent on instantaneous Joule heating or maximum temperature.","These insights have the potential to pave the way for optimizing the design of next-generation superconducting magnets, thereby directly influencing a wide array of technologically relevant and multidisciplinary applications."],"url":"http://arxiv.org/abs/2403.07666v1","category":"cond-mat.supr-con"}
{"created":"2024-03-12 13:48:46","title":"An overdetermined problem in 2D linearised hydrostatics","abstract":"In two spatial dimensions, we discuss the relation between the solvability of Schiffer's overdetermined problem and the optimality, among sets of prescribed area, of the first eigenvalue in the buckling problem for a clamped plate and that of the first eigenvalue of the Stokes operator. For the latter, we deduce that the minimisers under area constraint that are smooth and simply connected must be discs from the fact that a pressureless velocity is a necessary condition of optimality.","sentences":["In two spatial dimensions, we discuss the relation between the solvability of Schiffer's overdetermined problem and the optimality, among sets of prescribed area, of the first eigenvalue in the buckling problem for a clamped plate and that of the first eigenvalue of the Stokes operator.","For the latter, we deduce that the minimisers under area constraint that are smooth and simply connected must be discs from the fact that a pressureless velocity is a necessary condition of optimality."],"url":"http://arxiv.org/abs/2403.07658v2","category":"math.AP"}
{"created":"2024-03-12 13:46:47","title":"Optimal control of stochastic cylinder flow using data-driven compressive sensing method","abstract":"A stochastic optimal control problem for incompressible Newtonian channel flow past a circular cylinder is used as a prototype optimal control problem for the stochastic Navier-Stokes equations. The inlet flow and the rotation speed of the cylinder are allowed to have stochastic perturbations. The control acts on the cylinder via adjustment of the rotation speed. Possible objectives of the control include, among others, tracking a desired (given) velocity field or minimizing the kinetic energy, enstrophy, or the drag of the flow over a given body. Owing to the high computational requirements, the direct application of the classical Monte Carlo methods for our problem is limited. To overcome the difficulty, we use a multi-fidelity data-driven compressive sensing based polynomial chaos expansions (MDCS-PCE). An effective gradient-based optimization for the discrete optimality systems resulted from the MDCS-PCE discretization is developed. The strategy can be applied broadly to many stochastic flow control problems. Numerical tests are performed to validate our methodology.","sentences":["A stochastic optimal control problem for incompressible Newtonian channel flow past a circular cylinder is used as a prototype optimal control problem for the stochastic Navier-Stokes equations.","The inlet flow and the rotation speed of the cylinder are allowed to have stochastic perturbations.","The control acts on the cylinder via adjustment of the rotation speed.","Possible objectives of the control include, among others, tracking a desired (given) velocity field or minimizing the kinetic energy, enstrophy, or the drag of the flow over a given body.","Owing to the high computational requirements, the direct application of the classical Monte Carlo methods for our problem is limited.","To overcome the difficulty, we use a multi-fidelity data-driven compressive sensing based polynomial chaos expansions (MDCS-PCE).","An effective gradient-based optimization for the discrete optimality systems resulted from the MDCS-PCE discretization is developed.","The strategy can be applied broadly to many stochastic flow control problems.","Numerical tests are performed to validate our methodology."],"url":"http://arxiv.org/abs/2403.07656v1","category":"math.OC"}
{"created":"2024-03-12 13:45:29","title":"Enhancing Physical Layer Security in Dual-Function Radar-Communication Systems with Hybrid Beamforming Architecture","abstract":"In this letter, we investigate enhancing the physical layer security (PLS) for the dual-function radar-communication (DFRC) system with hybrid beamforming (HBF) architecture, where the base station (BS) achieves downlink communication and radar target detection simultaneously. We consider an eavesdropper intercepting the information transmitted from the BS to the downlink communication users with imperfectly known channel state information. Additionally, the location of the radar target is also imperfectly known by the BS. To enhance PLS in the considered DFRC system, we propose a novel HBF architecture, which introduces a new integrated sensing and security (I2S) symbol. The secure HBF design problem for DFRC is formulated by maximizing the minimum legitimate user communication rate subject to radar interference-plus-noise ratio, eavesdropping rate, hardware and power constraints. To solve this non-convex problem, we propose an alternating optimization based method to jointly optimize transmit and receive beamformers. Numerical simulation results validate the effectiveness of the proposed algorithm and show the superiority of the proposed I2S-aided HBF architecture for achieving DFRC and enhancing PLS.","sentences":["In this letter, we investigate enhancing the physical layer security (PLS) for the dual-function radar-communication (DFRC) system with hybrid beamforming (HBF) architecture, where the base station (BS) achieves downlink communication and radar target detection simultaneously.","We consider an eavesdropper intercepting the information transmitted from the BS to the downlink communication users with imperfectly known channel state information.","Additionally, the location of the radar target is also imperfectly known by the BS.","To enhance PLS in the considered DFRC system, we propose a novel HBF architecture, which introduces a new integrated sensing and security (I2S) symbol.","The secure HBF design problem for DFRC is formulated by maximizing the minimum legitimate user communication rate subject to radar interference-plus-noise ratio, eavesdropping rate, hardware and power constraints.","To solve this non-convex problem, we propose an alternating optimization based method to jointly optimize transmit and receive beamformers.","Numerical simulation results validate the effectiveness of the proposed algorithm and show the superiority of the proposed I2S-aided HBF architecture for achieving DFRC and enhancing PLS."],"url":"http://arxiv.org/abs/2403.07655v1","category":"eess.SP"}
{"created":"2024-03-12 13:27:11","title":"Quantitative 2D propagation of smallness and control for 1D heat equations with power growth potentials","abstract":"We study the relation between propagation of smallness in the plane and control for heat equations. The former has been proved by Zhu who showed how the value of solutions in some small set propagates to a larger domain. By reviewing his proof, we establish a quantitative version with the explicit dependence of parameters. Using this explicit version, we establish new exact null-controllability results of 1D heat equations with any nonnegative power growth potentials $V\\in\\mathcal{C}(\\mathbb{R})$. As a key ingredient, new spectral inequalities are established. The control set $\\Omega$ that we consider satisfy \\begin{equation*}   \\left|\\Omega\\cap [x-L\\langle x\\rangle ^{-s},x+L\\langle x\\rangle ^{-s}]\\right|\\ge \\gamma^{\\langle x\\rangle^\\tau}2L\\langle x\\rangle^{-s} \\end{equation*} for some $\\gamma\\in(0,1)$, $L>0$, $\\tau,s\\ge 0$, and $\\langle x\\rangle:=(1+|x|^2)^{1 /2} $. In particular, the null-controllability result for the case of thick sets that allow the decay of the density (\\textit{i.e.}, $s=0$ and $\\tau\\ge 0$) is included. These extend the Zhu-Zhuge's results from $\\Omega$ being the union of equidistributive open sets to thick sets in the 1-dimensional case, and Su-Sun-Yuan's result from bounded potentials to certain unbounded ones.","sentences":["We study the relation between propagation of smallness in the plane and control for heat equations.","The former has been proved by Zhu who showed how the value of solutions in some small set propagates to a larger domain.","By reviewing his proof, we establish a quantitative version with the explicit dependence of parameters.","Using this explicit version, we establish new exact null-controllability results of 1D heat equations with any nonnegative power growth potentials $V\\in\\mathcal{C}(\\mathbb{R})$. As a key ingredient, new spectral inequalities are established.","The control set $\\Omega$ that we consider satisfy \\begin{equation*}   \\left|\\Omega\\cap","[x-L\\langle x\\rangle ^{-s},x+L\\langle x\\rangle ^{-s}]\\right|\\ge \\gamma^{\\langle x\\rangle^\\tau}2L\\langle x\\rangle^{-s} \\end{equation*} for some $\\gamma\\in(0,1)$, $L>0$, $\\tau,s\\ge 0$, and $\\langle x\\rangle:=(1+|x|^2)^{1 /2} $.","In particular, the null-controllability result for the case of thick sets that allow the decay of the density (\\textit{i.e.}, $s=0$ and $\\tau\\ge 0$) is included.","These extend the Zhu-Zhuge's results from $\\Omega$ being the union of equidistributive open sets to thick sets in the 1-dimensional case, and Su-Sun-Yuan's result from bounded potentials to certain unbounded ones."],"url":"http://arxiv.org/abs/2403.07643v1","category":"math.AP"}
{"created":"2024-03-12 13:18:22","title":"Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Matching Framework","abstract":"Medical vision language pre-training (VLP) has emerged as a frontier of research, enabling zero-shot pathological recognition by comparing the query image with the textual descriptions for each disease. Due to the complex semantics of biomedical texts, current methods struggle to align medical images with key pathological findings in unstructured reports. This leads to the misalignment with the target disease's textual representation. In this paper, we introduce a novel VLP framework designed to dissect disease descriptions into their fundamental aspects, leveraging prior knowledge about the visual manifestations of pathologies. This is achieved by consulting a large language model and medical experts. Integrating a Transformer module, our approach aligns an input image with the diverse elements of a disease, generating aspect-centric image representations. By consolidating the matches from each aspect, we improve the compatibility between an image and its associated disease. Additionally, capitalizing on the aspect-oriented representations, we present a dual-head Transformer tailored to process known and unknown diseases, optimizing the comprehensive detection efficacy. Conducting experiments on seven downstream datasets, ours outperforms recent methods by up to 8.07% and 11.23% in AUC scores for seen and novel categories, respectively. Our code is released at \\href{https://github.com/HieuPhan33/MAVL}{https://github.com/HieuPhan33/MAVL}.","sentences":["Medical vision language pre-training (VLP) has emerged as a frontier of research, enabling zero-shot pathological recognition by comparing the query image with the textual descriptions for each disease.","Due to the complex semantics of biomedical texts, current methods struggle to align medical images with key pathological findings in unstructured reports.","This leads to the misalignment with the target disease's textual representation.","In this paper, we introduce a novel VLP framework designed to dissect disease descriptions into their fundamental aspects, leveraging prior knowledge about the visual manifestations of pathologies.","This is achieved by consulting a large language model and medical experts.","Integrating a Transformer module, our approach aligns an input image with the diverse elements of a disease, generating aspect-centric image representations.","By consolidating the matches from each aspect, we improve the compatibility between an image and its associated disease.","Additionally, capitalizing on the aspect-oriented representations, we present a dual-head Transformer tailored to process known and unknown diseases, optimizing the comprehensive detection efficacy.","Conducting experiments on seven downstream datasets, ours outperforms recent methods by up to 8.07% and 11.23% in AUC scores for seen and novel categories, respectively.","Our code is released at \\href{https://github.com/HieuPhan33/MAVL}{https://github.com/HieuPhan33/MAVL}."],"url":"http://arxiv.org/abs/2403.07636v1","category":"cs.CV"}
{"created":"2024-03-12 13:12:24","title":"CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability","abstract":"Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel. It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity. In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity. The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade. These models can also serve independently as effective components of a virtual screening pipeline. We applied the complete framework to pimozide, an FDA-approved antipsychotic agent that demonstrates high affinity to the hERG channel, and generated 100 refined candidates. Remarkably, among the candidates is fluspirilene, a compound which is of the same class of drugs (diphenylmethanes) as pimozide and therefore has similar pharmacological activity, yet exhibits over 700-fold weaker binding to hERG. We have made all of our software open-source to facilitate integration of the CardioGenAI framework for molecular hypothesis generation into drug discovery workflows.","sentences":["Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel.","It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity.","In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity.","The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5","and","CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade.","These models can also serve independently as effective components of a virtual screening pipeline.","We applied the complete framework to pimozide, an FDA-approved antipsychotic agent that demonstrates high affinity to the hERG channel, and generated 100 refined candidates.","Remarkably, among the candidates is fluspirilene, a compound which is of the same class of drugs (diphenylmethanes) as pimozide and therefore has similar pharmacological activity, yet exhibits over 700-fold weaker binding to hERG.","We have made all of our software open-source to facilitate integration of the CardioGenAI framework for molecular hypothesis generation into drug discovery workflows."],"url":"http://arxiv.org/abs/2403.07632v1","category":"cs.LG"}
{"created":"2024-03-12 13:12:11","title":"Efficient Global Navigational Planning in 3D Structures based on Point Cloud Tomography","abstract":"Navigation in complex 3D scenarios requires appropriate environment representation for efficient scene understanding and trajectory generation. We propose a highly efficient and extensible global navigation framework based on a tomographic understanding of the environment to navigate ground robots in multi-layer structures. Our approach generates tomogram slices using the point cloud map to encode the geometric structure as ground and ceiling elevations. Then it evaluates the scene traversability considering the robot's motion capabilities. Both the tomogram construction and the scene evaluation are accelerated through parallel computation. Our approach further alleviates the trajectory generation complexity compared with planning in 3D spaces directly. It generates 3D trajectories by searching through multiple tomogram slices and separately adjusts the robot height to avoid overhangs. We evaluate our framework in various simulation scenarios and further test it in the real world on a quadrupedal robot. Our approach reduces the scene evaluation time by 3 orders of magnitude and improves the path planning speed by 3 times compared with existing approaches, demonstrating highly efficient global navigation in various complex 3D environments. The code is available at: https://github.com/byangw/PCT_planner.","sentences":["Navigation in complex 3D scenarios requires appropriate environment representation for efficient scene understanding and trajectory generation.","We propose a highly efficient and extensible global navigation framework based on a tomographic understanding of the environment to navigate ground robots in multi-layer structures.","Our approach generates tomogram slices using the point cloud map to encode the geometric structure as ground and ceiling elevations.","Then it evaluates the scene traversability considering the robot's motion capabilities.","Both the tomogram construction and the scene evaluation are accelerated through parallel computation.","Our approach further alleviates the trajectory generation complexity compared with planning in 3D spaces directly.","It generates 3D trajectories by searching through multiple tomogram slices and separately adjusts the robot height to avoid overhangs.","We evaluate our framework in various simulation scenarios and further test it in the real world on a quadrupedal robot.","Our approach reduces the scene evaluation time by 3 orders of magnitude and improves the path planning speed by 3 times compared with existing approaches, demonstrating highly efficient global navigation in various complex 3D environments.","The code is available at: https://github.com/byangw/PCT_planner."],"url":"http://arxiv.org/abs/2403.07631v1","category":"cs.RO"}
{"created":"2024-03-12 12:51:15","title":"Memory of a Random Walk: Astrometric deflections from gravitational wave memory accumulation over cosmological scales","abstract":"We study the impact of gravitational wave memory on the distribution of far away light sources in the sky. For the first time we compute the built up of small, but permanent tensor distortions of the metric over cosmological time-scales using realistic models of compact binary coalescences (CBCs) whose rate of occurrence is extrapolated at $z\\sim {\\cal O}(1)$. This allows for a consistent computation of the random-walk like evolution of gravitational wave memory which, in turn, is used to estimate the overall shape and magnitude of astrometric deflections of far away sources of light. We find that for pulsar or quasar proper motions, the near-Earth contribution to the astrometric deflections dominates the result and the deflection is analogous to a stochastic gravitational wave memory background that is generally subdominant to the primary stochastic gravitational wave background. We find that this contribution can be within the reach of future surveys such as Theia. Finally, we also study the deviation of the presently observed angular distribution of quasars from perfect isotropy, which arises from the slow build-up of gravitational wave memory over the entire history of the universe. In this case, we find that astrometric deflections depend on the entire light trajectory from the source to the Earth, yielding a quadruple pattern whose magnitude is unlikely to be within reach of the next generation of astrometric surveys due to shot noise and cosmic variance limitations.","sentences":["We study the impact of gravitational wave memory on the distribution of far away light sources in the sky.","For the first time we compute the built up of small, but permanent tensor distortions of the metric over cosmological time-scales using realistic models of compact binary coalescences (CBCs) whose rate of occurrence is extrapolated at $z\\sim {\\cal O}(1)$. This allows for a consistent computation of the random-walk like evolution of gravitational wave memory which, in turn, is used to estimate the overall shape and magnitude of astrometric deflections of far away sources of light.","We find that for pulsar or quasar proper motions, the near-Earth contribution to the astrometric deflections dominates the result and the deflection is analogous to a stochastic gravitational wave memory background that is generally subdominant to the primary stochastic gravitational wave background.","We find that this contribution can be within the reach of future surveys such as Theia.","Finally, we also study the deviation of the presently observed angular distribution of quasars from perfect isotropy, which arises from the slow build-up of gravitational wave memory over the entire history of the universe.","In this case, we find that astrometric deflections depend on the entire light trajectory from the source to the Earth, yielding a quadruple pattern whose magnitude is unlikely to be within reach of the next generation of astrometric surveys due to shot noise and cosmic variance limitations."],"url":"http://arxiv.org/abs/2403.07614v1","category":"astro-ph.CO"}
{"created":"2024-03-12 12:50:19","title":"Imagine a dragon made of seaweed: How images enhance learning in Wikipedia","abstract":"Though images are ubiquitous across Wikipedia, it is not obvious that the image choices optimally support learning. When well selected, images can enhance learning by dual coding, complementing, or supporting articles. When chosen poorly, images can mislead, distract, and confuse. We developed a large dataset containing 470 questions & answers to 94 Wikipedia articles with images on a wide range of topics. Through an online experiment (n=704), we determined whether the images displayed alongside the text of the article are effective in helping readers understand and learn. For certain tasks, such as learning to identify targets visually (e.g., \"which of these pictures is a gujia?\"), article images significantly improve accuracy. Images did not significantly improve general knowledge questions (e.g., \"where are gujia from?\"). Most interestingly, only some images helped with visual knowledge questions (e.g., \"what shape is a gujia?\"). Using our findings, we reflect on the implications for editors and tools to support image selection.","sentences":["Though images are ubiquitous across Wikipedia, it is not obvious that the image choices optimally support learning.","When well selected, images can enhance learning by dual coding, complementing, or supporting articles.","When chosen poorly, images can mislead, distract, and confuse.","We developed a large dataset containing 470 questions & answers to 94 Wikipedia articles with images on a wide range of topics.","Through an online experiment (n=704), we determined whether the images displayed alongside the text of the article are effective in helping readers understand and learn.","For certain tasks, such as learning to identify targets visually (e.g., \"which of these pictures is a gujia?\"), article images significantly improve accuracy.","Images did not significantly improve general knowledge questions (e.g., \"where are gujia from?\").","Most interestingly, only some images helped with visual knowledge questions (e.g., \"what shape is a gujia?\").","Using our findings, we reflect on the implications for editors and tools to support image selection."],"url":"http://arxiv.org/abs/2403.07613v1","category":"cs.HC"}
{"created":"2024-03-12 12:24:11","title":"Robustifying and Boosting Training-Free Neural Architecture Search","abstract":"Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics. Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric. Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance. To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimization to develop a robust and consistently better-performing metric on diverse tasks, and (b) applies greedy search, i.e., the exploitation, on the newly developed metric to bridge the aforementioned gap and consequently to boost the search performance of standard training-free NAS further. Remarkably, the expected performance of our RoBoT can be theoretically guaranteed, which improves over the existing training-free NAS under mild conditions with additional interesting insights. Our extensive experiments on various NAS benchmark tasks yield substantial empirical evidence to support our theoretical results.","sentences":["Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks.","Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics.","Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric.","Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance.","To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimization to develop a robust and consistently better-performing metric on diverse tasks, and (b) applies greedy search, i.e., the exploitation, on the newly developed metric to bridge the aforementioned gap and consequently to boost the search performance of standard training-free NAS further.","Remarkably, the expected performance of our RoBoT can be theoretically guaranteed, which improves over the existing training-free NAS under mild conditions with additional interesting insights.","Our extensive experiments on various NAS benchmark tasks yield substantial empirical evidence to support our theoretical results."],"url":"http://arxiv.org/abs/2403.07591v1","category":"cs.LG"}
{"created":"2024-03-12 12:19:05","title":"PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution","abstract":"Recently, some large kernel convnets strike back with appealing performance and efficiency. However, given the square complexity of convolution, scaling up kernels can bring about an enormous amount of parameters and the proliferated parameters can induce severe optimization problem. Due to these issues, current CNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e., 51x5 + 5x51) and start to saturate as the kernel size continues growing. In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains. Inspired by human vision, we propose a human-like peripheral convolution that efficiently reduces over 90% parameter count of dense grid convolution through parameter sharing, and manage to scale up kernel size to extremely large. Our peripheral convolution behaves highly similar to human, reducing the complexity of convolution from O(K^2) to O(logK) without backfiring performance. Built on this, we propose Parameter-efficient Large Kernel Network (PeLK). Our PeLK outperforms modern vision Transformers and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet classification, semantic segmentation on ADE20K and object detection on MS COCO. For the first time, we successfully scale up the kernel size of CNNs to an unprecedented 101x101 and demonstrate consistent improvements.","sentences":["Recently, some large kernel convnets strike back with appealing performance and efficiency.","However, given the square complexity of convolution, scaling up kernels can bring about an enormous amount of parameters and the proliferated parameters can induce severe optimization problem.","Due to these issues, current CNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e., 51x5 + 5x51) and start to saturate as the kernel size continues growing.","In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains.","Inspired by human vision, we propose a human-like peripheral convolution that efficiently reduces over 90% parameter count of dense grid convolution through parameter sharing, and manage to scale up kernel size to extremely large.","Our peripheral convolution behaves highly similar to human, reducing the complexity of convolution from O(K^2) to O(logK) without backfiring performance.","Built on this, we propose Parameter-efficient Large Kernel Network (PeLK).","Our PeLK outperforms modern vision Transformers and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet classification, semantic segmentation on ADE20K and object detection on MS COCO.","For the first time, we successfully scale up the kernel size of CNNs to an unprecedented 101x101 and demonstrate consistent improvements."],"url":"http://arxiv.org/abs/2403.07589v1","category":"cs.CV"}
{"created":"2024-03-12 12:02:48","title":"On Weakly Contracting Dynamics for Convex Optimization","abstract":"We investigate the convergence characteristics of dynamics that are \\emph{globally weakly} and \\emph{locally strongly contracting}. Such dynamics naturally arise in the context of convex optimization problems with a unique minimizer. We show that convergence to the equilibrium is \\emph{linear-exponential}, in the sense that the distance between each solution and the equilibrium is upper bounded by a function that first decreases linearly and then exponentially. As we show, the linear-exponential dependency arises naturally in certain dynamics with saturations. Additionally, we provide a sufficient condition for local input-to-state stability. Finally, we illustrate our results on, and propose a conjecture for, continuous-time dynamical systems solving linear programs.","sentences":["We investigate the convergence characteristics of dynamics that are \\emph{globally weakly} and \\emph{locally strongly contracting}.","Such dynamics naturally arise in the context of convex optimization problems with a unique minimizer.","We show that convergence to the equilibrium is \\emph{linear-exponential}, in the sense that the distance between each solution and the equilibrium is upper bounded by a function that first decreases linearly and then exponentially.","As we show, the linear-exponential dependency arises naturally in certain dynamics with saturations.","Additionally, we provide a sufficient condition for local input-to-state stability.","Finally, we illustrate our results on, and propose a conjecture for, continuous-time dynamical systems solving linear programs."],"url":"http://arxiv.org/abs/2403.07572v1","category":"math.OC"}
{"created":"2024-03-12 11:56:22","title":"Theoretical demonstration of mode transmission in ZGP-based micrometer waveguide platforms","abstract":"Birefringence phase-matching based \\c{hi}(2) ZnGeP2 (ZGP) waveguide platform has been recently reported for excellent mid-infrared laser generation. Here, a detailed theoretical characterization of mode transmission taking waveguide anisotropy and substrate material absorption into account in a micrometer ZGP waveguide platform (ZGP-on-SiO2) is conducted. Benefited from high-index contrast between ZGP and substrate (SiO2/Air), Transverse electric and magnetic (TM and TE) mode transmission loss at interested wavelengths range of 2 - 12 {\\mu}m is calculated to be less than 4 dB/cm and 1.5 dB/cm, respectively, in the designed ZGP waveguide. Notably, non-obvious oscillation of mode transmission loss versus phase-matching angles is observed, which is different from that in the previously reported weakly guided anisotropic waveguide. A vital phenomenon named mode crossing at some wavelengths in TM polarization is also exhibited in our waveguide platforms, which jeopardizes waveguide performances and could be avoided by changing the phase-matching angle in practice. This work provides a significant indication of ZGP waveguide design optimization in future and also exhibits extendibility to other birefringent crystal waveguide platforms.","sentences":["Birefringence phase-matching based \\c{hi}(2) ZnGeP2 (ZGP) waveguide platform has been recently reported for excellent mid-infrared laser generation.","Here, a detailed theoretical characterization of mode transmission taking waveguide anisotropy and substrate material absorption into account in a micrometer ZGP waveguide platform (ZGP-on-SiO2) is conducted.","Benefited from high-index contrast between ZGP and substrate (SiO2/Air), Transverse electric and magnetic (TM and TE) mode transmission loss at interested wavelengths range of 2 - 12 {\\mu}m is calculated to be less than 4 dB/cm and 1.5 dB/cm, respectively, in the designed ZGP waveguide.","Notably, non-obvious oscillation of mode transmission loss versus phase-matching angles is observed, which is different from that in the previously reported weakly guided anisotropic waveguide.","A vital phenomenon named mode crossing at some wavelengths in TM polarization is also exhibited in our waveguide platforms, which jeopardizes waveguide performances and could be avoided by changing the phase-matching angle in practice.","This work provides a significant indication of ZGP waveguide design optimization in future and also exhibits extendibility to other birefringent crystal waveguide platforms."],"url":"http://arxiv.org/abs/2403.07568v1","category":"physics.optics"}
{"created":"2024-03-12 11:53:27","title":"Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation","abstract":"Most data-to-text datasets are for English, so the difficulties of modelling data-to-text for low-resource languages are largely unexplored. In this paper we tackle data-to-text for isiXhosa, which is low-resource and agglutinative. We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of WebNLG, which presents a new linguistic context that shifts modelling demands to subword-driven techniques. We also develop an evaluation framework for T2X that measures how accurately generated text describes the data. This enables future users of T2X to go beyond surface-level metrics in evaluation. On the modelling side we explore two classes of methods - dedicated data-to-text models trained from scratch and pretrained language models (PLMs). We propose a new dedicated architecture aimed at agglutinative data-to-text, the Subword Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy entities, and outperforms existing dedicated models for 2 agglutinative languages (isiXhosa and Finnish). We investigate pretrained solutions for T2X, which reveals that standard PLMs come up short. Fine-tuning machine translation models emerges as the best method overall. These findings underscore the distinct challenge presented by T2X: neither well-established data-to-text architectures nor customary pretrained methodologies prove optimal. We conclude with a qualitative analysis of generation errors and an ablation study.","sentences":["Most data-to-text datasets are for English, so the difficulties of modelling data-to-text for low-resource languages are largely unexplored.","In this paper we tackle data-to-text for isiXhosa, which is low-resource and agglutinative.","We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of WebNLG, which presents a new linguistic context that shifts modelling demands to subword-driven techniques.","We also develop an evaluation framework for T2X that measures how accurately generated text describes the data.","This enables future users of T2X to go beyond surface-level metrics in evaluation.","On the modelling side we explore two classes of methods - dedicated data-to-text models trained from scratch and pretrained language models (PLMs).","We propose a new dedicated architecture aimed at agglutinative data-to-text, the Subword Segmental Pointer Generator (SSPG).","It jointly learns to segment words and copy entities, and outperforms existing dedicated models for 2 agglutinative languages (isiXhosa and Finnish).","We investigate pretrained solutions for T2X, which reveals that standard PLMs come up short.","Fine-tuning machine translation models emerges as the best method overall.","These findings underscore the distinct challenge presented by T2X: neither well-established data-to-text architectures nor customary pretrained methodologies prove optimal.","We conclude with a qualitative analysis of generation errors and an ablation study."],"url":"http://arxiv.org/abs/2403.07567v1","category":"cs.CL"}
{"created":"2024-03-12 11:34:33","title":"Consensus under Persistence Excitation","abstract":"We prove that a first-order cooperative system of interacting agents converges to consensus if the so-called Persistence Excitation condition holds. This condition requires that the interaction function between any pair of agents satisfies an integral lower bound. The interpretation is that the interaction needs to ensure a minimal amount of service.","sentences":["We prove that a first-order cooperative system of interacting agents converges to consensus if the so-called Persistence Excitation condition holds.","This condition requires that the interaction function between any pair of agents satisfies an integral lower bound.","The interpretation is that the interaction needs to ensure a minimal amount of service."],"url":"http://arxiv.org/abs/2403.07549v1","category":"math.OC"}
{"created":"2024-03-12 11:05:44","title":"Comments on characterizing demand flexibility to provide power grid services","abstract":"Many loads have flexibility in demand that can be used to provide ancillary services to power grids. A large body of literature exists on designing algorithms to coordinate actions of many loads to provide such a service. The topic of characterizing the flexibility of one or a collection of loads - to determine what kinds of demand deviation from the baseline is feasible - has also been studied. However, there is a large diversity in definitions of flexibility and methods proposed to characterize flexibility. As a result there are several gaps in the literature on flexibility characterization. Some approaches on flexibility characterization are based on ad-hoc approximations that lead to highly conservative estimates. In this paper we point out some of these issues and their implications, with the hope to encourage additional research to address them.","sentences":["Many loads have flexibility in demand that can be used to provide ancillary services to power grids.","A large body of literature exists on designing algorithms to coordinate actions of many loads to provide such a service.","The topic of characterizing the flexibility of one or a collection of loads - to determine what kinds of demand deviation from the baseline is feasible - has also been studied.","However, there is a large diversity in definitions of flexibility and methods proposed to characterize flexibility.","As a result there are several gaps in the literature on flexibility characterization.","Some approaches on flexibility characterization are based on ad-hoc approximations that lead to highly conservative estimates.","In this paper we point out some of these issues and their implications, with the hope to encourage additional research to address them."],"url":"http://arxiv.org/abs/2403.07529v1","category":"math.OC"}
{"created":"2024-03-12 11:05:07","title":"Evaluation and thermodynamic optimization of phase diagram of lithium niobate tantalate solid solutions","abstract":"The phase diagram of the lithium niobate and lithium tantalate solid solutions was investigated using experimental data from differential thermal analysis (DTA) and crystal growth. We used XRF analysis to determine the elemental composition of crystals. Based on the Neumann-Kopp rule, essential data of end members lithium niobate (LN) and lithium tantalate (LT) was created. The heats of fusion of end members given by DTA measurements of LN (103 kJ/mol at 1531 K) and LT (289 kJ/mol at 1913 K) were given as input parameters to generate the data. This data served as the basis for calculating a phase diagram for LN and LT solid solutions. Finally, based on the experimental data and thermodynamic solution model, the phase diagram was optimized in the Calphad Factsage module. We also generated thermodynamic parameters for Gibb's excess energy of the solid solution. A plot of segregation coefficient as a function of Ta concentration was derived from the phase diagram.","sentences":["The phase diagram of the lithium niobate and lithium tantalate solid solutions was investigated using experimental data from differential thermal analysis (DTA) and crystal growth.","We used XRF analysis to determine the elemental composition of crystals.","Based on the Neumann-Kopp rule, essential data of end members lithium niobate (LN) and lithium tantalate (LT) was created.","The heats of fusion of end members given by DTA measurements of LN (103 kJ/mol at 1531 K) and LT (289 kJ/mol at 1913 K) were given as input parameters to generate the data.","This data served as the basis for calculating a phase diagram for LN and LT solid solutions.","Finally, based on the experimental data and thermodynamic solution model, the phase diagram was optimized in the Calphad Factsage module.","We also generated thermodynamic parameters for Gibb's excess energy of the solid solution.","A plot of segregation coefficient as a function of Ta concentration was derived from the phase diagram."],"url":"http://arxiv.org/abs/2403.07527v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-12 11:03:25","title":"Universal Chemical Formula Dependence of $Ab$ $Initio$ Low-Energy Effective Hamiltonian in Single-Layer Carrier Doped Cuprate Superconductors -- Study by Hierarchical Dependence Extraction Algorithm","abstract":"We explore the possibility to control the superconducting (SC) transition temperature at optimal hole doping $T_{c}^{\\rm opt}$ in cuprates by tuning the chemical formula (CF). $T_{c}^{\\rm opt}$ can be theoretically predicted from the parameters of the \\textit{ab initio} low-energy effective Hamiltonian (LEH) with one antibonding (AB) Cu$3d_{x^2-y^2}$/O$2p_{\\sigma}$ orbital per Cu atom in the CuO$_2$ plane, notably the nearest neighbor hopping amplitude $|t_1|$ and the ratio $u=U/|t_1|$, where $U$ is the onsite effective Coulomb repulsion. However, the CF dependence of $|t_1|$ and $u$ is a highly nontrivial question. In this paper, we propose the universal dependence of $|t_1|$ and $u$ on the CF and structural features in hole doped cuprates with a single CuO$_2$ layer sandwiched between block layers. To do so, we perform extensive \\textit{ab initio} calculations of $|t_1|$ and $u$ and analyze the results by employing a machine learning method called Hierarchical Dependence Extraction (HDE). The main results are the following: (a) $|t_1|$ has a main-order dependence on the radii $R_{\\rm X}$ and $R_{\\rm A}$ of the apical anion X and cation A in the block layer. ($|t_1|$ increases when $R_{\\rm X}$ or $R_{\\rm A}$ decreases.) (b) $u$ has a main-order dependence on the negative ionic charge $Z_{\\rm X}$ of X and the hole doping $\\delta$ of the AB orbital. ($u$ decreases when $|Z_{\\rm X}|$ increases or $\\delta$ increases.) We elucidate and discuss the microscopic mechanism of (a,b). We demonstrate the predictive power of the HDE by showing the consistency between (a,b) and results from previous works. The present results provide a basis for optimizing SC properties in cuprates and possibly akin materials. Also, the HDE method offers a general platform to identify dependencies between physical quantities.","sentences":["We explore the possibility to control the superconducting (SC) transition temperature at optimal hole doping $T_{c}^{\\rm opt}$ in cuprates by tuning the chemical formula (CF).","$T_{c}^{\\rm opt}$ can be theoretically predicted from the parameters of the \\textit{ab initio} low-energy effective Hamiltonian (LEH) with one antibonding (AB) Cu$3d_{x^2-y^2}$/O$2p_{\\sigma}$ orbital per Cu atom in the CuO$_2$ plane, notably the nearest neighbor hopping amplitude $|t_1|$ and the ratio $u=U/|t_1|$, where $U$ is the onsite effective Coulomb repulsion.","However, the CF dependence of $|t_1|$ and $u$ is a highly nontrivial question.","In this paper, we propose the universal dependence of $|t_1|$ and $u$ on the CF and structural features in hole doped cuprates with a single CuO$_2$ layer sandwiched between block layers.","To do so, we perform extensive \\textit{ab initio} calculations of $|t_1|$ and $u$ and analyze the results by employing a machine learning method called Hierarchical Dependence Extraction (HDE).","The main results are the following: (a) $|t_1|$ has a main-order dependence on the radii $R_{\\rm X}$ and $R_{\\rm A}$ of the apical anion X and cation A in the block layer.","($|t_1|$ increases when $R_{\\rm X}$ or $R_{\\rm A}$ decreases.)","(b) $u$ has a main-order dependence on the negative ionic charge $Z_{\\rm X}$ of X and the hole doping $\\delta$ of the AB orbital.","($u$ decreases when $|Z_{\\rm X}|$ increases or $\\delta$ increases.)","We elucidate and discuss the microscopic mechanism of (a,b).","We demonstrate the predictive power of the HDE by showing the consistency between (a,b) and results from previous works.","The present results provide a basis for optimizing SC properties in cuprates and possibly akin materials.","Also, the HDE method offers a general platform to identify dependencies between physical quantities."],"url":"http://arxiv.org/abs/2403.07525v1","category":"cond-mat.supr-con"}
