{"created":"2024-02-13 18:59:51","title":"IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation","abstract":"Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images. They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets.","sentences":["Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images.","They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts.","A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly.","In this paper, we further explore the design space of text-to-3D models.","We significantly improve multi-view generation by considering video instead of image generators.","Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views.","Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets."],"url":"http://arxiv.org/abs/2402.08682v1","category":"cs.CV"}
{"created":"2024-02-13 18:59:05","title":"Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance","abstract":"The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Through comprehensive evaluations across $6$ popular LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it not only reduces hallucinations but also improves the detailedness of LVLMs' generations, as assessed by GPT-4V.","sentences":["The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images.","To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs.","However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation.","In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process.","Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations.","Through comprehensive evaluations across $6$ popular LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods.","Remarkably, it not only reduces hallucinations but also improves the detailedness of LVLMs' generations, as assessed by GPT-4V."],"url":"http://arxiv.org/abs/2402.08680v1","category":"cs.LG"}
{"created":"2024-02-13 18:58:48","title":"COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability","abstract":"Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent suffix attacks, but also allow us to address new controllable attack settings such as revising a user query adversarially with minimal paraphrasing, and inserting stealthy attacks in context with left-right-coherence. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at https://github.com/Yu-Fangxu/COLD-Attack.","sentences":["Jailbreaks on Large language models (LLMs) have recently received increasing attention.","For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks.","In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing.","Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence.","The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent suffix attacks, but also allow us to address new controllable attack settings such as revising a user query adversarially with minimal paraphrasing, and inserting stealthy attacks in context with left-right-coherence.","Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability.","Our code is available at https://github.com/Yu-Fangxu/COLD-Attack."],"url":"http://arxiv.org/abs/2402.08679v1","category":"cs.LG"}
{"created":"2024-02-13 18:58:17","title":"Graph Mamba: Towards Learning on Graphs with State Space Models","abstract":"Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). In this paper, we show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general framework for a new class of GNNs based on selective SSMs. We discuss and categorize the new challenges when adopting SSMs to graph-structured data, and present four required and one optional steps to design GMNs, where we choose (1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of Bidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE and SE. We further provide theoretical justification for the power of GMNs. Experiments demonstrate that despite much less computational cost, GMNs attain an outstanding performance in long-range, small-scale, large-scale, and heterophilic benchmark datasets.","sentences":["Graph Neural Networks (GNNs) have shown promising potential in graph representation learning.","The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers.","These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies.","Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs).","GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE).","In this paper, we show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary.","Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general framework for a new class of GNNs based on selective SSMs.","We discuss and categorize the new challenges when adopting SSMs to graph-structured data, and present four required and one optional steps to design GMNs, where we choose (1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of Bidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE and SE.","We further provide theoretical justification for the power of GMNs.","Experiments demonstrate that despite much less computational cost, GMNs attain an outstanding performance in long-range, small-scale, large-scale, and heterophilic benchmark datasets."],"url":"http://arxiv.org/abs/2402.08678v1","category":"cs.LG"}
{"created":"2024-02-13 18:54:08","title":"Model Assessment and Selection under Temporal Distribution Shift","abstract":"We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.","sentences":["We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs.","To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model.","This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors.","We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates.","Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data."],"url":"http://arxiv.org/abs/2402.08672v1","category":"cs.LG"}
{"created":"2024-02-13 18:53:13","title":"Are Semi-Dense Detector-Free Methods Good at Matching Local Features?","abstract":"Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image matching methods. While SDF methods are trained to establish correspondences between two images, their performances are almost exclusively evaluated using relative pose estimation metrics. Thus, the link between their ability to establish correspondences and the quality of the resulting estimated pose has thus far received little attention. This paper is a first attempt to study this link. We start with proposing a novel structured attention-based image matching architecture (SAM). It allows us to show a counter-intuitive result on two datasets (MegaDepth and HPatches): on the one hand SAM either outperforms or is on par with SDF methods in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly better than SAM in terms of matching accuracy. We then propose to limit the computation of the matching accuracy to textured regions, and show that in this case SAM often surpasses SDF methods. Our findings highlight a strong correlation between the ability to establish accurate correspondences in textured regions and the accuracy of the resulting estimated pose/homography. Our code will be made available.","sentences":["Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image matching methods.","While SDF methods are trained to establish correspondences between two images, their performances are almost exclusively evaluated using relative pose estimation metrics.","Thus, the link between their ability to establish correspondences and the quality of the resulting estimated pose has thus far received little attention.","This paper is a first attempt to study this link.","We start with proposing a novel structured attention-based image matching architecture (SAM).","It allows us to show a counter-intuitive result on two datasets (MegaDepth and HPatches): on the one hand SAM either outperforms or is on par with SDF methods in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly better than SAM in terms of matching accuracy.","We then propose to limit the computation of the matching accuracy to textured regions, and show that in this case SAM often surpasses SDF methods.","Our findings highlight a strong correlation between the ability to establish accurate correspondences in textured regions and the accuracy of the resulting estimated pose/homography.","Our code will be made available."],"url":"http://arxiv.org/abs/2402.08671v1","category":"cs.CV"}
{"created":"2024-02-13 18:51:18","title":"Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models","abstract":"The development of large vision-language models (LVLMs) offers the potential to address challenges faced by traditional multimodal recommendations thanks to their proficient understanding of static images and textual dynamics. However, the application of LVLMs in this field is still limited due to the following complexities: First, LVLMs lack user preference knowledge as they are trained from vast general datasets. Second, LVLMs suffer setbacks in addressing multiple image dynamics in scenarios involving discrete, noisy, and redundant image sequences. To overcome these issues, we propose the novel reasoning scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large vision-language models for multimodal recommendation. We utilize user history as in-context user preferences to address the first challenge. Next, we prompt LVLMs to generate item image summaries and utilize image comprehension in natural language space combined with item titles to query the user preferences over candidate items. We conduct comprehensive experiments across four datasets with three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b. The numerical results indicate the efficacy of VST.","sentences":["The development of large vision-language models (LVLMs) offers the potential to address challenges faced by traditional multimodal recommendations thanks to their proficient understanding of static images and textual dynamics.","However, the application of LVLMs in this field is still limited due to the following complexities:","First, LVLMs lack user preference knowledge as they are trained from vast general datasets.","Second, LVLMs suffer setbacks in addressing multiple image dynamics in scenarios involving discrete, noisy, and redundant image sequences.","To overcome these issues, we propose the novel reasoning scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large vision-language models for multimodal recommendation.","We utilize user history as in-context user preferences to address the first challenge.","Next, we prompt LVLMs to generate item image summaries and utilize image comprehension in natural language space combined with item titles to query the user preferences over candidate items.","We conduct comprehensive experiments across four datasets with three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b.","The numerical results indicate the efficacy of VST."],"url":"http://arxiv.org/abs/2402.08670v1","category":"cs.AI"}
{"created":"2024-02-13 18:50:40","title":"Soliton gas of the integrable Boussinesq equation and its generalised hydrodynamics","abstract":"Generalised hydrodynamics (GHD) is a recent and powerful framework to study many-body integrable systems, quantum or classical, out of equilibrium. It has been applied to several models, from the delta Bose gas to the XXZ spin chain, the KdV soliton gas and many more. Yet it has only been applied to (1+1)-dimensional systems and generalisation to higher dimensions of space is non-trivial. We study the Boussinesq equation which, while generally considered to be less physically relevant than the KdV equation, is interesting as a stationary reduction of the (boosted) Kadomtsev-Petviashvili (KP) equation, a prototypical and universal example of a nonlinear integrable PDE in (2+1) dimensions. We follow a heuristic approach inspired by the Thermodynamic Bethe Ansatz in order to construct the GHD of the Boussinesq soliton gas. Such approach allows for a statistical mechanics interpretation of the Boussinesq soliton gas that comes naturally with the GHD picture. This is to be seen as a first step in the construction of the KP soliton gas, yielding insight on some classes of solutions from which we may be able to build an intuition on how to devise a more general theory. This also offers another perspective on the construction of anisotropic bidirectional soliton gases previously introduced phenomenologically by Congy et al (2021).","sentences":["Generalised hydrodynamics (GHD) is a recent and powerful framework to study many-body integrable systems, quantum or classical, out of equilibrium.","It has been applied to several models, from the delta Bose gas to the XXZ spin chain, the KdV soliton gas and many more.","Yet it has only been applied to (1+1)-dimensional systems and generalisation to higher dimensions of space is non-trivial.","We study the Boussinesq equation which, while generally considered to be less physically relevant than the KdV equation, is interesting as a stationary reduction of the (boosted) Kadomtsev-Petviashvili (KP) equation, a prototypical and universal example of a nonlinear integrable PDE in (2+1) dimensions.","We follow a heuristic approach inspired by the Thermodynamic Bethe Ansatz in order to construct the GHD of the Boussinesq soliton gas.","Such approach allows for a statistical mechanics interpretation of the Boussinesq soliton gas that comes naturally with the GHD picture.","This is to be seen as a first step in the construction of the KP soliton gas, yielding insight on some classes of solutions from which we may be able to build an intuition on how to devise a more general theory.","This also offers another perspective on the construction of anisotropic bidirectional soliton gases previously introduced phenomenologically by Congy et al (2021)."],"url":"http://arxiv.org/abs/2402.08669v1","category":"nlin.PS"}
{"created":"2024-02-13 18:48:23","title":"Improving Generalization in Semantic Parsing by Increasing Natural Language Variation","abstract":"Text-to-SQL semantic parsing has made significant progress in recent years, with various models demonstrating impressive performance on the challenging Spider benchmark. However, it has also been shown that these models often struggle to generalize even when faced with small perturbations of previously (accurately) parsed expressions. This is mainly due to the linguistic form of questions in Spider which are overly specific, unnatural, and display limited variation. In this work, we use data augmentation to enhance the robustness of text-to-SQL parsers against natural language variations. Existing approaches generate question reformulations either via models trained on Spider or only introduce local changes. In contrast, we leverage the capabilities of large language models to generate more realistic and diverse questions. Using only a few prompts, we achieve a two-fold increase in the number of questions in Spider. Training on this augmented dataset yields substantial improvements on a range of evaluation sets, including robustness benchmarks and out-of-domain data.","sentences":["Text-to-SQL semantic parsing has made significant progress in recent years, with various models demonstrating impressive performance on the challenging Spider benchmark.","However, it has also been shown that these models often struggle to generalize even when faced with small perturbations of previously (accurately) parsed expressions.","This is mainly due to the linguistic form of questions in Spider which are overly specific, unnatural, and display limited variation.","In this work, we use data augmentation to enhance the robustness of text-to-SQL parsers against natural language variations.","Existing approaches generate question reformulations either via models trained on Spider or only introduce local changes.","In contrast, we leverage the capabilities of large language models to generate more realistic and diverse questions.","Using only a few prompts, we achieve a two-fold increase in the number of questions in Spider.","Training on this augmented dataset yields substantial improvements on a range of evaluation sets, including robustness benchmarks and out-of-domain data."],"url":"http://arxiv.org/abs/2402.08666v1","category":"cs.CL"}
{"created":"2024-02-13 18:47:43","title":"Crystallization of C*-algebras","abstract":"Given a C$^*$-algebra $A$ with an almost periodic time evolution $\\sigma$, we define a new C$^*$-algebra $A_c$, which we call the crystal of $(A,\\sigma)$, that represents the zero temperature limit of $(A, \\sigma)$. We prove that there is a one-to-one correspondence between the ground states of $(A,\\sigma)$ and the states on $A_c$, justifying the name. In order to investigate further the relation between low temperature equilibrium states on $A$ and traces on $A_c$, we define a Fock module $\\mathcal F$ over the crystal and construct a vacuum representation of $A$ on $\\mathcal F$. This allows us to show, under relatively mild assumptions, that for sufficiently large inverse temperatures $\\beta$ the $\\sigma$-KMS$_\\beta$-states on $A$ are induced from traces on $A_c$ by means of the Fock module. In the second part, we compare the K-theoretic structures of $A$ and $A_c$. Previous work by various authors suggest that they have (rationally) isomorphic K-groups. We analyze this phenomenon in detail, confirming it under favorable conditions, but showing that, in general, there is apparently no easy way to relate these groups. As examples, we discuss in particular Exel's results on semi-saturated circle actions, and recent results of Miller on the K-theory of inverse semigroup C$^*$-algebras. In relation to the latter, we introduce the notion of a scale $N$ on an inverse semigroup $I$ and define a new inverse semigroup $I_c$, which we call the crystal of $(I,N)$.","sentences":["Given a C$^*$-algebra $A$ with an almost periodic time evolution $\\sigma$, we define a new C$^*$-algebra $A_c$, which we call the crystal of $(A,\\sigma)$, that represents the zero temperature limit of $(A, \\sigma)$. We prove that there is a one-to-one correspondence between the ground states of $(A,\\sigma)$ and the states on $A_c$, justifying the name.","In order to investigate further the relation between low temperature equilibrium states on $A$ and traces on $A_c$, we define a Fock module $\\mathcal F$ over the crystal and construct a vacuum representation of $A$ on $\\mathcal F$.","This allows us to show, under relatively mild assumptions, that for sufficiently large inverse temperatures $\\beta$ the $\\sigma$-KMS$_\\beta$-states on $A$ are induced from traces on $A_c$ by means of the Fock module.","In the second part, we compare the K-theoretic structures of $A$ and $A_c$. Previous work by various authors suggest that they have (rationally) isomorphic K-groups.","We analyze this phenomenon in detail, confirming it under favorable conditions, but showing that, in general, there is apparently no easy way to relate these groups.","As examples, we discuss in particular Exel's results on semi-saturated circle actions, and recent results of Miller on the K-theory of inverse semigroup C$^*$-algebras.","In relation to the latter, we introduce the notion of a scale $N$ on an inverse semigroup $I$ and define a new inverse semigroup $I_c$, which we call the crystal of $(I,N)$."],"url":"http://arxiv.org/abs/2402.08665v1","category":"math.OA"}
{"created":"2024-02-13 18:42:28","title":"Filtered derived categories of curved deformations","abstract":"We propose a solution to the ''curvature problem'' from arXiv:1505.03698 and arXiv:0905.3845 for infinitesimal deformations. Let $k$ be a field, $A$ a dg algebra over $k$ and $A_n = A[t]/(t^{n+1})$ a cdg algebra over $R_n = k[t]/(t^{n+1})$, $n \\geq 0$, with reduction $A_n/tA_n = A$. We define the $n$-derived category $D^n(A_n)$ as the quotient of the homotopy category by the modules for which all quotients appearing in the associated graded object are acyclic. We prove this to be a compactly generated triangulated category with a semiorthogonal decomposition by $n + 1$ copies of $D(A)$, in which Positselski's semiderived category embeds admissibly.","sentences":["We propose a solution to the ''curvature problem'' from arXiv:1505.03698 and arXiv:0905.3845 for infinitesimal deformations.","Let $k$ be a field, $A$ a dg algebra over $k$ and $A_n = A[t]/(t^{n+1})$ a cdg algebra over $R_n = k[t]/(t^{n+1})$, $n \\geq 0$, with reduction $A_n/tA_n = A$.","We define the $n$-derived category $D^n(A_n)$ as the quotient of the homotopy category by the modules for which all quotients appearing in the associated graded object are acyclic.","We prove this to be a compactly generated triangulated category with a semiorthogonal decomposition by $n + 1$ copies of $D(A)$, in which Positselski's semiderived category embeds admissibly."],"url":"http://arxiv.org/abs/2402.08660v1","category":"math.KT"}
{"created":"2024-02-13 18:40:15","title":"A demonstration of the effect of fringe-rate filtering in the Hydrogen Epoch of Reionization Array delay power spectrum pipeline","abstract":"Radio interferometers targeting the 21cm brightness temperature fluctuations at high redshift are subject to systematic effects that operate over a range of different timescales. These can be isolated by designing appropriate Fourier filters that operate in fringe-rate (FR) space, the Fourier pair of local sidereal time (LST). Applications of FR filtering include separating effects that are correlated with the rotating sky vs. those relative to the ground, down-weighting emission in the primary beam sidelobes, and suppressing noise. FR filtering causes the noise contributions to the visibility data to become correlated in time however, making interpretation of subsequent averaging and error estimation steps more subtle. In this paper, we describe fringe rate filters that are implemented using discrete prolate spheroidal sequences, and designed for two different purposes -- beam sidelobe/horizon suppression (the `mainlobe' filter), and ground-locked systematics removal (the `notch' filter). We apply these to simulated data, and study how their properties affect visibilities and power spectra generated from the simulations. Included is an introduction to fringe-rate filtering and a demonstration of fringe-rate filters applied to simple situations to aid understanding.","sentences":["Radio interferometers targeting the 21cm brightness temperature fluctuations at high redshift are subject to systematic effects that operate over a range of different timescales.","These can be isolated by designing appropriate Fourier filters that operate in fringe-rate (FR) space, the Fourier pair of local sidereal time (LST).","Applications of FR filtering include separating effects that are correlated with the rotating sky vs. those relative to the ground, down-weighting emission in the primary beam sidelobes, and suppressing noise.","FR filtering causes the noise contributions to the visibility data to become correlated in time however, making interpretation of subsequent averaging and error estimation steps more subtle.","In this paper, we describe fringe rate filters that are implemented using discrete prolate spheroidal sequences, and designed for two different purposes -- beam sidelobe/horizon suppression (the `mainlobe' filter), and ground-locked systematics removal (the `notch' filter).","We apply these to simulated data, and study how their properties affect visibilities and power spectra generated from the simulations.","Included is an introduction to fringe-rate filtering and a demonstration of fringe-rate filters applied to simple situations to aid understanding."],"url":"http://arxiv.org/abs/2402.08659v1","category":"astro-ph.CO"}
{"created":"2024-02-13 18:39:36","title":"The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting","abstract":"We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health. JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual's current context and needs. However, traditional rule-based and machine learning models for JITAI implementation face scalability and reliability limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity. To investigate JITAI implementation via LLMs, we tested the contemporary overall performance-leading model 'GPT-4' with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation. Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs. Subsequently, we generated a total of 450 proposed JITAI decisions and message content, divided equally into JITAIs generated by 10 iterations with GPT-4, a baseline provided by 10 laypersons (LayPs), and a gold standard set by 10 healthcare professionals (HCPs). Ratings from 27 LayPs indicated that JITAIs generated by GPT-4 were superior to those by HCPs and LayPs over all assessed scales: i.e., appropriateness, engagement, effectiveness, and professionality. This study indicates that LLMs have significant potential for implementing JITAIs as a building block of personalized or \"precision\" health, offering scalability, effective personalization based on opportunistically sampled information, and good acceptability.","sentences":["We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health.","JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual's current context and needs.","However, traditional rule-based and machine learning models for JITAI implementation face scalability and reliability limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity.","To investigate JITAI implementation via LLMs, we tested the contemporary overall performance-leading model 'GPT-4' with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation.","Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs.","Subsequently, we generated a total of 450 proposed JITAI decisions and message content, divided equally into JITAIs generated by 10 iterations with GPT-4, a baseline provided by 10 laypersons (LayPs), and a gold standard set by 10 healthcare professionals (HCPs).","Ratings from 27 LayPs indicated that JITAIs generated by GPT-4 were superior to those by HCPs and LayPs over all assessed scales: i.e., appropriateness, engagement, effectiveness, and professionality.","This study indicates that LLMs have significant potential for implementing JITAIs as a building block of personalized or \"precision\" health, offering scalability, effective personalization based on opportunistically sampled information, and good acceptability."],"url":"http://arxiv.org/abs/2402.08658v1","category":"cs.HC"}
{"created":"2024-02-13 18:38:18","title":"NeuroBench: An Open-Source Benchmark Framework for the Standardization of Methodology in Brainwave-based Authentication Research","abstract":"Biometric systems based on brain activity have been proposed as an alternative to passwords or to complement current authentication techniques. By leveraging the unique brainwave patterns of individuals, these systems offer the possibility of creating authentication solutions that are resistant to theft, hands-free, accessible, and potentially even revocable. However, despite the growing stream of research in this area, faster advance is hindered by reproducibility problems. Issues such as the lack of standard reporting schemes for performance results and system configuration, or the absence of common evaluation benchmarks, make comparability and proper assessment of different biometric solutions challenging. Further, barriers are erected to future work when, as so often, source code is not published open access. To bridge this gap, we introduce NeuroBench, a flexible open source tool to benchmark brainwave-based authentication models. It incorporates nine diverse datasets, implements a comprehensive set of pre-processing parameters and machine learning algorithms, enables testing under two common adversary models (known vs unknown attacker), and allows researchers to generate full performance reports and visualizations. We use NeuroBench to investigate the shallow classifiers and deep learning-based approaches proposed in the literature, and to test robustness across multiple sessions. We observe a 37.6\\% reduction in Equal Error Rate (EER) for unknown attacker scenarios (typically not tested in the literature), and we highlight the importance of session variability to brainwave authentication. All in all, our results demonstrate the viability and relevance of NeuroBench in streamlining fair comparisons of algorithms, thereby furthering the advancement of brainwave-based authentication through robust methodological practices.","sentences":["Biometric systems based on brain activity have been proposed as an alternative to passwords or to complement current authentication techniques.","By leveraging the unique brainwave patterns of individuals, these systems offer the possibility of creating authentication solutions that are resistant to theft, hands-free, accessible, and potentially even revocable.","However, despite the growing stream of research in this area, faster advance is hindered by reproducibility problems.","Issues such as the lack of standard reporting schemes for performance results and system configuration, or the absence of common evaluation benchmarks, make comparability and proper assessment of different biometric solutions challenging.","Further, barriers are erected to future work when, as so often, source code is not published open access.","To bridge this gap, we introduce NeuroBench, a flexible open source tool to benchmark brainwave-based authentication models.","It incorporates nine diverse datasets, implements a comprehensive set of pre-processing parameters and machine learning algorithms, enables testing under two common adversary models (known vs unknown attacker), and allows researchers to generate full performance reports and visualizations.","We use NeuroBench to investigate the shallow classifiers and deep learning-based approaches proposed in the literature, and to test robustness across multiple sessions.","We observe a 37.6\\% reduction in Equal Error Rate (EER) for unknown attacker scenarios (typically not tested in the literature), and we highlight the importance of session variability to brainwave authentication.","All in all, our results demonstrate the viability and relevance of NeuroBench in streamlining fair comparisons of algorithms, thereby furthering the advancement of brainwave-based authentication through robust methodological practices."],"url":"http://arxiv.org/abs/2402.08656v1","category":"cs.CR"}
{"created":"2024-02-13 18:34:10","title":"Learning Continuous 3D Words for Text-to-Image Generation","abstract":"Current controls over diffusion models (e.g., through text or ControlNet) for image generation fall short in recognizing abstract, continuous attributes like illumination direction or non-rigid shape change. In this paper, we present an approach for allowing users of text-to-image models to have fine-grained control of several attributes in an image. We do this by engineering special sets of input tokens that can be transformed in a continuous manner -- we call them Continuous 3D Words. These attributes can, for example, be represented as sliders and applied jointly with text prompts for fine-grained control over image generation. Given only a single mesh and a rendering engine, we show that our approach can be adopted to provide continuous user control over several 3D-aware attributes, including time-of-day illumination, bird wing orientation, dollyzoom effect, and object poses. Our method is capable of conditioning image creation with multiple Continuous 3D Words and text descriptions simultaneously while adding no overhead to the generative process. Project Page: https://ttchengab.github.io/continuous_3d_words","sentences":["Current controls over diffusion models (e.g., through text or ControlNet) for image generation fall short in recognizing abstract, continuous attributes like illumination direction or non-rigid shape change.","In this paper, we present an approach for allowing users of text-to-image models to have fine-grained control of several attributes in an image.","We do this by engineering special sets of input tokens that can be transformed in a continuous manner -- we call them Continuous 3D Words.","These attributes can, for example, be represented as sliders and applied jointly with text prompts for fine-grained control over image generation.","Given only a single mesh and a rendering engine, we show that our approach can be adopted to provide continuous user control over several 3D-aware attributes, including time-of-day illumination, bird wing orientation, dollyzoom effect, and object poses.","Our method is capable of conditioning image creation with multiple Continuous 3D Words and text descriptions simultaneously while adding no overhead to the generative process.","Project Page: https://ttchengab.github.io/continuous_3d_words"],"url":"http://arxiv.org/abs/2402.08654v1","category":"cs.CV"}
{"created":"2024-02-13 18:33:45","title":"SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds","abstract":"Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature perturbations, offering a scalable approach for evaluating the stability of GNNs, extending to applications within recommendation systems. Furthermore, we illustrate its utility in downstream tasks, notably in enhancing GNN stability and facilitating adversarial targeted attacks.","sentences":["Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance.","In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs.","This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability.","We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis.","Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature perturbations, offering a scalable approach for evaluating the stability of GNNs, extending to applications within recommendation systems.","Furthermore, we illustrate its utility in downstream tasks, notably in enhancing GNN stability and facilitating adversarial targeted attacks."],"url":"http://arxiv.org/abs/2402.08653v1","category":"cs.LG"}
{"created":"2024-02-13 18:31:12","title":"Dark matter as the trigger of flavor changing neutral current decays of the top quark","abstract":"We suggest a simplified model that simultaneously addresses the dark-matter problem and give rise to top quark flavor changing neutral current (FCNC) interactions at the one-loop order. The model consists of two extra $SU(2)_L$ gauge singlets: a colored mediator of spin zero ($S$) and a right-handed fermion ($\\chi$) both are odd under an ad-hoc $Z_2$ symmetry. The right-handed fermion plays the role of the dark-matter candidate. In this model, the presence of the two dark sector particles generates one-loop induced FCNC decays of the top quark into light quarks and bosons such as the gluon, the photon, the $Z$-boson or the Higgs boson. As a case study, we analyze the top quark FCNC decays into light quarks ($u$ or $c$) and a $Z$ or Higgs bosons. We then study the reliable solutions to the dark-matter problem by estimating the regions in the parameter space that are consistent with the \\textsc{Planck} measurement of the dark-matter relic density. We also revisit the bounds from the searches of dark matter in events with at least one high-$p_T$ jet and large missing transverse energy at the Large Hadron Collider (LHC). We then define four benchmark points that are consistent with the existing constraints from collider experiments and cosmology. We finally estimate, for these benchmark scenarios, the rates of a broad range of channels that can be used to probe the connection between the top FCNC transitions and dark matter both at the HL-LHC and a future $100$ TeV collider.","sentences":["We suggest a simplified model that simultaneously addresses the dark-matter problem and give rise to top quark flavor changing neutral current (FCNC) interactions at the one-loop order.","The model consists of two extra $SU(2)_L$ gauge singlets: a colored mediator of spin zero ($S$) and a right-handed fermion ($\\chi$) both are odd under an ad-hoc $Z_2$ symmetry.","The right-handed fermion plays the role of the dark-matter candidate.","In this model, the presence of the two dark sector particles generates one-loop induced FCNC decays of the top quark into light quarks and bosons such as the gluon, the photon, the $Z$-boson or the Higgs boson.","As a case study, we analyze the top quark FCNC decays into light quarks ($u$ or $c$) and a $Z$ or Higgs bosons.","We then study the reliable solutions to the dark-matter problem by estimating the regions in the parameter space that are consistent with the \\textsc{Planck} measurement of the dark-matter relic density.","We also revisit the bounds from the searches of dark matter in events with at least one high-$p_T$ jet and large missing transverse energy at the Large Hadron Collider (LHC).","We then define four benchmark points that are consistent with the existing constraints from collider experiments and cosmology.","We finally estimate, for these benchmark scenarios, the rates of a broad range of channels that can be used to probe the connection between the top FCNC transitions and dark matter both at the HL-LHC and a future $100$ TeV collider."],"url":"http://arxiv.org/abs/2402.08652v1","category":"hep-ph"}
{"created":"2024-02-13 18:28:36","title":"Sharing Spectrum and Services in the 7-24 GHz Upper Midband","abstract":"The upper midband, spanning 7 to 24 GHz, strikes a good balance between large bandwidths and favorable propagation environments for future 6th Generation (6G) networks. Wireless networks in the upper midband, however, will need to share the spectrum and safely coexist with a variety of incumbents, ranging from radiolocation to fixed satellite services, as well as Earth exploration and sensing. In this paper, we take the first step toward understanding the potential and challenges associated with cellular systems between 7 and 24 GHz. Our focus is on the enabling technologies and policies for coexistence with established incumbents. We consider dynamic spectrum sharing solutions enabled by programmable and adaptive cellular networks, but also the possibility of leveraging the cellular infrastructure for incumbent services. Our comprehensive analysis employs ray tracing and examines real-world urban scenarios to evaluate throughput, coverage tradeoffs, and the potential impact on incumbent services. Our findings highlight the advantages of FR-3 over FR-2 and FR-1 in terms of coverage and bandwidth, respectively. We conclude by discussing a network architecture based on Open RAN, aimed at enabling dynamic spectrum and service sharing.","sentences":["The upper midband, spanning 7 to 24 GHz, strikes a good balance between large bandwidths and favorable propagation environments for future 6th Generation (6G) networks.","Wireless networks in the upper midband, however, will need to share the spectrum and safely coexist with a variety of incumbents, ranging from radiolocation to fixed satellite services, as well as Earth exploration and sensing.","In this paper, we take the first step toward understanding the potential and challenges associated with cellular systems between 7 and 24 GHz.","Our focus is on the enabling technologies and policies for coexistence with established incumbents.","We consider dynamic spectrum sharing solutions enabled by programmable and adaptive cellular networks, but also the possibility of leveraging the cellular infrastructure for incumbent services.","Our comprehensive analysis employs ray tracing and examines real-world urban scenarios to evaluate throughput, coverage tradeoffs, and the potential impact on incumbent services.","Our findings highlight the advantages of FR-3 over FR-2 and FR-1 in terms of coverage and bandwidth, respectively.","We conclude by discussing a network architecture based on Open RAN, aimed at enabling dynamic spectrum and service sharing."],"url":"http://arxiv.org/abs/2402.08649v1","category":"cs.NI"}
{"created":"2024-02-13 18:27:53","title":"Generating Universal Adversarial Perturbations for Quantum Classifiers","abstract":"Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies. Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of Universal Adversarial Perturbations (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers. In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers. We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence. We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks. Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a novel loss function based on fidelity constraints. We evaluate the performance of the proposed framework and show that our method achieves state-of-the-art misclassification rates, while maintaining high fidelity between legitimate and adversarial samples.","sentences":["Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies.","Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to adversarial attacks.","Moreover, the existence of Universal Adversarial Perturbations (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers.","In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers.","We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence.","We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks.","Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a novel loss function based on fidelity constraints.","We evaluate the performance of the proposed framework and show that our method achieves state-of-the-art misclassification rates, while maintaining high fidelity between legitimate and adversarial samples."],"url":"http://arxiv.org/abs/2402.08648v1","category":"cs.LG"}
{"created":"2024-02-13 18:25:55","title":"The formation of the magnetic symbiotic star FN Sgr","abstract":"To shed light on the origin of magnetic symbiotic stars, we investigated the system FN Sgr in detail. We searched for a reasonable formation pathway to explain its stellar and binary parameters including the magnetic field of the accreting white dwarf. We used the MESA code to carry out pre-CE and post-CE binary evolution and determined the outcome of CE evolution assuming the energy formalism. For the origin and evolution of the white dwarf magnetic field, we adopted the crystallization scenario. We found that FN Sgr can be explained as follows. First, a non-magnetic white dwarf is formed through CE evolution. Later, during post-CE evolution, the white dwarf starts to crystallize and a weak magnetic field is generated. After a few hundred Myr, the magnetic field penetrates the white dwarf surface and becomes detectable. Meanwhile, its companion evolves and becomes an evolved red giant. Subsequently, the white dwarf accretes part of the angular momentum from the red giant stellar winds. As a result, the white dwarf spin period decreases and its magnetic field reaches super-equipartition, getting amplified due to a rotation- and crystallization-driven dynamo. The binary then evolves into a symbiotic star, with a magnetic white dwarf accreting from an evolved red giant through atmospheric Roche-lobe overflow. We conclude that the rotation- and crystallization-driven dynamo scenario, or any age-dependent scenario, can explain the origin of magnetic symbiotic stars reasonably well. This adds another piece to the pile of evidence supporting this scenario. If our formation channel is correct, our findings suggest that white dwarfs in most symbiotic stars formed through CE evolution might be magnetic, provided that the red giant has spent >3 Gyr as a main-sequence star.","sentences":["To shed light on the origin of magnetic symbiotic stars, we investigated the system FN Sgr in detail.","We searched for a reasonable formation pathway to explain its stellar and binary parameters including the magnetic field of the accreting white dwarf.","We used the MESA code to carry out pre-CE and post-CE binary evolution and determined the outcome of CE evolution assuming the energy formalism.","For the origin and evolution of the white dwarf magnetic field, we adopted the crystallization scenario.","We found that FN Sgr can be explained as follows.","First, a non-magnetic white dwarf is formed through CE evolution.","Later, during post-CE evolution, the white dwarf starts to crystallize and a weak magnetic field is generated.","After a few hundred Myr, the magnetic field penetrates the white dwarf surface and becomes detectable.","Meanwhile, its companion evolves and becomes an evolved red giant.","Subsequently, the white dwarf accretes part of the angular momentum from the red giant stellar winds.","As a result, the white dwarf spin period decreases and its magnetic field reaches super-equipartition, getting amplified due to a rotation- and crystallization-driven dynamo.","The binary then evolves into a symbiotic star, with a magnetic white dwarf accreting from an evolved red giant through atmospheric Roche-lobe overflow.","We conclude that the rotation- and crystallization-driven dynamo scenario, or any age-dependent scenario, can explain the origin of magnetic symbiotic stars reasonably well.","This adds another piece to the pile of evidence supporting this scenario.","If our formation channel is correct, our findings suggest that white dwarfs in most symbiotic stars formed through CE evolution might be magnetic, provided that the red giant has spent >3 Gyr as a main-sequence star."],"url":"http://arxiv.org/abs/2402.08647v1","category":"astro-ph.SR"}
{"created":"2024-02-13 18:24:23","title":"Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data","abstract":"Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into reasoning towards human-like machine intelligence.","sentences":["Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data.","We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation.","The theory gives new insights into reasoning towards human-like machine intelligence."],"url":"http://arxiv.org/abs/2402.08646v1","category":"cs.AI"}
{"created":"2024-02-13 18:24:08","title":"Tandem Transformers for Inference Efficient LLMs","abstract":"The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance. We further incorporate the tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model. This ensures that the Tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy.","sentences":["The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially.","While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   ","We introduce a novel architecture, Tandem transformers, to address these issues.","This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously).","The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations.","On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance.","We further incorporate the tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model.","This ensures that the Tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy."],"url":"http://arxiv.org/abs/2402.08644v1","category":"cs.AI"}
{"created":"2024-02-13 18:19:00","title":"Clustering of primordial black holes from quantum diffusion during inflation","abstract":"We study how large fluctuations are spatially correlated in the presence of quantum diffusion during inflation. This is done by computing real-space correlation functions in the stochastic-$\\delta N$ formalism. We first derive an exact description of physical distances as measured by a local observer at the end of inflation, improving on previous works. Our approach is based on recursive algorithmic methods that consistently include volume-weighting effects. We then propose a \"large-volume'' approximation under which calculations can be done using first-passage time analysis only, and from which a new formula for the power spectrum in stochastic inflation is derived. We then study the full two-point statistics of the curvature perturbation. Due to the presence of exponential tails, we find that the joint distribution of large fluctuations is of the form $P(\\zeta_{R_1}, \\zeta_{R_2}) = F(R_1,R_2,r) P(\\zeta_{R_1})P( \\zeta_{R_2})$, where $\\zeta_{R_1}$ and $\\zeta_{R_2}$ denote the curvature perturbation coarse-grained at radii $R_1$ and $R_2$, around two spatial points distant by $r$. This implies that, on the tail, the reduced correlation function, defined as $P(\\zeta_{R_1}>\\zeta_{\\rm{c}}, \\zeta_{R_2}>\\zeta_{\\rm{c}})/[P(\\zeta_{R_1}>\\zeta_{\\rm{c}}) P(\\zeta_{R_2}>\\zeta_{\\rm{c}})]-1$, is independent of the threshold value $\\zeta_{\\rm{c}}$. This contrasts with Gaussian statistics where the same quantity strongly decays with $\\zeta_{\\rm{c}}$, and shows the existence of a universal clustering profile for all structures forming in the exponential tails. Structures forming in the intermediate (i.e. not yet exponential) tails may feature different, model-dependent behaviours.","sentences":["We study how large fluctuations are spatially correlated in the presence of quantum diffusion during inflation.","This is done by computing real-space correlation functions in the stochastic-$\\delta N$ formalism.","We first derive an exact description of physical distances as measured by a local observer at the end of inflation, improving on previous works.","Our approach is based on recursive algorithmic methods that consistently include volume-weighting effects.","We then propose a \"large-volume'' approximation under which calculations can be done using first-passage time analysis only, and from which a new formula for the power spectrum in stochastic inflation is derived.","We then study the full two-point statistics of the curvature perturbation.","Due to the presence of exponential tails, we find that the joint distribution of large fluctuations is of the form $P(\\zeta_{R_1}, \\zeta_{R_2})","= F(R_1,R_2,r)","P(\\zeta_{R_1})P( \\zeta_{R_2})$, where $\\zeta_{R_1}$ and $\\zeta_{R_2}$ denote the curvature perturbation coarse-grained at radii $R_1$ and $R_2$, around two spatial points distant by $r$. This implies that, on the tail, the reduced correlation function, defined as $P(\\zeta_{R_1}>\\zeta_{\\rm{c}}, \\zeta_{R_2}>\\zeta_{\\rm{c}})/[P(\\zeta_{R_1}>\\zeta_{\\rm{c}}) P(\\zeta_{R_2}>\\zeta_{\\rm{c}})]-1$, is independent of the threshold value $\\zeta_{\\rm{c}}$. This contrasts with Gaussian statistics where the same quantity strongly decays with $\\zeta_{\\rm{c}}$, and shows the existence of a universal clustering profile for all structures forming in the exponential tails.","Structures forming in the intermediate (i.e. not yet exponential) tails may feature different, model-dependent behaviours."],"url":"http://arxiv.org/abs/2402.08642v1","category":"astro-ph.CO"}
{"created":"2024-02-13 18:09:38","title":"Forecasting high-impact research topics via machine learning on evolving knowledge graphs","abstract":"The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to predict the impact of new ideas will be a crucial component of future artificial muses that can inspire new impactful and interesting scientific ideas.","sentences":["The exponential growth in scientific publications poses a severe challenge for human researchers.","It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field.","While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived.","Here we show how to predict the impact of onsets of ideas that have never been published by researchers.","For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers.","It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers.","Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions.","We envision that the ability to predict the impact of new ideas will be a crucial component of future artificial muses that can inspire new impactful and interesting scientific ideas."],"url":"http://arxiv.org/abs/2402.08640v1","category":"cs.DL"}
{"created":"2024-02-13 18:06:17","title":"Morse theory of Euclidean distance functions and applications to real algebraic geometry","abstract":"Given two closed subsets $X, Y$ in $\\mathbb{R}^n$, we construct a version of Morse Theory for $\\mathrm{dist}_Y|_X \\colon X \\to \\mathbb{R}$, the restriction to $X$ of the Euclidean distance function from $Y$. We use the notion of critical points of Lipschitz functions introduced by Clarke and apply the more general Morse Theory of continuous selections, as presented by Agrachev, Pallaschke, and Scholtes. In this framework, nondegenerate critical points have two indices: a quadratic index as in classical Morse Theory, and a piecewise linear index that relates to the notion of bottlenecks.   This framework is flexible enough to simultaneously treat two cases of interest for computational algebraic geometry: the Bottleneck Degree (BND) and the Euclidean Distance Degree (EDD). We provide bounds on the number of critical points of $\\mathrm{dist}_Y|_X$ when $X$ and $Y$ are generic real algebraic hypersurfaces and relate these bounds to the BND and EDD. We also prove a duality formula relating the Euler Characteristics of $X$ and $Y$ with the number of critical points of $\\mathrm{dist}_Y|_X$ and $\\mathrm{dist}_X|_Y$, respectively.   Moreover, we introduce a technical toolset of independent interest, which guarantees that our Morse Theory can be used in the generic algebraic case.","sentences":["Given two closed subsets $X, Y$ in $\\mathbb{R}^n$, we construct a version of Morse Theory for $\\mathrm{dist}_Y|_X \\colon X \\to \\mathbb{R}$, the restriction to $X$ of the Euclidean distance function from $Y$. We use the notion of critical points of Lipschitz functions introduced by Clarke and apply the more general Morse Theory of continuous selections, as presented by Agrachev, Pallaschke, and Scholtes.","In this framework, nondegenerate critical points have two indices: a quadratic index as in classical Morse Theory, and a piecewise linear index that relates to the notion of bottlenecks.   ","This framework is flexible enough to simultaneously treat two cases of interest for computational algebraic geometry: the Bottleneck Degree (BND) and the Euclidean Distance Degree (EDD).","We provide bounds on the number of critical points of $\\mathrm{dist}_Y|_X$ when $X$ and $Y$ are generic real algebraic hypersurfaces and relate these bounds to the BND and EDD.","We also prove a duality formula relating the Euler Characteristics of $X$ and $Y$ with the number of critical points of $\\mathrm{dist}_Y|_X$ and $\\mathrm{dist}_X|_Y$, respectively.   ","Moreover, we introduce a technical toolset of independent interest, which guarantees that our Morse Theory can be used in the generic algebraic case."],"url":"http://arxiv.org/abs/2402.08639v1","category":"math.AG"}
{"created":"2024-02-13 18:03:56","title":"Strategizing against No-Regret Learners in First-Price Auctions","abstract":"We study repeated first-price auctions and general repeated Bayesian games between two players, where one player, the learner, employs a no-regret learning algorithm, and the other player, the optimizer, knowing the learner's algorithm, strategizes to maximize its own utility. For a commonly used class of no-regret learning algorithms called mean-based algorithms, we show that (i) in standard (i.e., full-information) first-price auctions, the optimizer cannot get more than the Stackelberg utility -- a standard benchmark in the literature, but (ii) in Bayesian first-price auctions, there are instances where the optimizer can achieve much higher than the Stackelberg utility.   On the other hand, Mansour et al. (2022) showed that a more sophisticated class of algorithms called no-polytope-swap-regret algorithms are sufficient to cap the optimizer's utility at the Stackelberg utility in any repeated Bayesian game (including Bayesian first-price auctions), and they pose the open question whether no-polytope-swap-regret algorithms are necessary to cap the optimizer's utility. For general Bayesian games, under a reasonable and necessary condition, we prove that no-polytope-swap-regret algorithms are indeed necessary to cap the optimizer's utility and thus answer their open question. For Bayesian first-price auctions, we give a simple improvement of the standard algorithm for minimizing the polytope swap regret by exploiting the structure of Bayesian first-price auctions.","sentences":["We study repeated first-price auctions and general repeated Bayesian games between two players, where one player, the learner, employs a no-regret learning algorithm, and the other player, the optimizer, knowing the learner's algorithm, strategizes to maximize its own utility.","For a commonly used class of no-regret learning algorithms called mean-based algorithms, we show that (i) in standard (i.e., full-information) first-price auctions, the optimizer cannot get more than the Stackelberg utility -- a standard benchmark in the literature, but (ii) in Bayesian first-price auctions, there are instances where the optimizer can achieve much higher than the Stackelberg utility.   ","On the other hand, Mansour et al. (2022) showed that a more sophisticated class of algorithms called no-polytope-swap-regret algorithms are sufficient to cap the optimizer's utility at the Stackelberg utility in any repeated Bayesian game (including Bayesian first-price auctions), and they pose the open question whether no-polytope-swap-regret algorithms are necessary to cap the optimizer's utility.","For general Bayesian games, under a reasonable and necessary condition, we prove that no-polytope-swap-regret algorithms are indeed necessary to cap the optimizer's utility and thus answer their open question.","For Bayesian first-price auctions, we give a simple improvement of the standard algorithm for minimizing the polytope swap regret by exploiting the structure of Bayesian first-price auctions."],"url":"http://arxiv.org/abs/2402.08637v1","category":"cs.GT"}
{"created":"2024-02-13 18:02:52","title":"Maxwell construction for a nonreciprocal Cahn-Hilliard model","abstract":"We consider a class of continuum models with a spurious gradient dynamics structure that form a bridge between passive and active systems: They formally maintain variational properties similar to passive systems, but describe certain active systems with sustained out-of-equilibrium dynamics as, e.g., oscillatory instabilities. After introducing the general class of systems, we focus on the example of a nonreciprocal Cahn-Hilliard (NRCH) model, present a Maxwell construction for coexisting states as well as resulting phase diagrams in the thermodynamic limit. These are then related to the bifurcation structures of corresponding finite-size systems. Our considerations directly apply to crystalline phases and also indicate the coexistence of uniform stationary and oscillatory phases.","sentences":["We consider a class of continuum models with a spurious gradient dynamics structure that form a bridge between passive and active systems: They formally maintain variational properties similar to passive systems, but describe certain active systems with sustained out-of-equilibrium dynamics as, e.g., oscillatory instabilities.","After introducing the general class of systems, we focus on the example of a nonreciprocal Cahn-Hilliard (NRCH) model, present a Maxwell construction for coexisting states as well as resulting phase diagrams in the thermodynamic limit.","These are then related to the bifurcation structures of corresponding finite-size systems.","Our considerations directly apply to crystalline phases and also indicate the coexistence of uniform stationary and oscillatory phases."],"url":"http://arxiv.org/abs/2402.08634v1","category":"nlin.PS"}
{"created":"2024-02-13 18:02:13","title":"A local variational principle for fracture","abstract":"The seminal paper of Francfort and Marigo [FM] introduced a variational formulation for Griffith fracture that has resulted in substantial theoretical and practical progress in modeling and simulating fracture. In particular, it led to the phase-field approximation proposed in [BFM], which has been widely implemented. However, the formulation in [FM] is known to have limitations, including its inability to treat applied loads and its reliance on global minimization. In addition, the phase-field model [BFM] and its extensions, as implemented, are not generally approximations of the global minimizers in [FM]. In this paper, we show that there is a local variational principle satisfied by global and local minimizers of the energy introduced in [FM], which is compatible with loads, and which is a generalization of the stress intensity factor. We use this principle to reformulate variational fracture, including formulations that, for the first time, can include all forms of applied loads. We conclude by showing the connection between phase-field models, as implemented, and our formulations.","sentences":["The seminal paper of Francfort and Marigo [FM] introduced a variational formulation for Griffith fracture that has resulted in substantial theoretical and practical progress in modeling and simulating fracture.","In particular, it led to the phase-field approximation proposed in [BFM], which has been widely implemented.","However, the formulation in [FM] is known to have limitations, including its inability to treat applied loads and its reliance on global minimization.","In addition, the phase-field model [BFM] and its extensions, as implemented, are not generally approximations of the global minimizers in [FM].","In this paper, we show that there is a local variational principle satisfied by global and local minimizers of the energy introduced in [FM], which is compatible with loads, and which is a generalization of the stress intensity factor.","We use this principle to reformulate variational fracture, including formulations that, for the first time, can include all forms of applied loads.","We conclude by showing the connection between phase-field models, as implemented, and our formulations."],"url":"http://arxiv.org/abs/2402.08633v1","category":"math.AP"}
{"created":"2024-02-13 17:59:34","title":"Knowledge Editing on Black-box Large Language Models","abstract":"Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all baselines and achieves strong generalization, especially with huge improvements on style retention (average $+20.82\\%\\uparrow$).","sentences":["Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge.","Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available.","To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time.","To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses.","Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all baselines and achieves strong generalization, especially with huge improvements on style retention (average $+20.82\\%\\uparrow$)."],"url":"http://arxiv.org/abs/2402.08631v1","category":"cs.CL"}
{"created":"2024-02-13 17:54:03","title":"The zero divisor conjecture and Mealy automata","abstract":"The zero divisor conjecture is sufficient to prove for certain class of finitely presented groups where the relations are given by a pairing of generators. We associate Mealy automata to such pairings, and prove that the zero divisor conjecture holds for groups corresponding to invertible automata with three states. In particular, there cannot be zero divisors of support three corresponding to invertible pairings.","sentences":["The zero divisor conjecture is sufficient to prove for certain class of finitely presented groups where the relations are given by a pairing of generators.","We associate Mealy automata to such pairings, and prove that the zero divisor conjecture holds for groups corresponding to invertible automata with three states.","In particular, there cannot be zero divisors of support three corresponding to invertible pairings."],"url":"http://arxiv.org/abs/2402.08625v1","category":"math.GR"}
{"created":"2024-02-13 17:47:42","title":"NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs","abstract":"A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry and appearance of a scene. We here ask the question whether we can transfer the appearance from a source NeRF onto a target 3D geometry in a semantically meaningful way, such that the resulting new NeRF retains the target geometry but has an appearance that is an analogy to the source NeRF. To this end, we generalize classic image analogies from 2D images to NeRFs. We leverage correspondence transfer along semantic affinity that is driven by semantic features from large, pre-trained 2D image models to achieve multi-view consistent appearance transfer. Our method allows exploring the mix-and-match product space of 3D geometry and appearance. We show that our method outperforms traditional stylization-based methods and that a large majority of users prefer our method over several typical baselines.","sentences":["A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry and appearance of a scene.","We here ask the question whether we can transfer the appearance from a source NeRF onto a target 3D geometry in a semantically meaningful way, such that the resulting new NeRF retains the target geometry but has an appearance that is an analogy to the source NeRF.","To this end, we generalize classic image analogies from 2D images to NeRFs.","We leverage correspondence transfer along semantic affinity that is driven by semantic features from large, pre-trained 2D image models to achieve multi-view consistent appearance transfer.","Our method allows exploring the mix-and-match product space of 3D geometry and appearance.","We show that our method outperforms traditional stylization-based methods and that a large majority of users prefer our method over several typical baselines."],"url":"http://arxiv.org/abs/2402.08622v1","category":"cs.CV"}
{"created":"2024-02-13 17:42:27","title":"A Generalized Approach to Online Convex Optimization","abstract":"In this paper, we analyze the problem of online convex optimization in different settings. We show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization. We also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound. We further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries. We use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds. Our framework allows us to analyze online optimization in various settings, such full-information feedback, bandit feedback, stochastic regret, adversarial regret and various forms of non-stationary regret. Using our analysis, we provide the first efficient projection-free online convex optimization algorithm using linear optimization oracles.","sentences":["In this paper, we analyze the problem of online convex optimization in different settings.","We show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization.","We also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound.","We further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries.","We use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds.","Our framework allows us to analyze online optimization in various settings, such full-information feedback, bandit feedback, stochastic regret, adversarial regret and various forms of non-stationary regret.","Using our analysis, we provide the first efficient projection-free online convex optimization algorithm using linear optimization oracles."],"url":"http://arxiv.org/abs/2402.08621v1","category":"cs.LG"}
{"created":"2024-02-13 17:42:12","title":"Quasineutral multistability in an epidemiological-like model for defective-helper betacoronavirus infection in cell cultures","abstract":"It is well known that, during replication, RNA viruses spontaneously generate defective viral genomes (DVGs). DVGs are unable to complete an infectious cycle autonomously, and depend on coinfection with a helper wild-type virus (HV) for their replication and/or transmission. The study of the dynamics arising from a HV and its DVGs has been a longstanding question in virology. It has been shown that DVGs can modulate HV replication and, depending on the strength of interference, result in HV extinctions or self-sustained persistent fluctuations. Extensive experimental work has provided mechanistic explanations for DVG generation and compelling evidences of HV-DVGs virus coevolution. Some of these observations have been captured in mathematical models. Here, we develop and investigate an epidemiological-like mathematical model specifically designed to study the dynamics of betacoronavirus in cell cultures experiments. The dynamics of the model is governed by several degenerate normally hyperbolic invariant manifolds given by quasineutral planes - i.e. filled by equilibrium points. Three different quasineutral planes have been identified depending on parameters and involving: (i) persistence of HV and DVGs; (\\emph{ii}) persistence of non-infected cells and DVG-infected cells; and (iii) persistence of DVG-infected cells and DVGs. Sensitivity analyses indicate that model dynamics largely depend on the maximum burst size ($B$), and both the production rate ($\\beta$) and replicative advantage ($\\delta$) of DVGs. Finally, the model has been fitted to single-passage experimental data using artificial intelligence and key virological parameters have been estimated.","sentences":["It is well known that, during replication, RNA viruses spontaneously generate defective viral genomes (DVGs).","DVGs are unable to complete an infectious cycle autonomously, and depend on coinfection with a helper wild-type virus (HV) for their replication and/or transmission.","The study of the dynamics arising from a HV and its DVGs has been a longstanding question in virology.","It has been shown that DVGs can modulate HV replication and, depending on the strength of interference, result in HV extinctions or self-sustained persistent fluctuations.","Extensive experimental work has provided mechanistic explanations for DVG generation and compelling evidences of HV-DVGs virus coevolution.","Some of these observations have been captured in mathematical models.","Here, we develop and investigate an epidemiological-like mathematical model specifically designed to study the dynamics of betacoronavirus in cell cultures experiments.","The dynamics of the model is governed by several degenerate normally hyperbolic invariant manifolds given by quasineutral planes - i.e. filled by equilibrium points.","Three different quasineutral planes have been identified depending on parameters and involving: (i) persistence of HV and DVGs; (\\emph{ii}) persistence of non-infected cells and DVG-infected cells; and (iii) persistence of DVG-infected cells and DVGs.","Sensitivity analyses indicate that model dynamics largely depend on the maximum burst size ($B$), and both the production rate ($\\beta$) and replicative advantage ($\\delta$) of DVGs.","Finally, the model has been fitted to single-passage experimental data using artificial intelligence and key virological parameters have been estimated."],"url":"http://arxiv.org/abs/2402.08620v1","category":"math.DS"}
{"created":"2024-02-13 17:26:32","title":"CaPS: Collaborative and Private Synthetic Data Generation from Distributed Sources","abstract":"Data is the lifeblood of the modern world, forming a fundamental part of AI, decision-making, and research advances. With increase in interest in data, governments have taken important steps towards a regulated data world, drastically impacting data sharing and data usability and resulting in massive amounts of data confined within the walls of organizations. While synthetic data generation (SDG) is an appealing solution to break down these walls and enable data sharing, the main drawback of existing solutions is the assumption of a trusted aggregator for generative model training. Given that many data holders may not want to, or be legally allowed to, entrust a central entity with their raw data, we propose a framework for the collaborative and private generation of synthetic tabular data from distributed data holders. Our solution is general, applicable to any marginal-based SDG, and provides input privacy by replacing the trusted aggregator with secure multi-party computation (MPC) protocols and output privacy via differential privacy (DP). We demonstrate the applicability and scalability of our approach for the state-of-the-art select-measure-generate SDG algorithms MWEM+PGM and AIM.","sentences":["Data is the lifeblood of the modern world, forming a fundamental part of AI, decision-making, and research advances.","With increase in interest in data, governments have taken important steps towards a regulated data world, drastically impacting data sharing and data usability and resulting in massive amounts of data confined within the walls of organizations.","While synthetic data generation (SDG) is an appealing solution to break down these walls and enable data sharing, the main drawback of existing solutions is the assumption of a trusted aggregator for generative model training.","Given that many data holders may not want to, or be legally allowed to, entrust a central entity with their raw data, we propose a framework for the collaborative and private generation of synthetic tabular data from distributed data holders.","Our solution is general, applicable to any marginal-based SDG, and provides input privacy by replacing the trusted aggregator with secure multi-party computation (MPC) protocols and output privacy via differential privacy (DP).","We demonstrate the applicability and scalability of our approach for the state-of-the-art select-measure-generate SDG algorithms MWEM+PGM and AIM."],"url":"http://arxiv.org/abs/2402.08614v1","category":"cs.CR"}
{"created":"2024-02-13 17:18:56","title":"Mixtures of Experts Unlock Parameter Scaling for Deep RL","abstract":"The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.","sentences":["The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size.","Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance.","In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes.","This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning."],"url":"http://arxiv.org/abs/2402.08609v1","category":"cs.LG"}
{"created":"2024-02-13 17:12:01","title":"Arbitrary Polynomial Separations in Trainable Quantum Machine Learning","abstract":"Recent theoretical results in quantum machine learning have demonstrated a general trade-off between the expressive power of quantum neural networks (QNNs) and their trainability; as a corollary of these results, practical exponential separations in expressive power over classical machine learning models are believed to be infeasible as such QNNs take a time to train that is exponential in the model size. We here circumvent these negative results by constructing a hierarchy of efficiently trainable QNNs that exhibit unconditionally provable, polynomial memory separations of arbitrary constant degree over classical neural networks in performing a classical sequence modeling task. Furthermore, each unit cell of the introduced class of QNNs is computationally efficient, implementable in constant time on a quantum device. The classical networks we prove a separation over include well-known examples such as recurrent neural networks and Transformers. We show that quantum contextuality is the source of the expressivity separation, suggesting that other classical sequence learning problems with long-time correlations may be a regime where practical advantages in quantum machine learning may exist.","sentences":["Recent theoretical results in quantum machine learning have demonstrated a general trade-off between the expressive power of quantum neural networks (QNNs) and their trainability; as a corollary of these results, practical exponential separations in expressive power over classical machine learning models are believed to be infeasible as such QNNs take a time to train that is exponential in the model size.","We here circumvent these negative results by constructing a hierarchy of efficiently trainable QNNs that exhibit unconditionally provable, polynomial memory separations of arbitrary constant degree over classical neural networks in performing a classical sequence modeling task.","Furthermore, each unit cell of the introduced class of QNNs is computationally efficient, implementable in constant time on a quantum device.","The classical networks we prove a separation over include well-known examples such as recurrent neural networks and Transformers.","We show that quantum contextuality is the source of the expressivity separation, suggesting that other classical sequence learning problems with long-time correlations may be a regime where practical advantages in quantum machine learning may exist."],"url":"http://arxiv.org/abs/2402.08606v1","category":"quant-ph"}
{"created":"2024-02-13 17:06:07","title":"On an optimal problem of bilinear forms","abstract":"We study an optimization problem originated from the Grothendieck constant. A generalized normal equation is proposed and analyzed. We establish a correspondence between solutions of the general normal equation and its dual equation. Explicit solutions are described for the two-dimensional case.","sentences":["We study an optimization problem originated from the Grothendieck constant.","A generalized normal equation is proposed and analyzed.","We establish a correspondence between solutions of the general normal equation and its dual equation.","Explicit solutions are described for the two-dimensional case."],"url":"http://arxiv.org/abs/2402.08599v1","category":"math.FA"}
{"created":"2024-02-13 17:03:59","title":"Lower bounds on fibered Yang-Mills functionals: generic nefness and semistability of direct images","abstract":"The main goal of this paper is to generalize a part of the relationship between mean curvature and Harder-Narasimhan filtrations of holomorphic vector bundles to arbitrary polarized fibrations. More precisely, for a polarized family of complex projective manifolds, we establish lower bounds on a fibered version of Yang-Mills functionals in terms of the Harder-Narasimhan slopes of direct image sheaves associated with high tensor powers of the polarization. We discuss the optimality of these lower bounds and, as an application, provide an analytic characterisation of a fibered version of generic nefness. As another application, we refine the existent obstructions for finding metrics with constant horizontal mean curvature. The study of the semiclassical limit of Hermitian Yang-Mills functionals lies at the heart of our approach.","sentences":["The main goal of this paper is to generalize a part of the relationship between mean curvature and Harder-Narasimhan filtrations of holomorphic vector bundles to arbitrary polarized fibrations.","More precisely, for a polarized family of complex projective manifolds, we establish lower bounds on a fibered version of Yang-Mills functionals in terms of the Harder-Narasimhan slopes of direct image sheaves associated with high tensor powers of the polarization.","We discuss the optimality of these lower bounds and, as an application, provide an analytic characterisation of a fibered version of generic nefness.","As another application, we refine the existent obstructions for finding metrics with constant horizontal mean curvature.","The study of the semiclassical limit of Hermitian Yang-Mills functionals lies at the heart of our approach."],"url":"http://arxiv.org/abs/2402.08598v1","category":"math.DG"}
{"created":"2024-02-13 16:57:02","title":"Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning","abstract":"Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in a multi-task transfer learning setting. These methods generally involve individually training prompts for each source task and then aggregating them to provide the initialization of the prompt for the target task. However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other. We argue that when we extract knowledge from source tasks via training source prompts, we need to consider this correlation among source tasks for better transfer to target tasks. To this end, we propose a Bayesian approach where we work with the posterior distribution of prompts across source tasks. We obtain representative source prompts corresponding to the samples from the posterior utilizing Stein Variational Gradient Descent, which are then aggregated to constitute the initial target prompt. We show extensive experimental results on the standard benchmark NLP tasks, where our Bayesian multi-task transfer learning approach outperforms the state-of-the-art methods in many settings. Furthermore, our approach requires no auxiliary models other than the prompt itself, achieving a high degree of parameter efficiency.","sentences":["Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in a multi-task transfer learning setting.","These methods generally involve individually training prompts for each source task and then aggregating them to provide the initialization of the prompt for the target task.","However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other.","We argue that when we extract knowledge from source tasks via training source prompts, we need to consider this correlation among source tasks for better transfer to target tasks.","To this end, we propose a Bayesian approach where we work with the posterior distribution of prompts across source tasks.","We obtain representative source prompts corresponding to the samples from the posterior utilizing Stein Variational Gradient Descent, which are then aggregated to constitute the initial target prompt.","We show extensive experimental results on the standard benchmark NLP tasks, where our Bayesian multi-task transfer learning approach outperforms the state-of-the-art methods in many settings.","Furthermore, our approach requires no auxiliary models other than the prompt itself, achieving a high degree of parameter efficiency."],"url":"http://arxiv.org/abs/2402.08594v1","category":"cs.CL"}
{"created":"2024-02-13 16:53:48","title":"Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs","abstract":"In this paper, we present \"Graph Feature Preprocessor\", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time. These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection. We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models. Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner. We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets. In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging. Our solution, which combines our Graph Feature Preprocessor and gradient-boosting-based machine learning models, is able to detect these illicit transactions with higher minority-class F1 scores than standard graph neural networks. In addition, the end-to-end throughput rate of our solution executed on a multicore CPU outperforms the graph neural network baselines executed on a powerful V100 GPU. Overall, the combination of high accuracy, a high throughput rate, and low latency of our solution demonstrates the practical value of our library in real-world applications. Graph Feature Preprocessor has been integrated into IBM mainframe software products, namely \"IBM Cloud Pak for Data on Z\" and \"AI Toolkit for IBM Z and LinuxONE\".","sentences":["In this paper, we present \"Graph Feature Preprocessor\", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time.","These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection.","We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models.","Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner.","We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets.","In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging.","Our solution, which combines our Graph Feature Preprocessor and gradient-boosting-based machine learning models, is able to detect these illicit transactions with higher minority-class F1 scores than standard graph neural networks.","In addition, the end-to-end throughput rate of our solution executed on a multicore CPU outperforms the graph neural network baselines executed on a powerful V100 GPU.","Overall, the combination of high accuracy, a high throughput rate, and low latency of our solution demonstrates the practical value of our library in real-world applications.","Graph Feature Preprocessor has been integrated into IBM mainframe software products, namely \"IBM Cloud Pak for Data on Z\" and \"AI Toolkit for IBM Z and LinuxONE\"."],"url":"http://arxiv.org/abs/2402.08593v1","category":"cs.LG"}
{"created":"2024-02-13 16:46:11","title":"Machine Learning based Pointing Models for Radio/Sub-millimeter Telescopes","abstract":"Radio, sub-millimiter and millimeter ground-based telescopes are powerful instruments for studying the gas and dust-rich regions of the Universe that are invisible at optical wavelengths, but the pointing accuracy is crucial for obtaining high-quality data. Pointing errors are small deviations of the telescope's orientation from its desired direction. The telescopes use linear regression pointing models to correct for these errors, taking into account various factors such as weather conditions, telescope mechanical structure, and the target's position in the sky. However, residual pointing errors can still occur due to factors that are hard to model accurately, such as thermal and gravitational deformation and environmental conditions like humidity and wind. Here we present a proof-of-concept for reducing pointing error for the Atacama Pathfinder EXperiment (APEX) telescope in the high-altitude Atacama Desert in Chile based on machine learning. Using historic pointing data from 2022, we trained eXtreme Gradient Boosting (XGBoost) models that reduced the root-mean-square errors (RMSE) for azimuth and elevation (horizontal and vertical angle) pointing corrections by 4.3% and 9.5%, respectively, on hold-out test data. Our results will inform operations of current and future facilities such as the next-generation Atacama Large Aperture Submillimeter Telescope (AtLAST).","sentences":["Radio, sub-millimiter and millimeter ground-based telescopes are powerful instruments for studying the gas and dust-rich regions of the Universe that are invisible at optical wavelengths, but the pointing accuracy is crucial for obtaining high-quality data.","Pointing errors are small deviations of the telescope's orientation from its desired direction.","The telescopes use linear regression pointing models to correct for these errors, taking into account various factors such as weather conditions, telescope mechanical structure, and the target's position in the sky.","However, residual pointing errors can still occur due to factors that are hard to model accurately, such as thermal and gravitational deformation and environmental conditions like humidity and wind.","Here we present a proof-of-concept for reducing pointing error for the Atacama Pathfinder EXperiment (APEX) telescope in the high-altitude Atacama Desert in Chile based on machine learning.","Using historic pointing data from 2022, we trained eXtreme Gradient Boosting (XGBoost) models that reduced the root-mean-square errors (RMSE) for azimuth and elevation (horizontal and vertical angle) pointing corrections by 4.3% and 9.5%, respectively, on hold-out test data.","Our results will inform operations of current and future facilities such as the next-generation Atacama Large Aperture Submillimeter Telescope (AtLAST)."],"url":"http://arxiv.org/abs/2402.08589v1","category":"astro-ph.IM"}
{"created":"2024-02-13 16:37:33","title":"Tail behavior and sample path properties of positive supOU processes","abstract":"In this paper we consider sample path growth of superpositions of positive Ornstein--Uhlenbeck type processes (supOU). SupOU are stationary infinitely divisible processes defined as integrals with respect to a random measure. They allow marginal distributions and correlations to be modeled independently. Our results show that the almost sure behavior is primarily governed by the tail of the marginal distribution. In particular, we obtain a general integral test for the sample path growth that covers both heavy-tailed and light-tailed scenarios. We also investigate the tail behavior of the marginal distributions in connection with the characteristics of the underlying random measure.","sentences":["In this paper we consider sample path growth of superpositions of positive Ornstein--Uhlenbeck type processes (supOU).","SupOU are stationary infinitely divisible processes defined as integrals with respect to a random measure.","They allow marginal distributions and correlations to be modeled independently.","Our results show that the almost sure behavior is primarily governed by the tail of the marginal distribution.","In particular, we obtain a general integral test for the sample path growth that covers both heavy-tailed and light-tailed scenarios.","We also investigate the tail behavior of the marginal distributions in connection with the characteristics of the underlying random measure."],"url":"http://arxiv.org/abs/2402.08584v1","category":"math.PR"}
{"created":"2024-02-13 16:36:21","title":"FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis","abstract":"Medical image segmentation is a critical process in the field of medical imaging, playing a pivotal role in diagnosis, treatment, and research. It involves partitioning of an image into multiple regions, representing distinct anatomical or pathological structures. Conventional methods often grapple with the challenge of balancing spatial precision and comprehensive feature representation due to their reliance on traditional loss functions. To overcome this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that integrates the benefits of contrastive learning (which extracts intricate features, particularly in the nuanced domain of medical imaging) with the spatial accuracy inherent in the Dice loss. The objective is to augment both spatial precision and feature-based representation in the segmentation of medical images. FESS Loss signifies a notable advancement, offering a more accurate and refined segmentation process, ultimately contributing to heightened precision in the analysis of medical images. Further, FESS loss demonstrates superior performance in limited annotated data availability scenarios often present in the medical domain.","sentences":["Medical image segmentation is a critical process in the field of medical imaging, playing a pivotal role in diagnosis, treatment, and research.","It involves partitioning of an image into multiple regions, representing distinct anatomical or pathological structures.","Conventional methods often grapple with the challenge of balancing spatial precision and comprehensive feature representation due to their reliance on traditional loss functions.","To overcome this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that integrates the benefits of contrastive learning (which extracts intricate features, particularly in the nuanced domain of medical imaging) with the spatial accuracy inherent in the Dice loss.","The objective is to augment both spatial precision and feature-based representation in the segmentation of medical images.","FESS Loss signifies a notable advancement, offering a more accurate and refined segmentation process, ultimately contributing to heightened precision in the analysis of medical images.","Further, FESS loss demonstrates superior performance in limited annotated data availability scenarios often present in the medical domain."],"url":"http://arxiv.org/abs/2402.08582v1","category":"cs.CV"}
{"created":"2024-02-13 16:35:48","title":"Improving Factual Error Correction for Abstractive Summarization via Data Distillation and Conditional-generation Cloze","abstract":"Improving factual consistency in abstractive summarization has been a focus of current research. One promising approach is the post-editing method. However, previous works have yet to make sufficient use of factual factors in summaries and suffers from the negative effect of the training datasets. In this paper, we first propose a novel factual error correction model FactCloze based on a conditional-generation cloze task. FactCloze can construct the causality among factual factors while being able to determine whether the blank can be answered or not. Then, we propose a data distillation method to generate a more faithful summarization dataset SummDSC via multiple-dimensional evaluation. We experimentally validate the effectiveness of our approach, which leads to an improvement in multiple factual consistency metrics compared to baselines.","sentences":["Improving factual consistency in abstractive summarization has been a focus of current research.","One promising approach is the post-editing method.","However, previous works have yet to make sufficient use of factual factors in summaries and suffers from the negative effect of the training datasets.","In this paper, we first propose a novel factual error correction model FactCloze based on a conditional-generation cloze task.","FactCloze can construct the causality among factual factors while being able to determine whether the blank can be answered or not.","Then, we propose a data distillation method to generate a more faithful summarization dataset SummDSC via multiple-dimensional evaluation.","We experimentally validate the effectiveness of our approach, which leads to an improvement in multiple factual consistency metrics compared to baselines."],"url":"http://arxiv.org/abs/2402.08581v1","category":"cs.CL"}
{"created":"2024-02-13 16:33:34","title":"Tracking topological defect motion and incommensurate charge order melting in a perovskite manganite","abstract":"Charge order pervades the phase diagrams of many quantum materials where it competes with superconducting and magnetic phases, hosts electronic phase transitions and topological defects, and couples to the lattice generating intricate structural distortions. Incommensurate charge order is readily stabilized in manganese oxides where it is associated with anomalous electronic and magnetic properties, but its nanoscale structural inhomogeneity complicates precise characterization and understanding of these phases. Leveraging atomic-resolution variable temperature cryogenic scanning transmission electron microscopy (cryo-STEM), we characterize the thermal evolution of charge order in a model manganite system, finding that mobile networks of topological defects generate phase inhomogeneity and induce incommensuration in an otherwise lattice-locked modulation. The melting of charge order at high temperatures is accompanied by generation of additional defects which decouple large areas of the order from the lattice.","sentences":["Charge order pervades the phase diagrams of many quantum materials where it competes with superconducting and magnetic phases, hosts electronic phase transitions and topological defects, and couples to the lattice generating intricate structural distortions.","Incommensurate charge order is readily stabilized in manganese oxides where it is associated with anomalous electronic and magnetic properties, but its nanoscale structural inhomogeneity complicates precise characterization and understanding of these phases.","Leveraging atomic-resolution variable temperature cryogenic scanning transmission electron microscopy (cryo-STEM), we characterize the thermal evolution of charge order in a model manganite system, finding that mobile networks of topological defects generate phase inhomogeneity and induce incommensuration in an otherwise lattice-locked modulation.","The melting of charge order at high temperatures is accompanied by generation of additional defects which decouple large areas of the order from the lattice."],"url":"http://arxiv.org/abs/2402.08580v1","category":"cond-mat.str-el"}
{"created":"2024-02-13 16:31:04","title":"Training Coupled Phase Oscillators as a Neuromorphic Platform using Equilibrium Propagation","abstract":"Given the rapidly growing scale and resource requirements of machine learning applications, the idea of building more efficient learning machines much closer to the laws of physics is an attractive proposition. One central question for identifying promising candidates for such neuromorphic platforms is whether not only inference but also training can exploit the physical dynamics. In this work, we show that it is possible to successfully train a system of coupled phase oscillators - one of the most widely investigated nonlinear dynamical systems with a multitude of physical implementations, comprising laser arrays, coupled mechanical limit cycles, superfluids, and exciton-polaritons. To this end, we apply the approach of equilibrium propagation, which permits to extract training gradients via a physical realization of backpropagation, based only on local interactions. The complex energy landscape of the XY/ Kuramoto model leads to multistability, and we show how to address this challenge. Our study identifies coupled phase oscillators as a new general-purpose neuromorphic platform and opens the door towards future experimental implementations.","sentences":["Given the rapidly growing scale and resource requirements of machine learning applications, the idea of building more efficient learning machines much closer to the laws of physics is an attractive proposition.","One central question for identifying promising candidates for such neuromorphic platforms is whether not only inference but also training can exploit the physical dynamics.","In this work, we show that it is possible to successfully train a system of coupled phase oscillators - one of the most widely investigated nonlinear dynamical systems with a multitude of physical implementations, comprising laser arrays, coupled mechanical limit cycles, superfluids, and exciton-polaritons.","To this end, we apply the approach of equilibrium propagation, which permits to extract training gradients via a physical realization of backpropagation, based only on local interactions.","The complex energy landscape of the XY/ Kuramoto model leads to multistability, and we show how to address this challenge.","Our study identifies coupled phase oscillators as a new general-purpose neuromorphic platform and opens the door towards future experimental implementations."],"url":"http://arxiv.org/abs/2402.08579v1","category":"cs.ET"}
{"created":"2024-02-13 16:30:30","title":"FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing","abstract":"Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices. By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy. Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity. However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios. In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders. To further reduce resource consumption, a channel-wise model pruning algorithm that shrinks the footprint of local models while accounting for both data and system heterogeneity is employed in FedLPS. Additionally, a novel heterogeneous model aggregation algorithm is proposed to aggregate the heterogeneous predictors in FedLPS. We implemented the proposed FedLPS on a real FL platform and compared it with state-of-the-art (SOTA) FL frameworks. The experimental results on five popular datasets and two modern DNN models illustrate that the proposed FedLPS significantly outperforms the SOTA FL frameworks by up to 4.88% and reduces the computational resource consumption by 21.3%. Our code is available at:https://github.com/jyzgh/FedLPS.","sentences":["Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices.","By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy.","Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity.","However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios.","In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap.","FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders.","To further reduce resource consumption, a channel-wise model pruning algorithm that shrinks the footprint of local models while accounting for both data and system heterogeneity is employed in FedLPS.","Additionally, a novel heterogeneous model aggregation algorithm is proposed to aggregate the heterogeneous predictors in FedLPS.","We implemented the proposed FedLPS on a real FL platform and compared it with state-of-the-art (SOTA) FL frameworks.","The experimental results on five popular datasets and two modern DNN models illustrate that the proposed FedLPS significantly outperforms the SOTA FL frameworks by up to 4.88% and reduces the computational resource consumption by 21.3%.","Our code is available at:https://github.com/jyzgh/FedLPS."],"url":"http://arxiv.org/abs/2402.08578v1","category":"cs.LG"}
{"created":"2024-02-13 16:21:18","title":"Two Tales of Single-Phase Contrastive Hebbian Learning","abstract":"The search for \"biologically plausible\" learning algorithms has converged on the idea of representing gradients as activity differences. However, most approaches require a high degree of synchronization (distinct phases during learning) and introduce substantial computational overhead, which raises doubts regarding their biological plausibility as well as their potential utility for neuromorphic computing. Furthermore, they commonly rely on applying infinitesimal perturbations (nudges) to output units, which is impractical in noisy environments. Recently it has been shown that by modelling artificial neurons as dyads with two oppositely nudged compartments, it is possible for a fully local learning algorithm named ``dual propagation'' to bridge the performance gap to backpropagation, without requiring separate learning phases or infinitesimal nudging. However, the algorithm has the drawback that its numerical stability relies on symmetric nudging, which may be restrictive in biological and analog implementations. In this work we first provide a solid foundation for the objective underlying the dual propagation method, which also reveals a surprising connection with adversarial robustness. Second, we demonstrate how dual propagation is related to a particular adjoint state method, which is stable regardless of asymmetric nudging.","sentences":["The search for \"biologically plausible\" learning algorithms has converged on the idea of representing gradients as activity differences.","However, most approaches require a high degree of synchronization (distinct phases during learning) and introduce substantial computational overhead, which raises doubts regarding their biological plausibility as well as their potential utility for neuromorphic computing.","Furthermore, they commonly rely on applying infinitesimal perturbations (nudges) to output units, which is impractical in noisy environments.","Recently it has been shown that by modelling artificial neurons as dyads with two oppositely nudged compartments, it is possible for a fully local learning algorithm named ``dual propagation'' to bridge the performance gap to backpropagation, without requiring separate learning phases or infinitesimal nudging.","However, the algorithm has the drawback that its numerical stability relies on symmetric nudging, which may be restrictive in biological and analog implementations.","In this work we first provide a solid foundation for the objective underlying the dual propagation method, which also reveals a surprising connection with adversarial robustness.","Second, we demonstrate how dual propagation is related to a particular adjoint state method, which is stable regardless of asymmetric nudging."],"url":"http://arxiv.org/abs/2402.08573v1","category":"cs.LG"}
{"created":"2024-02-13 16:17:24","title":"On the Topology $\u03c4_R^{\\diamond}$ of Primal Topological Spaces","abstract":"The main purpose of this paper is to introduce and study two new operators $(\\cdot)_R^{\\diamond}$ and $cl_R^{\\diamond}(\\cdot)$ via primal which is a new notion. We also show that the operator $cl_R^{\\diamond}(\\cdot)$ is a Kuratowski closure operator, while the operator $(\\cdot)_R^{\\diamond}$ is not. In addition, we prove that the topology on $X$, shown as $\\tau_R^{\\diamond},$ obtained by means of the operator $cl_R^{\\diamond}(\\cdot)$ is finer than $\\tau_{\\delta},$ where $\\tau_{\\delta}$ is the family of $\\delta$-open subsets of a space $(X,\\tau).$ Moreover, we not only obtain a base for the topology $\\tau_R^{\\diamond}$ but also prove many fundamental results concerning this new structure. Furthermore, we give many counterexamples related to our results.","sentences":["The main purpose of this paper is to introduce and study two new operators $(\\cdot)_R^{\\diamond}$ and $cl_R^{\\diamond}(\\cdot)$ via primal which is a new notion.","We also show that the operator $cl_R^{\\diamond}(\\cdot)$ is a Kuratowski closure operator, while the operator $(\\cdot)_R^{\\diamond}$ is not.","In addition, we prove that the topology on $X$, shown as $\\tau_R^{\\diamond},$ obtained by means of the operator $cl_R^{\\diamond}(\\cdot)$ is finer than $\\tau_{\\delta},$ where $\\tau_{\\delta}$ is the family of $\\delta$-open subsets of a space $(X,\\tau).$","Moreover, we not only obtain a base for the topology $\\tau_R^{\\diamond}$ but also prove many fundamental results concerning this new structure.","Furthermore, we give many counterexamples related to our results."],"url":"http://arxiv.org/abs/2402.08572v1","category":"math.GN"}
{"created":"2024-02-13 16:14:32","title":"Online Foundation Model Selection in Robotics","abstract":"Foundation models have recently expanded into robotics after excelling in computer vision and natural language processing. The models are accessible in two ways: open-source or paid, closed-source options. Users with access to both face a problem when deciding between effective yet costly closed-source models and free but less powerful open-source alternatives. We call it the model selection problem. Existing supervised-learning methods are impractical due to the high cost of collecting extensive training data from closed-source models. Hence, we focus on the online learning setting where algorithms learn while collecting data, eliminating the need for large pre-collected datasets. We thus formulate a user-centric online model selection problem and propose a novel solution that combines an open-source encoder to output context and an online learning algorithm that processes this context. The encoder distills vast data distributions into low-dimensional features, i.e., the context, without additional training. The online learning algorithm aims to maximize a composite reward that includes model performance, execution time, and costs based on the context extracted from the data. It results in an improved trade-off between selecting open-source and closed-source models compared to non-contextual methods, as validated by our theoretical analysis. Experiments across language-based robotic tasks such as Waymo Open Dataset, ALFRED, and Open X-Embodiment demonstrate real-world applications of the solution. The results show that the solution significantly improves the task success rate by up to 14%.","sentences":["Foundation models have recently expanded into robotics after excelling in computer vision and natural language processing.","The models are accessible in two ways: open-source or paid, closed-source options.","Users with access to both face a problem when deciding between effective yet costly closed-source models and free but less powerful open-source alternatives.","We call it the model selection problem.","Existing supervised-learning methods are impractical due to the high cost of collecting extensive training data from closed-source models.","Hence, we focus on the online learning setting where algorithms learn while collecting data, eliminating the need for large pre-collected datasets.","We thus formulate a user-centric online model selection problem and propose a novel solution that combines an open-source encoder to output context and an online learning algorithm that processes this context.","The encoder distills vast data distributions into low-dimensional features, i.e., the context, without additional training.","The online learning algorithm aims to maximize a composite reward that includes model performance, execution time, and costs based on the context extracted from the data.","It results in an improved trade-off between selecting open-source and closed-source models compared to non-contextual methods, as validated by our theoretical analysis.","Experiments across language-based robotic tasks such as Waymo Open Dataset, ALFRED, and Open X-Embodiment demonstrate real-world applications of the solution.","The results show that the solution significantly improves the task success rate by up to 14%."],"url":"http://arxiv.org/abs/2402.08570v1","category":"cs.RO"}
{"created":"2024-02-13 16:13:28","title":"Manifold functional multiple regression model with LRD error term","abstract":"This paper considers the problem of manifold functional multiple regression with functional response, time--varying scalar regressors, and functional error term displaying Long Range Dependence (LRD) in time. Specifically, the error term is given by a manifold multifractionally integrated functional time series (see, e.g., Ovalle--Mu\\~noz \\& Ruiz--Medina, 2024)). The manifold is defined by a connected and compact two--point homogeneous space. The functional regression parameters have support in the manifold. The Generalized Least--Squares (GLS) estimator of the vector functional regression parameter is computed, and its asymptotic properties are analyzed under a totally specified and misspecified model scenario. A multiscale residual correlation analysis in the simulation study undertaken illustrates the empirical distributional properties of the errors at different spherical resolution levels.","sentences":["This paper considers the problem of manifold functional multiple regression with functional response, time--varying scalar regressors, and functional error term displaying Long Range Dependence (LRD) in time.","Specifically, the error term is given by a manifold multifractionally integrated functional time series (see, e.g., Ovalle--Mu\\~noz \\& Ruiz--Medina, 2024)).","The manifold is defined by a connected and compact two--point homogeneous space.","The functional regression parameters have support in the manifold.","The Generalized Least--Squares (GLS) estimator of the vector functional regression parameter is computed, and its asymptotic properties are analyzed under a totally specified and misspecified model scenario.","A multiscale residual correlation analysis in the simulation study undertaken illustrates the empirical distributional properties of the errors at different spherical resolution levels."],"url":"http://arxiv.org/abs/2402.08569v1","category":"math.ST"}
{"created":"2024-02-13 16:11:30","title":"Convergence Analysis of a Variable Projection Method for Regularized Separable Nonlinear Inverse Problems","abstract":"Variable projection methods prove highly efficient in solving separable nonlinear least squares problems by transforming them into a reduced nonlinear least squares problem, typically solvable via the Gauss-Newton method. When solving large-scale separable nonlinear inverse problems with general-form Tikhonov regularization, the computational demand for computing Jacobians in the Gauss-Newton method becomes very challenging. To mitigate this, iterative methods, specifically LSQR, can be used as inner solvers to compute approximate Jacobians. This article analyzes the impact of these approximate Jacobians within the variable projection method and introduces stopping criteria to ensure convergence. We also present numerical experiments where we apply the proposed method to solve a blind deconvolution problem to illustrate and confirm our theoretical results.","sentences":["Variable projection methods prove highly efficient in solving separable nonlinear least squares problems by transforming them into a reduced nonlinear least squares problem, typically solvable via the Gauss-Newton method.","When solving large-scale separable nonlinear inverse problems with general-form Tikhonov regularization, the computational demand for computing Jacobians in the Gauss-Newton method becomes very challenging.","To mitigate this, iterative methods, specifically LSQR, can be used as inner solvers to compute approximate Jacobians.","This article analyzes the impact of these approximate Jacobians within the variable projection method and introduces stopping criteria to ensure convergence.","We also present numerical experiments where we apply the proposed method to solve a blind deconvolution problem to illustrate and confirm our theoretical results."],"url":"http://arxiv.org/abs/2402.08568v1","category":"math.NA"}
{"created":"2024-02-13 16:05:51","title":"Artificial Intelligence for Literature Reviews: Opportunities and Challenges","abstract":"This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research.","sentences":["This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs).","A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic.","Numerous tools have been developed to assist and partially automate the SLR process.","The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews.","Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases.","We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features.","We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing.","Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research."],"url":"http://arxiv.org/abs/2402.08565v1","category":"cs.AI"}
{"created":"2024-02-13 16:04:41","title":"Denoising Diffusion Restoration Tackles Forward and Inverse Problems for the Laplace Operator","abstract":"Diffusion models have emerged as a promising class of generative models that map noisy inputs to realistic images. More recently, they have been employed to generate solutions to partial differential equations (PDEs). However, they still struggle with inverse problems in the Laplacian operator, for instance, the Poisson equation, because the eigenvalues that are large in magnitude amplify the measurement noise. This paper presents a novel approach for the inverse and forward solution of PDEs through the use of denoising diffusion restoration models (DDRM). DDRMs were used in linear inverse problems to restore original clean signals by exploiting the singular value decomposition (SVD) of the linear operator. Equivalently, we present an approach to restore the solution and the parameters in the Poisson equation by exploiting the eigenvalues and the eigenfunctions of the Laplacian operator. Our results show that using denoising diffusion restoration significantly improves the estimation of the solution and parameters. Our research, as a result, pioneers the integration of diffusion models with the principles of underlying physics to solve PDEs.","sentences":["Diffusion models have emerged as a promising class of generative models that map noisy inputs to realistic images.","More recently, they have been employed to generate solutions to partial differential equations (PDEs).","However, they still struggle with inverse problems in the Laplacian operator, for instance, the Poisson equation, because the eigenvalues that are large in magnitude amplify the measurement noise.","This paper presents a novel approach for the inverse and forward solution of PDEs through the use of denoising diffusion restoration models (DDRM).","DDRMs were used in linear inverse problems to restore original clean signals by exploiting the singular value decomposition (SVD) of the linear operator.","Equivalently, we present an approach to restore the solution and the parameters in the Poisson equation by exploiting the eigenvalues and the eigenfunctions of the Laplacian operator.","Our results show that using denoising diffusion restoration significantly improves the estimation of the solution and parameters.","Our research, as a result, pioneers the integration of diffusion models with the principles of underlying physics to solve PDEs."],"url":"http://arxiv.org/abs/2402.08563v1","category":"cs.LG"}
{"created":"2024-02-13 16:04:21","title":"Higher Layers Need More LoRA Experts","abstract":"Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, \\textit{\\textbf{M}oE-L\\textbf{o}RA with \\textbf{L}ayer-wise Expert \\textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benchmarks demonstrate that MoLA achieves equal or superior performance compared to all baselines. We find that allocating more LoRA experts to higher layers further enhances the effectiveness of models with a certain number of experts in total. With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer. This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications. The code is available at https://github.com/GCYZSL/MoLA.","sentences":["Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited.","Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods.","Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages.","Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy.","Does this statement also apply to parameter-efficient MoE?","In this paper, we introduce a novel parameter-efficient MoE method, \\textit{\\textbf{M}oE-L\\textbf{o}RA with \\textbf{L}ayer-wise Expert \\textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts.","We investigate several architectures with varying layer-wise expert configurations.","Experiments on six well-known NLP and commonsense QA benchmarks demonstrate that MoLA achieves equal or superior performance compared to all baselines.","We find that allocating more LoRA experts to higher layers further enhances the effectiveness of models with a certain number of experts in total.","With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer.","This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications.","The code is available at https://github.com/GCYZSL/MoLA."],"url":"http://arxiv.org/abs/2402.08562v1","category":"cs.CL"}
{"created":"2024-02-13 16:03:17","title":"Data efficiency and long term prediction capabilities for neural operator surrogate models of core and edge plasma codes","abstract":"Simulation-based plasma scenario development, optimization and control are crucial elements towards the successful deployment of next-generation experimental tokamaks and Fusion power plants. Current simulation codes require extremely intensive use of HPC resources that make them unsuitable for iterative or real time applications. Neural network based surrogate models of expensive simulators have been proposed to speed up such costly workflows. Current efforts in this direction in the Fusion community are mostly limited to point estimates of quantities of interest or simple 1D PDE models, with a few notable exceptions. While the AI literature on methods for neural PDE surrogate models is rich, performance benchmarks for Fusion-relevant 2D fields has so far remained flimited. In this work neural PDE surrogates are trained for the JOREK MHD code and the STORM scrape-off layer code using the PDEArena library (https://github.com/microsoft/pdearena). The performance of these surrogate models is investigated as a function of training set size as well as for long-term predictions. The performance of surrogate models that are trained on either one variable or multiple variables at once is also considered. It is found that surrogates that are trained on more data perform best for both long- and short-term predictions. Additionally, surrogate models trained on multiple variables achieve higher accuracy and more stable performance. Downsampling the training set in time may provide stability in the long term at the expense of the short term predictive capability, but visual inspection of the resulting fields suggests that multiple metrics should be used to evaluate performance.","sentences":["Simulation-based plasma scenario development, optimization and control are crucial elements towards the successful deployment of next-generation experimental tokamaks and Fusion power plants.","Current simulation codes require extremely intensive use of HPC resources that make them unsuitable for iterative or real time applications.","Neural network based surrogate models of expensive simulators","have been proposed to speed up such costly workflows.","Current efforts in this direction in the Fusion community are mostly limited to point estimates of quantities of interest or simple 1D PDE models, with a few notable exceptions.","While the AI literature on methods for neural PDE surrogate models is rich, performance benchmarks for Fusion-relevant 2D fields has so far remained flimited.","In this work neural PDE surrogates are trained for the JOREK MHD code and the STORM scrape-off layer code using the PDEArena library (https://github.com/microsoft/pdearena).","The performance of these surrogate models is investigated as a function of training set size as well as for long-term predictions.","The performance of surrogate models that are trained on either one variable or multiple variables at once is also considered.","It is found that surrogates that are trained on more data perform best for both long- and short-term predictions.","Additionally, surrogate models trained on multiple variables achieve higher accuracy and more stable performance.","Downsampling the training set in time may provide stability in the long term at the expense of the short term predictive capability, but visual inspection of the resulting fields suggests that multiple metrics should be used to evaluate performance."],"url":"http://arxiv.org/abs/2402.08561v1","category":"physics.plasm-ph"}
{"created":"2024-02-13 16:01:21","title":"A conformal mapping approach to broadband nonlinear optics on chip","abstract":"Integrated nonlinear optical devices play an important role in modern optical communications. However, conventional on-chip optical devices with homogeneous or periodic translation dimensions generally have limited bandwidth when applied to nonlinear optical applications. Up today, there lacks a general method to design compact nonlinear optical devices over a broadband continuous frequency range. In this work, we propose a general strategy based on transformation optics (TO) to design curved accelerating waveguides (CAWs) with spatially gradient curvatures able to achieve broadband nonlinear frequency conversion on chip. Through rigorous analytical calculation, we show that increasing the acceleration (i.e. gradient in the waveguide curvature) broadens the output signal spectrum in the nonlinear process. In the experiment, we take the sum-frequency generation for infrared signal upconversion (SFG-ISU) as the example and fabricated a variety of CAWs using thin-film lithium niobate on insulator (LNOI). Efficient SFG is observed over a broadband continuous spectrum. Our conformal mapping approach offers a platform for various nonlinear optical processes and works in any frequency range, including visible, infrared and terahertz bands. Apart from LNOI, our approach is also compatible with other nonlinear materials, such as silicon, silicon nitride and chalcogenide glasses etc.","sentences":["Integrated nonlinear optical devices play an important role in modern optical communications.","However, conventional on-chip optical devices with homogeneous or periodic translation dimensions generally have limited bandwidth when applied to nonlinear optical applications.","Up today, there lacks a general method to design compact nonlinear optical devices over a broadband continuous frequency range.","In this work, we propose a general strategy based on transformation optics (TO) to design curved accelerating waveguides (CAWs) with spatially gradient curvatures able to achieve broadband nonlinear frequency conversion on chip.","Through rigorous analytical calculation, we show that increasing the acceleration (i.e. gradient in the waveguide curvature) broadens the output signal spectrum in the nonlinear process.","In the experiment, we take the sum-frequency generation for infrared signal upconversion (SFG-ISU) as the example and fabricated a variety of CAWs using thin-film lithium niobate on insulator (LNOI).","Efficient SFG is observed over a broadband continuous spectrum.","Our conformal mapping approach offers a platform for various nonlinear optical processes and works in any frequency range, including visible, infrared and terahertz bands.","Apart from LNOI, our approach is also compatible with other nonlinear materials, such as silicon, silicon nitride and chalcogenide glasses etc."],"url":"http://arxiv.org/abs/2402.08559v1","category":"physics.optics"}
{"created":"2024-02-13 15:57:15","title":"In-in correlators and scattering amplitudes on a causal set","abstract":"Causal set theory is an approach to quantum gravity in which spacetime is fundamentally discrete at the Planck scale and takes the form of a Lorentzian lattice, or \"casual set\", from which continuum spacetime emerges in a large-scale (low-energy) approximation. In this work, we present new developments in the framework of interacting quantum field theory on causal sets. We derive a diagrammatic expansion for in-in correlators in local scalar field theories with finite polynomial interactions. We outline how these same correlators can be computed using the double-path integral which acts as a generating functional for the in-in correlators. We modify the in-in generating functional to obtain a generating functional for in-out correlators. We define a notion of scattering amplitudes on causal sets with non-interacting past and future regions and verify that they are given by S-matrix elements (matrix elements of the time-evolution operator). We describe how these formal developments can be implemented to compute early universe observables under the assumption that spacetime is fundamentally discrete.","sentences":["Causal set theory is an approach to quantum gravity in which spacetime is fundamentally discrete at the Planck scale and takes the form of a Lorentzian lattice, or \"casual set\", from which continuum spacetime emerges in a large-scale (low-energy) approximation.","In this work, we present new developments in the framework of interacting quantum field theory on causal sets.","We derive a diagrammatic expansion for in-in correlators in local scalar field theories with finite polynomial interactions.","We outline how these same correlators can be computed using the double-path integral which acts as a generating functional for the in-in correlators.","We modify the in-in generating functional to obtain a generating functional for in-out correlators.","We define a notion of scattering amplitudes on causal sets with non-interacting past and future regions and verify that they are given by S-matrix elements (matrix elements of the time-evolution operator).","We describe how these formal developments can be implemented to compute early universe observables under the assumption that spacetime is fundamentally discrete."],"url":"http://arxiv.org/abs/2402.08555v1","category":"hep-th"}
{"created":"2024-02-13 15:56:13","title":"On Harder-Narasimhan slopes of direct images","abstract":"For a polarized family of complex projective manifolds, we study the asymptotic distribution of Harder-Narasimhan slopes of direct image sheaves associated with high tensor powers of the polarization. We establish a theorem of Mehta-Ramanathan type, showing that this asymptotic distribution can be recovered from the analogous asymptotic distributions associated with base changes of the family over generic curves.","sentences":["For a polarized family of complex projective manifolds, we study the asymptotic distribution of Harder-Narasimhan slopes of direct image sheaves associated with high tensor powers of the polarization.","We establish a theorem of Mehta-Ramanathan type, showing that this asymptotic distribution can be recovered from the analogous asymptotic distributions associated with base changes of the family over generic curves."],"url":"http://arxiv.org/abs/2402.08554v1","category":"math.AG"}
{"created":"2024-02-13 15:55:41","title":"Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases","abstract":"Bridging the gap between diffusion models and human preferences is crucial for their integration into practical generative workflows. While optimizing downstream reward models has emerged as a promising alignment strategy, concerns arise regarding the risk of excessive optimization with learned reward models, which potentially compromises ground-truth performance. In this work, we confront the reward overoptimization problem in diffusion model alignment through the lenses of both inductive and primacy biases. We first identify the divergence of current methods from the temporal inductive bias inherent in the multi-step denoising process of diffusion models as a potential source of overoptimization. Then, we surprisingly discover that dormant neurons in our critic model act as a regularization against overoptimization, while active neurons reflect primacy bias in this setting. Motivated by these observations, we propose Temporal Diffusion Policy Optimization with critic active neuron Reset (TDPO-R), a policy gradient algorithm that exploits the temporal inductive bias of intermediate timesteps, along with a novel reset strategy that targets active neurons to counteract the primacy bias. Empirical results demonstrate the superior efficacy of our algorithms in mitigating reward overoptimization.","sentences":["Bridging the gap between diffusion models and human preferences is crucial for their integration into practical generative workflows.","While optimizing downstream reward models has emerged as a promising alignment strategy, concerns arise regarding the risk of excessive optimization with learned reward models, which potentially compromises ground-truth performance.","In this work, we confront the reward overoptimization problem in diffusion model alignment through the lenses of both inductive and primacy biases.","We first identify the divergence of current methods from the temporal inductive bias inherent in the multi-step denoising process of diffusion models as a potential source of overoptimization.","Then, we surprisingly discover that dormant neurons in our critic model act as a regularization against overoptimization, while active neurons reflect primacy bias in this setting.","Motivated by these observations, we propose Temporal Diffusion Policy Optimization with critic active neuron Reset (TDPO-R), a policy gradient algorithm that exploits the temporal inductive bias of intermediate timesteps, along with a novel reset strategy that targets active neurons to counteract the primacy bias.","Empirical results demonstrate the superior efficacy of our algorithms in mitigating reward overoptimization."],"url":"http://arxiv.org/abs/2402.08552v1","category":"cs.LG"}
{"created":"2024-02-13 15:54:31","title":"Competitive Revenue Extraction from Time-Discounted Transactions in the Semi-Myopic Regime","abstract":"Decentralized cryptocurrencies are payment systems that rely on aligning the incentives of users and miners to operate correctly and offer a high quality of service to users. Recent literature studies the mechanism design problem of the auction serving as a cryptocurrency's transaction fee mechanism (TFM). We find that a non-myopic modelling of miners falls close to another well-known problem: that of online buffer management for packet switching. The main difference is that unlike packets which are of a fixed size throughout their lifetime, in a financial environment, user preferences (and therefore revenue extraction) may be time-dependent. We study the competitive ratio guarantees given a certain discount rate, and show how existing methods from packet scheduling, which we call \"the undiscounted case\", perform suboptimally in the more general discounted setting. Most notably, we find a novel, simple, memoryless, and optimal deterministic algorithm for the semi-myopic case, when the discount factor is up to ~0.770018. We also present a randomized algorithm that achieves better performance than the best possible deterministic algorithm, for any discount rate.","sentences":["Decentralized cryptocurrencies are payment systems that rely on aligning the incentives of users and miners to operate correctly and offer a high quality of service to users.","Recent literature studies the mechanism design problem of the auction serving as a cryptocurrency's transaction fee mechanism (TFM).","We find that a non-myopic modelling of miners falls close to another well-known problem: that of online buffer management for packet switching.","The main difference is that unlike packets which are of a fixed size throughout their lifetime, in a financial environment, user preferences (and therefore revenue extraction) may be time-dependent.","We study the competitive ratio guarantees given a certain discount rate, and show how existing methods from packet scheduling, which we call \"the undiscounted case\", perform suboptimally in the more general discounted setting.","Most notably, we find a novel, simple, memoryless, and optimal deterministic algorithm for the semi-myopic case, when the discount factor is up to ~0.770018.","We also present a randomized algorithm that achieves better performance than the best possible deterministic algorithm, for any discount rate."],"url":"http://arxiv.org/abs/2402.08549v1","category":"cs.GT"}
{"created":"2024-02-13 15:54:31","title":"Motion-Adaptive Inference for Flexible Learned B-Frame Compression","abstract":"While the performance of recent learned intra and sequential video compression models exceed that of respective traditional codecs, the performance of learned B-frame compression models generally lag behind traditional B-frame coding. The performance gap is bigger for complex scenes with large motions. This is related to the fact that the distance between the past and future references vary in hierarchical B-frame compression depending on the level of hierarchy, which causes motion range to vary. The inability of a single B-frame compression model to adapt to various motion ranges causes loss of performance. As a remedy, we propose controlling the motion range for flow prediction during inference (to approximately match the range of motions in the training data) by downsampling video frames adaptively according to amount of motion and level of hierarchy in order to compress all B-frames using a single flexible-rate model. We present state-of-the-art BD rate results to demonstrate the superiority of our proposed single-model motion-adaptive inference approach to all existing learned B-frame compression models.","sentences":["While the performance of recent learned intra and sequential video compression models exceed that of respective traditional codecs, the performance of learned B-frame compression models generally lag behind traditional B-frame coding.","The performance gap is bigger for complex scenes with large motions.","This is related to the fact that the distance between the past and future references vary in hierarchical B-frame compression depending on the level of hierarchy, which causes motion range to vary.","The inability of a single B-frame compression model to adapt to various motion ranges causes loss of performance.","As a remedy, we propose controlling the motion range for flow prediction during inference (to approximately match the range of motions in the training data) by downsampling video frames adaptively according to amount of motion and level of hierarchy in order to compress all B-frames using a single flexible-rate model.","We present state-of-the-art BD rate results to demonstrate the superiority of our proposed single-model motion-adaptive inference approach to all existing learned B-frame compression models."],"url":"http://arxiv.org/abs/2402.08550v1","category":"eess.IV"}
{"created":"2024-02-13 15:53:41","title":"Branching Interval Partition Diffusions","abstract":"We introduce and study branching interval partition diffusions in their natural generality. We let interval widths evolve independently according to a general real-valued diffusion subject only to conditions that ensure finite lifetimes of intervals and allow the continuous generation of new intervals. The latter is governed by the Pitman-Yor excursion measure of the real-valued diffusion and an associated spectrally positive L\\'evy process to order both these excursions and their start times. This generalises previous work by Forman, Pal, Rizzolo and Winkel on the self-similar case and gives rise to a new class of general Markovian homogeneous Crump-Mode-Jagers-type branching processes with characteristics varying diffusively during their lifetimes.","sentences":["We introduce and study branching interval partition diffusions in their natural generality.","We let interval widths evolve independently according to a general real-valued diffusion subject only to conditions that ensure finite lifetimes of intervals and allow the continuous generation of new intervals.","The latter is governed by the Pitman-Yor excursion measure of the real-valued diffusion and an associated spectrally positive L\\'evy process to order both these excursions and their start times.","This generalises previous work by Forman, Pal, Rizzolo and Winkel on the self-similar case and gives rise to a new class of general Markovian homogeneous Crump-Mode-Jagers-type branching processes with characteristics varying diffusively during their lifetimes."],"url":"http://arxiv.org/abs/2402.08548v1","category":"math.PR"}
{"created":"2024-02-13 15:53:09","title":"Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting","abstract":"We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice. We consider two versions: sequential, where Bob observes Alice's cut point before choosing left/right, and simultaneous, where he only observes her cut point after making his choice. The simultaneous version was first considered by Aumann and Maschler (1995).   We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.   We analyze the limits of how much a player can exploit the other one and show that fair utility profiles are in fact achievable. Specifically, the players can enforce the equitable utility profile of $(1/2, 1/2)$ in the limit on every trajectory of play, by keeping the other player's utility to approximately $1/2$ on average while guaranteeing they themselves get at least approximately $1/2$ on average. We show this theorem using a connection with Blackwell approachability.   Finally, we analyze a natural dynamic known as fictitious play, where players best respond to the empirical distribution of the other player. We show that fictitious play converges to the equitable utility profile of $(1/2, 1/2)$ at a rate of $O(1/\\sqrt{T})$.","sentences":["We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake.","In each round, a new cake arrives, which is identical to the ones in previous rounds.","Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice.","We consider two versions: sequential, where Bob observes Alice's cut point before choosing left/right, and simultaneous, where he only observes her cut point after making his choice.","The simultaneous version was first considered by Aumann and Maschler (1995).   ","We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search.","This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.   ","We analyze the limits of how much a player can exploit the other one and show that fair utility profiles are in fact achievable.","Specifically, the players can enforce the equitable utility profile of $(1/2, 1/2)$ in the limit on every trajectory of play, by keeping the other player's utility to approximately $1/2$ on average while guaranteeing they themselves get at least approximately $1/2$ on average.","We show this theorem using a connection with Blackwell approachability.   ","Finally, we analyze a natural dynamic known as fictitious play, where players best respond to the empirical distribution of the other player.","We show that fictitious play converges to the equitable utility profile of $(1/2, 1/2)$ at a rate of $O(1/\\sqrt{T})$."],"url":"http://arxiv.org/abs/2402.08547v1","category":"cs.GT"}
{"created":"2024-02-13 15:50:32","title":"On the past maximal development of near-FLRW data for the Einstein scalar-field Vlasov system","abstract":"We show that the maximal globally hyperbolic development of near-FLRW initial data for the Einstein scalar-field Vlasov system exhibits stable Big Bang formation in the collapsing direction, i.e., stable Kretschmann scalar blow-up, causing the spacetime to become past incomplete. This is the first stability result towards the collapsing direction in the presence of Vlasov matter. Furthermore, while the asymptotic behaviour of Vlasov matter is close to that of the FLRW solution up to a small change in asymptotic order controlled by initial data, generically, the distribution function itself asymptotically concentrates in certain preferred directions that are teleologically determined. To ensure that this behaviour is sufficiently mitigated by the scalar field, we crucially exploit a scaling hierarchy between horizontal and vertical derivatives in the Vlasov equation.","sentences":["We show that the maximal globally hyperbolic development of near-FLRW initial data for the Einstein scalar-field Vlasov system exhibits stable Big Bang formation in the collapsing direction, i.e., stable Kretschmann scalar blow-up, causing the spacetime to become past incomplete.","This is the first stability result towards the collapsing direction in the presence of Vlasov matter.","Furthermore, while the asymptotic behaviour of Vlasov matter is close to that of the FLRW solution up to a small change in asymptotic order controlled by initial data, generically, the distribution function itself asymptotically concentrates in certain preferred directions that are teleologically determined.","To ensure that this behaviour is sufficiently mitigated by the scalar field, we crucially exploit a scaling hierarchy between horizontal and vertical derivatives in the Vlasov equation."],"url":"http://arxiv.org/abs/2402.08544v1","category":"gr-qc"}
{"created":"2024-02-13 15:48:10","title":"Theoretical Analysis of Leave-one-out Cross Validation for Non-differentiable Penalties under High-dimensional Settings","abstract":"Despite a large and significant body of recent work focused on estimating the out-of-sample risk of regularized models in the high dimensional regime, a theoretical understanding of this problem for non-differentiable penalties such as generalized LASSO and nuclear norm is missing. In this paper we resolve this challenge. We study this problem in the proportional high dimensional regime where both the sample size n and number of features p are large, and n/p and the signal-to-noise ratio (per observation) remain finite. We provide finite sample upper bounds on the expected squared error of leave-one-out cross-validation (LO) in estimating the out-of-sample risk. The theoretical framework presented here provides a solid foundation for elucidating empirical findings that show the accuracy of LO.","sentences":["Despite a large and significant body of recent work focused on estimating the out-of-sample risk of regularized models in the high dimensional regime, a theoretical understanding of this problem for non-differentiable penalties such as generalized LASSO and nuclear norm is missing.","In this paper we resolve this challenge.","We study this problem in the proportional high dimensional regime where both the sample size n and number of features p are large, and n/p and the signal-to-noise ratio (per observation) remain finite.","We provide finite sample upper bounds on the expected squared error of leave-one-out cross-validation (LO) in estimating the out-of-sample risk.","The theoretical framework presented here provides a solid foundation for elucidating empirical findings that show the accuracy of LO."],"url":"http://arxiv.org/abs/2402.08543v1","category":"math.ST"}
{"created":"2024-02-13 15:45:20","title":"Generative VS non-Generative Models in Engineering Shape Optimization","abstract":"In this work, we perform a systematic comparison of the effectiveness and efficiency of generative and non-generative models in constructing design spaces for novel and efficient design exploration and shape optimization. We apply these models in the case of airfoil/hydrofoil design and conduct the comparison on the resulting design spaces. A conventional Generative Adversarial Network (GAN) and a state-of-the-art generative model, the Performance-Augmented Diverse Generative Adversarial Network (PaDGAN), are juxtaposed with a linear non-generative model based on the coupling of the Karhunen-Lo\\`eve Expansion and a physics-informed Shape Signature Vector (SSV-KLE). The comparison demonstrates that, with an appropriate shape encoding and a physics-augmented design space, non-generative models have the potential to cost-effectively generate high-performing valid designs with enhanced coverage of the design space. In this work, both approaches are applied to two large foil profile datasets comprising real-world and artificial designs generated through either a profile-generating parametric model or deep-learning approach. These datasets are further enriched with integral properties of their members' shapes as well as physics-informed parameters. Our results illustrate that the design spaces constructed by the non-generative model outperform the generative model in terms of design validity, generating robust latent spaces with none or significantly fewer invalid designs when compared to generative models. We aspire that these findings will aid the engineering design community in making informed decisions when constructing designs spaces for shape optimization, as we have show that under certain conditions computationally inexpensive approaches can closely match or even outperform state-of-the art generative models.","sentences":["In this work, we perform a systematic comparison of the effectiveness and efficiency of generative and non-generative models in constructing design spaces for novel and efficient design exploration and shape optimization.","We apply these models in the case of airfoil/hydrofoil design and conduct the comparison on the resulting design spaces.","A conventional Generative Adversarial Network (GAN) and a state-of-the-art generative model, the Performance-Augmented Diverse Generative Adversarial Network (PaDGAN), are juxtaposed with a linear non-generative model based on the coupling of the Karhunen-Lo\\`eve Expansion and a physics-informed Shape Signature Vector (SSV-KLE).","The comparison demonstrates that, with an appropriate shape encoding and a physics-augmented design space, non-generative models have the potential to cost-effectively generate high-performing valid designs with enhanced coverage of the design space.","In this work, both approaches are applied to two large foil profile datasets comprising real-world and artificial designs generated through either a profile-generating parametric model or deep-learning approach.","These datasets are further enriched with integral properties of their members' shapes as well as physics-informed parameters.","Our results illustrate that the design spaces constructed by the non-generative model outperform the generative model in terms of design validity, generating robust latent spaces with none or significantly fewer invalid designs when compared to generative models.","We aspire that these findings will aid the engineering design community in making informed decisions when constructing designs spaces for shape optimization, as we have show that under certain conditions computationally inexpensive approaches can closely match or even outperform state-of-the art generative models."],"url":"http://arxiv.org/abs/2402.08540v1","category":"cs.LG"}
{"created":"2024-02-13 15:43:30","title":"Intelligent Diagnosis of Alzheimer's Disease Based on Machine Learning","abstract":"This study is based on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and aims to explore early detection and disease progression in Alzheimer's disease (AD). We employ innovative data preprocessing strategies, including the use of the random forest algorithm to fill missing data and the handling of outliers and invalid data, thereby fully mining and utilizing these limited data resources. Through Spearman correlation coefficient analysis, we identify some features strongly correlated with AD diagnosis. We build and test three machine learning models using these features: random forest, XGBoost, and support vector machine (SVM). Among them, the XGBoost model performs the best in terms of diagnostic performance, achieving an accuracy of 91%. Overall, this study successfully overcomes the challenge of missing data and provides valuable insights into early detection of Alzheimer's disease, demonstrating its unique research value and practical significance.","sentences":["This study is based on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and aims to explore early detection and disease progression in Alzheimer's disease (AD).","We employ innovative data preprocessing strategies, including the use of the random forest algorithm to fill missing data and the handling of outliers and invalid data, thereby fully mining and utilizing these limited data resources.","Through Spearman correlation coefficient analysis, we identify some features strongly correlated with AD diagnosis.","We build and test three machine learning models using these features: random forest, XGBoost, and support vector machine (SVM).","Among them, the XGBoost model performs the best in terms of diagnostic performance, achieving an accuracy of 91%.","Overall, this study successfully overcomes the challenge of missing data and provides valuable insights into early detection of Alzheimer's disease, demonstrating its unique research value and practical significance."],"url":"http://arxiv.org/abs/2402.08539v1","category":"cs.LG"}
{"created":"2024-02-13 15:36:39","title":"Captions Are Worth a Thousand Words: Enhancing Product Retrieval with Pretrained Image-to-Text Models","abstract":"This paper explores the usage of multimodal image-to-text models to enhance text-based item retrieval. We propose utilizing pre-trained image captioning and tagging models, such as instructBLIP and CLIP, to generate text-based product descriptions which are combined with existing text descriptions. Our work is particularly impactful for smaller eCommerce businesses who are unable to maintain the high-quality text descriptions necessary to effectively perform item retrieval for search and recommendation use cases. We evaluate the searchability of ground-truth text, image-generated text, and combinations of both texts on several subsets of Amazon's publicly available ESCI dataset. The results demonstrate the dual capability of our proposed models to enhance the retrieval of existing text and generate highly-searchable standalone descriptions.","sentences":["This paper explores the usage of multimodal image-to-text models to enhance text-based item retrieval.","We propose utilizing pre-trained image captioning and tagging models, such as instructBLIP and CLIP, to generate text-based product descriptions which are combined with existing text descriptions.","Our work is particularly impactful for smaller eCommerce businesses who are unable to maintain the high-quality text descriptions necessary to effectively perform item retrieval for search and recommendation use cases.","We evaluate the searchability of ground-truth text, image-generated text, and combinations of both texts on several subsets of Amazon's publicly available ESCI dataset.","The results demonstrate the dual capability of our proposed models to enhance the retrieval of existing text and generate highly-searchable standalone descriptions."],"url":"http://arxiv.org/abs/2402.08532v1","category":"cs.IR"}
{"created":"2024-02-13 15:35:24","title":"A Distributional Analogue to the Successor Representation","abstract":"This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible.","sentences":["This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process.","Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour.","We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning.","Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy.","Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state.","As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible."],"url":"http://arxiv.org/abs/2402.08530v1","category":"cs.LG"}
{"created":"2024-02-13 15:34:39","title":"Approximately Piecewise E(3) Equivariant Point Networks","abstract":"Integrating a notion of symmetry into point cloud neural networks is a provably effective way to improve their generalization capability. Of particular interest are $E(3)$ equivariant point cloud networks where Euclidean transformations applied to the inputs are preserved in the outputs. Recent efforts aim to extend networks that are $E(3)$ equivariant, to accommodate inputs made of multiple parts, each of which exhibits local $E(3)$ symmetry. In practical settings, however, the partitioning into individually transforming regions is unknown a priori. Errors in the partition prediction would unavoidably map to errors in respecting the true input symmetry. Past works have proposed different ways to predict the partition, which may exhibit uncontrolled errors in their ability to maintain equivariance to the actual partition. To this end, we introduce APEN: a general framework for constructing approximate piecewise-$E(3)$ equivariant point networks. Our primary insight is that functions that are equivariant with respect to a finer partition will also maintain equivariance in relation to the true partition. Leveraging this observation, we propose a design where the equivariance approximation error at each layers can be bounded solely in terms of (i) uncertainty quantification of the partition prediction, and (ii) bounds on the probability of failing to suggest a proper subpartition of the ground truth one. We demonstrate the effectiveness of APEN using two data types exemplifying part-based symmetry: (i) real-world scans of room scenes containing multiple furniture-type objects; and, (ii) human motions, characterized by articulated parts exhibiting rigid movement. Our empirical results demonstrate the advantage of integrating piecewise $E(3)$ symmetry into network design, showing a distinct improvement in generalization compared to prior works for both classification and segmentation tasks.","sentences":["Integrating a notion of symmetry into point cloud neural networks is a provably effective way to improve their generalization capability.","Of particular interest are $E(3)$ equivariant point cloud networks where Euclidean transformations applied to the inputs are preserved in the outputs.","Recent efforts aim to extend networks that are $E(3)$ equivariant, to accommodate inputs made of multiple parts, each of which exhibits local $E(3)$ symmetry.","In practical settings, however, the partitioning into individually transforming regions is unknown a priori.","Errors in the partition prediction would unavoidably map to errors in respecting the true input symmetry.","Past works have proposed different ways to predict the partition, which may exhibit uncontrolled errors in their ability to maintain equivariance to the actual partition.","To this end, we introduce APEN: a general framework for constructing approximate piecewise-$E(3)$ equivariant point networks.","Our primary insight is that functions that are equivariant with respect to a finer partition will also maintain equivariance in relation to the true partition.","Leveraging this observation, we propose a design where the equivariance approximation error at each layers can be bounded solely in terms of (i) uncertainty quantification of the partition prediction, and (ii) bounds on the probability of failing to suggest a proper subpartition of the ground truth one.","We demonstrate the effectiveness of APEN using two data types exemplifying part-based symmetry: (i) real-world scans of room scenes containing multiple furniture-type objects; and, (ii) human motions, characterized by articulated parts exhibiting rigid movement.","Our empirical results demonstrate the advantage of integrating piecewise $E(3)$ symmetry into network design, showing a distinct improvement in generalization compared to prior works for both classification and segmentation tasks."],"url":"http://arxiv.org/abs/2402.08529v1","category":"cs.LG"}
{"created":"2024-02-13 15:29:50","title":"Concept-1K: A Novel Benchmark for Instance Incremental Learning","abstract":"Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts are publicly available at https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.","sentences":["Incremental learning (IL) is essential to realize the human-level intelligence in the neural network.","However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting.","To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps.","Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size.","Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance.","Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs.","The data, code and scripts are publicly available at https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning."],"url":"http://arxiv.org/abs/2402.08526v1","category":"cs.LG"}
{"created":"2024-02-13 15:28:50","title":"Moments in the exact summation of the curious series of Kempner type","abstract":"We provide an exact geometrically convergent formula for the summation of the general Kempner series, i.e. the sum of all reciprocals allowed only certain digits in a given base. The coefficients are built from finite power sums and moments of a certain measure. The moments obey a linear recurrence and a priori bounds which means the theory can be immediately converted into an efficient numerical algorithm.","sentences":["We provide an exact geometrically convergent formula for the summation of the general Kempner series, i.e. the sum of all reciprocals allowed only certain digits in a given base.","The coefficients are built from finite power sums and moments of a certain measure.","The moments obey a linear recurrence and a priori bounds which means the theory can be immediately converted into an efficient numerical algorithm."],"url":"http://arxiv.org/abs/2402.08525v1","category":"math.NT"}
{"created":"2024-02-13 15:24:46","title":"Fairness Auditing with Multi-Agent Collaboration","abstract":"Existing work in fairness audits assumes that agents operate independently. In this paper, we consider the case of multiple agents auditing the same platform for different tasks. Agents have two levers: their collaboration strategy, with or without coordination beforehand, and their sampling method. We theoretically study their interplay when agents operate independently or collaborate. We prove that, surprisingly, coordination can sometimes be detrimental to audit accuracy, whereas uncoordinated collaboration generally yields good results. Experimentation on real-world datasets confirms this observation, as the audit accuracy of uncoordinated collaboration matches that of collaborative optimal sampling.","sentences":["Existing work in fairness audits assumes that agents operate independently.","In this paper, we consider the case of multiple agents auditing the same platform for different tasks.","Agents have two levers: their collaboration strategy, with or without coordination beforehand, and their sampling method.","We theoretically study their interplay when agents operate independently or collaborate.","We prove that, surprisingly, coordination can sometimes be detrimental to audit accuracy, whereas uncoordinated collaboration generally yields good results.","Experimentation on real-world datasets confirms this observation, as the audit accuracy of uncoordinated collaboration matches that of collaborative optimal sampling."],"url":"http://arxiv.org/abs/2402.08522v1","category":"cs.LG"}
{"created":"2024-02-13 15:20:44","title":"Hyperballistic transport in dense ionized matter under external AC electric fields","abstract":"The Langevin equation is ubiquitously employed to numerically simulate plasmas and dusty plasmas. However, the usual assumption of white noise becomes untenable when the system is subject to an external AC electric field. This is because the charged particles in the plasma, which provide the thermal bath for the particle transport, become themselves responsive to the AC field and the thermal noise is field-dependent and non-Markovian. We theoretically study the particle diffusivity in a Langevin transport model for a tagged charged particle immersed in a dense plasma of charged particles that act as the thermal bath, under an external AC electric field, by properly accounting for the effects of the AC field on the thermal bath statistics. We analytically derive the time-dependent generalized diffusivity $D(t)$ for different initial conditions. The generalized diffusivity exhibits damped oscillatory-like behaviour with initial very large peaks, where the generalized diffusion coefficient is enhanced by orders of magnitude with respect to the infinite-time steady-state value. The latter coincides with the Stokes-Einstein diffusivity in the absence of external field. For initial conditions where the external field is already on at $t=0$ and the system is thermalized under DC conditions for $t \\leq 0$, the short-time behaviour is hyperballistic, $MSD \\sim t^4$ (where MSD is the mean-squared displacement), leading to giant enhancement of the particle transport. Finally, the theory elucidates the role of medium polarization on the local Lorentz field, and allows for estimates of the effective electric charge due to polarization by the surrounding charges.","sentences":["The Langevin equation is ubiquitously employed to numerically simulate plasmas and dusty plasmas.","However, the usual assumption of white noise becomes untenable when the system is subject to an external AC electric field.","This is because the charged particles in the plasma, which provide the thermal bath for the particle transport, become themselves responsive to the AC field and the thermal noise is field-dependent and non-Markovian.","We theoretically study the particle diffusivity in a Langevin transport model for a tagged charged particle immersed in a dense plasma of charged particles that act as the thermal bath, under an external AC electric field, by properly accounting for the effects of the AC field on the thermal bath statistics.","We analytically derive the time-dependent generalized diffusivity $D(t)$ for different initial conditions.","The generalized diffusivity exhibits damped oscillatory-like behaviour with initial very large peaks, where the generalized diffusion coefficient is enhanced by orders of magnitude with respect to the infinite-time steady-state value.","The latter coincides with the Stokes-Einstein diffusivity in the absence of external field.","For initial conditions where the external field is already on at $t=0$ and the system is thermalized under DC conditions for $t \\leq 0$, the short-time behaviour is hyperballistic, $MSD \\sim t^4$ (where MSD is the mean-squared displacement), leading to giant enhancement of the particle transport.","Finally, the theory elucidates the role of medium polarization on the local Lorentz field, and allows for estimates of the effective electric charge due to polarization by the surrounding charges."],"url":"http://arxiv.org/abs/2402.08519v1","category":"physics.plasm-ph"}
{"created":"2024-02-13 15:15:48","title":"Path integral Lindblad master equation through transfer tensor method & the generalized quantum master equation","abstract":"Path integrals have, over the years, proven to be an extremely versatile tool for simulating the dynamics of open quantum systems. The initial limitations of applicability of these methods in terms of the size of the system has steadily been overcome through various developments, making numerical explorations of large systems a more-or-less regular feature. However, these simulations necessitate a detailed description of the system-environment interaction through accurate spectral densities, which are often difficult to obtain. Additionally, for several processes, such as spontaneous emission, one only has access to a rough estimation of an empirical timescale, and it is not possible to really define a proper spectral density at all. In this communication, an approach of incorporating such processes within an exact path integral description of other dissipative modes is developed through the Nakajima-Zwanzig master equations. This method will allow for a numerically exact non-perturbative inclusion of the degrees of freedom that are properly described by a bath using path integrals, while incorporating the empirical time scale through the Lindblad master equation. The cost of this approach is dominated by the cost of the path integral method used, and the impact of the Lindbladian terms is effectively obtained for free. This path integral Lindblad dynamics method is demonstrated with the example of electronic excitation transfer in a 4-site model of the Fenna-Matthews-Olson complex with the exciton has a propensity of being \"lost\" to the charge transfer state at the third chromophore. The impact of different time-scales of abstraction of the exciton is illustrated at no extra cost.","sentences":["Path integrals have, over the years, proven to be an extremely versatile tool for simulating the dynamics of open quantum systems.","The initial limitations of applicability of these methods in terms of the size of the system has steadily been overcome through various developments, making numerical explorations of large systems a more-or-less regular feature.","However, these simulations necessitate a detailed description of the system-environment interaction through accurate spectral densities, which are often difficult to obtain.","Additionally, for several processes, such as spontaneous emission, one only has access to a rough estimation of an empirical timescale, and it is not possible to really define a proper spectral density at all.","In this communication, an approach of incorporating such processes within an exact path integral description of other dissipative modes is developed through the Nakajima-Zwanzig master equations.","This method will allow for a numerically exact non-perturbative inclusion of the degrees of freedom that are properly described by a bath using path integrals, while incorporating the empirical time scale through the Lindblad master equation.","The cost of this approach is dominated by the cost of the path integral method used, and the impact of the Lindbladian terms is effectively obtained for free.","This path integral Lindblad dynamics method is demonstrated with the example of electronic excitation transfer in a 4-site model of the Fenna-Matthews-Olson complex with the exciton has a propensity of being \"lost\" to the charge transfer state at the third chromophore.","The impact of different time-scales of abstraction of the exciton is illustrated at no extra cost."],"url":"http://arxiv.org/abs/2402.08518v1","category":"quant-ph"}
{"created":"2024-02-13 15:12:37","title":"A Krylov Eigenvalue Solver Based on Filtered Time Domain Solutions","abstract":"This paper introduces a method for computing eigenvalues and eigenvectors of a generalized Hermitian, matrix eigenvalue problem. The work is focused on large scale eigenvalue problems, where the application of a direct inverse is out of reach. Instead, an explicit time-domain integrator for the corresponding wave problem is combined with a proper filtering and a Krylov iteration in order to solve for eigenvalues within a given region of interest. We report results of small scale model problems to confirm the reliability of the method, as well as the computation of acoustic resonances in a three dimensional model of a hunting horn to demonstrate the efficiency.","sentences":["This paper introduces a method for computing eigenvalues and eigenvectors of a generalized Hermitian, matrix eigenvalue problem.","The work is focused on large scale eigenvalue problems, where the application of a direct inverse is out of reach.","Instead, an explicit time-domain integrator for the corresponding wave problem is combined with a proper filtering and a Krylov iteration in order to solve for eigenvalues within a given region of interest.","We report results of small scale model problems to confirm the reliability of the method, as well as the computation of acoustic resonances in a three dimensional model of a hunting horn to demonstrate the efficiency."],"url":"http://arxiv.org/abs/2402.08515v1","category":"math.NA"}
{"created":"2024-02-13 15:10:30","title":"Counterfactual Influence in Markov Decision Processes","abstract":"Our work addresses a fundamental problem in the context of counterfactual inference for Markov Decision Processes (MDPs). Given an MDP path $\\tau$, this kind of inference allows us to derive counterfactual paths $\\tau'$ describing what-if versions of $\\tau$ obtained under different action sequences than those observed in $\\tau$. However, as the counterfactual states and actions deviate from the observed ones over time, the observation $\\tau$ may no longer influence the counterfactual world, meaning that the analysis is no longer tailored to the individual observation, resulting in interventional outcomes rather than counterfactual ones. Even though this issue specifically affects the popular Gumbel-max structural causal model used for MDP counterfactuals, it has remained overlooked until now. In this work, we introduce a formal characterisation of influence based on comparing counterfactual and interventional distributions. We devise an algorithm to construct counterfactual models that automatically satisfy influence constraints. Leveraging such models, we derive counterfactual policies that are not just optimal for a given reward structure but also remain tailored to the observed path. Even though there is an unavoidable trade-off between policy optimality and strength of influence constraints, our experiments demonstrate that it is possible to derive (near-)optimal policies while remaining under the influence of the observation.","sentences":["Our work addresses a fundamental problem in the context of counterfactual inference for Markov Decision Processes (MDPs).","Given an MDP path $\\tau$, this kind of inference allows us to derive counterfactual paths $\\tau'$ describing what-if versions of $\\tau$ obtained under different action sequences than those observed in $\\tau$. However, as the counterfactual states and actions deviate from the observed ones over time, the observation $\\tau$ may no longer influence the counterfactual world, meaning that the analysis is no longer tailored to the individual observation, resulting in interventional outcomes rather than counterfactual ones.","Even though this issue specifically affects the popular Gumbel-max structural causal model used for MDP counterfactuals, it has remained overlooked until now.","In this work, we introduce a formal characterisation of influence based on comparing counterfactual and interventional distributions.","We devise an algorithm to construct counterfactual models that automatically satisfy influence constraints.","Leveraging such models, we derive counterfactual policies that are not just optimal for a given reward structure but also remain tailored to the observed path.","Even though there is an unavoidable trade-off between policy optimality and strength of influence constraints, our experiments demonstrate that it is possible to derive (near-)optimal policies while remaining under the influence of the observation."],"url":"http://arxiv.org/abs/2402.08514v1","category":"cs.AI"}
{"created":"2024-02-13 15:06:28","title":"The finitude of tamely ramified pro-$p$ extensions of number fields with cyclic $p$-class groups","abstract":"Let $p$ be an odd prime and $F$ be a number field whose $p$-class group is cyclic. Let $F_{\\{\\mathfrak{q}\\}}$ be the maximal pro-$p$ extension of $F$ which is unramified outside a single non-$p$-adic prime ideal $\\mathfrak{q}$ of $F$. In this work, we study the finitude of the Galois group $G_{\\{\\mathfrak{q}\\}}(F)$ of $F_{\\{\\mathfrak{q}\\}}$ over $F$. We prove that $G_{\\{\\mathfrak{q}\\}}(F)$ is finite for the majority of $\\mathfrak{q}$'s such that the generator rank of $G_{\\{\\mathfrak{q}\\}}(F)$ is two, provided that for $p = 3$, $F$ is not a complex quartic field containing the primitive third roots of unity.","sentences":["Let $p$ be an odd prime and $F$ be a number field whose $p$-class group is cyclic.","Let $F_{\\{\\mathfrak{q}\\}}$ be the maximal pro-$p$ extension of $F$ which is unramified outside a single non-$p$-adic prime ideal $\\mathfrak{q}$ of $F$. In this work, we study the finitude of the Galois group $G_{\\{\\mathfrak{q}\\}}(F)$ of $F_{\\{\\mathfrak{q}\\}}$ over $F$.","We prove that $G_{\\{\\mathfrak{q}\\}}(F)$ is finite for the majority of $\\mathfrak{q}$'s such that the generator rank of $G_{\\{\\mathfrak{q}\\}}(F)$ is two, provided that for $p = 3$, $F$ is not a complex quartic field containing the primitive third roots of unity."],"url":"http://arxiv.org/abs/2402.08512v1","category":"math.NT"}
{"created":"2024-02-13 15:05:54","title":"Amplifying Exploration in Monte-Carlo Tree Search by Focusing on the Unknown","abstract":"Monte-Carlo tree search (MCTS) is an effective anytime algorithm with a vast amount of applications. It strategically allocates computational resources to focus on promising segments of the search tree, making it a very attractive search algorithm in large search spaces. However, it often expends its limited resources on reevaluating previously explored regions when they remain the most promising path. Our proposed methodology, denoted as AmEx-MCTS, solves this problem by introducing a novel MCTS formulation. Central to AmEx-MCTS is the decoupling of value updates, visit count updates, and the selected path during the tree search, thereby enabling the exclusion of already explored subtrees or leaves. This segregation preserves the utility of visit counts for both exploration-exploitation balancing and quality metrics within MCTS. The resultant augmentation facilitates in a considerably broader search using identical computational resources, preserving the essential characteristics of MCTS. The expanded coverage not only yields more precise estimations but also proves instrumental in larger and more complex problems. Our empirical evaluation demonstrates the superior performance of AmEx-MCTS, surpassing classical MCTS and related approaches by a substantial margin.","sentences":["Monte-Carlo tree search (MCTS) is an effective anytime algorithm with a vast amount of applications.","It strategically allocates computational resources to focus on promising segments of the search tree, making it a very attractive search algorithm in large search spaces.","However, it often expends its limited resources on reevaluating previously explored regions when they remain the most promising path.","Our proposed methodology, denoted as AmEx-MCTS, solves this problem by introducing a novel MCTS formulation.","Central to AmEx-MCTS is the decoupling of value updates, visit count updates, and the selected path during the tree search, thereby enabling the exclusion of already explored subtrees or leaves.","This segregation preserves the utility of visit counts for both exploration-exploitation balancing and quality metrics within MCTS.","The resultant augmentation facilitates in a considerably broader search using identical computational resources, preserving the essential characteristics of MCTS.","The expanded coverage not only yields more precise estimations but also proves instrumental in larger and more complex problems.","Our empirical evaluation demonstrates the superior performance of AmEx-MCTS, surpassing classical MCTS and related approaches by a substantial margin."],"url":"http://arxiv.org/abs/2402.08511v1","category":"cs.AI"}
{"created":"2024-02-13 15:04:11","title":"From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries (Extended Version)","abstract":"SPARQL CONSTRUCT queries allow for the specification of data processing pipelines that transform given input graphs into new output graphs. It is now common to constrain graphs through SHACL shapes allowing users to understand which data they can expect and which not. However, it becomes challenging to understand what graph data can be expected at the end of a data processing pipeline without knowing the particular input data: Shape constraints on the input graph may affect the output graph, but may no longer apply literally, and new shapes may be imposed by the query template. In this paper, we study the derivation of shape constraints that hold on all possible output graphs of a given SPARQL CONSTRUCT query. We assume that the SPARQL CONSTRUCT query is fixed, e.g., being part of a program, whereas the input graphs adhere to input shape constraints but may otherwise vary over time and, thus, are mostly unknown. We study a fragment of SPARQL CONSTRUCT queries (SCCQ) and a fragment of SHACL (Simple SHACL). We formally define the problem of deriving the most restrictive set of Simple SHACL shapes that constrain the results from evaluating a SCCQ over any input graph restricted by a given set of Simple SHACL shapes. We propose and implement an algorithm that statically analyses input SHACL shapes and CONSTRUCT queries and prove its soundness and complexity.","sentences":["SPARQL CONSTRUCT queries allow for the specification of data processing pipelines that transform given input graphs into new output graphs.","It is now common to constrain graphs through SHACL shapes allowing users to understand which data they can expect and which not.","However, it becomes challenging to understand what graph data can be expected at the end of a data processing pipeline without knowing the particular input data: Shape constraints on the input graph may affect the output graph, but may no longer apply literally, and new shapes may be imposed by the query template.","In this paper, we study the derivation of shape constraints that hold on all possible output graphs of a given SPARQL CONSTRUCT query.","We assume that the SPARQL CONSTRUCT query is fixed, e.g., being part of a program, whereas the input graphs adhere to input shape constraints but may otherwise vary over time and, thus, are mostly unknown.","We study a fragment of SPARQL CONSTRUCT queries (SCCQ) and a fragment of SHACL (Simple SHACL).","We formally define the problem of deriving the most restrictive set of Simple SHACL shapes that constrain the results from evaluating a SCCQ over any input graph restricted by a given set of Simple SHACL shapes.","We propose and implement an algorithm that statically analyses input SHACL shapes and CONSTRUCT queries and prove its soundness and complexity."],"url":"http://arxiv.org/abs/2402.08509v1","category":"cs.DB"}
{"created":"2024-02-13 15:02:00","title":"Reduced-order modeling of the dynamics of an inverted flag from experimental data","abstract":"We use video footage of a water tunnel experiment to construct a two-dimensional reduced-order model of the flapping dynamics of an inverted flag in uniform flow. The model is obtained as the reduced dynamics on an attracting spectral submanifold (SSM) that emanates from the two slowest modes of the unstable fixed point of the flag. Beyond an unstable fixed point and a limit cycle expected from observations, our SSM-reduced model also confirms the existence of two unstable fixed points for the flag, which were found by previous numerical simulations. Importantly, the model correctly reconstructs the dynamics from a small number of general trajectories and no further information on the system.","sentences":["We use video footage of a water tunnel experiment to construct a two-dimensional reduced-order model of the flapping dynamics of an inverted flag in uniform flow.","The model is obtained as the reduced dynamics on an attracting spectral submanifold (SSM) that emanates from the two slowest modes of the unstable fixed point of the flag.","Beyond an unstable fixed point and a limit cycle expected from observations, our SSM-reduced model also confirms the existence of two unstable fixed points for the flag, which were found by previous numerical simulations.","Importantly, the model correctly reconstructs the dynamics from a small number of general trajectories and no further information on the system."],"url":"http://arxiv.org/abs/2402.08504v1","category":"physics.flu-dyn"}
{"created":"2024-02-13 14:53:12","title":"Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style","abstract":"We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score >0.8). In further analyses, reciprocity-style counterarguments display higher counts in most categories, possibly indicating a more creatively persuasive use of evidence. In contrast, human-written counterarguments exhibited greater argumentative richness and diversity across categories. Despite human-written arguments being favored as the most persuasive in human evaluation, the 'No Style' generated text surprisingly exhibited the highest score, prompting further exploration and investigation on the trade-offs in generation for facts and style.","sentences":["We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation.","Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style.","The resultant Counterfire corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2 models and two of their finetuned variants (N = 32,000).","Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for 'reciprocity'), showing the ability of LLM to assimilate diverse styles.","Of all models, GPT-3.5 turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score >0.8).","In further analyses, reciprocity-style counterarguments display higher counts in most categories, possibly indicating a more creatively persuasive use of evidence.","In contrast, human-written counterarguments exhibited greater argumentative richness and diversity across categories.","Despite human-written arguments being favored as the most persuasive in human evaluation, the 'No Style' generated text surprisingly exhibited the highest score, prompting further exploration and investigation on the trade-offs in generation for facts and style."],"url":"http://arxiv.org/abs/2402.08498v1","category":"cs.CL"}
{"created":"2024-02-13 14:51:45","title":"A Systematic Review of Data-to-Text NLG","abstract":"This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.","sentences":["This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review.","We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures.","Our review provides a roadmap for future research in this rapidly evolving field."],"url":"http://arxiv.org/abs/2402.08496v1","category":"cs.CL"}
{"created":"2024-02-13 14:38:12","title":"The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale","abstract":"Background: Colonoscopy, a crucial diagnostic tool in gastroenterology, depends heavily on superior bowel preparation. ChatGPT, a large language model with emergent intelligence which also exhibits potential in medical applications. This study aims to assess the accuracy and consistency of ChatGPT in using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment. Methods: We retrospectively collected 233 colonoscopy images from 2020 to 2023. These images were evaluated using the BBPS by 3 senior endoscopists and 3 novice endoscopists. Additionally, ChatGPT also assessed these images, having been divided into three groups and undergone specific Fine-tuning. Consistency was evaluated through two rounds of testing. Results: In the initial round, ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists. Conclusion: While ChatGPT shows promise in bowel preparation scoring, it currently does not match the accuracy and consistency of experienced endoscopists. Future research should focus on in-depth Fine-tuning.","sentences":["Background: Colonoscopy, a crucial diagnostic tool in gastroenterology, depends heavily on superior bowel preparation.","ChatGPT, a large language model with emergent intelligence which also exhibits potential in medical applications.","This study aims to assess the accuracy and consistency of ChatGPT in using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment.","Methods: We retrospectively collected 233 colonoscopy images from 2020 to 2023.","These images were evaluated using the BBPS by 3 senior endoscopists and 3 novice endoscopists.","Additionally, ChatGPT also assessed these images, having been divided into three groups and undergone specific Fine-tuning.","Consistency was evaluated through two rounds of testing.","Results:","In the initial round, ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%.","Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists.","Conclusion: While ChatGPT shows promise in bowel preparation scoring, it currently does not match the accuracy and consistency of experienced endoscopists.","Future research should focus on in-depth Fine-tuning."],"url":"http://arxiv.org/abs/2402.08492v1","category":"cs.AI"}
{"created":"2024-02-13 14:36:46","title":"Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming","abstract":"Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.","sentences":["Cellular reprogramming can be used for both the prevention and cure of different diseases.","However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs.","In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies.","For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode.","Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training.","Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models."],"url":"http://arxiv.org/abs/2402.08491v1","category":"cs.LG"}
{"created":"2024-02-13 14:31:09","title":"Skew-symmetric solutions of the classical Yang-Baxter equation and $\\mathcal{O}$-operators of Malcev algebras","abstract":"We study connections between skew-symmetric solutions of the classical Yang-Baxter equation (CYBE) and $\\mathcal{O}$-operators of Malcev algebras. We prove that a skew-symmetric solution of the CYBE on a Malcev algebra can be interpreted as an $\\mathcal{O}$-operator associated to the coadjoint representation. We show that this connection can be enhanced with symplectic forms when considering non-degenerate skew-symmetric solutions. We also show that $\\mathcal{O}$-operators associated to a general representation could give skew-symmetric solutions of the CYBE on certain semi-direct products of Malcev algebras. We reveal the relationship between invertible $\\mathcal{O}$-operators and compatible pre-Malcev algebra structures on a Malcev algebra. We finally obtain several analogous results on connections between the CYBE and $\\mathcal{O}$-operators in the case of pre-Malcev algebras.","sentences":["We study connections between skew-symmetric solutions of the classical Yang-Baxter equation (CYBE) and $\\mathcal{O}$-operators of Malcev algebras.","We prove that a skew-symmetric solution of the CYBE on a Malcev algebra can be interpreted as an $\\mathcal{O}$-operator associated to the coadjoint representation.","We show that this connection can be enhanced with symplectic forms when considering non-degenerate skew-symmetric solutions.","We also show that $\\mathcal{O}$-operators associated to a general representation could give skew-symmetric solutions of the CYBE on certain semi-direct products of Malcev algebras.","We reveal the relationship between invertible $\\mathcal{O}$-operators and compatible pre-Malcev algebra structures on a Malcev algebra.","We finally obtain several analogous results on connections between the CYBE and $\\mathcal{O}$-operators in the case of pre-Malcev algebras."],"url":"http://arxiv.org/abs/2402.08489v1","category":"math.RA"}
{"created":"2024-02-13 14:27:13","title":"On the notion of a quaternionic holomorphic function","abstract":"A physically more adequate definition of a quaternionic holomorphic (H-holomorphic) function of one quaternionic variable compared to known ones and a quaternionic generalization of Cauchy-Riemann's equations are presented. At that a class of introduced H-holomorphic functions consists of those quaternionic functions whose left and right derivatives become equal after the transition to 3D space. The presented theory demonstrates a complete similarity of the algebraic properties and differentiation rules between the classes of H-holomorphic and ordinary complex holomorphic functions, including the fact that quaternionic multiplication of the H-holomorphic functions behaves as commutative and the fact that each H-holomorphic function can be created from its complex holomorphic analogue by replacing a complex variable by a quaternion one. A fairly large number of detailed examples are given to illustrate the presented theory efficiency.","sentences":["A physically more adequate definition of a quaternionic holomorphic (H-holomorphic) function of one quaternionic variable compared to known ones and a quaternionic generalization of Cauchy-Riemann's equations are presented.","At that a class of introduced H-holomorphic functions consists of those quaternionic functions whose left and right derivatives become equal after the transition to 3D space.","The presented theory demonstrates a complete similarity of the algebraic properties and differentiation rules between the classes of H-holomorphic and ordinary complex holomorphic functions, including the fact that quaternionic multiplication of the H-holomorphic functions behaves as commutative and the fact that each H-holomorphic function can be created from its complex holomorphic analogue by replacing a complex variable by a quaternion one.","A fairly large number of detailed examples are given to illustrate the presented theory efficiency."],"url":"http://arxiv.org/abs/2402.08487v1","category":"math.CV"}
{"created":"2024-02-13 14:25:06","title":"On a Ramanujan-type series associated with the Heegner number 163","abstract":"Using the Wolfram NumberTheory package and the Recognize command, together with numerical estimates involving the elliptic lambda and elliptic alpha functions, Bagis and Glasser, in 2013, introduced a conjectural Ramanujan-type series related to the class number $h(-d) = 1$ for a quadratic form with discriminant $d = 163$. This conjectured series is of level one and has positive terms, and recalls the Chudnovsky brothers' alternating series of the same level, given the connection between the Chudnovsky-Chudnovsky formula and the Heegner number $d = 163$ such that $\\mathbb{Q}\\left( \\sqrt{-d} \\right)$ has class number one. We prove Bagis and Glasser's conjecture by proving evaluations for $\\lambda^{\\ast}(163)$ and $\\alpha(163)$, which we derive using the Chudnovsky brothers' formula together with the analytic continuation of a formula due to the Borwein brothers for Ramanujan-type series of level one. As a byproduct of our method, we obtain an infinite family of Ramanujan-type series for $\\frac{1}{\\pi}$ generalizing the Chudnovsky algorithm.","sentences":["Using the Wolfram NumberTheory package and the Recognize command, together with numerical estimates involving the elliptic lambda and elliptic alpha functions, Bagis and Glasser, in 2013, introduced a conjectural Ramanujan-type series related to the class number $h(-d)","= 1$ for a quadratic form with discriminant $d = 163$.","This conjectured series is of level one and has positive terms, and recalls the Chudnovsky brothers' alternating series of the same level, given the connection between the Chudnovsky-Chudnovsky formula and the Heegner number $d = 163$ such that $\\mathbb{Q}\\left( \\sqrt{-d} \\right)$ has class number one.","We prove Bagis and Glasser's conjecture by proving evaluations for $\\lambda^{\\ast}(163)$ and $\\alpha(163)$, which we derive using the Chudnovsky brothers' formula together with the analytic continuation of a formula due to the Borwein brothers for Ramanujan-type series of level one.","As a byproduct of our method, we obtain an infinite family of Ramanujan-type series for $\\frac{1}{\\pi}$ generalizing the Chudnovsky algorithm."],"url":"http://arxiv.org/abs/2402.08485v1","category":"math.NT"}
{"created":"2024-02-13 14:20:41","title":"The Computational Complexity of the Housing Market","abstract":"We prove that the classic problem of finding a competitive equilibrium in an exchange economy with indivisible goods, money, and unit-demand agents is PPAD-complete. In this \"housing market\", agents have preferences over the house and amount of money they end up with, but can experience income effects. Our results contrast with the existence of polynomial-time algorithms for related problems: Top Trading Cycles for the \"housing exchange\" problem in which there are no transfers and the Hungarian algorithm for the \"housing assignment\" problem in which agents' utilities are linear in money. Along the way, we prove that the Rainbow-KKM problem, a total search problem based on a generalization by Gale of the Knaster-Kuratowski-Mazurkiewicz lemma, is PPAD-complete. Our reductions also imply bounds on the query complexity of finding competitive equilibrium.","sentences":["We prove that the classic problem of finding a competitive equilibrium in an exchange economy with indivisible goods, money, and unit-demand agents is PPAD-complete.","In this \"housing market\", agents have preferences over the house and amount of money they end up with, but can experience income effects.","Our results contrast with the existence of polynomial-time algorithms for related problems: Top Trading Cycles for the \"housing exchange\" problem in which there are no transfers and the Hungarian algorithm for the \"housing assignment\" problem in which agents' utilities are linear in money.","Along the way, we prove that the Rainbow-KKM problem, a total search problem based on a generalization by Gale of the Knaster-Kuratowski-Mazurkiewicz lemma, is PPAD-complete.","Our reductions also imply bounds on the query complexity of finding competitive equilibrium."],"url":"http://arxiv.org/abs/2402.08484v1","category":"cs.GT"}
{"created":"2024-02-13 14:15:00","title":"Migration to Microservices: A Comparative Study of Decomposition Strategies and Analysis Metrics","abstract":"The microservices architectural style is widely favored for its scalability, reusability, and easy maintainability, prompting increased adoption by developers. However, transitioning from a monolithic to a microservices-based architecture is intricate and costly. In response, we present a novel method utilizing clustering to identify potential microservices in a given monolithic application. Our approach employs a density-based clustering algorithm considering static analysis, structural, and semantic relationships between classes, ensuring a functionally and contextually coherent partitioning. To assess the reliability of our microservice suggestion approach, we conducted an in-depth analysis of hyperparameter sensitivity and compared it with two established clustering algorithms. A comprehensive comparative analysis involved seven applications, evaluating against six baselines, utilizing a dataset of four open-source Java projects. Metrics assessed the quality of generated microservices. Furthermore, we meticulously compared our suggested microservices with manually identified ones in three microservices-based applications. This comparison provided a nuanced understanding of our approach's efficacy and reliability. Our methodology demonstrated promising outcomes, showcasing remarkable effectiveness and commendable stability.","sentences":["The microservices architectural style is widely favored for its scalability, reusability, and easy maintainability, prompting increased adoption by developers.","However, transitioning from a monolithic to a microservices-based architecture is intricate and costly.","In response, we present a novel method utilizing clustering to identify potential microservices in a given monolithic application.","Our approach employs a density-based clustering algorithm considering static analysis, structural, and semantic relationships between classes, ensuring a functionally and contextually coherent partitioning.","To assess the reliability of our microservice suggestion approach, we conducted an in-depth analysis of hyperparameter sensitivity and compared it with two established clustering algorithms.","A comprehensive comparative analysis involved seven applications, evaluating against six baselines, utilizing a dataset of four open-source Java projects.","Metrics assessed the quality of generated microservices.","Furthermore, we meticulously compared our suggested microservices with manually identified ones in three microservices-based applications.","This comparison provided a nuanced understanding of our approach's efficacy and reliability.","Our methodology demonstrated promising outcomes, showcasing remarkable effectiveness and commendable stability."],"url":"http://arxiv.org/abs/2402.08481v1","category":"cs.SE"}
{"created":"2024-02-13 14:13:17","title":"Revealing Decurve Flows for Generalized Graph Propagation","abstract":"This study addresses the limitations of the traditional analysis of message-passing, central to graph learning, by defining {\\em \\textbf{generalized propagation}} with directed and weighted graphs. The significance manifest in two ways. \\textbf{Firstly}, we propose {\\em Generalized Propagation Neural Networks} (\\textbf{GPNNs}), a framework that unifies most propagation-based graph neural networks. By generating directed-weighted propagation graphs with adjacency function and connectivity function, GPNNs offer enhanced insights into attention mechanisms across various graph models. We delve into the trade-offs within the design space with empirical experiments and emphasize the crucial role of the adjacency function for model expressivity via theoretical analysis. \\textbf{Secondly}, we propose the {\\em Continuous Unified Ricci Curvature} (\\textbf{CURC}), an extension of celebrated {\\em Ollivier-Ricci Curvature} for directed and weighted graphs. Theoretically, we demonstrate that CURC possesses continuity, scale invariance, and a lower bound connection with the Dirichlet isoperimetric constant validating bottleneck analysis for GPNNs. We include a preliminary exploration of learned propagation patterns in datasets, a first in the field. We observe an intriguing ``{\\em \\textbf{decurve flow}}'' - a curvature reduction during training for models with learnable propagation, revealing the evolution of propagation over time and a deeper connection to over-smoothing and bottleneck trade-off.","sentences":["This study addresses the limitations of the traditional analysis of message-passing, central to graph learning, by defining {\\em \\textbf{generalized propagation}} with directed and weighted graphs.","The significance manifest in two ways.","\\textbf{Firstly}, we propose {\\em Generalized Propagation Neural Networks} (\\textbf{GPNNs}), a framework that unifies most propagation-based graph neural networks.","By generating directed-weighted propagation graphs with adjacency function and connectivity function, GPNNs offer enhanced insights into attention mechanisms across various graph models.","We delve into the trade-offs within the design space with empirical experiments and emphasize the crucial role of the adjacency function for model expressivity via theoretical analysis.","\\textbf{Secondly}, we propose the {\\em Continuous Unified Ricci Curvature} (\\textbf{CURC}), an extension of celebrated {\\em Ollivier-Ricci Curvature} for directed and weighted graphs.","Theoretically, we demonstrate that CURC possesses continuity, scale invariance, and a lower bound connection with the Dirichlet isoperimetric constant validating bottleneck analysis for GPNNs.","We include a preliminary exploration of learned propagation patterns in datasets, a first in the field.","We observe an intriguing ``{\\em \\textbf{decurve flow}}'' - a curvature reduction during training for models with learnable propagation, revealing the evolution of propagation over time and a deeper connection to over-smoothing and bottleneck trade-off."],"url":"http://arxiv.org/abs/2402.08480v1","category":"cs.LG"}
{"created":"2024-02-13 14:10:15","title":"A precise bare simulation approach to the minimization of some distances. II. Further Foundations","abstract":"The constrained minimization (respectively maximization) of directed distances and of related generalized entropies is a fundamental task in information theory as well as in the adjacent fields of statistics, machine learning, artificial intelligence, signal processing and pattern recognition. In our previous paper \"A precise bare simulation approach to the minimization of some distances. I. Foundations\", we obtained such kind of constrained optima by a new dimension-free precise bare (pure) simulation method, provided basically that (i) the underlying directed distance is of f-divergence type, and that (ii) this can be connected to a light-tailed probability distribution in a certain manner. In the present paper, we extend this approach such that constrained optimization problems of a very huge amount of directed distances and generalized entropies -- and beyond -- can be tackled by a newly developed dimension-free extended bare simulation method, for obtaining both optima as well as optimizers. Almost no assumptions (like convexity) on the set of constraints are needed, within our discrete setup of arbitrary dimension, and our method is precise (i.e., converges in the limit). For instance, we cover constrained optimizations of arbitrary f-divergences, Bregman distances, scaled Bregman distances and weighted Euclidean distances. The potential for wide-spread applicability is indicated, too; in particular, we deliver many recent references for uses of the involved distances/divergences in various different research fields (which may also serve as an interdisciplinary interface).","sentences":["The constrained minimization (respectively maximization) of directed distances and of related generalized entropies is a fundamental task in information theory as well as in the adjacent fields of statistics, machine learning, artificial intelligence, signal processing and pattern recognition.","In our previous paper \"A precise bare simulation approach to the minimization of some distances.","I. Foundations\", we obtained such kind of constrained optima by a new dimension-free precise bare (pure) simulation method, provided basically that (i) the underlying directed distance is of f-divergence type, and that (ii) this can be connected to a light-tailed probability distribution in a certain manner.","In the present paper, we extend this approach such that constrained optimization problems of a very huge amount of directed distances and generalized entropies -- and beyond -- can be tackled by a newly developed dimension-free extended bare simulation method, for obtaining both optima as well as optimizers.","Almost no assumptions (like convexity) on the set of constraints are needed, within our discrete setup of arbitrary dimension, and our method is precise (i.e., converges in the limit).","For instance, we cover constrained optimizations of arbitrary f-divergences, Bregman distances, scaled Bregman distances and weighted Euclidean distances.","The potential for wide-spread applicability is indicated, too; in particular, we deliver many recent references for uses of the involved distances/divergences in various different research fields (which may also serve as an interdisciplinary interface)."],"url":"http://arxiv.org/abs/2402.08478v1","category":"cs.IT"}
{"created":"2024-02-13 14:07:49","title":"Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models","abstract":"Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets. However, these models are poorly understood due to their complexity and size. While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets. In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model. Using the Imagenette dataset, we show that while the model achieves over 99\\% zero-shot classification performance, it fails systematic evaluations completely. Using a linear approximation, we provide a framework to explain the striking differences. We have also obtained similar results using a different model to support that our results are applicable to other transformer models with continuous inputs. We also propose a robust way to detect the modified images.","sentences":["Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets.","However, these models are poorly understood due to their complexity and size.","While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets.","In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model.","Using the Imagenette dataset, we show that while the model achieves over 99\\% zero-shot classification performance, it fails systematic evaluations completely.","Using a linear approximation, we provide a framework to explain the striking differences.","We have also obtained similar results using a different model to support that our results are applicable to other transformer models with continuous inputs.","We also propose a robust way to detect the modified images."],"url":"http://arxiv.org/abs/2402.08473v1","category":"cs.CV"}
{"created":"2024-02-13 14:05:02","title":"Large Language Models for the Automated Analysis of Optimization Algorithms","abstract":"The ability of Large Language Models (LLMs) to generate high-quality text and code has fuelled their rise in popularity. In this paper, we aim to demonstrate the potential of LLMs within the realm of optimization algorithms by integrating them into STNWeb. This is a web-based tool for the generation of Search Trajectory Networks (STNs), which are visualizations of optimization algorithm behavior. Although visualizations produced by STNWeb can be very informative for algorithm designers, they often require a certain level of prior knowledge to be interpreted. In an attempt to bridge this knowledge gap, we have incorporated LLMs, specifically GPT-4, into STNWeb to produce extensive written reports, complemented by automatically generated plots, thereby enhancing the user experience and reducing the barriers to the adoption of this tool by the research community. Moreover, our approach can be expanded to other tools from the optimization community, showcasing the versatility and potential of LLMs in this field.","sentences":["The ability of Large Language Models (LLMs) to generate high-quality text and code has fuelled their rise in popularity.","In this paper, we aim to demonstrate the potential of LLMs within the realm of optimization algorithms by integrating them into STNWeb.","This is a web-based tool for the generation of Search Trajectory Networks (STNs), which are visualizations of optimization algorithm behavior.","Although visualizations produced by STNWeb can be very informative for algorithm designers, they often require a certain level of prior knowledge to be interpreted.","In an attempt to bridge this knowledge gap, we have incorporated LLMs, specifically GPT-4, into STNWeb to produce extensive written reports, complemented by automatically generated plots, thereby enhancing the user experience and reducing the barriers to the adoption of this tool by the research community.","Moreover, our approach can be expanded to other tools from the optimization community, showcasing the versatility and potential of LLMs in this field."],"url":"http://arxiv.org/abs/2402.08472v1","category":"cs.AI"}
{"created":"2024-02-13 14:04:16","title":"Holographic Turbulence From a Random Gravitational Potential","abstract":"We study the turbulent dynamics of a relativistic (2 + 1)-dimensional fluid placed in a stochastic gravitational potential. We demonstrate that the dynamics of the fluid can be obtained using a dual holographic description realized by an asymptotically Anti-de Sitter black brane driven by a random boundary metric. Using the holographic duality we study the inverse cascade energy power spectrum of the fluid and show that it is compatible with that of a compressible fluid flow. We calculate the local energy dissipation and the local fluid velocity distribution which provide other measures of the holographic fluid turbulence.","sentences":["We study the turbulent dynamics of a relativistic (2 + 1)-dimensional fluid placed in a stochastic gravitational potential.","We demonstrate that the dynamics of the fluid can be obtained using a dual holographic description realized by an asymptotically Anti-de Sitter black brane driven by a random boundary metric.","Using the holographic duality we study the inverse cascade energy power spectrum of the fluid and show that it is compatible with that of a compressible fluid flow.","We calculate the local energy dissipation and the local fluid velocity distribution which provide other measures of the holographic fluid turbulence."],"url":"http://arxiv.org/abs/2402.08471v1","category":"hep-th"}
{"created":"2024-02-13 14:00:59","title":"Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale","abstract":"We propose a novel Spatio-Temporal Graph Neural Network empowered trend analysis approach (ST-GTrend) to perform fleet-level performance degradation analysis for Photovoltaic (PV) power networks. PV power stations have become an integral component to the global sustainable energy production landscape. Accurately estimating the performance of PV systems is critical to their feasibility as a power generation technology and as a financial asset. One of the most challenging problems in assessing the Levelized Cost of Energy (LCOE) of a PV system is to understand and estimate the long-term Performance Loss Rate (PLR) for large fleets of PV inverters. ST-GTrend integrates spatio-temporal coherence and graph attention to separate PLR as a long-term \"aging\" trend from multiple fluctuation terms in the PV input data. To cope with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled graph autoencoder array to extract aging and fluctuation terms simultaneously. ST-GTrend imposes flatness and smoothness regularization to ensure the disentanglement between aging and fluctuation. To scale the analysis to large PV systems, we also introduce Para-GTrend, a parallel algorithm to accelerate the training and inference of ST-GTrend. We have evaluated ST-GTrend on three large-scale PV datasets, spanning a time period of 10 years. Our results show that ST-GTrend reduces Mean Absolute Percent Error (MAPE) and Euclidean Distances by 34.74% and 33.66% compared to the SOTA methods. Our results demonstrate that Para-GTrend can speed up ST-GTrend by up to 7.92 times. We further verify the generality and effectiveness of ST-GTrend for trend analysis using financial and economic datasets.","sentences":["We propose a novel Spatio-Temporal Graph Neural Network empowered trend analysis approach (ST-GTrend) to perform fleet-level performance degradation analysis for Photovoltaic (PV) power networks.","PV power stations have become an integral component to the global sustainable energy production landscape.","Accurately estimating the performance of PV systems is critical to their feasibility as a power generation technology and as a financial asset.","One of the most challenging problems in assessing the Levelized Cost of Energy (LCOE) of a PV system is to understand and estimate the long-term Performance Loss Rate (PLR) for large fleets of PV inverters.","ST-GTrend integrates spatio-temporal coherence and graph attention to separate PLR as a long-term \"aging\" trend from multiple fluctuation terms in the PV input data.","To cope with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled graph autoencoder array to extract aging and fluctuation terms simultaneously.","ST-GTrend imposes flatness and smoothness regularization to ensure the disentanglement between aging and fluctuation.","To scale the analysis to large PV systems, we also introduce Para-GTrend, a parallel algorithm to accelerate the training and inference of ST-GTrend.","We have evaluated ST-GTrend on three large-scale PV datasets, spanning a time period of 10 years.","Our results show that ST-GTrend reduces Mean Absolute Percent Error (MAPE) and Euclidean Distances by 34.74% and 33.66% compared to the SOTA methods.","Our results demonstrate that Para-GTrend can speed up ST-GTrend by up to 7.92 times.","We further verify the generality and effectiveness of ST-GTrend for trend analysis using financial and economic datasets."],"url":"http://arxiv.org/abs/2402.08470v1","category":"cs.LG"}
{"created":"2024-02-13 13:58:35","title":"Shadows of rotating hairy Kerr black holes coupled to time periodic scalar fields with non-flat target space","abstract":"We study the shadows cast by rotating hairy black holes with two non-trivial time-periodic scalar fields having a non-flat Gaussian curvature of the target space spanned by the scalar fields. Such black holes are a viable alternative to the Kerr black hole, having a much more complicated geodesic structure and resulting shadows. We investigate how a nontrivial Gauss curvature alters the pictures for different amounts of scalar hair around the black holes, quantified by a normalized charge. Our results show that for high values of this charge, close to a boson star limit, chaotic shadows are observed with multiple small disconnected components for all considered Gaussian curvatures. For moderately large amounts of scalar hair and corresponding normalized charge, although the shadows still exhibit chaotic behavior, a dominant shadow component emerges, the size and shape of which are substantially influenced by the Gaussian curvature. For instance, highly chaotic shadows for flat target space, start developing a large central shadow region with the increase of the Gauss curvature even for black holes with substantially heavy scalar hair. For lower values of the normalized charge, the shadows resemble qualitatively the Kerr black hole while the Gaussian curvature has a small impact on their properties.","sentences":["We study the shadows cast by rotating hairy black holes with two non-trivial time-periodic scalar fields having a non-flat Gaussian curvature of the target space spanned by the scalar fields.","Such black holes are a viable alternative to the Kerr black hole, having a much more complicated geodesic structure and resulting shadows.","We investigate how a nontrivial Gauss curvature alters the pictures for different amounts of scalar hair around the black holes, quantified by a normalized charge.","Our results show that for high values of this charge, close to a boson star limit, chaotic shadows are observed with multiple small disconnected components for all considered Gaussian curvatures.","For moderately large amounts of scalar hair and corresponding normalized charge, although the shadows still exhibit chaotic behavior, a dominant shadow component emerges, the size and shape of which are substantially influenced by the Gaussian curvature.","For instance, highly chaotic shadows for flat target space, start developing a large central shadow region with the increase of the Gauss curvature even for black holes with substantially heavy scalar hair.","For lower values of the normalized charge, the shadows resemble qualitatively the Kerr black hole while the Gaussian curvature has a small impact on their properties."],"url":"http://arxiv.org/abs/2402.08469v1","category":"gr-qc"}
{"created":"2024-02-13 13:54:47","title":"ROSpace: Intrusion Detection Dataset for a ROS2-Based Cyber-Physical System","abstract":"Most of the intrusion detection datasets to research machine learning-based intrusion detection systems (IDSs) are devoted to cyber-only systems, and they typically collect data from one architectural layer. Additionally, often the attacks are generated in dedicated attack sessions, without reproducing the realistic alternation and overlap of normal and attack actions. We present a dataset for intrusion detection by performing penetration testing on an embedded cyber-physical system built over Robot Operating System 2 (ROS2). Features are monitored from three architectural layers: the Linux operating system, the network, and the ROS2 services. The dataset is structured as a time series and describes the expected behavior of the system and its response to ROS2-specific attacks: it repeatedly alternates periods of attack-free operation with periods when a specific attack is being performed. Noteworthy, this allows measuring the time to detect an attacker and the number of malicious activities performed before detection. Also, it allows training an intrusion detector to minimize both, by taking advantage of the numerous alternating periods of normal and attack operations.","sentences":["Most of the intrusion detection datasets to research machine learning-based intrusion detection systems (IDSs) are devoted to cyber-only systems, and they typically collect data from one architectural layer.","Additionally, often the attacks are generated in dedicated attack sessions, without reproducing the realistic alternation and overlap of normal and attack actions.","We present a dataset for intrusion detection by performing penetration testing on an embedded cyber-physical system built over Robot Operating System 2 (ROS2).","Features are monitored from three architectural layers: the Linux operating system, the network, and the ROS2 services.","The dataset is structured as a time series and describes the expected behavior of the system and its response to ROS2-specific attacks: it repeatedly alternates periods of attack-free operation with periods when a specific attack is being performed.","Noteworthy, this allows measuring the time to detect an attacker and the number of malicious activities performed before detection.","Also, it allows training an intrusion detector to minimize both, by taking advantage of the numerous alternating periods of normal and attack operations."],"url":"http://arxiv.org/abs/2402.08468v1","category":"cs.CR"}
{"created":"2024-02-13 13:50:08","title":"Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale","abstract":"As Large Language Models (LLMs) become more proficient, their misuse in large-scale viral disinformation campaigns is a growing concern. This study explores the capability of ChatGPT to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff, and evaluates whether such claims can be differentiated by human readers and automated tools from human-written ones. We compare war-related claims from ClaimReview, authored by IFCN-registered fact-checkers, and similar short-form content generated by ChatGPT. We demonstrate that ChatGPT can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools.","sentences":["As Large Language Models (LLMs) become more proficient, their misuse in large-scale viral disinformation campaigns is a growing concern.","This study explores the capability of ChatGPT to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff, and evaluates whether such claims can be differentiated by human readers and automated tools from human-written ones.","We compare war-related claims from ClaimReview, authored by IFCN-registered fact-checkers, and similar short-form content generated by ChatGPT.","We demonstrate that ChatGPT can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools."],"url":"http://arxiv.org/abs/2402.08467v1","category":"cs.CL"}
{"created":"2024-02-13 13:48:54","title":"Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence","abstract":"Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are leading to the adoption around the world of what regulatory scholars have called a management-based approach to regulation. Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory standards by the International Organization for Standardization, share in common a core management-based paradigm. These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and developed. Refinements and systematization of human-guided training techniques will thus be needed to fit within this emerging era of management-based regulatory paradigm. If taken seriously, human-guided training can alleviate some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better addressing the needs for fairness and effective explainability. In this paper, we discuss the connection between the emerging management-based regulatory frameworks governing AI and the need for human oversight during training. We broadly cover some of the technical components involved in human-guided training and then argue that the kinds of high-stakes use cases for AI that appear of most concern to regulators should lean more on human-guided training than on data-only training. We hope to foster a discussion between legal scholars and computer scientists involving how to govern a domain of technology that is vast, heterogenous, and dynamic in its applications and risks.","sentences":["Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are leading to the adoption around the world of what regulatory scholars have called a management-based approach to regulation.","Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory standards by the International Organization for Standardization, share in common a core management-based paradigm.","These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and developed.","Refinements and systematization of human-guided training techniques will thus be needed to fit within this emerging era of management-based regulatory paradigm.","If taken seriously, human-guided training can alleviate some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better addressing the needs for fairness and effective explainability.","In this paper, we discuss the connection between the emerging management-based regulatory frameworks governing AI and the need for human oversight during training.","We broadly cover some of the technical components involved in human-guided training and then argue that the kinds of high-stakes use cases for AI that appear of most concern to regulators should lean more on human-guided training than on data-only training.","We hope to foster a discussion between legal scholars and computer scientists involving how to govern a domain of technology that is vast, heterogenous, and dynamic in its applications and risks."],"url":"http://arxiv.org/abs/2402.08466v1","category":"cs.AI"}
{"created":"2024-02-13 13:48:52","title":"Solving Free Boundary Problems in Alloy Solidification under Universal Cooling Conditions","abstract":"The kinetics of interfaces in alloy solidification pose a classic free boundary problem. This paper introduces an approach that amalgamates the distinctive characteristics of sharp and diffuse interface models. The motion of the diffuse interface is governed by the phase-field equation featuring a traveling wave function [I. Steinbach, Modell. Simul. Mater. Sci. Eng. 17(7), 073001 (2009)]. To emulate solute rejection in the sharp interface model, the concept of the middle-obstacle and the casting operation are employed. Moreover, the undercooling along the interface normal is flattened to minimize the impact of bulk undercooling in the interface and the associated effects, such as stretching and arc-length diffusion. Notably, the results for interface kinetics under both equilibrium and non-equilibrium conditions closely approximate their analytical solutions, all achieved with artificially wide interfaces and comparatively low computational cost.","sentences":["The kinetics of interfaces in alloy solidification pose a classic free boundary problem.","This paper introduces an approach that amalgamates the distinctive characteristics of sharp and diffuse interface models.","The motion of the diffuse interface is governed by the phase-field equation featuring a traveling wave function [I. Steinbach, Modell.","Simul.","Mater.","Sci.","Eng. 17(7), 073001 (2009)].","To emulate solute rejection in the sharp interface model, the concept of the middle-obstacle and the casting operation are employed.","Moreover, the undercooling along the interface normal is flattened to minimize the impact of bulk undercooling in the interface and the associated effects, such as stretching and arc-length diffusion.","Notably, the results for interface kinetics under both equilibrium and non-equilibrium conditions closely approximate their analytical solutions, all achieved with artificially wide interfaces and comparatively low computational cost."],"url":"http://arxiv.org/abs/2402.08465v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-13 13:43:47","title":"Two-loop integrals of half-BPS six-point functions on a line","abstract":"We evaluated all two-loop conformal integrals of scalar half-BPS six-point functions in $\\mathcal{N} = 4$ SYM restricted to a configuration where all points lie on a line. Moreover, we also computed some of these integrals in the kinematical limit where adjacent points become null-separated. Our results can serve as cross-checks for future works which obtain these integrals for general kinematics or by different methods such as integrability.","sentences":["We evaluated all two-loop conformal integrals of scalar half-BPS six-point functions in $\\mathcal{N} = 4$ SYM restricted to a configuration where all points lie on a line.","Moreover, we also computed some of these integrals in the kinematical limit where adjacent points become null-separated.","Our results can serve as cross-checks for future works which obtain these integrals for general kinematics or by different methods such as integrability."],"url":"http://arxiv.org/abs/2402.08463v1","category":"hep-th"}
{"created":"2024-02-13 13:39:44","title":"The SRG-eROSITA All-Sky Survey : Constraints on f(R) Gravity from Cluster Abundance","abstract":"The evolution of the cluster mass function traces the growth of the linear density perturbations and can be utilized for constraining the parameters of cosmological and alternative gravity models. In this context, we present new constraints on potential deviations from general relativity by investigating the Hu-Sawicki parametrization of the f(R) gravity with the first SRG-eROSITA All-Sky Survey (eRASS1) cluster catalog in the Western Galactic Hemisphere in combination with the overlapping Dark Energy Survey Year 3, KiloDegree Survey and Hyper Supreme Camera data for weak lensing mass calibration. For the first time, we present constraints obtained from cluster abundances only. When we consider massless neutrinos, we find a strict upper limit of log |fR0| < -4.31 at 95% confidence level. Massive neutrinos suppress structure growth at small scales, and thus have the opposite effect of f(R) gravity. We consequently investigate the joint fit of the mass of the neutrinos with the modified gravity parameter. We obtain log |fR0| < -4.12 jointly with \\sum m_\\nu < 0.44 e.V. at 95% confidence level, tighter than the limits in the literature utilizing cluster counts only. At log |fR0|= - 6, the number of clusters is not significantly changed by the theory.   Consequently, we do not find any statistical deviation from general relativity from the study of eRASS1 cluster abundance. Deeper surveys with eROSITA, increasing the number of detected clusters, will further improve constraints on log |fR0| and investigate alternative gravity theories.","sentences":["The evolution of the cluster mass function traces the growth of the linear density perturbations and can be utilized for constraining the parameters of cosmological and alternative gravity models.","In this context, we present new constraints on potential deviations from general relativity by investigating the Hu-Sawicki parametrization of the f(R) gravity with the first SRG-eROSITA All-Sky Survey (eRASS1) cluster catalog in the Western Galactic Hemisphere in combination with the overlapping Dark Energy Survey Year 3, KiloDegree Survey and Hyper Supreme Camera data for weak lensing mass calibration.","For the first time, we present constraints obtained from cluster abundances only.","When we consider massless neutrinos, we find a strict upper limit of log |fR0| < -4.31","at 95% confidence level.","Massive neutrinos suppress structure growth at small scales, and thus have the opposite effect of f(R) gravity.","We consequently investigate the joint fit of the mass of the neutrinos with the modified gravity parameter.","We obtain log |fR0| < -4.12 jointly with \\sum m_\\nu < 0.44 e.V. at 95% confidence level, tighter than the limits in the literature utilizing cluster counts only.","At log |fR0|= - 6, the number of clusters is not significantly changed by the theory.   ","Consequently, we do not find any statistical deviation from general relativity from the study of eRASS1 cluster abundance.","Deeper surveys with eROSITA, increasing the number of detected clusters, will further improve constraints on log |fR0| and investigate alternative gravity theories."],"url":"http://arxiv.org/abs/2402.08459v1","category":"astro-ph.CO"}
{"created":"2024-02-13 13:39:43","title":"The SRG/eROSITA All-Sky Survey: Cosmology Constraints from Cluster Abundances in the Western Galactic Hemisphere","abstract":"The cluster mass function traces the growth of linear density perturbations and provides valuable insights into the growth of structures, the nature of dark matter, and the cosmological parameters governing the Universe. The primary science goal of eROSITA, on board the {\\it Spectrum Roentgen Gamma (SRG)} mission, launched in 2019, is to constrain cosmology through the evolution of cluster mass function. In this paper, we present the cosmological constraints obtained from 5259 clusters of galaxies detected over an area of 12791~deg$^2$ in the Western Galactic Hemisphere of the eROSITA's first All-Sky Survey (eRASS1). The common footprint region between the eROSITA Survey and DES, KiDS, and HSC surveys is used for calibration of the scaling between X-ray count rate and their total mass through measurements of their weak gravitational lensing signal. eRASS1 cluster abundances constrain the $\\Lambda$CDM parameters, which are the energy density of the total matter to $\\Omega_{\\mathrm{m}}=0.29^{+0.01}_{-0.02}$, and the normalization of the density fluctuations to $\\sigma_8=0.88\\pm0.02$ and their combination yields $S_8=\\sigma_8 (\\Omega_\\mathrm{m} / 0.3)^{0.5}=0.86\\pm0.01$, consistent and at a similar precision with the state-of-the-art CMB measurements. eRASS1 cosmological experiment places a most stringent upper limit on the summed masses of left-handed light neutrinos to $\\sum m_\\nu< 0.22\\mathrm{~eV}$ (95\\% confidence interval). Combining eRASS1 cluster abundance measurements with CMB and ground-based neutrino oscillation experiments, we measure the summed neutrino masses to be $\\sum m_\\nu=0.08_{-0.02}^{+0.03}\\mathrm{~eV}$ or $\\sum m_\\nu=0.12_{-0.01}^{+0.03}\\mathrm{~eV}$ depending on the mass hierarchy scenario for neutrino eigenstates. eRASS1 cluster abundances significantly improve the constraints on the dark energy equation of state parameter to $w=-1.12\\pm0.12$. (ABRIDGED)","sentences":["The cluster mass function traces the growth of linear density perturbations and provides valuable insights into the growth of structures, the nature of dark matter, and the cosmological parameters governing the Universe.","The primary science goal of eROSITA, on board the {\\it Spectrum Roentgen Gamma (SRG)} mission, launched in 2019, is to constrain cosmology through the evolution of cluster mass function.","In this paper, we present the cosmological constraints obtained from 5259 clusters of galaxies detected over an area of 12791~deg$^2$ in the Western Galactic Hemisphere of the eROSITA's first All-Sky Survey (eRASS1).","The common footprint region between the eROSITA Survey and DES, KiDS, and HSC surveys is used for calibration of the scaling between X-ray count rate and their total mass through measurements of their weak gravitational lensing signal.","eRASS1 cluster abundances constrain the $\\Lambda$CDM parameters, which are the energy density of the total matter to $\\Omega_{\\mathrm{m}}=0.29^{+0.01}_{-0.02}$, and the normalization of the density fluctuations to $\\sigma_8=0.88\\pm0.02$ and their combination yields $S_8=\\sigma_8 (\\Omega_\\mathrm{m} / 0.3)^{0.5}=0.86\\pm0.01$, consistent and at a similar precision with the state-of-the-art CMB measurements.","eRASS1 cosmological experiment places a most stringent upper limit on the summed masses of left-handed light neutrinos to $\\sum m_\\nu< 0.22\\mathrm{~eV}$ (95\\% confidence interval).","Combining eRASS1 cluster abundance measurements with CMB and ground-based neutrino oscillation experiments, we measure the summed neutrino masses to be $\\sum m_\\nu=0.08_{-0.02}^{+0.03}\\mathrm{~eV}$ or $\\sum m_\\nu=0.12_{-0.01}^{+0.03}\\mathrm{~eV}$ depending on the mass hierarchy scenario for neutrino eigenstates.","eRASS1 cluster abundances significantly improve the constraints on the dark energy equation of state parameter to $w=-1.12\\pm0.12$. (ABRIDGED)"],"url":"http://arxiv.org/abs/2402.08458v1","category":"astro-ph.CO"}
{"created":"2024-02-13 13:30:45","title":"Inevitability of Polarization in Geometric Opinion Exchange","abstract":"Polarization and unexpected correlations between opinions on diverse topics (including in politics, culture and consumer choices) are an object of sustained attention. However, numerous theoretical models do not seem to convincingly explain these phenomena.   This paper is motivated by a recent line of work, studying models where polarization can be explained in terms of biased assimilation and geometric interplay between opinions on various topics. The agent opinions are represented as unit vectors on a multidimensional sphere and updated according to geometric rules. In contrast to previous work, we focus on the classical opinion exchange setting, where the agents update their opinions in discrete time steps, with a pair of agents interacting randomly at every step. The opinions are updated according to an update rule belonging to a general class.   Our findings are twofold. First, polarization appears to be ubiquitous in the class of models we study, requiring only relatively modest assumptions reflecting biased assimilation. Second, there is a qualitative difference between two-dimensional dynamics on the one hand, and three or more dimensions on the other. Accordingly, we prove almost sure polarization for a large class of update rules in two dimensions. Then, we prove polarization in three and more dimensions in more limited cases and try to shed light on central difficulties that are absent in two dimensions.","sentences":["Polarization and unexpected correlations between opinions on diverse topics (including in politics, culture and consumer choices) are an object of sustained attention.","However, numerous theoretical models do not seem to convincingly explain these phenomena.   ","This paper is motivated by a recent line of work, studying models where polarization can be explained in terms of biased assimilation and geometric interplay between opinions on various topics.","The agent opinions are represented as unit vectors on a multidimensional sphere and updated according to geometric rules.","In contrast to previous work, we focus on the classical opinion exchange setting, where the agents update their opinions in discrete time steps, with a pair of agents interacting randomly at every step.","The opinions are updated according to an update rule belonging to a general class.   ","Our findings are twofold.","First, polarization appears to be ubiquitous in the class of models we study, requiring only relatively modest assumptions reflecting biased assimilation.","Second, there is a qualitative difference between two-dimensional dynamics on the one hand, and three or more dimensions on the other.","Accordingly, we prove almost sure polarization for a large class of update rules in two dimensions.","Then, we prove polarization in three and more dimensions in more limited cases and try to shed light on central difficulties that are absent in two dimensions."],"url":"http://arxiv.org/abs/2402.08446v1","category":"cs.SI"}
{"created":"2024-02-13 13:30:34","title":"$1$-Bit SubTHz RIS with Planar Tightly Coupled Dipoles: Beam Shaping and Prototypes","abstract":"In this paper, a proof-of-concept study of a $1$-bit wideband reconfigurable intelligent surface (RIS) comprising planar tightly coupled dipoles (PTCD) is presented. The developed RIS operates at subTHz frequencies and a $3$-dB gain bandwidth of $27.4\\%$ with the center frequency at $102$ GHz is shown to be obtainable via full-wave electromagnetic simulations. The binary phase shift offered by each RIS unit element is enabled by changing the polarization of the reflected wave by $180^\\circ$. The proposed PTCD-based RIS has a planar configuration with one dielectric layer bonded to a ground plane, and hence, it can be fabricated by using cost-effective printed circuit board (PCB) technology. We analytically calculate the response of the entire designed RIS and showcase that a good agreement between that result and equivalent full-wave simulations is obtained. To efficiently compute the $1$-bit RIS response for different pointing directions, thus, designing a directive beam codebook, we devise a fast approximate beamforming optimization approach, which is compared with time-consuming full-wave simulations. Finally, to prove our concept, we present several passive prototypes with frozen beams for the proposed $1$-bit wideband RIS.","sentences":["In this paper, a proof-of-concept study of a $1$-bit wideband reconfigurable intelligent surface (RIS) comprising planar tightly coupled dipoles (PTCD) is presented.","The developed RIS operates at subTHz frequencies and a $3$-dB gain bandwidth of $27.4\\%$ with the center frequency at $102$ GHz is shown to be obtainable via full-wave electromagnetic simulations.","The binary phase shift offered by each RIS unit element is enabled by changing the polarization of the reflected wave by $180^\\circ$. The proposed PTCD-based RIS has a planar configuration with one dielectric layer bonded to a ground plane, and hence, it can be fabricated by using cost-effective printed circuit board (PCB) technology.","We analytically calculate the response of the entire designed RIS and showcase that a good agreement between that result and equivalent full-wave simulations is obtained.","To efficiently compute the $1$-bit RIS response for different pointing directions, thus, designing a directive beam codebook, we devise a fast approximate beamforming optimization approach, which is compared with time-consuming full-wave simulations.","Finally, to prove our concept, we present several passive prototypes with frozen beams for the proposed $1$-bit wideband RIS."],"url":"http://arxiv.org/abs/2402.08445v1","category":"eess.SP"}
{"created":"2024-02-13 13:30:14","title":"The breakdown of the direct relation between the density scaling exponent and the intermolecular interaction potential for molecular systems with purely repulsive intermolecular forces","abstract":"In this work, we question the generally accepted statement that the character of intermolecular interactions can be directly determined from the scaling exponent. Based on detailed studies of polyatomic molecular systems with precisely defined and purely repulsive intermolecular potential, we show that the value of the density scaling exponent evidently differs from the one predicted by the intermolecular virial-potential-energy correlation. Since the latter value directly results from the intermolecular potential, information on the interactions between molecules within the system cannot be immediately gained from the density scaling exponent value. Moreover, we suggest that the recently proposed \"molecular force\" method also returns the value that varies from the one scaling the dynamics. Finally, basing on our results, it might be deduced that the intramolecular interactions influence the density scaling value for real liquids.","sentences":["In this work, we question the generally accepted statement that the character of intermolecular interactions can be directly determined from the scaling exponent.","Based on detailed studies of polyatomic molecular systems with precisely defined and purely repulsive intermolecular potential, we show that the value of the density scaling exponent evidently differs from the one predicted by the intermolecular virial-potential-energy correlation.","Since the latter value directly results from the intermolecular potential, information on the interactions between molecules within the system cannot be immediately gained from the density scaling exponent value.","Moreover, we suggest that the recently proposed \"molecular force\" method also returns the value that varies from the one scaling the dynamics.","Finally, basing on our results, it might be deduced that the intramolecular interactions influence the density scaling value for real liquids."],"url":"http://arxiv.org/abs/2402.08444v1","category":"cond-mat.soft"}
{"created":"2024-02-13 13:25:51","title":"Latent space configuration for improved generalization in supervised autoencoder neural networks","abstract":"Autoencoders (AE) are simple yet powerful class of neural networks that compress data by projecting input into low-dimensional latent space (LS). Whereas LS is formed according to the loss function minimization during training, its properties and topology are not controlled directly. In this paper we focus on AE LS properties and propose two methods for obtaining LS with desired topology, called LS configuration. The proposed methods include loss configuration using a geometric loss term that acts directly in LS, and encoder configuration. We show that the former allows to reliably obtain LS with desired configuration by defining the positions and shapes of LS clusters for supervised AE (SAE). Knowing LS configuration allows to define similarity measure in LS to predict labels or estimate similarity for multiple inputs without using decoders or classifiers. We also show that this leads to more stable and interpretable training. We show that SAE trained for clothes texture classification using the proposed method generalizes well to unseen data from LIP, Market1501, and WildTrack datasets without fine-tuning, and even allows to evaluate similarity for unseen classes. We further illustrate the advantages of pre-configured LS similarity estimation with cross-dataset searches and text-based search using a text query without language models.","sentences":["Autoencoders (AE) are simple yet powerful class of neural networks that compress data by projecting input into low-dimensional latent space (LS).","Whereas LS is formed according to the loss function minimization during training, its properties and topology are not controlled directly.","In this paper we focus on AE LS properties and propose two methods for obtaining LS with desired topology, called LS configuration.","The proposed methods include loss configuration using a geometric loss term that acts directly in LS, and encoder configuration.","We show that the former allows to reliably obtain LS with desired configuration by defining the positions and shapes of LS clusters for supervised AE (SAE).","Knowing LS configuration allows to define similarity measure in LS to predict labels or estimate similarity for multiple inputs without using decoders or classifiers.","We also show that this leads to more stable and interpretable training.","We show that SAE trained for clothes texture classification using the proposed method generalizes well to unseen data from LIP, Market1501, and WildTrack datasets without fine-tuning, and even allows to evaluate similarity for unseen classes.","We further illustrate the advantages of pre-configured LS similarity estimation with cross-dataset searches and text-based search using a text query without language models."],"url":"http://arxiv.org/abs/2402.08441v1","category":"cs.CV"}
{"created":"2024-02-13 13:04:07","title":"Weakly-monontone C*-algebras as Exel-Laca algebras","abstract":"An abstract characterization of weakly monotone $C^*$-algebras, namely the concrete $C^*$-algebras generated by creators and annihilators acting on the so-called weakly monotone Fock spaces, is given in terms of (quotient of) suitable Exel-Laca algebras. The weakly monotone $C^*$-algebra indexed by $\\mathbb{N}$ is shown to be a type-I $C^*$-algebra and its representation theory is entirely determined, whereas the weakly monotone $C^*$-algebra indexed by $\\mathbb{Z}$ is shown not to be of type $I$.","sentences":["An abstract characterization of weakly monotone $C^*$-algebras, namely the concrete $C^*$-algebras generated by creators and annihilators acting on the so-called weakly monotone Fock spaces, is given in terms of (quotient of) suitable Exel-Laca algebras.","The weakly monotone $C^*$-algebra indexed by $\\mathbb{N}$ is shown to be a type-I $C^*$-algebra and its representation theory is entirely determined, whereas the weakly monotone $C^*$-algebra indexed by $\\mathbb{Z}$ is shown not to be of type $I$."],"url":"http://arxiv.org/abs/2402.08435v1","category":"math.OA"}
{"created":"2024-02-13 13:02:51","title":"On the asymptotic density of $k$-tuples of positive integers with pairwise non-coprime components","abstract":"We use the convolution method for arithmetic functions of several variables to deduce an asymptotic formula for the number of $k$-tuples of positive integers with components which are pairwise non-coprime and $\\le x$. More generally, we obtain asymptotic formulas on the number of $k$-tuples $(n_1,\\ldots,n_k)\\in {\\Bbb N}^k$ such that at least $r$ pairs $(n_i,n_j)$, respectively exactly $r$ pairs are coprime. Our results answer the questions raised by Moree (2005, 2014), and generalize and refine related results obtained by Heyman (2014) and Hu (2014).","sentences":["We use the convolution method for arithmetic functions of several variables to deduce an asymptotic formula for the number of $k$-tuples of positive integers with components which are pairwise non-coprime and $\\le x$.","More generally, we obtain asymptotic formulas on the number of $k$-tuples $(n_1,\\ldots,n_k)\\in {\\Bbb N}^k$ such that at least $r$ pairs $(n_i,n_j)$, respectively exactly $r$ pairs are coprime.","Our results answer the questions raised by Moree (2005, 2014), and generalize and refine related results obtained by Heyman (2014) and Hu (2014)."],"url":"http://arxiv.org/abs/2402.08433v1","category":"math.NT"}
{"created":"2024-02-13 13:00:59","title":"Rhythmic soliton interactions for integrated dual-microcomb spectroscopy","abstract":"Rotation symmetry of microresonators supports the generation of phase-locked counter-propagating (CP) solitons that can potentially miniaturize dual-comb systems. Realization of these dual-comb compatible solitons in photonic integrated circuits remains a challenge. Here, we synthesized such CP solitons in an integrated silicon nitride microresonator and observed forced soliton oscillation due to rhythmic, time-varying soliton interactions. The interactions result in seconds mutual-coherence passively. Temporal motion in the soliton streams is discerned by measuring a quadratic-scaling frequency noise peaks and an inverse quadratic-scaling microcomb sidebands. By generating a CP soliton trimer to have two synchronized solitons in one of the orbiting directions, we resolve the incapability of measuring two unsynchronized CP soliton dimer pulses by optical cross-correlation, and show CP solitons undergo complex motion trajectory. We further prove that precise dual-comb spectroscopy with an acquisition time as short as 0.6 $\\mu$s is feasible using these solitons, although the temporal motion limits the dynamic range. Besides revealing soliton interactions with different group velocities, our work propels the realization of photonic integrated dual-comb spectrometers with high passive coherence.","sentences":["Rotation symmetry of microresonators supports the generation of phase-locked counter-propagating (CP) solitons that can potentially miniaturize dual-comb systems.","Realization of these dual-comb compatible solitons in photonic integrated circuits remains a challenge.","Here, we synthesized such CP solitons in an integrated silicon nitride microresonator and observed forced soliton oscillation due to rhythmic, time-varying soliton interactions.","The interactions result in seconds mutual-coherence passively.","Temporal motion in the soliton streams is discerned by measuring a quadratic-scaling frequency noise peaks and an inverse quadratic-scaling microcomb sidebands.","By generating a CP soliton trimer to have two synchronized solitons in one of the orbiting directions, we resolve the incapability of measuring two unsynchronized CP soliton dimer pulses by optical cross-correlation, and show CP solitons undergo complex motion trajectory.","We further prove that precise dual-comb spectroscopy with an acquisition time as short as 0.6 $\\mu$s is feasible using these solitons, although the temporal motion limits the dynamic range.","Besides revealing soliton interactions with different group velocities, our work propels the realization of photonic integrated dual-comb spectrometers with high passive coherence."],"url":"http://arxiv.org/abs/2402.08432v1","category":"physics.optics"}
{"created":"2024-02-13 12:59:20","title":"Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants","abstract":"AI-based code assistants are promising tools that can facilitate and speed up code development. They exploit machine learning algorithms and natural language processing to interact with developers, suggesting code snippets (e.g., method implementations) that can be incorporated into projects. Recent studies empirically investigated the effectiveness of code assistants using simple exemplary problems (e.g., the re-implementation of well-known algorithms), which fail to capture the spectrum and nature of the tasks actually faced by developers. In this paper, we expand the knowledge in the area by comparatively assessing four popular AI-based code assistants, namely GitHub Copilot, Tabnine, ChatGPT, and Google Bard, with a dataset of 100 methods that we constructed from real-life open-source Java projects, considering a variety of cases for complexity and dependency from contextual elements. Results show that Copilot is often more accurate than other techniques, yet none of the assistants is completely subsumed by the rest of the approaches. Interestingly, the effectiveness of these solutions dramatically decreases when dealing with dependencies outside the boundaries of single classes.","sentences":["AI-based code assistants are promising tools that can facilitate and speed up code development.","They exploit machine learning algorithms and natural language processing to interact with developers, suggesting code snippets (e.g., method implementations) that can be incorporated into projects.","Recent studies empirically investigated the effectiveness of code assistants using simple exemplary problems (e.g., the re-implementation of well-known algorithms), which fail to capture the spectrum and nature of the tasks actually faced by developers.","In this paper, we expand the knowledge in the area by comparatively assessing four popular AI-based code assistants, namely GitHub Copilot, Tabnine, ChatGPT, and Google Bard, with a dataset of 100 methods that we constructed from real-life open-source Java projects, considering a variety of cases for complexity and dependency from contextual elements.","Results show that Copilot is often more accurate than other techniques, yet none of the assistants is completely subsumed by the rest of the approaches.","Interestingly, the effectiveness of these solutions dramatically decreases when dealing with dependencies outside the boundaries of single classes."],"url":"http://arxiv.org/abs/2402.08431v1","category":"cs.SE"}
{"created":"2024-02-13 12:58:53","title":"Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot","abstract":"Generative AI is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers. For instance, developers can ask for new code directly from within their IDEs by writing natural language prompts, and integrated services based on generative AI, such as Copilot, immediately respond to prompts by providing ready-to-use code snippets. Formulating the prompt appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code. The task of designing good prompts is known as prompt engineering. In this paper, we systematically investigate the influence of eight prompt features on the style and the content of prompts, on the level of correctness, complexity, size, and similarity to the developers' code of the generated code. We specifically consider the task of using Copilot with 124,800 prompts obtained by systematically combining the eight considered prompt features to generate the implementation of 200 Java methods. Results show how some prompt features, such as the presence of examples and the summary of the purpose of the method, can significantly influence the quality of the result.","sentences":["Generative AI is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers.","For instance, developers can ask for new code directly from within their IDEs by writing natural language prompts, and integrated services based on generative AI, such as Copilot, immediately respond to prompts by providing ready-to-use code snippets.","Formulating the prompt appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code.","The task of designing good prompts is known as prompt engineering.","In this paper, we systematically investigate the influence of eight prompt features on the style and the content of prompts, on the level of correctness, complexity, size, and similarity to the developers' code of the generated code.","We specifically consider the task of using Copilot with 124,800 prompts obtained by systematically combining the eight considered prompt features to generate the implementation of 200 Java methods.","Results show how some prompt features, such as the presence of examples and the summary of the purpose of the method, can significantly influence the quality of the result."],"url":"http://arxiv.org/abs/2402.08430v1","category":"cs.SE"}
{"created":"2024-02-13 12:53:33","title":"Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection","abstract":"In recent years, driven by the need for safer and more autonomous transport systems, the automotive industry has shifted toward integrating a growing number of Advanced Driver Assistance Systems (ADAS). Among the array of sensors employed for object recognition tasks, radar sensors have emerged as a formidable contender due to their abilities in adverse weather conditions or low-light scenarios and their robustness in maintaining consistent performance across diverse environments. However, the small size of radar datasets and the complexity of the labelling of those data limit the performance of radar object detectors. Driven by the promising results of self-supervised learning in computer vision, this paper presents RiCL, an instance contrastive learning framework to pre-train radar object detectors. We propose to exploit the detection from the radar and the temporal information to pre-train the radar object detection model in a self-supervised way using contrastive learning. We aim to pre-train an object detector's backbone, head and neck to learn with fewer data. Experiments on the CARRADA and the RADDet datasets show the effectiveness of our approach in learning generic representations of objects in range-Doppler maps. Notably, our pre-training strategy allows us to use only 20% of the labelled data to reach a similar mAP@0.5 than a supervised approach using the whole training set.","sentences":["In recent years, driven by the need for safer and more autonomous transport systems, the automotive industry has shifted toward integrating a growing number of Advanced Driver Assistance Systems (ADAS).","Among the array of sensors employed for object recognition tasks, radar sensors have emerged as a formidable contender due to their abilities in adverse weather conditions or low-light scenarios and their robustness in maintaining consistent performance across diverse environments.","However, the small size of radar datasets and the complexity of the labelling of those data limit the performance of radar object detectors.","Driven by the promising results of self-supervised learning in computer vision, this paper presents RiCL, an instance contrastive learning framework to pre-train radar object detectors.","We propose to exploit the detection from the radar and the temporal information to pre-train the radar object detection model in a self-supervised way using contrastive learning.","We aim to pre-train an object detector's backbone, head and neck to learn with fewer data.","Experiments on the CARRADA and the RADDet datasets show the effectiveness of our approach in learning generic representations of objects in range-Doppler maps.","Notably, our pre-training strategy allows us to use only 20% of the labelled data to reach a similar mAP@0.5 than a supervised approach using the whole training set."],"url":"http://arxiv.org/abs/2402.08427v1","category":"cs.CV"}
{"created":"2024-02-13 12:52:02","title":"Conditional Neural Expert Processes for Learning from Demonstration","abstract":"Learning from Demonstration (LfD) is a widely used technique for skill acquisition in robotics. However, demonstrations of the same skill may exhibit significant variances, or learning systems may attempt to acquire different means of the same skill simultaneously, making it challenging to encode these motions into movement primitives. To address these challenges, we propose an LfD framework, namely the Conditional Neural Expert Processes (CNEP), that learns to assign demonstrations from different modes to distinct expert networks utilizing the inherent information within the latent space to match experts with the encoded representations. CNEP does not require supervision on which mode the trajectories belong to. Provided experiments on artificially generated datasets demonstrate the efficacy of CNEP. Furthermore, we compare the performance of CNEP with another LfD framework, namely Conditional Neural Movement Primitives (CNMP), on a range of tasks, including experiments on a real robot. The results reveal enhanced modeling performance for movement primitives, leading to the synthesis of trajectories that more accurately reflect those demonstrated by experts, particularly when the model inputs include intersection points from various trajectories. Additionally, CNEP offers improved interpretability and faster convergence by promoting expert specialization. Furthermore, we show that the CNEP model accomplishes obstacle avoidance tasks with a real manipulator when provided with novel start and destination points, in contrast to the CNMP model, which leads to collisions with the obstacle.","sentences":["Learning from Demonstration (LfD) is a widely used technique for skill acquisition in robotics.","However, demonstrations of the same skill may exhibit significant variances, or learning systems may attempt to acquire different means of the same skill simultaneously, making it challenging to encode these motions into movement primitives.","To address these challenges, we propose an LfD framework, namely the Conditional Neural Expert Processes (CNEP), that learns to assign demonstrations from different modes to distinct expert networks utilizing the inherent information within the latent space to match experts with the encoded representations.","CNEP does not require supervision on which mode the trajectories belong to.","Provided experiments on artificially generated datasets demonstrate the efficacy of CNEP.","Furthermore, we compare the performance of CNEP with another LfD framework, namely Conditional Neural Movement Primitives (CNMP), on a range of tasks, including experiments on a real robot.","The results reveal enhanced modeling performance for movement primitives, leading to the synthesis of trajectories that more accurately reflect those demonstrated by experts, particularly when the model inputs include intersection points from various trajectories.","Additionally, CNEP offers improved interpretability and faster convergence by promoting expert specialization.","Furthermore, we show that the CNEP model accomplishes obstacle avoidance tasks with a real manipulator when provided with novel start and destination points, in contrast to the CNMP model, which leads to collisions with the obstacle."],"url":"http://arxiv.org/abs/2402.08424v1","category":"cs.RO"}
{"created":"2024-02-13 12:50:04","title":"Vehicle Behavior Prediction by Episodic-Memory Implanted NDT","abstract":"In autonomous driving, predicting the behavior (turning left, stopping, etc.) of target vehicles is crucial for the self-driving vehicle to make safe decisions and avoid accidents. Existing deep learning-based methods have shown excellent and accurate performance, but the black-box nature makes it untrustworthy to apply them in practical use. In this work, we explore the interpretability of behavior prediction of target vehicles by an Episodic Memory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of eMem-NDT is constructed by hierarchically clustering the text embedding of vehicle behavior descriptions. eMem-NDT is a neural-backed part of a pre-trained deep learning model by changing the soft-max layer of the deep model to eMem-NDT, for grouping and aligning the memory prototypes of the historical vehicle behavior features in training data on a neural decision tree. Each leaf node of eMem-NDT is modeled by a neural network for aligning the behavior memory prototypes. By eMem-NDT, we infer each instance in behavior prediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching the appropriate leaf node and the links to the root node) and top-down Leaf Link Aggregation (LLA) (obtaining the probability of future behaviors of vehicles for certain instances). We validate eMem-NDT on BLVD and LOKI datasets, and the results show that our model can obtain a superior performance to other methods with clear explainability. The code is available at https://github.com/JWFangit/eMem-NDT.","sentences":["In autonomous driving, predicting the behavior (turning left, stopping, etc.) of target vehicles is crucial for the self-driving vehicle to make safe decisions and avoid accidents.","Existing deep learning-based methods have shown excellent and accurate performance, but the black-box nature makes it untrustworthy to apply them in practical use.","In this work, we explore the interpretability of behavior prediction of target vehicles by an Episodic Memory implanted Neural Decision Tree (abbrev.","eMem-NDT).","The structure of eMem-NDT is constructed by hierarchically clustering the text embedding of vehicle behavior descriptions.","eMem-NDT is a neural-backed part of a pre-trained deep learning model by changing the soft-max layer of the deep model to eMem-NDT, for grouping and aligning the memory prototypes of the historical vehicle behavior features in training data on a neural decision tree.","Each leaf node of eMem-NDT is modeled by a neural network for aligning the behavior memory prototypes.","By eMem-NDT, we infer each instance in behavior prediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching the appropriate leaf node and the links to the root node) and top-down Leaf Link Aggregation (LLA) (obtaining the probability of future behaviors of vehicles for certain instances).","We validate eMem-NDT on BLVD and LOKI datasets, and the results show that our model can obtain a superior performance to other methods with clear explainability.","The code is available at https://github.com/JWFangit/eMem-NDT."],"url":"http://arxiv.org/abs/2402.08423v1","category":"cs.AI"}
{"created":"2024-02-13 12:49:22","title":"Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins","abstract":"Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks. An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment. This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement (MARL) requires online interactions with the environment. A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data. In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data. To further exploit the offline data, we adapt the proposed scheme to the centralized training decentralized execution framework, allowing joint training of the agents' policies. The proposed MARL scheme, referred to as multi-agent conservative quantile regression (MA-CQR) addresses general risk-sensitive design criteria and is applied to the trajectory planning problem in drone networks, showcasing its advantages.","sentences":["Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks.","An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment.","This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement (MARL) requires online interactions with the environment.","A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data.","In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data.","To further exploit the offline data, we adapt the proposed scheme to the centralized training decentralized execution framework, allowing joint training of the agents' policies.","The proposed MARL scheme, referred to as multi-agent conservative quantile regression (MA-CQR) addresses general risk-sensitive design criteria and is applied to the trajectory planning problem in drone networks, showcasing its advantages."],"url":"http://arxiv.org/abs/2402.08421v1","category":"cs.LG"}
{"created":"2024-02-13 12:48:52","title":"Band gaps of hybrid metal halide perovskites: efficient estimation","abstract":"The employment of the parameter-free Armiento-K\\\"{u}mmel generalized gradient approximation (AK13-GGA) exchange functional was examined as means of the band gap prediction for hybrid metal halide perovskites (HaPs) or systems with strong spin-orbit coupling in the full-relativistic density-functional-theory (DFT) calculations. The new combination of AK13 with the nonseparable gradient approximation Minnesota correlation functional (GAM) was established as an approach allowing for the efficient band gap estimation with accuracy similar to the GW approximation method but at the computational costs of conventional DFT. This was further supported by results of the AK13/GAM calculations performed for various HaPs. The described approach creates an opportunity for the effective assessment of the electronic structure of large, complex, doped or defective HaPs and modelling of new materials.","sentences":["The employment of the parameter-free Armiento-K\\\"{u}mmel generalized gradient approximation (AK13-GGA) exchange functional was examined as means of the band gap prediction for hybrid metal halide perovskites (HaPs) or systems with strong spin-orbit coupling in the full-relativistic density-functional-theory (DFT) calculations.","The new combination of AK13 with the nonseparable gradient approximation Minnesota correlation functional (GAM) was established as an approach allowing for the efficient band gap estimation with accuracy similar to the GW approximation method but at the computational costs of conventional DFT.","This was further supported by results of the AK13/GAM calculations performed for various HaPs.","The described approach creates an opportunity for the effective assessment of the electronic structure of large, complex, doped or defective HaPs and modelling of new materials."],"url":"http://arxiv.org/abs/2402.08419v1","category":"physics.comp-ph"}
{"created":"2024-02-13 12:41:07","title":"Constructing model-agnostic likelihoods, a method for the reinterpretation of particle physics results","abstract":"Experimental High Energy Physics has entered an era of precision measurements. However, measurements of many of the accessible processes assume that the final states' underlying kinematic distribution is the same as the Standard Model prediction. This assumption introduces an implicit model-dependency into the measurement, rendering the reinterpretation of the experimental analysis complicated without reanalysing the underlying data. We present a novel reweighting method in order to perform reinterpretation of particle physics measurements. It makes use of reweighting the Standard Model templates according to kinematic signal distributions of alternative theoretical models, prior to performing the statistical analysis. The generality of this method allows us to perform statistical inference in the space of theoretical parameters, assuming different kinematic distributions, according to a beyond Standard Model prediction. We implement our method as an extension to the pyhf software and interface it with the EOS software, which allows us to perform flavor physics phenomenology studies. Furthermore, we argue that, beyond the pyhf or HistFactory likelihood specification, only minimal information is necessary to make a likelihood model-agnostic and hence easily reinterpretable. We showcase that publishing such likelihoods is crucial for a full exploitation of experimental results.","sentences":["Experimental High Energy Physics has entered an era of precision measurements.","However, measurements of many of the accessible processes assume that the final states' underlying kinematic distribution is the same as the Standard Model prediction.","This assumption introduces an implicit model-dependency into the measurement, rendering the reinterpretation of the experimental analysis complicated without reanalysing the underlying data.","We present a novel reweighting method in order to perform reinterpretation of particle physics measurements.","It makes use of reweighting the Standard Model templates according to kinematic signal distributions of alternative theoretical models, prior to performing the statistical analysis.","The generality of this method allows us to perform statistical inference in the space of theoretical parameters, assuming different kinematic distributions, according to a beyond Standard Model prediction.","We implement our method as an extension to the pyhf software and interface it with the EOS software, which allows us to perform flavor physics phenomenology studies.","Furthermore, we argue that, beyond the pyhf or HistFactory likelihood specification, only minimal information is necessary to make a likelihood model-agnostic and hence easily reinterpretable.","We showcase that publishing such likelihoods is crucial for a full exploitation of experimental results."],"url":"http://arxiv.org/abs/2402.08417v1","category":"hep-ph"}
{"created":"2024-02-13 12:40:39","title":"Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning","abstract":"Large Language Models~(LLMs) have gained immense popularity and are being increasingly applied in various domains. Consequently, ensuring the security of these models is of paramount importance. Jailbreak attacks, which manipulate LLMs to generate malicious content, are recognized as a significant vulnerability. While existing research has predominantly focused on direct jailbreak attacks on LLMs, there has been limited exploration of indirect methods. The integration of various plugins into LLMs, notably Retrieval Augmented Generation~(RAG), which enables LLMs to incorporate external knowledge bases into their response generation such as GPTs, introduces new avenues for indirect jailbreak attacks.   To fill this gap, we investigate indirect jailbreak attacks on LLMs, particularly GPTs, introducing a novel attack vector named Retrieval Augmented Generation Poisoning. This method, Pandora, exploits the synergy between LLMs and RAG through prompt manipulation to generate unexpected responses. Pandora uses maliciously crafted content to influence the RAG process, effectively initiating jailbreak attacks. Our preliminary tests show that Pandora successfully conducts jailbreak attacks in four different scenarios, achieving higher success rates than direct attacks, with 64.3\\% for GPT-3.5 and 34.8\\% for GPT-4.","sentences":["Large Language Models~(LLMs) have gained immense popularity and are being increasingly applied in various domains.","Consequently, ensuring the security of these models is of paramount importance.","Jailbreak attacks, which manipulate LLMs to generate malicious content, are recognized as a significant vulnerability.","While existing research has predominantly focused on direct jailbreak attacks on LLMs, there has been limited exploration of indirect methods.","The integration of various plugins into LLMs, notably Retrieval Augmented Generation~(RAG), which enables LLMs to incorporate external knowledge bases into their response generation such as GPTs, introduces new avenues for indirect jailbreak attacks.   ","To fill this gap, we investigate indirect jailbreak attacks on LLMs, particularly GPTs, introducing a novel attack vector named Retrieval Augmented Generation Poisoning.","This method, Pandora, exploits the synergy between LLMs and RAG through prompt manipulation to generate unexpected responses.","Pandora uses maliciously crafted content to influence the RAG process, effectively initiating jailbreak attacks.","Our preliminary tests show that Pandora successfully conducts jailbreak attacks in four different scenarios, achieving higher success rates than direct attacks, with 64.3\\% for GPT-3.5 and 34.8\\% for GPT-4."],"url":"http://arxiv.org/abs/2402.08416v1","category":"cs.CR"}
{"created":"2024-02-13 12:24:27","title":"Sparse systems with high local multiplicity","abstract":"Consider a sparse system of n Laurent polynomials in n variables with complex coefficients and support in a finite lattice set A. The maximal number of isolated roots of the system in the complex n-torus is known to be the normalized volume of the convex hull of A (the BKK bound). We explore the following question: if the cardinality of A equals n+m+1, what is the maximum local intersection multiplicity at one point in the torus in terms of n and m? This study was initiated by Gabrielov in the multivariate case. We give an upper bound that is always sharp when m=1 and, under a generic technical hypothesis, it is considerably smaller for any dimension n and codimension m. We also present, for any value of n and m, a particular sparse system with high local multiplicity with exponents in the vertices of a cyclic polytope and we explain the rationale of our choice. Our work raises several interesting questions.","sentences":["Consider a sparse system of n Laurent polynomials in n variables with complex coefficients and support in a finite lattice set A.","The maximal number of isolated roots of the system in the complex n-torus is known to be the normalized volume of the convex hull of A (the BKK bound).","We explore the following question: if the cardinality of A equals n+m+1, what is the maximum local intersection multiplicity at one point in the torus in terms of n and m?","This study was initiated by Gabrielov in the multivariate case.","We give an upper bound that is always sharp when m=1 and, under a generic technical hypothesis, it is considerably smaller for any dimension n and codimension m. We also present, for any value of n and m, a particular sparse system with high local multiplicity with exponents in the vertices of a cyclic polytope and we explain the rationale of our choice.","Our work raises several interesting questions."],"url":"http://arxiv.org/abs/2402.08410v1","category":"math.AG"}
{"created":"2024-02-13 12:21:06","title":"Transferring Ultrahigh-Field Representations for Intensity-Guided Brain Segmentation of Low-Field Magnetic Resonance Imaging","abstract":"Ultrahigh-field (UHF) magnetic resonance imaging (MRI), i.e., 7T MRI, provides superior anatomical details of internal brain structures owing to its enhanced signal-to-noise ratio and susceptibility-induced contrast. However, the widespread use of 7T MRI is limited by its high cost and lower accessibility compared to low-field (LF) MRI. This study proposes a deep-learning framework that systematically fuses the input LF magnetic resonance feature representations with the inferred 7T-like feature representations for brain image segmentation tasks in a 7T-absent environment. Specifically, our adaptive fusion module aggregates 7T-like features derived from the LF image by a pre-trained network and then refines them to be effectively assimilable UHF guidance into LF image features. Using intensity-guided features obtained from such aggregation and assimilation, segmentation models can recognize subtle structural representations that are usually difficult to recognize when relying only on LF features. Beyond such advantages, this strategy can seamlessly be utilized by modulating the contrast of LF features in alignment with UHF guidance, even when employing arbitrary segmentation models. Exhaustive experiments demonstrated that the proposed method significantly outperformed all baseline models on both brain tissue and whole-brain segmentation tasks; further, it exhibited remarkable adaptability and scalability by successfully integrating diverse segmentation models and tasks. These improvements were not only quantifiable but also visible in the superlative visual quality of segmentation masks.","sentences":["Ultrahigh-field (UHF) magnetic resonance imaging (MRI), i.e., 7T MRI, provides superior anatomical details of internal brain structures owing to its enhanced signal-to-noise ratio and susceptibility-induced contrast.","However, the widespread use of 7T MRI is limited by its high cost and lower accessibility compared to low-field (LF) MRI.","This study proposes a deep-learning framework that systematically fuses the input LF magnetic resonance feature representations with the inferred 7T-like feature representations for brain image segmentation tasks in a 7T-absent environment.","Specifically, our adaptive fusion module aggregates 7T-like features derived from the LF image by a pre-trained network and then refines them to be effectively assimilable UHF guidance into LF image features.","Using intensity-guided features obtained from such aggregation and assimilation, segmentation models can recognize subtle structural representations that are usually difficult to recognize when relying only on LF features.","Beyond such advantages, this strategy can seamlessly be utilized by modulating the contrast of LF features in alignment with UHF guidance, even when employing arbitrary segmentation models.","Exhaustive experiments demonstrated that the proposed method significantly outperformed all baseline models on both brain tissue and whole-brain segmentation tasks; further, it exhibited remarkable adaptability and scalability by successfully integrating diverse segmentation models and tasks.","These improvements were not only quantifiable but also visible in the superlative visual quality of segmentation masks."],"url":"http://arxiv.org/abs/2402.08409v1","category":"cs.CV"}
{"created":"2024-02-13 12:17:06","title":"The baryon cycle in modern cosmological hydrodynamical simulations","abstract":"In recent years, cosmological hydrodynamical simulations have proven their utility as key interpretative tools in the study of galaxy formation and evolution. In this work, we present a like-for-like comparison between the baryon cycle in three publicly available, leading cosmological simulation suites: EAGLE, IllustrisTNG, and SIMBA. While these simulations broadly agree in terms of their predictions for the stellar mass content and star formation rates of galaxies at $z\\approx0$, they achieve this result for markedly different reasons. In EAGLE and SIMBA, we demonstrate that at low halo masses ($M_{\\rm 200c}\\lesssim 10^{11.5}\\, M_{\\odot}$), stellar feedback (SF)-driven outflows can reach far beyond the scale of the halo, extending up to $2-3\\times R_{\\rm 200c}$. In contrast, in TNG, SF-driven outflows, while stronger at the scale of the ISM, recycle within the CGM (within $R_{\\rm 200c}$). We find that AGN-driven outflows in SIMBA are notably potent, reaching several times $R_{\\rm 200c}$ even at halo masses up to $M_{\\rm 200c}\\approx10^{13.5}\\, M_{\\odot}$. In both TNG and EAGLE, AGN feedback can eject gas beyond $R_{\\rm 200c}$ at this mass scale, but seldom beyond $2-3\\times R_{\\rm 200c}$. We find that the scale of feedback-driven outflows can be directly linked with the prevention of cosmological inflow, as well as the total baryon fraction of haloes within $R_{\\rm 200c}$. This work lays the foundation to develop targeted observational tests that can discriminate between feedback scenarios, and inform sub-grid feedback models in the next generation of simulations.","sentences":["In recent years, cosmological hydrodynamical simulations have proven their utility as key interpretative tools in the study of galaxy formation and evolution.","In this work, we present a like-for-like comparison between the baryon cycle in three publicly available, leading cosmological simulation suites: EAGLE, IllustrisTNG, and SIMBA.","While these simulations broadly agree in terms of their predictions for the stellar mass content and star formation rates of galaxies at $z\\approx0$, they achieve this result for markedly different reasons.","In EAGLE and SIMBA, we demonstrate that at low halo masses ($M_{\\rm 200c}\\lesssim 10^{11.5}\\, M_{\\odot}$), stellar feedback (SF)-driven outflows can reach far beyond the scale of the halo, extending up to $2-3\\times R_{\\rm 200c}$.","In contrast, in TNG, SF-driven outflows, while stronger at the scale of the ISM, recycle within the CGM (within $R_{\\rm 200c}$).","We find that AGN-driven outflows in SIMBA are notably potent, reaching several times $R_{\\rm 200c}$ even at halo masses up to $M_{\\rm 200c}\\approx10^{13.5}\\, M_{\\odot}$.","In both TNG and EAGLE, AGN feedback can eject gas beyond $R_{\\rm 200c}$ at this mass scale, but seldom beyond $2-3\\times R_{\\rm 200c}$. We find that the scale of feedback-driven outflows can be directly linked with the prevention of cosmological inflow, as well as the total baryon fraction of haloes within $R_{\\rm 200c}$.","This work lays the foundation to develop targeted observational tests that can discriminate between feedback scenarios, and inform sub-grid feedback models in the next generation of simulations."],"url":"http://arxiv.org/abs/2402.08408v1","category":"astro-ph.GA"}
{"created":"2024-02-13 12:09:15","title":"A Novel Approach to Regularising 1NN classifier for Improved Generalization","abstract":"In this paper, we propose a class of non-parametric classifiers, that learn arbitrary boundaries and generalize well.   Our approach is based on a novel way to regularize 1NN classifiers using a greedy approach. We refer to this class of classifiers as Watershed Classifiers. 1NN classifiers are known to trivially over-fit but have very large VC dimension, hence do not generalize well. We show that watershed classifiers can find arbitrary boundaries on any dense enough dataset, and, at the same time, have very small VC dimension; hence a watershed classifier leads to good generalization.   Traditional approaches to regularize 1NN classifiers are to consider $K$ nearest neighbours. Neighbourhood component analysis (NCA) proposes a way to learn representations consistent with ($n-1$) nearest neighbour classifier, where $n$ denotes the size of the dataset. In this article, we propose a loss function which can learn representations consistent with watershed classifiers, and show that it outperforms the NCA baseline.","sentences":["In this paper, we propose a class of non-parametric classifiers, that learn arbitrary boundaries and generalize well.   ","Our approach is based on a novel way to regularize 1NN classifiers using a greedy approach.","We refer to this class of classifiers as Watershed Classifiers.","1NN classifiers are known to trivially over-fit but have very large VC dimension, hence do not generalize well.","We show that watershed classifiers can find arbitrary boundaries on any dense enough dataset, and, at the same time, have very small VC dimension; hence a watershed classifier leads to good generalization.   ","Traditional approaches to regularize 1NN classifiers are to consider $K$ nearest neighbours.","Neighbourhood component analysis (NCA) proposes a way to learn representations consistent with ($n-1$) nearest neighbour classifier, where $n$ denotes the size of the dataset.","In this article, we propose a loss function which can learn representations consistent with watershed classifiers, and show that it outperforms the NCA baseline."],"url":"http://arxiv.org/abs/2402.08405v1","category":"cs.LG"}
{"created":"2024-02-13 12:04:43","title":"LLMs and the Human Condition","abstract":"This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action. Taking seriously the idea of language as action the model is then applied to the conversational user interfaces. Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what LLMs are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up. When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense. By understanding where ChatGPT's apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship with our world.","sentences":["This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action.","Taking seriously the idea of language as action the model is then applied to the conversational user interfaces.","Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what LLMs are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up.","When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense.","By understanding where ChatGPT's apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship with our world."],"url":"http://arxiv.org/abs/2402.08403v1","category":"cs.CL"}
{"created":"2024-02-13 12:02:40","title":"Enhanced Blandford Znajek Jet in Loop Quantum Black Hole","abstract":"The Blandford-Znajek (BZ) process powers energetic jets by extracting the rotating energy of a Kerr black hole. It is important to understand this process in non-Kerr black hole spacetimes. In this study, we conduct two-dimensional and three-dimensional two-temperature General Relativistic Magnetohydrodynamic (GRMHD) simulations of magnetized accretion flows onto a rotating Loop-Quantum black hole (LQBH). Our investigation focuses on the accretion flow structure and jet launching dynamics from our simulations. We observe that the loop quantum effects increase the black hole angular frequency for spinning black holes.This phenomenon intensifies the frame-dragging effect, leading to an amplification of the toroidal magnetic field within the funnel region and enhancement of the launching jet power. It is possible to fit the jet power following a similar fitting formula of the black hole angular frequency as seen in the Kerr black hole. Based on the General Relativistic Radiation Transfer (GRRT) calculation, we find that the jet image from LQBH has a wider opening angle and an extended structure than the Kerr BH.","sentences":["The Blandford-Znajek (BZ) process powers energetic jets by extracting the rotating energy of a Kerr black hole.","It is important to understand this process in non-Kerr black hole spacetimes.","In this study, we conduct two-dimensional and three-dimensional two-temperature General Relativistic Magnetohydrodynamic (GRMHD) simulations of magnetized accretion flows onto a rotating Loop-Quantum black hole (LQBH).","Our investigation focuses on the accretion flow structure and jet launching dynamics from our simulations.","We observe that the loop quantum effects increase the black hole angular frequency for spinning black holes.","This phenomenon intensifies the frame-dragging effect, leading to an amplification of the toroidal magnetic field within the funnel region and enhancement of the launching jet power.","It is possible to fit the jet power following a similar fitting formula of the black hole angular frequency as seen in the Kerr black hole.","Based on the General Relativistic Radiation Transfer (GRRT) calculation, we find that the jet image from LQBH has a wider opening angle and an extended structure than the Kerr BH."],"url":"http://arxiv.org/abs/2402.08402v1","category":"astro-ph.HE"}
{"created":"2024-02-13 12:02:37","title":"LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection","abstract":"In the era of widespread social networks, the rapid dissemination of fake news has emerged as a significant threat, inflicting detrimental consequences across various dimensions of people's lives. Machine learning and deep learning approaches have been extensively employed for identifying fake news. However, a significant challenge in identifying fake news is the limited availability of labeled news datasets. Therefore, the One-Class Learning (OCL) approach, utilizing only a small set of labeled data from the interest class, can be a suitable approach to address this challenge. On the other hand, representing data as a graph enables access to diverse content and structural information, and label propagation methods on graphs can be effective in predicting node labels. In this paper, we adopt a graph-based model for data representation and introduce a semi-supervised and one-class approach for fake news detection, called LOSS-GAT. Initially, we employ a two-step label propagation algorithm, utilizing Graph Neural Networks (GNNs) as an initial classifier to categorize news into two groups: interest (fake) and non-interest (real). Subsequently, we enhance the graph structure using structural augmentation techniques. Ultimately, we predict the final labels for all unlabeled data using a GNN that induces randomness within the local neighborhood of nodes through the aggregation function. We evaluate our proposed method on five common datasets and compare the results against a set of baseline models, including both OCL and binary labeled models. The results demonstrate that LOSS-GAT achieves a notable improvement, surpassing 10%, with the advantage of utilizing only a limited set of labeled fake news. Noteworthy, LOSS-GAT even outperforms binary labeled models.","sentences":["In the era of widespread social networks, the rapid dissemination of fake news has emerged as a significant threat, inflicting detrimental consequences across various dimensions of people's lives.","Machine learning and deep learning approaches have been extensively employed for identifying fake news.","However, a significant challenge in identifying fake news is the limited availability of labeled news datasets.","Therefore, the One-Class Learning (OCL) approach, utilizing only a small set of labeled data from the interest class, can be a suitable approach to address this challenge.","On the other hand, representing data as a graph enables access to diverse content and structural information, and label propagation methods on graphs can be effective in predicting node labels.","In this paper, we adopt a graph-based model for data representation and introduce a semi-supervised and one-class approach for fake news detection, called LOSS-GAT.","Initially, we employ a two-step label propagation algorithm, utilizing Graph Neural Networks (GNNs) as an initial classifier to categorize news into two groups: interest (fake) and non-interest (real).","Subsequently, we enhance the graph structure using structural augmentation techniques.","Ultimately, we predict the final labels for all unlabeled data using a GNN that induces randomness within the local neighborhood of nodes through the aggregation function.","We evaluate our proposed method on five common datasets and compare the results against a set of baseline models, including both OCL and binary labeled models.","The results demonstrate that LOSS-GAT achieves a notable improvement, surpassing 10%, with the advantage of utilizing only a limited set of labeled fake news.","Noteworthy, LOSS-GAT even outperforms binary labeled models."],"url":"http://arxiv.org/abs/2402.08401v1","category":"cs.LG"}
{"created":"2024-02-13 11:59:43","title":"Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing","abstract":"Common certification methods operate on a flat pre-defined set of fine-grained classes. In this paper, however, we propose a novel, more general, and practical setting, namely adaptive hierarchical certification for image semantic segmentation. In this setting, the certification can be within a multi-level hierarchical label space composed of fine to coarse levels. Unlike classic methods where the certification would abstain for unstable components, our approach adaptively relaxes the certification to a coarser level within the hierarchy. This relaxation lowers the abstain rate whilst providing more certified semantically meaningful information. We mathematically formulate the problem setup and introduce, for the first time, an adaptive hierarchical certification algorithm for image semantic segmentation, that certifies image pixels within a hierarchy and prove the correctness of its guarantees. Since certified accuracy does not take the loss of information into account when traversing into a coarser hierarchy level, we introduce a novel evaluation paradigm for adaptive hierarchical certification, namely the certified information gain metric, which is proportional to the class granularity level. Our evaluation experiments on real-world challenging datasets such as Cityscapes and ACDC demonstrate that our adaptive algorithm achieves a higher certified information gain and a lower abstain rate compared to the current state-of-the-art certification method, as well as other non-adaptive versions of it.","sentences":["Common certification methods operate on a flat pre-defined set of fine-grained classes.","In this paper, however, we propose a novel, more general, and practical setting, namely adaptive hierarchical certification for image semantic segmentation.","In this setting, the certification can be within a multi-level hierarchical label space composed of fine to coarse levels.","Unlike classic methods where the certification would abstain for unstable components, our approach adaptively relaxes the certification to a coarser level within the hierarchy.","This relaxation lowers the abstain rate whilst providing more certified semantically meaningful information.","We mathematically formulate the problem setup and introduce, for the first time, an adaptive hierarchical certification algorithm for image semantic segmentation, that certifies image pixels within a hierarchy and prove the correctness of its guarantees.","Since certified accuracy does not take the loss of information into account when traversing into a coarser hierarchy level, we introduce a novel evaluation paradigm for adaptive hierarchical certification, namely the certified information gain metric, which is proportional to the class granularity level.","Our evaluation experiments on real-world challenging datasets such as Cityscapes and ACDC demonstrate that our adaptive algorithm achieves a higher certified information gain and a lower abstain rate compared to the current state-of-the-art certification method, as well as other non-adaptive versions of it."],"url":"http://arxiv.org/abs/2402.08400v1","category":"cs.LG"}
{"created":"2024-02-13 11:42:12","title":"New limiter regions for multidimensional flows","abstract":"Accurate transport algorithms are crucial for computational fluid dynamics and more accurate and efficient schemes are always in development. One dimensional limiting is a commonly employed technique used to suppress nonphysical oscillations. However, the application of such limiters can reduce accuracy. It is important to identify the weakest set of sufficient conditions required on the limiter as to allow the development of successful numerical algorithms.   The main goal of this paper is to identify new less restrictive sufficient conditions for flux form in-compressible advection to remain monotonic. First, we identify conditions in which the Spekreijse limiter region can fail to be monotonic for incompressible flux form advection and demonstrate this numerically. Then a convex combination argument is used to derive new sufficient conditions that are less restrictive than the Sweby region for a discrete maximum principle. This allows the introduction of two new more general limiter regions suitable for flux form incompressible advection.","sentences":["Accurate transport algorithms are crucial for computational fluid dynamics and more accurate and efficient schemes are always in development.","One dimensional limiting is a commonly employed technique used to suppress nonphysical oscillations.","However, the application of such limiters can reduce accuracy.","It is important to identify the weakest set of sufficient conditions required on the limiter as to allow the development of successful numerical algorithms.   ","The main goal of this paper is to identify new less restrictive sufficient conditions for flux form in-compressible advection to remain monotonic.","First, we identify conditions in which the Spekreijse limiter region can fail to be monotonic for incompressible flux form advection and demonstrate this numerically.","Then a convex combination argument is used to derive new sufficient conditions that are less restrictive than the Sweby region for a discrete maximum principle.","This allows the introduction of two new more general limiter regions suitable for flux form incompressible advection."],"url":"http://arxiv.org/abs/2402.08395v1","category":"math.NA"}
{"created":"2024-02-13 11:32:06","title":"Towards the Densest Polydisperse Disk Packing","abstract":"Understanding the way disordered particle packings transition between jammed (rigid) and unjammed (fluid) states is of both great practical importance and strong fundamental interest. The values of critical packing fraction (and other state variables) at the jamming transition are known to be protocol dependent. Here, we demonstrate that this variability at transition can be systematically traced to structural measures of packing, as well as to energy measures inside the jamming regime. Unlike annealing simulations from the fluid state, we use a novel generalized simultaneous particle swap algorithm in the jammed regime to construct states of desired energy, which upon decompression lead to predictable critical packing fractions. Thus, jamming states with extraordinarily high critical packing fraction can be found with little computational effort. These states can sustain substantial shear strain and preserve their special packing structure over the entire jammed domain. The close relation revealed here between the energy landscape of overjammed soft-particle packings and the behavior near the jamming transition points towards new ways of understanding and constructing disordered materials with exceptional properties.","sentences":["Understanding the way disordered particle packings transition between jammed (rigid) and unjammed (fluid) states is of both great practical importance and strong fundamental interest.","The values of critical packing fraction (and other state variables) at the jamming transition are known to be protocol dependent.","Here, we demonstrate that this variability at transition can be systematically traced to structural measures of packing, as well as to energy measures inside the jamming regime.","Unlike annealing simulations from the fluid state, we use a novel generalized simultaneous particle swap algorithm in the jammed regime to construct states of desired energy, which upon decompression lead to predictable critical packing fractions.","Thus, jamming states with extraordinarily high critical packing fraction can be found with little computational effort.","These states can sustain substantial shear strain and preserve their special packing structure over the entire jammed domain.","The close relation revealed here between the energy landscape of overjammed soft-particle packings and the behavior near the jamming transition points towards new ways of understanding and constructing disordered materials with exceptional properties."],"url":"http://arxiv.org/abs/2402.08390v1","category":"cond-mat.soft"}
{"created":"2024-02-13 11:30:06","title":"Entropy bounds for the absolute convex hull of tensors","abstract":"We derive entropy bounds for the absolute convex hull of vectors $X= (x_1 , \\ldots , x_p)\\in \\mathbb{R}^{n \\times p} $ in $\\mathbb{R}^n$ and apply this to the case where $X$ is the $d$-fold tensor matrix $$X = \\underbrace{\\Psi \\otimes \\cdots \\otimes \\Psi}_{d \\ {\\rm times} }\\in \\mathbb{R}^{m^d \\times r^d },$$ with a given $\\Psi = ( \\psi_1 , \\ldots , \\psi_r ) \\in \\mathbb{R}^{m \\times r} $, normalized to that $ \\| \\psi_j \\|_2 \\le 1$ for all $j \\in \\{1 , \\ldots , r\\}$. For $\\epsilon >0$ we let ${\\cal V} \\subset \\mathbb{R}^m$ be the linear space with smallest dimension $M ( \\epsilon , \\Psi)$ such that $ \\max_{1 \\le j \\le r } \\min_{v \\in {\\cal V} } \\| \\psi_j - v \\|_2 \\le \\epsilon$. We call $M( \\epsilon , \\psi)$ the $\\epsilon$-approximation of $\\Psi$ and assume it is -- up to log terms -- polynomial in $\\epsilon$. We show that the entropy of the absolute convex hull of the $d$-fold tensor matrix $X$ is up to log-terms of the same order as the entropy for the case $d=1$. The results are generalized to absolute convex hulls of tensors of functions in $L_2 (\\mu)$ where $\\mu$ is Lebesgue measure on $[0,1]$. As an application we consider the space of functions on $[0,1]^d$ with bounded $q$-th order Vitali total variation for a given $q \\in \\mathbb{N}$. As a by-product, we construct an orthonormal, piecewise polynomial, wavelet dictionary for functions that are well-approximated by piecewise polynomials.","sentences":["We derive entropy bounds for the absolute convex hull of vectors $X= (x_1 , \\ldots , x_p)\\in","\\mathbb{R}^{n \\times p} $ in $\\mathbb{R}^n$ and apply this to the case where $X$ is the $d$-fold tensor matrix $$X = \\underbrace{\\Psi \\otimes \\cdots \\otimes \\Psi}_{d \\ {\\rm times} }\\in \\mathbb{R}^{m^d \\times r^d },$$ with a given $\\Psi = ( \\psi_1 , \\ldots , \\psi_r ) \\in \\mathbb{R}^{m \\times r} $, normalized to that $ \\| \\psi_j \\|_2 \\le 1$ for all $j \\in \\{1 , \\ldots , r\\}$. For $\\epsilon >0$ we let ${\\cal V} \\subset \\mathbb{R}^m$ be the linear space with smallest dimension $M ( \\epsilon , \\Psi)$ such that $ \\max_{1 \\le j \\le r } \\min_{v \\in {\\cal V} } \\| \\psi_j - v \\|_2 \\le \\epsilon$. We call $M( \\epsilon , \\psi)$ the $\\epsilon$-approximation of $\\Psi$ and assume it is -- up to log terms -- polynomial in $\\epsilon$. We show that the entropy of the absolute convex hull of the $d$-fold tensor matrix $X$ is up to log-terms of the same order as the entropy for the case $d=1$. The results are generalized to absolute convex hulls of tensors of functions in $L_2 (\\mu)$ where $\\mu$ is Lebesgue measure on $[0,1]$. As an application we consider the space of functions on $[0,1]^d$ with bounded $q$-th order Vitali total variation for a given $q \\in \\mathbb{N}$. As a by-product, we construct an orthonormal, piecewise polynomial, wavelet dictionary for functions that are well-approximated by piecewise polynomials."],"url":"http://arxiv.org/abs/2402.08388v1","category":"math.ST"}
{"created":"2024-02-13 11:26:17","title":"Portfolio Optimization under Transaction Costs with Recursive Preferences","abstract":"The Merton investment-consumption problem is fundamental, both in the field of finance, and in stochastic control. An important extension of the problem adds transaction costs, which is highly relevant from a financial perspective but also challenging from a control perspective because the solution now involves singular control. A further significant extension takes us from additive utility to stochastic differential utility (SDU), which allows time preferences and risk preferences to be disentangled.   In this paper, we study this extended version of the Merton problem with proportional transaction costs and Epstein-Zin SDU. We fully characterise all parameter combinations for which the problem is well posed (which may depend on the level of transaction costs) and provide a full verification argument that relies on no additional technical assumptions and uses primal methods only. The case with SDU requires new mathematical techniques as duality methods break down.   Even in the special case of (additive) power utility, our arguments are significantly simpler, more elegant and more far-reaching than the ones in the extant literature. This means that we can easily analyse aspects of the problem which previously have been very challenging, including comparative statics, boundary cases which heretofore have required separate treatment and the situation beyond the small transaction cost regime. A key and novel idea is to parametrise consumption and the value function in terms of the shadow fraction of wealth, which may be of much wider applicability.","sentences":["The Merton investment-consumption problem is fundamental, both in the field of finance, and in stochastic control.","An important extension of the problem adds transaction costs, which is highly relevant from a financial perspective but also challenging from a control perspective because the solution now involves singular control.","A further significant extension takes us from additive utility to stochastic differential utility (SDU), which allows time preferences and risk preferences to be disentangled.   ","In this paper, we study this extended version of the Merton problem with proportional transaction costs and Epstein-Zin SDU.","We fully characterise all parameter combinations for which the problem is well posed (which may depend on the level of transaction costs) and provide a full verification argument that relies on no additional technical assumptions and uses primal methods only.","The case with SDU requires new mathematical techniques as duality methods break down.   ","Even in the special case of (additive) power utility, our arguments are significantly simpler, more elegant and more far-reaching than the ones in the extant literature.","This means that we can easily analyse aspects of the problem which previously have been very challenging, including comparative statics, boundary cases which heretofore have required separate treatment and the situation beyond the small transaction cost regime.","A key and novel idea is to parametrise consumption and the value function in terms of the shadow fraction of wealth, which may be of much wider applicability."],"url":"http://arxiv.org/abs/2402.08387v1","category":"econ.GN"}
{"created":"2024-02-13 11:25:43","title":"Approaching the structure of rotating bodies from dimension reduction","abstract":"We show that the two-dimensional structure of a rigidly rotating self-gravitating body is accessible with relatively good precision by assuming a purely spheroidal stratification. With this hypothesis, the two-dimensional problem becomes one-dimensional, and consists in solving two coupled fixed-point equations in terms of equatorial mass density and eccentricity of isopycnics. We propose a simple algorithm of resolution based on the self-consistent field method. Compared to the full unconstrained-surface two-dimensional problem, the precision in the normalized enthalpy field is better than $10^{-3}$ in absolute, and the computing time is drastically reduced. In addition, this one-dimensional approach is fully appropriate to fast rotators, works for any density profile (including any barotropic equation of state), and can account for mass density jumps in the system, including the existence of an ambient pressure. Several tests are given.","sentences":["We show that the two-dimensional structure of a rigidly rotating self-gravitating body is accessible with relatively good precision by assuming a purely spheroidal stratification.","With this hypothesis, the two-dimensional problem becomes one-dimensional, and consists in solving two coupled fixed-point equations in terms of equatorial mass density and eccentricity of isopycnics.","We propose a simple algorithm of resolution based on the self-consistent field method.","Compared to the full unconstrained-surface two-dimensional problem, the precision in the normalized enthalpy field is better than $10^{-3}$ in absolute, and the computing time is drastically reduced.","In addition, this one-dimensional approach is fully appropriate to fast rotators, works for any density profile (including any barotropic equation of state), and can account for mass density jumps in the system, including the existence of an ambient pressure.","Several tests are given."],"url":"http://arxiv.org/abs/2402.08386v1","category":"astro-ph.SR"}
{"created":"2024-02-13 11:25:20","title":"Selective Learning: Towards Robust Calibration with Dynamic Regularization","abstract":"Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance. This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing. Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function. The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence. However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence). Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off. At a high level, DReg aims to obtain a more reliable model capable of acknowledging what it knows and does not know. Specifically, DReg effectively fits the labels for in-distribution samples (samples that should be learned) while applying regularization dynamically to samples beyond model capabilities (e.g., outliers), thereby obtaining a robust calibrated model especially on the samples beyond model capabilities. Both theoretical and empirical analyses sufficiently demonstrate the superiority of DReg compared with previous methods.","sentences":["Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance.","This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing.","Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function.","The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence.","However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence).","Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off.","At a high level, DReg aims to obtain a more reliable model capable of acknowledging what it knows and does not know.","Specifically, DReg effectively fits the labels for in-distribution samples (samples that should be learned) while applying regularization dynamically to samples beyond model capabilities (e.g., outliers), thereby obtaining a robust calibrated model especially on the samples beyond model capabilities.","Both theoretical and empirical analyses sufficiently demonstrate the superiority of DReg compared with previous methods."],"url":"http://arxiv.org/abs/2402.08384v1","category":"cs.LG"}
{"created":"2024-02-13 11:22:59","title":"Uncertainty Quantification for Forward and Inverse Problems of PDEs via Latent Global Evolution","abstract":"Deep learning-based surrogate models have demonstrated remarkable advantages over classical solvers in terms of speed, often achieving speedups of 10 to 1000 times over traditional partial differential equation (PDE) solvers. However, a significant challenge hindering their widespread adoption in both scientific and industrial domains is the lack of understanding about their prediction uncertainties, particularly in scenarios that involve critical decision making. To address this limitation, we propose a method that integrates efficient and precise uncertainty quantification into a deep learning-based surrogate model. Our method, termed Latent Evolution of PDEs with Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based surrogate models with robust and efficient uncertainty quantification capabilities for both forward and inverse problems. LE-PDE-UQ leverages latent vectors within a latent space to evolve both the system's state and its corresponding uncertainty estimation. The latent vectors are decoded to provide predictions for the system's state as well as estimates of its uncertainty. In extensive experiments, we demonstrate the accurate uncertainty quantification performance of our approach, surpassing that of strong baselines including deep ensembles, Bayesian neural network layers, and dropout. Our method excels at propagating uncertainty over extended auto-regressive rollouts, making it suitable for scenarios involving long-term predictions. Our code is available at: https://github.com/AI4Science-WestlakeU/le-pde-uq.","sentences":["Deep learning-based surrogate models have demonstrated remarkable advantages over classical solvers in terms of speed, often achieving speedups of 10 to 1000 times over traditional partial differential equation (PDE) solvers.","However, a significant challenge hindering their widespread adoption in both scientific and industrial domains is the lack of understanding about their prediction uncertainties, particularly in scenarios that involve critical decision making.","To address this limitation, we propose a method that integrates efficient and precise uncertainty quantification into a deep learning-based surrogate model.","Our method, termed Latent Evolution of PDEs with Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based surrogate models with robust and efficient uncertainty quantification capabilities for both forward and inverse problems.","LE-PDE-UQ leverages latent vectors within a latent space to evolve both the system's state and its corresponding uncertainty estimation.","The latent vectors are decoded to provide predictions for the system's state as well as estimates of its uncertainty.","In extensive experiments, we demonstrate the accurate uncertainty quantification performance of our approach, surpassing that of strong baselines including deep ensembles, Bayesian neural network layers, and dropout.","Our method excels at propagating uncertainty over extended auto-regressive rollouts, making it suitable for scenarios involving long-term predictions.","Our code is available at: https://github.com/AI4Science-WestlakeU/le-pde-uq."],"url":"http://arxiv.org/abs/2402.08383v1","category":"cs.LG"}
{"created":"2024-02-13 11:18:27","title":"The Duet of Representations and How Explanations Exacerbate It","abstract":"An algorithm effects a causal representation of relations between features and labels in the human's perception. Such a representation might conflict with the human's prior belief. Explanations can direct the human's attention to the conflicting feature and away from other relevant features. This leads to causal overattribution and may adversely affect the human's information processing. In a field experiment we implemented an XGBoost-trained model as a decision-making aid for counselors at a public employment service to predict candidates' risk of long-term unemployment. The treatment group of counselors was also provided with SHAP. The results show that the quality of the human's decision-making is worse when a feature on which the human holds a conflicting prior belief is displayed as part of the explanation.","sentences":["An algorithm effects a causal representation of relations between features and labels in the human's perception.","Such a representation might conflict with the human's prior belief.","Explanations can direct the human's attention to the conflicting feature and away from other relevant features.","This leads to causal overattribution and may adversely affect the human's information processing.","In a field experiment we implemented an XGBoost-trained model as a decision-making aid for counselors at a public employment service to predict candidates' risk of long-term unemployment.","The treatment group of counselors was also provided with SHAP.","The results show that the quality of the human's decision-making is worse when a feature on which the human holds a conflicting prior belief is displayed as part of the explanation."],"url":"http://arxiv.org/abs/2402.08379v1","category":"cs.HC"}
{"created":"2024-02-13 11:15:31","title":"\\texttt{VAMPyR} -- A High-Level Python Library for Mathematical Operations in a Multiwavelets Representation","abstract":"Wavelets and Multiwavelets have lately been adopted in Quantum Chemistry to overcome challenges presented by the two main families of basis sets: Gaussian atomic orbitals and plane waves. In addition to their numerical advantages (high precision, locality, fast algorithms for operator application, linear scaling with respect to system size, to mention a few) they provide a framework which narrows the gap between the theoretical formalism of the fundamental equations and the practical implementation in a working code. This realization led us to the development of the Python library called \\texttt{VAMPyR}, (Very Accurate Multiresolution Python Routines). \\texttt{VAMPyR} encodes the binding to a C++ library for Multiwavelet calculations (algebra, integral and differential operator application) and exposes the required functionality to write simple Python code to solve among others, the Hartree--Fock equations, the generalized Poisson Equation, the Dirac equation and the time-dependent Schr\\\"odinger equation up to any predefined precision. In this contribution we will outline the main features of Multiresolution Analysis using multiwavelets and we will describe the design of the code. A few illustrative examples will show the code capabilities and its interoperability with other software platforms.","sentences":["Wavelets and Multiwavelets have lately been adopted in Quantum Chemistry to overcome challenges presented by the two main families of basis sets: Gaussian atomic orbitals and plane waves.","In addition to their numerical advantages (high precision, locality, fast algorithms for operator application, linear scaling with respect to system size, to mention a few) they provide a framework which narrows the gap between the theoretical formalism of the fundamental equations and the practical implementation in a working code.","This realization led us to the development of the Python library called \\texttt{VAMPyR}, (Very Accurate Multiresolution Python Routines).","\\texttt{VAMPyR} encodes the binding to a C++ library for Multiwavelet calculations (algebra, integral and differential operator application) and exposes the required functionality to write simple Python code to solve among others, the Hartree--Fock equations, the generalized Poisson Equation, the Dirac equation and the time-dependent Schr\\\"odinger equation up to any predefined precision.","In this contribution we will outline the main features of Multiresolution Analysis using multiwavelets and we will describe the design of the code.","A few illustrative examples will show the code capabilities and its interoperability with other software platforms."],"url":"http://arxiv.org/abs/2402.08377v1","category":"physics.chem-ph"}
{"created":"2024-02-13 11:13:47","title":"The generalized Hausman test for detecting non-normality in the latent variable distribution of the two-parameter IRT model","abstract":"This paper introduces the generalized Hausman test as a novel method for detecting non-normality of the latent variable distribution of unidimensional Item Response Theory (IRT) models for binary data. The test utilizes the pairwise maximum likelihood estimator obtained for the parameters of the classical two-parameter IRT model, which assumes normality of the latent variable, and the quasi-maximum likelihood estimator obtained under a semi-nonparametric framework, allowing for a more flexible distribution of the latent variable. The performance of the generalized Hausman test is evaluated through a simulation study and it is compared with the likelihood-ratio and the M2 test statistics. Additionally, various information criteria are computed. The simulation results show that the generalized Hausman test outperforms the other tests under most conditions. However, the results obtained from the information criteria are somewhat contradictory under certain conditions, suggesting a need for further investigation and interpretation.","sentences":["This paper introduces the generalized Hausman test as a novel method for detecting non-normality of the latent variable distribution of unidimensional Item Response Theory (IRT) models for binary data.","The test utilizes the pairwise maximum likelihood estimator obtained for the parameters of the classical two-parameter IRT model, which assumes normality of the latent variable, and the quasi-maximum likelihood estimator obtained under a semi-nonparametric framework, allowing for a more flexible distribution of the latent variable.","The performance of the generalized Hausman test is evaluated through a simulation study and it is compared with the likelihood-ratio and the M2 test statistics.","Additionally, various information criteria are computed.","The simulation results show that the generalized Hausman test outperforms the other tests under most conditions.","However, the results obtained from the information criteria are somewhat contradictory under certain conditions, suggesting a need for further investigation and interpretation."],"url":"http://arxiv.org/abs/2402.08376v1","category":"stat.ME"}
{"created":"2024-02-13 11:12:44","title":"An adiabatic approach to the trans-Planckian problem in Loop Quantum Cosmology","abstract":"We study the scalar modes that, being observable today, were trans-Planckian before inflation, within the context of hybrid Loop Quantum Cosmology (LQC). We analyse the dynamics of these highly ultraviolet modes by introducing modified dispersion relations to their equations of motion and discuss the impact that these relations would introduce in the power spectrum by computing the adiabaticity coefficient. More precisely, we consider two different models compatible with observations for the standard linear dispersion relation which are based on different initial conditions for the perturbations and background. One of these models avoids the issue altogether by generating less $e$-folds of inflation, so that the observable modes are never trans-Planckian, whereas the other suffers (arguably softly) from the trans-Planckian problem. This shows that the existence of the trans-Planckian problem in LQC is model-dependent.","sentences":["We study the scalar modes that, being observable today, were trans-Planckian before inflation, within the context of hybrid Loop Quantum Cosmology (LQC).","We analyse the dynamics of these highly ultraviolet modes by introducing modified dispersion relations to their equations of motion and discuss the impact that these relations would introduce in the power spectrum by computing the adiabaticity coefficient.","More precisely, we consider two different models compatible with observations for the standard linear dispersion relation which are based on different initial conditions for the perturbations and background.","One of these models avoids the issue altogether by generating less $e$-folds of inflation, so that the observable modes are never trans-Planckian, whereas the other suffers (arguably softly) from the trans-Planckian problem.","This shows that the existence of the trans-Planckian problem in LQC is model-dependent."],"url":"http://arxiv.org/abs/2402.08375v1","category":"gr-qc"}
{"created":"2024-02-13 11:10:14","title":"Time-Series Classification for Dynamic Strategies in Multi-Step Forecasting","abstract":"Multi-step forecasting (MSF) in time-series, the ability to make predictions multiple time steps into the future, is fundamental to almost all temporal domains. To make such forecasts, one must assume the recursive complexity of the temporal dynamics. Such assumptions are referred to as the forecasting strategy used to train a predictive model. Previous work shows that it is not clear which forecasting strategy is optimal a priori to evaluating on unseen data. Furthermore, current approaches to MSF use a single (fixed) forecasting strategy.   In this paper, we characterise the instance-level variance of optimal forecasting strategies and propose Dynamic Strategies (DyStrat) for MSF. We experiment using 10 datasets from different scales, domains, and lengths of multi-step horizons. When using a random-forest-based classifier, DyStrat outperforms the best fixed strategy, which is not knowable a priori, 94% of the time, with an average reduction in mean-squared error of 11%. Our approach typically triples the top-1 accuracy compared to current approaches. Notably, we show DyStrat generalises well for any MSF task.","sentences":["Multi-step forecasting (MSF) in time-series, the ability to make predictions multiple time steps into the future, is fundamental to almost all temporal domains.","To make such forecasts, one must assume the recursive complexity of the temporal dynamics.","Such assumptions are referred to as the forecasting strategy used to train a predictive model.","Previous work shows that it is not clear which forecasting strategy is optimal a priori to evaluating on unseen data.","Furthermore, current approaches to MSF use a single (fixed) forecasting strategy.   ","In this paper, we characterise the instance-level variance of optimal forecasting strategies and propose Dynamic Strategies (DyStrat) for MSF.","We experiment using 10 datasets from different scales, domains, and lengths of multi-step horizons.","When using a random-forest-based classifier, DyStrat outperforms the best fixed strategy, which is not knowable a priori, 94% of the time, with an average reduction in mean-squared error of 11%.","Our approach typically triples the top-1 accuracy compared to current approaches.","Notably, we show DyStrat generalises well for any MSF task."],"url":"http://arxiv.org/abs/2402.08373v1","category":"cs.LG"}
{"created":"2024-02-13 11:06:33","title":"Dynamic Force Spectroscopy of Phase-Change Crystallographic Twinning","abstract":"Crystallographic twinning dynamics exhibit nanoscale force fluctuations that, when probed by dynamic force spectroscopy, reveal the underlying interactions within the lattice. We explore twinning in single crystalline titanium nickel, a type of lattice instability mediated by a martensitic phase transition, which creates mirror planes in the lattice. Exploiting the responses generated under imposed strain ramps, we show that the dynamic force spectra of the twinning rate process across various applied strain rates can be reconciled using a statistical mechanics-based framework, demonstrating a more effective utilisation of stochastic responses. The twinning mechanism is mapped onto a free energy landscape to model the microscopic evolution of atomic configurations under imposed time-dependent deformation. The framework enables consistent inference of the relevant fundamental properties that define the structural transition rate process. The study demonstrates how the statistical characterisation of nanomechanical response-stimulus patterns can offer microscopic insights into the deformation behaviours of crystalline materials.","sentences":["Crystallographic twinning dynamics exhibit nanoscale force fluctuations that, when probed by dynamic force spectroscopy, reveal the underlying interactions within the lattice.","We explore twinning in single crystalline titanium nickel, a type of lattice instability mediated by a martensitic phase transition, which creates mirror planes in the lattice.","Exploiting the responses generated under imposed strain ramps, we show that the dynamic force spectra of the twinning rate process across various applied strain rates can be reconciled using a statistical mechanics-based framework, demonstrating a more effective utilisation of stochastic responses.","The twinning mechanism is mapped onto a free energy landscape to model the microscopic evolution of atomic configurations under imposed time-dependent deformation.","The framework enables consistent inference of the relevant fundamental properties that define the structural transition rate process.","The study demonstrates how the statistical characterisation of nanomechanical response-stimulus patterns can offer microscopic insights into the deformation behaviours of crystalline materials."],"url":"http://arxiv.org/abs/2402.08372v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-13 11:01:52","title":"One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill","abstract":"One-shot imitation is to learn a new task from a single demonstration, yet it is a challenging problem to adopt it for complex tasks with the high domain diversity inherent in a non-stationary environment. To tackle the problem, we explore the compositionality of complex tasks, and present a novel skill-based imitation learning framework enabling one-shot imitation and zero-shot adaptation; from a single demonstration for a complex unseen task, a semantic skill sequence is inferred and then each skill in the sequence is converted into an action sequence optimized for environmental hidden dynamics that can vary over time. Specifically, we leverage a vision-language model to learn a semantic skill set from offline video datasets, where each skill is represented on the vision-language embedding space, and adapt meta-learning with dynamics inference to enable zero-shot skill adaptation. We evaluate our framework with various one-shot imitation scenarios for extended multi-stage Meta-world tasks, showing its superiority in learning complex tasks, generalizing to dynamics changes, and extending to different demonstration conditions and modalities, compared to other baselines.","sentences":["One-shot imitation is to learn a new task from a single demonstration, yet it is a challenging problem to adopt it for complex tasks with the high domain diversity inherent in a non-stationary environment.","To tackle the problem, we explore the compositionality of complex tasks, and present a novel skill-based imitation learning framework enabling one-shot imitation and zero-shot adaptation; from a single demonstration for a complex unseen task, a semantic skill sequence is inferred and then each skill in the sequence is converted into an action sequence optimized for environmental hidden dynamics that can vary over time.","Specifically, we leverage a vision-language model to learn a semantic skill set from offline video datasets, where each skill is represented on the vision-language embedding space, and adapt meta-learning with dynamics inference to enable zero-shot skill adaptation.","We evaluate our framework with various one-shot imitation scenarios for extended multi-stage Meta-world tasks, showing its superiority in learning complex tasks, generalizing to dynamics changes, and extending to different demonstration conditions and modalities, compared to other baselines."],"url":"http://arxiv.org/abs/2402.08369v1","category":"cs.AI"}
{"created":"2024-02-13 10:50:54","title":"NeuRes: Learning Proofs of Propositional Satisfiability","abstract":"We introduce NeuRes, a neuro-symbolic proof-based SAT solver. Unlike other neural SAT solving methods, NeuRes is capable of proving unsatisfiability as opposed to merely predicting it. By design, NeuRes operates in a certificate-driven fashion by employing propositional resolution to prove unsatisfiability and to accelerate the process of finding satisfying truth assignments in case of unsat and sat formulas, respectively. To realize this, we propose a novel architecture that adapts elements from Graph Neural Networks and Pointer Networks to autoregressively select pairs of nodes from a dynamic graph structure, which is essential to the generation of resolution proofs. Our model is trained and evaluated on a dataset of teacher proofs and truth assignments that we compiled with the same random formula distribution used by NeuroSAT. In our experiments, we show that NeuRes solves more test formulas than NeuroSAT by a rather wide margin on different distributions while being much more data-efficient. Furthermore, we show that NeuRes is capable of largely shortening teacher proofs by notable proportions. We use this feature to devise a bootstrapped training procedure that manages to reduce a dataset of proofs generated by an advanced solver by ~23% after training on it with no extra guidance.","sentences":["We introduce NeuRes, a neuro-symbolic proof-based SAT solver.","Unlike other neural SAT solving methods, NeuRes is capable of proving unsatisfiability as opposed to merely predicting it.","By design, NeuRes operates in a certificate-driven fashion by employing propositional resolution to prove unsatisfiability and to accelerate the process of finding satisfying truth assignments in case of unsat and sat formulas, respectively.","To realize this, we propose a novel architecture that adapts elements from Graph Neural Networks and Pointer Networks to autoregressively select pairs of nodes from a dynamic graph structure, which is essential to the generation of resolution proofs.","Our model is trained and evaluated on a dataset of teacher proofs and truth assignments that we compiled with the same random formula distribution used by NeuroSAT.","In our experiments, we show that NeuRes solves more test formulas than NeuroSAT by a rather wide margin on different distributions while being much more data-efficient.","Furthermore, we show that NeuRes is capable of largely shortening teacher proofs by notable proportions.","We use this feature to devise a bootstrapped training procedure that manages to reduce a dataset of proofs generated by an advanced solver by ~23% after training on it with no extra guidance."],"url":"http://arxiv.org/abs/2402.08365v1","category":"cs.LG"}
{"created":"2024-02-13 10:45:03","title":"Multiple higher-order poles solutions in spinor Bose-Einstein condensates","abstract":"In this study, we explore multiple higher-order pole solutions in spinor Bose--Einstein condensates. These solutions are associated with different pairs of higher-order poles of the transmission coefficient in the inverse scattering transform, and they represent solutions of the spin-1 Gross--Pitaevskii equation. We introduce a direct scattering map that maps initial data to scattering data, which includes discrete spectrums, reflection coefficients, and a polynomial that replaces normalization constants. To analyze symmetries and discrete spectrums in the direct problem, we introduce a generalized cross product in 4-dimensional vector space. Additionally, we characterize the inverse problem in terms of a $4\\times 4$ matrix Riemann--Hilbert problem that is subject to residue conditions at these higher-order poles. In the reflectionless scenario, the Riemann--Hilbert problem can be converted into a linear algebraic system. The resulting algebraic system has a unique solution, which allows us to display multiple higher-order poles solutions.","sentences":["In this study, we explore multiple higher-order pole solutions in spinor Bose--Einstein condensates.","These solutions are associated with different pairs of higher-order poles of the transmission coefficient in the inverse scattering transform, and they represent solutions of the spin-1 Gross--Pitaevskii equation.","We introduce a direct scattering map that maps initial data to scattering data, which includes discrete spectrums, reflection coefficients, and a polynomial that replaces normalization constants.","To analyze symmetries and discrete spectrums in the direct problem, we introduce a generalized cross product in 4-dimensional vector space.","Additionally, we characterize the inverse problem in terms of a $4\\times 4$ matrix Riemann--Hilbert problem that is subject to residue conditions at these higher-order poles.","In the reflectionless scenario, the Riemann--Hilbert problem can be converted into a linear algebraic system.","The resulting algebraic system has a unique solution, which allows us to display multiple higher-order poles solutions."],"url":"http://arxiv.org/abs/2402.08362v1","category":"nlin.SI"}
{"created":"2024-02-13 10:37:17","title":"The binary actions of simple groups of Lie type of characteristic 2","abstract":"Let $\\mathcal{C}$ be a conjugacy class of involutions in a group $G$. We study the graph $\\Gamma(\\mathcal{C})$ whose vertices are elements of $\\mathcal{C}$ with $g,h\\in\\mathcal{C}$ connected by an edge if and only if $gh\\in\\mathcal{C}$. For $t\\in \\mathcal{C}$, we define the component group of $t$ to be the subgroup of $G$ generated by all vertices in $\\Gamma(\\mathcal{C})$ that lie in the connected component of the graph that contains $t$.   We classify the component groups of all involutions in simple groups of Lie type over a field of characteristic $2$. We use this classification to partially classify the transitive binary actions of the simple groups of Lie type over a field of characteristic $2$ for which a point stabilizer has even order. The classification is complete unless the simple group in question is a symplectic or unitary group.","sentences":["Let $\\mathcal{C}$ be a conjugacy class of involutions in a group $G$. We study the graph $\\Gamma(\\mathcal{C})$ whose vertices are elements of $\\mathcal{C}$ with $g,h\\in\\mathcal{C}$ connected by an edge if and only if $gh\\in\\mathcal{C}$. For $t\\in \\mathcal{C}$, we define the component group of $t$ to be the subgroup of $G$ generated by all vertices in $\\Gamma(\\mathcal{C})$ that lie in the connected component of the graph that contains $t$.   We classify the component groups of all involutions in simple groups of Lie type over a field of characteristic $2$. We use this classification to partially classify the transitive binary actions of the simple groups of Lie type over a field of characteristic $2$ for which a point stabilizer has even order.","The classification is complete unless the simple group in question is a symplectic or unitary group."],"url":"http://arxiv.org/abs/2402.08357v1","category":"math.GR"}
{"created":"2024-02-13 10:28:57","title":"Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries","abstract":"Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA World Cup 2022, during which about 6K natural language questions were asked and executed. All of our data is based on real user questions that were asked live to the system. We manually labeled and translated a subset of these questions for three different data models. For each data model, we explore the performance of representative Text-to-SQL systems and language models. We further quantify the impact of training data size, pre-, and post-processing steps as well as language model inference time. Our comprehensive evaluation sheds light on the design choices of real-world Text-to-SQL systems and their impact on moving from research prototypes to real deployments. Last, we provide a new benchmark dataset to the community, which is the first to enable the evaluation of different data models for the same dataset and is substantially more challenging than most previous datasets in terms of query complexity.","sentences":["Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access.","These systems translate user requests in natural language to valid SQL statements for a specific database.","Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models.","However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing.","This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces.","Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA World Cup 2022, during which about 6K natural language questions were asked and executed.","All of our data is based on real user questions that were asked live to the system.","We manually labeled and translated a subset of these questions for three different data models.","For each data model, we explore the performance of representative Text-to-SQL systems and language models.","We further quantify the impact of training data size, pre-, and post-processing steps as well as language model inference time.","Our comprehensive evaluation sheds light on the design choices of real-world Text-to-SQL systems and their impact on moving from research prototypes to real deployments.","Last, we provide a new benchmark dataset to the community, which is the first to enable the evaluation of different data models for the same dataset and is substantially more challenging than most previous datasets in terms of query complexity."],"url":"http://arxiv.org/abs/2402.08349v1","category":"cs.DB"}
{"created":"2024-02-13 10:25:45","title":"Visually Dehallucinative Instruction Generation","abstract":"In recent years, synthetic visual instructions by generative language model have demonstrated plausible text generation performance on the visual question-answering tasks. However, challenges persist in the hallucination of generative language models, i.e., the generated image-text data contains unintended contents. This paper presents a novel and scalable method for generating visually dehallucinative instructions, dubbed CAP2QA, that constrains the scope to only image contents. Our key contributions lie in introducing image-aligned instructive QA dataset CAP2QA-COCO and its scalable recipe. In our experiments, we compare synthetic visual instruction datasets that share the same source data by visual instruction tuning and conduct general visual recognition tasks. It shows that our proposed method significantly reduces visual hallucination while consistently improving visual recognition ability and expressiveness.","sentences":["In recent years, synthetic visual instructions by generative language model have demonstrated plausible text generation performance on the visual question-answering tasks.","However, challenges persist in the hallucination of generative language models, i.e., the generated image-text data contains unintended contents.","This paper presents a novel and scalable method for generating visually dehallucinative instructions, dubbed CAP2QA, that constrains the scope to only image contents.","Our key contributions lie in introducing image-aligned instructive QA dataset CAP2QA-COCO and its scalable recipe.","In our experiments, we compare synthetic visual instruction datasets that share the same source data by visual instruction tuning and conduct general visual recognition tasks.","It shows that our proposed method significantly reduces visual hallucination while consistently improving visual recognition ability and expressiveness."],"url":"http://arxiv.org/abs/2402.08348v1","category":"cs.CV"}
{"created":"2024-02-13 10:23:45","title":"Conditional Information Gain Trellis","abstract":"Conditional computing processes an input using only part of the neural network's computational units. Learning to execute parts of a deep convolutional network by routing individual samples has several advantages: Reducing the computational burden is an obvious advantage. Furthermore, if similar classes are routed to the same path, that part of the network learns to discriminate between finer differences and better classification accuracies can be attained with fewer parameters. Recently, several papers have exploited this idea to take a particular child of a node in a tree-shaped network or to skip parts of a network. In this work, we follow a Trellis-based approach for generating specific execution paths in a deep convolutional neural network. We have designed routing mechanisms that use differentiable information gain-based cost functions to determine which subset of features in a convolutional layer will be executed. We call our method Conditional Information Gain Trellis (CIGT). We show that our conditional execution mechanism achieves comparable or better model performance compared to unconditional baselines, using only a fraction of the computational resources.","sentences":["Conditional computing processes an input using only part of the neural network's computational units.","Learning to execute parts of a deep convolutional network by routing individual samples has several advantages: Reducing the computational burden is an obvious advantage.","Furthermore, if similar classes are routed to the same path, that part of the network learns to discriminate between finer differences and better classification accuracies can be attained with fewer parameters.","Recently, several papers have exploited this idea to take a particular child of a node in a tree-shaped network or to skip parts of a network.","In this work, we follow a Trellis-based approach for generating specific execution paths in a deep convolutional neural network.","We have designed routing mechanisms that use differentiable information gain-based cost functions to determine which subset of features in a convolutional layer will be executed.","We call our method Conditional Information Gain Trellis (CIGT).","We show that our conditional execution mechanism achieves comparable or better model performance compared to unconditional baselines, using only a fraction of the computational resources."],"url":"http://arxiv.org/abs/2402.08345v1","category":"cs.CV"}
{"created":"2024-02-13 10:12:04","title":"Bernstein--Sato Polynomials for Positively Weighted Homogeneous Locally Everywhere Divisors, Hyperplane Arrangements, in $\\mathbb{C}^{3}$","abstract":"We consider the Bernstein--Sato polynomial of a polynomial $f \\in R = \\mathbb{C}[x_{1}, x_{2}, x_{3}]$ that analytically locally everywhere admits a positively weighted homogeneous defining equation. We construct, in the analytic category, a complex of $\\mathscr{D}_{X}[s]$-modules that can be used to compute the $\\mathscr{D}_{X}[s]$-dual of $\\mathscr{D}_{X}[s] f^{s-1}$ as the middle term of a short exact sequence where the outer terms are well understood. This extends a result by Narv\\'{a}ez Macarro where a freeness assumption was required.   We derive many results about the zeroes of the Bernstein--Sato polynomial. First, we prove each nonvanishing degree of the zeroeth local cohomology of the Milnor algebra $H_{\\mathfrak{m}}^{0} (R / (\\partial f))$ contributes a root to the Bernstein--Sato polynomial, generalizing a result of Saito's (where the argument cannot weaken homogeneity to positive homogeneity). Second, we prove the zeroes of Bernstein--Sato polynomial admit a partial symmetry about $-1$, extending a result of Narv\\'{a}ez Macarro that again required freeness. We give applications to very small roots, the twisted Logarithmic Comparison Theorem, and more precise statements when $f$ is additionally assumed to be homogeneous.   Finally, when $f$ defines a hyperplane arrangement in $\\mathbb{C}^{3}$ we give a complete formula for the zeroes of the Bernstein--Sato polynomial of $f$. We show all zeroes except the candidate root $-2 + (2 / \\text{deg}(f))$ are (easily) combinatorially given; we give many equivalent characterizations of when the only non-combinatorial candidate root $-2 + (2/ \\text{deg}(f))$ is in fact a zero of the Bernstein--Sato polynomial. One equivalent condition is the nonvanishing of $H_{\\mathfrak{m}}^{0}( R / (\\partial f))_{\\text{deg}(f) - 1}$.","sentences":["We consider the Bernstein--Sato polynomial of a polynomial $f \\in R = \\mathbb{C}[x_{1}, x_{2}, x_{3}]$ that analytically locally everywhere admits a positively weighted homogeneous defining equation.","We construct, in the analytic category, a complex of $\\mathscr{D}_{X}[s]$-modules that can be used to compute the $\\mathscr{D}_{X}[s]$-dual of $\\mathscr{D}_{X}[s] f^{s-1}$ as the middle term of a short exact sequence where the outer terms are well understood.","This extends a result by Narv\\'{a}ez Macarro where a freeness assumption was required.   ","We derive many results about the zeroes of the Bernstein--Sato polynomial.","First, we prove each nonvanishing degree of the zeroeth local cohomology of the Milnor algebra $H_{\\mathfrak{m}}^{0} (R / (\\partial f))$ contributes a root to the Bernstein--Sato polynomial, generalizing a result of Saito's (where the argument cannot weaken homogeneity to positive homogeneity).","Second, we prove the zeroes of Bernstein--Sato polynomial admit a partial symmetry about $-1$, extending a result of Narv\\'{a}ez Macarro that again required freeness.","We give applications to very small roots, the twisted Logarithmic Comparison Theorem, and more precise statements when $f$ is additionally assumed to be homogeneous.   ","Finally, when $f$ defines a hyperplane arrangement in $\\mathbb{C}^{3}$ we give a complete formula for the zeroes of the Bernstein--Sato polynomial of $f$. We show all zeroes except the candidate root $-2","+ (2 / \\text{deg}(f))$ are (easily) combinatorially given; we give many equivalent characterizations of when the only non-combinatorial candidate root $-2","+ (2/ \\text{deg}(f))$ is in fact a zero of the Bernstein--Sato polynomial.","One equivalent condition is the nonvanishing of $H_{\\mathfrak{m}}^{0}( R / (\\partial f))_{\\text{deg}(f) - 1}$."],"url":"http://arxiv.org/abs/2402.08342v1","category":"math.AG"}
{"created":"2024-02-13 10:09:00","title":"Eliciting Big Five Personality Traits in Large Language Models: A Textual Analysis with Classifier-Driven Approach","abstract":"Large Language Models (LLMs) are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these \"black-box\" models. Although previous studies have sought to increase the transparency of these models by investigating the personality traits of LLMs, many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts. Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly prompted multiple LMs with different parameter sizes, including Llama-2, Falcon, Mistral, Bloom, GPT, OPT, and XLNet (base and fine tuned versions) and examined their personality using classifiers trained on the myPersonality dataset. Our results reveal that, generally, all LLMs demonstrate high openness and low extraversion. However, whereas LMs with fewer parameters exhibit similar behaviour in personality traits, newer and LMs with more parameters exhibit a broader range of personality traits, with increased agreeableness, emotional stability, and openness. Furthermore, a greater number of parameters is positively associated with openness and conscientiousness. Moreover, fine-tuned models exhibit minor modulations in their personality traits, contingent on the dataset. Implications and directions for future research are discussed.","sentences":["Large Language Models (LLMs) are increasingly being utilized by both candidates and employers in the recruitment context.","However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these \"black-box\" models.","Although previous studies have sought to increase the transparency of these models by investigating the personality traits of LLMs, many of the previous studies have provided them with personality assessments to complete.","On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts.","Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs.","To do so, we repeatedly prompted multiple LMs with different parameter sizes, including Llama-2, Falcon, Mistral, Bloom, GPT, OPT, and XLNet (base and fine tuned versions) and examined their personality using classifiers trained on the myPersonality dataset.","Our results reveal that, generally, all LLMs demonstrate high openness and low extraversion.","However, whereas LMs with fewer parameters exhibit similar behaviour in personality traits, newer and LMs with more parameters exhibit a broader range of personality traits, with increased agreeableness, emotional stability, and openness.","Furthermore, a greater number of parameters is positively associated with openness and conscientiousness.","Moreover, fine-tuned models exhibit minor modulations in their personality traits, contingent on the dataset.","Implications and directions for future research are discussed."],"url":"http://arxiv.org/abs/2402.08341v1","category":"cs.CL"}
{"created":"2024-02-13 10:07:51","title":"Limiting behaviour of MacMahon-like q-series","abstract":"In a famous 1921 paper, MacMahon drew many connections between q-series, partitions, and divisor sums. Recently, it has been proven that many of MacMahon's generalized sum-of-divisor functions produce elements of the algebra of quasimodular forms and are intimately connected with q-multiple zeta values. A recent paper by Amdeberhan-Ono-Singh showed that MacMahon's infinite sequence of quasimodular forms is a family of approximations to the 3-colored partition function. In this note, we address a recent question of Ono by producing infinite families of generalized sum-of-divisor functions that approximate colored partition functions, and we determine their relationship to quasimodular forms and q-multiple zeta values.","sentences":["In a famous 1921 paper, MacMahon drew many connections between q-series, partitions, and divisor sums.","Recently, it has been proven that many of MacMahon's generalized sum-of-divisor functions produce elements of the algebra of quasimodular forms and are intimately connected with q-multiple zeta values.","A recent paper by Amdeberhan-Ono-Singh showed that MacMahon's infinite sequence of quasimodular forms is a family of approximations to the 3-colored partition function.","In this note, we address a recent question of Ono by producing infinite families of generalized sum-of-divisor functions that approximate colored partition functions, and we determine their relationship to quasimodular forms and q-multiple zeta values."],"url":"http://arxiv.org/abs/2402.08340v1","category":"math.NT"}
{"created":"2024-02-13 10:07:39","title":"Nonlocal elliptic PDEs with general nonlinearities","abstract":"In this thesis we investigate how the nonlocalities affect the study of different PDEs coming from physics, and we analyze these equations under almost optimal assumptions of the nonlinearity. In particular, we focus on the fractional Laplacian operator and on sources involving convolution with the Riesz potential, as well as on the interaction of the two, and we aim to do it through variational and topological methods.   We examine both quantitative and qualitative aspects, proving multiplicity of solutions for nonlocal nonlinear problems with free or prescribed mass, showing regularity, positivity, symmetry and sharp asymptotic decay of ground states, and exploring the influence of the topology of a potential well in presence of concentration phenomena. On the nonlinearities we consider general assumptions which avoid monotonicity and homogeneity: this generality obstructs the use of classical variational tools and forces the implementation of new ideas.   Throughout the thesis we develop some new tools: among them, a Lagrangian formulation modeled on Pohozaev mountains is used for the existence of normalized solutions, annuli-shaped multidimensional paths are built for genus-based multiplicity results, a fractional chain rule is proved to treat concave powers, and a fractional center of mass is defined to detect semiclassical standing waves. We believe that these tools could be used to face problems in different frameworks as well.","sentences":["In this thesis we investigate how the nonlocalities affect the study of different PDEs coming from physics, and we analyze these equations under almost optimal assumptions of the nonlinearity.","In particular, we focus on the fractional Laplacian operator and on sources involving convolution with the Riesz potential, as well as on the interaction of the two, and we aim to do it through variational and topological methods.   ","We examine both quantitative and qualitative aspects, proving multiplicity of solutions for nonlocal nonlinear problems with free or prescribed mass, showing regularity, positivity, symmetry and sharp asymptotic decay of ground states, and exploring the influence of the topology of a potential well in presence of concentration phenomena.","On the nonlinearities we consider general assumptions which avoid monotonicity and homogeneity: this generality obstructs the use of classical variational tools and forces the implementation of new ideas.   ","Throughout the thesis we develop some new tools: among them, a Lagrangian formulation modeled on Pohozaev mountains is used for the existence of normalized solutions, annuli-shaped multidimensional paths are built for genus-based multiplicity results, a fractional chain rule is proved to treat concave powers, and a fractional center of mass is defined to detect semiclassical standing waves.","We believe that these tools could be used to face problems in different frameworks as well."],"url":"http://arxiv.org/abs/2402.08338v1","category":"math.AP"}
{"created":"2024-02-13 09:57:46","title":"An Executable Specification of Oncology Dose-Escalation Protocols with Prolog","abstract":"We present, as a pure Prolog program, the first executable specification of the 3 + 3 dose-escalation protocol commonly used in early-phase oncology drug development. In this program, the imperative operations of the protocol emerge as consequences of clinically meaningful anticipatory-regret scenarios that are declared as CLP(Z) constraints. This 'regret-constrained' (RC) specification yields a robust formulation which can be used to prove clinically meaningful safety and liveness properties of the protocol before incorporating it into a trial, and then as an on-line decision support system while the trial is underway. Our RC specification also readily accommodates certain pragmatic modifications to trial enrollment which severely strain traditionally imperative formulations. The features of modern Prolog systems let us describe the 3 + 3 protocol with a short and general program that has desirable algebraic properties and can therefore be used, tested and reasoned about in several different ways.","sentences":["We present, as a pure Prolog program, the first executable specification of the 3 + 3 dose-escalation protocol commonly used in early-phase oncology drug development.","In this program, the imperative operations of the protocol emerge as consequences of clinically meaningful anticipatory-regret scenarios that are declared as CLP(Z) constraints.","This 'regret-constrained' (RC) specification yields a robust formulation which can be used to prove clinically meaningful safety and liveness properties of the protocol before incorporating it into a trial, and then as an on-line decision support system while the trial is underway.","Our RC specification also readily accommodates certain pragmatic modifications to trial enrollment which severely strain traditionally imperative formulations.","The features of modern Prolog systems let us describe the 3 + 3 protocol with a short and general program that has desirable algebraic properties and can therefore be used, tested and reasoned about in several different ways."],"url":"http://arxiv.org/abs/2402.08334v1","category":"cs.PL"}
{"created":"2024-02-13 09:57:04","title":"Detecting $K_{2,3}$ as an induced minor","abstract":"We consider a natural generalization of chordal graphs, in which every minimal separator induces a subgraph with independence number at most $2$. Such graphs can be equivalently defined as graphs that do not contain the complete bipartite graph $K_{2,3}$ as an induced minor, that is, graphs from which $K_{2,3}$ cannot be obtained by a sequence of edge contractions and vertex deletions.   We develop a polynomial-time algorithm for recognizing these graphs. Our algorithm relies on a characterization of $K_{2,3}$-induced minor-free graphs in terms of excluding particular induced subgraphs, called Truemper configurations.","sentences":["We consider a natural generalization of chordal graphs, in which every minimal separator induces a subgraph with independence number at most $2$. Such graphs can be equivalently defined as graphs that do not contain the complete bipartite graph $K_{2,3}$ as an induced minor, that is, graphs from which $K_{2,3}$ cannot be obtained by a sequence of edge contractions and vertex deletions.   ","We develop a polynomial-time algorithm for recognizing these graphs.","Our algorithm relies on a characterization of $K_{2,3}$-induced minor-free graphs in terms of excluding particular induced subgraphs, called Truemper configurations."],"url":"http://arxiv.org/abs/2402.08332v1","category":"math.CO"}
{"created":"2024-02-13 09:47:59","title":"A novel solution to the hyperon-puzzle in neutron stars","abstract":"Neutron stars offer a great opportunity to study highly compressed hadronic matter experimentally and theoretically. However, the so-called hyperon-puzzle arises at neutron star densities. The hyperon coexistence with other particles in compressed matter softens the equation of state and many widely-accepted models fail to reproduce precise observations of large neutron star masses. Here, we propose a novel mechanism to retain the stiffness of the high density state with hyperons by considering the explicit momentum dependence of their in-medium potentials. Our approach modifies conventional strangeness threshold conditions and generates new threshold effects on hyperons in high-density matter. We demonstrate these effects within the Non-Linear Derivative model, which incorporates baryon momentum-dependent fields based on empirical and microscopic studies. It turns out that even soft momentum-dependent strangeness fields do prohibit their populations in neutron star matter. The generic momentum dependence of strangeness potentials, as modeled by the non-linear derivative approach, is crucial for resolving the long-standing hyperon-puzzle in neutron stars.","sentences":["Neutron stars offer a great opportunity to study highly compressed hadronic matter experimentally and theoretically.","However, the so-called hyperon-puzzle arises at neutron star densities.","The hyperon coexistence with other particles in compressed matter softens the equation of state and many widely-accepted models fail to reproduce precise observations of large neutron star masses.","Here, we propose a novel mechanism to retain the stiffness of the high density state with hyperons by considering the explicit momentum dependence of their in-medium potentials.","Our approach modifies conventional strangeness threshold conditions and generates new threshold effects on hyperons in high-density matter.","We demonstrate these effects within the Non-Linear Derivative model, which incorporates baryon momentum-dependent fields based on empirical and microscopic studies.","It turns out that even soft momentum-dependent strangeness fields do prohibit their populations in neutron star matter.","The generic momentum dependence of strangeness potentials, as modeled by the non-linear derivative approach, is crucial for resolving the long-standing hyperon-puzzle in neutron stars."],"url":"http://arxiv.org/abs/2402.08329v1","category":"nucl-th"}
{"created":"2024-02-13 09:47:07","title":"PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers","abstract":"Large Multimodal Models (LMMs) excel in natural language and visual understanding but are challenged by exacting tasks such as Knowledge-based Visual Question Answering (KB-VQA) which involve the retrieval of relevant information from document collections to use in shaping answers to questions. We present an extensive training and evaluation framework, M2KR, for KB-VQA. M2KR contains a collection of vision and language tasks which we have incorporated into a single suite of benchmark tasks for training and evaluating general-purpose multi-modal retrievers. We use M2KR to develop PreFLMR, a pre-trained version of the recently developed Fine-grained Late-interaction Multi-modal Retriever (FLMR) approach to KB-VQA, and we report new state-of-the-art results across a range of tasks. We also present investigations into the scaling behaviors of PreFLMR intended to be useful in future developments in general-purpose multi-modal retrievers.","sentences":["Large Multimodal Models (LMMs) excel in natural language and visual understanding but are challenged by exacting tasks such as Knowledge-based Visual Question Answering (KB-VQA) which involve the retrieval of relevant information from document collections to use in shaping answers to questions.","We present an extensive training and evaluation framework, M2KR, for KB-VQA.","M2KR contains a collection of vision and language tasks which we have incorporated into a single suite of benchmark tasks for training and evaluating general-purpose multi-modal retrievers.","We use M2KR to develop PreFLMR, a pre-trained version of the recently developed Fine-grained Late-interaction Multi-modal Retriever (FLMR) approach to KB-VQA, and we report new state-of-the-art results across a range of tasks.","We also present investigations into the scaling behaviors of PreFLMR intended to be useful in future developments in general-purpose multi-modal retrievers."],"url":"http://arxiv.org/abs/2402.08327v1","category":"cs.CL"}
{"created":"2024-02-13 09:40:19","title":"Uncertainty Quantification via Stable Distribution Propagation","abstract":"We propose a new approach for propagating stable probability distributions through neural networks. Our method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity. This allows propagating Gaussian and Cauchy input uncertainties through neural networks to quantify their output uncertainties. To demonstrate the utility of propagating distributions, we apply the proposed method to predicting calibrated confidence intervals and selective prediction on out-of-distribution data. The results demonstrate a broad applicability of propagating distributions and show the advantages of our method over other approaches such as moment matching.","sentences":["We propose a new approach for propagating stable probability distributions through neural networks.","Our method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity.","This allows propagating Gaussian and Cauchy input uncertainties through neural networks to quantify their output uncertainties.","To demonstrate the utility of propagating distributions, we apply the proposed method to predicting calibrated confidence intervals and selective prediction on out-of-distribution data.","The results demonstrate a broad applicability of propagating distributions and show the advantages of our method over other approaches such as moment matching."],"url":"http://arxiv.org/abs/2402.08324v1","category":"cs.LG"}
{"created":"2024-02-13 09:38:17","title":"Mapping the Ethics of Generative AI: A Comprehensive Scoping Review","abstract":"The advent of generative artificial intelligence and the widespread adoption of it in society engendered intensive debates about its ethical implications and risks. These risks often differ from those associated with traditional discriminative machine learning. To synthesize the recent discourse and map its normative concepts, we conducted a scoping review on the ethics of generative artificial intelligence, including especially large language models and text-to-image models. Our analysis provides a taxonomy of 378 normative issues in 19 topic areas and ranks them according to their prevalence in the literature. The study offers a comprehensive overview for scholars, practitioners, or policymakers, condensing the ethical debates surrounding fairness, safety, harmful content, hallucinations, privacy, interaction risks, security, alignment, societal impacts, and others. We discuss the results, evaluate imbalances in the literature, and explore unsubstantiated risk scenarios.","sentences":["The advent of generative artificial intelligence and the widespread adoption of it in society engendered intensive debates about its ethical implications and risks.","These risks often differ from those associated with traditional discriminative machine learning.","To synthesize the recent discourse and map its normative concepts, we conducted a scoping review on the ethics of generative artificial intelligence, including especially large language models and text-to-image models.","Our analysis provides a taxonomy of 378 normative issues in 19 topic areas and ranks them according to their prevalence in the literature.","The study offers a comprehensive overview for scholars, practitioners, or policymakers, condensing the ethical debates surrounding fairness, safety, harmful content, hallucinations, privacy, interaction risks, security, alignment, societal impacts, and others.","We discuss the results, evaluate imbalances in the literature, and explore unsubstantiated risk scenarios."],"url":"http://arxiv.org/abs/2402.08323v1","category":"cs.CY"}
{"created":"2024-02-13 09:34:23","title":"zk-IoT: Securing the Internet of Things with Zero-Knowledge Proofs on Blockchain Platforms","abstract":"This paper introduces the zk-IoT framework, a novel approach to enhancing the security of Internet of Things (IoT) ecosystems through the use of Zero-Knowledge Proofs (ZKPs) on blockchain platforms. Our framework ensures the integrity of firmware execution and data processing in potentially compromised IoT devices. By leveraging the concept of ZKP, we establish a trust layer that facilitates secure, autonomous communication between IoT devices in environments where devices may not inherently trust each other. The framework comprises zk-Devices, which utilize functional commitment to generate proofs for executed programs, and service contracts for encoding interaction logic among devices. It also provides for IoT device automation using proof-carrying data (PCD) and a blockchain layer for transparent and verifiable data processing. We conduct experiments, the results of which show that proof generation, publication, and verification timings meet the practical requirements of IoT device communication, demonstrating the feasibility and efficiency of our solution. The zk-IoT framework represents a significant advancement in the realm of IoT security, paving the way for reliable and scalable IoT networks across various applications, such as smart city infrastructures, healthcare systems, and industrial automation.","sentences":["This paper introduces the zk-IoT framework, a novel approach to enhancing the security of Internet of Things (IoT) ecosystems through the use of Zero-Knowledge Proofs (ZKPs) on blockchain platforms.","Our framework ensures the integrity of firmware execution and data processing in potentially compromised IoT devices.","By leveraging the concept of ZKP, we establish a trust layer that facilitates secure, autonomous communication between IoT devices in environments where devices may not inherently trust each other.","The framework comprises zk-Devices, which utilize functional commitment to generate proofs for executed programs, and service contracts for encoding interaction logic among devices.","It also provides for IoT device automation using proof-carrying data (PCD) and a blockchain layer for transparent and verifiable data processing.","We conduct experiments, the results of which show that proof generation, publication, and verification timings meet the practical requirements of IoT device communication, demonstrating the feasibility and efficiency of our solution.","The zk-IoT framework represents a significant advancement in the realm of IoT security, paving the way for reliable and scalable IoT networks across various applications, such as smart city infrastructures, healthcare systems, and industrial automation."],"url":"http://arxiv.org/abs/2402.08322v1","category":"cs.CR"}
{"created":"2024-02-13 09:34:22","title":"Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret with Adversarial Robustness in Partial Monitoring","abstract":"Partial monitoring is a generic framework of online decision-making problems with limited observations. To make decisions from such limited observations, it is necessary to find an appropriate distribution for exploration. Recently, a powerful approach for this purpose, exploration by optimization (ExO), was proposed, which achieves the optimal bounds in adversarial environments with follow-the-regularized-leader for a wide range of online decision-making problems. However, a naive application of ExO in stochastic environments significantly degrades regret bounds. To resolve this problem in locally observable games, we first establish a novel framework and analysis for ExO with a hybrid regularizer. This development allows us to significantly improve the existing regret bounds of best-of-both-worlds (BOBW) algorithms, which achieves nearly optimal bounds both in stochastic and adversarial environments. In particular, we derive a stochastic regret bound of $O(\\sum_{a \\neq a^*} k^2 m^2 \\log T / \\Delta_a)$, where $k$, $m$, and $T$ are the numbers of actions, observations and rounds, $a^*$ is an optimal action, and $\\Delta_a$ is the suboptimality gap for action $a$. This bound is roughly $\\Theta(k^2 \\log T)$ times smaller than existing BOBW bounds. In addition, for globally observable games, we provide a new BOBW algorithm with the first $O(\\log T)$ stochastic bound.","sentences":["Partial monitoring is a generic framework of online decision-making problems with limited observations.","To make decisions from such limited observations, it is necessary to find an appropriate distribution for exploration.","Recently, a powerful approach for this purpose, exploration by optimization (ExO), was proposed, which achieves the optimal bounds in adversarial environments with follow-the-regularized-leader for a wide range of online decision-making problems.","However, a naive application of ExO in stochastic environments significantly degrades regret bounds.","To resolve this problem in locally observable games, we first establish a novel framework and analysis for ExO with a hybrid regularizer.","This development allows us to significantly improve the existing regret bounds of best-of-both-worlds (BOBW) algorithms, which achieves nearly optimal bounds both in stochastic and adversarial environments.","In particular, we derive a stochastic regret bound of $O(\\sum_{a \\neq a^*} k^2 m^2 \\log T / \\Delta_a)$, where $k$, $m$, and $T$ are the numbers of actions, observations and rounds, $a^*$ is an optimal action, and $\\Delta_a$ is the suboptimality gap for action $a$.","This bound is roughly $\\Theta(k^2 \\log T)$ times smaller than existing BOBW bounds.","In addition, for globally observable games, we provide a new BOBW algorithm with the first $O(\\log T)$ stochastic bound."],"url":"http://arxiv.org/abs/2402.08321v1","category":"cs.LG"}
{"created":"2024-02-13 09:16:40","title":"Channel-Combination Algorithms for Robust Distant Voice Activity and Overlapped Speech Detection","abstract":"Voice Activity Detection (VAD) and Overlapped Speech Detection (OSD) are key pre-processing tasks for speaker diarization. In the meeting context, it is often easier to capture speech with a distant device. This consideration however leads to severe performance degradation. We study a unified supervised learning framework to solve distant multi-microphone joint VAD and OSD (VAD+OSD). This paper investigates various multi-channel VAD+OSD front-ends that weight and combine incoming channels. We propose three algorithms based on the Self-Attention Channel Combinator (SACC), previously proposed in the literature. Experiments conducted on the AMI meeting corpus exhibit that channel combination approaches bring significant VAD+OSD improvements in the distant speech scenario. Specifically, we explore the use of learned complex combination weights and demonstrate the benefits of such an approach in terms of explainability. Channel combination-based VAD+OSD systems are evaluated on the final back-end task, i.e. speaker diarization, and show significant improvements. Finally, since multi-channel systems are trained given a fixed array configuration, they may fail in generalizing to other array set-ups, e.g. mismatched number of microphones. A channel-number invariant loss is proposed to learn a unique feature representation regardless of the number of available microphones. The evaluation conducted on mismatched array configurations highlights the robustness of this training strategy.","sentences":["Voice Activity Detection (VAD) and Overlapped Speech Detection (OSD) are key pre-processing tasks for speaker diarization.","In the meeting context, it is often easier to capture speech with a distant device.","This consideration however leads to severe performance degradation.","We study a unified supervised learning framework to solve distant multi-microphone joint VAD and OSD (VAD+OSD).","This paper investigates various multi-channel VAD+OSD front-ends that weight and combine incoming channels.","We propose three algorithms based on the Self-Attention Channel Combinator (SACC), previously proposed in the literature.","Experiments conducted on the AMI meeting corpus exhibit that channel combination approaches bring significant VAD+OSD improvements in the distant speech scenario.","Specifically, we explore the use of learned complex combination weights and demonstrate the benefits of such an approach in terms of explainability.","Channel combination-based VAD+OSD systems are evaluated on the final back-end task, i.e. speaker diarization, and show significant improvements.","Finally, since multi-channel systems are trained given a fixed array configuration, they may fail in generalizing to other array set-ups, e.g. mismatched number of microphones.","A channel-number invariant loss is proposed to learn a unique feature representation regardless of the number of available microphones.","The evaluation conducted on mismatched array configurations highlights the robustness of this training strategy."],"url":"http://arxiv.org/abs/2402.08312v1","category":"eess.AS"}
{"created":"2024-02-13 09:13:30","title":"One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a synthetically trained Generative Model","abstract":"Estimating the 3D shape of an object using a single image is a difficult problem. Modern approaches achieve good results for general objects, based on real photographs, but worse results on less expressive representations such as historic sketches. Our automated approach generates a variety of detailed 3D representation from a single sketch, depicting a medieval statue, and can be guided by multi-modal inputs, such as text prompts. It relies solely on synthetic data for training, making it adoptable even in cases of only small numbers of training examples. Our solution allows domain experts such as a curators to interactively reconstruct potential appearances of lost artifacts.","sentences":["Estimating the 3D shape of an object using a single image is a difficult problem.","Modern approaches achieve good results for general objects, based on real photographs, but worse results on less expressive representations such as historic sketches.","Our automated approach generates a variety of detailed 3D representation from a single sketch, depicting a medieval statue, and can be guided by multi-modal inputs, such as text prompts.","It relies solely on synthetic data for training, making it adoptable even in cases of only small numbers of training examples.","Our solution allows domain experts such as a curators to interactively reconstruct potential appearances of lost artifacts."],"url":"http://arxiv.org/abs/2402.08310v1","category":"cs.CV"}
{"created":"2024-02-13 09:12:55","title":"Prompted Contextual Vectors for Spear-Phishing Detection","abstract":"Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include an innovative document vectorization method utilizing LLM reasoning, a publicly available dataset of high-quality spear-phishing emails, and the demonstrated effectiveness of our method in detecting such emails. This methodology can be utilized for various document classification tasks, particularly in adversarial problem domains.","sentences":["Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance.","To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors.","By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model.","We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation.","Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails.","Key contributions include an innovative document vectorization method utilizing LLM reasoning, a publicly available dataset of high-quality spear-phishing emails, and the demonstrated effectiveness of our method in detecting such emails.","This methodology can be utilized for various document classification tasks, particularly in adversarial problem domains."],"url":"http://arxiv.org/abs/2402.08309v1","category":"cs.LG"}
{"created":"2024-02-13 09:09:26","title":"Axial perturbations in Kantowski-Sachs spacetimes and hybrid quantum cosmology","abstract":"Recently, there has been a growing interest in investigating homogeneous but anisotropic spacetimes owing to their relation with nonrotating, uncharged black hole interiors. We present a description of axial perturbations for a massless scalar field minimally coupled to this geometry. We truncate the action at the quadratic perturbative order and tailor our analysis to compact spatial sections. Perturbations are described in terms of perturbative gauge invariants, linear perturbative constraints, and their canonically conjugate variables. The entire set, encompassing perturbations and homogeneous degrees of freedom, is consolidated into a canonical one. We employ a hybrid approach to quantize this system, integrating a quantum representation of the homogeneous sector using Loop Quantum Cosmology techniques with a conventional field quantization of the perturbations.","sentences":["Recently, there has been a growing interest in investigating homogeneous but anisotropic spacetimes owing to their relation with nonrotating, uncharged black hole interiors.","We present a description of axial perturbations for a massless scalar field minimally coupled to this geometry.","We truncate the action at the quadratic perturbative order and tailor our analysis to compact spatial sections.","Perturbations are described in terms of perturbative gauge invariants, linear perturbative constraints, and their canonically conjugate variables.","The entire set, encompassing perturbations and homogeneous degrees of freedom, is consolidated into a canonical one.","We employ a hybrid approach to quantize this system, integrating a quantum representation of the homogeneous sector using Loop Quantum Cosmology techniques with a conventional field quantization of the perturbations."],"url":"http://arxiv.org/abs/2402.08307v1","category":"gr-qc"}
{"created":"2024-02-13 09:07:05","title":"The Saito criterion and some avatars","abstract":"Let k be a field of characteristic zero and R = k[x 1 ,. .. , x n ] be a graded commutative ring. To any set of 1 $\\le$ s $\\le$ n-1 algebraically independent polynomials f 1 ,. .. , f s $\\in$ R one associates a rank-(n+1-s) R-module Der(f 1 ,. .. , f s) of logarithmic derivations; these logarithmic derivations $\\delta$ $\\in$ Der(f 1 ,. .. , f s) verify $\\delta$(f 1) f 1 = $\\bullet$ $\\bullet$ $\\bullet$ = $\\delta$(f s) f s $\\in$ R. The aim of this note is to extend the Saito criterion, which characterizes freeness of the module Der(f) of tangent vector fields along a reduced divisor V (f), to the module Der(f 1 ,. .. , f s). More precisely, let $\\delta$ i be (n + 1-s) independent derivations in Der(f 1 ,. .. , f s) and U $\\subset$ Der(f 1 ,. .. , f s) the sub-module generated by the $\\delta$ i. We prove that U = Der(f 1 ,. .. , f s) if and only if codim k n V (n+1-s U) > 1 when s > 1 or V (n U) = V (f 1) when s = 1.","sentences":["Let k be a field of characteristic zero and R = k[x 1 ,.",".. , x n ] be a graded commutative ring.","To any set of 1 $\\le$ s $\\le$ n-1 algebraically independent polynomials f 1 ,. .. , f s $\\in$ R one associates a rank-(n+1-s) R-module Der(f 1 ,. .. , f s) of logarithmic derivations; these logarithmic derivations $\\delta$ $\\in$ Der(f 1 ,.",".. , f s) verify $\\delta$(f 1) f 1 = $\\bullet$ $\\bullet$ $\\bullet$ = $\\delta$(f s)","f s $\\in$ R. The aim of this note is to extend the Saito criterion, which characterizes freeness of the module Der(f) of tangent vector fields along a reduced divisor V (f), to the module Der(f 1 ,.",".. , f s).","More precisely, let $\\delta$ i be (n","+ 1-s) independent derivations in Der(f 1 ,.",".. , f s) and U $\\subset$ Der(f 1 ,.",".. , f s) the sub-module generated by the $\\delta$ i. We prove that U = Der(f 1 ,.",".. , f s) if and only if codim k n V (n+1-s U) > 1 when s > 1 or V (n U) = V (f 1)","when s = 1."],"url":"http://arxiv.org/abs/2402.08305v1","category":"math.AG"}
{"created":"2024-02-13 09:06:14","title":"ChatCell: Facilitating Single-Cell Analysis with Natural Language","abstract":"As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to deepen single-cell insights, paving the way for more accessible and intuitive exploration in this pivotal field. Our project homepage is available at https://zjunlp.github.io/project/ChatCell.","sentences":["As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent.","The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology.","However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges.","High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration.","To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language.","Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks.","Extensive experiments further demonstrate ChatCell's robust performance and potential to deepen single-cell insights, paving the way for more accessible and intuitive exploration in this pivotal field.","Our project homepage is available at https://zjunlp.github.io/project/ChatCell."],"url":"http://arxiv.org/abs/2402.08303v1","category":"cs.CL"}
{"created":"2024-02-13 09:03:48","title":"H\u00f6lder invariance of the Henry-Parusinski invariant","abstract":"In this article, we show the H\\\"older invariance of the Henry-Parusinski invariant. For a single germ $ f$, the Henry-Parusinski invariant of $ f $ is given in terms of the leading coefficients of the asymptotic expansion of $ f $ along the branches of the generic polar curve of $f$. As a consequence, we obtain that the classification problem of polynomial function-germs, with uniformly bounded degree, under H\\\"older equivalence, admits continuous moduli.","sentences":["In this article, we show the H\\\"older invariance of the Henry-Parusinski invariant.","For a single germ $ f$, the Henry-Parusinski invariant of $ f $ is given in terms of the leading coefficients of the asymptotic expansion of $ f $ along the branches of the generic polar curve of $f$. As a consequence, we obtain that the classification problem of polynomial function-germs, with uniformly bounded degree, under H\\\"older equivalence, admits continuous moduli."],"url":"http://arxiv.org/abs/2402.08301v1","category":"math.AG"}
{"created":"2024-02-13 09:03:03","title":"An Order-Complexity Aesthetic Assessment Model for Aesthetic-aware Music Recommendation","abstract":"Computational aesthetic evaluation has made remarkable contribution to visual art works, but its application to music is still rare. Currently, subjective evaluation is still the most effective form of evaluating artistic works. However, subjective evaluation of artistic works will consume a lot of human and material resources. The popular AI generated content (AIGC) tasks nowadays have flooded all industries, and music is no exception. While compared to music produced by humans, AI generated music still sounds mechanical, monotonous, and lacks aesthetic appeal. Due to the lack of music datasets with rating annotations, we have to choose traditional aesthetic equations to objectively measure the beauty of music. In order to improve the quality of AI music generation and further guide computer music production, synthesis, recommendation and other tasks, we use Birkhoff's aesthetic measure to design a aesthetic model, objectively measuring the aesthetic beauty of music, and form a recommendation list according to the aesthetic feeling of music. Experiments show that our objective aesthetic model and recommendation method are effective.","sentences":["Computational aesthetic evaluation has made remarkable contribution to visual art works, but its application to music is still rare.","Currently, subjective evaluation is still the most effective form of evaluating artistic works.","However, subjective evaluation of artistic works will consume a lot of human and material resources.","The popular AI generated content (AIGC) tasks nowadays have flooded all industries, and music is no exception.","While compared to music produced by humans, AI generated music still sounds mechanical, monotonous, and lacks aesthetic appeal.","Due to the lack of music datasets with rating annotations, we have to choose traditional aesthetic equations to objectively measure the beauty of music.","In order to improve the quality of AI music generation and further guide computer music production, synthesis, recommendation and other tasks, we use Birkhoff's aesthetic measure to design a aesthetic model, objectively measuring the aesthetic beauty of music, and form a recommendation list according to the aesthetic feeling of music.","Experiments show that our objective aesthetic model and recommendation method are effective."],"url":"http://arxiv.org/abs/2402.08300v1","category":"cs.CV"}
{"created":"2024-02-13 08:53:57","title":"Time to Stop and Think: What kind of research do we want to do?","abstract":"Experimentation is an intrinsic part of research in artificial intelligence since it allows for collecting quantitative observations, validating hypotheses, and providing evidence for their reformulation. For that reason, experimentation must be coherent with the purposes of the research, properly addressing the relevant questions in each case. Unfortunately, the literature is full of works whose experimentation is neither rigorous nor convincing, oftentimes designed to support prior beliefs rather than answering the relevant research questions.   In this paper, we focus on the field of metaheuristic optimization, since it is our main field of work, and it is where we have observed the misconduct that has motivated this letter. Even if we limit the focus of this manuscript to the experimental part of the research, our main goal is to sew the seed of sincere critical assessment of our work, sparking a reflection process both at the individual and the community level. Such a reflection process is too complex and extensive to be tackled as a whole. Therefore, to bring our feet to the ground, we will include in this document our reflections about the role of experimentation in our work, discussing topics such as the use of benchmark instances vs instance generators, or the statistical assessment of empirical results. That is, all the statements included in this document are personal views and opinions, which can be shared by others or not. Certainly, having different points of view is the basis to establish a good discussion process.","sentences":["Experimentation is an intrinsic part of research in artificial intelligence since it allows for collecting quantitative observations, validating hypotheses, and providing evidence for their reformulation.","For that reason, experimentation must be coherent with the purposes of the research, properly addressing the relevant questions in each case.","Unfortunately, the literature is full of works whose experimentation is neither rigorous nor convincing, oftentimes designed to support prior beliefs rather than answering the relevant research questions.   ","In this paper, we focus on the field of metaheuristic optimization, since it is our main field of work, and it is where we have observed the misconduct that has motivated this letter.","Even if we limit the focus of this manuscript to the experimental part of the research, our main goal is to sew the seed of sincere critical assessment of our work, sparking a reflection process both at the individual and the community level.","Such a reflection process is too complex and extensive to be tackled as a whole.","Therefore, to bring our feet to the ground, we will include in this document our reflections about the role of experimentation in our work, discussing topics such as the use of benchmark instances vs instance generators, or the statistical assessment of empirical results.","That is, all the statements included in this document are personal views and opinions, which can be shared by others or not.","Certainly, having different points of view is the basis to establish a good discussion process."],"url":"http://arxiv.org/abs/2402.08298v1","category":"cs.AI"}
{"created":"2024-02-13 08:50:17","title":"A Multi-Function Radiation-Hardened HV and LV Linear Regulator for SiPM-based HEP Detectors","abstract":"The use of silicon photomultipliers (SiPMs) to detect light signals in highly radioactive environments presents several challenges, particularly due to their sensitivity on radiation, temperature, and overvoltage, requiring a proper management of their bias supply. This article presents the design and performance of ALDO2, an application-specific integrated circuit and power management solution tailored for SiPM-based high-energy physics detectors. The chip's functions include adjustable and point-of-load linear regulation of the SiPM bias voltage (10-70 V, 50 mA), monitoring of SiPM current, shutdown, over-current and over-temperature protection. The same functions are also available for the low-voltage regulator (1.6-3.3 V, 800 mA), used to generate the power supply of SiPM readout chips that often demand stable and well-filtered input voltages while consuming currents of up to several hundred milliamperes. The chip is intended to operate in radioactive environments typical of particle physics experiments, where it must withstand significant levels of radiation (total ionizing dose and 1-MeV-equivalent neutron fluence in the range of Mrad and $\\mathbf{10^{14}}\\ \\mathrm{\\mathbf{n_{eq}/cm^2}}$, respectively). The article provides a comprehensive description of the chip design, as well as experimental measurements, offering insights into the chip's performance under various conditions. Finally, radiation hardening, radiation qualification and reliability are discussed.","sentences":["The use of silicon photomultipliers (SiPMs) to detect light signals in highly radioactive environments presents several challenges, particularly due to their sensitivity on radiation, temperature, and overvoltage, requiring a proper management of their bias supply.","This article presents the design and performance of ALDO2, an application-specific integrated circuit and power management solution tailored for SiPM-based high-energy physics detectors.","The chip's functions include adjustable and point-of-load linear regulation of the SiPM bias voltage (10-70 V, 50 mA), monitoring of SiPM current, shutdown, over-current and over-temperature protection.","The same functions are also available for the low-voltage regulator (1.6-3.3 V, 800 mA), used to generate the power supply of SiPM readout chips that often demand stable and well-filtered input voltages while consuming currents of up to several hundred milliamperes.","The chip is intended to operate in radioactive environments typical of particle physics experiments, where it must withstand significant levels of radiation (total ionizing dose and 1-MeV-equivalent neutron fluence in the range of Mrad and $\\mathbf{10^{14}}\\ \\mathrm{\\mathbf{n_{eq}/cm^2}}$, respectively).","The article provides a comprehensive description of the chip design, as well as experimental measurements, offering insights into the chip's performance under various conditions.","Finally, radiation hardening, radiation qualification and reliability are discussed."],"url":"http://arxiv.org/abs/2402.08297v1","category":"physics.ins-det"}
{"created":"2024-02-13 08:50:14","title":"Multi-Level GNN Preconditioner for Solving Large Scale Problems","abstract":"Large-scale numerical simulations often come at the expense of daunting computations. High-Performance Computing has enhanced the process, but adapting legacy codes to leverage parallel GPU computations remains challenging. Meanwhile, Machine Learning models can harness GPU computations effectively but often struggle with generalization and accuracy. Graph Neural Networks (GNNs), in particular, are great for learning from unstructured data like meshes but are often limited to small-scale problems. Moreover, the capabilities of the trained model usually restrict the accuracy of the data-driven solution. To benefit from both worlds, this paper introduces a novel preconditioner integrating a GNN model within a multi-level Domain Decomposition framework. The proposed GNN-based preconditioner is used to enhance the efficiency of a Krylov method, resulting in a hybrid solver that can converge with any desired level of accuracy. The efficiency of the Krylov method greatly benefits from the GNN preconditioner, which is adaptable to meshes of any size and shape, is executed on GPUs, and features a multi-level approach to enforce the scalability of the entire process. Several experiments are conducted to validate the numerical behavior of the hybrid solver, and an in-depth analysis of its performance is proposed to assess its competitiveness against a C++ legacy solver.","sentences":["Large-scale numerical simulations often come at the expense of daunting computations.","High-Performance Computing has enhanced the process, but adapting legacy codes to leverage parallel GPU computations remains challenging.","Meanwhile, Machine Learning models can harness GPU computations effectively but often struggle with generalization and accuracy.","Graph Neural Networks (GNNs), in particular, are great for learning from unstructured data like meshes but are often limited to small-scale problems.","Moreover, the capabilities of the trained model usually restrict the accuracy of the data-driven solution.","To benefit from both worlds, this paper introduces a novel preconditioner integrating a GNN model within a multi-level Domain Decomposition framework.","The proposed GNN-based preconditioner is used to enhance the efficiency of a Krylov method, resulting in a hybrid solver that can converge with any desired level of accuracy.","The efficiency of the Krylov method greatly benefits from the GNN preconditioner, which is adaptable to meshes of any size and shape, is executed on GPUs, and features a multi-level approach to enforce the scalability of the entire process.","Several experiments are conducted to validate the numerical behavior of the hybrid solver, and an in-depth analysis of its performance is proposed to assess its competitiveness against a C++ legacy solver."],"url":"http://arxiv.org/abs/2402.08296v1","category":"cs.LG"}
{"created":"2024-02-13 08:48:12","title":"Tuning residual stress, directional memory and aging in soft glassy materials","abstract":"When glassy materials are rapidly quenched from the liquid to the solid state upon flow cessation or cooling, they solidify in an out-of-equilibrium configuration, retaining the memory of the processing conditions for very long times. This is the origin of various phenomena, such as residual stresses and directional memory, which greatly affect their properties. At the same time, annealing the mechanical history encoded in disordered materials constitute a great challenge. Here, we address this problem for the case of colloidal glasses made of soft particles densely packed at a high volume fraction, using experiments and particle dynamic simulations. We demonstrate that periodically training soft particle glasses with a sequence of stress-controlled oscillations successfully anneals residual stress and directional memory when the stress amplitude corresponds to the yield point. At the microscopic level, annealing provides a fine tuning of the local distribution of the stress carried by the particles. Through the simulations, we show that the first moments of this distribution have precise physical meaning: the mean value of the distribution corresponds to the macroscopic stress; the skewness carries information about directional memory; and the standard deviation is related to mechanical aging. The same methodology is successfully applied to silica gels with thixotropic properties, suggesting that it is general and may be extended to other classes of disordered materials.","sentences":["When glassy materials are rapidly quenched from the liquid to the solid state upon flow cessation or cooling, they solidify in an out-of-equilibrium configuration, retaining the memory of the processing conditions for very long times.","This is the origin of various phenomena, such as residual stresses and directional memory, which greatly affect their properties.","At the same time, annealing the mechanical history encoded in disordered materials constitute a great challenge.","Here, we address this problem for the case of colloidal glasses made of soft particles densely packed at a high volume fraction, using experiments and particle dynamic simulations.","We demonstrate that periodically training soft particle glasses with a sequence of stress-controlled oscillations successfully anneals residual stress and directional memory when the stress amplitude corresponds to the yield point.","At the microscopic level, annealing provides a fine tuning of the local distribution of the stress carried by the particles.","Through the simulations, we show that the first moments of this distribution have precise physical meaning: the mean value of the distribution corresponds to the macroscopic stress; the skewness carries information about directional memory; and the standard deviation is related to mechanical aging.","The same methodology is successfully applied to silica gels with thixotropic properties, suggesting that it is general and may be extended to other classes of disordered materials."],"url":"http://arxiv.org/abs/2402.08293v1","category":"cond-mat.soft"}
{"created":"2024-02-13 08:41:32","title":"The Effect of Data Poisoning on Counterfactual Explanations","abstract":"Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \\& toolboxes are vulnerable to such data poisoning.","sentences":["Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output.","However, recent work highlighted their vulnerability to different types of manipulations.","This work studies the vulnerability of counterfactual explanations to data poisoning.","We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances.","We demonstrate that state-of-the-art counterfactual generation methods \\& toolboxes are vulnerable to such data poisoning."],"url":"http://arxiv.org/abs/2402.08290v1","category":"cs.LG"}
{"created":"2024-02-13 08:24:32","title":"A Logical Approach to Criminal Case Investigation","abstract":"XAI (eXplanable AI) techniques that have the property of explaining the reasons for their conclusions, i.e. explainability or interpretability, are attracting attention. XAI is expected to be used in the development of forensic science and the justice system. In today's forensic and criminal investigation environment, experts face many challenges due to large amounts of data, small pieces of evidence in a chaotic and complex environment, traditional laboratory structures and sometimes inadequate knowledge. All these can lead to failed investigations and miscarriages of justice. In this paper, we describe the application of one logical approach to crime scene investigation. The subject of the application is ``The Adventure of the Speckled Band'' from the Sherlock Holmes short stories. The applied data is the knowledge graph created for the Knowledge Graph Reasoning Challenge. We tried to find the murderer by inferring each person with the motive, opportunity, and method. We created an ontology of motives and methods of murder from dictionaries and dictionaries, added it to the knowledge graph of ``The Adventure of the Speckled Band'', and applied scripts to determine motives, opportunities, and methods.","sentences":["XAI (eXplanable AI) techniques that have the property of explaining the reasons for their conclusions, i.e. explainability or interpretability, are attracting attention.","XAI is expected to be used in the development of forensic science and the justice system.","In today's forensic and criminal investigation environment, experts face many challenges due to large amounts of data, small pieces of evidence in a chaotic and complex environment, traditional laboratory structures and sometimes inadequate knowledge.","All these can lead to failed investigations and miscarriages of justice.","In this paper, we describe the application of one logical approach to crime scene investigation.","The subject of the application is ``The Adventure of the Speckled Band'' from the Sherlock Holmes short stories.","The applied data is the knowledge graph created for the Knowledge Graph Reasoning Challenge.","We tried to find the murderer by inferring each person with the motive, opportunity, and method.","We created an ontology of motives and methods of murder from dictionaries and dictionaries, added it to the knowledge graph of ``The Adventure of the Speckled Band'', and applied scripts to determine motives, opportunities, and methods."],"url":"http://arxiv.org/abs/2402.08284v1","category":"cs.AI"}
{"created":"2024-02-13 08:22:42","title":"Classification Using Global and Local Mahalanobis Distances","abstract":"We propose a novel semi-parametric classifier based on Mahalanobis distances of an observation from the competing classes. Our tool is a generalized additive model with the logistic link function that uses these distances as features to estimate the posterior probabilities of the different classes. While popular parametric classifiers like linear and quadratic discriminant analyses are mainly motivated by the normality of the underlying distributions, the proposed classifier is more flexible and free from such parametric assumptions. Since the densities of elliptic distributions are functions of Mahalanobis distances, this classifier works well when the competing classes are (nearly) elliptic. In such cases, it often outperforms popular nonparametric classifiers, especially when the sample size is small compared to the dimension of the data. To cope with non-elliptic and possibly multimodal distributions, we propose a local version of the Mahalanobis distance. Subsequently, we propose another classifier based on a generalized additive model that uses the local Mahalanobis distances as features. This nonparametric classifier usually performs like the Mahalanobis distance based semiparametric classifier when the underlying distributions are elliptic, but outperforms it for several non-elliptic and multimodal distributions. We also investigate the behaviour of these two classifiers in high dimension, low sample size situations. A thorough numerical study involving several simulated and real datasets demonstrate the usefulness of the proposed classifiers in comparison to many state-of-the-art methods.","sentences":["We propose a novel semi-parametric classifier based on Mahalanobis distances of an observation from the competing classes.","Our tool is a generalized additive model with the logistic link function that uses these distances as features to estimate the posterior probabilities of the different classes.","While popular parametric classifiers like linear and quadratic discriminant analyses are mainly motivated by the normality of the underlying distributions, the proposed classifier is more flexible and free from such parametric assumptions.","Since the densities of elliptic distributions are functions of Mahalanobis distances, this classifier works well when the competing classes are (nearly) elliptic.","In such cases, it often outperforms popular nonparametric classifiers, especially when the sample size is small compared to the dimension of the data.","To cope with non-elliptic and possibly multimodal distributions, we propose a local version of the Mahalanobis distance.","Subsequently, we propose another classifier based on a generalized additive model that uses the local Mahalanobis distances as features.","This nonparametric classifier usually performs like the Mahalanobis distance based semiparametric classifier when the underlying distributions are elliptic, but outperforms it for several non-elliptic and multimodal distributions.","We also investigate the behaviour of these two classifiers in high dimension, low sample size situations.","A thorough numerical study involving several simulated and real datasets demonstrate the usefulness of the proposed classifiers in comparison to many state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.08283v1","category":"stat.ME"}
{"created":"2024-02-13 08:14:10","title":"Pix2Code: Learning to Compose Neural Visual Concepts as Programs","abstract":"The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning. Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model's learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and concept configurations. Particularly, in stark contrast to neural approaches, we show that Pix2Code's representations remain human interpretable and can be easily revised for improved performance.","sentences":["The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning.","Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model's learnt concepts and potentially revise false behaviours.","To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations.","This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs.","We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and concept configurations.","Particularly, in stark contrast to neural approaches, we show that Pix2Code's representations remain human interpretable and can be easily revised for improved performance."],"url":"http://arxiv.org/abs/2402.08280v1","category":"cs.AI"}
{"created":"2024-02-13 08:12:48","title":"Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering","abstract":"Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.","sentences":["Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors.","One avenue in reaching this goal is basing the answers on reliable sources.","However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability).","In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability.","Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale.","We further introduce four test sets to benchmark the robustness of fine-tuned specialist models.","Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution.","%Evidence-Based QA cases.","Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA."],"url":"http://arxiv.org/abs/2402.08277v1","category":"cs.CL"}
{"created":"2024-02-13 07:54:26","title":"Elliptic Approximate Message Passing and an application to theoretical ecology","abstract":"Approximate Message Passing (AMP) algorithmshave recently gathered significant attention across disciplines such as statistical physics, machine learning, and communication systems. This study aims to extend AMP algorithms to non-symmetric (elliptic) matrices, motivated by analyzing equilibrium properties in ecological systems featuring elliptic interaction matrices.In this article, we provide the general form of an AMP algorithm associated to a random elliptic matrix, the main change lying in a modification of the corrective (Onsager) term. In order to establish the statistical properties of this algorithm, we use and prove a generalized form of Bolthausen conditioning argument, pivotal to proceed by a Gaussian-based induction.We finally address the initial motivating question from theoretical ecology. Large foodwebs are often described by Lotka-Volterra systems of coupled differential equations, where the interaction matrix is elliptic random. In this context, we design an AMP algorithm to analyze the statistical properties of the equilibrium point in a high-dimensional regime. We rigorously recover the results established by [Bunin, 2017] and [Galla,2018] who used techniques from theoretical physics, and extend them with the help of propagation of chaos type arguments.","sentences":["Approximate Message Passing (AMP) algorithmshave recently gathered significant attention across disciplines such as statistical physics, machine learning, and communication systems.","This study aims to extend AMP algorithms to non-symmetric (elliptic) matrices, motivated by analyzing equilibrium properties in ecological systems featuring elliptic interaction matrices.","In this article, we provide the general form of an AMP algorithm associated to a random elliptic matrix, the main change lying in a modification of the corrective (Onsager) term.","In order to establish the statistical properties of this algorithm, we use and prove a generalized form of Bolthausen conditioning argument, pivotal to proceed by a Gaussian-based induction.","We finally address the initial motivating question from theoretical ecology.","Large foodwebs are often described by Lotka-Volterra systems of coupled differential equations, where the interaction matrix is elliptic random.","In this context, we design an AMP algorithm to analyze the statistical properties of the equilibrium point in a high-dimensional regime.","We rigorously recover the results established by [Bunin, 2017] and [Galla,2018] who used techniques from theoretical physics, and extend them with the help of propagation of chaos type arguments."],"url":"http://arxiv.org/abs/2402.08271v1","category":"math.PR"}
{"created":"2024-02-13 07:49:57","title":"Geometry-induced Implicit Regularization in Deep ReLU Neural Networks","abstract":"It is well known that neural networks with many more parameters than training examples do not overfit. Implicit regularization phenomena, which are still not well understood, occur during optimization and 'good' networks are favored. Thus the number of parameters is not an adequate measure of complexity if we do not consider all possible networks but only the 'good' ones. To better understand which networks are favored during optimization, we study the geometry of the output set as parameters vary. When the inputs are fixed, we prove that the dimension of this set changes and that the local dimension, called batch functional dimension, is almost surely determined by the activation patterns in the hidden layers. We prove that the batch functional dimension is invariant to the symmetries of the network parameterization: neuron permutations and positive rescalings. Empirically, we establish that the batch functional dimension decreases during optimization. As a consequence, optimization leads to parameters with low batch functional dimensions. We call this phenomenon geometry-induced implicit regularization.The batch functional dimension depends on both the network parameters and inputs. To understand the impact of the inputs, we study, for fixed parameters, the largest attainable batch functional dimension when the inputs vary. We prove that this quantity, called computable full functional dimension, is also invariant to the symmetries of the network's parameterization, and is determined by the achievable activation patterns. We also provide a sampling theorem, showing a fast convergence of the estimation of the computable full functional dimension for a random input of increasing size. Empirically we find that the computable full functional dimension remains close to the number of parameters, which is related to the notion of local identifiability. This differs from the observed values for the batch functional dimension computed on training inputs and test inputs. The latter are influenced by geometry-induced implicit regularization.","sentences":["It is well known that neural networks with many more parameters than training examples do not overfit.","Implicit regularization phenomena, which are still not well understood, occur during optimization and 'good' networks are favored.","Thus the number of parameters is not an adequate measure of complexity if we do not consider all possible networks but only the 'good' ones.","To better understand which networks are favored during optimization, we study the geometry of the output set as parameters vary.","When the inputs are fixed, we prove that the dimension of this set changes and that the local dimension, called batch functional dimension, is almost surely determined by the activation patterns in the hidden layers.","We prove that the batch functional dimension is invariant to the symmetries of the network parameterization: neuron permutations and positive rescalings.","Empirically, we establish that the batch functional dimension decreases during optimization.","As a consequence, optimization leads to parameters with low batch functional dimensions.","We call this phenomenon geometry-induced implicit regularization.","The batch functional dimension depends on both the network parameters and inputs.","To understand the impact of the inputs, we study, for fixed parameters, the largest attainable batch functional dimension when the inputs vary.","We prove that this quantity, called computable full functional dimension, is also invariant to the symmetries of the network's parameterization, and is determined by the achievable activation patterns.","We also provide a sampling theorem, showing a fast convergence of the estimation of the computable full functional dimension for a random input of increasing size.","Empirically we find that the computable full functional dimension remains close to the number of parameters, which is related to the notion of local identifiability.","This differs from the observed values for the batch functional dimension computed on training inputs and test inputs.","The latter are influenced by geometry-induced implicit regularization."],"url":"http://arxiv.org/abs/2402.08269v1","category":"cs.AI"}
{"created":"2024-02-13 07:47:36","title":"World Model on Million-Length Video And Language With RingAttention","abstract":"Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language sequences, setting new benchmarks in difficult retrieval tasks and long video understanding. (b) Solutions for overcoming vision-language training challenges, including using masked sequence packing for mixing different sequence lengths, loss weighting to balance language and vision, and model-generated QA dataset for long sequence chat. (c) A highly-optimized implementation with RingAttention, masked sequence packing, and other key features for training on millions-length multimodal sequences. (d) Fully open-sourced a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens. This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the multimodal world, and broader capabilities.","sentences":["Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks.","Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language.","Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans.","However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets.","To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens.","This paper makes the following contributions: (a) Largest context size neural network:","We train one of the largest context size transformers on long video and language sequences, setting new benchmarks in difficult retrieval tasks and long video understanding.","(b) Solutions for overcoming vision-language training challenges, including using masked sequence packing for mixing different sequence lengths, loss weighting to balance language and vision, and model-generated QA dataset for long sequence chat.","(c) A highly-optimized implementation with RingAttention, masked sequence packing, and other key features for training on millions-length multimodal sequences.","(d) Fully open-sourced a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens.","This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the multimodal world, and broader capabilities."],"url":"http://arxiv.org/abs/2402.08268v1","category":"cs.LG"}
{"created":"2024-02-13 07:45:25","title":"Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss","abstract":"Image coding for machines (ICM) aims to compress images for machine analysis using recognition models rather than human vision. Hence, in ICM, it is important for the encoder to recognize and compress the information necessary for the machine recognition task. There are two main approaches in learned ICM; optimization of the compression model based on task loss, and Region of Interest (ROI) based bit allocation. These approaches provide the encoder with the recognition capability. However, optimization with task loss becomes difficult when the recognition model is deep, and ROI-based methods often involve extra overhead during evaluation. In this study, we propose a novel training method for learned ICM models that applies auxiliary loss to the encoder to improve its recognition capability and rate-distortion performance. Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in object detection and semantic segmentation tasks, compared to the conventional training method.","sentences":["Image coding for machines (ICM) aims to compress images for machine analysis using recognition models rather than human vision.","Hence, in ICM, it is important for the encoder to recognize and compress the information necessary for the machine recognition task.","There are two main approaches in learned ICM; optimization of the compression model based on task loss, and Region of Interest (ROI) based bit allocation.","These approaches provide the encoder with the recognition capability.","However, optimization with task loss becomes difficult when the recognition model is deep, and ROI-based methods often involve extra overhead during evaluation.","In this study, we propose a novel training method for learned ICM models that applies auxiliary loss to the encoder to improve its recognition capability and rate-distortion performance.","Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in object detection and semantic segmentation tasks, compared to the conventional training method."],"url":"http://arxiv.org/abs/2402.08267v1","category":"cs.CV"}
{"created":"2024-02-13 07:37:24","title":"A Dense Reward View on Aligning Text-to-Image Diffusion with Preference","abstract":"Aligning text-to-image diffusion model (T2I) with preference has been gaining increasing research attention. While prior works exist on directly optimizing T2I by preference data, these methods are developed under the bandit assumption of a latent reward on the entire diffusion reverse chain, while ignoring the sequential nature of the generation process. From literature, this may harm the efficacy and efficiency of alignment. In this paper, we take on a finer dense reward perspective and derive a tractable alignment objective that emphasizes the initial steps of the T2I reverse chain. In particular, we introduce temporal discounting into the DPO-style explicit-reward-free loss, to break the temporal symmetry therein and suit the T2I generation hierarchy. In experiments on single and multiple prompt generation, our method is competitive with strong relevant baselines, both quantitatively and qualitatively. Further studies are conducted to illustrate the insight of our approach.","sentences":["Aligning text-to-image diffusion model (T2I) with preference has been gaining increasing research attention.","While prior works exist on directly optimizing T2I by preference data, these methods are developed under the bandit assumption of a latent reward on the entire diffusion reverse chain, while ignoring the sequential nature of the generation process.","From literature, this may harm the efficacy and efficiency of alignment.","In this paper, we take on a finer dense reward perspective and derive a tractable alignment objective that emphasizes the initial steps of the T2I reverse chain.","In particular, we introduce temporal discounting into the DPO-style explicit-reward-free loss, to break the temporal symmetry therein and suit the T2I generation hierarchy.","In experiments on single and multiple prompt generation, our method is competitive with strong relevant baselines, both quantitatively and qualitatively.","Further studies are conducted to illustrate the insight of our approach."],"url":"http://arxiv.org/abs/2402.08265v1","category":"cs.CV"}
{"created":"2024-02-13 07:26:36","title":"Poisson transforms, the BGG complex, and discrete series representations of SU(n+1,1)","abstract":"The aim of this article is to construct a specific Poisson transform mapping differential forms on the sphere $S^{2n+1}$ endowed with its natural CR structure to forms on complex hyperbolic space. The transforms we construct have values that are harmonic and co-closed and they descend to the BGG (Rumin) complex and intertwine the differential operators in that complex with the exterior derivative.   Passing to the Poincar\\'e ball model, we analyze the boundary asymptotics of the values of our transforms proving that they admit a continuous extension to the boundary in degrees $\\leq n$. Finally, we show that composing the exterior derivative with the transform in degree $n$, one obtains an isomorphism between the kernel of the Rumin operator in degree $n$ and a dense subspace of the $L^2$-harmonic forms on complex hyperbolic space. These are well known to realize the direct sum of all discrete series representations of $SU(n+1,1)$, which we therefore realize on spaces of differential forms on the compact manifold $S^{2n+1}$.   The developments in this article are motivated by a program of the third author to prove some instances of the Baum-Connes conjecture. The first part of the article is valid in a much more general setting, and is also relevant for cases in which the conjecture is still open.","sentences":["The aim of this article is to construct a specific Poisson transform mapping differential forms on the sphere $S^{2n+1}$ endowed with its natural CR structure to forms on complex hyperbolic space.","The transforms we construct have values that are harmonic and co-closed and they descend to the BGG (Rumin) complex and intertwine the differential operators in that complex with the exterior derivative.   ","Passing to the Poincar\\'e ball model, we analyze the boundary asymptotics of the values of our transforms proving that they admit a continuous extension to the boundary in degrees $\\leq n$. Finally, we show that composing the exterior derivative with the transform in degree $n$, one obtains an isomorphism between the kernel of the Rumin operator in degree $n$ and a dense subspace of the $L^2$-harmonic forms on complex hyperbolic space.","These are well known to realize the direct sum of all discrete series representations of $SU(n+1,1)$, which we therefore realize on spaces of differential forms on the compact manifold $S^{2n+1}$.   The developments in this article are motivated by a program of the third author to prove some instances of the Baum-Connes conjecture.","The first part of the article is valid in a much more general setting, and is also relevant for cases in which the conjecture is still open."],"url":"http://arxiv.org/abs/2402.08262v1","category":"math.DG"}
{"created":"2024-02-13 07:23:42","title":"QuApprox: A Framework for Benchmarking the Approximability of Variational Quantum Circuit","abstract":"Most of the existing quantum neural network models, such as variational quantum circuits (VQCs), are limited in their ability to explore the non-linear relationships in input data. This gradually becomes the main obstacle for it to tackle realistic applications, such as natural language processing, medical image processing, and wireless communications. Recently, there have emerged research efforts that enable VQCs to perform non-linear operations. However, it is still unclear on the approximability of a given VQC (i.e., the order of non-linearity that can be handled by a specified design). In response to this issue, we developed an automated tool designed to benchmark the approximation of a given VQC. The proposed tool will generate a set of synthetic datasets with different orders of non-linearity and train the given VQC on these datasets to estimate their approximability. Our experiments benchmark VQCs with different designs, where we know their theoretic approximability. We then show that the proposed tool can precisely estimate the approximability, which is consistent with the theoretic value, indicating that the proposed tool can be used for benchmarking the approximability of a given quantum circuit for learning tasks.","sentences":["Most of the existing quantum neural network models, such as variational quantum circuits (VQCs), are limited in their ability to explore the non-linear relationships in input data.","This gradually becomes the main obstacle for it to tackle realistic applications, such as natural language processing, medical image processing, and wireless communications.","Recently, there have emerged research efforts that enable VQCs to perform non-linear operations.","However, it is still unclear on the approximability of a given VQC (i.e., the order of non-linearity that can be handled by a specified design).","In response to this issue, we developed an automated tool designed to benchmark the approximation of a given VQC.","The proposed tool will generate a set of synthetic datasets with different orders of non-linearity and train the given VQC on these datasets to estimate their approximability.","Our experiments benchmark VQCs with different designs, where we know their theoretic approximability.","We then show that the proposed tool can precisely estimate the approximability, which is consistent with the theoretic value, indicating that the proposed tool can be used for benchmarking the approximability of a given quantum circuit for learning tasks."],"url":"http://arxiv.org/abs/2402.08261v1","category":"quant-ph"}
{"created":"2024-02-13 07:18:08","title":"The perturbation method applied to a robust optimization problem with constraint","abstract":"The present paper studies a kind of robust optimization problems with constraint. The problem is formulated through Backward Stochastic Differential Equations (BSDEs) with quadratic generators. A necessary condition is established for the optimal solution using a terminal perturbation method and properties of Bounded Mean Oscillation (BMO) martingales. The necessary condition is further proved to be sufficient for the existence of an optimal solution under an additional convexity assumption. Finally, the optimality condition is applied to discuss problems of partial hedging with ambiguity, fundraising under ambiguity and randomized testing problems for a quadratic $g$-expectation.","sentences":["The present paper studies a kind of robust optimization problems with constraint.","The problem is formulated through Backward Stochastic Differential Equations (BSDEs) with quadratic generators.","A necessary condition is established for the optimal solution using a terminal perturbation method and properties of Bounded Mean Oscillation (BMO) martingales.","The necessary condition is further proved to be sufficient for the existence of an optimal solution under an additional convexity assumption.","Finally, the optimality condition is applied to discuss problems of partial hedging with ambiguity, fundraising under ambiguity and randomized testing problems for a quadratic $g$-expectation."],"url":"http://arxiv.org/abs/2402.08260v1","category":"math.OC"}
{"created":"2024-02-13 07:17:52","title":"A Survey of Table Reasoning with Large Language Models","abstract":"Table reasoning, which aims to generate the corresponding answer to the question following the user requirement according to the provided table, and optionally a text description of the table, effectively improving the efficiency of obtaining information. Recently, using Large Language Models (LLMs) has become the mainstream method for table reasoning, because it not only significantly reduces the annotation cost but also exceeds the performance of previous methods. However, existing research still lacks a summary of LLM-based table reasoning works. Due to the existing lack of research, questions about which techniques can improve table reasoning performance in the era of LLMs, why LLMs excel at table reasoning, and how to enhance table reasoning abilities in the future, remain largely unexplored. This gap significantly limits progress in research. To answer the above questions and advance table reasoning research with LLMs, we present this survey to analyze existing research, inspiring future work. In this paper, we analyze the mainstream techniques used to improve table reasoning performance in the LLM era, and the advantages of LLMs compared to pre-LLMs for solving table reasoning. We provide research directions from both the improvement of existing methods and the expansion of practical applications to inspire future research.","sentences":["Table reasoning, which aims to generate the corresponding answer to the question following the user requirement according to the provided table, and optionally a text description of the table, effectively improving the efficiency of obtaining information.","Recently, using Large Language Models (LLMs) has become the mainstream method for table reasoning, because it not only significantly reduces the annotation cost but also exceeds the performance of previous methods.","However, existing research still lacks a summary of LLM-based table reasoning works.","Due to the existing lack of research, questions about which techniques can improve table reasoning performance in the era of LLMs, why LLMs excel at table reasoning, and how to enhance table reasoning abilities in the future, remain largely unexplored.","This gap significantly limits progress in research.","To answer the above questions and advance table reasoning research with LLMs, we present this survey to analyze existing research, inspiring future work.","In this paper, we analyze the mainstream techniques used to improve table reasoning performance in the LLM era, and the advantages of LLMs compared to pre-LLMs for solving table reasoning.","We provide research directions from both the improvement of existing methods and the expansion of practical applications to inspire future research."],"url":"http://arxiv.org/abs/2402.08259v1","category":"cs.CL"}
{"created":"2024-02-13 07:12:44","title":"Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs","abstract":"The knowledge concept recommendation in Massive Open Online Courses (MOOCs) is a significant issue that has garnered widespread attention. Existing methods primarily rely on the explicit relations between users and knowledge concepts on the MOOC platforms for recommendation. However, there are numerous implicit relations (e.g., shared interests or same knowledge levels between users) generated within the users' learning activities on the MOOC platforms. Existing methods fail to consider these implicit relations, and these relations themselves are difficult to learn and represent, causing poor performance in knowledge concept recommendation and an inability to meet users' personalized needs. To address this issue, we propose a novel framework based on contrastive learning, which can represent and balance the explicit and implicit relations for knowledge concept recommendation in MOOCs (CL-KCRec). Specifically, we first construct a MOOCs heterogeneous information network (HIN) by modeling the data from the MOOC platforms. Then, we utilize a relation-updated graph convolutional network and stacked multi-channel graph neural network to represent the explicit and implicit relations in the HIN, respectively. Considering that the quantity of explicit relations is relatively fewer compared to implicit relations in MOOCs, we propose a contrastive learning with prototypical graph to enhance the representations of both relations to capture their fruitful inherent relational knowledge, which can guide the propagation of students' preferences within the HIN. Based on these enhanced representations, to ensure the balanced contribution of both towards the final recommendation, we propose a dual-head attention mechanism for balanced fusion. Experimental results demonstrate that CL-KCRec outperforms several state-of-the-art baselines on real-world datasets in terms of HR, NDCG and MRR.","sentences":["The knowledge concept recommendation in Massive Open Online Courses (MOOCs) is a significant issue that has garnered widespread attention.","Existing methods primarily rely on the explicit relations between users and knowledge concepts on the MOOC platforms for recommendation.","However, there are numerous implicit relations (e.g., shared interests or same knowledge levels between users) generated within the users' learning activities on the MOOC platforms.","Existing methods fail to consider these implicit relations, and these relations themselves are difficult to learn and represent, causing poor performance in knowledge concept recommendation and an inability to meet users' personalized needs.","To address this issue, we propose a novel framework based on contrastive learning, which can represent and balance the explicit and implicit relations for knowledge concept recommendation in MOOCs (CL-KCRec).","Specifically, we first construct a MOOCs heterogeneous information network (HIN) by modeling the data from the MOOC platforms.","Then, we utilize a relation-updated graph convolutional network and stacked multi-channel graph neural network to represent the explicit and implicit relations in the HIN, respectively.","Considering that the quantity of explicit relations is relatively fewer compared to implicit relations in MOOCs, we propose a contrastive learning with prototypical graph to enhance the representations of both relations to capture their fruitful inherent relational knowledge, which can guide the propagation of students' preferences within the HIN.","Based on these enhanced representations, to ensure the balanced contribution of both towards the final recommendation, we propose a dual-head attention mechanism for balanced fusion.","Experimental results demonstrate that CL-KCRec outperforms several state-of-the-art baselines on real-world datasets in terms of HR, NDCG and MRR."],"url":"http://arxiv.org/abs/2402.08256v1","category":"cs.IR"}
{"created":"2024-02-13 07:07:37","title":"Distal Interference: Exploring the Limits of Model-Based Continual Learning","abstract":"Continual learning is the sequential learning of different tasks by a machine learning model. Continual learning is known to be hindered by catastrophic interference or forgetting, i.e. rapid unlearning of earlier learned tasks when new tasks are learned. Despite their practical success, artificial neural networks (ANNs) are prone to catastrophic interference. This study analyses how gradient descent and overlapping representations between distant input points lead to distal interference and catastrophic interference. Distal interference refers to the phenomenon where training a model on a subset of the domain leads to non-local changes on other subsets of the domain. This study shows that uniformly trainable models without distal interference must be exponentially large. A novel antisymmetric bounded exponential layer B-spline ANN architecture named ABEL-Spline is proposed that can approximate any continuous function, is uniformly trainable, has polynomial computational complexity, and provides some guarantees for distal interference. Experiments are presented to demonstrate the theoretical properties of ABEL-Splines. ABEL-Splines are also evaluated on benchmark regression problems. It is concluded that the weaker distal interference guarantees in ABEL-Splines are insufficient for model-only continual learning. It is conjectured that continual learning with polynomial complexity models requires augmentation of the training data or algorithm.","sentences":["Continual learning is the sequential learning of different tasks by a machine learning model.","Continual learning is known to be hindered by catastrophic interference or forgetting, i.e. rapid unlearning of earlier learned tasks when new tasks are learned.","Despite their practical success, artificial neural networks (ANNs) are prone to catastrophic interference.","This study analyses how gradient descent and overlapping representations between distant input points lead to distal interference and catastrophic interference.","Distal interference refers to the phenomenon where training a model on a subset of the domain leads to non-local changes on other subsets of the domain.","This study shows that uniformly trainable models without distal interference must be exponentially large.","A novel antisymmetric bounded exponential layer B-spline ANN architecture named ABEL-Spline is proposed that can approximate any continuous function, is uniformly trainable, has polynomial computational complexity, and provides some guarantees for distal interference.","Experiments are presented to demonstrate the theoretical properties of ABEL-Splines.","ABEL-Splines are also evaluated on benchmark regression problems.","It is concluded that the weaker distal interference guarantees in ABEL-Splines are insufficient for model-only continual learning.","It is conjectured that continual learning with polynomial complexity models requires augmentation of the training data or algorithm."],"url":"http://arxiv.org/abs/2402.08255v1","category":"cs.LG"}
{"created":"2024-02-13 06:51:37","title":"Inherent photocurrent of infrared diode under negative illumination","abstract":"We investigate inherent photovoltaic power generation of narrow gap p-n junction diodes under negative illumination condition, which has been expected to power source in the nighttime with terrestrial radiation to outer space. To reveal factors restricting the energy conversion efficiency under negative illumination, we improve measurement method using sufficiently cold surface cooled by liquid nitrogen. Since the diode faces the cold surface in a vacuum chamber, we can avoid the effect of the atmosphere absorptions of the real cold sky, and maximize the photovoltage or photocurrent, which is an inherent property of each diode. We reveal that the temperature dependence on the inherent photovoltage and photocurrent. We analyze the experimental data with a single homo junction model and find that the ratio between the diffusion length of the minority carriers and the light penetration length plays an important role for the limitation of the power generation efficiency.","sentences":["We investigate inherent photovoltaic power generation of narrow gap p-n junction diodes under negative illumination condition, which has been expected to power source in the nighttime with terrestrial radiation to outer space.","To reveal factors restricting the energy conversion efficiency under negative illumination, we improve measurement method using sufficiently cold surface cooled by liquid nitrogen.","Since the diode faces the cold surface in a vacuum chamber, we can avoid the effect of the atmosphere absorptions of the real cold sky, and maximize the photovoltage or photocurrent, which is an inherent property of each diode.","We reveal that the temperature dependence on the inherent photovoltage and photocurrent.","We analyze the experimental data with a single homo junction model and find that the ratio between the diffusion length of the minority carriers and the light penetration length plays an important role for the limitation of the power generation efficiency."],"url":"http://arxiv.org/abs/2402.08253v1","category":"physics.app-ph"}
{"created":"2024-02-13 06:47:26","title":"Unrestricted Global Phase Bias-Aware Single-channel Speech Enhancement with Conformer-based Metric GAN","abstract":"With the rapid development of neural networks in recent years, the ability of various networks to enhance the magnitude spectrum of noisy speech in the single-channel speech enhancement domain has become exceptionally outstanding. However, enhancing the phase spectrum using neural networks is often ineffective, which remains a challenging problem. In this paper, we found that the human ear cannot sensitively perceive the difference between a precise phase spectrum and a biased phase (BP) spectrum. Therefore, we propose an optimization method of phase reconstruction, allowing freedom on the global-phase bias instead of reconstructing the precise phase spectrum. We applied it to a Conformer-based Metric Generative Adversarial Networks (CMGAN) baseline model, which relaxes the existing constraints of precise phase and gives the neural network a broader learning space. Results show that this method achieves a new state-of-the-art performance without incurring additional computational overhead.","sentences":["With the rapid development of neural networks in recent years, the ability of various networks to enhance the magnitude spectrum of noisy speech in the single-channel speech enhancement domain has become exceptionally outstanding.","However, enhancing the phase spectrum using neural networks is often ineffective, which remains a challenging problem.","In this paper, we found that the human ear cannot sensitively perceive the difference between a precise phase spectrum and a biased phase (BP) spectrum.","Therefore, we propose an optimization method of phase reconstruction, allowing freedom on the global-phase bias instead of reconstructing the precise phase spectrum.","We applied it to a Conformer-based Metric Generative Adversarial Networks (CMGAN) baseline model, which relaxes the existing constraints of precise phase and gives the neural network a broader learning space.","Results show that this method achieves a new state-of-the-art performance without incurring additional computational overhead."],"url":"http://arxiv.org/abs/2402.08252v1","category":"eess.AS"}
{"created":"2024-02-13 06:38:46","title":"A survey of recent methods for addressing AI fairness and bias in biomedicine","abstract":"Artificial intelligence (AI) systems have the potential to revolutionize clinical practices, including improving diagnostic accuracy and surgical decision-making, while also reducing costs and manpower. However, it is important to recognize that these systems may perpetuate social inequities or demonstrate biases, such as those based on race or gender. Such biases can occur before, during, or after the development of AI models, making it critical to understand and address potential biases to enable the accurate and reliable application of AI models in clinical settings. To mitigate bias concerns during model development, we surveyed recent publications on different debiasing methods in the fields of biomedical natural language processing (NLP) or computer vision (CV). Then we discussed the methods that have been applied in the biomedical domain to address bias. We performed our literature search on PubMed, ACM digital library, and IEEE Xplore of relevant articles published between January 2018 and December 2023 using multiple combinations of keywords. We then filtered the result of 10,041 articles automatically with loose constraints, and manually inspected the abstracts of the remaining 890 articles to identify the 55 articles included in this review. Additional articles in the references are also included in this review. We discuss each method and compare its strengths and weaknesses. Finally, we review other potential methods from the general domain that could be applied to biomedicine to address bias and improve fairness.The bias of AIs in biomedicine can originate from multiple sources. Existing debiasing methods that focus on algorithms can be categorized into distributional or algorithmic.","sentences":["Artificial intelligence (AI) systems have the potential to revolutionize clinical practices, including improving diagnostic accuracy and surgical decision-making, while also reducing costs and manpower.","However, it is important to recognize that these systems may perpetuate social inequities or demonstrate biases, such as those based on race or gender.","Such biases can occur before, during, or after the development of AI models, making it critical to understand and address potential biases to enable the accurate and reliable application of AI models in clinical settings.","To mitigate bias concerns during model development, we surveyed recent publications on different debiasing methods in the fields of biomedical natural language processing (NLP) or computer vision (CV).","Then we discussed the methods that have been applied in the biomedical domain to address bias.","We performed our literature search on PubMed, ACM digital library, and IEEE Xplore of relevant articles published between January 2018 and December 2023 using multiple combinations of keywords.","We then filtered the result of 10,041 articles automatically with loose constraints, and manually inspected the abstracts of the remaining 890 articles to identify the 55 articles included in this review.","Additional articles in the references are also included in this review.","We discuss each method and compare its strengths and weaknesses.","Finally, we review other potential methods from the general domain that could be applied to biomedicine to address bias and improve fairness.","The bias of AIs in biomedicine can originate from multiple sources.","Existing debiasing methods that focus on algorithms can be categorized into distributional or algorithmic."],"url":"http://arxiv.org/abs/2402.08250v1","category":"cs.AI"}
{"created":"2024-02-13 06:35:00","title":"SepRep-Net: Multi-source Free Domain Adaptation via Model Separation And Reparameterization","abstract":"We consider multi-source free domain adaptation, the problem of adapting multiple existing models to a new domain without accessing the source data. Among existing approaches, methods based on model ensemble are effective in both the source and target domains, but incur significantly increased computational costs. Towards this dilemma, in this work, we propose a novel framework called SepRep-Net, which tackles multi-source free domain adaptation via model Separation and Reparameterization.Concretely, SepRep-Net reassembled multiple existing models to a unified network, while maintaining separate pathways (Separation). During training, separate pathways are optimized in parallel with the information exchange regularly performed via an additional feature merging unit. With our specific design, these pathways can be further reparameterized into a single one to facilitate inference (Reparameterization). SepRep-Net is characterized by 1) effectiveness: competitive performance on the target domain, 2) efficiency: low computational costs, and 3) generalizability: maintaining more source knowledge than existing solutions. As a general approach, SepRep-Net can be seamlessly plugged into various methods. Extensive experiments validate the performance of SepRep-Net on mainstream benchmarks.","sentences":["We consider multi-source free domain adaptation, the problem of adapting multiple existing models to a new domain without accessing the source data.","Among existing approaches, methods based on model ensemble are effective in both the source and target domains, but incur significantly increased computational costs.","Towards this dilemma, in this work, we propose a novel framework called SepRep-Net, which tackles multi-source free domain adaptation via model Separation and Reparameterization.","Concretely, SepRep-Net reassembled multiple existing models to a unified network, while maintaining separate pathways (Separation).","During training, separate pathways are optimized in parallel with the information exchange regularly performed via an additional feature merging unit.","With our specific design, these pathways can be further reparameterized into a single one to facilitate inference (Reparameterization).","SepRep-Net is characterized by 1) effectiveness: competitive performance on the target domain, 2) efficiency: low computational costs, and 3) generalizability: maintaining more source knowledge than existing solutions.","As a general approach, SepRep-Net can be seamlessly plugged into various methods.","Extensive experiments validate the performance of SepRep-Net on mainstream benchmarks."],"url":"http://arxiv.org/abs/2402.08249v1","category":"cs.CV"}
{"created":"2024-02-13 06:20:37","title":"Ant Colony Optimization for Cooperative Inspection Path Planning Using Multiple Unmanned Aerial Vehicles","abstract":"This paper presents a new swarm intelligence-based approach to deal with the cooperative path planning problem of unmanned aerial vehicles (UAVs), which is essential for the automatic inspection of infrastructure. The approach uses a 3D model of the structure to generate viewpoints for the UAVs. The calculation of the viewpoints considers the constraints related to the UAV formation model, camera parameters, and requirements for data post-processing. The viewpoints are then used as input to formulate the path planning as an extended traveling salesman problem and the definition of a new cost function. Ant colony optimization is finally used to solve the problem to yield optimal inspection paths. Experiments with 3D models of real structures have been conducted to evaluate the performance of the proposed approach. The results show that our system is not only capable of generating feasible inspection paths for UAVs but also reducing the path length by 29.47\\% for complex structures when compared with another heuristic approach. The source code of the algorithm can be found at https://github.com/duynamrcv/aco_3d_ipp.","sentences":["This paper presents a new swarm intelligence-based approach to deal with the cooperative path planning problem of unmanned aerial vehicles (UAVs), which is essential for the automatic inspection of infrastructure.","The approach uses a 3D model of the structure to generate viewpoints for the UAVs.","The calculation of the viewpoints considers the constraints related to the UAV formation model, camera parameters, and requirements for data post-processing.","The viewpoints are then used as input to formulate the path planning as an extended traveling salesman problem and the definition of a new cost function.","Ant colony optimization is finally used to solve the problem to yield optimal inspection paths.","Experiments with 3D models of real structures have been conducted to evaluate the performance of the proposed approach.","The results show that our system is not only capable of generating feasible inspection paths for UAVs but also reducing the path length by 29.47\\% for complex structures when compared with another heuristic approach.","The source code of the algorithm can be found at https://github.com/duynamrcv/aco_3d_ipp."],"url":"http://arxiv.org/abs/2402.08246v1","category":"eess.SY"}
{"created":"2024-02-13 06:13:17","title":"Towards Equitable Agile Research and Development of AI and Robotics","abstract":"Machine Learning (ML) and 'Artificial Intelligence' ('AI') methods tend to replicate and amplify existing biases and prejudices, as do Robots with AI. For example, robots with facial recognition have failed to identify Black Women as human, while others have categorized people, such as Black Men, as criminals based on appearance alone. A 'culture of modularity' means harms are perceived as 'out of scope', or someone else's responsibility, throughout employment positions in the 'AI supply chain'. Incidents are routine enough (incidentdatabase.ai lists over 2000 examples) to indicate that few organizations are capable of completely respecting peoples' rights; meeting claimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and then addressing such failures in their organizations and artifacts. We propose a framework for adapting widely practiced Research and Development (R&D) project management methodologies to build organizational equity capabilities and better integrate known evidence-based best practices. We describe how project teams can organize and operationalize the most promising practices, skill sets, organizational cultures, and methods to detect and address rights-based fairness, equity, accountability, and ethical problems as early as possible when they are often less harmful and easier to mitigate; then monitor for unforeseen incidents to adaptively and constructively address them. Our primary example adapts an Agile development process based on Scrum, one of the most widely adopted approaches to organizing R&D teams. We also discuss limitations of our proposed framework and future research directions.","sentences":["Machine Learning (ML) and 'Artificial Intelligence' ('AI') methods tend to replicate and amplify existing biases and prejudices, as do Robots with AI.","For example, robots with facial recognition have failed to identify Black Women as human, while others have categorized people, such as Black Men, as criminals based on appearance alone.","A 'culture of modularity' means harms are perceived as 'out of scope', or someone else's responsibility, throughout employment positions in the 'AI supply chain'.","Incidents are routine enough (incidentdatabase.ai lists over 2000 examples) to indicate that few organizations are capable of completely respecting peoples' rights; meeting claimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and then addressing such failures in their organizations and artifacts.","We propose a framework for adapting widely practiced Research and Development (R&D) project management methodologies to build organizational equity capabilities and better integrate known evidence-based best practices.","We describe how project teams can organize and operationalize the most promising practices, skill sets, organizational cultures, and methods to detect and address rights-based fairness, equity, accountability, and ethical problems as early as possible when they are often less harmful and easier to mitigate; then monitor for unforeseen incidents to adaptively and constructively address them.","Our primary example adapts an Agile development process based on Scrum, one of the most widely adopted approaches to organizing R&D teams.","We also discuss limitations of our proposed framework and future research directions."],"url":"http://arxiv.org/abs/2402.08242v1","category":"cs.AI"}
{"created":"2024-02-13 06:09:48","title":"Forecasts for Constraining Lorentz-violating Damping of Gravitational Waves from Compact Binary Inspirals","abstract":"Violation of Lorentz symmetry can result in two distinct effects in the propagation of the gravitational waves (GWs). One is a modified dispersion relation and another is a frequency-dependent damping of GWs. While the former has been extensively studied in the literature, in this paper we concentrate on the frequency-dependent damping effect that arises from several specific Lorentz-violating theories, such as spatial covariant gravities, Ho\\v{r}ava-Lifshitz gravities, etc. This Lorentz-violating damping effect changes the damping rate of GWs at different frequencies and leads to an amplitude correction to the GW waveform of compact binary inspiral systems. With this modified waveform, we then use the Fisher information matrix to investigate the prospects of constraining the Lorentz-violating damping effect with GW observations. We consider both ground-based and space-based GW detectors, including the advanced LIGO, Einstein Telescope, Cosmic Explorer (CE), Taiji, TianQin, and LISA. Our results indicate that the ground-based detectors in general give tighter constraints than those from the space-based detectors. Among the considered three ground-based detectors, CE can give the tightest constraints on the Lorentz-violating damping effect, which improves the current constraint from LIGO-Virgo-KAGRA events by about 8 times.","sentences":["Violation of Lorentz symmetry can result in two distinct effects in the propagation of the gravitational waves (GWs).","One is a modified dispersion relation and another is a frequency-dependent damping of GWs.","While the former has been extensively studied in the literature, in this paper we concentrate on the frequency-dependent damping effect that arises from several specific Lorentz-violating theories, such as spatial covariant gravities, Ho\\v{r}ava-Lifshitz gravities, etc.","This Lorentz-violating damping effect changes the damping rate of GWs at different frequencies and leads to an amplitude correction to the GW waveform of compact binary inspiral systems.","With this modified waveform, we then use the Fisher information matrix to investigate the prospects of constraining the Lorentz-violating damping effect with GW observations.","We consider both ground-based and space-based GW detectors, including the advanced LIGO, Einstein Telescope, Cosmic Explorer (CE), Taiji, TianQin, and LISA.","Our results indicate that the ground-based detectors in general give tighter constraints than those from the space-based detectors.","Among the considered three ground-based detectors, CE can give the tightest constraints on the Lorentz-violating damping effect, which improves the current constraint from LIGO-Virgo-KAGRA events by about 8 times."],"url":"http://arxiv.org/abs/2402.08240v1","category":"gr-qc"}
{"created":"2024-02-13 06:07:35","title":"Opportunistic Scheduling Using Statistical Information of Wireless Channels","abstract":"This paper considers opportunistic scheduler (OS) design using statistical channel state information~(CSI). We apply max-weight schedulers (MWSs) to maximize a utility function of users' average data rates. MWSs schedule the user with the highest weighted instantaneous data rate every time slot. Existing methods require hundreds of time slots to adjust the MWS's weights according to the instantaneous CSI before finding the optimal weights that maximize the utility function. In contrast, our MWS design requires few slots for estimating the statistical CSI. Specifically, we formulate a weight optimization problem using the mean and variance of users' signal-to-noise ratios (SNRs) to construct constraints bounding users' feasible average rates. Here, the utility function is the formulated objective, and the MWS's weights are optimization variables. We develop an iterative solver for the problem and prove that it finds the optimal weights. We also design an online architecture where the solver adaptively generates optimal weights for networks with varying mean and variance of the SNRs. Simulations show that our methods effectively require $4\\sim10$ times fewer slots to find the optimal weights and achieve $5\\sim15\\%$ better average rates than the existing methods.","sentences":["This paper considers opportunistic scheduler (OS) design using statistical channel state information~(CSI).","We apply max-weight schedulers (MWSs) to maximize a utility function of users' average data rates.","MWSs schedule the user with the highest weighted instantaneous data rate every time slot.","Existing methods require hundreds of time slots to adjust the MWS's weights according to the instantaneous CSI before finding the optimal weights that maximize the utility function.","In contrast, our MWS design requires few slots for estimating the statistical CSI.","Specifically, we formulate a weight optimization problem using the mean and variance of users' signal-to-noise ratios (SNRs) to construct constraints bounding users' feasible average rates.","Here, the utility function is the formulated objective, and the MWS's weights are optimization variables.","We develop an iterative solver for the problem and prove that it finds the optimal weights.","We also design an online architecture where the solver adaptively generates optimal weights for networks with varying mean and variance of the SNRs.","Simulations show that our methods effectively require $4\\sim10$ times fewer slots to find the optimal weights and achieve $5\\sim15\\%$ better average rates than the existing methods."],"url":"http://arxiv.org/abs/2402.08238v1","category":"cs.IT"}
{"created":"2024-02-13 06:02:05","title":"BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT","abstract":"We propose BERT4FCA, a novel method for link prediction in bipartite networks, using formal concept analysis (FCA) and BERT. Link prediction in bipartite networks is an important task that can solve various practical problems like friend recommendation in social networks and co-authorship prediction in author-paper networks. Recent research has found that in bipartite networks, maximal bi-cliques provide important information for link prediction, and they can be extracted by FCA. Some FCA-based bipartite link prediction methods have achieved good performance. However, we figured out that their performance could be further improved because these methods did not fully capture the rich information of the extracted maximal bi-cliques. To address this limitation, we propose an approach using BERT, which can learn more information from the maximal bi-cliques extracted by FCA and use them to make link prediction. We conduct experiments on three real-world bipartite networks and demonstrate that our method outperforms previous FCA-based methods, and some classic methods such as matrix-factorization and node2vec.","sentences":["We propose BERT4FCA, a novel method for link prediction in bipartite networks, using formal concept analysis (FCA) and BERT.","Link prediction in bipartite networks is an important task that can solve various practical problems like friend recommendation in social networks and co-authorship prediction in author-paper networks.","Recent research has found that in bipartite networks, maximal bi-cliques provide important information for link prediction, and they can be extracted by FCA.","Some FCA-based bipartite link prediction methods have achieved good performance.","However, we figured out that their performance could be further improved because these methods did not fully capture the rich information of the extracted maximal bi-cliques.","To address this limitation, we propose an approach using BERT, which can learn more information from the maximal bi-cliques extracted by FCA and use them to make link prediction.","We conduct experiments on three real-world bipartite networks and demonstrate that our method outperforms previous FCA-based methods, and some classic methods such as matrix-factorization and node2vec."],"url":"http://arxiv.org/abs/2402.08236v1","category":"cs.AI"}
{"created":"2024-02-13 05:53:00","title":"End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture","abstract":"In Statistical Arbitrage (StatArb), classical mean reversion trading strategies typically hinge on asset-pricing or PCA based models to identify the mean of a synthetic asset. Once such a (linear) model is identified, a separate mean reversion strategy is then devised to generate a trading signal. With a view of generalising such an approach and turning it truly data-driven, we study the utility of Autoencoder architectures in StatArb. As a first approach, we employ a standard Autoencoder trained on US stock returns to derive trading strategies based on the Ornstein-Uhlenbeck (OU) process. To further enhance this model, we take a policy-learning approach and embed the Autoencoder network into a neural network representation of a space of portfolio trading policies. This integration outputs portfolio allocations directly and is end-to-end trainable by backpropagation of the risk-adjusted returns of the neural policy. Our findings demonstrate that this innovative end-to-end policy learning approach not only simplifies the strategy development process, but also yields superior gross returns over its competitors illustrating the potential of end-to-end training over classical two-stage approaches.","sentences":["In Statistical Arbitrage (StatArb), classical mean reversion trading strategies typically hinge on asset-pricing or PCA based models to identify the mean of a synthetic asset.","Once such a (linear) model is identified, a separate mean reversion strategy is then devised to generate a trading signal.","With a view of generalising such an approach and turning it truly data-driven, we study the utility of Autoencoder architectures in StatArb.","As a first approach, we employ a standard Autoencoder trained on US stock returns to derive trading strategies based on the Ornstein-Uhlenbeck (OU) process.","To further enhance this model, we take a policy-learning approach and embed the Autoencoder network into a neural network representation of a space of portfolio trading policies.","This integration outputs portfolio allocations directly and is end-to-end trainable by backpropagation of the risk-adjusted returns of the neural policy.","Our findings demonstrate that this innovative end-to-end policy learning approach not only simplifies the strategy development process, but also yields superior gross returns over its competitors illustrating the potential of end-to-end training over classical two-stage approaches."],"url":"http://arxiv.org/abs/2402.08233v1","category":"q-fin.TR"}
{"created":"2024-02-13 05:51:43","title":"Integrating High-Dimensional Functions Deterministically","abstract":"We design a Quasi-Polynomial time deterministic approximation algorithm for computing the integral of a multi-dimensional separable function, supported by some underlying hyper-graph structure, appropriately defined. Equivalently, our integral is the partition function of a graphical model with continuous potentials. While randomized algorithms for high-dimensional integration are widely known, deterministic counterparts generally do not exist. We use the correlation decay method applied to the Riemann sum of the function to produce our algorithm. For our method to work, we require that the domain is bounded and the hyper-edge potentials are positive and bounded on the domain. We further assume that upper and lower bounds on the potentials separated by a multiplicative factor of $1 + O(1/\\Delta^2)$, where $\\Delta$ is the maximum degree of the graph. When $\\Delta = 3$, our method works provided the upper and lower bounds are separated by a factor of at most $1.0479$. To the best of our knowledge, our algorithm is the first deterministic algorithm for high-dimensional integration of a continuous function, apart from the case of trivial product form distributions.","sentences":["We design a Quasi-Polynomial time deterministic approximation algorithm for computing the integral of a multi-dimensional separable function, supported by some underlying hyper-graph structure, appropriately defined.","Equivalently, our integral is the partition function of a graphical model with continuous potentials.","While randomized algorithms for high-dimensional integration are widely known, deterministic counterparts generally do not exist.","We use the correlation decay method applied to the Riemann sum of the function to produce our algorithm.","For our method to work, we require that the domain is bounded and the hyper-edge potentials are positive and bounded on the domain.","We further assume that upper and lower bounds on the potentials separated by a multiplicative factor of $1 + O(1/\\Delta^2)$, where $\\Delta$ is the maximum degree of the graph.","When $\\Delta = 3$, our method works provided the upper and lower bounds are separated by a factor of at most $1.0479$. To the best of our knowledge, our algorithm is the first deterministic algorithm for high-dimensional integration of a continuous function, apart from the case of trivial product form distributions."],"url":"http://arxiv.org/abs/2402.08232v1","category":"cs.DS"}
{"created":"2024-02-13 18:13:02","title":"Tuning the Spin Interaction in Non-planar Organic Diradicals Through Mechanical Manipulation","abstract":"Open-shell polycyclic aromatic hydrocarbons (PAHs) represent promising building blocks for carbon-based functional magnetic materials. Their magnetic properties stem from the presence of unpaired electrons localized in radical states of $\\pi$ character. Consequently, these materials are inclined to exhibit spin delocalization, form extended collective states, and respond to the flexibility of the molecular backbones. However, they are also highly reactive, requiring structural strategies to protect the radical states from reacting with the environment. Here, we demonstrate that the open-shell ground state of the diradical 2-OS survives on a Au(111) substrate as a global singlet formed by two unpaired electrons with anti-parallel spins coupled through a conformational dependent interaction. The 2-OS molecule is a protected derivative of the Chichibabin's diradical, featuring a non-planar geometry that destabilizes the closed-shell quinoidal structure. Using scanning tunneling microscopy (STM), we localized the two interacting spins at the molecular edges, and detected an excited triplet state a few millielectronvolts above the singlet ground state. Mean-field Hubbard simulations reveal that the exchange coupling between the two spins strongly depends on the torsional angles between the different molecular moieties, suggesting the possibility of influencing the molecule's magnetic state through structural changes. This was demonstrated here using the STM tip to manipulate the molecular conformation, while simultaneously detecting changes in the spin excitation spectrum. Our work suggests the potential of these PAHs for a new class of all-carbon spin-crossover materials.","sentences":["Open-shell polycyclic aromatic hydrocarbons (PAHs) represent promising building blocks for carbon-based functional magnetic materials.","Their magnetic properties stem from the presence of unpaired electrons localized in radical states of $\\pi$ character.","Consequently, these materials are inclined to exhibit spin delocalization, form extended collective states, and respond to the flexibility of the molecular backbones.","However, they are also highly reactive, requiring structural strategies to protect the radical states from reacting with the environment.","Here, we demonstrate that the open-shell ground state of the diradical 2-OS survives on a Au(111) substrate as a global singlet formed by two unpaired electrons with anti-parallel spins coupled through a conformational dependent interaction.","The 2-OS molecule is a protected derivative of the Chichibabin's diradical, featuring a non-planar geometry that destabilizes the closed-shell quinoidal structure.","Using scanning tunneling microscopy (STM), we localized the two interacting spins at the molecular edges, and detected an excited triplet state a few millielectronvolts above the singlet ground state.","Mean-field Hubbard simulations reveal that the exchange coupling between the two spins strongly depends on the torsional angles between the different molecular moieties, suggesting the possibility of influencing the molecule's magnetic state through structural changes.","This was demonstrated here using the STM tip to manipulate the molecular conformation, while simultaneously detecting changes in the spin excitation spectrum.","Our work suggests the potential of these PAHs for a new class of all-carbon spin-crossover materials."],"url":"http://arxiv.org/abs/2402.08641v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-13 18:04:53","title":"SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages","abstract":"Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs). While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. The scores are obtained using a comparative annotation framework. We describe the data collection and annotation processes, related challenges when building the datasets, and their impact and utility in NLP. We further report experiments for each language and across the different languages.","sentences":["Exploring and quantifying semantic relatedness is central to representing language.","It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs).","While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness.","In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu.","These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources.","Each instance in the SemRel datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences.","The scores are obtained using a comparative annotation framework.","We describe the data collection and annotation processes, related challenges when building the datasets, and their impact and utility in NLP.","We further report experiments for each language and across the different languages."],"url":"http://arxiv.org/abs/2402.08638v1","category":"cs.CL"}
{"created":"2024-02-13 17:56:47","title":"Nonlinear Graphon mean-field systems","abstract":"We address a system of weakly interacting particles where the heterogenous connections among the particles are described by a graph sequence and the number of particles grows to infinity. Our results extend the existing law of large numbers and propagation of chaos results to the case where the interaction between one particle and its neighbors is expressed as a nonlinear function of the local empirical measure. In the limit of the number of particles which tends to infinity, if the graph sequence converges to a graphon, then we show that the limit system is described by an infinite collection of processes and can be seen as a process in a suitable $L^2$ space constructed via a Fubini extension. The proof is built on decoupling techniques and careful estimates of the Wasserstein distance.","sentences":["We address a system of weakly interacting particles where the heterogenous connections among the particles are described by a graph sequence and the number of particles grows to infinity.","Our results extend the existing law of large numbers and propagation of chaos results to the case where the interaction between one particle and its neighbors is expressed as a nonlinear function of the local empirical measure.","In the limit of the number of particles which tends to infinity, if the graph sequence converges to a graphon, then we show that the limit system is described by an infinite collection of processes and can be seen as a process in a suitable $L^2$ space constructed via a Fubini extension.","The proof is built on decoupling techniques and careful estimates of the Wasserstein distance."],"url":"http://arxiv.org/abs/2402.08628v1","category":"math.PR"}
{"created":"2024-02-13 17:09:29","title":"Globally-Optimal Greedy Experiment Selection for Active Sequential Estimation","abstract":"Motivated by modern applications such as computerized adaptive testing, sequential rank aggregation, and heterogeneous data source selection, we study the problem of active sequential estimation, which involves adaptively selecting experiments for sequentially collected data. The goal is to design experiment selection rules for more accurate model estimation. Greedy information-based experiment selection methods, optimizing the information gain for one-step ahead, have been employed in practice thanks to their computational convenience, flexibility to context or task changes, and broad applicability. However, statistical analysis is restricted to one-dimensional cases due to the problem's combinatorial nature and the seemingly limited capacity of greedy algorithms, leaving the multidimensional problem open.   In this study, we close the gap for multidimensional problems. In particular, we propose adopting a class of greedy experiment selection methods and provide statistical analysis for the maximum likelihood estimator following these selection rules. This class encompasses both existing methods and introduces new methods with improved numerical efficiency. We prove that these methods produce consistent and asymptotically normal estimators. Additionally, within a decision theory framework, we establish that the proposed methods achieve asymptotic optimality when the risk measure aligns with the selection rule. We also conduct extensive numerical studies on both simulated and real data to illustrate the efficacy of the proposed methods.   From a technical perspective, we devise new analytical tools to address theoretical challenges. These analytical tools are of independent theoretical interest and may be reused in related problems involving stochastic approximation and sequential designs.","sentences":["Motivated by modern applications such as computerized adaptive testing, sequential rank aggregation, and heterogeneous data source selection, we study the problem of active sequential estimation, which involves adaptively selecting experiments for sequentially collected data.","The goal is to design experiment selection rules for more accurate model estimation.","Greedy information-based experiment selection methods, optimizing the information gain for one-step ahead, have been employed in practice thanks to their computational convenience, flexibility to context or task changes, and broad applicability.","However, statistical analysis is restricted to one-dimensional cases due to the problem's combinatorial nature and the seemingly limited capacity of greedy algorithms, leaving the multidimensional problem open.   ","In this study, we close the gap for multidimensional problems.","In particular, we propose adopting a class of greedy experiment selection methods and provide statistical analysis for the maximum likelihood estimator following these selection rules.","This class encompasses both existing methods and introduces new methods with improved numerical efficiency.","We prove that these methods produce consistent and asymptotically normal estimators.","Additionally, within a decision theory framework, we establish that the proposed methods achieve asymptotic optimality when the risk measure aligns with the selection rule.","We also conduct extensive numerical studies on both simulated and real data to illustrate the efficacy of the proposed methods.   ","From a technical perspective, we devise new analytical tools to address theoretical challenges.","These analytical tools are of independent theoretical interest and may be reused in related problems involving stochastic approximation and sequential designs."],"url":"http://arxiv.org/abs/2402.08602v1","category":"math.ST"}
{"created":"2024-02-13 16:52:10","title":"Convolutional Neural Networks Towards Facial Skin Lesions Detection","abstract":"Facial analysis has emerged as a prominent area of research with diverse applications, including cosmetic surgery programs, the beauty industry, photography, and entertainment. Manipulating patient images often necessitates professional image processing software. This study contributes by providing a model that facilitates the detection of blemishes and skin lesions on facial images through a convolutional neural network and machine learning approach. The proposed method offers advantages such as simple architecture, speed and suitability for image processing while avoiding the complexities associated with traditional methods. The model comprises four main steps: area selection, scanning the chosen region, lesion diagnosis, and marking the identified lesion. Raw data for this research were collected from a reputable clinic in Tehran specializing in skincare and beauty services. The dataset includes administrative information, clinical data, and facial and profile images. A total of 2300 patient images were extracted from this raw data. A software tool was developed to crop and label lesions, with input from two treatment experts. In the lesion preparation phase, the selected area was standardized to 50 * 50 pixels. Subsequently, a convolutional neural network model was employed for lesion labeling. The classification model demonstrated high accuracy, with a measure of 0.98 for healthy skin and 0.97 for lesioned skin specificity. Internal validation involved performance indicators and cross-validation, while external validation compared the model's performance indicators with those of the transfer learning method using the Vgg16 deep network model. Compared to existing studies, the results of this research showcase the efficacy and desirability of the proposed model and methodology.","sentences":["Facial analysis has emerged as a prominent area of research with diverse applications, including cosmetic surgery programs, the beauty industry, photography, and entertainment.","Manipulating patient images often necessitates professional image processing software.","This study contributes by providing a model that facilitates the detection of blemishes and skin lesions on facial images through a convolutional neural network and machine learning approach.","The proposed method offers advantages such as simple architecture, speed and suitability for image processing while avoiding the complexities associated with traditional methods.","The model comprises four main steps: area selection, scanning the chosen region, lesion diagnosis, and marking the identified lesion.","Raw data for this research were collected from a reputable clinic in Tehran specializing in skincare and beauty services.","The dataset includes administrative information, clinical data, and facial and profile images.","A total of 2300 patient images were extracted from this raw data.","A software tool was developed to crop and label lesions, with input from two treatment experts.","In the lesion preparation phase, the selected area was standardized to 50 * 50 pixels.","Subsequently, a convolutional neural network model was employed for lesion labeling.","The classification model demonstrated high accuracy, with a measure of 0.98 for healthy skin and 0.97 for lesioned skin specificity.","Internal validation involved performance indicators and cross-validation, while external validation compared the model's performance indicators with those of the transfer learning method using the Vgg16 deep network model.","Compared to existing studies, the results of this research showcase the efficacy and desirability of the proposed model and methodology."],"url":"http://arxiv.org/abs/2402.08592v1","category":"eess.IV"}
{"created":"2024-02-13 15:59:19","title":"Exploring diversity perceptions in a community through a Q&A chatbot","abstract":"While diversity has become a debated issue in design, very little research exists on positive use-cases for diversity beyond scholarly criticism. The current work addresses this gap through the case of a diversity-aware chatbot, exploring what benefits a diversity-aware chatbot could bring to people and how do people interpret diversity when being presented with it. In this paper, we motivate a Q&A chatbot as a technology probe and deploy it in two student communities within a study. During the study, we collected contextual data on people's expectations and perceptions when presented with diversity during the study. Our key findings show that people seek out others with shared niche interests, or their search is driven by exploration and inspiration when presented with diversity. Although interacting with chatbots is limited, participants found the engagement novel and interesting to motivate future research.","sentences":["While diversity has become a debated issue in design, very little research exists on positive use-cases for diversity beyond scholarly criticism.","The current work addresses this gap through the case of a diversity-aware chatbot, exploring what benefits a diversity-aware chatbot could bring to people and how do people interpret diversity when being presented with it.","In this paper, we motivate a Q&A chatbot as a technology probe and deploy it in two student communities within a study.","During the study, we collected contextual data on people's expectations and perceptions when presented with diversity during the study.","Our key findings show that people seek out others with shared niche interests, or their search is driven by exploration and inspiration when presented with diversity.","Although interacting with chatbots is limited, participants found the engagement novel and interesting to motivate future research."],"url":"http://arxiv.org/abs/2402.08558v1","category":"cs.HC"}
{"created":"2024-02-13 15:40:38","title":"Multiplicity dependence of the freezeout parameters in high energy hadron-hadron collisions","abstract":"We examined the transverse momentum spectra of various identified particles, across different multiplicity classes in proton-proton collisions at a center-of-mass energy of $\\sqrt{s}$ = 7 TeV. Utilizing the Tsallis and Hagedorn models, parameters relevant to the bulk properties of nuclear matter were extracted. Both models exhibit good agreement with experimental data. In our analyses, we observed a consistent decrease in the effective temperature for the Tsallis model and the kinetic or thermal freeze-out temperature for the Hagedorn model, as we transition from higher multiplicity (class-I) to lower multiplicity (class-X). Additionally, the transverse flow velocity experiences a decline from class-I to class-X. The normalization constant which represents the multiplicity of produced particles is observed to decrease as we move towards higher multiplicity classes. While the effective and kinetic freeze-out temperatures, as well as the transverse flow velocity, show a mild dependency on multiplicity for lighter particles, this relationship becomes more pronounced for heavier particles. Various particle species are observed to undergo decoupling from the fireball at distinct temperatures: lighter particles exhibit lower temperatures, while heavier ones show higher temperatures, thereby supporting the concept of multiple freeze-out scenarios. Moreover, we identified a positive correlation between the kinetic freeze-out temperature and transverse flow velocity, a scenario where particles experience stronger collective motion at higher freeze-out temperature. The reason for this positive correlation is that as the multiplicity increases, more energy is transferred into the system. This heightened energy causes greater excitation and pressure within the system, leading to a quick expansion.","sentences":["We examined the transverse momentum spectra of various identified particles, across different multiplicity classes in proton-proton collisions at a center-of-mass energy of $\\sqrt{s}$ = 7 TeV. Utilizing the Tsallis and Hagedorn models, parameters relevant to the bulk properties of nuclear matter were extracted.","Both models exhibit good agreement with experimental data.","In our analyses, we observed a consistent decrease in the effective temperature for the Tsallis model and the kinetic or thermal freeze-out temperature for the Hagedorn model, as we transition from higher multiplicity (class-I) to lower multiplicity (class-X).","Additionally, the transverse flow velocity experiences a decline from class-I to class-X.","The normalization constant which represents the multiplicity of produced particles is observed to decrease as we move towards higher multiplicity classes.","While the effective and kinetic freeze-out temperatures, as well as the transverse flow velocity, show a mild dependency on multiplicity for lighter particles, this relationship becomes more pronounced for heavier particles.","Various particle species are observed to undergo decoupling from the fireball at distinct temperatures: lighter particles exhibit lower temperatures, while heavier ones show higher temperatures, thereby supporting the concept of multiple freeze-out scenarios.","Moreover, we identified a positive correlation between the kinetic freeze-out temperature and transverse flow velocity, a scenario where particles experience stronger collective motion at higher freeze-out temperature.","The reason for this positive correlation is that as the multiplicity increases, more energy is transferred into the system.","This heightened energy causes greater excitation and pressure within the system, leading to a quick expansion."],"url":"http://arxiv.org/abs/2402.08535v1","category":"hep-ph"}
{"created":"2024-02-13 13:39:42","title":"The SRG/eROSITA All-Sky Survey: X-ray selection function models for the eRASS1 galaxy cluster cosmology","abstract":"Characterising galaxy cluster populations from catalog of sources selected in astronomical surveys requires knowledge of sample incompleteness, known as selection function. The first All-Sky Survey (eRASS1) by eROSITA onboard Spectrum Roentgen Gamma (SRG) has enabled the collection of large samples of galaxy clusters detected in the soft X-ray band over the Western Galactic hemisphere. The driving goal consists in constraining cosmological parameters, which puts stringent requirements on accuracy, flexibility and explainability of the selection function models. We use a large set of mock observations of the eRASS1 survey and we process simulated data identically to the real eRASS1 events. We match detected sources to simulated clusters and we associate detections to intrinsic cluster properties. We train a series of models to build selection functions depending only on observable surface brightness data. We develop a second series of models relying on global cluster characteristics such as X-ray luminosity, flux, and expected instrumental count-rate as well as on morphological properties. We validate our models using our simulations and we rank them according to selected performance metrics. We validate the models with datasets of clusters detected in X-rays and via the Sunyaev-Zeldovich effect. We present the complete Bayesian population modelling framework developed for this purpose. Our results reveal the surface brightness characteristics most relevant to cluster selection in the eRASS1 sample, in particular the ambiguous role of central surface brightness at the scale of the instrument resolution. We have produced a series of user-friendly selection function models and demonstrated their validity and their limitations. Our selection function for bright sources reproduces well the catalog matches with external datasets. (abridged)","sentences":["Characterising galaxy cluster populations from catalog of sources selected in astronomical surveys requires knowledge of sample incompleteness, known as selection function.","The first All-Sky Survey (eRASS1) by eROSITA onboard Spectrum Roentgen Gamma (SRG) has enabled the collection of large samples of galaxy clusters detected in the soft X-ray band over the Western Galactic hemisphere.","The driving goal consists in constraining cosmological parameters, which puts stringent requirements on accuracy, flexibility and explainability of the selection function models.","We use a large set of mock observations of the eRASS1 survey and we process simulated data identically to the real eRASS1 events.","We match detected sources to simulated clusters and we associate detections to intrinsic cluster properties.","We train a series of models to build selection functions depending only on observable surface brightness data.","We develop a second series of models relying on global cluster characteristics such as X-ray luminosity, flux, and expected instrumental count-rate as well as on morphological properties.","We validate our models using our simulations and we rank them according to selected performance metrics.","We validate the models with datasets of clusters detected in X-rays and via the Sunyaev-Zeldovich effect.","We present the complete Bayesian population modelling framework developed for this purpose.","Our results reveal the surface brightness characteristics most relevant to cluster selection in the eRASS1 sample, in particular the ambiguous role of central surface brightness at the scale of the instrument resolution.","We have produced a series of user-friendly selection function models and demonstrated their validity and their limitations.","Our selection function for bright sources reproduces well the catalog matches with external datasets.","(abridged)"],"url":"http://arxiv.org/abs/2402.08457v1","category":"astro-ph.CO"}
{"created":"2024-02-13 13:39:38","title":"The First SRG/eROSITA All-Sky Survey: Optical Identification and Properties of Galaxy Clusters and Groups in the Western Galactic Hemisphere","abstract":"The first SRG/eROSITA All-Sky Survey (eRASS1) provides the largest intracluster medium-selected galaxy cluster and group catalog covering the western galactic hemisphere. Compared to samples selected purely on X-ray extent, the sample purity can be enhanced by identifying cluster candidates using optical and near-infrared data from the DESI Legacy Imaging Surveys. Using the red-sequence-based cluster finder eROMaPPer, we measure individual photometric properties (redshift $z_\\lambda$, richness $\\lambda$, optical center, and BCG position) for 12,000 eRASS1 clusters over a sky area of 13,116 deg$^2$, augmented by 247 cases identified by matching the candidates with known clusters from the literature. The median redshift of the identified eRASS1 sample is $z=0.31$, with 10% of the clusters at $z>0.72$. The photometric redshifts have an accuracy of $\\delta z/(1+z)<0.005$ for $0.05<z<0.9$. Spectroscopic cluster properties (redshift $z_{\\rm spec}$ and velocity dispersion $\\sigma$) are measured a posteriori for a subsample of 3,210 and 1,499 eRASS1 clusters, respectively, using an extensive compilation of spectroscopic redshifts of galaxies from the literature. We infer that the primary eRASS1 sample has a purity of 86% and optical completeness >95% for $z>0.05$. For these and further quality assessments of the eRASS1 identified catalog, we apply our identification method to a collection of galaxy cluster catalogs in the literature, as well as blindly on the full Legacy Surveys covering 24,069 deg$^2$. Using a combination of these cluster samples, we investigate the velocity dispersion-richness relation, finding $\\log(\\lambda)=2.401\\times\\log(\\sigma)-5.074$ with an intrinsic scatter of $0.10\\pm0.01$ dex. Our main result is the identified eRASS1 cluster catalog with a high purity and a well-defined X-ray selection process, enabling precise cosmological analyses presented in companion papers.","sentences":["The first SRG/eROSITA All-Sky Survey (eRASS1) provides the largest intracluster medium-selected galaxy cluster and group catalog covering the western galactic hemisphere.","Compared to samples selected purely on X-ray extent, the sample purity can be enhanced by identifying cluster candidates using optical and near-infrared data from the DESI Legacy Imaging Surveys.","Using the red-sequence-based cluster finder eROMaPPer, we measure individual photometric properties (redshift $z_\\lambda$, richness $\\lambda$, optical center, and BCG position) for 12,000 eRASS1 clusters over a sky area of 13,116 deg$^2$, augmented by 247 cases identified by matching the candidates with known clusters from the literature.","The median redshift of the identified eRASS1 sample is $z=0.31$, with 10% of the clusters at $z>0.72$. The photometric redshifts have an accuracy of $\\delta z/(1+z)<0.005$ for $0.05<z<0.9$. Spectroscopic cluster properties (redshift $z_{\\rm spec}$ and velocity dispersion $\\sigma$) are measured a posteriori for a subsample of 3,210 and 1,499 eRASS1 clusters, respectively, using an extensive compilation of spectroscopic redshifts of galaxies from the literature.","We infer that the primary eRASS1 sample has a purity of 86% and optical completeness >95% for $z>0.05$. For these and further quality assessments of the eRASS1 identified catalog, we apply our identification method to a collection of galaxy cluster catalogs in the literature, as well as blindly on the full Legacy Surveys covering 24,069 deg$^2$. Using a combination of these cluster samples, we investigate the velocity dispersion-richness relation, finding $\\log(\\lambda)=2.401\\times\\log(\\sigma)-5.074$ with an intrinsic scatter of $0.10\\pm0.01$ dex.","Our main result is the identified eRASS1 cluster catalog with a high purity and a well-defined X-ray selection process, enabling precise cosmological analyses presented in companion papers."],"url":"http://arxiv.org/abs/2402.08453v1","category":"astro-ph.CO"}
{"created":"2024-02-13 13:36:19","title":"Unveiling interatomic distances influencing the reaction coordinates in alanine dipeptide isomerization: An explainable deep learning approach","abstract":"The present work shows that the free energy landscape associated with alanine dipeptide isomerization can be effectively represented by specific interatomic distances without explicit reference to dihedral angles. Conventionally, two stable states of alanine dipeptide in vacuum, i.e., C$7_{\\mathrm{eq}}$ ($\\beta$-sheet structure) and C$7_{\\mathrm{ax}}$ (left handed $\\alpha$-helix structure), have been primarily characterized using the main chain dihedral angles, $\\varphi$ (C-N-C$_\\alpha$-C) and $\\psi$ (N-C$_\\alpha$-C-N). However, our recent deep learning combined with \"Explainable AI\" (XAI) framework has shown that the transition state can be adequately captured by a free energy landscape using $\\varphi$ and $\\theta$ (O-C-N-C$_\\alpha$) [T. Kikutsuji, et al. J. Chem. Phys. 156, 154108 (2022)]. In perspective of extending these insights to other collective variables, a more detailed characterization of transition state is required. In this work, we employ the interatomic distances and bond angles as input variables for deep learning, rather than the conventional and more elaborate dihedral angles. Our approach utilizes deep learning to investigate whether changes in the main chain dihedral angle can be expressed in terms of interatomic distances and bond angles. Furthermore, by incorporating XAI into our predictive analysis, we quantified the importance of each input variable and succeeded in clarifying the specific interatomic distance that affects the transition state. The results indicate that constructing a free energy landscape based on using the identified interatomic distance can clearly distinguish between the two stable states and provide a comprehensive explanation for the energy barrier crossing.","sentences":["The present work shows that the free energy landscape associated with alanine dipeptide isomerization can be effectively represented by specific interatomic distances without explicit reference to dihedral angles.","Conventionally, two stable states of alanine dipeptide in vacuum, i.e., C$7_{\\mathrm{eq}}$ ($\\beta$-sheet structure) and C$7_{\\mathrm{ax}}$ (left handed $\\alpha$-helix structure), have been primarily characterized using the main chain dihedral angles, $\\varphi$ (C-N-C$_\\alpha$-C) and $\\psi$ (N-C$_\\alpha$-C-N).","However, our recent deep learning combined with \"Explainable AI\" (XAI) framework has shown that the transition state can be adequately captured by a free energy landscape using $\\varphi$ and $\\theta$ (O-C-N-C$_\\alpha$)","[T. Kikutsuji, et al. J. Chem.","Phys. 156, 154108 (2022)].","In perspective of extending these insights to other collective variables, a more detailed characterization of transition state is required.","In this work, we employ the interatomic distances and bond angles as input variables for deep learning, rather than the conventional and more elaborate dihedral angles.","Our approach utilizes deep learning to investigate whether changes in the main chain dihedral angle can be expressed in terms of interatomic distances and bond angles.","Furthermore, by incorporating XAI into our predictive analysis, we quantified the importance of each input variable and succeeded in clarifying the specific interatomic distance that affects the transition state.","The results indicate that constructing a free energy landscape based on using the identified interatomic distance can clearly distinguish between the two stable states and provide a comprehensive explanation for the energy barrier crossing."],"url":"http://arxiv.org/abs/2402.08448v1","category":"physics.chem-ph"}
{"created":"2024-02-13 12:49:13","title":"Vision-Based Hand Gesture Customization from a Single Demonstration","abstract":"Hand gesture recognition is becoming a more prevalent mode of human-computer interaction, especially as cameras proliferate across everyday devices. Despite continued progress in this field, gesture customization is often underexplored. Customization is crucial since it enables users to define and demonstrate gestures that are more natural, memorable, and accessible. However, customization requires efficient usage of user-provided data. We introduce a method that enables users to easily design bespoke gestures with a monocular camera from one demonstration. We employ transformers and meta-learning techniques to address few-shot learning challenges. Unlike prior work, our method supports any combination of one-handed, two-handed, static, and dynamic gestures, including different viewpoints. We evaluated our customization method through a user study with 20 gestures collected from 21 participants, achieving up to 97% average recognition accuracy from one demonstration. Our work provides a viable path for vision-based gesture customization, laying the foundation for future advancements in this domain.","sentences":["Hand gesture recognition is becoming a more prevalent mode of human-computer interaction, especially as cameras proliferate across everyday devices.","Despite continued progress in this field, gesture customization is often underexplored.","Customization is crucial since it enables users to define and demonstrate gestures that are more natural, memorable, and accessible.","However, customization requires efficient usage of user-provided data.","We introduce a method that enables users to easily design bespoke gestures with a monocular camera from one demonstration.","We employ transformers and meta-learning techniques to address few-shot learning challenges.","Unlike prior work, our method supports any combination of one-handed, two-handed, static, and dynamic gestures, including different viewpoints.","We evaluated our customization method through a user study with 20 gestures collected from 21 participants, achieving up to 97% average recognition accuracy from one demonstration.","Our work provides a viable path for vision-based gesture customization, laying the foundation for future advancements in this domain."],"url":"http://arxiv.org/abs/2402.08420v1","category":"cs.HC"}
{"created":"2024-02-13 10:07:41","title":"Interleaved snowballing: Reducing the workload of literature curators","abstract":"We formally define the literature (reference) snowballing method and present a refined version of it. We show that the improved algorithm can substantially reduce curator work, even before application of text classification, by reducing the number of candidates to classify. We also present a desktop application named LitBall that implements this and other literature collection methods, through access to the Semantic Scholar academic graph (S2AG).","sentences":["We formally define the literature (reference) snowballing method and present a refined version of it.","We show that the improved algorithm can substantially reduce curator work, even before application of text classification, by reducing the number of candidates to classify.","We also present a desktop application named LitBall that implements this and other literature collection methods, through access to the Semantic Scholar academic graph (S2AG)."],"url":"http://arxiv.org/abs/2402.08339v1","category":"cs.DL"}
{"created":"2024-02-13 10:03:25","title":"Joint Modeling of Multivariate Longitudinal and Survival Outcomes with the R package INLAjoint","abstract":"This paper introduces the R package INLAjoint, designed as a toolbox for fitting a diverse range of regression models addressing both longitudinal and survival outcomes. INLAjoint relies on the computational efficiency of the integrated nested Laplace approximations methodology, an efficient alternative to Markov chain Monte Carlo for Bayesian inference, ensuring both speed and accuracy in parameter estimation and uncertainty quantification. The package facilitates the construction of complex joint models by treating individual regression models as building blocks, which can be assembled to address specific research questions. Joint models are relevant in biomedical studies where the collection of longitudinal markers alongside censored survival times is common. They have gained significant interest in recent literature, demonstrating the ability to rectify biases present in separate modeling approaches such as informative censoring by a survival event or confusion bias due to population heterogeneity. We provide a comprehensive overview of the joint modeling framework embedded in INLAjoint with illustrative examples. Through these examples, we demonstrate the practical utility of INLAjoint in handling complex data scenarios encountered in biomedical research.","sentences":["This paper introduces the R package INLAjoint, designed as a toolbox for fitting a diverse range of regression models addressing both longitudinal and survival outcomes.","INLAjoint relies on the computational efficiency of the integrated nested Laplace approximations methodology, an efficient alternative to Markov chain Monte Carlo for Bayesian inference, ensuring both speed and accuracy in parameter estimation and uncertainty quantification.","The package facilitates the construction of complex joint models by treating individual regression models as building blocks, which can be assembled to address specific research questions.","Joint models are relevant in biomedical studies where the collection of longitudinal markers alongside censored survival times is common.","They have gained significant interest in recent literature, demonstrating the ability to rectify biases present in separate modeling approaches such as informative censoring by a survival event or confusion bias due to population heterogeneity.","We provide a comprehensive overview of the joint modeling framework embedded in INLAjoint with illustrative examples.","Through these examples, we demonstrate the practical utility of INLAjoint in handling complex data scenarios encountered in biomedical research."],"url":"http://arxiv.org/abs/2402.08335v1","category":"stat.ME"}
{"created":"2024-02-13 07:57:07","title":"Regional Adaptive Metropolis Light Transport","abstract":"The design of the proposal distributions, and most notably the kernel parameters, are crucial for the performance of Markov chain Monte Carlo (MCMC) rendering. A poor selection of parameters can increase the correlation of the Markov chain and result in bad rendering performance. We approach this problem by a novel path perturbation strategy for online-learning of state-dependent kernel parameters. We base our approach on the theoretical framework of regional adaptive MCMC which enables the adaptation of parameters depending on the region of the state space which contains the current sample, and on information collected from previous samples. For this, we define a partitioning of the path space on a low-dimensional canonical space to capture the characteristics of paths, with a focus on path segments closer to the sensor. Fast convergence is achieved by adaptive refinement of the partitions. Exemplarily, we present two novel regional adaptive path perturbation techniques akin to lens and multi-chain perturbations. Our approach can easily be used on top of existing path space MLT methods to improve rendering efficiency, while being agnostic to the initial choice of kernel parameters.","sentences":["The design of the proposal distributions, and most notably the kernel parameters, are crucial for the performance of Markov chain Monte Carlo (MCMC) rendering.","A poor selection of parameters can increase the correlation of the Markov chain and result in bad rendering performance.","We approach this problem by a novel path perturbation strategy for online-learning of state-dependent kernel parameters.","We base our approach on the theoretical framework of regional adaptive MCMC which enables the adaptation of parameters depending on the region of the state space which contains the current sample, and on information collected from previous samples.","For this, we define a partitioning of the path space on a low-dimensional canonical space to capture the characteristics of paths, with a focus on path segments closer to the sensor.","Fast convergence is achieved by adaptive refinement of the partitions.","Exemplarily, we present two novel regional adaptive path perturbation techniques akin to lens and multi-chain perturbations.","Our approach can easily be used on top of existing path space MLT methods to improve rendering efficiency, while being agnostic to the initial choice of kernel parameters."],"url":"http://arxiv.org/abs/2402.08273v1","category":"cs.GR"}
{"created":"2024-02-13 06:40:55","title":"Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles","abstract":"This work presents a neural network model capable of recognizing small and tiny objects in thermal images collected by unmanned aerial vehicles. Our model consists of three parts, the backbone, the neck, and the prediction head. The backbone is developed based on the structure of YOLOv5 combined with the use of a transformer encoder at the end. The neck includes a BI-FPN block combined with the use of a sliding window and a transformer to increase the information fed into the prediction head. The prediction head carries out the detection by evaluating feature maps with the Sigmoid function. The use of transformers with attention and sliding windows increases recognition accuracy while keeping the model at a reasonable number of parameters and computation requirements for embedded systems. Experiments conducted on public dataset VEDAI and our collected datasets show that our model has a higher accuracy than state-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5, SMPNet, and DPNetV3. Experiments on the embedded computer Jetson AGX show that our model achieves a real-time computation speed with a stability rate of over 90%.","sentences":["This work presents a neural network model capable of recognizing small and tiny objects in thermal images collected by unmanned aerial vehicles.","Our model consists of three parts, the backbone, the neck, and the prediction head.","The backbone is developed based on the structure of YOLOv5 combined with the use of a transformer encoder at the end.","The neck includes a BI-FPN block combined with the use of a sliding window and a transformer to increase the information fed into the prediction head.","The prediction head carries out the detection by evaluating feature maps with the Sigmoid function.","The use of transformers with attention and sliding windows increases recognition accuracy while keeping the model at a reasonable number of parameters and computation requirements for embedded systems.","Experiments conducted on public dataset VEDAI and our collected datasets show that our model has a higher accuracy than state-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5, SMPNet, and DPNetV3.","Experiments on the embedded computer Jetson AGX show that our model achieves a real-time computation speed with a stability rate of over 90%."],"url":"http://arxiv.org/abs/2402.08251v1","category":"cs.CV"}
{"created":"2024-02-13 05:45:45","title":"Adaptive Modulus RF Beamforming for Enhanced Self-Interference Suppression in Full-Duplex Massive MIMO Systems","abstract":"This study employs a uniform rectangular array (URA) sub-connected hybrid beamforming (SC-HBF) architecture to provide a novel self-interference (SI) suppression scheme in a full-duplex (FD) massive multiple-input multiple-output (mMIMO) system. Our primary objective is to mitigate the strong SI through the design of RF beamforming stages for uplink and downlink transmissions that utilize the spatial degrees of freedom provided due to the use of large array structures. We propose a non-constant modulus RF beamforming (NCM-BF-SIS) scheme that incorporates the gain controllers for both transmit (Tx) and receive (Rx) RF beamforming stages and optimizes the uplink and downlink beam directions jointly with gain controller coefficients. To solve this challenging non-convex optimization problem, we propose a swarm intelligence-based algorithmic solution that finds the optimal beam perturbations while also adjusting the Tx/Rx gain controllers to alleviate SI subject to the directivity degradation constraints for the beams. The data-driven analysis based on the measured SI channel in an anechoic chamber shows that the proposed NCM-BF-SIS scheme can suppress SI by around 80 dB in FD mMIMO systems.","sentences":["This study employs a uniform rectangular array (URA) sub-connected hybrid beamforming (SC-HBF) architecture to provide a novel self-interference (SI) suppression scheme in a full-duplex (FD) massive multiple-input multiple-output (mMIMO) system.","Our primary objective is to mitigate the strong SI through the design of RF beamforming stages for uplink and downlink transmissions that utilize the spatial degrees of freedom provided due to the use of large array structures.","We propose a non-constant modulus RF beamforming (NCM-BF-SIS) scheme that incorporates the gain controllers for both transmit (Tx) and receive (Rx) RF beamforming stages and optimizes the uplink and downlink beam directions jointly with gain controller coefficients.","To solve this challenging non-convex optimization problem, we propose a swarm intelligence-based algorithmic solution that finds the optimal beam perturbations while also adjusting the Tx/Rx gain controllers to alleviate SI subject to the directivity degradation constraints for the beams.","The data-driven analysis based on the measured SI channel in an anechoic chamber shows that the proposed NCM-BF-SIS scheme can suppress SI by around 80 dB in FD mMIMO systems."],"url":"http://arxiv.org/abs/2402.08230v1","category":"cs.IT"}
{"created":"2024-02-13 05:38:45","title":"Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective","abstract":"Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \\textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \\textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism and the decoupled architecture contribute positively to graph OOD generalization. In contrast, we observe that the linear classification layer tends to compromise graph OOD generalization capability. Furthermore, we provide in-depth theoretical insights and discussions to underpin these discoveries. These insights have empowered us to develop a novel GNN backbone model, DGAT, designed to harness the robust properties of both graph self-attention mechanism and the decoupled architecture. Extensive experimental results demonstrate the effectiveness of our model under graph OOD, exhibiting substantial and consistent enhancements across various training strategies.","sentences":["Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data.","However, in real-world scenarios, this assumption may not always be valid.","Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs.","Most existing efforts have primarily concentrated on improving graph OOD generalization from two \\textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning.","However, there has been limited attention dedicated to investigating the impact of well-known \\textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research.","In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs.","Through extensive experiments, we reveal that both the graph self-attention mechanism and the decoupled architecture contribute positively to graph OOD generalization.","In contrast, we observe that the linear classification layer tends to compromise graph OOD generalization capability.","Furthermore, we provide in-depth theoretical insights and discussions to underpin these discoveries.","These insights have empowered us to develop a novel GNN backbone model, DGAT, designed to harness the robust properties of both graph self-attention mechanism and the decoupled architecture.","Extensive experimental results demonstrate the effectiveness of our model under graph OOD, exhibiting substantial and consistent enhancements across various training strategies."],"url":"http://arxiv.org/abs/2402.08228v1","category":"cs.LG"}
{"created":"2024-02-13 05:30:35","title":"Two-Dimensional Direction-of-Arrival Estimation Using Stacked Intelligent Metasurfaces","abstract":"Stacked intelligent metasurfaces (SIM) are capable of emulating reconfigurable physical neural networks by relying on electromagnetic (EM) waves as carriers. They can also perform various complex computational and signal processing tasks. A SIM is fabricated by densely integrating multiple metasurface layers, each consisting of a large number of small meta-atoms that can control the EM waves passing through it. In this paper, we harness a SIM for two-dimensional (2D) direction-of-arrival (DOA) estimation. In contrast to the conventional designs, an advanced SIM in front of the receiver array automatically carries out the 2D discrete Fourier transform (DFT) as the incident waves propagate through it. As a result, the receiver array directly observes the angular spectrum of the incoming signal. In this context, the DOA estimates can be readily obtained by using probes to detect the energy distribution on the receiver array. This avoids the need for power-thirsty radio frequency (RF) chains. To enable SIM to perform the 2D DFT, we formulate the optimization problem of minimizing the fitting error between the SIM's EM response and the 2D DFT matrix. Furthermore, a gradient descent algorithm is customized for iteratively updating the phase shift of each meta-atom in SIM. To further improve the DOA estimation accuracy, we configure the phase shift pattern in the zeroth layer of the SIM to generate a set of 2D DFT matrices associated with orthogonal spatial frequency bins. Additionally, we analytically evaluate the performance of the proposed SIM-based DOA estimator by deriving a tight upper bound for the mean square error (MSE). Our numerical simulations verify the capability of a well-trained SIM to perform DOA estimation and corroborate our theoretical analysis. It is demonstrated that a SIM having an optical computational speed achieves an MSE of $10^{-4}$ for DOA estimation.","sentences":["Stacked intelligent metasurfaces (SIM) are capable of emulating reconfigurable physical neural networks by relying on electromagnetic (EM) waves as carriers.","They can also perform various complex computational and signal processing tasks.","A SIM is fabricated by densely integrating multiple metasurface layers, each consisting of a large number of small meta-atoms that can control the EM waves passing through it.","In this paper, we harness a SIM for two-dimensional (2D) direction-of-arrival (DOA) estimation.","In contrast to the conventional designs, an advanced SIM in front of the receiver array automatically carries out the 2D discrete Fourier transform (DFT) as the incident waves propagate through it.","As a result, the receiver array directly observes the angular spectrum of the incoming signal.","In this context, the DOA estimates can be readily obtained by using probes to detect the energy distribution on the receiver array.","This avoids the need for power-thirsty radio frequency (RF) chains.","To enable SIM to perform the 2D DFT, we formulate the optimization problem of minimizing the fitting error between the SIM's EM response and the 2D DFT matrix.","Furthermore, a gradient descent algorithm is customized for iteratively updating the phase shift of each meta-atom in SIM.","To further improve the DOA estimation accuracy, we configure the phase shift pattern in the zeroth layer of the SIM to generate a set of 2D DFT matrices associated with orthogonal spatial frequency bins.","Additionally, we analytically evaluate the performance of the proposed SIM-based DOA estimator by deriving a tight upper bound for the mean square error (MSE).","Our numerical simulations verify the capability of a well-trained SIM to perform DOA estimation and corroborate our theoretical analysis.","It is demonstrated that a SIM having an optical computational speed achieves an MSE of $10^{-4}$ for DOA estimation."],"url":"http://arxiv.org/abs/2402.08224v1","category":"cs.IT"}
{"created":"2024-02-13 05:15:46","title":"BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models","abstract":"Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively.","sentences":["Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging.","Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable.","Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost.","To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs.","BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative.","It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain.","Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations.","Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency.","It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively."],"url":"http://arxiv.org/abs/2402.08219v1","category":"cs.CL"}
{"created":"2024-02-13 04:28:43","title":"Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks","abstract":"Models based on the Transformer neural network architecture have seen success on a wide variety of tasks that appear to require complex \"cognitive branching\" -- or the ability to maintain pursuit of one goal while accomplishing others. In cognitive neuroscience, success on such tasks is thought to rely on sophisticated frontostriatal mechanisms for selective \\textit{gating}, which enable role-addressable updating -- and later readout -- of information to and from distinct \"addresses\" of memory, in the form of clusters of neurons. However, Transformer models have no such mechanisms intentionally built-in. It is thus an open question how Transformers solve such tasks, and whether the mechanisms that emerge to help them to do so bear any resemblance to the gating mechanisms in the human brain. In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer trained on a simple sequence modeling task inspired by a task explicitly designed to study working memory gating in computational cognitive neuroscience. We find that, as a result of training, the self-attention mechanism within the Transformer specializes in a way that mirrors the input and output gating mechanisms which were explicitly incorporated into earlier, more biologically-inspired architectures. These results suggest opportunities for future research on computational similarities between modern AI architectures and models of the human brain.","sentences":["Models based on the Transformer neural network architecture have seen success on a wide variety of tasks that appear to require complex \"cognitive branching\" -- or the ability to maintain pursuit of one goal while accomplishing others.","In cognitive neuroscience, success on such tasks is thought to rely on sophisticated frontostriatal mechanisms for selective \\textit{gating}, which enable role-addressable updating -- and later readout -- of information to and from distinct \"addresses\" of memory, in the form of clusters of neurons.","However, Transformer models have no such mechanisms intentionally built-in.","It is thus an open question how Transformers solve such tasks, and whether the mechanisms that emerge to help them to do so bear any resemblance to the gating mechanisms in the human brain.","In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer trained on a simple sequence modeling task inspired by a task explicitly designed to study working memory gating in computational cognitive neuroscience.","We find that, as a result of training, the self-attention mechanism within the Transformer specializes in a way that mirrors the input and output gating mechanisms which were explicitly incorporated into earlier, more biologically-inspired architectures.","These results suggest opportunities for future research on computational similarities between modern AI architectures and models of the human brain."],"url":"http://arxiv.org/abs/2402.08211v1","category":"cs.AI"}
{"created":"2024-02-13 04:17:48","title":"Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits","abstract":"Data cleansing aims to improve model performance by removing a set of harmful instances from the training dataset. Data Shapley is a common theoretically guaranteed method to evaluate the contribution of each instance to model performance; however, it requires training on all subsets of the training data, which is computationally expensive. In this paper, we propose an iterativemethod to fast identify a subset of instances with low data Shapley values by using the thresholding bandit algorithm. We provide a theoretical guarantee that the proposed method can accurately select harmful instances if a sufficiently large number of iterations is conducted. Empirical evaluation using various models and datasets demonstrated that the proposed method efficiently improved the computational speed while maintaining the model performance.","sentences":["Data cleansing aims to improve model performance by removing a set of harmful instances from the training dataset.","Data Shapley is a common theoretically guaranteed method to evaluate the contribution of each instance to model performance; however, it requires training on all subsets of the training data, which is computationally expensive.","In this paper, we propose an iterativemethod to fast identify a subset of instances with low data Shapley values by using the thresholding bandit algorithm.","We provide a theoretical guarantee that the proposed method can accurately select harmful instances if a sufficiently large number of iterations is conducted.","Empirical evaluation using various models and datasets demonstrated that the proposed method efficiently improved the computational speed while maintaining the model performance."],"url":"http://arxiv.org/abs/2402.08209v1","category":"cs.LG"}
{"created":"2024-02-13 04:15:26","title":"Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications","abstract":"This paper explores the role and challenges of Artificial Intelligence (AI) algorithms, specifically AI-based software elements, in autonomous driving systems. These AI systems are fundamental in executing real-time critical functions in complex and high-dimensional environments. They handle vital tasks like multi-modal perception, cognition, and decision-making tasks such as motion planning, lane keeping, and emergency braking. A primary concern relates to the ability (and necessity) of AI models to generalize beyond their initial training data. This generalization issue becomes evident in real-time scenarios, where models frequently encounter inputs not represented in their training or validation data. In such cases, AI systems must still function effectively despite facing distributional or domain shifts. This paper investigates the risk associated with overconfident AI models in safety-critical applications like autonomous driving. To mitigate these risks, methods for training AI models that help maintain performance without overconfidence are proposed. This involves implementing certainty reporting architectures and ensuring diverse training data. While various distribution-based methods exist to provide safety mechanisms for AI models, there is a noted lack of systematic assessment of these methods, especially in the context of safety-critical automotive applications. Many methods in the literature do not adapt well to the quick response times required in safety-critical edge applications. This paper reviews these methods, discusses their suitability for safety-critical applications, and highlights their strengths and limitations. The paper also proposes potential improvements to enhance the safety and reliability of AI algorithms in autonomous vehicles in the context of rapid and accurate decision-making processes.","sentences":["This paper explores the role and challenges of Artificial Intelligence (AI) algorithms, specifically AI-based software elements, in autonomous driving systems.","These AI systems are fundamental in executing real-time critical functions in complex and high-dimensional environments.","They handle vital tasks like multi-modal perception, cognition, and decision-making tasks such as motion planning, lane keeping, and emergency braking.","A primary concern relates to the ability (and necessity) of AI models to generalize beyond their initial training data.","This generalization issue becomes evident in real-time scenarios, where models frequently encounter inputs not represented in their training or validation data.","In such cases, AI systems must still function effectively despite facing distributional or domain shifts.","This paper investigates the risk associated with overconfident AI models in safety-critical applications like autonomous driving.","To mitigate these risks, methods for training AI models that help maintain performance without overconfidence are proposed.","This involves implementing certainty reporting architectures and ensuring diverse training data.","While various distribution-based methods exist to provide safety mechanisms for AI models, there is a noted lack of systematic assessment of these methods, especially in the context of safety-critical automotive applications.","Many methods in the literature do not adapt well to the quick response times required in safety-critical edge applications.","This paper reviews these methods, discusses their suitability for safety-critical applications, and highlights their strengths and limitations.","The paper also proposes potential improvements to enhance the safety and reliability of AI algorithms in autonomous vehicles in the context of rapid and accurate decision-making processes."],"url":"http://arxiv.org/abs/2402.08208v1","category":"cs.AI"}
{"created":"2024-02-13 03:55:56","title":"Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap","abstract":"Doubly robust methods hold considerable promise for off-policy evaluation in Markov decision processes (MDPs) under sequential ignorability: They have been shown to converge as $1/\\sqrt{T}$ with the horizon $T$, to be statistically efficient in large samples, and to allow for modular implementation where preliminary estimation tasks can be executed using standard reinforcement learning techniques. Existing results, however, make heavy use of a strong distributional overlap assumption whereby the stationary distributions of the target policy and the data-collection policy are within a bounded factor of each other -- and this assumption is typically only credible when the state space of the MDP is bounded. In this paper, we re-visit the task of off-policy evaluation in MDPs under a weaker notion of distributional overlap, and introduce a class of truncated doubly robust (TDR) estimators which we find to perform well in this setting. When the distribution ratio of the target and data-collection policies is square-integrable (but not necessarily bounded), our approach recovers the large-sample behavior previously established under strong distributional overlap. When this ratio is not square-integrable, TDR is still consistent but with a slower-than-$1/\\sqrt{T}$; furthermore, this rate of convergence is minimax over a class of MDPs defined only using mixing conditions. We validate our approach numerically and find that, in our experiments, appropriate truncation plays a major role in enabling accurate off-policy evaluation when strong distributional overlap does not hold.","sentences":["Doubly robust methods hold considerable promise for off-policy evaluation in Markov decision processes (MDPs) under sequential ignorability: They have been shown to converge as $1/\\sqrt{T}$ with the horizon $T$, to be statistically efficient in large samples, and to allow for modular implementation where preliminary estimation tasks can be executed using standard reinforcement learning techniques.","Existing results, however, make heavy use of a strong distributional overlap assumption whereby the stationary distributions of the target policy and the data-collection policy are within a bounded factor of each other -- and this assumption is typically only credible when the state space of the MDP is bounded.","In this paper, we re-visit the task of off-policy evaluation in MDPs under a weaker notion of distributional overlap, and introduce a class of truncated doubly robust (TDR) estimators which we find to perform well in this setting.","When the distribution ratio of the target and data-collection policies is square-integrable (but not necessarily bounded), our approach recovers the large-sample behavior previously established under strong distributional overlap.","When this ratio is not square-integrable, TDR is still consistent but with a slower-than-$1/\\sqrt{T}$; furthermore, this rate of convergence is minimax over a class of MDPs defined only using mixing conditions.","We validate our approach numerically and find that, in our experiments, appropriate truncation plays a major role in enabling accurate off-policy evaluation when strong distributional overlap does not hold."],"url":"http://arxiv.org/abs/2402.08201v1","category":"stat.ML"}
{"created":"2024-02-13 03:51:10","title":"PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction","abstract":"Compound-Protein Interaction (CPI) prediction aims to predict the pattern and strength of compound-protein interactions for rational drug discovery. Existing deep learning-based methods utilize only the single modality of protein sequences or structures and lack the co-modeling of the joint distribution of the two modalities, which may lead to significant performance drops in complex real-world scenarios due to various factors, e.g., modality missing and domain shifting. More importantly, these methods only model protein sequences and structures at a single fixed scale, neglecting more fine-grained multi-scale information, such as those embedded in key protein fragments. In this paper, we propose a novel multi-scale Protein Sequence-structure Contrasting framework for CPI prediction (PSC-CPI), which captures the dependencies between protein sequences and structures through both intra-modality and cross-modality contrasting. We further apply length-variable protein augmentation to allow contrasting to be performed at different scales, from the amino acid level to the sequence level. Finally, in order to more fairly evaluate the model generalizability, we split the test data into four settings based on whether compounds and proteins have been observed during the training stage. Extensive experiments have shown that PSC-CPI generalizes well in all four settings, particularly in the more challenging ``Unseen-Both\" setting, where neither compounds nor proteins have been observed during training. Furthermore, even when encountering a situation of modality missing, i.e., inference with only single-modality protein data, PSC-CPI still exhibits comparable or even better performance than previous approaches.","sentences":["Compound-Protein Interaction (CPI) prediction aims to predict the pattern and strength of compound-protein interactions for rational drug discovery.","Existing deep learning-based methods utilize only the single modality of protein sequences or structures and lack the co-modeling of the joint distribution of the two modalities, which may lead to significant performance drops in complex real-world scenarios due to various factors, e.g., modality missing and domain shifting.","More importantly, these methods only model protein sequences and structures at a single fixed scale, neglecting more fine-grained multi-scale information, such as those embedded in key protein fragments.","In this paper, we propose a novel multi-scale Protein Sequence-structure Contrasting framework for CPI prediction (PSC-CPI), which captures the dependencies between protein sequences and structures through both intra-modality and cross-modality contrasting.","We further apply length-variable protein augmentation to allow contrasting to be performed at different scales, from the amino acid level to the sequence level.","Finally, in order to more fairly evaluate the model generalizability, we split the test data into four settings based on whether compounds and proteins have been observed during the training stage.","Extensive experiments have shown that PSC-CPI generalizes well in all four settings, particularly in the more challenging ``Unseen-Both\" setting, where neither compounds nor proteins have been observed during training.","Furthermore, even when encountering a situation of modality missing, i.e., inference with only single-modality protein data, PSC-CPI still exhibits comparable or even better performance than previous approaches."],"url":"http://arxiv.org/abs/2402.08198v1","category":"q-bio.BM"}
{"created":"2024-02-13 03:25:33","title":"THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation","abstract":"To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades $\\geq$75%. We identify that changing the number of distractor objects, target object color, or lighting conditions are the perturbations that reduce model performance the most. To verify the ecological validity of our results, we show that our results in simulation are correlated ($\\bar{R}^2 = 0.614$) to similar perturbations in real-world experiments. We open source code for others to use THE COLOSSEUM, and also release code to 3D print the objects used to replicate the real-world perturbations. Ultimately, we hope that THE COLOSSEUM will serve as a benchmark to identify modeling decisions that systematically improve generalization for manipulation. See https://robot-colosseum.github.io/ for more details.","sentences":["To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions.","Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup.","We present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations.","These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose.","Using THE COLOSSEUM, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors.","When multiple perturbations are applied in unison, the success rate degrades $\\geq$75%.","We identify that changing the number of distractor objects, target object color, or lighting conditions are the perturbations that reduce model performance the most.","To verify the ecological validity of our results, we show that our results in simulation are correlated ($\\bar{R}^2 = 0.614$) to similar perturbations in real-world experiments.","We open source code for others to use THE COLOSSEUM, and also release code to 3D print the objects used to replicate the real-world perturbations.","Ultimately, we hope that THE COLOSSEUM will serve as a benchmark to identify modeling decisions that systematically improve generalization for manipulation.","See https://robot-colosseum.github.io/ for more details."],"url":"http://arxiv.org/abs/2402.08191v1","category":"cs.RO"}
{"created":"2024-02-13 03:01:22","title":"Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5","abstract":"Modern deep learning techniques, which mimic traditional numerical weather prediction (NWP) models and are derived from global atmospheric reanalysis data, have caused a significant revolution within a few years. In this new paradigm, our research introduces a novel strategy that deviates from the common dependence on high-resolution data, which is often constrained by computational resources, and instead utilizes low-resolution data (2.5 degrees) for global weather prediction and climate data analysis. Our main focus is evaluating data-driven weather prediction (DDWP) frameworks, specifically addressing sample size adequacy, structural improvements to the model, and the ability of climate data to represent current climatic trends. By using the Adaptive Fourier Neural Operator (AFNO) model via FourCastNet and a proposed time-sliding method to inflate the dataset of the ECMWF Reanalysis v5 (ERA5), this paper improves on conventional approaches by adding more variables and a novel approach to data augmentation and processing. Our findings reveal that despite the lower resolution, the proposed approach demonstrates considerable accuracy in predicting atmospheric conditions, effectively rivaling higher-resolution models. Furthermore, the study confirms the model's proficiency in reflecting current climate trends and its potential in predicting future climatic events, underscoring its utility in climate change strategies. This research marks a pivotal step in the realm of meteorological forecasting, showcasing the feasibility of lower-resolution data in producing reliable predictions and opening avenues for more accessible and inclusive climate modeling. The insights gleaned from this study not only contribute to the advancement of climate science but also lay the groundwork for future innovations in the field.","sentences":["Modern deep learning techniques, which mimic traditional numerical weather prediction (NWP) models and are derived from global atmospheric reanalysis data, have caused a significant revolution within a few years.","In this new paradigm, our research introduces a novel strategy that deviates from the common dependence on high-resolution data, which is often constrained by computational resources, and instead utilizes low-resolution data (2.5 degrees) for global weather prediction and climate data analysis.","Our main focus is evaluating data-driven weather prediction (DDWP) frameworks, specifically addressing sample size adequacy, structural improvements to the model, and the ability of climate data to represent current climatic trends.","By using the Adaptive Fourier Neural Operator (AFNO) model via FourCastNet and a proposed time-sliding method to inflate the dataset of the ECMWF Reanalysis v5 (ERA5), this paper improves on conventional approaches by adding more variables and a novel approach to data augmentation and processing.","Our findings reveal that despite the lower resolution, the proposed approach demonstrates considerable accuracy in predicting atmospheric conditions, effectively rivaling higher-resolution models.","Furthermore, the study confirms the model's proficiency in reflecting current climate trends and its potential in predicting future climatic events, underscoring its utility in climate change strategies.","This research marks a pivotal step in the realm of meteorological forecasting, showcasing the feasibility of lower-resolution data in producing reliable predictions and opening avenues for more accessible and inclusive climate modeling.","The insights gleaned from this study not only contribute to the advancement of climate science but also lay the groundwork for future innovations in the field."],"url":"http://arxiv.org/abs/2402.08185v1","category":"cs.AI"}
{"created":"2024-02-13 02:48:18","title":"Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation","abstract":"Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned from other scenarios compared to agents learning from scratch. Furthermore, we adopted Curriculum Transfer Learning (CTL), enabling our deep learning policy to progressively acquire knowledge and skills across pre-designed homogeneous learning scenarios organized by difficulty levels. This process promotes inter- and intra-agent knowledge transfer, leading to high multi-agent learning performance in more complicated heterogeneous scenarios.","sentences":["Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS).","However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity.","Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms.","In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS.","We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned from other scenarios compared to agents learning from scratch.","Furthermore, we adopted Curriculum Transfer Learning (CTL), enabling our deep learning policy to progressively acquire knowledge and skills across pre-designed homogeneous learning scenarios organized by difficulty levels.","This process promotes inter- and intra-agent knowledge transfer, leading to high multi-agent learning performance in more complicated heterogeneous scenarios."],"url":"http://arxiv.org/abs/2402.08184v1","category":"cs.AI"}
{"created":"2024-02-13 02:28:57","title":"LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents","abstract":"Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.","sentences":["Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning.","However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction.","To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents.","Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome.","Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner.","We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners."],"url":"http://arxiv.org/abs/2402.08178v1","category":"cs.AI"}
{"created":"2024-02-13 02:13:12","title":"Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction","abstract":"Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various levels of hierarchy such as nodes' distances to landmarks, inter-landmark distances and hierarchical grouping of clusters. Experiments show that HPLC achieves state-of-the-art performances of link prediction on various datasets in terms of HIT@K, MRR, and AUC. The code is available at \\url{https://github.com/kmswin1/HPLC}.","sentences":["Learning positional information of nodes in a graph is important for link prediction tasks.","We propose a representation of positional information using representative nodes called landmarks.","A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions.","We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks.","In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances.","We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC).","HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks.","HPLC leverages the positional information of nodes based on landmarks at various levels of hierarchy such as nodes' distances to landmarks, inter-landmark distances and hierarchical grouping of clusters.","Experiments show that HPLC achieves state-of-the-art performances of link prediction on various datasets in terms of HIT@K, MRR, and AUC.","The code is available at \\url{https://github.com/kmswin1/HPLC}."],"url":"http://arxiv.org/abs/2402.08174v1","category":"cs.AI"}
{"created":"2024-02-13 02:03:26","title":"LLaGA: Large Language and Graph Assistant","abstract":"Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \\textbf{L}arge \\textbf{L}anguage \\textbf{a}nd \\textbf{G}raph \\textbf{A}ssistant (\\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs. Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-of-the-art graph models in both supervised and zero-shot scenarios. Our code is available at \\url{https://github.com/ChenRunjin/LLaGA}","sentences":["Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis.","Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning.","However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language.","To this end, we introduce the \\textbf{L}arge \\textbf{L}anguage \\textbf{a}nd \\textbf{G}raph \\textbf{A}ssistant (\\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data.","LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input.","LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector.","LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs.","Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-of-the-art graph models in both supervised and zero-shot scenarios.","Our code is available at \\url{https://github.com/ChenRunjin/LLaGA}"],"url":"http://arxiv.org/abs/2402.08170v1","category":"cs.LG"}
{"created":"2024-02-13 01:52:15","title":"On Limitations of the Transformer Architecture","abstract":"What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.","sentences":["What are the root causes of hallucinations in large language models (LLMs)?","We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small.","We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true."],"url":"http://arxiv.org/abs/2402.08164v1","category":"stat.ML"}
{"created":"2024-02-13 01:39:07","title":"Direct numerical simulation of a thermal turbulent boundary layer: an analogy to simulate bushfires and a testbed for artificial intelligence remote sensing of bushfire propagation","abstract":"Direct numerical simulation of a turbulent thermal boundary layer (TTBL) can perform the role of an analogy to simulate bushfires that can serve as a testbed for artificial intelligence (AI) enhanced remote sensing of bushfire propagation. By solving the Navier-Stokes equations for a turbulent flow, DNS predicts the flow field and allows for a detailed study of the interactions between the turbulent flow and thermal plumes. In addition to potentially providing insights into the complex bushfire behaviour, direct numerical simulation (DNS) can generate synthetic remote sensing data to train AI algorithms such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which can process large amounts of remotely sensed data associated with bushfire. Using the results of DNS as training data can improve the accuracy of AI remote sensing in predicting firefront propagation of bushfires. DNS can also test the accuracy of the AI remote sensing algorithms by generating synthetic remote sensing data that allows their performance assessment and uncertainty quantification in predicting the evolution of a bushfire. The combination of DNS and AI can improve our understanding of bushfire dynamics, develop more accurate prediction models, and aid in bushfire management and mitigation.","sentences":["Direct numerical simulation of a turbulent thermal boundary layer (TTBL) can perform the role of an analogy to simulate bushfires that can serve as a testbed for artificial intelligence (AI) enhanced remote sensing of bushfire propagation.","By solving the Navier-Stokes equations for a turbulent flow, DNS predicts the flow field and allows for a detailed study of the interactions between the turbulent flow and thermal plumes.","In addition to potentially providing insights into the complex bushfire behaviour, direct numerical simulation (DNS) can generate synthetic remote sensing data to train AI algorithms such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which can process large amounts of remotely sensed data associated with bushfire.","Using the results of DNS as training data can improve the accuracy of AI remote sensing in predicting firefront propagation of bushfires.","DNS can also test the accuracy of the AI remote sensing algorithms by generating synthetic remote sensing data that allows their performance assessment and uncertainty quantification in predicting the evolution of a bushfire.","The combination of DNS and AI can improve our understanding of bushfire dynamics, develop more accurate prediction models, and aid in bushfire management and mitigation."],"url":"http://arxiv.org/abs/2402.08157v1","category":"physics.flu-dyn"}
{"created":"2024-02-13 01:38:01","title":"Group Decision-Making among Privacy-Aware Agents","abstract":"How can individuals exchange information to learn from each other despite their privacy needs and security concerns? For example, consider individuals deliberating a contentious topic and being concerned about divulging their private experiences. Preserving individual privacy and enabling efficient social learning are both important desiderata but seem fundamentally at odds with each other and very hard to reconcile. We do so by controlling information leakage using rigorous statistical guarantees that are based on differential privacy (DP). Our agents use log-linear rules to update their beliefs after communicating with their neighbors. Adding DP randomization noise to beliefs provides communicating agents with plausible deniability with regard to their private information and their network neighborhoods. We consider two learning environments one for distributed maximum-likelihood estimation given a finite number of private signals and another for online learning from an infinite, intermittent signal stream. Noisy information aggregation in the finite case leads to interesting tradeoffs between rejecting low-quality states and making sure all high-quality states are accepted in the algorithm output. Our results flesh out the nature of the trade-offs in both cases between the quality of the group decision outcomes, learning accuracy, communication cost, and the level of privacy protections that the agents are afforded.","sentences":["How can individuals exchange information to learn from each other despite their privacy needs and security concerns?","For example, consider individuals deliberating a contentious topic and being concerned about divulging their private experiences.","Preserving individual privacy and enabling efficient social learning are both important desiderata but seem fundamentally at odds with each other and very hard to reconcile.","We do so by controlling information leakage using rigorous statistical guarantees that are based on differential privacy (DP).","Our agents use log-linear rules to update their beliefs after communicating with their neighbors.","Adding DP randomization noise to beliefs provides communicating agents with plausible deniability with regard to their private information and their network neighborhoods.","We consider two learning environments one for distributed maximum-likelihood estimation given a finite number of private signals and another for online learning from an infinite, intermittent signal stream.","Noisy information aggregation in the finite case leads to interesting tradeoffs between rejecting low-quality states and making sure all high-quality states are accepted in the algorithm output.","Our results flesh out the nature of the trade-offs in both cases between the quality of the group decision outcomes, learning accuracy, communication cost, and the level of privacy protections that the agents are afforded."],"url":"http://arxiv.org/abs/2402.08156v1","category":"cs.LG"}
{"created":"2024-02-13 01:31:08","title":"CMA-R:Causal Mediation Analysis for Explaining Rumour Detection","abstract":"We apply causal mediation analysis to explain the decision-making process of neural models for rumour detection on Twitter. Interventions at the input and network level reveal the causal impacts of tweets and words in the model output. We find that our approach CMA-R -- Causal Mediation Analysis for Rumour detection -- identifies salient tweets that explain model predictions and show strong agreement with human judgements for critical tweets determining the truthfulness of stories. CMA-R can further highlight causally impactful words in the salient tweets, providing another layer of interpretability and transparency into these blackbox rumour detection systems. Code is available at: https://github.com/ltian678/cma-r.","sentences":["We apply causal mediation analysis to explain the decision-making process of neural models for rumour detection on Twitter.","Interventions at the input and network level reveal the causal impacts of tweets and words in the model output.","We find that our approach CMA-R -- Causal Mediation Analysis for Rumour detection -- identifies salient tweets that explain model predictions and show strong agreement with human judgements for critical tweets determining the truthfulness of stories.","CMA-R can further highlight causally impactful words in the salient tweets, providing another layer of interpretability and transparency into these blackbox rumour detection systems.","Code is available at: https://github.com/ltian678/cma-r."],"url":"http://arxiv.org/abs/2402.08155v1","category":"cs.CL"}
{"created":"2024-02-13 01:03:39","title":"Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models","abstract":"We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models. One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC. By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in the cases of logistic regression and shallow ReLU-activated artificial neural networks, and provide a simple approximation that sidesteps the need to compute full Hessian matrices and their spectra. We test the methodology on an $n\\ll p$ dataset that is known to produce unstable LOO IS weights.","sentences":["We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models.","One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC.","By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions.","In doing so, the transformations stabilize importance weights.","Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian.","We derive closed-form exact formulae for these Jacobian determinants in the cases of logistic regression and shallow ReLU-activated artificial neural networks, and provide a simple approximation that sidesteps the need to compute full Hessian matrices and their spectra.","We test the methodology on an $n\\ll p$ dataset that is known to produce unstable LOO IS weights."],"url":"http://arxiv.org/abs/2402.08151v1","category":"stat.ME"}
{"created":"2024-02-13 00:55:14","title":"Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search","abstract":"We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search .","sentences":["We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq.","Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step.","In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models.","On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes.","The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems.","Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search ."],"url":"http://arxiv.org/abs/2402.08147v1","category":"cs.SE"}
{"created":"2024-02-13 00:50:06","title":"Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings","abstract":"This paper introduces a new approach for continual planning and model learning in non-stationary stochastic environments expressed using relational representations. Such capabilities are essential for the deployment of sequential decision-making systems in the uncertain, constantly evolving real world. Working in such practical settings with unknown (and non-stationary) transition systems and changing tasks, the proposed framework models gaps in the agent's current state of knowledge and uses them to conduct focused, investigative explorations. Data collected using these explorations is used for learning generalizable probabilistic models for solving the current task despite continual changes in the environment dynamics. Empirical evaluations on several benchmark domains show that this approach significantly outperforms planning and RL baselines in terms of sample complexity in non-stationary settings. Theoretical results show that the system reverts to exhibit desirable convergence properties when stationarity holds.","sentences":["This paper introduces a new approach for continual planning and model learning in non-stationary stochastic environments expressed using relational representations.","Such capabilities are essential for the deployment of sequential decision-making systems in the uncertain, constantly evolving real world.","Working in such practical settings with unknown (and non-stationary) transition systems and changing tasks, the proposed framework models gaps in the agent's current state of knowledge and uses them to conduct focused, investigative explorations.","Data collected using these explorations is used for learning generalizable probabilistic models for solving the current task despite continual changes in the environment dynamics.","Empirical evaluations on several benchmark domains show that this approach significantly outperforms planning and RL baselines in terms of sample complexity in non-stationary settings.","Theoretical results show that the system reverts to exhibit desirable convergence properties when stationarity holds."],"url":"http://arxiv.org/abs/2402.08145v1","category":"cs.AI"}
{"created":"2024-02-13 00:46:46","title":"Average-Case Analysis of Iterative Voting","abstract":"Iterative voting is a natural model of repeated strategic decision-making in social choice when agents have the opportunity to update their votes prior to finalizing the group decision. Prior work has analyzed the efficacy of iterative plurality on the welfare of the chosen outcome at equilibrium, relative to the truthful vote profile, via an adaptation of the price of anarchy. However, prior analyses have only studied the worst-case and average-case performances when agents' preferences are distributed by the impartial culture. This work extends average-case analyses to a wider class of distributions and distinguishes when iterative plurality improves or degrades asymptotic welfare.","sentences":["Iterative voting is a natural model of repeated strategic decision-making in social choice when agents have the opportunity to update their votes prior to finalizing the group decision.","Prior work has analyzed the efficacy of iterative plurality on the welfare of the chosen outcome at equilibrium, relative to the truthful vote profile, via an adaptation of the price of anarchy.","However, prior analyses have only studied the worst-case and average-case performances when agents' preferences are distributed by the impartial culture.","This work extends average-case analyses to a wider class of distributions and distinguishes when iterative plurality improves or degrades asymptotic welfare."],"url":"http://arxiv.org/abs/2402.08144v1","category":"cs.GT"}
{"created":"2024-02-13 00:36:10","title":"Artificial intelligence and the transformation of higher education institutions","abstract":"Artificial intelligence (AI) advances and the rapid adoption of generative AI tools like ChatGPT present new opportunities and challenges for higher education. While substantial literature discusses AI in higher education, there is a lack of a systemic approach that captures a holistic view of the AI transformation of higher education institutions (HEIs). To fill this gap, this article, taking a complex systems approach, develops a causal loop diagram (CLD) to map the causal feedback mechanisms of AI transformation in a typical HEI. Our model accounts for the forces that drive the AI transformation and the consequences of the AI transformation on value creation in a typical HEI. The article identifies and analyzes several reinforcing and balancing feedback loops, showing how, motivated by AI technology advances, the HEI invests in AI to improve student learning, research, and administration. The HEI must take measures to deal with academic integrity problems and adapt to changes in available jobs due to AI, emphasizing AI-complementary skills for its students. However, HEIs face a competitive threat and several policy traps that may lead to decline. HEI leaders need to become systems thinkers to manage the complexity of the AI transformation and benefit from the AI feedback loops while avoiding the associated pitfalls. We also discuss long-term scenarios, the notion of HEIs influencing the direction of AI, and directions for future research on AI transformation.","sentences":["Artificial intelligence (AI) advances and the rapid adoption of generative AI tools like ChatGPT present new opportunities and challenges for higher education.","While substantial literature discusses AI in higher education, there is a lack of a systemic approach that captures a holistic view of the AI transformation of higher education institutions (HEIs).","To fill this gap, this article, taking a complex systems approach, develops a causal loop diagram (CLD) to map the causal feedback mechanisms of AI transformation in a typical HEI.","Our model accounts for the forces that drive the AI transformation and the consequences of the AI transformation on value creation in a typical HEI.","The article identifies and analyzes several reinforcing and balancing feedback loops, showing how, motivated by AI technology advances, the HEI invests in AI to improve student learning, research, and administration.","The HEI must take measures to deal with academic integrity problems and adapt to changes in available jobs due to AI, emphasizing AI-complementary skills for its students.","However, HEIs face a competitive threat and several policy traps that may lead to decline.","HEI leaders need to become systems thinkers to manage the complexity of the AI transformation and benefit from the AI feedback loops while avoiding the associated pitfalls.","We also discuss long-term scenarios, the notion of HEIs influencing the direction of AI, and directions for future research on AI transformation."],"url":"http://arxiv.org/abs/2402.08143v1","category":"econ.GN"}
{"created":"2024-02-12 23:59:59","title":"Detecting Low-Degree Truncation","abstract":"We consider the following basic, and very broad, statistical problem: Given a known high-dimensional distribution ${\\cal D}$ over $\\mathbb{R}^n$ and a collection of data points in $\\mathbb{R}^n$, distinguish between the two possibilities that (i) the data was drawn from ${\\cal D}$, versus (ii) the data was drawn from ${\\cal D}|_S$, i.e. from ${\\cal D}$ subject to truncation by an unknown truncation set $S \\subseteq \\mathbb{R}^n$.   We study this problem in the setting where ${\\cal D}$ is a high-dimensional i.i.d. product distribution and $S$ is an unknown degree-$d$ polynomial threshold function (one of the most well-studied types of Boolean-valued function over $\\mathbb{R}^n$). Our main results are an efficient algorithm when ${\\cal D}$ is a hypercontractive distribution, and a matching lower bound:   $\\bullet$ For any constant $d$, we give a polynomial-time algorithm which successfully distinguishes ${\\cal D}$ from ${\\cal D}|_S$ using $O(n^{d/2})$ samples (subject to mild technical conditions on ${\\cal D}$ and $S$);   $\\bullet$ Even for the simplest case of ${\\cal D}$ being the uniform distribution over $\\{+1, -1\\}^n$, we show that for any constant $d$, any distinguishing algorithm for degree-$d$ polynomial threshold functions must use $\\Omega(n^{d/2})$ samples.","sentences":["We consider the following basic, and very broad, statistical problem: Given a known high-dimensional distribution ${\\cal D}$ over $\\mathbb{R}^n$ and a collection of data points in $\\mathbb{R}^n$, distinguish between the two possibilities that (i) the data was drawn from ${\\cal D}$, versus (ii) the data was drawn from ${\\cal D}|_S$, i.e. from ${\\cal D}$ subject to truncation by an unknown truncation set $S \\subseteq \\mathbb{R}^n$.   We study this problem in the setting where ${\\cal D}$ is a high-dimensional i.i.d. product distribution and $S$ is an unknown degree-$d$ polynomial threshold function (one of the most well-studied types of Boolean-valued function over $\\mathbb{R}^n$).","Our main results are an efficient algorithm when ${\\cal D}$ is a hypercontractive distribution, and a matching lower bound:   $\\bullet$ For any constant $d$, we give a polynomial-time algorithm which successfully distinguishes ${\\cal D}$ from ${\\cal D}|_S$ using $O(n^{d/2})$ samples (subject to mild technical conditions on ${\\cal D}$ and $S$);   $\\bullet$ Even for the simplest case of ${\\cal D}$ being the uniform distribution over $\\{+1, -1\\}^n$, we show that for any constant $d$, any distinguishing algorithm for degree-$d$ polynomial threshold functions must use $\\Omega(n^{d/2})$ samples."],"url":"http://arxiv.org/abs/2402.08133v1","category":"cs.CC"}
{"created":"2024-02-12 23:54:28","title":"Automated Design of Affine Maximizer Mechanisms in Dynamic Settings","abstract":"Dynamic mechanism design is a challenging extension to ordinary mechanism design in which the mechanism designer must make a sequence of decisions over time in the face of possibly untruthful reports of participating agents. Optimizing dynamic mechanisms for welfare is relatively well understood. However, there has been less work on optimizing for other goals (e.g. revenue), and without restrictive assumptions on valuations, it is remarkably challenging to characterize good mechanisms. Instead, we turn to automated mechanism design to find mechanisms with good performance in specific problem instances. In fact, the situation is similar even in static mechanism design. However, in the static case, optimization/machine learning-based automated mechanism design techniques have been successful in finding high-revenue mechanisms in cases beyond the reach of analytical results. We extend the class of affine maximizer mechanisms to MDPs where agents may untruthfully report their rewards. This extension results in a challenging bilevel optimization problem in which the upper problem involves choosing optimal mechanism parameters, and the lower problem involves solving the resulting MDP. Our approach can find truthful dynamic mechanisms that achieve strong performance on goals other than welfare, and can be applied to essentially any problem setting-without restrictions on valuations-for which RL can learn optimal policies.","sentences":["Dynamic mechanism design is a challenging extension to ordinary mechanism design in which the mechanism designer must make a sequence of decisions over time in the face of possibly untruthful reports of participating agents.","Optimizing dynamic mechanisms for welfare is relatively well understood.","However, there has been less work on optimizing for other goals (e.g. revenue), and without restrictive assumptions on valuations, it is remarkably challenging to characterize good mechanisms.","Instead, we turn to automated mechanism design to find mechanisms with good performance in specific problem instances.","In fact, the situation is similar even in static mechanism design.","However, in the static case, optimization/machine learning-based automated mechanism design techniques have been successful in finding high-revenue mechanisms in cases beyond the reach of analytical results.","We extend the class of affine maximizer mechanisms to MDPs where agents may untruthfully report their rewards.","This extension results in a challenging bilevel optimization problem in which the upper problem involves choosing optimal mechanism parameters, and the lower problem involves solving the resulting MDP.","Our approach can find truthful dynamic mechanisms that achieve strong performance on goals other than welfare, and can be applied to essentially any problem setting-without restrictions on valuations-for which RL can learn optimal policies."],"url":"http://arxiv.org/abs/2402.08129v1","category":"cs.GT"}
{"created":"2024-02-12 23:53:46","title":"Recursive Joint Simulation in Games","abstract":"Game-theoretic dynamics between AI agents could differ from traditional human-human interactions in various ways. One such difference is that it may be possible to accurately simulate an AI agent, for example because its source code is known. Our aim is to explore ways of leveraging this possibility to achieve more cooperative outcomes in strategic settings. In this paper, we study an interaction between AI agents where the agents run a recursive joint simulation. That is, the agents first jointly observe a simulation of the situation they face. This simulation in turn recursively includes additional simulations (with a small chance of failure, to avoid infinite recursion), and the results of all these nested simulations are observed before an action is chosen. We show that the resulting interaction is strategically equivalent to an infinitely repeated version of the original game, allowing a direct transfer of existing results such as the various folk theorems.","sentences":["Game-theoretic dynamics between AI agents could differ from traditional human-human interactions in various ways.","One such difference is that it may be possible to accurately simulate an AI agent, for example because its source code is known.","Our aim is to explore ways of leveraging this possibility to achieve more cooperative outcomes in strategic settings.","In this paper, we study an interaction between AI agents where the agents run a recursive joint simulation.","That is, the agents first jointly observe a simulation of the situation they face.","This simulation in turn recursively includes additional simulations (with a small chance of failure, to avoid infinite recursion), and the results of all these nested simulations are observed before an action is chosen.","We show that the resulting interaction is strategically equivalent to an infinitely repeated version of the original game, allowing a direct transfer of existing results such as the various folk theorems."],"url":"http://arxiv.org/abs/2402.08128v1","category":"cs.AI"}
{"created":"2024-02-12 23:49:40","title":"Customizable Perturbation Synthesis for Robust SLAM Benchmarking","abstract":"Robustness is a crucial factor for the successful deployment of robots in unstructured environments, particularly in the domain of Simultaneous Localization and Mapping (SLAM). Simulation-based benchmarks have emerged as a highly scalable approach for robustness evaluation compared to real-world data collection. However, crafting a challenging and controllable noisy world with diverse perturbations remains relatively under-explored. To this end, we propose a novel, customizable pipeline for noisy data synthesis, aimed at assessing the resilience of multi-modal SLAM models against various perturbations. This pipeline incorporates customizable hardware setups, software components, and perturbed environments. In particular, we introduce comprehensive perturbation taxonomy along with a perturbation composition toolbox, allowing the transformation of clean simulations into challenging noisy environments. Utilizing the pipeline, we instantiate the Robust-SLAM benchmark, which includes diverse perturbation types, to evaluate the risk tolerance of existing advanced multi-modal SLAM models. Our extensive analysis uncovers the susceptibilities of existing SLAM models to real-world disturbance, despite their demonstrated accuracy in standard benchmarks. Our perturbation synthesis toolbox, SLAM robustness evaluation pipeline, and Robust-SLAM benchmark will be made publicly available at https://github.com/Xiaohao-Xu/SLAM-under-Perturbation/.","sentences":["Robustness is a crucial factor for the successful deployment of robots in unstructured environments, particularly in the domain of Simultaneous Localization and Mapping (SLAM).","Simulation-based benchmarks have emerged as a highly scalable approach for robustness evaluation compared to real-world data collection.","However, crafting a challenging and controllable noisy world with diverse perturbations remains relatively under-explored.","To this end, we propose a novel, customizable pipeline for noisy data synthesis, aimed at assessing the resilience of multi-modal SLAM models against various perturbations.","This pipeline incorporates customizable hardware setups, software components, and perturbed environments.","In particular, we introduce comprehensive perturbation taxonomy along with a perturbation composition toolbox, allowing the transformation of clean simulations into challenging noisy environments.","Utilizing the pipeline, we instantiate the Robust-SLAM benchmark, which includes diverse perturbation types, to evaluate the risk tolerance of existing advanced multi-modal SLAM models.","Our extensive analysis uncovers the susceptibilities of existing SLAM models to real-world disturbance, despite their demonstrated accuracy in standard benchmarks.","Our perturbation synthesis toolbox, SLAM robustness evaluation pipeline, and Robust-SLAM benchmark will be made publicly available at https://github.com/Xiaohao-Xu/SLAM-under-Perturbation/."],"url":"http://arxiv.org/abs/2402.08125v1","category":"cs.RO"}
{"created":"2024-02-12 23:33:22","title":"Unmasking honey adulteration : a breakthrough in quality assurance through cutting-edge convolutional neural network analysis of thermal images","abstract":"Honey, a natural product generated from organic sources, is widely recognized for its revered reputation. Nevertheless, honey is susceptible to adulteration, a situation that has substantial consequences for both the well-being of the general population and the financial well-being of a country. Conventional approaches for detecting honey adulteration are often associated with extensive time requirements and restricted sensitivity. This paper presents a novel approach to address the aforementioned issue by employing Convolutional Neural Networks (CNNs) for the classification of honey samples based on thermal images. The use of thermal imaging technique offers a significant advantage in detecting adulterants, as it can reveal differences in temperature in honey samples caused by variations in sugar composition, moisture levels, and other substances used for adulteration. To establish a meticulous approach to categorizing honey, a thorough dataset comprising thermal images of authentic and tainted honey samples was collected. Several state-of-the-art Convolutional Neural Network (CNN) models were trained and optimized using the dataset that was gathered. Within this set of models, there exist pre-trained models such as InceptionV3, Xception, VGG19, and ResNet that have exhibited exceptional performance, achieving classification accuracies ranging from 88% to 98%. Furthermore, we have implemented a more streamlined and less complex convolutional neural network (CNN) model, outperforming comparable models with an outstanding accuracy rate of 99%. This simplification offers not only the sole advantage of the model, but it also concurrently offers a more efficient solution in terms of resources and time. This approach offers a viable way to implement quality control measures in the honey business, so guaranteeing the genuineness and safety of this valuable organic commodity.","sentences":["Honey, a natural product generated from organic sources, is widely recognized for its revered reputation.","Nevertheless, honey is susceptible to adulteration, a situation that has substantial consequences for both the well-being of the general population and the financial well-being of a country.","Conventional approaches for detecting honey adulteration are often associated with extensive time requirements and restricted sensitivity.","This paper presents a novel approach to address the aforementioned issue by employing Convolutional Neural Networks (CNNs) for the classification of honey samples based on thermal images.","The use of thermal imaging technique offers a significant advantage in detecting adulterants, as it can reveal differences in temperature in honey samples caused by variations in sugar composition, moisture levels, and other substances used for adulteration.","To establish a meticulous approach to categorizing honey, a thorough dataset comprising thermal images of authentic and tainted honey samples was collected.","Several state-of-the-art Convolutional Neural Network (CNN) models were trained and optimized using the dataset that was gathered.","Within this set of models, there exist pre-trained models such as InceptionV3, Xception, VGG19, and ResNet that have exhibited exceptional performance, achieving classification accuracies ranging from 88% to 98%.","Furthermore, we have implemented a more streamlined and less complex convolutional neural network (CNN) model, outperforming comparable models with an outstanding accuracy rate of 99%.","This simplification offers not only the sole advantage of the model, but it also concurrently offers a more efficient solution in terms of resources and time.","This approach offers a viable way to implement quality control measures in the honey business, so guaranteeing the genuineness and safety of this valuable organic commodity."],"url":"http://arxiv.org/abs/2402.08122v1","category":"cs.CV"}
{"created":"2024-02-12 23:15:16","title":"A Universal Non-Parametric Approach For Improved Molecular Sequence Analysis","abstract":"In the field of biological research, it is essential to comprehend the characteristics and functions of molecular sequences. The classification of molecular sequences has seen widespread use of neural network-based techniques. Despite their astounding accuracy, these models often require a substantial number of parameters and more data collection. In this work, we present a novel approach based on the compression-based Model, motivated from \\cite{jiang2023low}, which combines the simplicity of basic compression algorithms like Gzip and Bz2, with Normalized Compression Distance (NCD) algorithm to achieve better performance on classification tasks without relying on handcrafted features or pre-trained models. Firstly, we compress the molecular sequence using well-known compression algorithms, such as Gzip and Bz2. By leveraging the latent structure encoded in compressed files, we compute the Normalized Compression Distance between each pair of molecular sequences, which is derived from the Kolmogorov complexity. This gives us a distance matrix, which is the input for generating a kernel matrix using a Gaussian kernel. Next, we employ kernel Principal Component Analysis (PCA) to get the vector representations for the corresponding molecular sequence, capturing important structural and functional information. The resulting vector representations provide an efficient yet effective solution for molecular sequence analysis and can be used in ML-based downstream tasks. The proposed approach eliminates the need for computationally intensive Deep Neural Networks (DNNs), with their large parameter counts and data requirements. Instead, it leverages a lightweight and universally accessible compression-based model.","sentences":["In the field of biological research, it is essential to comprehend the characteristics and functions of molecular sequences.","The classification of molecular sequences has seen widespread use of neural network-based techniques.","Despite their astounding accuracy, these models often require a substantial number of parameters and more data collection.","In this work, we present a novel approach based on the compression-based Model, motivated from \\cite{jiang2023low}, which combines the simplicity of basic compression algorithms like Gzip and Bz2, with Normalized Compression Distance (NCD) algorithm to achieve better performance on classification tasks without relying on handcrafted features or pre-trained models.","Firstly, we compress the molecular sequence using well-known compression algorithms, such as Gzip and Bz2.","By leveraging the latent structure encoded in compressed files, we compute the Normalized Compression Distance between each pair of molecular sequences, which is derived from the Kolmogorov complexity.","This gives us a distance matrix, which is the input for generating a kernel matrix using a Gaussian kernel.","Next, we employ kernel Principal Component Analysis (PCA) to get the vector representations for the corresponding molecular sequence, capturing important structural and functional information.","The resulting vector representations provide an efficient yet effective solution for molecular sequence analysis and can be used in ML-based downstream tasks.","The proposed approach eliminates the need for computationally intensive Deep Neural Networks (DNNs), with their large parameter counts and data requirements.","Instead, it leverages a lightweight and universally accessible compression-based model."],"url":"http://arxiv.org/abs/2402.08117v1","category":"cs.LG"}
{"created":"2024-02-12 23:13:39","title":"OGLE-2023-BLG-0836L: The sixth microlensing planet in a binary stellar system","abstract":"Light curves of microlensing events occasionally deviate from the smooth and symmetric form of a single-lens single-source event. While most of these anomalous events can be accounted for by employing a binary-lens single-source (2L1S) or a single-lens binary-source (1L2S) framework, it is established that a small fraction of events remain unexplained by either of these interpretations. We carry out a project in which data collected by high-cadence microlensing surveys were reinvestigated with the aim of uncovering the nature of anomalous lensing events with no proposed 2L1S or 1L2S models. From the project, we find that the anomaly appearing in the lensing event OGLE-2023-BLG-0836 cannot be explained by the usual interpretations and conduct a comprehensive analysis of the event. From thorough modeling of the light curve under sophisticated lens-system configurations, we have arrived at the conclusion that a triple-mass lens system is imperative to account for the anomaly features observed in the lensing light curve. From the Bayesian analysis using the measured observables of the event time scale and angular Einstein radius, we determine that the least massive component of the lens has a planetary mass of $4.36^{+2.35}_{-2.18}~M_{\\rm J}$. This planet orbits within a stellar binary system composed of two stars with masses $0.71^{+0.38}_{-0.36}~M_\\odot$ and $0.56^{+0.30}_{-0.28}~M_\\odot$. This lensing event signifies the sixth occurrence of a planetary microlensing system in which a planet belongs to a stellar binary system.","sentences":["Light curves of microlensing events occasionally deviate from the smooth and symmetric form of a single-lens single-source event.","While most of these anomalous events can be accounted for by employing a binary-lens single-source (2L1S) or a single-lens binary-source (1L2S) framework, it is established that a small fraction of events remain unexplained by either of these interpretations.","We carry out a project in which data collected by high-cadence microlensing surveys were reinvestigated with the aim of uncovering the nature of anomalous lensing events with no proposed 2L1S or 1L2S models.","From the project, we find that the anomaly appearing in the lensing event OGLE-2023-BLG-0836 cannot be explained by the usual interpretations and conduct a comprehensive analysis of the event.","From thorough modeling of the light curve under sophisticated lens-system configurations, we have arrived at the conclusion that a triple-mass lens system is imperative to account for the anomaly features observed in the lensing light curve.","From the Bayesian analysis using the measured observables of the event time scale and angular Einstein radius, we determine that the least massive component of the lens has a planetary mass of $4.36^{+2.35}_{-2.18}~M_{\\rm J}$.","This planet orbits within a stellar binary system composed of two stars with masses $0.71^{+0.38}_{-0.36}~M_\\odot$ and $0.56^{+0.30}_{-0.28}~M_\\odot$. This lensing event signifies the sixth occurrence of a planetary microlensing system in which a planet belongs to a stellar binary system."],"url":"http://arxiv.org/abs/2402.08116v1","category":"astro-ph.EP"}
{"created":"2024-02-12 23:11:01","title":"On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks","abstract":"There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions. In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance. We observe significant performance collapse with self-critique, significant performance gains with sound external verification, but that the content of critique doesn't matter to the performance of the system. In fact, merely re-prompting with a sound verifier maintains most of the benefits of more involved setups.","sentences":["There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs).","While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion.","This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval.","In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning.","We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning.","We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions.","In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance.","We observe significant performance collapse with self-critique, significant performance gains with sound external verification, but that the content of critique doesn't matter to the performance of the system.","In fact, merely re-prompting with a sound verifier maintains most of the benefits of more involved setups."],"url":"http://arxiv.org/abs/2402.08115v1","category":"cs.AI"}
{"created":"2024-02-12 23:09:00","title":"Active Preference Learning for Large Language Models","abstract":"As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.","sentences":["As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important.","A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles.","Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable.","Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative.","In this work, we develop an active learning strategy for DPO to make better use of preference labels.","We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO.","We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data."],"url":"http://arxiv.org/abs/2402.08114v1","category":"cs.LG"}
{"created":"2024-02-12 23:08:17","title":"A Competition Winning Deep Reinforcement Learning Agent in microRTS","abstract":"Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. RAISocketAI is the first DRL agent to win the IEEE microRTS competition. In a benchmark without performance constraints, RAISocketAI regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research. Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to RAISocketAI's winning performance. These strategies can be used to economically train future DRL agents. Further work in Imitation Learning using Behavior Cloning and fine-tuning these models with DRL has proven promising as an efficient way to bootstrap models with demonstrated, competitive behaviors.","sentences":["Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents.","RAISocketAI is the first DRL agent to win the IEEE microRTS competition.","In a benchmark without performance constraints, RAISocketAI regularly defeated the two prior competition winners.","This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research.","Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to RAISocketAI's winning performance.","These strategies can be used to economically train future DRL agents.","Further work in Imitation Learning using Behavior Cloning and fine-tuning these models with DRL has proven promising as an efficient way to bootstrap models with demonstrated, competitive behaviors."],"url":"http://arxiv.org/abs/2402.08112v1","category":"cs.LG"}
{"created":"2024-02-12 22:10:06","title":"Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control","abstract":"Background: Machine learning (ML) methods often fail with data that deviates from their training distribution. This is a significant concern for ML-enabled devices in clinical settings, where data drift may cause unexpected performance that jeopardizes patient safety.   Method: We propose a ML-enabled Statistical Process Control (SPC) framework for out-of-distribution (OOD) detection and drift monitoring. SPC is advantageous as it visually and statistically highlights deviations from the expected distribution. To demonstrate the utility of the proposed framework for monitoring data drift in radiological images, we investigated different design choices, including methods for extracting feature representations, drift quantification, and SPC parameter selection.   Results: We demonstrate the effectiveness of our framework for two tasks: 1) differentiating axial vs. non-axial computed tomography (CT) images and 2) separating chest x-ray (CXR) from other modalities. For both tasks, we achieved high accuracy in detecting OOD inputs, with 0.913 in CT and 0.995 in CXR, and sensitivity of 0.980 in CT and 0.984 in CXR. Our framework was also adept at monitoring data streams and identifying the time a drift occurred. In a simulation with 100 daily CXR cases, we detected a drift in OOD input percentage from 0-1% to 3-5% within two days, maintaining a low false-positive rate. Through additional experimental results, we demonstrate the framework's data-agnostic nature and independence from the underlying model's structure.   Conclusion: We propose a framework for OOD detection and drift monitoring that is agnostic to data, modality, and model. The framework is customizable and can be adapted for specific applications.","sentences":["Background: Machine learning (ML) methods often fail with data that deviates from their training distribution.","This is a significant concern for ML-enabled devices in clinical settings, where data drift may cause unexpected performance that jeopardizes patient safety.   ","Method: We propose a ML-enabled Statistical Process Control (SPC) framework for out-of-distribution (OOD) detection and drift monitoring.","SPC is advantageous as it visually and statistically highlights deviations from the expected distribution.","To demonstrate the utility of the proposed framework for monitoring data drift in radiological images, we investigated different design choices, including methods for extracting feature representations, drift quantification, and SPC parameter selection.   ","Results:","We demonstrate the effectiveness of our framework for two tasks: 1) differentiating axial vs. non-axial computed tomography (CT) images and 2) separating chest x-ray (CXR) from other modalities.","For both tasks, we achieved high accuracy in detecting OOD inputs, with 0.913 in CT and 0.995 in CXR, and sensitivity of 0.980 in CT and 0.984 in CXR.","Our framework was also adept at monitoring data streams and identifying the time a drift occurred.","In a simulation with 100 daily CXR cases, we detected a drift in OOD input percentage from 0-1% to 3-5% within two days, maintaining a low false-positive rate.","Through additional experimental results, we demonstrate the framework's data-agnostic nature and independence from the underlying model's structure.   ","Conclusion: We propose a framework for OOD detection and drift monitoring that is agnostic to data, modality, and model.","The framework is customizable and can be adapted for specific applications."],"url":"http://arxiv.org/abs/2402.08088v1","category":"cs.AI"}
{"created":"2024-02-12 22:06:37","title":"Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning","abstract":"Graph learning is crucial in the fields of bioinformatics, social networks, and chemicals. Although high-order graphlets, such as cycles, are critical to achieving an informative graph representation for node classification, edge prediction, and graph recognition, modeling high-order topological characteristics poses significant computational challenges, restricting its widespread applications in machine learning. To address this limitation, we introduce the concept of \\textit{message detouring} to hierarchically characterize cycle representation throughout the entire graph, which capitalizes on the contrast between the shortest and longest pathways within a range of local topologies associated with each graph node. The topological feature representations derived from our message detouring landscape demonstrate comparable expressive power to high-order \\textit{Weisfeiler-Lehman} (WL) tests but much less computational demands. In addition to the integration with graph kernel and message passing neural networks, we present a novel message detouring neural network, which uses Transformer backbone to integrate cycle representations across nodes and edges. Aside from theoretical results, experimental results on expressiveness, graph classification, and node classification show message detouring can significantly outperform current counterpart approaches on various benchmark datasets.","sentences":["Graph learning is crucial in the fields of bioinformatics, social networks, and chemicals.","Although high-order graphlets, such as cycles, are critical to achieving an informative graph representation for node classification, edge prediction, and graph recognition, modeling high-order topological characteristics poses significant computational challenges, restricting its widespread applications in machine learning.","To address this limitation, we introduce the concept of \\textit{message detouring} to hierarchically characterize cycle representation throughout the entire graph, which capitalizes on the contrast between the shortest and longest pathways within a range of local topologies associated with each graph node.","The topological feature representations derived from our message detouring landscape demonstrate comparable expressive power to high-order \\textit{Weisfeiler-Lehman} (WL) tests but much less computational demands.","In addition to the integration with graph kernel and message passing neural networks, we present a novel message detouring neural network, which uses Transformer backbone to integrate cycle representations across nodes and edges.","Aside from theoretical results, experimental results on expressiveness, graph classification, and node classification show message detouring can significantly outperform current counterpart approaches on various benchmark datasets."],"url":"http://arxiv.org/abs/2402.08085v1","category":"cs.LG"}
{"created":"2024-02-12 21:48:57","title":"User Perception of Partially Automated Driving Systems: A Meaningful Human Control Perspective on the Perception among Tesla Users","abstract":"The use of partially automated driving systems raises concerns about potential responsibility issues, posing risk to the system safety, acceptance, and adoption of these technologies. The concept of meaningful human control has emerged in response to the responsibility gap problem, requiring the fulfillment of two conditions, tracking and tracing. While this concept has provided important philosophical and design insights on automated driving systems, there is currently little knowledge on how meaningful human control relates to subjective experiences of actual users of these systems. To address this gap, our study aimed to investigate the alignment between the degree of meaningful human control and drivers' perceptions of safety and trust in a real-world partially automated driving system. We utilized previously collected data from interviews with Tesla \"Full Self-Driving\" (FSD) Beta users, investigating the alignment between the user perception and how well the system was tracking the users' reasons. We found that tracking of users' reasons for driving tasks (such as safe maneuvers) correlated with perceived safety and trust, albeit with notable exceptions. Surprisingly, failure to track lane changing and braking reasons was not necessarily associated with negative perceptions of safety. However, the failure of the system to track expected maneuvers in dangerous situations always resulted in low trust and perceived lack of safety. Overall, our analyses highlight alignment points but also possible discrepancies between perceived safety and trust on the one hand, and meaningful human control on the other hand. Our results can help the developers of automated driving technology to design systems under meaningful human control and are perceived as safe and trustworthy.","sentences":["The use of partially automated driving systems raises concerns about potential responsibility issues, posing risk to the system safety, acceptance, and adoption of these technologies.","The concept of meaningful human control has emerged in response to the responsibility gap problem, requiring the fulfillment of two conditions, tracking and tracing.","While this concept has provided important philosophical and design insights on automated driving systems, there is currently little knowledge on how meaningful human control relates to subjective experiences of actual users of these systems.","To address this gap, our study aimed to investigate the alignment between the degree of meaningful human control and drivers' perceptions of safety and trust in a real-world partially automated driving system.","We utilized previously collected data from interviews with Tesla \"Full Self-Driving\" (FSD)","Beta users, investigating the alignment between the user perception and how well the system was tracking the users' reasons.","We found that tracking of users' reasons for driving tasks (such as safe maneuvers) correlated with perceived safety and trust, albeit with notable exceptions.","Surprisingly, failure to track lane changing and braking reasons was not necessarily associated with negative perceptions of safety.","However, the failure of the system to track expected maneuvers in dangerous situations always resulted in low trust and perceived lack of safety.","Overall, our analyses highlight alignment points but also possible discrepancies between perceived safety and trust on the one hand, and meaningful human control on the other hand.","Our results can help the developers of automated driving technology to design systems under meaningful human control and are perceived as safe and trustworthy."],"url":"http://arxiv.org/abs/2402.08080v1","category":"cs.HC"}
{"created":"2024-02-12 21:40:45","title":"Efficient and Scalable Fine-Tune of Language Models for Genome Understanding","abstract":"Although DNA foundation models have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data. This limitation starkly contrasts with the success of natural language foundation models, which thrive on substantially larger scales. Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust fine-tuning methods tailored for genomics. Here, we present \\textsc{Lingo}: \\textsc{L}anguage prefix f\\textsc{In}e-tuning for \\textsc{G}en\\textsc{O}mes. Unlike DNA foundation models, \\textsc{Lingo} strategically leverages natural language foundation models' contextual cues, recalibrating their linguistic knowledge to genomic sequences. \\textsc{Lingo} further accommodates numerous, heterogeneous downstream fine-tune tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors within small computational budgets. Adaptive rank sampling outperformed existing fine-tuning methods on all benchmarked 14 genome understanding tasks, while requiring fewer than 2\\% of trainable parameters as genomic-specific adapters. Impressively, applying these adapters on natural language foundation models matched or even exceeded the performance of DNA foundation models. \\textsc{Lingo} presents a new paradigm of efficient and scalable genome understanding via genomic-specific adapters on language models.","sentences":["Although DNA foundation models have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data.","This limitation starkly contrasts with the success of natural language foundation models, which thrive on substantially larger scales.","Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust fine-tuning methods tailored for genomics.","Here, we present \\textsc{Lingo}: \\textsc{L}anguage prefix f\\textsc{In}e-tuning for \\textsc{G}en\\textsc{O}mes.","Unlike DNA foundation models, \\textsc{Lingo} strategically leverages natural language foundation models' contextual cues, recalibrating their linguistic knowledge to genomic sequences.","\\textsc{Lingo} further accommodates numerous, heterogeneous downstream fine-tune tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors within small computational budgets.","Adaptive rank sampling outperformed existing fine-tuning methods on all benchmarked 14 genome understanding tasks, while requiring fewer than 2\\% of trainable parameters as genomic-specific adapters.","Impressively, applying these adapters on natural language foundation models matched or even exceeded the performance of DNA foundation models.","\\textsc{Lingo} presents a new paradigm of efficient and scalable genome understanding via genomic-specific adapters on language models."],"url":"http://arxiv.org/abs/2402.08075v1","category":"q-bio.GN"}
{"created":"2024-02-12 21:32:05","title":"Enhancing Programming Error Messages in Real Time with Generative AI","abstract":"Generative AI is changing the way that many disciplines are taught, including computer science. Researchers have shown that generative AI tools are capable of solving programming problems, writing extensive blocks of code, and explaining complex code in simple terms. Particular promise has been shown in using generative AI to enhance programming error messages. Both students and instructors have complained for decades that these messages are often cryptic and difficult to understand. Yet recent work has shown that students make fewer repeated errors when enhanced via GPT-4. We extend this work by implementing feedback from ChatGPT for all programs submitted to our automated assessment tool, Athene, providing help for compiler, run-time, and logic errors. Our results indicate that adding generative AI to an automated assessment tool does not necessarily make it better and that design of the interface matters greatly to the usability of the feedback that GPT-4 provided.","sentences":["Generative AI is changing the way that many disciplines are taught, including computer science.","Researchers have shown that generative AI tools are capable of solving programming problems, writing extensive blocks of code, and explaining complex code in simple terms.","Particular promise has been shown in using generative AI to enhance programming error messages.","Both students and instructors have complained for decades that these messages are often cryptic and difficult to understand.","Yet recent work has shown that students make fewer repeated errors when enhanced via GPT-4.","We extend this work by implementing feedback from ChatGPT for all programs submitted to our automated assessment tool, Athene, providing help for compiler, run-time, and logic errors.","Our results indicate that adding generative AI to an automated assessment tool does not necessarily make it better and that design of the interface matters greatly to the usability of the feedback that GPT-4 provided."],"url":"http://arxiv.org/abs/2402.08072v1","category":"cs.HC"}
{"created":"2024-02-12 21:14:45","title":"Beyond LLMs: Advancing the Landscape of Complex Reasoning","abstract":"Since the advent of Large Language Models a few years ago, they have often been considered the de facto solution for many AI problems. However, in addition to the many deficiencies of LLMs that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that Large Language Models perform poorly on, namely, constraint satisfaction and optimization problems. These problems are ubiquitous and current solutions are highly specialized and expensive to implement. At Elemental Cognition, we developed our EC AI platform which takes a neuro-symbolic approach to solving constraint satisfaction and optimization problems. The platform employs, at its core, a precise and high performance logical reasoning engine, and leverages LLMs for knowledge acquisition and user interaction. This platform supports developers in specifying application logic in natural and concise language while generating application user interfaces to interact with users effectively. We evaluated LLMs against systems built on the EC AI platform in three domains and found the EC AI systems to significantly outperform LLMs on constructing valid and optimal solutions, on validating proposed solutions, and on repairing invalid solutions.","sentences":["Since the advent of Large Language Models a few years ago, they have often been considered the de facto solution for many AI problems.","However, in addition to the many deficiencies of LLMs that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that Large Language Models perform poorly on, namely, constraint satisfaction and optimization problems.","These problems are ubiquitous and current solutions are highly specialized and expensive to implement.","At Elemental Cognition, we developed our EC AI platform which takes a neuro-symbolic approach to solving constraint satisfaction and optimization problems.","The platform employs, at its core, a precise and high performance logical reasoning engine, and leverages LLMs for knowledge acquisition and user interaction.","This platform supports developers in specifying application logic in natural and concise language while generating application user interfaces to interact with users effectively.","We evaluated LLMs against systems built on the EC AI platform in three domains and found the EC AI systems to significantly outperform LLMs on constructing valid and optimal solutions, on validating proposed solutions, and on repairing invalid solutions."],"url":"http://arxiv.org/abs/2402.08064v1","category":"cs.AI"}
{"created":"2024-02-12 21:12:11","title":"Avoiding Catastrophe in Continuous Spaces by Asking for Help","abstract":"Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively \"simple\" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly asks for help or is nearly guaranteed to cause catastrophe. Finally, we identify the key obstacle to generalizing our algorithm to a multi-dimensional state space.","sentences":["Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options.","This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic.","We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe.","Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe).","To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function.","We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively \"simple\" payoff function.","We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly asks for help or is nearly guaranteed to cause catastrophe.","Finally, we identify the key obstacle to generalizing our algorithm to a multi-dimensional state space."],"url":"http://arxiv.org/abs/2402.08062v1","category":"cs.LG"}
{"created":"2024-02-12 19:49:58","title":"Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking","abstract":"Large Language Model (LLM) assistants, such as ChatGPT, have emerged as potential alternatives to search methods for helping users navigate complex, feature-rich software. LLMs use vast training data from domain-specific texts, software manuals, and code repositories to mimic human-like interactions, offering tailored assistance, including step-by-step instructions. In this work, we investigated LLM-generated software guidance through a within-subject experiment with 16 participants and follow-up interviews. We compared a baseline LLM assistant with an LLM optimized for particular software contexts, SoftAIBot, which also offered guidelines for constructing appropriate prompts. We assessed task completion, perceived accuracy, relevance, and trust. Surprisingly, although SoftAIBot outperformed the baseline LLM, our results revealed no significant difference in LLM usage and user perceptions with or without prompt guidelines and the integration of domain context. Most users struggled to understand how the prompt's text related to the LLM's responses and often followed the LLM's suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the LLM's advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM's responses, indicating a gap between their lack of software expertise and their ability to evaluate the LLM's assistance. With the growing push for designing domain-specific LLM assistants, we emphasize the importance of incorporating explainable, context-aware cues into LLMs to help users understand prompt-based interactions, identify biases, and maximize the utility of LLM assistants.","sentences":["Large Language Model (LLM) assistants, such as ChatGPT, have emerged as potential alternatives to search methods for helping users navigate complex, feature-rich software.","LLMs use vast training data from domain-specific texts, software manuals, and code repositories to mimic human-like interactions, offering tailored assistance, including step-by-step instructions.","In this work, we investigated LLM-generated software guidance through a within-subject experiment with 16 participants and follow-up interviews.","We compared a baseline LLM assistant with an LLM optimized for particular software contexts, SoftAIBot, which also offered guidelines for constructing appropriate prompts.","We assessed task completion, perceived accuracy, relevance, and trust.","Surprisingly, although SoftAIBot outperformed the baseline LLM, our results revealed no significant difference in LLM usage and user perceptions with or without prompt guidelines and the integration of domain context.","Most users struggled to understand how the prompt's text related to the LLM's responses and often followed the LLM's suggestions verbatim, even if they were incorrect.","This resulted in difficulties when using the LLM's advice for software tasks, leading to low task completion rates.","Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM's responses, indicating a gap between their lack of software expertise and their ability to evaluate the LLM's assistance.","With the growing push for designing domain-specific LLM assistants, we emphasize the importance of incorporating explainable, context-aware cues into LLMs to help users understand prompt-based interactions, identify biases, and maximize the utility of LLM assistants."],"url":"http://arxiv.org/abs/2402.08030v1","category":"cs.HC"}
{"created":"2024-02-12 19:39:26","title":"UGMAE: A Unified Framework for Graph Masked Autoencoders","abstract":"Generative self-supervised learning on graphs, particularly graph masked autoencoders, has emerged as a popular learning paradigm and demonstrated its efficacy in handling non-Euclidean data. However, several remaining issues limit the capability of existing methods: 1) the disregard of uneven node significance in masking, 2) the underutilization of holistic graph information, 3) the ignorance of semantic knowledge in the representation space due to the exclusive use of reconstruction loss in the output space, and 4) the unstable reconstructions caused by the large volume of masked contents. In light of this, we propose UGMAE, a unified framework for graph masked autoencoders to address these issues from the perspectives of adaptivity, integrity, complementarity, and consistency. Specifically, we first develop an adaptive feature mask generator to account for the unique significance of nodes and sample informative masks (adaptivity). We then design a ranking-based structure reconstruction objective joint with feature reconstruction to capture holistic graph information and emphasize the topological proximity between neighbors (integrity). After that, we present a bootstrapping-based similarity module to encode the high-level semantic knowledge in the representation space, complementary to the low-level reconstruction in the output space (complementarity). Finally, we build a consistency assurance module to provide reconstruction objectives with extra stabilized consistency targets (consistency). Extensive experiments demonstrate that UGMAE outperforms both contrastive and generative state-of-the-art baselines on several tasks across multiple datasets.","sentences":["Generative self-supervised learning on graphs, particularly graph masked autoencoders, has emerged as a popular learning paradigm and demonstrated its efficacy in handling non-Euclidean data.","However, several remaining issues limit the capability of existing methods: 1) the disregard of uneven node significance in masking, 2) the underutilization of holistic graph information, 3) the ignorance of semantic knowledge in the representation space due to the exclusive use of reconstruction loss in the output space, and 4) the unstable reconstructions caused by the large volume of masked contents.","In light of this, we propose UGMAE, a unified framework for graph masked autoencoders to address these issues from the perspectives of adaptivity, integrity, complementarity, and consistency.","Specifically, we first develop an adaptive feature mask generator to account for the unique significance of nodes and sample informative masks (adaptivity).","We then design a ranking-based structure reconstruction objective joint with feature reconstruction to capture holistic graph information and emphasize the topological proximity between neighbors (integrity).","After that, we present a bootstrapping-based similarity module to encode the high-level semantic knowledge in the representation space, complementary to the low-level reconstruction in the output space (complementarity).","Finally, we build a consistency assurance module to provide reconstruction objectives with extra stabilized consistency targets (consistency).","Extensive experiments demonstrate that UGMAE outperforms both contrastive and generative state-of-the-art baselines on several tasks across multiple datasets."],"url":"http://arxiv.org/abs/2402.08023v1","category":"cs.LG"}
{"created":"2024-02-12 19:18:50","title":"Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning","abstract":"We describe the emergence of a Convolution Bottleneck (CBN) structure in CNNs, where the network uses its first few layers to transform the input representation into a representation that is supported only along a few frequencies and channels, before using the last few layers to map back to the outputs. We define the CBN rank, which describes the number and type of frequencies that are kept inside the bottleneck, and partially prove that the parameter norm required to represent a function $f$ scales as depth times the CBN rank $f$. We also show that the parameter norm depends at next order on the regularity of $f$. We show that any network with almost optimal parameter norm will exhibit a CBN structure in both the weights and - under the assumption that the network is stable under large learning rate - the activations, which motivates the common practice of down-sampling; and we verify that the CBN results still hold with down-sampling. Finally we use the CBN structure to interpret the functions learned by CNNs on a number of tasks.","sentences":["We describe the emergence of a Convolution Bottleneck (CBN) structure in CNNs, where the network uses its first few layers to transform the input representation into a representation that is supported only along a few frequencies and channels, before using the last few layers to map back to the outputs.","We define the CBN rank, which describes the number and type of frequencies that are kept inside the bottleneck, and partially prove that the parameter norm required to represent a function $f$ scales as depth times the CBN rank $f$.","We also show that the parameter norm depends at next order on the regularity of $f$. We show that any network with almost optimal parameter norm will exhibit a CBN structure in both the weights and - under the assumption that the network is stable under large learning rate - the activations, which motivates the common practice of down-sampling; and we verify that the CBN results still hold with down-sampling.","Finally we use the CBN structure to interpret the functions learned by CNNs on a number of tasks."],"url":"http://arxiv.org/abs/2402.08010v1","category":"cs.LG"}
{"created":"2024-02-12 19:02:39","title":"Distributed Compression in the Era of Machine Learning: A Review of Recent Advances","abstract":"Many applications from camera arrays to sensor networks require efficient compression and processing of correlated data, which in general is collected in a distributed fashion. While information-theoretic foundations of distributed compression are well investigated, the impact of theory in practice-oriented applications to this day has been somewhat limited. As the field of data compression is undergoing a transformation with the emergence of learning-based techniques, machine learning is becoming an important tool to reap the long-promised benefits of distributed compression. In this paper, we review the recent contributions in the broad area of learned distributed compression techniques for abstract sources and images. In particular, we discuss approaches that provide interpretable results operating close to information-theoretic bounds. We also highlight unresolved research challenges, aiming to inspire fresh interest and advancements in the field of learned distributed compression.","sentences":["Many applications from camera arrays to sensor networks require efficient compression and processing of correlated data, which in general is collected in a distributed fashion.","While information-theoretic foundations of distributed compression are well investigated, the impact of theory in practice-oriented applications to this day has been somewhat limited.","As the field of data compression is undergoing a transformation with the emergence of learning-based techniques, machine learning is becoming an important tool to reap the long-promised benefits of distributed compression.","In this paper, we review the recent contributions in the broad area of learned distributed compression techniques for abstract sources and images.","In particular, we discuss approaches that provide interpretable results operating close to information-theoretic bounds.","We also highlight unresolved research challenges, aiming to inspire fresh interest and advancements in the field of learned distributed compression."],"url":"http://arxiv.org/abs/2402.07997v1","category":"cs.IT"}
{"created":"2024-02-12 19:00:47","title":"Relaxation of weakly collisional plasma: continuous spectra, Landau eigenmodes, and transition from the collisionless to the fluid limit","abstract":"The relaxation of a weakly collisional plasma is described by the Boltzmann-Poisson equations with the Lenard-Bernstein collision operator. We perform a perturbative analysis of these equations, and obtain, for the first time, exact analytic solutions, enabling definitive resolutions to long-standing controversies regarding the impact of weak collisions on continuous spectra and Landau eigenmodes. Unlike some previous studies, we retain both damping and diffusion terms in the collision operator. We find that the linear response is a temporal convolution of a continuum that depends on the continuous velocities of particles, and discrete normal modes that encapsulate coherent oscillations. The normal modes are exponentially damped over time due to collective effects (Landau damping) as well as collisional dissipation. The continuum is also damped by collisions but somewhat differently. Up to a collision time, which is the inverse of the collision frequency $\\nu_{\\mathrm{c}}$, the continuum decay is driven by velocity diffusion and occurs super-exponentially over a timescale $\\sim \\nu^{-1/3}_{\\mathrm{c}}$. After a collision time, however, the continuum decay is driven by the collisional damping of particle velocities and diffusion of their positions, and occurs exponentially over a timescale $\\sim \\nu_{\\mathrm{c}}$. This hitherto unknown, slow exponential decay causes perturbations to damp the most on scales comparable to the mean free path, but very slowly on larger scales, which establishes the local thermal equilibrium, the essence of the fluid limit. The long-term decay of the response is driven by the normal modes on scales smaller than the mean free path, but, on larger scales, is governed by the slowly decaying continuum and the least damped normal mode. Our analysis firmly establishes a long-sought connection between the collisionless and fluid limits of weakly collisional plasmas.","sentences":["The relaxation of a weakly collisional plasma is described by the Boltzmann-Poisson equations with the Lenard-Bernstein collision operator.","We perform a perturbative analysis of these equations, and obtain, for the first time, exact analytic solutions, enabling definitive resolutions to long-standing controversies regarding the impact of weak collisions on continuous spectra and Landau eigenmodes.","Unlike some previous studies, we retain both damping and diffusion terms in the collision operator.","We find that the linear response is a temporal convolution of a continuum that depends on the continuous velocities of particles, and discrete normal modes that encapsulate coherent oscillations.","The normal modes are exponentially damped over time due to collective effects (Landau damping) as well as collisional dissipation.","The continuum is also damped by collisions but somewhat differently.","Up to a collision time, which is the inverse of the collision frequency $\\nu_{\\mathrm{c}}$, the continuum decay is driven by velocity diffusion and occurs super-exponentially over a timescale $\\sim \\nu^{-1/3}_{\\mathrm{c}}$. After a collision time, however, the continuum decay is driven by the collisional damping of particle velocities and diffusion of their positions, and occurs exponentially over a timescale $\\sim \\nu_{\\mathrm{c}}$.","This hitherto unknown, slow exponential decay causes perturbations to damp the most on scales comparable to the mean free path, but very slowly on larger scales, which establishes the local thermal equilibrium, the essence of the fluid limit.","The long-term decay of the response is driven by the normal modes on scales smaller than the mean free path, but, on larger scales, is governed by the slowly decaying continuum and the least damped normal mode.","Our analysis firmly establishes a long-sought connection between the collisionless and fluid limits of weakly collisional plasmas."],"url":"http://arxiv.org/abs/2402.07992v1","category":"physics.plasm-ph"}
{"created":"2024-02-12 19:00:03","title":"Dispelling the $\\sqrt{L}$ myth for the High-Luminosity LHC","abstract":"Extrapolations of sensitivity to new interactions and standard model parameters critically inform the programme at the Large Hadron Collider (LHC) and potential future collider cases. To this end, statistical considerations based on inclusive quantities and established analysis strategies typically give rise to a sensitivity scaling with the square root of the luminosity, $\\sqrt{L}$. This suggests only a mild sensitivity improvement for LHC's high-luminosity phase, compared to the collected data up to the year 2025. We discuss representative analyses in top quark, Higgs boson and electroweak gauge boson phenomenology and provide clear evidence that the assumption that the sensitivity in searches and measurement scales only with $\\sqrt{L}$ at the High-Luminosity LHC is overly conservative at best, and unrealistic in practice. As kinematic coverage enables more targeted search strategies with sufficient statistical sensitivity, employing a multitude of features in data dispels the scaling based on more inclusive selections.","sentences":["Extrapolations of sensitivity to new interactions and standard model parameters critically inform the programme at the Large Hadron Collider (LHC) and potential future collider cases.","To this end, statistical considerations based on inclusive quantities and established analysis strategies typically give rise to a sensitivity scaling with the square root of the luminosity, $\\sqrt{L}$. This suggests only a mild sensitivity improvement for LHC's high-luminosity phase, compared to the collected data up to the year 2025.","We discuss representative analyses in top quark, Higgs boson and electroweak gauge boson phenomenology and provide clear evidence that the assumption that the sensitivity in searches and measurement scales only with $\\sqrt{L}$ at the High-Luminosity LHC is overly conservative at best, and unrealistic in practice.","As kinematic coverage enables more targeted search strategies with sufficient statistical sensitivity, employing a multitude of features in data dispels the scaling based on more inclusive selections."],"url":"http://arxiv.org/abs/2402.07985v1","category":"hep-ph"}
{"created":"2024-02-12 11:49:42","title":"Can smartphone apps reveal fishing catch rates and durations?","abstract":"Data on angler behaviour are conventionally collected by creel surveys. An innovative, cost-effective method is the use of smartphone applications for recreational anglers. Correlations were found between these citizen-reported data and the data from creel surveys. It is, however, unclear whether angler behaviour measured from the two sources is directly related, or the citizen-reported information can be obtained mainly from other \"intermediate\" variables. We used Bayesian networks to investigate this question for two management-related quantities, daily catch rate, and fishing duration, sourced from creel surveys and the MyCatch smartphone application in two river systems in Alberta, Canada. Environmental variables and website views of the waterbodies were included as possible intermediate variables. We found direct relationships between mean catch rates from creel surveys and smartphone applications. In contrast, the daily mean fishing durations were only indirectly related to intermediate variables \"wind speed\", \"degree days\" and \"solar radiation\". The study provides insight into the potential use of citizen-reported data to understand angler behaviour on a large scale.","sentences":["Data on angler behaviour are conventionally collected by creel surveys.","An innovative, cost-effective method is the use of smartphone applications for recreational anglers.","Correlations were found between these citizen-reported data and the data from creel surveys.","It is, however, unclear whether angler behaviour measured from the two sources is directly related, or the citizen-reported information can be obtained mainly from other \"intermediate\" variables.","We used Bayesian networks to investigate this question for two management-related quantities, daily catch rate, and fishing duration, sourced from creel surveys and the MyCatch smartphone application in two river systems in Alberta, Canada.","Environmental variables and website views of the waterbodies were included as possible intermediate variables.","We found direct relationships between mean catch rates from creel surveys and smartphone applications.","In contrast, the daily mean fishing durations were only indirectly related to intermediate variables \"wind speed\", \"degree days\" and \"solar radiation\".","The study provides insight into the potential use of citizen-reported data to understand angler behaviour on a large scale."],"url":"http://arxiv.org/abs/2402.07964v1","category":"physics.soc-ph"}
{"created":"2024-02-12 10:32:47","title":"SMX: Sequential Monte Carlo Planning for Expert Iteration","abstract":"Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on hardware accelerators to optimise computing efficiency. SMX demonstrates a statistically significant improvement in performance compared to AlphaZero, as well as demonstrating its performance as an improvement operator for a model-free policy, matching or exceeding top model-free methods across both continuous and discrete environments.","sentences":["Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence.","Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms.","Yet, these methods typically face scaling challenges due to the sequential nature of their search.","While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability.","In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism.","Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings.","Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces.","Furthermore, SMX allows for high parallelisation and can run on hardware accelerators to optimise computing efficiency.","SMX demonstrates a statistically significant improvement in performance compared to AlphaZero, as well as demonstrating its performance as an improvement operator for a model-free policy, matching or exceeding top model-free methods across both continuous and discrete environments."],"url":"http://arxiv.org/abs/2402.07963v1","category":"cs.AI"}
{"created":"2024-02-13 18:57:56","title":"Striped electronic phases in an incommensurately modulated van der Waals superlattice","abstract":"Electronic properties of crystals can be manipulated using spatially periodic modulations. Long-wavelength, incommensurate modulations are of particular interest, exemplified recently by moir\\'e patterned van der Waals (vdW) heterostructures. Bulk vdW superlattices hosting interfaces between clean 2D layers represent scalable bulk analogs of vdW heterostructures and present a complementary venue to explore incommensurately modulated 2D states. Here we report the bulk vdW superlattice SrTa$_2$S$_5$ realizing an incommensurate 1D modulation of 2D transition metal dichalcogenide (TMD) $H$-TaS$_2$ layers. High-quality electronic transport in the $H$-TaS$_2$ layers, evidenced by quantum oscillations, is made anisotropic by the modulation and shows commensurability oscillations akin to lithographically modulated 2D systems. We also find unconventional, clean-limit superconductivity (SC) in SrTa$_2$S$_5$ with a pronounced suppression of interlayer coherence relative to intralayer coherence. Such a hierarchy can arise from pair-density wave (PDW) SC with mismatched spatial arrangement in adjacent superconducting layers. Examining the in-plane magnetic field $H_{ab}$ dependence of interlayer critical current density $J_c$, we find anisotropy with respect to $H_{ab}$ orientation: $J_c$ is maximized (minimized) when $H_{ab}$ is perpendicular (parallel) to the stripes, consistent with 1D PDW SC. From diffraction we find the structural modulation is shifted between adjacent $H$-TaS$_2$ layers, suggesting mismatched 1D PDW is seeded by the striped structure. With a high-mobility Fermi liquid in a coherently modulated structure, SrTa$_2$S$_5$ is a promising host for novel phenomena anticipated in clean, striped metals and superconductors. More broadly, SrTa$_2$S$_5$ establishes bulk vdW superlattices as macroscopic platforms to address long-standing predictions for modulated electronic phases.","sentences":["Electronic properties of crystals can be manipulated using spatially periodic modulations.","Long-wavelength, incommensurate modulations are of particular interest, exemplified recently by moir\\'e patterned van der Waals (vdW) heterostructures.","Bulk vdW superlattices hosting interfaces between clean 2D layers represent scalable bulk analogs of vdW heterostructures and present a complementary venue to explore incommensurately modulated 2D states.","Here we report the bulk vdW superlattice SrTa$_2$S$_5$ realizing an incommensurate 1D modulation of 2D transition metal dichalcogenide (TMD) $H$-TaS$_2$ layers.","High-quality electronic transport in the $H$-TaS$_2$ layers, evidenced by quantum oscillations, is made anisotropic by the modulation and shows commensurability oscillations akin to lithographically modulated 2D systems.","We also find unconventional, clean-limit superconductivity (SC) in SrTa$_2$S$_5$ with a pronounced suppression of interlayer coherence relative to intralayer coherence.","Such a hierarchy can arise from pair-density wave (PDW) SC with mismatched spatial arrangement in adjacent superconducting layers.","Examining the in-plane magnetic field $H_{ab}$ dependence of interlayer critical current density $J_c$, we find anisotropy with respect to $H_{ab}$ orientation: $J_c$ is maximized (minimized) when $H_{ab}$ is perpendicular (parallel) to the stripes, consistent with 1D PDW SC.","From diffraction we find the structural modulation is shifted between adjacent $H$-TaS$_2$ layers, suggesting mismatched 1D PDW is seeded by the striped structure.","With a high-mobility Fermi liquid in a coherently modulated structure, SrTa$_2$S$_5$ is a promising host for novel phenomena anticipated in clean, striped metals and superconductors.","More broadly, SrTa$_2$S$_5$ establishes bulk vdW superlattices as macroscopic platforms to address long-standing predictions for modulated electronic phases."],"url":"http://arxiv.org/abs/2402.08677v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-13 18:55:41","title":"Coarse-Graining in Space versus Time","abstract":"Understanding the structure and dynamics of liquids is pivotal for the study of larger spatiotemporal processes, particularly for glass-forming materials at low temperatures. The so-called thermodynamic scaling relation, validated for many molecular systems through experiments, offers an efficient means to explore a vast range of time scales along a one-dimensional phase diagram. Isomorph theory provides a theoretical framework for thermodynamic scaling based on strong virial-potential energy correlations, but this approach is most successful for simple point particles. In particular, isomorph theory has resisted extension to complex molecular liquids due to the existence of high-frequency intramolecular interactions. To elucidate the microscopic origin of density scaling for molecular systems, we employ two distinct approaches for coarse-graining in space or in time. The former eliminates fast degrees of freedom by reducing a molecule to a center-of-mass-level description, while the latter involves temporally averaged fluctuations or correlation functions over the characteristic time scale. We show that both approaches yield a consistent density scaling coefficient for ortho-terphenyl, which is moreover in agreement with the experimental value. Building upon these findings, we derive the density scaling relationship exhibiting a single-parameter phase diagram from fully atomistic simulations. Our results unravel the microscopic nature underlying thermodynamic scaling and shed light on the role of coarse-graining for assessing the slow fluctuations in molecular systems, ultimately enabling the extension of systematic bottom-up approaches to larger and more complex molecular liquids that are experimentally challenging to probe.","sentences":["Understanding the structure and dynamics of liquids is pivotal for the study of larger spatiotemporal processes, particularly for glass-forming materials at low temperatures.","The so-called thermodynamic scaling relation, validated for many molecular systems through experiments, offers an efficient means to explore a vast range of time scales along a one-dimensional phase diagram.","Isomorph theory provides a theoretical framework for thermodynamic scaling based on strong virial-potential energy correlations, but this approach is most successful for simple point particles.","In particular, isomorph theory has resisted extension to complex molecular liquids due to the existence of high-frequency intramolecular interactions.","To elucidate the microscopic origin of density scaling for molecular systems, we employ two distinct approaches for coarse-graining in space or in time.","The former eliminates fast degrees of freedom by reducing a molecule to a center-of-mass-level description, while the latter involves temporally averaged fluctuations or correlation functions over the characteristic time scale.","We show that both approaches yield a consistent density scaling coefficient for ortho-terphenyl, which is moreover in agreement with the experimental value.","Building upon these findings, we derive the density scaling relationship exhibiting a single-parameter phase diagram from fully atomistic simulations.","Our results unravel the microscopic nature underlying thermodynamic scaling and shed light on the role of coarse-graining for assessing the slow fluctuations in molecular systems, ultimately enabling the extension of systematic bottom-up approaches to larger and more complex molecular liquids that are experimentally challenging to probe."],"url":"http://arxiv.org/abs/2402.08675v1","category":"cond-mat.soft"}
{"created":"2024-02-13 18:39:18","title":"PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs","abstract":"Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding. While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not using any supervised detection data. To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spatial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons.","sentences":["Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems.","Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding.","While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models.","In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not using any supervised detection data.","To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spatial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities.","Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads.","Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons."],"url":"http://arxiv.org/abs/2402.08657v1","category":"cs.CV"}
{"created":"2024-02-13 17:38:09","title":"An integral renewal equation approach to behavioural epidemic models with information index","abstract":"We propose an integral model describing an epidemic of an infectious disease. The model is behavioural in the sense that the constitutive law for the force of infection includes a distributed delay, called \"information index\", that describes the opinion-driven human behavioural changes. The information index, in turn, contains a memory kernel to mimic how the individuals maintain memory of the past values of the infection. We obtain sufficient conditions for the endemic equilibrium to be locally stable. In particular, we show that when the infectivity function is represented by an exponential distribution, stability is guaranteed by the weak Erlang memory kernel. However, through numerical simulations, we show that self-sustained oscillations may arise when the memory is more focused in the disease's past history, as exemplified by the strong Erlang kernel. We also show the model solutions in cases of different infectivity functions describing infectious diseases like influenza and SARS.","sentences":["We propose an integral model describing an epidemic of an infectious disease.","The model is behavioural in the sense that the constitutive law for the force of infection includes a distributed delay, called \"information index\", that describes the opinion-driven human behavioural changes.","The information index, in turn, contains a memory kernel to mimic how the individuals maintain memory of the past values of the infection.","We obtain sufficient conditions for the endemic equilibrium to be locally stable.","In particular, we show that when the infectivity function is represented by an exponential distribution, stability is guaranteed by the weak Erlang memory kernel.","However, through numerical simulations, we show that self-sustained oscillations may arise when the memory is more focused in the disease's past history, as exemplified by the strong Erlang kernel.","We also show the model solutions in cases of different infectivity functions describing infectious diseases like influenza and SARS."],"url":"http://arxiv.org/abs/2402.08618v1","category":"math.DS"}
{"created":"2024-02-13 17:36:18","title":"Demystifying Quantum Power Flow: Unveiling the Limits of Practical Quantum Advantage","abstract":"Quantum computers hold promise for solving problems intractable for classical computers, especially those with high time and/or space complexity. The reduction of the power flow (PF) problem into a linear system of equations, allows formulation of quantum power flow (QPF) algorithms, based on quantum linear system solving methods such as the Harrow-Hassidim-Lloyd (HHL) algorithm. The speedup due to QPF algorithms is claimed to be exponential when compared to classical PF solved by state-of-the-art algorithms. We investigate the potential for practical quantum advantage (PQA) in solving QPF compared to classical methods on gate-based quantum computers. We meticulously scrutinize the end-to-end complexity of QPF, providing a nuanced evaluation of the purported quantum speedup in this problem. Our analysis establishes a best-case bound for the HHL-QPF complexity, conclusively demonstrating the absence of any PQA in the direct current power flow (DCPF) and fast decoupled load flow (FDLF) problem. Additionally, we establish that for potential PQA to exist it is necessary to consider DCPF-type problems with a very narrow range of condition number values and readout requirements.","sentences":["Quantum computers hold promise for solving problems intractable for classical computers, especially those with high time and/or space complexity.","The reduction of the power flow (PF) problem into a linear system of equations, allows formulation of quantum power flow (QPF) algorithms, based on quantum linear system solving methods such as the Harrow-Hassidim-Lloyd (HHL) algorithm.","The speedup due to QPF algorithms is claimed to be exponential when compared to classical PF solved by state-of-the-art algorithms.","We investigate the potential for practical quantum advantage (PQA) in solving QPF compared to classical methods on gate-based quantum computers.","We meticulously scrutinize the end-to-end complexity of QPF, providing a nuanced evaluation of the purported quantum speedup in this problem.","Our analysis establishes a best-case bound for the HHL-QPF complexity, conclusively demonstrating the absence of any PQA in the direct current power flow (DCPF) and fast decoupled load flow (FDLF) problem.","Additionally, we establish that for potential PQA to exist it is necessary to consider DCPF-type problems with a very narrow range of condition number values and readout requirements."],"url":"http://arxiv.org/abs/2402.08617v1","category":"quant-ph"}
{"created":"2024-02-13 17:18:12","title":"Evidence Tetris in the Pixelated World of Validity Threats","abstract":"Valid empirical studies build confidence in scientific findings. Fortunately, it is now common for software engineering researchers to consider threats to validity when designing their studies and to discuss them as part of their publication. Yet, in complex experiments with human participants, there is often an overwhelming number of intuitively plausible threats to validity -- more than a researcher can feasibly cover. Therefore, prioritizing potential threats to validity becomes crucial. We suggest moving away from relying solely on intuition for prioritizing validity threats, and propose that evidence on the actual impact of suspected threats to validity should complement intuition.","sentences":["Valid empirical studies build confidence in scientific findings.","Fortunately, it is now common for software engineering researchers to consider threats to validity when designing their studies and to discuss them as part of their publication.","Yet, in complex experiments with human participants, there is often an overwhelming number of intuitively plausible threats to validity -- more than a researcher can feasibly cover.","Therefore, prioritizing potential threats to validity becomes crucial.","We suggest moving away from relying solely on intuition for prioritizing validity threats, and propose that evidence on the actual impact of suspected threats to validity should complement intuition."],"url":"http://arxiv.org/abs/2402.08608v1","category":"cs.SE"}
{"created":"2024-02-13 17:11:30","title":"Measurement induced phase transitions in quantum raise and peel models","abstract":"We present a quantum circuit model which emulates the interface growth of the classical raise-and-peel model. Our model consists of Clifford unitary gates interspersed with projective measurements, applied according to prescribed feedback rules. We numerically find via large-scale simulations that, depending on the feedback rules, the system may undergo several measurement-induced entanglement transitions, including continuous transitions within a universality class not previously observed in hybrid random Clifford systems as well as a first-order transition.","sentences":["We present a quantum circuit model which emulates the interface growth of the classical raise-and-peel model.","Our model consists of Clifford unitary gates interspersed with projective measurements, applied according to prescribed feedback rules.","We numerically find via large-scale simulations that, depending on the feedback rules, the system may undergo several measurement-induced entanglement transitions, including continuous transitions within a universality class not previously observed in hybrid random Clifford systems as well as a first-order transition."],"url":"http://arxiv.org/abs/2402.08605v1","category":"quant-ph"}
{"created":"2024-02-13 17:08:35","title":"Latent Inversion with Timestep-aware Sampling for Training-free Non-rigid Editing","abstract":"Text-guided non-rigid editing involves complex edits for input images, such as changing motion or compositions within their surroundings. Since it requires manipulating the input structure, existing methods often struggle with preserving object identity and background, particularly when combined with Stable Diffusion. In this work, we propose a training-free approach for non-rigid editing with Stable Diffusion, aimed at improving the identity preservation quality without compromising editability. Our approach comprises three stages: text optimization, latent inversion, and timestep-aware text injection sampling. Inspired by the recent success of Imagic, we employ their text optimization for smooth editing. Then, we introduce latent inversion to preserve the input image's identity without additional model fine-tuning. To fully utilize the input reconstruction ability of latent inversion, we suggest timestep-aware text inject sampling. This effectively retains the structure of the input image by injecting the source text prompt in early sampling steps and then transitioning to the target prompt in subsequent sampling steps. This strategic approach seamlessly harmonizes with text optimization, facilitating complex non-rigid edits to the input without losing the original identity. We demonstrate the effectiveness of our method in terms of identity preservation, editability, and aesthetic quality through extensive experiments.","sentences":["Text-guided non-rigid editing involves complex edits for input images, such as changing motion or compositions within their surroundings.","Since it requires manipulating the input structure, existing methods often struggle with preserving object identity and background, particularly when combined with Stable Diffusion.","In this work, we propose a training-free approach for non-rigid editing with Stable Diffusion, aimed at improving the identity preservation quality without compromising editability.","Our approach comprises three stages: text optimization, latent inversion, and timestep-aware text injection sampling.","Inspired by the recent success of Imagic, we employ their text optimization for smooth editing.","Then, we introduce latent inversion to preserve the input image's identity without additional model fine-tuning.","To fully utilize the input reconstruction ability of latent inversion, we suggest timestep-aware text inject sampling.","This effectively retains the structure of the input image by injecting the source text prompt in early sampling steps and then transitioning to the target prompt in subsequent sampling steps.","This strategic approach seamlessly harmonizes with text optimization, facilitating complex non-rigid edits to the input without losing the original identity.","We demonstrate the effectiveness of our method in terms of identity preservation, editability, and aesthetic quality through extensive experiments."],"url":"http://arxiv.org/abs/2402.08601v1","category":"cs.CV"}
{"created":"2024-02-13 16:57:06","title":"Homomorphism Counts for Graph Neural Networks: All About That Basis","abstract":"Graph neural networks are architectures for learning invariant functions over graphs. A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the \"basis\" of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical results on node-level and graph-level motif parameters and empirically validate them on standard benchmark datasets.","sentences":["Graph neural networks are architectures for learning invariant functions over graphs.","A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power.","Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns.","Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts.","In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the \"basis\" of the target pattern.","This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches.","We prove a series of theoretical results on node-level and graph-level motif parameters and empirically validate them on standard benchmark datasets."],"url":"http://arxiv.org/abs/2402.08595v1","category":"cs.LG"}
{"created":"2024-02-13 16:41:40","title":"Totally geodesic submanifolds and polar actions on Stiefel manifolds","abstract":"We classify totally geodesic submanifolds of the real Stiefel manifolds of orthogonal two-frames. We also classify polar actions on these Stiefel manifolds, specifically, we prove that the orbits of polar actions are lifts of polar actions on the corresponding Grassmannian. In the case of cohomogeneity-one actions we are able to obtain a classification for all real, complex and quaternionic Stiefel manifolds of $k$-frames.","sentences":["We classify totally geodesic submanifolds of the real Stiefel manifolds of orthogonal two-frames.","We also classify polar actions on these Stiefel manifolds, specifically, we prove that the orbits of polar actions are lifts of polar actions on the corresponding Grassmannian.","In the case of cohomogeneity-one actions we are able to obtain a classification for all real, complex and quaternionic Stiefel manifolds of $k$-frames."],"url":"http://arxiv.org/abs/2402.08585v1","category":"math.DG"}
{"created":"2024-02-13 16:21:36","title":"Semiclassical asymptotics of the Bloch--Torrey operator in two dimensions","abstract":"The Bloch--Torrey operator $-h^2\\Delta+e^{i\\alpha}x_1$ on a bounded smooth planar domain, subject to Dirichlet boundary conditions, is analyzed. Assuming $\\alpha\\in\\left[0,\\frac{3\\pi}{5}\\right)$ and a non-degeneracy assumption on the left-hand side of the domain, asymptotics of the eigenvalues with the smallest real part in the limit $h \\to 0$ are derived. The strategy is a backward complex scaling and the reduction to a tensorized operator involving a real Airy operator and a complex harmonic oscillator.","sentences":["The Bloch--Torrey operator $-h^2\\Delta+e^{i\\alpha}x_1$ on a bounded smooth planar domain, subject to Dirichlet boundary conditions, is analyzed.","Assuming $\\alpha\\in\\left[0,\\frac{3\\pi}{5}\\right)$ and a non-degeneracy assumption on the left-hand side of the domain, asymptotics of the eigenvalues with the smallest real part in the limit $h \\to 0$ are derived.","The strategy is a backward complex scaling and the reduction to a tensorized operator involving a real Airy operator and a complex harmonic oscillator."],"url":"http://arxiv.org/abs/2402.08574v1","category":"math.SP"}
{"created":"2024-02-13 16:06:17","title":"Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast","abstract":"A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at https://sail-sg.github.io/Agent-Smith/.","sentences":["A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use.","Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors.","In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak.","It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors.","To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction.","Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak.","Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate.","Our project page is available at https://sail-sg.github.io/Agent-Smith/."],"url":"http://arxiv.org/abs/2402.08567v1","category":"cs.CL"}
{"created":"2024-02-13 16:05:54","title":"Gaussian-Sum Filter for Range-based 3D Relative Pose Estimation in the Presence of Ambiguities","abstract":"Multi-robot systems must have the ability to accurately estimate relative states between robots in order to perform collaborative tasks, possibly with no external aiding. Three-dimensional relative pose estimation using range measurements oftentimes suffers from a finite number of non-unique solutions, or ambiguities. This paper: 1) identifies and accurately estimates all possible ambiguities in 2D; 2) treats them as components of a Gaussian mixture model; and 3) presents a computationally-efficient estimator, in the form of a Gaussian-sum filter (GSF), to realize range-based relative pose estimation in an infrastructure-free, 3D, setup. This estimator is evaluated in simulation and experiment and is shown to avoid divergence to local minima induced by the ambiguous poses. Furthermore, the proposed GSF outperforms an extended Kalman filter, demonstrates similar performance to the computationally-demanding particle filter, and is shown to be consistent.","sentences":["Multi-robot systems must have the ability to accurately estimate relative states between robots in order to perform collaborative tasks, possibly with no external aiding.","Three-dimensional relative pose estimation using range measurements oftentimes suffers from a finite number of non-unique solutions, or ambiguities.","This paper: 1) identifies and accurately estimates all possible ambiguities in 2D; 2) treats them as components of a Gaussian mixture model; and 3) presents a computationally-efficient estimator, in the form of a Gaussian-sum filter (GSF), to realize range-based relative pose estimation in an infrastructure-free, 3D, setup.","This estimator is evaluated in simulation and experiment and is shown to avoid divergence to local minima induced by the ambiguous poses.","Furthermore, the proposed GSF outperforms an extended Kalman filter, demonstrates similar performance to the computationally-demanding particle filter, and is shown to be consistent."],"url":"http://arxiv.org/abs/2402.08566v1","category":"cs.RO"}
{"created":"2024-02-13 15:58:45","title":"Real-space analysis of Hatsugai-Kohmoto interaction","abstract":"The Hatsugai-Kohmoto interaction model has gained a lot of attention in recent years, due to the fact it is exactly solvable in momentum space in any dimension while capturing some key features of the Mott phase. Here a one-dimensional lattice model with this interaction is approached from the real-space perspective, to explore how breaking the translation invariance of a lattice affects the intuition built by studying the exact solution in $k$-space. The ground state properties of chains with periodic and open boundary conditions are calculated and compared with both the exact solution in momentum space, as well as with analogous solutions of the Hubbard model. The results show that introducing hard edges enhances the ferromagnetic correlations and the system undergoes a magnetic transition before reaching the strong coupling limit. Understanding the impact of hard edges is a crucial step toward answering the looming question of the existence of edge states and other topological phenomena in systems with this type of interaction.","sentences":["The Hatsugai-Kohmoto interaction model has gained a lot of attention in recent years, due to the fact it is exactly solvable in momentum space in any dimension while capturing some key features of the Mott phase.","Here a one-dimensional lattice model with this interaction is approached from the real-space perspective, to explore how breaking the translation invariance of a lattice affects the intuition built by studying the exact solution in $k$-space.","The ground state properties of chains with periodic and open boundary conditions are calculated and compared with both the exact solution in momentum space, as well as with analogous solutions of the Hubbard model.","The results show that introducing hard edges enhances the ferromagnetic correlations and the system undergoes a magnetic transition before reaching the strong coupling limit.","Understanding the impact of hard edges is a crucial step toward answering the looming question of the existence of edge states and other topological phenomena in systems with this type of interaction."],"url":"http://arxiv.org/abs/2402.08557v1","category":"cond-mat.str-el"}
{"created":"2024-02-13 15:57:47","title":"Tensor network noise characterization for near-term quantum computers","abstract":"Characterization of noise in current near-term quantum devices is of paramount importance to fully use their computational power. However, direct quantum process tomography becomes unfeasible for systems composed of tens of qubits. A promising alternative method based on tensor networks was recently proposed [Nat Commun 14, 2858 (2023)]. In this work, we adapt it for the characterization of noise channels on near-term quantum computers and investigate its performance thoroughly. In particular, we show how experimentally feasible tomographic samples are sufficient to accurately characterize realistic correlated noise models affecting individual layers of quantum circuits, and study its performance on systems composed of up to 20 qubits. Furthermore, we combine this noise characterization method with a recently proposed noise-aware tensor network error mitigation protocol for correcting outcomes in noisy circuits, resulting accurate estimations even on deep circuit instances. This positions the tensor-network-based noise characterization protocol as a valuable tool for practical error characterization and mitigation in the near-term quantum computing era.","sentences":["Characterization of noise in current near-term quantum devices is of paramount importance to fully use their computational power.","However, direct quantum process tomography becomes unfeasible for systems composed of tens of qubits.","A promising alternative method based on tensor networks was recently proposed","[Nat Commun 14, 2858 (2023)].","In this work, we adapt it for the characterization of noise channels on near-term quantum computers and investigate its performance thoroughly.","In particular, we show how experimentally feasible tomographic samples are sufficient to accurately characterize realistic correlated noise models affecting individual layers of quantum circuits, and study its performance on systems composed of up to 20 qubits.","Furthermore, we combine this noise characterization method with a recently proposed noise-aware tensor network error mitigation protocol for correcting outcomes in noisy circuits, resulting accurate estimations even on deep circuit instances.","This positions the tensor-network-based noise characterization protocol as a valuable tool for practical error characterization and mitigation in the near-term quantum computing era."],"url":"http://arxiv.org/abs/2402.08556v1","category":"quant-ph"}
{"created":"2024-02-13 15:55:39","title":"Nonstabilizerness of permutationally invariant systems","abstract":"Typical measures of nonstabilizerness of a system of $N$ qubits require computing $4^N$ expectation values, one for each Pauli string in the Pauli group, over a state of dimension $2^N$. In this Letter, we show that, if the system is invariant under permutations, this resource overhead can be exponentially decreased to $O(N^3)$ expectation values on a state with a dimension $O(N)$, allowing for a polynomial-time evaluation of the nonstabilizerness up to hundreds of qubits.","sentences":["Typical measures of nonstabilizerness of a system of $N$ qubits require computing $4^N$ expectation values, one for each Pauli string in the Pauli group, over a state of dimension $2^N$. In this Letter, we show that, if the system is invariant under permutations, this resource overhead can be exponentially decreased to $O(N^3)$ expectation values on a state with a dimension $O(N)$, allowing for a polynomial-time evaluation of the nonstabilizerness up to hundreds of qubits."],"url":"http://arxiv.org/abs/2402.08551v1","category":"quant-ph"}
{"created":"2024-02-13 15:51:58","title":"Grounding LLMs For Robot Task Planning Using Closed-loop State Feedback","abstract":"Robotic planning algorithms direct agents to perform actions within diverse environments to accomplish a task. Large Language Models (LLMs) like PaLM 2, GPT-3.5, and GPT-4 have revolutionized this domain, using their embedded real-world knowledge to tackle complex tasks involving multiple agents and objects. This paper introduces an innovative planning algorithm that integrates LLMs into the robotics context, enhancing task-focused execution and success rates. Key to our algorithm is a closed-loop feedback which provides real-time environmental states and error messages, crucial for refining plans when discrepancies arise. The algorithm draws inspiration from the human neural system, emulating its brain-body architecture by dividing planning across two LLMs in a structured, hierarchical fashion. Our method not only surpasses baselines within the VirtualHome Environment, registering a notable 35% average increase in task-oriented success rates, but achieves an impressive execution score of 85%, approaching the human-level benchmark of 94%. Moreover, effectiveness of the algorithm in real robot scenarios is shown using a realistic physics simulator and the Franka Research 3 Arm.","sentences":["Robotic planning algorithms direct agents to perform actions within diverse environments to accomplish a task.","Large Language Models (LLMs) like PaLM 2, GPT-3.5, and GPT-4 have revolutionized this domain, using their embedded real-world knowledge to tackle complex tasks involving multiple agents and objects.","This paper introduces an innovative planning algorithm that integrates LLMs into the robotics context, enhancing task-focused execution and success rates.","Key to our algorithm is a closed-loop feedback which provides real-time environmental states and error messages, crucial for refining plans when discrepancies arise.","The algorithm draws inspiration from the human neural system, emulating its brain-body architecture by dividing planning across two LLMs in a structured, hierarchical fashion.","Our method not only surpasses baselines within the VirtualHome Environment, registering a notable 35% average increase in task-oriented success rates, but achieves an impressive execution score of 85%, approaching the human-level benchmark of 94%.","Moreover, effectiveness of the algorithm in real robot scenarios is shown using a realistic physics simulator and the Franka Research 3 Arm."],"url":"http://arxiv.org/abs/2402.08546v1","category":"cs.RO"}
{"created":"2024-02-13 15:42:40","title":"Self-Induced Superradiant Masing","abstract":"We study superradiant masing in a hybrid system composed of nitrogen-vacancy center spins in diamond coupled to a superconducting microwave cavity. After the first fast superradiant decay we observe transient pulsed and then quasi-continuous masing. This emission dynamics can be described by a phenomenological model incorporating the transfer of inverted spin excitations into the superradiant window of spins resonant with the cavity. After experimentally excluding cQED effects associated with the pumping of the masing transition we conjecture that direct higher-order spin-spin interactions are responsible for creating the dynamics and the transition to the sustained masing. Our experiment thus opens up a novel way to explore many-body physics in disordered systems through cQED and superradiance.","sentences":["We study superradiant masing in a hybrid system composed of nitrogen-vacancy center spins in diamond coupled to a superconducting microwave cavity.","After the first fast superradiant decay we observe transient pulsed and then quasi-continuous masing.","This emission dynamics can be described by a phenomenological model incorporating the transfer of inverted spin excitations into the superradiant window of spins resonant with the cavity.","After experimentally excluding cQED effects associated with the pumping of the masing transition we conjecture that direct higher-order spin-spin interactions are responsible for creating the dynamics and the transition to the sustained masing.","Our experiment thus opens up a novel way to explore many-body physics in disordered systems through cQED and superradiance."],"url":"http://arxiv.org/abs/2402.08537v1","category":"quant-ph"}
{"created":"2024-02-13 15:36:29","title":"Galaxies decomposition with spiral arms -- II: A multiwavelength case study of M 51","abstract":"Spiral structure can contribute significantly to a galaxy's luminosity. However, only rarely are proper photometric models of spiral arms used in decompositions. As we show in the previous work, including the spirals as a separate component in a photometric model of a galaxy would both allow to obtain their structural parameters, and reduce the systematic errors in estimating the parameters of other components. Doing so in different wavebands, one can explore how their properties vary with the wavelength. In this paper, second in this series, we perform decomposition of M 51 in 17 bands, from the far UV to far IR, using imaging from the DustPedia project. We use the same 2D photometric model of spiral structure where each arm is modelled independently. The complex and asymmetric spiral structure in M 51 is reproduced relatively well with our model. We analyze the differences between models with and without spiral arms, and investigate how the fit parameters change with wavelength. In particular, we find that the spiral arms demonstrate the largest width in the optical, whereas their contribution to the galaxy luminosity is most significant in the UV. The disk central intensity drops by a factor of 1.25--3 and its exponential scale changes by 5--10\\% when spiral arms are included, depending on wavelength. Taking into account the full light distribution across the arms, we do not observe the signs of a long-lived density wave in the spiral pattern of M 51 as a whole.","sentences":["Spiral structure can contribute significantly to a galaxy's luminosity.","However, only rarely are proper photometric models of spiral arms used in decompositions.","As we show in the previous work, including the spirals as a separate component in a photometric model of a galaxy would both allow to obtain their structural parameters, and reduce the systematic errors in estimating the parameters of other components.","Doing so in different wavebands, one can explore how their properties vary with the wavelength.","In this paper, second in this series, we perform decomposition of M 51 in 17 bands, from the far UV to far IR, using imaging from the DustPedia project.","We use the same 2D photometric model of spiral structure where each arm is modelled independently.","The complex and asymmetric spiral structure in M 51 is reproduced relatively well with our model.","We analyze the differences between models with and without spiral arms, and investigate how the fit parameters change with wavelength.","In particular, we find that the spiral arms demonstrate the largest width in the optical, whereas their contribution to the galaxy luminosity is most significant in the UV.","The disk central intensity drops by a factor of 1.25--3 and its exponential scale changes by 5--10\\% when spiral arms are included, depending on wavelength.","Taking into account the full light distribution across the arms, we do not observe the signs of a long-lived density wave in the spiral pattern of M 51 as a whole."],"url":"http://arxiv.org/abs/2402.08531v1","category":"astro-ph.GA"}
{"created":"2024-02-13 15:27:30","title":"A new framework for calibrating COVID-19 SEIR models with spatial-/time-varying coefficients using genetic and sliding window algorithms","abstract":"A susceptible-exposed-infected-removed (SEIR) model assumes spatial-/time-varying coefficients to model the effect of non-pharmaceutical interventions (NPIs) on the regional and temporal distribution of COVID-19 disease epidemics. A significant challenge in using such model is their fast and accurate calibration to observed data from geo-referenced hospitalized data, i.e., efficient estimation of the spatial-/time-varying parameters. In this work, a new calibration framework is proposed towards optimizing the spatial-/time-varying parameters of the SEIR model. We also devise a method for combing the overlapping sliding window technique (OSW) with a genetic algorithm (GA) calibration routine to automatically search the segmented parameter space. Parallelized GA is used to reduce the computational burden. Our framework abstracts the implementation complexity of the method away from the user. It provides high-level APIs for setting up a customized calibration system and consuming the optimized values of parameters. We evaluated the application of our method on the calibration of a spatial age-structured microsimulation model using a single objective function that comprises observed COVID-19-related ICU demand. The results reflect the effectiveness of the proposed method towards estimating the parameters in a changing environment.","sentences":["A susceptible-exposed-infected-removed (SEIR) model assumes spatial-/time-varying coefficients to model the effect of non-pharmaceutical interventions (NPIs) on the regional and temporal distribution of COVID-19 disease epidemics.","A significant challenge in using such model is their fast and accurate calibration to observed data from geo-referenced hospitalized data, i.e., efficient estimation of the spatial-/time-varying parameters.","In this work, a new calibration framework is proposed towards optimizing the spatial-/time-varying parameters of the SEIR model.","We also devise a method for combing the overlapping sliding window technique (OSW) with a genetic algorithm (GA) calibration routine to automatically search the segmented parameter space.","Parallelized GA is used to reduce the computational burden.","Our framework abstracts the implementation complexity of the method away from the user.","It provides high-level APIs for setting up a customized calibration system and consuming the optimized values of parameters.","We evaluated the application of our method on the calibration of a spatial age-structured microsimulation model using a single objective function that comprises observed COVID-19-related ICU demand.","The results reflect the effectiveness of the proposed method towards estimating the parameters in a changing environment."],"url":"http://arxiv.org/abs/2402.08524v1","category":"cs.DC"}
{"created":"2024-02-13 15:02:55","title":"Particle-hole instabilities in photonic time-varying systems","abstract":"Photonic systems with time-varying modulations have attracted considerable attention as they allow for the design of non-reciprocal devices without the need for an external magnetic bias. Unlike time-invariant systems, such modulations couple modes with different frequencies. Here, we discuss how this coupling and particle-hole symmetry may lead to the resonant interaction of positive and negative frequency oscillators. To illustrate this idea, we analyze a dispersive spacetime crystal described by a Drude-Lorentz model with a travelingwave modulation. Our findings demonstrate that the interaction between positive and negative frequency bands can induce parametric instabilities under certain conditions, stemming from the interplay between dispersion and spacetime modulations. In particular, we find that material dispersion creates the conditions for the formation of instabilities for arbitrarily small modulations speeds in the absence of dissipative channels.","sentences":["Photonic systems with time-varying modulations have attracted considerable attention as they allow for the design of non-reciprocal devices without the need for an external magnetic bias.","Unlike time-invariant systems, such modulations couple modes with different frequencies.","Here, we discuss how this coupling and particle-hole symmetry may lead to the resonant interaction of positive and negative frequency oscillators.","To illustrate this idea, we analyze a dispersive spacetime crystal described by a Drude-Lorentz model with a travelingwave modulation.","Our findings demonstrate that the interaction between positive and negative frequency bands can induce parametric instabilities under certain conditions, stemming from the interplay between dispersion and spacetime modulations.","In particular, we find that material dispersion creates the conditions for the formation of instabilities for arbitrarily small modulations speeds in the absence of dissipative channels."],"url":"http://arxiv.org/abs/2402.08507v1","category":"physics.optics"}
{"created":"2024-02-13 15:02:46","title":"P-Mamba: Marrying Perona Malik Diffusion with Mamba for Efficient Pediatric Echocardiographic Left Ventricular Segmentation","abstract":"In pediatric cardiology, the accurate and immediate assessment of cardiac function through echocardiography is important since it can determine whether urgent intervention is required in many emergencies. However, echocardiography is characterized by ambiguity and heavy background noise interference, bringing more difficulty to accurate segmentation. Present methods lack efficiency and are also prone to mistakenly segmenting some background noise areas as the left ventricular area due to noise disturbance. To relieve the two issues, we introduce P-Mamba for efficient pediatric echocardiographic left ventricular segmentation. Specifically, we turn to the recently proposed vision mamba layers in our vision mamba encoder branch to improve the computing and memory efficiency of our model while modeling global dependencies. In the other DWT-based PMD encoder branch, we devise DWT-based Perona-Malik Diffusion (PMD) Blocks that utilize PMD for noise suppression, while simultaneously preserving the local shape cues of the left ventricle. Leveraging the strengths of both the two encoder branches, P-Mamba achieves superior accuracy and efficiency to established models, such as vision transformers with quadratic and linear computational complexity. This innovative approach promises significant advancements in pediatric cardiac imaging and beyond.","sentences":["In pediatric cardiology, the accurate and immediate assessment of cardiac function through echocardiography is important since it can determine whether urgent intervention is required in many emergencies.","However, echocardiography is characterized by ambiguity and heavy background noise interference, bringing more difficulty to accurate segmentation.","Present methods lack efficiency and are also prone to mistakenly segmenting some background noise areas as the left ventricular area due to noise disturbance.","To relieve the two issues, we introduce P-Mamba for efficient pediatric echocardiographic left ventricular segmentation.","Specifically, we turn to the recently proposed vision mamba layers in our vision mamba encoder branch to improve the computing and memory efficiency of our model while modeling global dependencies.","In the other DWT-based PMD encoder branch, we devise DWT-based Perona-Malik Diffusion (PMD) Blocks that utilize PMD for noise suppression, while simultaneously preserving the local shape cues of the left ventricle.","Leveraging the strengths of both the two encoder branches, P-Mamba achieves superior accuracy and efficiency to established models, such as vision transformers with quadratic and linear computational complexity.","This innovative approach promises significant advancements in pediatric cardiac imaging and beyond."],"url":"http://arxiv.org/abs/2402.08506v1","category":"cs.CV"}
{"created":"2024-02-13 15:02:33","title":"Q-COSMIC: Quantum Software Metrics Based on COSMIC (ISO/IEC19761)","abstract":"Quantum engineering seeks to exploit quantum information to build, among others, computing, cybersecurity, and metrology technologies. Quantum Software Engineering (QSE) focuses on the information processing side of these technologies. Historically, quantum (software) engineering has focused on development in controlled research environments and 'in the small'. As the field progresses, we should expect to see more large-scale quantum systems to be deployed as 'real-world' products and services. An essential tool in (classical) software engineering and development has been software size metrics. Calculating/estimating the size of a piece of software, to be developed or pre-existing, is an essential step in its engineering. Quantum software will be no different. Here we introduce Q-COSMIC, a technique for measuring the functional size of quantum software, based on the well-regarded COSMIC standard (ISO/IEC19761) for classical software","sentences":["Quantum engineering seeks to exploit quantum information to build, among others, computing, cybersecurity, and metrology technologies.","Quantum Software Engineering (QSE) focuses on the information processing side of these technologies.","Historically, quantum (software) engineering has focused on development in controlled research environments and 'in the small'.","As the field progresses, we should expect to see more large-scale quantum systems to be deployed as 'real-world' products and services.","An essential tool in (classical) software engineering and development has been software size metrics.","Calculating/estimating the size of a piece of software, to be developed or pre-existing, is an essential step in its engineering.","Quantum software will be no different.","Here we introduce Q-COSMIC, a technique for measuring the functional size of quantum software, based on the well-regarded COSMIC standard (ISO/IEC19761) for classical software"],"url":"http://arxiv.org/abs/2402.08505v1","category":"quant-ph"}
{"created":"2024-02-13 14:59:19","title":"Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea","abstract":"Autonomous vehicles have to obey traffic rules. These rules are often formalized using temporal logic, resulting in constraints that are hard to solve using optimization-based motion planners. Reinforcement Learning (RL) is a promising method to find motion plans adhering to temporal logic specifications. However, vanilla RL algorithms are based on random exploration, which is inherently unsafe. To address this issue, we propose a provably safe RL approach that always complies with traffic rules. As a specific application area, we consider vessels on the open sea, which must adhere to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGS). We introduce an efficient verification approach that determines the compliance of actions with respect to the COLREGS formalized using temporal logic. Our action verification is integrated into the RL process so that the agent only selects verified actions. In contrast to agents that only integrate the traffic rule information in the reward function, our provably safe agent always complies with the formalized rules in critical maritime traffic situations and, thus, never causes a collision.","sentences":["Autonomous vehicles have to obey traffic rules.","These rules are often formalized using temporal logic, resulting in constraints that are hard to solve using optimization-based motion planners.","Reinforcement Learning (RL) is a promising method to find motion plans adhering to temporal logic specifications.","However, vanilla RL algorithms are based on random exploration, which is inherently unsafe.","To address this issue, we propose a provably safe RL approach that always complies with traffic rules.","As a specific application area, we consider vessels on the open sea, which must adhere to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGS).","We introduce an efficient verification approach that determines the compliance of actions with respect to the COLREGS formalized using temporal logic.","Our action verification is integrated into the RL process so that the agent only selects verified actions.","In contrast to agents that only integrate the traffic rule information in the reward function, our provably safe agent always complies with the formalized rules in critical maritime traffic situations and, thus, never causes a collision."],"url":"http://arxiv.org/abs/2402.08502v1","category":"cs.LG"}
{"created":"2024-02-13 14:53:18","title":"Defect versus defect: phases of single file marching in periodic landscapes with road blocks","abstract":"We consider an inhomogeneous asymmetric exclusion processes in a closed geometry with a point and an extended defects across which the hopping rates are less than elsewhere. We study the stationary density profiles that result from the competition between the point and extended defects. By using a minimum current principle, we identify the conditions of the existence of a localised shock or a localised domain wall. We further elucidate delocalisation of these localised domain walls, when the strengths of the point and extended defects follow a certain mutual relationship, which originates again from the minimum current principle. We show that in that case the system admits a pair of delocalised domain walls, none of which can penetrate the extended defect. We calculate the fluctuation properties of the domain walls and obtain the long time averaged shapes of the delocalised domain walls.","sentences":["We consider an inhomogeneous asymmetric exclusion processes in a closed geometry with a point and an extended defects across which the hopping rates are less than elsewhere.","We study the stationary density profiles that result from the competition between the point and extended defects.","By using a minimum current principle, we identify the conditions of the existence of a localised shock or a localised domain wall.","We further elucidate delocalisation of these localised domain walls, when the strengths of the point and extended defects follow a certain mutual relationship, which originates again from the minimum current principle.","We show that in that case the system admits a pair of delocalised domain walls, none of which can penetrate the extended defect.","We calculate the fluctuation properties of the domain walls and obtain the long time averaged shapes of the delocalised domain walls."],"url":"http://arxiv.org/abs/2402.08499v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-13 14:41:28","title":"Sparsity via Sparse Group $k$-max Regularization","abstract":"For the linear inverse problem with sparsity constraints, the $l_0$ regularized problem is NP-hard, and existing approaches either utilize greedy algorithms to find almost-optimal solutions or to approximate the $l_0$ regularization with its convex counterparts. In this paper, we propose a novel and concise regularization, namely the sparse group $k$-max regularization, which can not only simultaneously enhance the group-wise and in-group sparsity, but also casts no additional restraints on the magnitude of variables in each group, which is especially important for variables at different scales, so that it approximate the $l_0$ norm more closely. We also establish an iterative soft thresholding algorithm with local optimality conditions and complexity analysis provided. Through numerical experiments on both synthetic and real-world datasets, we verify the effectiveness and flexibility of the proposed method.","sentences":["For the linear inverse problem with sparsity constraints, the $l_0$ regularized problem is NP-hard, and existing approaches either utilize greedy algorithms to find almost-optimal solutions or to approximate the $l_0$ regularization with its convex counterparts.","In this paper, we propose a novel and concise regularization, namely the sparse group $k$-max regularization, which can not only simultaneously enhance the group-wise and in-group sparsity, but also casts no additional restraints on the magnitude of variables in each group, which is especially important for variables at different scales, so that it approximate the $l_0$ norm more closely.","We also establish an iterative soft thresholding algorithm with local optimality conditions and complexity analysis provided.","Through numerical experiments on both synthetic and real-world datasets, we verify the effectiveness and flexibility of the proposed method."],"url":"http://arxiv.org/abs/2402.08493v1","category":"cs.LG"}
{"created":"2024-02-13 14:27:33","title":"Polarimetry of the Ly-alpha envelope of the radio-quiet quasar SDSS J124020.91+145535.6","abstract":"The radio-quiet quasar SDSS J1240+1455 lies at a redshift of z=3.11, is surrounded by a Ly-alpha blob (LAB), and is absorbed by a proximate damped Ly-alpha system. In order to better define the morphology of the blob and determine its emission mechanism, we gathered deep narrow-band images isolating the Ly-alpha line of this object in linearly polarized light. We provide a deep intensity image of the blob, showing a filamentary structure extending up to 16'' (or ~122 physical kpc) in diameter. No significant polarization signal could be extracted from the data, but 95% probability upper limits were defined through simulations. They vary between ~3% in the central 0.75'' disk (after subtraction of the unpolarized quasar continuum) and ~10% in the 3.8-5.5'' annulus. The low polarization suggests that the Ly-alpha photons are emitted mostly in situ, by recombination and de-excitation in a gas largely ionized by the quasar ultraviolet light, rather than by a central source and scattered subsequently by neutral hydrogen gas. This blob shows no detectable polarization signal, contrary to LAB1, a brighter and more extended blob that is not related to the nearby active galactic nucleus (AGN) in any obvious way, and where a significant polarization signal of about 18% was detected.","sentences":["The radio-quiet quasar SDSS J1240+1455 lies at a redshift of z=3.11, is surrounded by a Ly-alpha blob (LAB), and is absorbed by a proximate damped Ly-alpha system.","In order to better define the morphology of the blob and determine its emission mechanism, we gathered deep narrow-band images isolating the Ly-alpha line of this object in linearly polarized light.","We provide a deep intensity image of the blob, showing a filamentary structure extending up to 16'' (or ~122 physical kpc) in diameter.","No significant polarization signal could be extracted from the data, but 95% probability upper limits were defined through simulations.","They vary between ~3% in the central 0.75'' disk (after subtraction of the unpolarized quasar continuum) and ~10% in the 3.8-5.5'' annulus.","The low polarization suggests that the Ly-alpha photons are emitted mostly in situ, by recombination and de-excitation in a gas largely ionized by the quasar ultraviolet light, rather than by a central source and scattered subsequently by neutral hydrogen gas.","This blob shows no detectable polarization signal, contrary to LAB1, a brighter and more extended blob that is not related to the nearby active galactic nucleus (AGN) in any obvious way, and where a significant polarization signal of about 18% was detected."],"url":"http://arxiv.org/abs/2402.08488v1","category":"astro-ph.GA"}
{"created":"2024-02-13 14:26:03","title":"Differential cross section measurements for the production of top quark pairs and of additional jets using dilepton events from pp collisions at $\\sqrt{s}$ = 13 TeV","abstract":"Differential cross sections for top quark pair ($\\mathrm{t\\bar{t}}$) production are measured in proton-proton collisions at a center-of-mass energy of 13 TeV using a sample of events containing two oppositely charged leptons. The data were recorded with the CMS detector at the CERN LHC and correspond to an integrated luminosity of 138 fb$^{-1}$. The differential cross sections are measured as functions of kinematic observables of the $\\mathrm{t\\bar{t}}$ system, the top quark and antiquark and their decay products, as well as of the number of additional jets in the event. The results are presented as functions of up to three variables and are corrected to the parton and particle levels. When compared to standard model predictions based on quantum chromodynamics at different levels of accuracy, it is found that the calculations do not always describe the observed data. The deviations are found to be largest for the multi-differential cross sections.","sentences":["Differential cross sections for top quark pair ($\\mathrm{t\\bar{t}}$) production are measured in proton-proton collisions at a center-of-mass energy of 13 TeV using a sample of events containing two oppositely charged leptons.","The data were recorded with the CMS detector at the CERN LHC and correspond to an integrated luminosity of 138 fb$^{-1}$. The differential cross sections are measured as functions of kinematic observables of the $\\mathrm{t\\bar{t}}$ system, the top quark and antiquark and their decay products, as well as of the number of additional jets in the event.","The results are presented as functions of up to three variables and are corrected to the parton and particle levels.","When compared to standard model predictions based on quantum chromodynamics at different levels of accuracy, it is found that the calculations do not always describe the observed data.","The deviations are found to be largest for the multi-differential cross sections."],"url":"http://arxiv.org/abs/2402.08486v1","category":"hep-ex"}
{"created":"2024-02-13 14:15:06","title":"A Note on the Uniform Ergodicity of Dynamical Systems","abstract":"We study the uniform ergodicity property for non-invertible topological and measure-preserving dynamical systems. It is shown that for topological dynamical systems uniform ergodicity is equivalent to eventually periodicity and that for measure preserving systems it is equivalent to periodicity. To obtain our results, we prove a result on the long-term behavior of lattice homomorphisms that have $1$ isolated in its spectrum.","sentences":["We study the uniform ergodicity property for non-invertible topological and measure-preserving dynamical systems.","It is shown that for topological dynamical systems uniform ergodicity is equivalent to eventually periodicity and that for measure preserving systems it is equivalent to periodicity.","To obtain our results, we prove a result on the long-term behavior of lattice homomorphisms that have $1$ isolated in its spectrum."],"url":"http://arxiv.org/abs/2402.08482v1","category":"math.DS"}
{"created":"2024-02-13 14:12:32","title":"Plausible Extractive Rationalization through Semi-Supervised Entailment Signal","abstract":"The increasing use of complex and opaque black box models requires the adoption of interpretable measures, one such option is extractive rationalizing models, which serve as a more interpretable alternative. These models, also known as Explain-Then-Predict models, employ an explainer model to extract rationales and subsequently condition the predictor with the extracted information. Their primary objective is to provide precise and faithful explanations, represented by the extracted rationales. In this paper, we take a semi-supervised approach to optimize for the plausibility of extracted rationales. We adopt a pre-trained natural language inference (NLI) model and further fine-tune it on a small set of supervised rationales ($10\\%$). The NLI predictor is leveraged as a source of supervisory signals to the explainer via entailment alignment. We show that, by enforcing the alignment agreement between the explanation and answer in a question-answering task, the performance can be improved without access to ground truth labels. We evaluate our approach on the ERASER dataset and show that our approach achieves comparable results with supervised extractive models and outperforms unsupervised approaches by $> 100\\%$.","sentences":["The increasing use of complex and opaque black box models requires the adoption of interpretable measures, one such option is extractive rationalizing models, which serve as a more interpretable alternative.","These models, also known as Explain-Then-Predict models, employ an explainer model to extract rationales and subsequently condition the predictor with the extracted information.","Their primary objective is to provide precise and faithful explanations, represented by the extracted rationales.","In this paper, we take a semi-supervised approach to optimize for the plausibility of extracted rationales.","We adopt a pre-trained natural language inference (NLI) model and further fine-tune it on a small set of supervised rationales ($10\\%$).","The NLI predictor is leveraged as a source of supervisory signals to the explainer via entailment alignment.","We show that, by enforcing the alignment agreement between the explanation and answer in a question-answering task, the performance can be improved without access to ground truth labels.","We evaluate our approach on the ERASER dataset and show that our approach achieves comparable results with supervised extractive models and outperforms unsupervised approaches by $> 100\\%$."],"url":"http://arxiv.org/abs/2402.08479v1","category":"cs.CL"}
{"created":"2024-02-13 14:10:10","title":"Closures of Harmonic Bergman Besov Spaces in the Weighted Harmonic Bloch Spaces on the Unit Ball","abstract":"In this paper, via invertible radial differential operators, we characterize the closures of the harmonic Bergman Besov Spaces in the weighted harmonic Bloch spaces on the unit ball of Rn in terms of natural level sets. To this end, we first show that the harmonic Bergman Besov space is contained in the weighted harmonic little Bloch space.","sentences":["In this paper, via invertible radial differential operators, we characterize the closures of the harmonic Bergman Besov Spaces in the weighted harmonic Bloch spaces on the unit ball of Rn in terms of natural level sets.","To this end, we first show that the harmonic Bergman Besov space is contained in the weighted harmonic little Bloch space."],"url":"http://arxiv.org/abs/2402.08477v1","category":"math.CV"}
{"created":"2024-02-13 13:44:41","title":"Computationally Predicted Electronic Properties and Energetics of Native Defects in Cubic Boron Nitride","abstract":"In this study, we employ a first-principles approach to conduct a comprehensive investigation of the properties of nine common native point defects in cubic boron nitride. This analysis combines standard semi-local and dielectric hybrid density-exchange-correlation functional calculations, encompassing vacancies, interstitials, antisites, and their complexes. Our findings elucidate the influence of these defects on the structural and electronic characteristics of cubic boron nitride, such as local structures, formation energy, magnetism, and the energies of defect states within the band gap. Notably, we accurately simulate the photoluminescent spectra of cubic boron nitride induced by these defects, demonstrating excellent agreement with experimental observations. This outcome indicates that the prominent peaks in the photoluminescent spectrum at 2.5 and 2.8 eV can be attributed to the nitrogen to boron antisite (N$_{\\rm B}$) and boron interstitial (B$_{\\rm i}$) defects, respectively. Additionally, we investigate the energetic stability of defects under various charge states, providing valuable references for benchmarking purposes.","sentences":["In this study, we employ a first-principles approach to conduct a comprehensive investigation of the properties of nine common native point defects in cubic boron nitride.","This analysis combines standard semi-local and dielectric hybrid density-exchange-correlation functional calculations, encompassing vacancies, interstitials, antisites, and their complexes.","Our findings elucidate the influence of these defects on the structural and electronic characteristics of cubic boron nitride, such as local structures, formation energy, magnetism, and the energies of defect states within the band gap.","Notably, we accurately simulate the photoluminescent spectra of cubic boron nitride induced by these defects, demonstrating excellent agreement with experimental observations.","This outcome indicates that the prominent peaks in the photoluminescent spectrum at 2.5 and 2.8 eV can be attributed to the nitrogen to boron antisite (N$_{\\rm B}$) and boron interstitial (B$_{\\rm i}$) defects, respectively.","Additionally, we investigate the energetic stability of defects under various charge states, providing valuable references for benchmarking purposes."],"url":"http://arxiv.org/abs/2402.08464v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-13 13:39:39","title":"The SRG/eROSITA All-Sky Survey: First catalog of superclusters in the western Galactic hemisphere","abstract":"Superclusters of galaxies mark the large-scale overdense regions in the Universe. Superclusters provide an ideal environment to study structure formation and to search for the emission of the intergalactic medium such as cosmic filaments and WHIM. In this work, we present the largest-to-date catalog of X-ray-selected superclusters identified in the first SRG/eROSITA All-Sky Survey (eRASS1). By applying the Friends-of-Friends method on the galaxy clusters detected in eRASS1, we identified 1338 supercluster systems in the western Galactic hemisphere up to redshift 0.8, including 818 cluster pairs and 520 rich superclusters with $\\ge 3$ members. The most massive and richest supercluster system is the Shapley supercluster at redshift 0.05 with 45 members and a total mass of $2.58\\pm0.51 \\times10^{16} M_{\\odot}$. The most extensive system has a projected length of 127~Mpc. The sizes of the superclusters we identified in this work are comparable to the structures found with galaxy survey data. We also found a good association between the eRASS1 superclusters and the large-scale structures formed by optical galaxies. 3948 clusters, corresponding to $45\\%$ of the cluster sample, were identified as supercluster members. The reliability of each supercluster was estimated by considering the uncertainties in cluster redshifts and peculiar velocities. 63\\% of the systems have a reliability larger than 0.7. The eRASS1 supercluster catalog provided in this work represents the most extensive sample of superclusters selected in the X-ray band in terms of the unprecedented sample volume, sky coverage, redshift range, the availability of X-ray properties, and the well-understood selection function of the parent cluster sample, which enables direct comparisons with numerical simulations. This legacy catalog will greatly advance our understanding of superclusters and the cosmic large-scale structure.","sentences":["Superclusters of galaxies mark the large-scale overdense regions in the Universe.","Superclusters provide an ideal environment to study structure formation and to search for the emission of the intergalactic medium such as cosmic filaments and WHIM.","In this work, we present the largest-to-date catalog of X-ray-selected superclusters identified in the first SRG/eROSITA All-Sky Survey (eRASS1).","By applying the Friends-of-Friends method on the galaxy clusters detected in eRASS1, we identified 1338 supercluster systems in the western Galactic hemisphere up to redshift 0.8, including 818 cluster pairs and 520 rich superclusters with $\\ge 3$ members.","The most massive and richest supercluster system is the Shapley supercluster at redshift 0.05 with 45 members and a total mass of $2.58\\pm0.51 \\times10^{16} M_{\\odot}$. The most extensive system has a projected length of 127~Mpc.","The sizes of the superclusters we identified in this work are comparable to the structures found with galaxy survey data.","We also found a good association between the eRASS1 superclusters and the large-scale structures formed by optical galaxies.","3948 clusters, corresponding to $45\\%$ of the cluster sample, were identified as supercluster members.","The reliability of each supercluster was estimated by considering the uncertainties in cluster redshifts and peculiar velocities.","63\\% of the systems have a reliability larger than 0.7.","The eRASS1 supercluster catalog provided in this work represents the most extensive sample of superclusters selected in the X-ray band in terms of the unprecedented sample volume, sky coverage, redshift range, the availability of X-ray properties, and the well-understood selection function of the parent cluster sample, which enables direct comparisons with numerical simulations.","This legacy catalog will greatly advance our understanding of superclusters and the cosmic large-scale structure."],"url":"http://arxiv.org/abs/2402.08454v1","category":"astro-ph.CO"}
{"created":"2024-02-13 13:18:18","title":"JeFaPaTo -- A joint toolbox for blinking analysis and facial features extraction","abstract":"Analyzing facial features and expressions is a complex task in computer vision. The human face is intricate, with significant shape, texture, and appearance variations. In medical contexts, facial structures that differ from the norm, such as those affected by paralysis, are particularly important to study and require precise analysis. One area of interest is the subtle movements involved in blinking, a process that is not yet fully understood and needs high-resolution, time-specific analysis for detailed understanding. However, a significant challenge is that many advanced computer vision techniques demand programming skills, making them less accessible to medical professionals who may not have these skills. The Jena Facial Palsy Toolbox (JeFaPaTo) has been developed to bridge this gap. It utilizes cutting-edge computer vision algorithms and offers a user-friendly interface for those without programming expertise. This toolbox is designed to make advanced facial analysis more accessible to medical experts, simplifying integration into their workflow.   The state of the eye closure is of high interest to medical experts, e.g., in the context of facial palsy or Parkinson's disease. Due to facial nerve damage, the eye-closing process might be impaired and could lead to many undesirable side effects. Hence, more than a simple distinction between open and closed eyes is required for a detailed analysis. Factors such as duration, synchronicity, velocity, complete closure, the time between blinks, and frequency over time are highly relevant. Such detailed analysis could help medical experts better understand the blinking process, its deviations, and possible treatments for better eye care.","sentences":["Analyzing facial features and expressions is a complex task in computer vision.","The human face is intricate, with significant shape, texture, and appearance variations.","In medical contexts, facial structures that differ from the norm, such as those affected by paralysis, are particularly important to study and require precise analysis.","One area of interest is the subtle movements involved in blinking, a process that is not yet fully understood and needs high-resolution, time-specific analysis for detailed understanding.","However, a significant challenge is that many advanced computer vision techniques demand programming skills, making them less accessible to medical professionals who may not have these skills.","The Jena Facial Palsy Toolbox (JeFaPaTo) has been developed to bridge this gap.","It utilizes cutting-edge computer vision algorithms and offers a user-friendly interface for those without programming expertise.","This toolbox is designed to make advanced facial analysis more accessible to medical experts, simplifying integration into their workflow.   ","The state of the eye closure is of high interest to medical experts, e.g., in the context of facial palsy or Parkinson's disease.","Due to facial nerve damage, the eye-closing process might be impaired and could lead to many undesirable side effects.","Hence, more than a simple distinction between open and closed eyes is required for a detailed analysis.","Factors such as duration, synchronicity, velocity, complete closure, the time between blinks, and frequency over time are highly relevant.","Such detailed analysis could help medical experts better understand the blinking process, its deviations, and possible treatments for better eye care."],"url":"http://arxiv.org/abs/2402.08439v1","category":"cs.CV"}
{"created":"2024-02-13 13:03:49","title":"Solving promise equations over monoids and groups","abstract":"We give a complete complexity classification for the problem of finding a solution to a given system of equations over a fixed finite monoid, given that a solut ion over a more restricted monoid exists. As a corollary, we obtain a complexity classification for the same problem over groups.","sentences":["We give a complete complexity classification for the problem of finding a solution to a given system of equations over a fixed finite monoid, given that a solut ion over a more restricted monoid exists.","As a corollary, we obtain a complexity classification for the same problem over groups."],"url":"http://arxiv.org/abs/2402.08434v1","category":"cs.CC"}
{"created":"2024-02-13 12:55:06","title":"Mighty Tracker -- Performance Studies of the MightyPix for LHCb","abstract":"A new downstream tracking system, known as the Mighty Tracker, is planned to be installed at LHCb during LS4 of the LHC. This will allow an increase in instantaneous luminosity from $2\\cdot10^{33}~\\mathrm{cm}^{-2}\\mathrm{s}^{-1}$ to $1.5\\cdot10^{34}~\\mathrm{cm}^{-2}\\mathrm{s}^{-1}$ and therefore an overall higher irradiation and up to six times higher occupancy. To keep the material budget as low or even lower as for the current detector, the Mighty Tracker is planned as a hybrid system with silicon pixels in the inner and scintillating fibers in the outer region. For the pixel detector part HV-CMOS MAPS with a pixel size of $55~\\mathrm{x}~165~\\mathrm{\\mu m^2}$ will be used. This technology has been chosen because of its low production costs, low material budget, high radiation tolerance and good timing resolution. To fulfill the requirements, the development and characterization of the sensors focuses on radiation damage with fluences up to $2\\cdot10^{15~}\\mathrm{MeVn_{eq}}/\\mathrm{cm^2}$ and a timing resolution $\\leq3~\\mathrm{ns}$. The timing resolution is restricted due to the $40~\\mathrm{MHz}$ trigger-less DAQ by LHCb. To characterize and further develop the MightyPix a new readout system called MARS (Mighty TrAcker Readout System) has been developed. MARS is a modular system, able to test different single chips in the laboratory as well as at testbeam facilities. It has been used to perform first characterization studies of development-chips investigating radiation tolerance and timing resolution as well as the dependence of the sensor settings on the overall performance.","sentences":["A new downstream tracking system, known as the Mighty Tracker, is planned to be installed at LHCb during LS4 of the LHC.","This will allow an increase in instantaneous luminosity from $2\\cdot10^{33}~\\mathrm{cm}^{-2}\\mathrm{s}^{-1}$ to $1.5\\cdot10^{34}~\\mathrm{cm}^{-2}\\mathrm{s}^{-1}$ and therefore an overall higher irradiation and up to six times higher occupancy.","To keep the material budget as low or even lower as for the current detector, the Mighty Tracker is planned as a hybrid system with silicon pixels in the inner and scintillating fibers in the outer region.","For the pixel detector part HV-CMOS MAPS with a pixel size of $55~\\mathrm{x}~165~\\mathrm{\\mu m^2}$ will be used.","This technology has been chosen because of its low production costs, low material budget, high radiation tolerance and good timing resolution.","To fulfill the requirements, the development and characterization of the sensors focuses on radiation damage with fluences up to $2\\cdot10^{15~}\\mathrm{MeVn_{eq}}/\\mathrm{cm^2}$ and a timing resolution $\\leq3~\\mathrm{ns}$. The timing resolution is restricted due to the $40~\\mathrm{MHz}$ trigger-less DAQ by LHCb.","To characterize and further develop the MightyPix a new readout system called MARS (Mighty TrAcker Readout System) has been developed.","MARS is a modular system, able to test different single chips in the laboratory as well as at testbeam facilities.","It has been used to perform first characterization studies of development-chips investigating radiation tolerance and timing resolution as well as the dependence of the sensor settings on the overall performance."],"url":"http://arxiv.org/abs/2402.08428v1","category":"physics.ins-det"}
{"created":"2024-02-13 12:52:41","title":"Transfer Operators from Batches of Unpaired Points via Entropic Transport Kernels","abstract":"In this paper, we are concerned with estimating the joint probability of random variables $X$ and $Y$, given $N$ independent observation blocks $(\\boldsymbol{x}^i,\\boldsymbol{y}^i)$, $i=1,\\ldots,N$, each of $M$ samples $(\\boldsymbol{x}^i,\\boldsymbol{y}^i) = \\bigl((x^i_j, y^i_{\\sigma^i(j)}) \\bigr)_{j=1}^M$, where $\\sigma^i$ denotes an unknown permutation of i.i.d. sampled pairs $(x^i_j,y_j^i)$, $j=1,\\ldots,M$. This means that the internal ordering of the $M$ samples within an observation block is not known. We derive a maximum-likelihood inference functional, propose a computationally tractable approximation and analyze their properties. In particular, we prove a $\\Gamma$-convergence result showing that we can recover the true density from empirical approximations as the number $N$ of blocks goes to infinity. Using entropic optimal transport kernels, we model a class of hypothesis spaces of density functions over which the inference functional can be minimized. This hypothesis class is particularly suited for approximate inference of transfer operators from data. We solve the resulting discrete minimization problem by a modification of the EMML algorithm to take addional transition probability constraints into account and prove the convergence of this algorithm. Proof-of-concept examples demonstrate the potential of our method.","sentences":["In this paper, we are concerned with estimating the joint probability of random variables $X$ and $Y$, given $N$ independent observation blocks $(\\boldsymbol{x}^i,\\boldsymbol{y}^i)$, $i=1,\\ldots,N$, each of $M$ samples $(\\boldsymbol{x}^i,\\boldsymbol{y}^i) = \\bigl((x^i_j, y^i_{\\sigma^i(j)})","\\bigr)_{j=1}^M$, where $\\sigma^i$ denotes an unknown permutation of i.i.d. sampled pairs $(x^i_j,y_j^i)$, $j=1,\\ldots,M$.","This means that the internal ordering of the $M$ samples within an observation block is not known.","We derive a maximum-likelihood inference functional, propose a computationally tractable approximation and analyze their properties.","In particular, we prove a $\\Gamma$-convergence result showing that we can recover the true density from empirical approximations as the number $N$ of blocks goes to infinity.","Using entropic optimal transport kernels, we model a class of hypothesis spaces of density functions over which the inference functional can be minimized.","This hypothesis class is particularly suited for approximate inference of transfer operators from data.","We solve the resulting discrete minimization problem by a modification of the EMML algorithm to take addional transition probability constraints into account and prove the convergence of this algorithm.","Proof-of-concept examples demonstrate the potential of our method."],"url":"http://arxiv.org/abs/2402.08425v1","category":"stat.ML"}
{"created":"2024-02-13 12:29:38","title":"Interacting Particle Systems on Networks: joint inference of the network and the interaction kernel","abstract":"Modeling multi-agent systems on networks is a fundamental challenge in a wide variety of disciplines. We jointly infer the weight matrix of the network and the interaction kernel, which determine respectively which agents interact with which others and the rules of such interactions from data consisting of multiple trajectories. The estimator we propose leads naturally to a non-convex optimization problem, and we investigate two approaches for its solution: one is based on the alternating least squares (ALS) algorithm; another is based on a new algorithm named operator regression with alternating least squares (ORALS). Both algorithms are scalable to large ensembles of data trajectories. We establish coercivity conditions guaranteeing identifiability and well-posedness. The ALS algorithm appears statistically efficient and robust even in the small data regime but lacks performance and convergence guarantees. The ORALS estimator is consistent and asymptotically normal under a coercivity condition. We conduct several numerical experiments ranging from Kuramoto particle systems on networks to opinion dynamics in leader-follower models.","sentences":["Modeling multi-agent systems on networks is a fundamental challenge in a wide variety of disciplines.","We jointly infer the weight matrix of the network and the interaction kernel, which determine respectively which agents interact with which others and the rules of such interactions from data consisting of multiple trajectories.","The estimator we propose leads naturally to a non-convex optimization problem, and we investigate two approaches for its solution: one is based on the alternating least squares (ALS) algorithm; another is based on a new algorithm named operator regression with alternating least squares (ORALS).","Both algorithms are scalable to large ensembles of data trajectories.","We establish coercivity conditions guaranteeing identifiability and well-posedness.","The ALS algorithm appears statistically efficient and robust even in the small data regime but lacks performance and convergence guarantees.","The ORALS estimator is consistent and asymptotically normal under a coercivity condition.","We conduct several numerical experiments ranging from Kuramoto particle systems on networks to opinion dynamics in leader-follower models."],"url":"http://arxiv.org/abs/2402.08412v1","category":"stat.ML"}
{"created":"2024-02-13 11:58:02","title":"Correlating parent-fragment relationships in cluster photoionization","abstract":"Fragment signals in ordinary mass spectra carry no label to identify their parent molecule. By correlating mass signals with rotational Raman spectra, we created a method to label each ion signal with the spectroscopic fingerprint of its neutral parent molecule. In data for a carbon disulfide molecular cluster beam, we assigned 28 distinct ionization and fragmentation channels based on their mass-correlated rotational fingerprints. Unexpected observations included the formation of energetic S2 and SCCS cationic fragments from the CS2-dimer cluster and a significant CS3 signal, uncorrelated to the dimer. The large number of observed channels revealed a surprising complexity that could only be addressed with correlated spectroscopy and computer-aided correlation analysis.","sentences":["Fragment signals in ordinary mass spectra carry no label to identify their parent molecule.","By correlating mass signals with rotational Raman spectra, we created a method to label each ion signal with the spectroscopic fingerprint of its neutral parent molecule.","In data for a carbon disulfide molecular cluster beam, we assigned 28 distinct ionization and fragmentation channels based on their mass-correlated rotational fingerprints.","Unexpected observations included the formation of energetic S2 and SCCS cationic fragments from the CS2-dimer cluster and a significant CS3 signal, uncorrelated to the dimer.","The large number of observed channels revealed a surprising complexity that could only be addressed with correlated spectroscopy and computer-aided correlation analysis."],"url":"http://arxiv.org/abs/2402.08398v1","category":"physics.atm-clus"}
{"created":"2024-02-13 11:42:09","title":"Dipolar many-body complexes and their interactions in stacked 2D heterobilayers","abstract":"Highly customizable interfaces created by van der Waals stacked 2D materials provide an extremely flexible opportunity for engineering and effectively controlling material properties. The atomic-thin nature and strong scalability of transition metal dichalcogenides (TMDs), the star family of two-dimensional semiconducting materials, allow for the modulation of their inherent optical and electrical characteristics by utilizing various environmental stimuli. In such a material system, the stacking mechanism with spatial separation in the structure enables recent observations of dipolar many-body complexes with the interplay of multi-particles, leading to some exotic and novel excitonic phenomena and enabling the closer study of high-correlated quantum physics. The presence of powerful dipole-dipole interactions among long-lived interlayer excitons can cause the system to enter unique classical and quantum phases with multiparticle correlations, such as dipolar liquids, dipolar crystals and superfluids. The strong binding energy of interlayer excitons in TMD-based hetero-bilayers especially enhances the critical temperature of these exotic phenomena. Here, we provide a concise summary of the recent frontier research progress on dipolar complexes and many-body effects in TMD double layers, encompassing fundamental theory and properties modulation. We reveal the significance and current challenges of this research field and present the potential developing directions of the hetero-bilayers in quantum physics and quantum devices by adding new levels of external control or integration.","sentences":["Highly customizable interfaces created by van der Waals stacked 2D materials provide an extremely flexible opportunity for engineering and effectively controlling material properties.","The atomic-thin nature and strong scalability of transition metal dichalcogenides (TMDs), the star family of two-dimensional semiconducting materials, allow for the modulation of their inherent optical and electrical characteristics by utilizing various environmental stimuli.","In such a material system, the stacking mechanism with spatial separation in the structure enables recent observations of dipolar many-body complexes with the interplay of multi-particles, leading to some exotic and novel excitonic phenomena and enabling the closer study of high-correlated quantum physics.","The presence of powerful dipole-dipole interactions among long-lived interlayer excitons can cause the system to enter unique classical and quantum phases with multiparticle correlations, such as dipolar liquids, dipolar crystals and superfluids.","The strong binding energy of interlayer excitons in TMD-based hetero-bilayers especially enhances the critical temperature of these exotic phenomena.","Here, we provide a concise summary of the recent frontier research progress on dipolar complexes and many-body effects in TMD double layers, encompassing fundamental theory and properties modulation.","We reveal the significance and current challenges of this research field and present the potential developing directions of the hetero-bilayers in quantum physics and quantum devices by adding new levels of external control or integration."],"url":"http://arxiv.org/abs/2402.08394v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-13 11:38:52","title":"NfgTransformer: Equivariant Representation Learning for Normal-form Games","abstract":"Normal-form games (NFGs) are the fundamental model of strategic interaction. We study their representation using neural networks. We describe the inherent equivariance of NFGs -- any permutation of strategies describes an equivalent game -- as well as the challenges this poses for representation learning. We then propose the NfgTransformer architecture that leverages this equivariance, leading to state-of-the-art performance in a range of game-theoretic tasks including equilibrium-solving, deviation gain estimation and ranking, with a common approach to NFG representation. We show that the resulting model is interpretable and versatile, paving the way towards deep learning systems capable of game-theoretic reasoning when interacting with humans and with each other.","sentences":["Normal-form games (NFGs) are the fundamental model of strategic interaction.","We study their representation using neural networks.","We describe the inherent equivariance of NFGs -- any permutation of strategies describes an equivalent game -- as well as the challenges this poses for representation learning.","We then propose the NfgTransformer architecture that leverages this equivariance, leading to state-of-the-art performance in a range of game-theoretic tasks including equilibrium-solving, deviation gain estimation and ranking, with a common approach to NFG representation.","We show that the resulting model is interpretable and versatile, paving the way towards deep learning systems capable of game-theoretic reasoning when interacting with humans and with each other."],"url":"http://arxiv.org/abs/2402.08393v1","category":"cs.GT"}
{"created":"2024-02-13 11:25:32","title":"Hitchin systems: some recent advances","abstract":"A survey of some recent advances in parabolic Hitchin systems (parabolic Bouville--Narasimhan--Ramanan correspondence, mirror symmetry for parabolic Hitchin systems), and in exact methods of solving the non-parabolic Hitchin systems.","sentences":["A survey of some recent advances in parabolic Hitchin systems (parabolic Bouville--Narasimhan--Ramanan correspondence, mirror symmetry for parabolic Hitchin systems), and in exact methods of solving the non-parabolic Hitchin systems."],"url":"http://arxiv.org/abs/2402.08385v1","category":"math-ph"}
{"created":"2024-02-13 11:22:52","title":"Punctuation Restoration Improves Structure Understanding without Supervision","abstract":"Unsupervised learning objectives like language modeling and de-noising constitute a significant part in producing pre-trained models that perform various downstream applications from natural language understanding to conversational tasks. However, despite impressive conversational capabilities of recent large language model, their abilities to capture syntactic or semantic structure within text lag behind. We hypothesize that the mismatch between linguistic performance and competence in machines is attributable to insufficient transfer of linguistic structure knowledge to computational systems with currently popular pre-training objectives. We show that punctuation restoration transfers to improvements in in- and out-of-distribution performance on structure-related tasks like named entity recognition, open information extraction, chunking, and part-of-speech tagging. Punctuation restoration is an effective learning objective that can improve structure understanding and yield a more robust structure-aware representations of natural language.","sentences":["Unsupervised learning objectives like language modeling and de-noising constitute a significant part in producing pre-trained models that perform various downstream applications from natural language understanding to conversational tasks.","However, despite impressive conversational capabilities of recent large language model, their abilities to capture syntactic or semantic structure within text lag behind.","We hypothesize that the mismatch between linguistic performance and competence in machines is attributable to insufficient transfer of linguistic structure knowledge to computational systems with currently popular pre-training objectives.","We show that punctuation restoration transfers to improvements in in- and out-of-distribution performance on structure-related tasks like named entity recognition, open information extraction, chunking, and part-of-speech tagging.","Punctuation restoration is an effective learning objective that can improve structure understanding and yield a more robust structure-aware representations of natural language."],"url":"http://arxiv.org/abs/2402.08382v1","category":"cs.CL"}
{"created":"2024-02-13 11:22:23","title":"MAVRL: Learn to Fly in Cluttered Environments with Varying Speed","abstract":"Many existing obstacle avoidance algorithms overlook the crucial balance between safety and agility, especially in environments of varying complexity. In our study, we introduce an obstacle avoidance pipeline based on reinforcement learning. This pipeline enables drones to adapt their flying speed according to the environmental complexity. Moreover, to improve the obstacle avoidance performance in cluttered environments, we propose a novel latent space. The latent space in this representation is explicitly trained to retain memory of previous depth map observations. Our findings confirm that varying speed leads to a superior balance of success rate and agility in cluttered environments. Additionally, our memory-augmented latent representation outperforms the latent representation commonly used in reinforcement learning. Finally, after minimal fine-tuning, we successfully deployed our network on a real drone for enhanced obstacle avoidance.","sentences":["Many existing obstacle avoidance algorithms overlook the crucial balance between safety and agility, especially in environments of varying complexity.","In our study, we introduce an obstacle avoidance pipeline based on reinforcement learning.","This pipeline enables drones to adapt their flying speed according to the environmental complexity.","Moreover, to improve the obstacle avoidance performance in cluttered environments, we propose a novel latent space.","The latent space in this representation is explicitly trained to retain memory of previous depth map observations.","Our findings confirm that varying speed leads to a superior balance of success rate and agility in cluttered environments.","Additionally, our memory-augmented latent representation outperforms the latent representation commonly used in reinforcement learning.","Finally, after minimal fine-tuning, we successfully deployed our network on a real drone for enhanced obstacle avoidance."],"url":"http://arxiv.org/abs/2402.08381v1","category":"cs.RO"}
{"created":"2024-02-13 11:11:04","title":"Biophysical Fluid Dynamics in a Petri Dish","abstract":"The humble Petri dish is perhaps the simplest setting in which to examine the locomotion of swimming organisms, particularly those whose body size is tens of microns to millimetres. The fluid layer in such a container has a bottom no-slip surface and a stress-free upper boundary. It is of fundamental interest to understand the flow fields produced by the elementary and composite singularities of Stokes flow in this geometry. Building on the few particular cases that have previously been considered in the literature, we study here the image systems for the primary singularities of Stokes flow subject to such boundary conditions - the stokeslet, rotlet, source, rotlet dipole, source dipole and stresslet - paying particular attention to the far-field behavior. In several key situations, the depth-averaged fluid flow is accurately captured by the solution of an associated Brinkman equation whose screening length is proportional to the depth of the fluid layer. The case of hydrodynamic bound states formed by spinning microswimmers near a no-slip surface, discovered first using the alga $Volvox$, is reconsidered in the geometry of a Petri dish, where the power-law attractive interaction between microswimmers acquires unusual exponentially screened oscillations.","sentences":["The humble Petri dish is perhaps the simplest setting in which to examine the locomotion of swimming organisms, particularly those whose body size is tens of microns to millimetres.","The fluid layer in such a container has a bottom no-slip surface and a stress-free upper boundary.","It is of fundamental interest to understand the flow fields produced by the elementary and composite singularities of Stokes flow in this geometry.","Building on the few particular cases that have previously been considered in the literature, we study here the image systems for the primary singularities of Stokes flow subject to such boundary conditions - the stokeslet, rotlet, source, rotlet dipole, source dipole and stresslet - paying particular attention to the far-field behavior.","In several key situations, the depth-averaged fluid flow is accurately captured by the solution of an associated Brinkman equation whose screening length is proportional to the depth of the fluid layer.","The case of hydrodynamic bound states formed by spinning microswimmers near a no-slip surface, discovered first using the alga $Volvox$, is reconsidered in the geometry of a Petri dish, where the power-law attractive interaction between microswimmers acquires unusual exponentially screened oscillations."],"url":"http://arxiv.org/abs/2402.08374v1","category":"cond-mat.soft"}
{"created":"2024-02-13 11:02:12","title":"Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization","abstract":"The wide availability of specific courses together with the flexibility of academic plans in university studies reveal the importance of Recommendation Systems (RSs) in this area. These systems appear as tools that help students to choose courses that suit to their personal interests and their academic performance. This paper presents a hybrid RS that combines Collaborative Filtering (CF) and Content-based Filtering (CBF) using multiple criteria related both to student and course information to recommend the most suitable courses to the students. A Genetic Algorithm (GA) has been developed to automatically discover the optimal RS configuration which include both the most relevant criteria and the configuration of the rest of parameters. The experimental study has used real information of Computer Science Degree of University of Cordoba (Spain) including information gathered from students during three academic years, counting on 2500 entries of 95 students and 63 courses. Experimental results show a study of the most relevant criteria for the course recommendation, the importance of using a hybrid model that combines both student information and course information to increase the reliability of the recommendations as well as an excellent performance compared to previous models.","sentences":["The wide availability of specific courses together with the flexibility of academic plans in university studies reveal the importance of Recommendation Systems (RSs) in this area.","These systems appear as tools that help students to choose courses that suit to their personal interests and their academic performance.","This paper presents a hybrid RS that combines Collaborative Filtering (CF) and Content-based Filtering (CBF) using multiple criteria related both to student and course information to recommend the most suitable courses to the students.","A Genetic Algorithm (GA) has been developed to automatically discover the optimal RS configuration which include both the most relevant criteria and the configuration of the rest of parameters.","The experimental study has used real information of Computer Science Degree of University of Cordoba (Spain) including information gathered from students during three academic years, counting on 2500 entries of 95 students and 63 courses.","Experimental results show a study of the most relevant criteria for the course recommendation, the importance of using a hybrid model that combines both student information and course information to increase the reliability of the recommendations as well as an excellent performance compared to previous models."],"url":"http://arxiv.org/abs/2402.08371v1","category":"cs.LG"}
{"created":"2024-02-13 10:50:20","title":"An improved Magellan weak lensing analysis of the galaxy cluster Abell 2744","abstract":"We present a new weak lensing analysis of the Hubble Frontier Fields galaxy cluster Abell 2744 ($z$ = 0.308) using new Magellan/MegaCam multi-band $gri$ imaging data. We carry out our study by applying brand-new PSF and shape measurement softwares that allow for the use of multi-band data simultaneously, which we first test on Subaru/Suprime-Cam $BR_cz'$ imaging data of the same cluster. The projected total mass of this system within $2.35 \\, \\mathrm{Mpc}$ from the south-west BCG is $(2.56 \\pm 0.26) \\times 10^{15} \\, \\mathrm{M}_\\odot$, which makes Abell 2744 one of the most massive clusters known. This value is consistent, within the errors, with previous weak lensing and dynamical studies. Our analysis reveals the presence of three high-density substructures, thus supporting the picture of a complex merging scenario. This result is also confirmed by a comparison with a recent strong lensing study based on high-resolution JWST imaging. Moreover, our reconstructed total mass profile nicely agrees with an extrapolation of the strong lensing best-fit model up to several Mpc from the BCG centre.","sentences":["We present a new weak lensing analysis of the Hubble Frontier Fields galaxy cluster Abell 2744 ($z$ = 0.308) using new Magellan/MegaCam multi-band $gri$ imaging data.","We carry out our study by applying brand-new PSF and shape measurement softwares that allow for the use of multi-band data simultaneously, which we first test on Subaru/Suprime-Cam $BR_cz'$ imaging data of the same cluster.","The projected total mass of this system within $2.35 \\, \\mathrm{Mpc}$ from the south-west BCG is $(2.56 \\pm 0.26)","\\times 10^{15} \\, \\mathrm{M}_\\odot$, which makes Abell 2744 one of the most massive clusters known.","This value is consistent, within the errors, with previous weak lensing and dynamical studies.","Our analysis reveals the presence of three high-density substructures, thus supporting the picture of a complex merging scenario.","This result is also confirmed by a comparison with a recent strong lensing study based on high-resolution JWST imaging.","Moreover, our reconstructed total mass profile nicely agrees with an extrapolation of the strong lensing best-fit model up to several Mpc from the BCG centre."],"url":"http://arxiv.org/abs/2402.08364v1","category":"astro-ph.CO"}
{"created":"2024-02-13 10:36:40","title":"Structural Descriptors and Information Extraction from X-ray Spectra of Liquids","abstract":"Machine learning can reveal new insights into X-ray spectroscopy of liquids when the local atomistic environment is presented to the model in a suitable way. Many unique structural descriptor families have been developed for this purpose. We benchmark the performance of six different descriptor types using a computational data set of 24200 sulfur K\\b{eta} X-ray emission spectra of aqueous sulfuric acid simulated at six different concentrations. We train a feed-forward neural network to predict the spectra from the corresponding descriptor vectors and find that the local many-body tensor representation, smooth overlap of atomic positions and atom-centered symmetry functions excel in this comparison. We found a similar hierarchy when applying the emulator-based component analysis to identify the spectrally relevant structural characteristics. The spectra were dominantly dependent on the concentration of the system, whereas adding the second most significant degree of freedom in the decomposition allowed for distinction of the protonation state of the acid molecule.","sentences":["Machine learning can reveal new insights into X-ray spectroscopy of liquids when the local atomistic environment is presented to the model in a suitable way.","Many unique structural descriptor families have been developed for this purpose.","We benchmark the performance of six different descriptor types using a computational data set of 24200 sulfur K\\b{eta} X-ray emission spectra of aqueous sulfuric acid simulated at six different concentrations.","We train a feed-forward neural network to predict the spectra from the corresponding descriptor vectors and find that the local many-body tensor representation, smooth overlap of atomic positions and atom-centered symmetry functions excel in this comparison.","We found a similar hierarchy when applying the emulator-based component analysis to identify the spectrally relevant structural characteristics.","The spectra were dominantly dependent on the concentration of the system, whereas adding the second most significant degree of freedom in the decomposition allowed for distinction of the protonation state of the acid molecule."],"url":"http://arxiv.org/abs/2402.08355v1","category":"physics.chem-ph"}
{"created":"2024-02-13 10:33:14","title":"Riemann--Hilbert method to the Ablowitz--Ladik equation: higher-order case","abstract":"We focused on the Ablowitz--Ladik equation on a zero background, specifically considering the scenario of $N$ pairs of multiple poles. Our first goal was to establish a mapping between the initial data and the scattering data. This allowed us to introduce a direct problem by analyzing the discrete spectrum associated with $N$ pairs of higher-order zeros. Next, we constructed another mapping from the scattering data to a $2\\times2$ matrix Riemann--Hilbert problem equipped with several residue conditions set at $N$ pairs of multiple poles. By characterizing the inverse problem based on this Riemann--Hilbert problem, we were able to derive higher-order soliton solutions in the reflectionless case. Furthermore, we expressed an infinite-order soliton solution using a special Riemann--Hilbert problem formulation.","sentences":["We focused on the Ablowitz--Ladik equation on a zero background, specifically considering the scenario of $N$ pairs of multiple poles.","Our first goal was to establish a mapping between the initial data and the scattering data.","This allowed us to introduce a direct problem by analyzing the discrete spectrum associated with $N$ pairs of higher-order zeros.","Next, we constructed another mapping from the scattering data to a $2\\times2$ matrix Riemann--Hilbert problem equipped with several residue conditions set at $N$ pairs of multiple poles.","By characterizing the inverse problem based on this Riemann--Hilbert problem, we were able to derive higher-order soliton solutions in the reflectionless case.","Furthermore, we expressed an infinite-order soliton solution using a special Riemann--Hilbert problem formulation."],"url":"http://arxiv.org/abs/2402.08352v1","category":"nlin.SI"}
{"created":"2024-02-13 10:24:20","title":"Tight (Double) Exponential Bounds for Identification Problems: Locating-Dominating Set and Test Cover","abstract":"We investigate fine-grained algorithmic aspects of identification problems in graphs and set systems, with a focus on Locating-Dominating Set and Test Cover. We prove, among other things, the following three (tight) conditional lower bounds. \\begin{enumerate} \\item \\textsc{Locating-Dominating Set} does not admit an algorithm running in time $2^{o(k^2)} \\cdot poly(n)$, nor a polynomial time kernelization algorithm that reduces the solution size and outputs a kernel with $2^{o(k)}$ vertices, unless the \\ETH\\ fails. \\end{enumerate} To the best of our knowledge, \\textsc{Locating-Dominating Set} is the first problem that admits such an algorithmic lower-bound (with a quadratic function in the exponent) when parameterized by the solution size. \\begin{enumerate}[resume] \\item \\textsc{Test Cover} does not admit an algorithm running in time $2^{2^{o(k)}} \\cdot poly(|U| + |\\calF|)$. \\end{enumerate} After \\textsc{Edge Clique Cover} and \\textsc{BiClique Cover}, this is the only example that we know of that admits a double exponential lower bound when parameterized by the solution size. \\begin{enumerate}[resume] \\item \\textsc{Locating-Dominating Set} (respectively, \\textsc{Test Cover}) parameterized by the treewidth of the input graph (respectively, of the natural auxiliary graph) does not admit an algorithm running in time $2^{2^{o(\\tw)}} \\cdot poly(n)$ (respectively, $2^{2^{o(\\tw)}} \\cdot poly(|U| + |\\calF|))$. \\end{enumerate} This result augments the small list of NP-Complete problems that admit double exponential lower bounds when parameterized by treewidth. We also present algorithms whose running times match the above lower bounds. We also investigate the parameterizations by several other structural graph parameters, answering some open problems from the literature.","sentences":["We investigate fine-grained algorithmic aspects of identification problems in graphs and set systems, with a focus on Locating-Dominating Set and Test Cover.","We prove, among other things, the following three (tight) conditional lower bounds.","\\begin{enumerate} \\item \\textsc{Locating-Dominating Set} does not admit an algorithm running in time $2^{o(k^2)} \\cdot poly(n)$, nor a polynomial time kernelization algorithm that reduces the solution size and outputs a kernel with $2^{o(k)}$ vertices, unless the \\ETH\\ fails.","\\end{enumerate} To the best of our knowledge, \\textsc{Locating-Dominating Set} is the first problem that admits such an algorithmic lower-bound (with a quadratic function in the exponent) when parameterized by the solution size.","\\begin{enumerate}[resume] \\item \\textsc{Test Cover} does not admit an algorithm running in time $2^{2^{o(k)}} \\cdot poly(|U| + |\\calF|)$. \\end{enumerate} After \\textsc{Edge Clique Cover} and \\textsc{BiClique Cover}, this is the only example that we know of that admits a double exponential lower bound when parameterized by the solution size.","\\begin{enumerate}[resume] \\item \\textsc{Locating-Dominating Set} (respectively, \\textsc{Test Cover}) parameterized by the treewidth of the input graph (respectively, of the natural auxiliary graph) does not admit an algorithm running in time $2^{2^{o(\\tw)}} \\cdot poly(n)$ (respectively, $2^{2^{o(\\tw)}} \\cdot poly(|U| + |\\calF|))$. \\end{enumerate} This result augments the small list of NP-Complete problems that admit double exponential lower bounds when parameterized by treewidth.","We also present algorithms whose running times match the above lower bounds.","We also investigate the parameterizations by several other structural graph parameters, answering some open problems from the literature."],"url":"http://arxiv.org/abs/2402.08346v1","category":"cs.DS"}
{"created":"2024-02-13 10:15:20","title":"A New Hybrid Approach for Identifying Obsolescence Features: Applied to Railway Signaling Infrastructure","abstract":"Electrical component obsolescence poses a major issue especially within systems with large life cycles. Thus, finding the optimal management solution for each obsolescence case is as crucial as knowing what to consider when faced with an obsolescence case. In this paper, a novel hybrid approach for identifying features affecting electrical component obsolescence management is introduced, which combines features engineering techniques and expert knowledge. The method then uses machine learning to predict obsolescence resolution techniques in order to find the optimal resolution. The motivation behind this research is driven by the imperative need for SNCF RESEAU to optimally address and mitigate the challenges posed by electrical component obsolescence in railway infrastructure.","sentences":["Electrical component obsolescence poses a major issue especially within systems with large life cycles.","Thus, finding the optimal management solution for each obsolescence case is as crucial as knowing what to consider when faced with an obsolescence case.","In this paper, a novel hybrid approach for identifying features affecting electrical component obsolescence management is introduced, which combines features engineering techniques and expert knowledge.","The method then uses machine learning to predict obsolescence resolution techniques in order to find the optimal resolution.","The motivation behind this research is driven by the imperative need for SNCF RESEAU to optimally address and mitigate the challenges posed by electrical component obsolescence in railway infrastructure."],"url":"http://arxiv.org/abs/2402.08343v1","category":"eess.SY"}
{"created":"2024-02-13 09:47:49","title":"Dual Likelihood for Causal Inference under Structure Uncertainty","abstract":"Knowledge of the underlying causal relations is essential for inferring the effect of interventions in complex systems. In a widely studied approach, structural causal models postulate noisy functional relations among interacting variables, where the underlying causal structure is then naturally represented by a directed graph whose edges indicate direct causal dependencies. In the typical application, this underlying causal structure must be learned from data, and thus, the remaining structure uncertainty needs to be incorporated into causal inference in order to draw reliable conclusions. In recent work, test inversions provide an ansatz to account for this data-driven model choice and, therefore, combine structure learning with causal inference. In this article, we propose the use of dual likelihood to greatly simplify the treatment of the involved testing problem. Indeed, dual likelihood leads to a closed-form solution for constructing confidence regions for total causal effects that rigorously capture both sources of uncertainty: causal structure and numerical size of nonzero effects. The proposed confidence regions can be computed with a bottom-up procedure starting from sink nodes. To render the causal structure identifiable, we develop our ideas in the context of linear causal relations with equal error variances.","sentences":["Knowledge of the underlying causal relations is essential for inferring the effect of interventions in complex systems.","In a widely studied approach, structural causal models postulate noisy functional relations among interacting variables, where the underlying causal structure is then naturally represented by a directed graph whose edges indicate direct causal dependencies.","In the typical application, this underlying causal structure must be learned from data, and thus, the remaining structure uncertainty needs to be incorporated into causal inference in order to draw reliable conclusions.","In recent work, test inversions provide an ansatz to account for this data-driven model choice and, therefore, combine structure learning with causal inference.","In this article, we propose the use of dual likelihood to greatly simplify the treatment of the involved testing problem.","Indeed, dual likelihood leads to a closed-form solution for constructing confidence regions for total causal effects that rigorously capture both sources of uncertainty: causal structure and numerical size of nonzero effects.","The proposed confidence regions can be computed with a bottom-up procedure starting from sink nodes.","To render the causal structure identifiable, we develop our ideas in the context of linear causal relations with equal error variances."],"url":"http://arxiv.org/abs/2402.08328v1","category":"stat.ME"}
{"created":"2024-02-13 09:17:20","title":"Approximating Families of Sharp Solutions to Fisher's Equation with Physics-Informed Neural Networks","abstract":"This paper employs physics-informed neural networks (PINNs) to solve Fisher's equation, a fundamental representation of a reaction-diffusion system with both simplicity and significance. The focus lies specifically in investigating Fisher's equation under conditions of large reaction rate coefficients, wherein solutions manifest as traveling waves, posing a challenge for numerical methods due to the occurring steepness of the wave front. To address optimization challenges associated with the standard PINN approach, a residual weighting scheme is introduced. This scheme is designed to enhance the tracking of propagating wave fronts by considering the reaction term in the reaction-diffusion equation. Furthermore, a specific network architecture is studied which is tailored for solutions in the form of traveling waves. Lastly, the capacity of PINNs to approximate an entire family of solutions is assessed by incorporating the reaction rate coefficient as an additional input to the network architecture. This modification enables the approximation of the solution across a broad and continuous range of reaction rate coefficients, thus solving a class of reaction-diffusion systems using a single PINN instance.","sentences":["This paper employs physics-informed neural networks (PINNs) to solve Fisher's equation, a fundamental representation of a reaction-diffusion system with both simplicity and significance.","The focus lies specifically in investigating Fisher's equation under conditions of large reaction rate coefficients, wherein solutions manifest as traveling waves, posing a challenge for numerical methods due to the occurring steepness of the wave front.","To address optimization challenges associated with the standard PINN approach, a residual weighting scheme is introduced.","This scheme is designed to enhance the tracking of propagating wave fronts by considering the reaction term in the reaction-diffusion equation.","Furthermore, a specific network architecture is studied which is tailored for solutions in the form of traveling waves.","Lastly, the capacity of PINNs to approximate an entire family of solutions is assessed by incorporating the reaction rate coefficient as an additional input to the network architecture.","This modification enables the approximation of the solution across a broad and continuous range of reaction rate coefficients, thus solving a class of reaction-diffusion systems using a single PINN instance."],"url":"http://arxiv.org/abs/2402.08313v1","category":"cs.LG"}
{"created":"2024-02-13 09:15:15","title":"A new treatment of telluric and stellar features for medium resolution spectroscopy and molecular mapping. Application to the abundance determination on Beta Pic b","abstract":"Molecular mapping is a supervised method exploiting the spectral diversity of integral field spectrographs to detect and characterize resolved exoplanets blurred into the stellar halo. We present an evolution of the method to remove the stellar halo and the nuisance of telluric features in the datacubes and access a continuum-subtracted spectra of the planets at R$\\sim$4000. We derive planet atmosphere properties from a direct analysis of the planet telluric-corrected absorption spectrum. We applied our methods to the SINFONI observation of the planet $\\beta$ Pictoris b. We recover the CO and H$_2$O detections in the atmosphere of $\\beta$ Pic b using molecular mapping. We further determine some basic properties of its atmosphere, with $T_\\text{eq}=1748^{+3}_{-4}$ K, a sub-solar [Fe/H]=$-0.235^{+0.015}_{-0.013}$ dex, and a solar C/O=$0.551 \\pm 0.002$ in contrast with values measured for the same exoplanet with other infrared instruments. We confirm a low projected equatorial velocity of 25$^{+5}_{-6}$ km s$^{-1}$. We are also able to measure, for the first time with a medium-resolution spectrograph, the radial velocity of $\\beta$ Pic b relative to the central star at MJD=56910.38 with a km/s precision of $-11.3 \\pm 1.1$ km s$^{-1}$, compatible with ephemerides based on the current knowledge of the $\\beta$ Pic system.","sentences":["Molecular mapping is a supervised method exploiting the spectral diversity of integral field spectrographs to detect and characterize resolved exoplanets blurred into the stellar halo.","We present an evolution of the method to remove the stellar halo and the nuisance of telluric features in the datacubes and access a continuum-subtracted spectra of the planets at R$\\sim$4000.","We derive planet atmosphere properties from a direct analysis of the planet telluric-corrected absorption spectrum.","We applied our methods to the SINFONI observation of the planet $\\beta$ Pictoris b.","We recover the CO and H$_2$O detections in the atmosphere of $\\beta$ Pic b using molecular mapping.","We further determine some basic properties of its atmosphere, with $T_\\text{eq}=1748^{+3}_{-4}$ K, a sub-solar [Fe/H]=$-0.235^{+0.015}_{-0.013}$ dex, and a solar C/O=$0.551 \\pm 0.002$ in contrast with values measured for the same exoplanet with other infrared instruments.","We confirm a low projected equatorial velocity of 25$^{+5}_{-6}$ km s$^{-1}$.","We are also able to measure, for the first time with a medium-resolution spectrograph, the radial velocity of $\\beta$ Pic b relative to the central star at MJD=56910.38 with a km/s precision of $-11.3 \\pm 1.1$ km s$^{-1}$, compatible with ephemerides based on the current knowledge of the $\\beta$ Pic system."],"url":"http://arxiv.org/abs/2402.08311v1","category":"astro-ph.EP"}
{"created":"2024-02-13 09:07:09","title":"Reinforcement Learning for Docking Maneuvers with Prescribed Performance","abstract":"We propose a two-component data-driven controller to safely perform docking maneuvers for satellites. Reinforcement Learning is used to deduce an optimal control policy based on measurement data. To safeguard the learning phase, an additional feedback law is implemented in the control unit, which guarantees the evolution of the system within predefined performance bounds. We define safe and safety-critical areas to train the feedback controller based on actual measurements. To avoid chattering, a dwell-time activation scheme is implemented. We provide numerical evidence for the performance of the proposed controller for a satellite docking maneuver with collision avoidance.","sentences":["We propose a two-component data-driven controller to safely perform docking maneuvers for satellites.","Reinforcement Learning is used to deduce an optimal control policy based on measurement data.","To safeguard the learning phase, an additional feedback law is implemented in the control unit, which guarantees the evolution of the system within predefined performance bounds.","We define safe and safety-critical areas to train the feedback controller based on actual measurements.","To avoid chattering, a dwell-time activation scheme is implemented.","We provide numerical evidence for the performance of the proposed controller for a satellite docking maneuver with collision avoidance."],"url":"http://arxiv.org/abs/2402.08306v1","category":"math.OC"}
{"created":"2024-02-13 08:49:03","title":"Duality solutions to the hard-congestion model for the dissipative Aw-Rascle system","abstract":"We introduce the notion of duality solution for the hard-congestion model on the real line, and additionally prove an existence result for this class of solutions. Our study revolves around the analysis of a generalised Aw-Rascle system, where the offset function is replaced by the gradient of a singular function, such as $\\rho$ $\\gamma$ n , where $\\gamma$ $\\rightarrow$ $\\infty$. We prove that under suitable assumptions on the initial data, solutions to the Aw-Rascle system converge towards the so-called duality solutions, which have previously found applications in other systems which exhibit compressive dynamics. We also prove that one can obtain weak solutions to the limiting system under stricter assumptions on the initial data. Finally, we discuss (non-)uniqueness issues.","sentences":["We introduce the notion of duality solution for the hard-congestion model on the real line, and additionally prove an existence result for this class of solutions.","Our study revolves around the analysis of a generalised Aw-Rascle system, where the offset function is replaced by the gradient of a singular function, such as $\\rho$ $\\gamma$ n , where $\\gamma$ $\\rightarrow$ $\\infty$. We prove that under suitable assumptions on the initial data, solutions to the Aw-Rascle system converge towards the so-called duality solutions, which have previously found applications in other systems which exhibit compressive dynamics.","We also prove that one can obtain weak solutions to the limiting system under stricter assumptions on the initial data.","Finally, we discuss (non-)uniqueness issues."],"url":"http://arxiv.org/abs/2402.08295v1","category":"math.AP"}
{"created":"2024-02-13 08:40:20","title":"Why Studying Cut-ins? Comparing Cut-ins and Other Lane Changes Based on Naturalistic Driving Data","abstract":"Extensive research has been conducted to explore vehicle lane changes, while the study on cut-ins has not received sufficient attention. The existing studies have not addressed the fundamental question of why studying cut-ins is crucial, despite the extensive investigation into lane changes. To tackle this issue, it is important to demonstrate how cut-ins, as a special type of lane change, differ from other lane changes. In this paper, we explore to compare driving characteristics of cut-ins and other lane changes based on naturalistic driving data. The highD dataset is employed to conduct the comparison. We extract all lane-change events from the dataset and exclude events that are not suitable for our comparison. Lane-change events are then categorized into the cut-in events and other lane-change events based on various gap-based rules. Several performance metrics are designed to measure the driving characteristics of the two types of events. We prove the significant differences between the cut-in behavior and other lane-change behavior by using the Wilcoxon rank-sum test. The results suggest the necessity of conducting specialized studies on cut-ins, offering valuable insights for future research in this field.","sentences":["Extensive research has been conducted to explore vehicle lane changes, while the study on cut-ins has not received sufficient attention.","The existing studies have not addressed the fundamental question of why studying cut-ins is crucial, despite the extensive investigation into lane changes.","To tackle this issue, it is important to demonstrate how cut-ins, as a special type of lane change, differ from other lane changes.","In this paper, we explore to compare driving characteristics of cut-ins and other lane changes based on naturalistic driving data.","The highD dataset is employed to conduct the comparison.","We extract all lane-change events from the dataset and exclude events that are not suitable for our comparison.","Lane-change events are then categorized into the cut-in events and other lane-change events based on various gap-based rules.","Several performance metrics are designed to measure the driving characteristics of the two types of events.","We prove the significant differences between the cut-in behavior and other lane-change behavior by using the Wilcoxon rank-sum test.","The results suggest the necessity of conducting specialized studies on cut-ins, offering valuable insights for future research in this field."],"url":"http://arxiv.org/abs/2402.08289v1","category":"eess.SY"}
{"created":"2024-02-13 08:32:21","title":"MetaVRadar: Measuring Metaverse Virtual Reality Network Activity","abstract":"The \"metaverse\", wherein users can enter virtual worlds to work, study, play, shop, socialize, and entertain, is fast becoming a reality, attracting billions of dollars in investment from companies such as Meta, Microsoft, and Clipo Labs. Further, virtual reality (VR) headsets from entities like Oculus, HTC, and Microsoft are rapidly maturing to provide fully immersive experiences to metaverse users. However, little is known about the network dynamics of metaverse VR applications in terms of service domains, flow counts, traffic rates and volumes, content location and latency, etc., which are needed to make telecommunications network infrastructure \"metaverse ready\". This paper is an empirical measurement study of metaverse VR network behavior aimed at helping telecommunications network operators better provision and manage the network to ensure good user experience. Using illustrative hour-long network traces of metaverse sessions on the Oculus VR headset, we first develop a categorization of user activity into distinct states ranging from login home to streetwalking and event attendance to asset trading, and undertake a detailed analysis of network traffic per state, identifying unique service domains, protocols, flow profiles, and volumetric patterns, thereby highlighting the vastly more complex nature of a metaverse session compared to streaming video or gaming. Armed with the network behavioral profiles, our second contribution develops a real-time method MetaVRadar to detect metaverse session and classify the user activity state leveraging formalized flow signatures and volumetric attributes. Our third contribution practically implements MetaVRadar, evaluates its accuracy in our lab environment, and demonstrates its usability in a large university network so operators can better monitor and plan resources to support requisite metaverse user experience.","sentences":["The \"metaverse\", wherein users can enter virtual worlds to work, study, play, shop, socialize, and entertain, is fast becoming a reality, attracting billions of dollars in investment from companies such as Meta, Microsoft, and Clipo Labs.","Further, virtual reality (VR) headsets from entities like Oculus, HTC, and Microsoft are rapidly maturing to provide fully immersive experiences to metaverse users.","However, little is known about the network dynamics of metaverse VR applications in terms of service domains, flow counts, traffic rates and volumes, content location and latency, etc., which are needed to make telecommunications network infrastructure \"metaverse ready\".","This paper is an empirical measurement study of metaverse VR network behavior aimed at helping telecommunications network operators better provision and manage the network to ensure good user experience.","Using illustrative hour-long network traces of metaverse sessions on the Oculus VR headset, we first develop a categorization of user activity into distinct states ranging from login home to streetwalking and event attendance to asset trading, and undertake a detailed analysis of network traffic per state, identifying unique service domains, protocols, flow profiles, and volumetric patterns, thereby highlighting the vastly more complex nature of a metaverse session compared to streaming video or gaming.","Armed with the network behavioral profiles, our second contribution develops a real-time method MetaVRadar to detect metaverse session and classify the user activity state leveraging formalized flow signatures and volumetric attributes.","Our third contribution practically implements MetaVRadar, evaluates its accuracy in our lab environment, and demonstrates its usability in a large university network so operators can better monitor and plan resources to support requisite metaverse user experience."],"url":"http://arxiv.org/abs/2402.08286v1","category":"cs.NI"}
{"created":"2024-02-13 08:18:14","title":"Logic of Awareness for Nested Knowledge","abstract":"Reasoning abilities of human beings are limited. Logics that treat logical inference for human knowledge should reflect these limited abilities. Logic of awareness is one of those logics. In the logic, what an agent with a limited reasoning ability actually knows at a given moment (explicit knowledge) is distinguished from the ideal knowledge that an agent obtains by performing all possible inferences with what she already knows (implicit knowledge). This paper proposes a logic for explicit knowledge. In particular, we focus more on nested explicit knowledge, which means another agent's knowledge that an agent actually knows at a given moment. We develope a new formalization of two ideas and propose Kripke-style semantics. The first idea is the effect on an agent's reasoning ability by a state of an agent's awareness. We incorporate a relation on possible worlds called an indistinguishable relation to represent ignorance due to lack of awareness. The second idea is a state of each agent's awareness in the other agent's mind. We incorporate a non-empty finite sequence of agents called \\textit{a chain of belief for awareness}. Our logic is called Awareness Logic with Partitions and Chains (ALPC). Employing an example, we show how nested explicit knowledge is formalized with our logic. Thereafter, we propose the proof system and prove the completeness. Finally, we discuss directions for extending and applying our logic and conclude. Our logic offers a foundation for a formal representation of human knowledge. We expect that the logic can be applied to computer science and game theory by describing and analyzing strategic behavior in a game and practical agent communication.","sentences":["Reasoning abilities of human beings are limited.","Logics that treat logical inference for human knowledge should reflect these limited abilities.","Logic of awareness is one of those logics.","In the logic, what an agent with a limited reasoning ability actually knows at a given moment (explicit knowledge) is distinguished from the ideal knowledge that an agent obtains by performing all possible inferences with what she already knows (implicit knowledge).","This paper proposes a logic for explicit knowledge.","In particular, we focus more on nested explicit knowledge, which means another agent's knowledge that an agent actually knows at a given moment.","We develope a new formalization of two ideas and propose Kripke-style semantics.","The first idea is the effect on an agent's reasoning ability by a state of an agent's awareness.","We incorporate a relation on possible worlds called an indistinguishable relation to represent ignorance due to lack of awareness.","The second idea is a state of each agent's awareness in the other agent's mind.","We incorporate a non-empty finite sequence of agents called \\textit{a chain of belief for awareness}.","Our logic is called Awareness Logic with Partitions and Chains (ALPC).","Employing an example, we show how nested explicit knowledge is formalized with our logic.","Thereafter, we propose the proof system and prove the completeness.","Finally, we discuss directions for extending and applying our logic and conclude.","Our logic offers a foundation for a formal representation of human knowledge.","We expect that the logic can be applied to computer science and game theory by describing and analyzing strategic behavior in a game and practical agent communication."],"url":"http://arxiv.org/abs/2402.08282v1","category":"cs.MA"}
{"created":"2024-02-13 08:13:38","title":"Reflexive subspace lattices in Banach spaces","abstract":"In this survey paper we present known results about reflexive subspace lattices. We show that every nest and every atomic Boolean subspace lattice in a complex Banach space is reflexive, even strongly reflexive. Our main tool is Ringrose's Lemma about presence of rank-one operators in a reflexive algebra of operators. We consider realizations of small lattices as subspace lattices in Banach spaces. We exhibit some realizations of the pentagon and prove that some of these realizations are reflexive. On the other hand, we show that double triangle subspace lattices in finite-dimensional Banach spaces are non-reflexive.","sentences":["In this survey paper we present known results about reflexive subspace lattices.","We show that every nest and every atomic Boolean subspace lattice in a complex Banach space is reflexive, even strongly reflexive.","Our main tool is Ringrose's Lemma about presence of rank-one operators in a reflexive algebra of operators.","We consider realizations of small lattices as subspace lattices in Banach spaces.","We exhibit some realizations of the pentagon and prove that some of these realizations are reflexive.","On the other hand, we show that double triangle subspace lattices in finite-dimensional Banach spaces are non-reflexive."],"url":"http://arxiv.org/abs/2402.08279v1","category":"math.FA"}
{"created":"2024-02-13 07:59:34","title":"Implementation of Recommendation Algorithm based on Recommendation Sessions in E-commerce IT System","abstract":"This paper presents a study on the implementation of the author's Algorithm of Recommendation Sessions (ARS) in an operational e-commerce information system and analyses the basic parameters of the resulting recommendation system. It begins with a synthetic overview of recommendation systems, followed by a presentation of the proprietary ARS algorithm, which is based on recommendation sessions. A mathematical model of the recommendation session, constructed using graph and network theory, serves as the input for the ARS algorithm. This paper also explores graph structure representation methods and the implementation of a G graph (representing a set of recommendation sessions) in a relational database using the SQL standard. The ARS algorithm was implemented in a working e-commerce information system, leading to the development of a fully functional recommendation system adaptable to various e-commerce IT systems. The effectiveness of the algorithm is demonstrated by research on the recommendation system's parameters presented in the final section of the paper.","sentences":["This paper presents a study on the implementation of the author's Algorithm of Recommendation Sessions (ARS) in an operational e-commerce information system and analyses the basic parameters of the resulting recommendation system.","It begins with a synthetic overview of recommendation systems, followed by a presentation of the proprietary ARS algorithm, which is based on recommendation sessions.","A mathematical model of the recommendation session, constructed using graph and network theory, serves as the input for the ARS algorithm.","This paper also explores graph structure representation methods and the implementation of a G graph (representing a set of recommendation sessions) in a relational database using the SQL standard.","The ARS algorithm was implemented in a working e-commerce information system, leading to the development of a fully functional recommendation system adaptable to various e-commerce IT systems.","The effectiveness of the algorithm is demonstrated by research on the recommendation system's parameters presented in the final section of the paper."],"url":"http://arxiv.org/abs/2402.08275v1","category":"cs.IR"}
{"created":"2024-02-13 07:35:34","title":"Iiro Honkala's contributions to identifying codes","abstract":"A set $C$ of vertices in a graph $G=(V,E)$ is an identifying code if it is dominating and any two vertices of $V$ are dominated by distinct sets of codewords. This paper presents a survey of Iiro Honkala's contributions to the study of identifying codes with respect to several aspects: complexity of computing an identifying code, combinatorics in binary Hamming spaces, infinite grids, relationships between identifying codes and usual parameters in graphs, structural properties of graphs admitting identifying codes, and number of optimal identifying codes.","sentences":["A set $C$ of vertices in a graph $G=(V,E)$ is an identifying code if it is dominating and any two vertices of $V$ are dominated by distinct sets of codewords.","This paper presents a survey of Iiro Honkala's contributions to the study of identifying codes with respect to several aspects: complexity of computing an identifying code, combinatorics in binary Hamming spaces, infinite grids, relationships between identifying codes and usual parameters in graphs, structural properties of graphs admitting identifying codes, and number of optimal identifying codes."],"url":"http://arxiv.org/abs/2402.08264v1","category":"cs.DM"}
{"created":"2024-02-13 06:23:42","title":"Introenumerability, autoreducibility, and randomness","abstract":"We define $\\Psi$-autoreducible sets given an autoreduction procedure $\\Psi$. Then, we show that for any $\\Psi$, a measurable class of $\\Psi$-autoreducible sets has measure zero. Using this, we show that classes of cototal, uniformly introenumerable, introenumerable, and hyper-cototal enumeration degrees all have measure zero. By analyzing the arithmetical complexity of the classes of cototal sets and cototal enumeration degrees, we show that weakly 2-random sets cannot be cototal and weakly 3-random sets cannot be of cototal enumeration degree. Then, we see that this result is optimal by showing that there exists a 1-random cototal set and a 2-random set of cototal enumeration degree. For uniformly introenumerable degrees and introenumerable degrees, we utilize $\\Psi$-autoreducibility again to show the optimal result that no weakly 3-random sets can have introenumerable enumeration degree. We also show that no 1-random set can be introenumerable.","sentences":["We define $\\Psi$-autoreducible sets given an autoreduction procedure $\\Psi$.","Then, we show that for any $\\Psi$, a measurable class of $\\Psi$-autoreducible sets has measure zero.","Using this, we show that classes of cototal, uniformly introenumerable, introenumerable, and hyper-cototal enumeration degrees all have measure zero.","By analyzing the arithmetical complexity of the classes of cototal sets and cototal enumeration degrees, we show that weakly 2-random sets cannot be cototal and weakly","3-random sets cannot be of cototal enumeration degree.","Then, we see that this result is optimal by showing that there exists a 1-random cototal set and a 2-random set of cototal enumeration degree.","For uniformly introenumerable degrees and introenumerable degrees, we utilize $\\Psi$-autoreducibility again to show the optimal result that no weakly 3-random sets can have introenumerable enumeration degree.","We also show that no 1-random set can be introenumerable."],"url":"http://arxiv.org/abs/2402.08247v1","category":"math.LO"}
{"created":"2024-02-13 06:19:11","title":"Self-Reconfigurable V-shape Formation of Multiple UAVs in Narrow Space Environments","abstract":"This paper presents the design and implementation of a self-reconfigurable V-shape formation controller for multiple unmanned aerial vehicles (UAVs) navigating through narrow spaces in a dense obstacle environment. The selection of the V-shape formation is motivated by its maneuverability and visibility advantages. The main objective is to develop an effective formation control strategy that allows UAVs to autonomously adjust their positions to form the desired formation while navigating through obstacles. To achieve this, we propose a distributed behavior-based control algorithm that combines the behaviors designed for individual UAVs so that they together navigate the UAVs to their desired positions. The reconfiguration process is automatic, utilizing individual UAV sensing within the formation, allowing for dynamic adaptations such as opening/closing wings or merging into a straight line. Simulation results show that the self-reconfigurable V-shape formation offers adaptability and effectiveness for UAV formations in complex operational scenarios.","sentences":["This paper presents the design and implementation of a self-reconfigurable V-shape formation controller for multiple unmanned aerial vehicles (UAVs) navigating through narrow spaces in a dense obstacle environment.","The selection of the V-shape formation is motivated by its maneuverability and visibility advantages.","The main objective is to develop an effective formation control strategy that allows UAVs to autonomously adjust their positions to form the desired formation while navigating through obstacles.","To achieve this, we propose a distributed behavior-based control algorithm that combines the behaviors designed for individual UAVs so that they together navigate the UAVs to their desired positions.","The reconfiguration process is automatic, utilizing individual UAV sensing within the formation, allowing for dynamic adaptations such as opening/closing wings or merging into a straight line.","Simulation results show that the self-reconfigurable V-shape formation offers adaptability and effectiveness for UAV formations in complex operational scenarios."],"url":"http://arxiv.org/abs/2402.08245v1","category":"cs.RO"}
{"created":"2024-02-13 06:18:42","title":"APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks","abstract":"Activation function is a pivotal component of deep learning, facilitating the extraction of intricate data patterns. While classical activation functions like ReLU and its variants are extensively utilized, their static nature and simplicity, despite being advantageous, often limit their effectiveness in specialized tasks. The trainable activation functions also struggle sometimes to adapt to the unique characteristics of the data. Addressing these limitations, we introduce a novel trainable activation function, adaptive piecewise approximated activation linear unit (APALU), to enhance the learning performance of deep learning across a broad range of tasks. It presents a unique set of features that enable it to maintain stability and efficiency in the learning process while adapting to complex data representations. Experiments reveal significant improvements over widely used activation functions for different tasks. In image classification, APALU increases MobileNet and GoogleNet accuracy by 0.37% and 0.04%, respectively, on the CIFAR10 dataset. In anomaly detection, it improves the average area under the curve of One-CLASS Deep SVDD by 0.8% on the MNIST dataset, 1.81% and 1.11% improvements with DifferNet, and knowledge distillation, respectively, on the MVTech dataset. Notably, APALU achieves 100% accuracy on a sign language recognition task with a limited dataset. For regression tasks, APALU enhances the performance of deep neural networks and recurrent neural networks on different datasets. These improvements highlight the robustness and adaptability of APALU across diverse deep-learning applications.","sentences":["Activation function is a pivotal component of deep learning, facilitating the extraction of intricate data patterns.","While classical activation functions like ReLU and its variants are extensively utilized, their static nature and simplicity, despite being advantageous, often limit their effectiveness in specialized tasks.","The trainable activation functions also struggle sometimes to adapt to the unique characteristics of the data.","Addressing these limitations, we introduce a novel trainable activation function, adaptive piecewise approximated activation linear unit (APALU), to enhance the learning performance of deep learning across a broad range of tasks.","It presents a unique set of features that enable it to maintain stability and efficiency in the learning process while adapting to complex data representations.","Experiments reveal significant improvements over widely used activation functions for different tasks.","In image classification, APALU increases MobileNet and GoogleNet accuracy by 0.37% and 0.04%, respectively, on the CIFAR10 dataset.","In anomaly detection, it improves the average area under the curve of One-CLASS Deep SVDD by 0.8% on the MNIST dataset, 1.81% and 1.11% improvements with DifferNet, and knowledge distillation, respectively, on the MVTech dataset.","Notably, APALU achieves 100% accuracy on a sign language recognition task with a limited dataset.","For regression tasks, APALU enhances the performance of deep neural networks and recurrent neural networks on different datasets.","These improvements highlight the robustness and adaptability of APALU across diverse deep-learning applications."],"url":"http://arxiv.org/abs/2402.08244v1","category":"cs.LG"}
{"created":"2024-02-13 06:17:07","title":"Sensitivity of quantum walk to phase reversal and geometric perturbations: an exploration in complete graphs","abstract":"In this paper, we analyze the dynamics of quantum walks on a graph structure resulting from the integration of a main connected graph $G$ and a secondary connected graph $G'$. This composite graph is formed by a disjoint union of $G$ and $G'$, followed by the contraction of a selected pair of vertices creating a cut vertex $v^*$ and leading to a unique form of geometric perturbation. Our study focuses on instances where $G$ is a complete graph $K_N$ and $G'$ is a star graph $S_m$. The core of our analysis lies in exploring the impact of this geometric perturbation on the success probability of quantum walk-based search algorithms, particularly in an oracle-free context. Despite initial findings suggesting a low probability of locating the perturbed vertex $v^*$, we demonstrate that introducing a phase reversal to the system significantly enhances the success rate. Our results reveal that with an optimal running time and specific parameter conditions, the success probability can be substantially increased. The paper is structured to first define the theoretical framework, followed by the presentation of our main results, detailed proofs, and concluding with a summary of our findings and potential future research directions.","sentences":["In this paper, we analyze the dynamics of quantum walks on a graph structure resulting from the integration of a main connected graph $G$ and a secondary connected graph $G'$.","This composite graph is formed by a disjoint union of $G$ and $G'$, followed by the contraction of a selected pair of vertices creating a cut vertex $v^*$ and leading to a unique form of geometric perturbation.","Our study focuses on instances where $G$ is a complete graph $K_N$ and $G'$ is a star graph $S_m$. The core of our analysis lies in exploring the impact of this geometric perturbation on the success probability of quantum walk-based search algorithms, particularly in an oracle-free context.","Despite initial findings suggesting a low probability of locating the perturbed vertex $v^*$, we demonstrate that introducing a phase reversal to the system significantly enhances the success rate.","Our results reveal that with an optimal running time and specific parameter conditions, the success probability can be substantially increased.","The paper is structured to first define the theoretical framework, followed by the presentation of our main results, detailed proofs, and concluding with a summary of our findings and potential future research directions."],"url":"http://arxiv.org/abs/2402.08243v1","category":"quant-ph"}
{"created":"2024-02-13 06:12:17","title":"Causal Learning for Trustworthy Recommender Systems: A Survey","abstract":"Recommender Systems (RS) have significantly advanced online content discovery and personalized decision-making. However, emerging vulnerabilities in RS have catalyzed a paradigm shift towards Trustworthy RS (TRS). Despite numerous progress on TRS, most of them focus on data correlations while overlooking the fundamental causal nature in recommendation. This drawback hinders TRS from identifying the cause in addressing trustworthiness issues, leading to limited fairness, robustness, and explainability. To bridge this gap, causal learning emerges as a class of promising methods to augment TRS. These methods, grounded in reliable causality, excel in mitigating various biases and noises while offering insightful explanations for TRS. However, there lacks a timely survey in this vibrant area. This paper creates an overview of TRS from the perspective of causal learning. We begin by presenting the advantages and common procedures of Causality-oriented TRS (CTRS). Then, we identify potential trustworthiness challenges at each stage and link them to viable causal solutions, followed by a classification of CTRS methods. Finally, we discuss several future directions for advancing this field.","sentences":["Recommender Systems (RS) have significantly advanced online content discovery and personalized decision-making.","However, emerging vulnerabilities in RS have catalyzed a paradigm shift towards Trustworthy RS (TRS).","Despite numerous progress on TRS, most of them focus on data correlations while overlooking the fundamental causal nature in recommendation.","This drawback hinders TRS from identifying the cause in addressing trustworthiness issues, leading to limited fairness, robustness, and explainability.","To bridge this gap, causal learning emerges as a class of promising methods to augment TRS.","These methods, grounded in reliable causality, excel in mitigating various biases and noises while offering insightful explanations for TRS.","However, there lacks a timely survey in this vibrant area.","This paper creates an overview of TRS from the perspective of causal learning.","We begin by presenting the advantages and common procedures of Causality-oriented TRS (CTRS).","Then, we identify potential trustworthiness challenges at each stage and link them to viable causal solutions, followed by a classification of CTRS methods.","Finally, we discuss several future directions for advancing this field."],"url":"http://arxiv.org/abs/2402.08241v1","category":"cs.IR"}
{"created":"2024-02-13 05:51:09","title":"Asynchronous Distributed Coordinated Hybrid Precoding in Multi-cell mmWave Wireless Networks","abstract":"Asynchronous distributed hybrid beamformers (ADBF) are conceived for minimizing the total transmit power subject to signal-to-interference-plus-noise ratio (SINR) constraints at the users. Our design requires only limited information exchange between the base stations (BSs) of the mmWave multi-cell coordinated (MCC) networks considered. To begin with, a semidefinite relaxation (SDR)-based fully-digital (FD) beamformer is designed for a centralized MCC system. Subsequently, a Bayesian learning (BL) technique is harnessed for decomposing the FD beamformer into its analog and baseband components and construct a hybrid transmit precoder (TPC). However, the centralized TPC design requires global channel state information (CSI), hence it results in a high signaling overhead. An alternating direction based method of multipliers (ADMM) technique is developed for a synchronous distributed beamformer (SDBF) design, which relies only on limited information exchange among the BSs, thus reducing the signaling overheads required by the centralized TPC design procedure.   However, the SDBF design is challenging, since it requires the updates from the BSs to be strictly synchronized. As a remedy, an ADBF framework is developed that mitigates the inter-cell interference (ICI) and also control the asynchrony in the system.   Furthermore, the above ADBF framework is also extended to the robust ADBF (R-ADBF) algorithm that incorporates the CSI uncertainty into the design procedure for minimizing the the worst-case transmit power. Our simulation results illustrate both the enhanced performance and the improved convergence properties of the ADMM-based ADBF and R-ADBF schemes.","sentences":["Asynchronous distributed hybrid beamformers (ADBF) are conceived for minimizing the total transmit power subject to signal-to-interference-plus-noise ratio (SINR) constraints at the users.","Our design requires only limited information exchange between the base stations (BSs) of the mmWave multi-cell coordinated (MCC) networks considered.","To begin with, a semidefinite relaxation (SDR)-based fully-digital (FD) beamformer is designed for a centralized MCC system.","Subsequently, a Bayesian learning (BL) technique is harnessed for decomposing the FD beamformer into its analog and baseband components and construct a hybrid transmit precoder (TPC).","However, the centralized TPC design requires global channel state information (CSI), hence it results in a high signaling overhead.","An alternating direction based method of multipliers (ADMM) technique is developed for a synchronous distributed beamformer (SDBF) design, which relies only on limited information exchange among the BSs, thus reducing the signaling overheads required by the centralized TPC design procedure.   ","However, the SDBF design is challenging, since it requires the updates from the BSs to be strictly synchronized.","As a remedy, an ADBF framework is developed that mitigates the inter-cell interference (ICI) and also control the asynchrony in the system.   ","Furthermore, the above ADBF framework is also extended to the robust ADBF (R-ADBF) algorithm that incorporates the CSI uncertainty into the design procedure for minimizing the the worst-case transmit power.","Our simulation results illustrate both the enhanced performance and the improved convergence properties of the ADMM-based ADBF and R-ADBF schemes."],"url":"http://arxiv.org/abs/2402.08231v1","category":"cs.IT"}
{"created":"2024-02-13 05:26:25","title":"Integration of multiview microbiome data for deciphering microbiome-metabolome-disease pathways","abstract":"The intricate interplay between host organisms and their gut microbiota has catalyzed research into the microbiome's role in disease, shedding light on novel aspects of disease pathogenesis. However, the mechanisms through which the microbiome exerts its influence on disease remain largely unclear. In this study, we first introduce a structural equation model to delineate the pathways connecting the microbiome, metabolome, and disease processes, utilizing a target multiview microbiome data. To mitigate the challenges posed by hidden confounders, we further propose an integrative approach that incorporates data from an external microbiome cohort. This method also supports the identification of disease-specific and microbiome-associated metabolites that are missing in the target cohort. We provide theoretical underpinnings for the estimations derived from our integrative approach, demonstrating estimation consistency and asymptotic normality. The effectiveness of our methodologies is validated through comprehensive simulation studies and an empirical application to inflammatory bowel disease, highlighting their potential to unravel the complex relationships between the microbiome, metabolome, and disease.","sentences":["The intricate interplay between host organisms and their gut microbiota has catalyzed research into the microbiome's role in disease, shedding light on novel aspects of disease pathogenesis.","However, the mechanisms through which the microbiome exerts its influence on disease remain largely unclear.","In this study, we first introduce a structural equation model to delineate the pathways connecting the microbiome, metabolome, and disease processes, utilizing a target multiview microbiome data.","To mitigate the challenges posed by hidden confounders, we further propose an integrative approach that incorporates data from an external microbiome cohort.","This method also supports the identification of disease-specific and microbiome-associated metabolites that are missing in the target cohort.","We provide theoretical underpinnings for the estimations derived from our integrative approach, demonstrating estimation consistency and asymptotic normality.","The effectiveness of our methodologies is validated through comprehensive simulation studies and an empirical application to inflammatory bowel disease, highlighting their potential to unravel the complex relationships between the microbiome, metabolome, and disease."],"url":"http://arxiv.org/abs/2402.08222v1","category":"stat.ME"}
{"created":"2024-02-13 05:25:20","title":"Computing Threshold Circuits with Void Reactions in Step Chemical Reaction Networks","abstract":"We introduce a new model of \\emph{step} Chemical Reaction Networks (step CRNs), motivated by the step-wise addition of materials in standard lab procedures. Step CRNs have ordered reactants that transform into products via reaction rules over a series of steps. We study an important subset of weak reaction rules, \\emph{void} rules, in which chemical species may only be deleted but never changed. We demonstrate the capabilities of these simple limited systems to simulate threshold circuits and compute functions using various configurations of rule sizes and step constructions, and prove that without steps, void rules are incapable of these computations, which further motivates the step model. Additionally, we prove the coNP-completeness of verifying if a given step CRN computes a function, holding even for $O(1)$ step systems.","sentences":["We introduce a new model of \\emph{step} Chemical Reaction Networks (step CRNs), motivated by the step-wise addition of materials in standard lab procedures.","Step CRNs have ordered reactants that transform into products via reaction rules over a series of steps.","We study an important subset of weak reaction rules, \\emph{void} rules, in which chemical species may only be deleted but never changed.","We demonstrate the capabilities of these simple limited systems to simulate threshold circuits and compute functions using various configurations of rule sizes and step constructions, and prove that without steps, void rules are incapable of these computations, which further motivates the step model.","Additionally, we prove the coNP-completeness of verifying if a given step CRN computes a function, holding even for $O(1)$ step systems."],"url":"http://arxiv.org/abs/2402.08220v1","category":"q-bio.MN"}
{"created":"2024-02-13 05:11:44","title":"Integrable Approximations of Dispersive Shock Waves of the Granular Chain","abstract":"In the present work we revisit the shock wave dynamics in a granular chain with precompression. By approximating the model by an $\\alpha$-Fermi-Pasta-Ulam-Tsingou chain, we leverage the connection of the latter in the strain variable formulation to two separate integrable models, one continuum, namely the KdV equation, and one discrete, namely the Toda lattice. We bring to bear the Whitham modulation theory analysis of such integrable systems and the analytical approximation of their dispersive shock waves in order to provide, through the lens of the reductive connection to the granular crystal, an approximation to the shock wave of the granular problem. A detailed numerical comparison of the original granular chain and its approximate integrable-system based dispersive shocks proves very favorable in a wide parametric range. The gradual deviations between (approximate) theory and numerical computation, as amplitude parameters of the solution increase are quantified and discussed.","sentences":["In the present work we revisit the shock wave dynamics in a granular chain with precompression.","By approximating the model by an $\\alpha$-Fermi-Pasta-Ulam-Tsingou chain, we leverage the connection of the latter in the strain variable formulation to two separate integrable models, one continuum, namely the KdV equation, and one discrete, namely the Toda lattice.","We bring to bear the Whitham modulation theory analysis of such integrable systems and the analytical approximation of their dispersive shock waves in order to provide, through the lens of the reductive connection to the granular crystal, an approximation to the shock wave of the granular problem.","A detailed numerical comparison of the original granular chain and its approximate integrable-system based dispersive shocks proves very favorable in a wide parametric range.","The gradual deviations between (approximate) theory and numerical computation, as amplitude parameters of the solution increase are quantified and discussed."],"url":"http://arxiv.org/abs/2402.08218v1","category":"nlin.PS"}
{"created":"2024-02-13 04:32:29","title":"BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents","abstract":"Embodied agents capable of complex physical skills can improve productivity, elevate life quality, and reshape human-machine collaboration. We aim at autonomous training of embodied agents for various tasks involving mainly large foundation models. It is believed that these models could act as a brain for embodied agents; however, existing methods heavily rely on humans for task proposal and scene customization, limiting the learning autonomy, training efficiency, and generalization of the learned policies. In contrast, we introduce a brain-body synchronization ({\\it BBSEA}) scheme to promote embodied learning in unknown environments without human involvement. The proposed combines the wisdom of foundation models (``brain'') with the physical capabilities of embodied agents (``body''). Specifically, it leverages the ``brain'' to propose learnable physical tasks and success metrics, enabling the ``body'' to automatically acquire various skills by continuously interacting with the scene. We carry out an exploration of the proposed autonomous learning scheme in a table-top setting, and we demonstrate that the proposed synchronization can generate diverse tasks and develop multi-task policies with promising adaptability to new tasks and configurations. We will release our data, code, and trained models to facilitate future studies in building autonomously learning agents with large foundation models in more complex scenarios. More visualizations are available at \\href{https://bbsea-embodied-ai.github.io}{https://bbsea-embodied-ai.github.io}","sentences":["Embodied agents capable of complex physical skills can improve productivity, elevate life quality, and reshape human-machine collaboration.","We aim at autonomous training of embodied agents for various tasks involving mainly large foundation models.","It is believed that these models could act as a brain for embodied agents; however, existing methods heavily rely on humans for task proposal and scene customization, limiting the learning autonomy, training efficiency, and generalization of the learned policies.","In contrast, we introduce a brain-body synchronization ({\\it BBSEA}) scheme to promote embodied learning in unknown environments without human involvement.","The proposed combines the wisdom of foundation models (``brain'') with the physical capabilities of embodied agents (``body'').","Specifically, it leverages the ``brain'' to propose learnable physical tasks and success metrics, enabling the ``body'' to automatically acquire various skills by continuously interacting with the scene.","We carry out an exploration of the proposed autonomous learning scheme in a table-top setting, and we demonstrate that the proposed synchronization can generate diverse tasks and develop multi-task policies with promising adaptability to new tasks and configurations.","We will release our data, code, and trained models to facilitate future studies in building autonomously learning agents with large foundation models in more complex scenarios.","More visualizations are available at \\href{https://bbsea-embodied-ai.github.io}{https://bbsea-embodied-ai.github.io}"],"url":"http://arxiv.org/abs/2402.08212v1","category":"cs.RO"}
{"created":"2024-02-13 04:09:26","title":"TurtleRabbit 2024 SSL Team Description Paper","abstract":"TurtleRabbit is a new RoboCup SSL team from Western Sydney University. This team description paper presents our approach in navigating some of the challenges in developing a new SSL team from scratch. SSL is dominated by teams with extensive experience and customised equipment that has been developed over many years. Here, we outline our approach in overcoming some of the complexities associated with replicating advanced open-sourced designs and managing the high costs of custom components. Opting for simplicity and cost-effectiveness, our strategy primarily employs off-the-shelf electronics components and ``hobby'' brushless direct current (BLDC) motors, complemented by 3D printing and CNC milling. This approach helped us to streamline the development process and, with our open-sourced hardware design, hopefully will also lower the bar for other teams to enter RoboCup SSL in the future. The paper details the specific hardware choices, their approximate costs, the integration of electronics and mechanics, and the initial steps taken in software development, for our entry into SSL that aims to be simple yet competitive.","sentences":["TurtleRabbit is a new RoboCup SSL team from Western Sydney University.","This team description paper presents our approach in navigating some of the challenges in developing a new SSL team from scratch.","SSL is dominated by teams with extensive experience and customised equipment that has been developed over many years.","Here, we outline our approach in overcoming some of the complexities associated with replicating advanced open-sourced designs and managing the high costs of custom components.","Opting for simplicity and cost-effectiveness, our strategy primarily employs off-the-shelf electronics components and ``hobby'' brushless direct current (BLDC) motors, complemented by 3D printing and CNC milling.","This approach helped us to streamline the development process and, with our open-sourced hardware design, hopefully will also lower the bar for other teams to enter RoboCup SSL in the future.","The paper details the specific hardware choices, their approximate costs, the integration of electronics and mechanics, and the initial steps taken in software development, for our entry into SSL that aims to be simple yet competitive."],"url":"http://arxiv.org/abs/2402.08205v1","category":"cs.RO"}
{"created":"2024-02-13 03:43:25","title":"Explicit Periodic Solutions in a Delay Differential Equation","abstract":"We construct stable periodic solutions for a simple form nonlinear delay differential equation (DDE) with a periodic coefficient. The equation involves one underlying nonlinearity with the multiplicative periodic coefficient. The well-known idea of reduction to interval maps is used in the case under consideration, when both the defining nonlinearity and the periodic coefficient are piece-wise constant functions. The stable periodic dynamics persist under a smoothing procedure in a small neighborhood of the discontinuity set. This work continues the research in recent paper [7] on stable periodic solutions of differential delay equations with periodic coefficients.","sentences":["We construct stable periodic solutions for a simple form nonlinear delay differential equation (DDE) with a periodic coefficient.","The equation involves one underlying nonlinearity with the multiplicative periodic coefficient.","The well-known idea of reduction to interval maps is used in the case under consideration, when both the defining nonlinearity and the periodic coefficient are piece-wise constant functions.","The stable periodic dynamics persist under a smoothing procedure in a small neighborhood of the discontinuity set.","This work continues the research in recent paper [7] on stable periodic solutions of differential delay equations with periodic coefficients."],"url":"http://arxiv.org/abs/2402.08197v1","category":"math.DS"}
{"created":"2024-02-13 03:40:51","title":"Basic (Dolbeault) Cohomology of Foliated Manifolds with Boundary","abstract":"In this paper, we develop $L^2$ theory for Riemannian and Hermitian foliations on manifolds with basic boundary. We establish a decomposition theorem, various vanishing theorems, a twisted duality theorem for basic cohomologies and an extension theorem for basic forms of induced Riemannian foliation on the boundary. We prove the complex analogues for Hermitian foliations. To show the Dolbeault decomposition of basic forms, we extend Morrey's basic estimate to foliation version. We also investigate the global regularity for $\\bar{\\partial}_B$-equations.","sentences":["In this paper, we develop $L^2$ theory for Riemannian and Hermitian foliations on manifolds with basic boundary.","We establish a decomposition theorem, various vanishing theorems, a twisted duality theorem for basic cohomologies and an extension theorem for basic forms of induced Riemannian foliation on the boundary.","We prove the complex analogues for Hermitian foliations.","To show the Dolbeault decomposition of basic forms, we extend Morrey's basic estimate to foliation version.","We also investigate the global regularity for $\\bar{\\partial}_B$-equations."],"url":"http://arxiv.org/abs/2402.08196v1","category":"math.DG"}
{"created":"2024-02-13 03:31:36","title":"Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems","abstract":"Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem structures, including jointly learning system parameters, observation parameters, and latent state variables.","sentences":["Efficient inference in high-dimensional models remains a central challenge in machine learning.","This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods.","GEnBP updates ensembles by passing low-rank local messages in a graphical model structure.","This combination inherits favourable qualities from each method.","Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes.","The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures.","GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension.","This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion.","GEnBP can be applied to general problem structures, including jointly learning system parameters, observation parameters, and latent state variables."],"url":"http://arxiv.org/abs/2402.08193v1","category":"cs.LG"}
{"created":"2024-02-13 03:31:20","title":"Monolithic Silicon-Photonics Linear-Algebra Accelerators Enabling Next-Gen Massive MIMO","abstract":"A system-on-chip (SoC) photonic-electronic linear-algebra accelerator with the features of wavelength-division-multiplexing (WDM) based broadband photodetections and high-dimensional matrix-inversion operations fabricated in advanced monolithic silicon-photonics (M-SiPh) semiconductor process technology is proposed to achieve substantial leaps in computation density and energy efficiency, including realistic considerations of energy/area overhead due to electronic/photonic on-chip conversions, integrations, and calibrations through holistic co-design methodologies to support linear-detection based massive multiple-input multiple-output (MIMO) decoding technology requiring the inversion of channel matrices and other emergent applications limited by linear-algebra computation capacities.","sentences":["A system-on-chip (SoC) photonic-electronic linear-algebra accelerator with the features of wavelength-division-multiplexing (WDM) based broadband photodetections and high-dimensional matrix-inversion operations fabricated in advanced monolithic silicon-photonics (M-SiPh) semiconductor process technology is proposed to achieve substantial leaps in computation density and energy efficiency, including realistic considerations of energy/area overhead due to electronic/photonic on-chip conversions, integrations, and calibrations through holistic co-design methodologies to support linear-detection based massive multiple-input multiple-output (MIMO) decoding technology requiring the inversion of channel matrices and other emergent applications limited by linear-algebra computation capacities."],"url":"http://arxiv.org/abs/2402.08192v1","category":"eess.SY"}
{"created":"2024-02-13 03:19:09","title":"Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs","abstract":"When creating plans, policies, or applications for people, it is challenging for designers to think through the strategic ways that different people will behave. Recently, Large Language Models (LLMs) have been shown to create realistic simulations of human-like behavior based on personas. We build on this to investigate whether LLMs can simulate human strategic behavior. Human strategies are complex because they take into account social norms in addition to aiming to maximize personal gain. The ultimatum game is a classic economics experiment used to understand human strategic behavior in a social setting. It shows that people will often choose to \"punish\" other players to enforce social norms rather than to maximize personal profits. We test whether LLMs can replicate this complex behavior in simulations. We compare two architectures: single- and multi-agent LLMs. We compare their abilities to (1) simulate human-like actions in the ultimatum game, (2) simulate two player personalities, greedy and fair, and (3) create robust strategies that are logically complete and consistent with personality. Our evaluation shows the multi-agent architecture is much more accurate than single LLMs (88% vs. 50%) in simulating human strategy creation and actions for personality pairs. Thus there is potential to use LLMs to simulate human strategic behavior to help designers, planners, and policymakers perform preliminary exploration of how people behave in systems.","sentences":["When creating plans, policies, or applications for people, it is challenging for designers to think through the strategic ways that different people will behave.","Recently, Large Language Models (LLMs) have been shown to create realistic simulations of human-like behavior based on personas.","We build on this to investigate whether LLMs can simulate human strategic behavior.","Human strategies are complex because they take into account social norms in addition to aiming to maximize personal gain.","The ultimatum game is a classic economics experiment used to understand human strategic behavior in a social setting.","It shows that people will often choose to \"punish\" other players to enforce social norms rather than to maximize personal profits.","We test whether LLMs can replicate this complex behavior in simulations.","We compare two architectures: single- and multi-agent LLMs.","We compare their abilities to (1) simulate human-like actions in the ultimatum game, (2) simulate two player personalities, greedy and fair, and (3) create robust strategies that are logically complete and consistent with personality.","Our evaluation shows the multi-agent architecture is much more accurate than single LLMs (88% vs. 50%) in simulating human strategy creation and actions for personality pairs.","Thus there is potential to use LLMs to simulate human strategic behavior to help designers, planners, and policymakers perform preliminary exploration of how people behave in systems."],"url":"http://arxiv.org/abs/2402.08189v1","category":"cs.HC"}
{"created":"2024-02-13 03:16:59","title":"High-cadence Timing of Binary Pulsars with CHIME","abstract":"We performed near-daily observations on the binary pulsars PSR J0218+4232, PSR J1518+4904 and PSR J2023+2853 with the Canadian Hydrogen Intensity Mapping Experiment (CHIME). For the first time, we detected the Shapiro time delay in all three pulsar-binary systems, using only 2--4 years of CHIME/Pulsar timing data. We measured the pulsar masses to be $1.49^{+0.23}_{-0.20}$ M$_\\odot$, $1.470^{+0.030}_{-0.034}$ M$_\\odot$ and $1.50^{+0.49}_{-0.38}$ M$_\\odot$ respectively. The companion mass to PSR J0218+4232 was found to be $0.179^{+0.018}_{-0.016}$ M$_\\odot$. We constrained the mass of the neutron-star companion of PSR J1518+4904 to be $1.248^{+0.035}_{-0.029}$ M$_\\odot$, using the observed apsidal motion as a constraint on mass estimation. The binary companion to PSR J2023+2853 was found to have a mass of $0.93^{+0.17}_{-0.14}$ M$_\\odot$; in the context of the near-circular orbit, this mass estimate suggests that the companion to PSR J2023+2853 is likely a high-mass white dwarf. By comparing the timing model obtained for PSR J0218+4232 with previous observations, we found a significant change in the observed orbital period of the system of $\\dot{P_{\\rm b}} = 0.14(2) \\times 10^{-12}$; we determined that this variation arises from ``Shklovskii acceleration\" due to relative motion of the binary system, and used this measurement to estimate a distance of $d=(6.7 \\pm 1.0)$ kpc to PSR J0218+4232. This work demonstrates the capability of high-cadence observations, enabled by the CHIME/Pulsar system, to detect and refine general-relativistic effects of binary pulsars over short observing timescales.","sentences":["We performed near-daily observations on the binary pulsars PSR J0218+4232, PSR J1518+4904 and PSR J2023+2853 with the Canadian Hydrogen Intensity Mapping Experiment (CHIME).","For the first time, we detected the Shapiro time delay in all three pulsar-binary systems, using only 2--4 years of CHIME/Pulsar timing data.","We measured the pulsar masses to be $1.49^{+0.23}_{-0.20}$ M$_\\odot$, $1.470^{+0.030}_{-0.034}$ M$_\\odot$ and $1.50^{+0.49}_{-0.38}$ M$_\\odot$ respectively.","The companion mass to PSR J0218+4232 was found to be $0.179^{+0.018}_{-0.016}$ M$_\\odot$. We constrained the mass of the neutron-star companion of PSR J1518+4904 to be $1.248^{+0.035}_{-0.029}$ M$_\\odot$, using the observed apsidal motion as a constraint on mass estimation.","The binary companion to PSR J2023+2853 was found to have a mass of $0.93^{+0.17}_{-0.14}$ M$_\\odot$; in the context of the near-circular orbit, this mass estimate suggests that the companion to PSR J2023+2853 is likely a high-mass white dwarf.","By comparing the timing model obtained for PSR J0218+4232 with previous observations, we found a significant change in the observed orbital period of the system of $\\dot{P_{\\rm b}} = 0.14(2)","\\times 10^{-12}$; we determined that this variation arises from ``Shklovskii acceleration\" due to relative motion of the binary system, and used this measurement to estimate a distance of $d=(6.7 \\pm 1.0)$ kpc to PSR J0218","+4232.","This work demonstrates the capability of high-cadence observations, enabled by the CHIME/Pulsar system, to detect and refine general-relativistic effects of binary pulsars over short observing timescales."],"url":"http://arxiv.org/abs/2402.08188v1","category":"astro-ph.HE"}
{"created":"2024-02-13 03:11:37","title":"A POD approach to identify and control PDEs online through State Dependent Riccati equations","abstract":"We address the control of Partial Differential equations (PDEs) with unknown parameters. Our objective is to devise an efficient algorithm capable of both identifying and controlling the unknown system. We assume that the desired PDE is observable provided a control input and an initial condition. The method works as follows, given an estimated parameter configuration, we compute the corresponding control using the State-Dependent Riccati Equation (SDRE) approach. Subsequently, after computing the control, we observe the trajectory and estimate a new parameter configuration using Bayesian Linear Regression method. This process iterates until reaching the final time, incorporating a defined stopping criterion for updating the parameter configuration. We also focus on the computational cost of the algorithm, since we deal with high dimensional systems. To enhance the efficiency of the method, indeed, we employ model order reduction through the Proper Orthogonal Decomposition (POD) method. The considered problem's dimension is notably large, and POD provides impressive speedups. Further, a detailed description on the coupling between POD and SDRE is also provided. Finally, numerical examples will show the accurateness of our method across two test cases.","sentences":["We address the control of Partial Differential equations (PDEs) with unknown parameters.","Our objective is to devise an efficient algorithm capable of both identifying and controlling the unknown system.","We assume that the desired PDE is observable provided a control input and an initial condition.","The method works as follows, given an estimated parameter configuration, we compute the corresponding control using the State-Dependent Riccati Equation (SDRE) approach.","Subsequently, after computing the control, we observe the trajectory and estimate a new parameter configuration using Bayesian Linear Regression method.","This process iterates until reaching the final time, incorporating a defined stopping criterion for updating the parameter configuration.","We also focus on the computational cost of the algorithm, since we deal with high dimensional systems.","To enhance the efficiency of the method, indeed, we employ model order reduction through the Proper Orthogonal Decomposition (POD) method.","The considered problem's dimension is notably large, and POD provides impressive speedups.","Further, a detailed description on the coupling between POD and SDRE is also provided.","Finally, numerical examples will show the accurateness of our method across two test cases."],"url":"http://arxiv.org/abs/2402.08186v1","category":"math.OC"}
{"created":"2024-02-13 02:26:49","title":"Representations of a quantum-deformed Lorentz algebra, Clebsch-Gordan map, and Fenchel-Nielsen representation of quantum complex flat connections at level-$k$","abstract":"A family of infinite-dimensional irreducible $\\star$-representations on $\\mathcal{H}\\simeq L^2(\\mathbb{R})\\otimes\\mathbb{C}^k$ is defined for a quantum-deformed Lorentz algebra $U_\\mathbf{q}(sl_2)\\otimes U_{\\tilde{\\mathbf{q}}}(sl_2)$, where $\\mathbf{q}=\\exp[\\frac{2\\pi i}{k}(1+b^2)]$ and $\\tilde{\\mathbf{q}}=\\exp[\\frac{2\\pi i}{k}(1+b^{-2})]$ with $k\\in\\mathbb{Z}_+$ and $|b|=1$. The representations are constructed with the irreducible representation of quantum torus algebra at level-$k$, which is developed from the quantization of $\\mathrm{SL}(2,\\mathbb{C})$ Chern-Simons theory. We study the Clebsch-Gordan decomposition of the tensor product representation, and we show that it reduces to the same problem as diagonalizing the complex Fenchel-Nielson length operators in quantizing $\\mathrm{SL}(2,\\mathbb{C})$ flat connections on 4-holed sphere. Finally, the spectral decomposition of the complex Fenchel-Nielson length operators results in the direct-integral representation of the Hilbert space $\\mathcal{H}$, which we call the Fenchel-Nielson representation.","sentences":["A family of infinite-dimensional irreducible $\\star$-representations on $\\mathcal{H}\\simeq L^2(\\mathbb{R})\\otimes\\mathbb{C}^k$ is defined for a quantum-deformed Lorentz algebra $U_\\mathbf{q}(sl_2)\\otimes U_{\\tilde{\\mathbf{q}}}(sl_2)$, where $\\mathbf{q}=\\exp[\\frac{2\\pi i}{k}(1+b^2)]$ and $\\tilde{\\mathbf{q}}=\\exp[\\frac{2\\pi i}{k}(1+b^{-2})]$ with $k\\in\\mathbb{Z}_+$ and $|b|=1$. The representations are constructed with the irreducible representation of quantum torus algebra at level-$k$, which is developed from the quantization of $\\mathrm{SL}(2,\\mathbb{C})$ Chern-Simons theory.","We study the Clebsch-Gordan decomposition of the tensor product representation, and we show that it reduces to the same problem as diagonalizing the complex Fenchel-Nielson length operators in quantizing $\\mathrm{SL}(2,\\mathbb{C})$ flat connections on 4-holed sphere.","Finally, the spectral decomposition of the complex Fenchel-Nielson length operators results in the direct-integral representation of the Hilbert space $\\mathcal{H}$, which we call the Fenchel-Nielson representation."],"url":"http://arxiv.org/abs/2402.08176v1","category":"hep-th"}
{"created":"2024-02-13 02:10:03","title":"On the Submultiplicativity of Matrix Norms Induced by Random Vectors","abstract":"In a recent article, Ch\\'avez, Garcia and Hurley introduced a new family of norms $\\|\\cdot\\|_{\\mathbf{X},d}$ on the space of $n \\times n$ complex matrices which are induced by random vectors $\\mathbf{X}$ having finite $d$-moments. Therein, the authors asked under which conditions the norms induced by a scalar multiple of $\\mathbf{X}$ are submultiplicative. In this paper, this question is completely answered by proving that this is always the case, as long as the entries of $\\mathbf{X}$ have finite $p$-moments for $p=\\max\\{2+\\varepsilon,d\\}$.","sentences":["In a recent article, Ch\\'avez, Garcia and Hurley introduced a new family of norms $\\|\\cdot\\|_{\\mathbf{X},d}$ on the space of $n \\times n$ complex matrices which are induced by random vectors $\\mathbf{X}$ having finite $d$-moments.","Therein, the authors asked under which conditions the norms induced by a scalar multiple of $\\mathbf{X}$ are submultiplicative.","In this paper, this question is completely answered by proving that this is always the case, as long as the entries of $\\mathbf{X}$ have finite $p$-moments for $p=\\max\\{2+\\varepsilon,d\\}$."],"url":"http://arxiv.org/abs/2402.08173v1","category":"math.MG"}
{"created":"2024-02-13 01:52:42","title":"What is a \"bug\"? On subjectivity, epistemic power, and implications for software research","abstract":"Considerable effort in software research and practice is spent on bugs. Finding, reporting, tracking, triaging, attempting to fix them automatically, detecting \"bug smells\" -these comprise a substantial portion of large projects' time and development cost, and are of significant interest to researchers in Software Engineering, Programming Languages, and beyond.   But, what is a bug, exactly? While segmentation faults rarely spark joy, most bugs are not so clear cut. Per the Oxford English Dictionary, the word \"bug\" has been a colloquialism for an engineering \"defect\" at least since the 1870s. Most modern software-oriented definitions speak to a disconnect between what a developer intended and what a program actually does. Formal verification, from its inception, has developed means to identify deviations from a formal specification, expected to more or less fully encode desired behavior. However, software is rarely accompanied by full and formal specifications, and this intention is instead treated as implicit or partially-documented at best. The International Software Testing Qualifications board writes: \"A human being can make an error (mistake), which produces a defect (fault, bug) in the program code, or in a document. If a defect in code is executed, the system may fail to do what it should do (or do something it shouldn't), causing a failure. Defects may result in failures, but not all [do]\". Most sources forsake this precision. The influential paper \"Finding bugs is easy\" begins by saying \"bug patterns are code idioms that are often errors\"-with no particular elaboration. Other work relies on imperfect practical proxies for specifications. For example, in automatic program repair research, a bug corresponds to a failing test case: when the test passes, the bug is considered fixed.   However, when we interrogate fairly straightforward definitions, they start to break down...","sentences":["Considerable effort in software research and practice is spent on bugs.","Finding, reporting, tracking, triaging, attempting to fix them automatically, detecting \"bug smells\" -these comprise a substantial portion of large projects' time and development cost, and are of significant interest to researchers in Software Engineering, Programming Languages, and beyond.   ","But, what is a bug, exactly?","While segmentation faults rarely spark joy, most bugs are not so clear cut.","Per the Oxford English Dictionary, the word \"bug\" has been a colloquialism for an engineering \"defect\" at least since the 1870s.","Most modern software-oriented definitions speak to a disconnect between what a developer intended and what a program actually does.","Formal verification, from its inception, has developed means to identify deviations from a formal specification, expected to more or less fully encode desired behavior.","However, software is rarely accompanied by full and formal specifications, and this intention is instead treated as implicit or partially-documented at best.","The International Software Testing Qualifications board writes: \"A human being can make an error (mistake), which produces a defect (fault, bug) in the program code, or in a document.","If a defect in code is executed, the system may fail to do what it should do (or do something it shouldn't), causing a failure.","Defects may result in failures, but not all","[do]\".","Most sources forsake this precision.","The influential paper \"Finding bugs is easy\" begins by saying \"bug patterns are code idioms that are often errors\"-with no particular elaboration.","Other work relies on imperfect practical proxies for specifications.","For example, in automatic program repair research, a bug corresponds to a failing test case: when the test passes, the bug is considered fixed.   ","However, when we interrogate fairly straightforward definitions, they start to break down..."],"url":"http://arxiv.org/abs/2402.08165v1","category":"cs.SE"}
{"created":"2024-02-13 01:30:48","title":"AC-Josephson Effect and Sub-Comb Mode-Locking in a Kerr-Induced Synchronized Cavity Soliton","abstract":"Kerr-induced synchronization (KIS) [1] involves the capture of a dissipative Kerr soliton (DKS) microcomb [2] tooth by a reference laser injected into the DKS resonator. This phase-locking behavior is described by an Adler equation whose analogous form describes numerous other physical systems [3], such as Josephson junctions [4]. We present an AC version of KIS whose behavior is similar to microwave-driven Josephson junctions, where periodic synchronization occurs as so-called Shapiro steps. We demonstrate consistent results in the AC-KIS dynamics predicted by the Adler model, Lugiato-Lefever equation, and experimental data from a chip-integrated microresonator system. The (integer) Shapiro steps in KIS can simply be explained as the sideband created through the reference laser phase modulation triggering the synchronization. Notably, our optical system allows for easy tuning of the Adler damping parameter, enabling the further observation of fractional-Shapiro steps, where the synchronization happens at a fraction of the driving microwave frequency. Here, we show that the comb tooth is indirectly captured thanks to a four-wave mixing Bragg-scattering process, leading to sub-comb mode-locking, and we demonstrate this experimentally through noise considerations. Our work opens the door to the study of synchronization phenomena in the context of microresonator frequency combs, synthesis of condensed-matter state analogues with DKSs, and the use of the fractional Shapiro steps for flexible and tunable access to the KIS regime.","sentences":["Kerr-induced synchronization (KIS)","[1] involves the capture of a dissipative Kerr soliton (DKS) microcomb","[2] tooth by a reference laser injected into the DKS resonator.","This phase-locking behavior is described by an Adler equation whose analogous form describes numerous other physical systems","[3], such as Josephson junctions [4].","We present an AC version of KIS whose behavior is similar to microwave-driven Josephson junctions, where periodic synchronization occurs as so-called Shapiro steps.","We demonstrate consistent results in the AC-KIS dynamics predicted by the Adler model, Lugiato-Lefever equation, and experimental data from a chip-integrated microresonator system.","The (integer) Shapiro steps in KIS can simply be explained as the sideband created through the reference laser phase modulation triggering the synchronization.","Notably, our optical system allows for easy tuning of the Adler damping parameter, enabling the further observation of fractional-Shapiro steps, where the synchronization happens at a fraction of the driving microwave frequency.","Here, we show that the comb tooth is indirectly captured thanks to a four-wave mixing Bragg-scattering process, leading to sub-comb mode-locking, and we demonstrate this experimentally through noise considerations.","Our work opens the door to the study of synchronization phenomena in the context of microresonator frequency combs, synthesis of condensed-matter state analogues with DKSs, and the use of the fractional Shapiro steps for flexible and tunable access to the KIS regime."],"url":"http://arxiv.org/abs/2402.08154v1","category":"physics.optics"}
{"created":"2024-02-13 00:29:56","title":"Thermal processing of primordial pebbles in evolving protoplanetary disks","abstract":"During protoplanetary disk formation, dust grains located in the outer disk retain their pristine icy composition, while solids in the inner stellar-heated disk undergo volatile loss. This process may have left a fossil record in Solar System material showing different nucelosynthetic imprints that have been attributed to different degrees of thermal processing. However, it remains unclear how a large mass fraction of thermally-processed inner-disk pebbles is produced and how these grains are subsequently transported throughout the disk. In this work we numerically investigate the evolution in time of a two-component pebble disk, consisting of pristine pebbles and those that underwent ice sublimation. We find that stellar outbursts exceeding 1000 times the solar luminosity are efficient in thermally altering, through ice sublimation, a large mass fraction of pebbles (around 80%). After the establishment of this initial radial dust composition gradient throughout the disk, the subsequent mixing and inward drift of pristine outer-disk pebbles alter the inner disk bulk composition from processed to more unprocessed in time. Therefore, if processed pebbles without ice mantles have an isotopic composition similar to ureilite meteorites from the inner Solar System, inner-disk minor bodies forming from the early pebble flux (<1Myr) will be isotopically ureilite-like, while later-formed bodies will be increasingly admixed with the signature of the late incoming CI chondrite-like unprocessed pebbles. This appears to be largely consistent with the trend seen between the accretion age of different meteoric classes and their different stable isotope composition anomalies (in $\\mu$54Cr, $\\mu$48Ca, $\\mu$30Si, $\\mu$58Ni), but further work may be needed to explain the role of isotopically anomalous refractory inclusions and anomaly trends in other elements.","sentences":["During protoplanetary disk formation, dust grains located in the outer disk retain their pristine icy composition, while solids in the inner stellar-heated disk undergo volatile loss.","This process may have left a fossil record in Solar System material showing different nucelosynthetic imprints that have been attributed to different degrees of thermal processing.","However, it remains unclear how a large mass fraction of thermally-processed inner-disk pebbles is produced and how these grains are subsequently transported throughout the disk.","In this work we numerically investigate the evolution in time of a two-component pebble disk, consisting of pristine pebbles and those that underwent ice sublimation.","We find that stellar outbursts exceeding 1000 times the solar luminosity are efficient in thermally altering, through ice sublimation, a large mass fraction of pebbles (around 80%).","After the establishment of this initial radial dust composition gradient throughout the disk, the subsequent mixing and inward drift of pristine outer-disk pebbles alter the inner disk bulk composition from processed to more unprocessed in time.","Therefore, if processed pebbles without ice mantles have an isotopic composition similar to ureilite meteorites from the inner Solar System, inner-disk minor bodies forming from the early pebble flux (<1Myr) will be isotopically ureilite-like, while later-formed bodies will be increasingly admixed with the signature of the late incoming CI chondrite-like unprocessed pebbles.","This appears to be largely consistent with the trend seen between the accretion age of different meteoric classes and their different stable isotope composition anomalies (in $\\mu$54Cr, $\\mu$48Ca, $\\mu$30Si, $\\mu$58Ni), but further work may be needed to explain the role of isotopically anomalous refractory inclusions and anomaly trends in other elements."],"url":"http://arxiv.org/abs/2402.08141v1","category":"astro-ph.EP"}
{"created":"2024-02-13 00:16:09","title":"FORECASTOR -- I. Finding Optics Requirements and Exposure times for the Cosmological Advanced Survey Telescope for Optical and UV Research mission","abstract":"The Cosmological Advanced Survey Telescope for Optical and ultraviolet Research (CASTOR) is a proposed Canadian-led 1m-class space telescope that will carry out ultraviolet and blue-optical wide-field imaging, spectroscopy, and photometry. CASTOR will provide an essential bridge in the post-Hubble era, preventing a protracted UV-optical gap in space astronomy and enabling an enormous range of discovery opportunities from the solar system to the nature of the Cosmos, in conjunction with the other great wide-field observatories of the next decade (e.g., Euclid, Roman, Vera Rubin). FORECASTOR (Finding Optics Requirements and Exposure times for CASTOR) will supply a coordinated suite of mission-planning tools that will serve as the one-stop shop for proposal preparation, data reduction, and analysis for the CASTOR mission. We present the first of these tools: a pixel-based, user-friendly, extensible, multi-mission exposure time calculator (ETC) built in Python, including a modern browser-based graphical user interface that updates in real time. We then provide several illustrative examples of FORECASTOR's use that advance the design of planned legacy surveys for the CASTOR mission: a search for the most massive white dwarfs in the Magellanic Clouds; a study of the frequency of flaring activity in M stars, their distribution and impacts on habitability of exoplanets; mapping the proper motions of faint stars in the Milky Way; wide and deep galaxy surveys; and time-domain studies of active galactic nuclei.","sentences":["The Cosmological Advanced Survey Telescope for Optical and ultraviolet Research (CASTOR) is a proposed Canadian-led 1m-class space telescope that will carry out ultraviolet and blue-optical wide-field imaging, spectroscopy, and photometry.","CASTOR will provide an essential bridge in the post-Hubble era, preventing a protracted UV-optical gap in space astronomy and enabling an enormous range of discovery opportunities from the solar system to the nature of the Cosmos, in conjunction with the other great wide-field observatories of the next decade (e.g., Euclid, Roman, Vera Rubin).","FORECASTOR (Finding Optics Requirements and Exposure times for CASTOR) will supply a coordinated suite of mission-planning tools that will serve as the one-stop shop for proposal preparation, data reduction, and analysis for the CASTOR mission.","We present the first of these tools: a pixel-based, user-friendly, extensible, multi-mission exposure time calculator (ETC) built in Python, including a modern browser-based graphical user interface that updates in real time.","We then provide several illustrative examples of FORECASTOR's use that advance the design of planned legacy surveys for the CASTOR mission: a search for the most massive white dwarfs in the Magellanic Clouds; a study of the frequency of flaring activity in M stars, their distribution and impacts on habitability of exoplanets; mapping the proper motions of faint stars in the Milky Way; wide and deep galaxy surveys; and time-domain studies of active galactic nuclei."],"url":"http://arxiv.org/abs/2402.08137v1","category":"astro-ph.IM"}
{"created":"2024-02-13 00:08:21","title":"Early Exploration of a Flexible Framework for Efficient Quantum Linear Solvers in Power Systems","abstract":"The rapid integration of renewable energy resources presents formidable challenges in managing power grids. While advanced computing and machine learning techniques offer some solutions for accelerating grid modeling and simulation, there remain complex problems that classical computers cannot effectively address. Quantum computing, a promising technology, has the potential to fundamentally transform how we manage power systems, especially in scenarios with a higher proportion of renewable energy sources. One critical aspect is solving large-scale linear systems of equations, crucial for power system applications like power flow analysis, for which the Harrow-Hassidim-Lloyd (HHL) algorithm is a well-known quantum solution. However, HHL quantum circuits often exhibit excessive depth, making them impractical for current Noisy-Intermediate-Scale-Quantum (NISQ) devices. In this paper, we introduce a versatile framework, powered by NWQSim, that bridges the gap between power system applications and quantum linear solvers available in Qiskit. This framework empowers researchers to efficiently explore power system applications using quantum linear solvers. Through innovative gate fusion strategies, reduced circuit depth, and GPU acceleration, our simulator significantly enhances resource efficiency. Power flow case studies have demonstrated up to a eight-fold speedup compared to Qiskit Aer, all while maintaining comparable levels of accuracy.","sentences":["The rapid integration of renewable energy resources presents formidable challenges in managing power grids.","While advanced computing and machine learning techniques offer some solutions for accelerating grid modeling and simulation, there remain complex problems that classical computers cannot effectively address.","Quantum computing, a promising technology, has the potential to fundamentally transform how we manage power systems, especially in scenarios with a higher proportion of renewable energy sources.","One critical aspect is solving large-scale linear systems of equations, crucial for power system applications like power flow analysis, for which the Harrow-Hassidim-Lloyd (HHL) algorithm is a well-known quantum solution.","However, HHL quantum circuits often exhibit excessive depth, making them impractical for current Noisy-Intermediate-Scale-Quantum (NISQ) devices.","In this paper, we introduce a versatile framework, powered by NWQSim, that bridges the gap between power system applications and quantum linear solvers available in Qiskit.","This framework empowers researchers to efficiently explore power system applications using quantum linear solvers.","Through innovative gate fusion strategies, reduced circuit depth, and GPU acceleration, our simulator significantly enhances resource efficiency.","Power flow case studies have demonstrated up to a eight-fold speedup compared to Qiskit Aer, all while maintaining comparable levels of accuracy."],"url":"http://arxiv.org/abs/2402.08136v1","category":"quant-ph"}
{"created":"2024-02-13 00:03:32","title":"A scalable, synergy-first backbone decomposition of higher-order structures in complex systems","abstract":"Since its introduction in 2011, the partial information decomposition (PID) has triggered an explosion of interest in the field of multivariate information theory and the study of emergent, higher-order (\"synergistic\") interactions in complex systems. Despite its power, however, the PID has a number of limitations that restrict its general applicability: it scales poorly with system size and the standard approach to decomposition hinges on a definition of \"redundancy\", leaving synergy only vaguely defined as \"that information not redundant.\" Other heuristic measures, such as the O-information, have been introduced, although these measures typically only provided a summary statistic of redundancy/synergy dominance, rather than direct insight into the synergy itself. To address this issue, we present an alternative decomposition that is synergy-first, scales much more gracefully than the PID, and has a straightforward interpretation. Our approach defines synergy as that information in a set that would be lost following the minimally invasive perturbation on any single element. By generalizing this idea to sets of elements, we construct a totally ordered \"backbone\" of partial synergy atoms that sweeps systems scales. Our approach starts with entropy, but can be generalized to the Kullback-Leibler divergence, and by extension, to the total correlation and the single-target mutual information. Finally, we show that this approach can be used to decompose higher-order interactions beyond just information theory: we demonstrate this by showing how synergistic combinations of pairwise edges in a complex network supports signal communicability and global integration. We conclude by discussing how this perspective on synergistic structure (information-based or otherwise) can deepen our understanding of part-whole relationships in complex systems.","sentences":["Since its introduction in 2011, the partial information decomposition (PID) has triggered an explosion of interest in the field of multivariate information theory and the study of emergent, higher-order (\"synergistic\") interactions in complex systems.","Despite its power, however, the PID has a number of limitations that restrict its general applicability: it scales poorly with system size and the standard approach to decomposition hinges on a definition of \"redundancy\", leaving synergy only vaguely defined as \"that information not redundant.\"","Other heuristic measures, such as the O-information, have been introduced, although these measures typically only provided a summary statistic of redundancy/synergy dominance, rather than direct insight into the synergy itself.","To address this issue, we present an alternative decomposition that is synergy-first, scales much more gracefully than the PID, and has a straightforward interpretation.","Our approach defines synergy as that information in a set that would be lost following the minimally invasive perturbation on any single element.","By generalizing this idea to sets of elements, we construct a totally ordered \"backbone\" of partial synergy atoms that sweeps systems scales.","Our approach starts with entropy, but can be generalized to the Kullback-Leibler divergence, and by extension, to the total correlation and the single-target mutual information.","Finally, we show that this approach can be used to decompose higher-order interactions beyond just information theory: we demonstrate this by showing how synergistic combinations of pairwise edges in a complex network supports signal communicability and global integration.","We conclude by discussing how this perspective on synergistic structure (information-based or otherwise) can deepen our understanding of part-whole relationships in complex systems."],"url":"http://arxiv.org/abs/2402.08135v1","category":"cs.IT"}
{"created":"2024-02-12 23:55:55","title":"On the Resurgence of Recurrent Models for Long Sequences: Survey and Research Opportunities in the Transformer Era","abstract":"A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data. The outstanding results of Transformers-based networks (e.g., Large Language Models) promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of Recurrent Models. However, in the last few years, researchers who were concerned by the quadratic complexity of self-attention have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) Recurrent Neural Networks. This survey is aimed at providing an overview of these trends framed under the unifying umbrella of Recurrence. Moreover, it emphasizes novel research opportunities that become prominent when abandoning the idea of processing long sequences whose length is known-in-advance for the more realistic setting of potentially infinite-length sequences, thus intersecting the field of lifelong-online learning from streamed data.","sentences":["A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data.","The outstanding results of Transformers-based networks (e.g., Large Language Models) promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of Recurrent Models.","However, in the last few years, researchers who were concerned by the quadratic complexity of self-attention have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., Transformers and Recurrent Nets.","Meanwhile, Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) Recurrent Neural Networks.","This survey is aimed at providing an overview of these trends framed under the unifying umbrella of Recurrence.","Moreover, it emphasizes novel research opportunities that become prominent when abandoning the idea of processing long sequences whose length is known-in-advance for the more realistic setting of potentially infinite-length sequences, thus intersecting the field of lifelong-online learning from streamed data."],"url":"http://arxiv.org/abs/2402.08132v1","category":"cs.LG"}
{"created":"2024-02-12 23:31:18","title":"Orbit misbehavior, isotropy discontinuity, and large isotypic components","abstract":"Let $\\mathbb{G}$ be a compact Hausdorff group acting on a compact Hausdorff space $X$, $\\alpha$ an irreducible $\\mathbb{G}$-representation, and $C(X)$ the $C^*$-algebra of complex-valued continuous functions on $X$. We prove that the isotypic component $C(X)_{\\alpha}$ is finitely generated as a module over the invariant subalgebra $C(X/\\mathbb{G})\\subseteq C(X)$ precisely when the map sending $x\\in X$ to the dimension of the space of vectors in $\\alpha$ invariant under the isotropy group $\\mathbb{G}_x$ is locally constant. This (a) specializes back to an observation of De Commer-Yamashita equating the finite generation of all $C(X)_{\\alpha}$ with the Vietoris continuity of $x\\mapsto \\mathbb{G}_x$, and (b) recovers and extends Watatani's examples of infinite-index expectations resulting from non-free finite-group actions.   We also show that the action of a compact group $\\mathbb{G}$ on the maximal equivariant compactification on the disjoint union of its Lie-group quotients has tubes about all orbits precisely when $\\mathbb{G}$ is Lie. This is the converse (via a canonical construction) of the well-known fact that actions of compact Lie groups on Tychonoff spaces admit tubes.","sentences":["Let $\\mathbb{G}$ be a compact Hausdorff group acting on a compact Hausdorff space $X$, $\\alpha$ an irreducible $\\mathbb{G}$-representation, and $C(X)$ the $C^*$-algebra of complex-valued continuous functions on $X$. We prove that the isotypic component $C(X)_{\\alpha}$ is finitely generated as a module over the invariant subalgebra $C(X/\\mathbb{G})\\subseteq C(X)$ precisely when the map sending $x\\in X$ to the dimension of the space of vectors in $\\alpha$ invariant under the isotropy group $\\mathbb{G}_x$ is locally constant.","This (a) specializes back to an observation of De Commer-Yamashita equating the finite generation of all $C(X)_{\\alpha}$ with the Vietoris continuity of $x\\mapsto \\mathbb{G}_x$, and (b) recovers and extends Watatani's examples of infinite-index expectations resulting from non-free finite-group actions.   ","We also show that the action of a compact group $\\mathbb{G}$ on the maximal equivariant compactification on the disjoint union of its Lie-group quotients has tubes about all orbits precisely when $\\mathbb{G}$ is Lie.","This is the converse (via a canonical construction) of the well-known fact that actions of compact Lie groups on Tychonoff spaces admit tubes."],"url":"http://arxiv.org/abs/2402.08121v1","category":"math.OA"}
{"created":"2024-02-12 23:20:48","title":"Topological excitations in a spin-orbit coupled spin-1 Bose-Einstein condensate under sinusoidally varying magnetic fields","abstract":"We present a theoretical study of a spin-orbit coupled spin-1 Bose-Einstein condensate under the influence of sinusoidally varying magnetic fields. In the ground state of the ferromagnetic spin-1 condensate, we investigate topological excitations in the system arising due to the combined effect of Rashba spin-orbit coupling and an in-plane sinusoidally varying magnetic field. In this work, we offer a comparative study for various choices of magnetic fields in the $x\\rm{-}y$ plane. For a fixed field strength, the spin-orbit coupled system sustains a rich variety of exotic vortex structures ranging from vortex-antivortex lattices to vortex clusters as we increase the coupling strength.","sentences":["We present a theoretical study of a spin-orbit coupled spin-1","Bose-Einstein condensate under the influence of sinusoidally varying magnetic fields.","In the ground state of the ferromagnetic spin-1 condensate, we investigate topological excitations in the system arising due to the combined effect of Rashba spin-orbit coupling and an in-plane sinusoidally varying magnetic field.","In this work, we offer a comparative study for various choices of magnetic fields in the $x\\rm{-}y$ plane.","For a fixed field strength, the spin-orbit coupled system sustains a rich variety of exotic vortex structures ranging from vortex-antivortex lattices to vortex clusters as we increase the coupling strength."],"url":"http://arxiv.org/abs/2402.08118v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-12 23:08:37","title":"Addressing cognitive bias in medical language models","abstract":"The integration of large language models (LLMs) into the medical field has gained significant attention due to their promising accuracy in simulated clinical decision-making settings. However, clinical decision-making is more complex than simulations because physicians' decisions are shaped by many factors, including the presence of cognitive bias. However, the degree to which LLMs are susceptible to the same cognitive biases that affect human clinicians remains unexplored. Our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases.In this study, we developed BiasMedQA, a novel benchmark for evaluating cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and the medically specialized PMC Llama 13B. We tested these models on 1,273 questions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3, modified to replicate common clinically-relevant cognitive biases. Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias. Our findings highlight the critical need for bias mitigation in the development of medical LLMs, pointing towards safer and more reliable applications in healthcare.","sentences":["The integration of large language models (LLMs) into the medical field has gained significant attention due to their promising accuracy in simulated clinical decision-making settings.","However, clinical decision-making is more complex than simulations because physicians' decisions are shaped by many factors, including the presence of cognitive bias.","However, the degree to which LLMs are susceptible to the same cognitive biases that affect human clinicians remains unexplored.","Our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases.","In this study, we developed BiasMedQA, a novel benchmark for evaluating cognitive biases in LLMs applied to medical tasks.","Using BiasMedQA","we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and the medically specialized PMC Llama 13B. We tested these models on 1,273 questions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3, modified to replicate common clinically-relevant cognitive biases.","Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias.","Our findings highlight the critical need for bias mitigation in the development of medical LLMs, pointing towards safer and more reliable applications in healthcare."],"url":"http://arxiv.org/abs/2402.08113v1","category":"cs.CL"}
{"created":"2024-02-12 22:56:18","title":"From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations","abstract":"This research aims to explore the impact of Machine Learning (ML) on the evolution and efficacy of Recommendation Systems (RS), particularly in the context of their growing significance in commercial business environments. Methodologically, the study delves into the role of ML in crafting and refining these systems, focusing on aspects such as data sourcing, feature engineering, and the importance of evaluation metrics, thereby highlighting the iterative nature of enhancing recommendation algorithms. The deployment of Recommendation Engines (RE), driven by advanced algorithms and data analytics, is explored across various domains, showcasing their significant impact on user experience and decision-making processes. These engines not only streamline information discovery and enhance collaboration but also accelerate knowledge acquisition, proving vital in navigating the digital landscape for businesses. They contribute significantly to sales, revenue, and the competitive edge of enterprises by offering improved recommendations that align with individual customer needs. The research identifies the increasing expectation of users for a seamless, intuitive online experience, where content is personalized and dynamically adapted to changing preferences. Future research directions include exploring advancements in deep learning models, ethical considerations in the deployment of RS, and addressing scalability challenges. This study emphasizes the indispensability of comprehending and leveraging ML in RS for researchers and practitioners, to tap into the full potential of personalized recommendation in commercial business prospects.","sentences":["This research aims to explore the impact of Machine Learning (ML) on the evolution and efficacy of Recommendation Systems (RS), particularly in the context of their growing significance in commercial business environments.","Methodologically, the study delves into the role of ML in crafting and refining these systems, focusing on aspects such as data sourcing, feature engineering, and the importance of evaluation metrics, thereby highlighting the iterative nature of enhancing recommendation algorithms.","The deployment of Recommendation Engines (RE), driven by advanced algorithms and data analytics, is explored across various domains, showcasing their significant impact on user experience and decision-making processes.","These engines not only streamline information discovery and enhance collaboration but also accelerate knowledge acquisition, proving vital in navigating the digital landscape for businesses.","They contribute significantly to sales, revenue, and the competitive edge of enterprises by offering improved recommendations that align with individual customer needs.","The research identifies the increasing expectation of users for a seamless, intuitive online experience, where content is personalized and dynamically adapted to changing preferences.","Future research directions include exploring advancements in deep learning models, ethical considerations in the deployment of RS, and addressing scalability challenges.","This study emphasizes the indispensability of comprehending and leveraging ML in RS for researchers and practitioners, to tap into the full potential of personalized recommendation in commercial business prospects."],"url":"http://arxiv.org/abs/2402.08109v1","category":"cs.DC"}
{"created":"2024-02-12 22:54:52","title":"Blueprint for efficient nuclear spin characterization with color center","abstract":"Nuclear spins in solids offer a promising avenue for developing scalable quantum hardware. Leveraging nearby single-color centers, these spins can be efficiently addressed at the single-site level through spin resonance. However, characterising individual nuclear spins is quite cumbersome since the characterisation protocols may differ depending on the strength of the hyperfine coupling, necessitating tailored approaches and experimental conditions. While modified electron spin Hahn echoes like CPMG and XY8 pulse sequences are commonly employed, they encounter significant limitations in scenarios involving spin-1/2 systems, strongly coupled spins, or nuclear spin baths comprising distinct isotopes. Here, we present a more straightforward approach for determining the hyperfine interactions among each nuclear and the electron spin. This method holds promise across diverse platforms, especially for emerging S=1/2 group IV defects in diamond (e.g., SiV, GeV, SnV, PbV) or silicon (T-centre, P-donors). We provide a theoretical framework and adapt it for color-centers exhibiting various spins. Through simulations conducted on nuclear spin clusters, we evaluate different protocols and compare their performance using the Fisher information matrix and Cramer Rao bounds.","sentences":["Nuclear spins in solids offer a promising avenue for developing scalable quantum hardware.","Leveraging nearby single-color centers, these spins can be efficiently addressed at the single-site level through spin resonance.","However, characterising individual nuclear spins is quite cumbersome since the characterisation protocols may differ depending on the strength of the hyperfine coupling, necessitating tailored approaches and experimental conditions.","While modified electron spin Hahn echoes like CPMG and XY8 pulse sequences are commonly employed, they encounter significant limitations in scenarios involving spin-1/2 systems, strongly coupled spins, or nuclear spin baths comprising distinct isotopes.","Here, we present a more straightforward approach for determining the hyperfine interactions among each nuclear and the electron spin.","This method holds promise across diverse platforms, especially for emerging S=1/2 group IV defects in diamond (e.g., SiV, GeV, SnV, PbV) or silicon (T-centre, P-donors).","We provide a theoretical framework and adapt it for color-centers exhibiting various spins.","Through simulations conducted on nuclear spin clusters, we evaluate different protocols and compare their performance using the Fisher information matrix and Cramer Rao bounds."],"url":"http://arxiv.org/abs/2402.08107v1","category":"quant-ph"}
{"created":"2024-02-12 22:48:30","title":"Learning Cartesian Product Graphs with Laplacian Constraints","abstract":"Graph Laplacian learning, also known as network topology inference, is a problem of great interest to multiple communities. In Gaussian graphical models (GM), graph learning amounts to endowing covariance selection with the Laplacian structure. In graph signal processing (GSP), it is essential to infer the unobserved graph from the outputs of a filtering system. In this paper, we study the problem of learning Cartesian product graphs under Laplacian constraints. The Cartesian graph product is a natural way for modeling higher-order conditional dependencies and is also the key for generalizing GSP to multi-way tensors. We establish statistical consistency for the penalized maximum likelihood estimation (MLE) of a Cartesian product Laplacian, and propose an efficient algorithm to solve the problem. We also extend our method for efficient joint graph learning and imputation in the presence of structural missing values. Experiments on synthetic and real-world datasets demonstrate that our method is superior to previous GSP and GM methods.","sentences":["Graph Laplacian learning, also known as network topology inference, is a problem of great interest to multiple communities.","In Gaussian graphical models (GM), graph learning amounts to endowing covariance selection with the Laplacian structure.","In graph signal processing (GSP), it is essential to infer the unobserved graph from the outputs of a filtering system.","In this paper, we study the problem of learning Cartesian product graphs under Laplacian constraints.","The Cartesian graph product is a natural way for modeling higher-order conditional dependencies and is also the key for generalizing GSP to multi-way tensors.","We establish statistical consistency for the penalized maximum likelihood estimation (MLE) of a Cartesian product Laplacian, and propose an efficient algorithm to solve the problem.","We also extend our method for efficient joint graph learning and imputation in the presence of structural missing values.","Experiments on synthetic and real-world datasets demonstrate that our method is superior to previous GSP and GM methods."],"url":"http://arxiv.org/abs/2402.08105v1","category":"cs.LG"}
{"created":"2024-02-12 22:45:05","title":"High Pressure Suppression of Plasticity due to Over-Nucleation of Shear Strain","abstract":"High pressure shear band formation is a critical phenomenon in energetic materials due to its influence on both mechanical strength and mechanochemical activation. While shear banding is know to occur in a variety of these materials, the governing dynamics of the mechanisms is not well defined for molecular crystals. We conduct molecular dynamics simulations of shock wave induced shear band formation in the energetic material 1,3,5-trinitroperhydro-1,3,5-triazine (RDX) to assess shear band nucleation processes. We find, that at high pressures, the initial formation sites for shear bands 'over-nucleate' and rapidly lower deviatoric stresses prior to shear band formation and growth. This results in the suppression of plastic deformation. A local cluster analysis is used to quantify and contrast this mechanism with a more typical shear banding seen at lower pressures. These results demonstrate a mechanism that is reversible in nature and that supersedes shear band formation at increased pressures. We anticipate that these results will have a broad impact on the modeling and development of high strain rate application materials such as those for high explosives and hypersonic systems.","sentences":["High pressure shear band formation is a critical phenomenon in energetic materials due to its influence on both mechanical strength and mechanochemical activation.","While shear banding is know to occur in a variety of these materials, the governing dynamics of the mechanisms is not well defined for molecular crystals.","We conduct molecular dynamics simulations of shock wave induced shear band formation in the energetic material 1,3,5-trinitroperhydro-1,3,5-triazine (RDX) to assess shear band nucleation processes.","We find, that at high pressures, the initial formation sites for shear bands 'over-nucleate' and rapidly lower deviatoric stresses prior to shear band formation and growth.","This results in the suppression of plastic deformation.","A local cluster analysis is used to quantify and contrast this mechanism with a more typical shear banding seen at lower pressures.","These results demonstrate a mechanism that is reversible in nature and that supersedes shear band formation at increased pressures.","We anticipate that these results will have a broad impact on the modeling and development of high strain rate application materials such as those for high explosives and hypersonic systems."],"url":"http://arxiv.org/abs/2402.08104v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-12 22:39:56","title":"Routing entanglement through quantum networks","abstract":"Entanglement, one of the clearest manifestations of non-classical physics, holds significant promise for technological applications such as more secure communications and faster computations. In this paper we explore the use of non-reciprocal transport in a network of continuous-variable systems to route entanglement in one direction through the network. We develop the theory and discuss a potential realization of controllable flow of entanglement in quantum systems; our method employs only Gaussian interactions and engineered dissipation to break the symmetry. We also explore the conditions under which thermal fluctuations limit the distance over which the entanglement propagates and observe a counter-intuitive behavior between this distance, the strength of the entanglement source, and the strength of the hopping through the network.","sentences":["Entanglement, one of the clearest manifestations of non-classical physics, holds significant promise for technological applications such as more secure communications and faster computations.","In this paper we explore the use of non-reciprocal transport in a network of continuous-variable systems to route entanglement in one direction through the network.","We develop the theory and discuss a potential realization of controllable flow of entanglement in quantum systems; our method employs only Gaussian interactions and engineered dissipation to break the symmetry.","We also explore the conditions under which thermal fluctuations limit the distance over which the entanglement propagates and observe a counter-intuitive behavior between this distance, the strength of the entanglement source, and the strength of the hopping through the network."],"url":"http://arxiv.org/abs/2402.08102v1","category":"quant-ph"}
{"created":"2024-02-12 22:37:15","title":"Auditing Work: Exploring the New York City algorithmic bias audit regime","abstract":"In July 2023, New York City (NYC) initiated the first algorithm auditing system for commercial machine-learning systems. Local Law 144 (LL 144) mandates NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to undergo annual bias audits conducted by an independent auditor. This paper examines lessons from LL 144 for other national algorithm auditing attempts. Through qualitative interviews with 16 experts and practitioners within the regime, we find that LL 144 has not effectively established an auditing regime. The law fails to clearly define key aspects, such as AEDTs and independent auditors, leading auditors, AEDT vendors, and companies using AEDTs to define the law's practical implementation in ways that failed to protect job applicants. Contributing factors include the law's flawed transparency-driven theory of change, industry lobbying narrowing the definition of AEDTs, practical and cultural challenges faced by auditors in accessing data, and wide disagreement over what constitutes a legitimate auditor, resulting in four distinct 'auditor roles.' We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes, emphasizing clearer definitions, metrics, and increased accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, providing guidance for policymakers seeking to create similar regimes.","sentences":["In July 2023, New York City (NYC) initiated the first algorithm auditing system for commercial machine-learning systems.","Local Law 144 (LL 144) mandates NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to undergo annual bias audits conducted by an independent auditor.","This paper examines lessons from LL 144 for other national algorithm auditing attempts.","Through qualitative interviews with 16 experts and practitioners within the regime, we find that LL 144 has not effectively established an auditing regime.","The law fails to clearly define key aspects, such as AEDTs and independent auditors, leading auditors, AEDT vendors, and companies using AEDTs to define the law's practical implementation in ways that failed to protect job applicants.","Contributing factors include the law's flawed transparency-driven theory of change, industry lobbying narrowing the definition of AEDTs, practical and cultural challenges faced by auditors in accessing data, and wide disagreement over what constitutes a legitimate auditor, resulting in four distinct 'auditor roles.'","We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes, emphasizing clearer definitions, metrics, and increased accountability.","By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, providing guidance for policymakers seeking to create similar regimes."],"url":"http://arxiv.org/abs/2402.08101v1","category":"cs.CY"}
{"created":"2024-02-12 22:34:53","title":"An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem","abstract":"In this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem. We present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set. We measure the performance of our method in terms of suboptimality and infeasibility errors and provide non-asymptotic convergence guarantees for both error criteria. Specifically, when the feasible set is compact, we show that our method requires at most $\\mathcal{O}(\\max\\{1/\\sqrt{\\epsilon_{f}}, 1/\\epsilon_g\\})$ iterations to find a solution that is $\\epsilon_f$-suboptimal and $\\epsilon_g$-infeasible. Moreover, under the additional assumption that the lower-level objective satisfies the $r$-th H\\\"olderian error bound, we show that our method achieves an iteration complexity of $\\mathcal{O}(\\max\\{\\epsilon_{f}^{-\\frac{2r-1}{2r}},\\epsilon_{g}^{-\\frac{2r-1}{2r}}\\})$, which matches the optimal complexity of single-level convex constrained optimization when $r=1$.","sentences":["In this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem.","We present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set.","We measure the performance of our method in terms of suboptimality and infeasibility errors and provide non-asymptotic convergence guarantees for both error criteria.","Specifically, when the feasible set is compact, we show that our method requires at most $\\mathcal{O}(\\max\\{1/\\sqrt{\\epsilon_{f}}, 1/\\epsilon_g\\})$ iterations to find a solution that is $\\epsilon_f$-suboptimal and $\\epsilon_g$-infeasible.","Moreover, under the additional assumption that the lower-level objective satisfies the $r$-th H\\\"olderian error bound, we show that our method achieves an iteration complexity of $\\mathcal{O}(\\max\\{\\epsilon_{f}^{-\\frac{2r-1}{2r}},\\epsilon_{g}^{-\\frac{2r-1}{2r}}\\})$, which matches the optimal complexity of single-level convex constrained optimization when $r=1$."],"url":"http://arxiv.org/abs/2402.08097v1","category":"math.OC"}
{"created":"2024-02-12 22:22:16","title":"Using AI for Wavefront Estimation with the Rubin Observatory Active Optics System","abstract":"The Vera C. Rubin Observatory will, over a period of 10 years, repeatedly survey the southern sky. To ensure that images generated by Rubin meet the quality requirements for precision science, the observatory will use an Active Optics System (AOS) to correct for alignment and mirror surface perturbations introduced by gravity and temperature gradients in the optical system. To accomplish this Rubin will use out-of-focus images from sensors located at the edge of the focal plane to learn and correct for perturbations to the wavefront. We have designed and integrated a deep learning model for wavefront estimation into the AOS pipeline. In this paper, we compare the performance of this deep learning approach to Rubin's baseline algorithm when applied to images from two different simulations of the Rubin optical system. We show the deep learning approach is faster and more accurate, achieving the atmospheric error floor both for high-quality images, and low-quality images with heavy blending and vignetting. Compared to the baseline algorithm, the deep learning model is 40x faster, the median error 2x better under ideal conditions, 5x better in the presence of vignetting by the Rubin camera, and 14x better in the presence of blending in crowded fields. In addition, the deep learning model surpasses the required optical quality in simulations of the AOS closed loop. This system promises to increase the survey area useful for precision science by up to 8%. We discuss how this system might be deployed when commissioning and operating Rubin.","sentences":["The Vera C. Rubin Observatory will, over a period of 10 years, repeatedly survey the southern sky.","To ensure that images generated by Rubin meet the quality requirements for precision science, the observatory will use an Active Optics System (AOS) to correct for alignment and mirror surface perturbations introduced by gravity and temperature gradients in the optical system.","To accomplish this Rubin will use out-of-focus images from sensors located at the edge of the focal plane to learn and correct for perturbations to the wavefront.","We have designed and integrated a deep learning model for wavefront estimation into the AOS pipeline.","In this paper, we compare the performance of this deep learning approach to Rubin's baseline algorithm when applied to images from two different simulations of the Rubin optical system.","We show the deep learning approach is faster and more accurate, achieving the atmospheric error floor both for high-quality images, and low-quality images with heavy blending and vignetting.","Compared to the baseline algorithm, the deep learning model is 40x faster, the median error 2x better under ideal conditions, 5x better in the presence of vignetting by the Rubin camera, and 14x better in the presence of blending in crowded fields.","In addition, the deep learning model surpasses the required optical quality in simulations of the AOS closed loop.","This system promises to increase the survey area useful for precision science by up to 8%.","We discuss how this system might be deployed when commissioning and operating Rubin."],"url":"http://arxiv.org/abs/2402.08094v1","category":"astro-ph.IM"}
{"created":"2024-02-12 22:21:30","title":"BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data","abstract":"We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.","sentences":["We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities.","BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness.","It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner.","Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding.","Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences.","We design and share a specialized dataset to measure these emergent abilities for text-to-speech.","We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS.","Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/."],"url":"http://arxiv.org/abs/2402.08093v1","category":"cs.LG"}
{"created":"2024-02-12 22:17:28","title":"Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees","abstract":"Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D LASA datasets.","sentences":["Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty.","We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics.","The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field.","In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric.","To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space.","We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D LASA datasets."],"url":"http://arxiv.org/abs/2402.08090v1","category":"cs.LG"}
{"created":"2024-02-12 22:07:43","title":"Text-centric Alignment for Multi-Modality Learning","abstract":"This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availability is dynamic and uncertain.","sentences":["This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training.","We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions.","By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations.","TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations.","This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availability is dynamic and uncertain."],"url":"http://arxiv.org/abs/2402.08086v1","category":"cs.LG"}
{"created":"2024-02-12 22:02:23","title":"Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions","abstract":"While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is interesting in its own right.","sentences":["While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited.","In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions.","We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure.","We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate.","We illustrate our theory through examples, which include certain mixtures of Gaussians.","An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is interesting in its own right."],"url":"http://arxiv.org/abs/2402.08082v1","category":"stat.ML"}
{"created":"2024-02-12 21:44:32","title":"Large Language Models as Agents in Two-Player Games","abstract":"By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.","sentences":["By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies.","This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems.","We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games.","This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations.","Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs."],"url":"http://arxiv.org/abs/2402.08078v1","category":"cs.CL"}
{"created":"2024-02-12 21:44:20","title":"Diffeomorphic Measure Matching with Kernels for Generative Modeling","abstract":"This article presents a general framework for the transport of probability measures towards minimum divergence generative modeling and sampling using ordinary differential equations (ODEs) and Reproducing Kernel Hilbert Spaces (RKHSs), inspired by ideas from diffeomorphic matching and image registration. A theoretical analysis of the proposed method is presented, giving a priori error bounds in terms of the complexity of the model, the number of samples in the training set, and model misspecification. An extensive suite of numerical experiments further highlights the properties, strengths, and weaknesses of the method and extends its applicability to other tasks, such as conditional simulation and inference.","sentences":["This article presents a general framework for the transport of probability measures towards minimum divergence generative modeling and sampling using ordinary differential equations (ODEs) and Reproducing Kernel Hilbert Spaces (RKHSs), inspired by ideas from diffeomorphic matching and image registration.","A theoretical analysis of the proposed method is presented, giving a priori error bounds in terms of the complexity of the model, the number of samples in the training set, and model misspecification.","An extensive suite of numerical experiments further highlights the properties, strengths, and weaknesses of the method and extends its applicability to other tasks, such as conditional simulation and inference."],"url":"http://arxiv.org/abs/2402.08077v1","category":"stat.ML"}
{"created":"2024-02-12 21:42:40","title":"A note on double Floquet-Bloch transforms and the far-field asymptotics of Green's functions tailored to periodic structures","abstract":"We propose a general procedure to study double integrals arising when considering wave propagation in periodic structures. This method, based on a complex deformation of the integration surface to bypass the integrands' singularities, is particularly efficient to estimate the Green's functions of such structures in the far field. We provide several illustrative examples and explicit asymptotic formulae. Special attention is devoted to the pathological cases of degeneracies, such as Dirac conical points for instance.","sentences":["We propose a general procedure to study double integrals arising when considering wave propagation in periodic structures.","This method, based on a complex deformation of the integration surface to bypass the integrands' singularities, is particularly efficient to estimate the Green's functions of such structures in the far field.","We provide several illustrative examples and explicit asymptotic formulae.","Special attention is devoted to the pathological cases of degeneracies, such as Dirac conical points for instance."],"url":"http://arxiv.org/abs/2402.08076v1","category":"math.AP"}
{"created":"2024-02-12 21:32:49","title":"Grounding Data Science Code Generation with Input-Output Specifications","abstract":"Large language models (LLMs) have recently demonstrated a remarkable ability to generate code from natural language (NL) prompts. However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification. In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program I/O specifications, is provided to the LLM to facilitate instruction fine-tuning. We evaluated our approach on two challenging data science benchmarks, Arcade and DS-1000. The results demonstrate a significant improvement in the LLM's ability to generate code that is not only executable but also accurately aligned with user specifications, substantially improving the quality of code generation for complex data science tasks.","sentences":["Large language models (LLMs) have recently demonstrated a remarkable ability to generate code from natural language (NL) prompts.","However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications.","Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification.","In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity.","Specifically, we propose GIFT4Code, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications.","Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal.","This feedback, in the form of program I/O specifications, is provided to the LLM to facilitate instruction fine-tuning.","We evaluated our approach on two challenging data science benchmarks, Arcade and DS-1000.","The results demonstrate a significant improvement in the LLM's ability to generate code that is not only executable but also accurately aligned with user specifications, substantially improving the quality of code generation for complex data science tasks."],"url":"http://arxiv.org/abs/2402.08073v1","category":"cs.LG"}
{"created":"2024-02-13 18:55:27","title":"Human Curriculum Effects Emerge with In-Context Learning in Neural Networks","abstract":"Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with \"in-context learning\" (ICL) both in neural networks trained with metalearning and in large language models (LLMs). ICL is the ability to learn new tasks \"in context\" - without weight changes - via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structure.","sentences":["Human learning is sensitive to rule-like structure and the curriculum of examples used for training.","In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective.","To date, no neural model has simultaneously captured these seemingly contradictory effects.","Here we show that this same tradeoff spontaneously emerges with \"in-context learning\" (ICL) both in neural networks trained with metalearning and in large language models (LLMs).","ICL is the ability to learn new tasks \"in context\" - without weight changes - via an inner-loop algorithm implemented in activation dynamics.","Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structure."],"url":"http://arxiv.org/abs/2402.08674v1","category":"cs.NE"}
{"created":"2024-02-13 18:29:52","title":"Propagation-invariant optical meron lattices","abstract":"We introduce and produce experimentally optical beams exhibiting periodic skyrmionic polarization lattices at each transverse plane of propagation. These textures are meron lattices formed by tiles mapping hemispheres of the Poincar\\'e sphere. All presented fields are combinations of a small number of plane waves. Firstly, we propose square lattices with a Skyrme density (the Jacobian of the mapping between the Poincar\\'e sphere and physical space) that oscillates in sign but whose intensity distribution is constant. Secondly, we present triangular lattices preserving the Skyrme density's sign. Both lattices are invariant under propagation. Finally, we introduce a family of lattices with uniform Skyrme density sign, composed of square tiles that map to the same hemisphere of the Poincar\\'e sphere. In these lattices, the polarization state undergoes a uniform local periodic rotation during propagation, thus preserving the texture's Skyrme density distribution.","sentences":["We introduce and produce experimentally optical beams exhibiting periodic skyrmionic polarization lattices at each transverse plane of propagation.","These textures are meron lattices formed by tiles mapping hemispheres of the Poincar\\'e sphere.","All presented fields are combinations of a small number of plane waves.","Firstly, we propose square lattices with a Skyrme density (the Jacobian of the mapping between the Poincar\\'e sphere and physical space) that oscillates in sign but whose intensity distribution is constant.","Secondly, we present triangular lattices preserving the Skyrme density's sign.","Both lattices are invariant under propagation.","Finally, we introduce a family of lattices with uniform Skyrme density sign, composed of square tiles that map to the same hemisphere of the Poincar\\'e sphere.","In these lattices, the polarization state undergoes a uniform local periodic rotation during propagation, thus preserving the texture's Skyrme density distribution."],"url":"http://arxiv.org/abs/2402.08650v1","category":"physics.optics"}
{"created":"2024-02-13 18:02:58","title":"BdSLW60: A Word-Level Bangla Sign Language Dataset","abstract":"Sign language discourse is an essential mode of daily communication for the deaf and hard-of-hearing people. However, research on Bangla Sign Language (BdSL) faces notable limitations, primarily due to the lack of datasets. Recognizing wordlevel signs in BdSL (WL-BdSL) presents a multitude of challenges, including the need for well-annotated datasets, capturing the dynamic nature of sign gestures from facial or hand landmarks, developing suitable machine learning or deep learning-based models with substantial video samples, and so on. In this paper, we address these challenges by creating a comprehensive BdSL word-level dataset named BdSLW60 in an unconstrained and natural setting, allowing positional and temporal variations and allowing sign users to change hand dominance freely. The dataset encompasses 60 Bangla sign words, with a significant scale of 9307 video trials provided by 18 signers under the supervision of a sign language professional. The dataset was rigorously annotated and cross-checked by 60 annotators. We also introduced a unique approach of a relative quantization-based key frame encoding technique for landmark based sign gesture recognition. We report the benchmarking of our BdSLW60 dataset using the Support Vector Machine (SVM) with testing accuracy up to 67.6% and an attention-based bi-LSTM with testing accuracy up to 75.1%. The dataset is available at https://www.kaggle.com/datasets/hasaniut/bdslw60 and the code base is accessible from https://github.com/hasanssl/BdSLW60_Code.","sentences":["Sign language discourse is an essential mode of daily communication for the deaf and hard-of-hearing people.","However, research on Bangla Sign Language (BdSL) faces notable limitations, primarily due to the lack of datasets.","Recognizing wordlevel signs in BdSL (WL-BdSL) presents a multitude of challenges, including the need for well-annotated datasets, capturing the dynamic nature of sign gestures from facial or hand landmarks, developing suitable machine learning or deep learning-based models with substantial video samples, and so on.","In this paper, we address these challenges by creating a comprehensive BdSL word-level dataset named BdSLW60 in an unconstrained and natural setting, allowing positional and temporal variations and allowing sign users to change hand dominance freely.","The dataset encompasses 60 Bangla sign words, with a significant scale of 9307 video trials provided by 18 signers under the supervision of a sign language professional.","The dataset was rigorously annotated and cross-checked by 60 annotators.","We also introduced a unique approach of a relative quantization-based key frame encoding technique for landmark based sign gesture recognition.","We report the benchmarking of our BdSLW60 dataset using the Support Vector Machine (SVM) with testing accuracy up to 67.6% and an attention-based bi-LSTM with testing accuracy up to 75.1%.","The dataset is available at https://www.kaggle.com/datasets/hasaniut/bdslw60 and the code base is accessible from https://github.com/hasanssl/BdSLW60_Code."],"url":"http://arxiv.org/abs/2402.08635v1","category":"cs.CV"}
{"created":"2024-02-13 16:49:03","title":"Tracking solar radio bursts using Bayesian multilateration","abstract":"Solar radio bursts (SRBs), are emitted by electrons propagating through the corona and interplanetary space. Tracking such bursts is key to understanding the properties of accelerated electrons and radio wave propagation as well as the local plasma environment that they propagate through. Here, we present a novel multilateration algorithm called BayEsian LocaLisation Algorithm (BELLA). In addition, apparent SRB positions from BELLA are compared with comparable localisation methods and the predictions of solar wind models. BELLA uses Bayesian inference to create probabilistic distributions of source positions and their uncertainties. This facilitates the estimation of algorithmic, instrumental, and physical uncertainties in a quantitative manner. We validated BELLA using simulations and a Type III SRB observed by STEREO A/B and Wind. BELLA tracked the Type III source from $\\sim$ 10--150 $R_{sun}$ (2-0.15 MHz) along a spiral trajectory. This allowed for an estimate of an apparent solar wind speed of $v_{sw} \\sim$ 400 km s$^{-1}$ and a source longitude of $\\phi_0 \\sim$ 30deg. We compared these results with well-established methods of positioning: Goniopolarimetric (GP), analytical time-difference-of-arrival (TDOA), and Solar radio burst Electron Motion Tracker (SEMP). We found them to be in agreement with the results obtained by BELLA. Additionally, the results aligned with solar wind properties assimilated by the Heliospheric Upwind Extrapolation with time dependence (HUXt) model. We have validated BELLA and used it to identify apparent source positions as well as velocities and densities of the solar wind. Furthermore, we identified higher than expected electron densities, suggesting that the true emission sources were at lower altitudes than those identified by BELLA, an effect that may be due to appreciable scattering of electromagnetic waves by electrons in interplanetary space.","sentences":["Solar radio bursts (SRBs), are emitted by electrons propagating through the corona and interplanetary space.","Tracking such bursts is key to understanding the properties of accelerated electrons and radio wave propagation as well as the local plasma environment that they propagate through.","Here, we present a novel multilateration algorithm called BayEsian LocaLisation Algorithm (BELLA).","In addition, apparent SRB positions from BELLA are compared with comparable localisation methods and the predictions of solar wind models.","BELLA uses Bayesian inference to create probabilistic distributions of source positions and their uncertainties.","This facilitates the estimation of algorithmic, instrumental, and physical uncertainties in a quantitative manner.","We validated BELLA using simulations and a Type III SRB observed by STEREO A/B and Wind.","BELLA tracked the Type III source from $\\sim$ 10--150 $R_{sun}$ (2-0.15 MHz) along a spiral trajectory.","This allowed for an estimate of an apparent solar wind speed of $v_{sw} \\sim$ 400 km s$^{-1}$ and a source longitude of $\\phi_0 \\sim$ 30deg.","We compared these results with well-established methods of positioning: Goniopolarimetric (GP), analytical time-difference-of-arrival (TDOA), and Solar radio burst Electron Motion Tracker (SEMP).","We found them to be in agreement with the results obtained by BELLA.","Additionally, the results aligned with solar wind properties assimilated by the Heliospheric Upwind Extrapolation with time dependence (HUXt) model.","We have validated BELLA and used it to identify apparent source positions as well as velocities and densities of the solar wind.","Furthermore, we identified higher than expected electron densities, suggesting that the true emission sources were at lower altitudes than those identified by BELLA, an effect that may be due to appreciable scattering of electromagnetic waves by electrons in interplanetary space."],"url":"http://arxiv.org/abs/2402.08590v1","category":"astro-ph.SR"}
{"created":"2024-02-13 16:21:47","title":"Heterogeneity, Uncertainty and Learning: Semiparametric Identification and Estimation","abstract":"We provide semiparametric identification results for a broad class of learning models in which continuous outcomes depend on three types of unobservables: i) known heterogeneity, ii) initially unknown heterogeneity that may be revealed over time, and iii) transitory uncertainty. We consider a common environment where the researcher only has access to a short panel on choices and realized outcomes. We establish identification of the outcome equation parameters and the distribution of the three types of unobservables, under the standard assumption that unknown heterogeneity and uncertainty are normally distributed. We also show that, absent known heterogeneity, the model is identified without making any distributional assumption. We then derive the asymptotic properties of a sieve MLE estimator for the model parameters, and devise a tractable profile likelihood based estimation procedure. Monte Carlo simulation results indicate that our estimator exhibits good finite-sample properties.","sentences":["We provide semiparametric identification results for a broad class of learning models in which continuous outcomes depend on three types of unobservables: i) known heterogeneity, ii) initially unknown heterogeneity that may be revealed over time, and iii) transitory uncertainty.","We consider a common environment where the researcher only has access to a short panel on choices and realized outcomes.","We establish identification of the outcome equation parameters and the distribution of the three types of unobservables, under the standard assumption that unknown heterogeneity and uncertainty are normally distributed.","We also show that, absent known heterogeneity, the model is identified without making any distributional assumption.","We then derive the asymptotic properties of a sieve MLE estimator for the model parameters, and devise a tractable profile likelihood based estimation procedure.","Monte Carlo simulation results indicate that our estimator exhibits good finite-sample properties."],"url":"http://arxiv.org/abs/2402.08575v1","category":"econ.EM"}
{"created":"2024-02-13 16:14:32","title":"Glass Segmentation with Multi Scales and Primary Prediction Guiding","abstract":"Glass-like objects can be seen everywhere in our daily life which are very hard for existing methods to segment them. The properties of transparencies pose great challenges of detecting them from the chaotic background and the vague separation boundaries further impede the acquisition of their exact contours. Moving machines which ignore glasses have great risks of crashing into transparent barriers or difficulties in analysing objects reflected in the mirror, thus it is of substantial significance to accurately locate glass-like objects and completely figure out their contours. In this paper, inspired by the scale integration strategy and the refinement method, we proposed a brand-new network, named as MGNet, which consists of a Fine-Rescaling and Merging module (FRM) to improve the ability to extract spatially relationship and a Primary Prediction Guiding module (PPG) to better mine the leftover semantics from the fused features. Moreover, we supervise the model with a novel loss function with the uncertainty-aware loss to produce high-confidence segmentation maps. Unlike the existing glass segmentation models that must be trained on different settings with respect to varied datasets, our model are trained under consistent settings and has achieved superior performance on three popular public datasets. Code is available at","sentences":["Glass-like objects can be seen everywhere in our daily life which are very hard for existing methods to segment them.","The properties of transparencies pose great challenges of detecting them from the chaotic background and the vague separation boundaries further impede the acquisition of their exact contours.","Moving machines which ignore glasses have great risks of crashing into transparent barriers or difficulties in analysing objects reflected in the mirror, thus it is of substantial significance to accurately locate glass-like objects and completely figure out their contours.","In this paper, inspired by the scale integration strategy and the refinement method, we proposed a brand-new network, named as MGNet, which consists of a Fine-Rescaling and Merging module (FRM) to improve the ability to extract spatially relationship and a Primary Prediction Guiding module (PPG) to better mine the leftover semantics from the fused features.","Moreover, we supervise the model with a novel loss function with the uncertainty-aware loss to produce high-confidence segmentation maps.","Unlike the existing glass segmentation models that must be trained on different settings with respect to varied datasets, our model are trained under consistent settings and has achieved superior performance on three popular public datasets.","Code is available at"],"url":"http://arxiv.org/abs/2402.08571v1","category":"cs.CV"}
{"created":"2024-02-13 15:39:28","title":"Grace Period is All You Need: Individual Fairness without Revenue Loss in Revenue Management","abstract":"Imagine you and a friend purchase identical items at a store, yet only your friend received a discount. Would your friend's discount make you feel unfairly treated by the store? And would you be less willing to purchase from that store again in the future? Based on a large-scale online survey that we ran on Prolific, it turns out that the answers to the above questions are positive. Motivated by these findings, in this work we propose a notion of individual fairness in online revenue management and an algorithmic module (called ``Grace Period'') that can be embedded in traditional revenue management algorithms and guarantee individual fairness. Specifically, we show how to embed the Grace Period in five common revenue management algorithms including Deterministic Linear Programming with Probabilistic Assignment, Resolving Deterministic Linear Programming with Probabilistic Assignment, Static Bid Price Control, Booking Limit, and Nesting, thus covering both stochastic and adversarial customer arrival settings. Embedding the Grace Period does not incur additional regret for any of these algorithms. This finding indicates that there is no tradeoff between a seller maximizing their revenue and guaranteeing that each customer feels fairly treated.","sentences":["Imagine you and a friend purchase identical items at a store, yet only your friend received a discount.","Would your friend's discount make you feel unfairly treated by the store?","And would you be less willing to purchase from that store again in the future?","Based on a large-scale online survey that we ran on Prolific, it turns out that the answers to the above questions are positive.","Motivated by these findings, in this work we propose a notion of individual fairness in online revenue management and an algorithmic module (called ``Grace Period'') that can be embedded in traditional revenue management algorithms and guarantee individual fairness.","Specifically, we show how to embed the Grace Period in five common revenue management algorithms including Deterministic Linear Programming with Probabilistic Assignment, Resolving Deterministic Linear Programming with Probabilistic Assignment, Static Bid Price Control, Booking Limit, and Nesting, thus covering both stochastic and adversarial customer arrival settings.","Embedding the Grace Period does not incur additional regret for any of these algorithms.","This finding indicates that there is no tradeoff between a seller maximizing their revenue and guaranteeing that each customer feels fairly treated."],"url":"http://arxiv.org/abs/2402.08533v1","category":"cs.GT"}
{"created":"2024-02-13 15:03:02","title":"A PAC-Bayesian Link Between Generalisation and Flat Minima","abstract":"Modern machine learning usually involves predictors in the overparametrised setting (number of trained parameters greater than dataset size), and their training yield not only good performances on training data, but also good generalisation capacity. This phenomenon challenges many theoretical results, and remains an open problem. To reach a better understanding, we provide novel generalisation bounds involving gradient terms. To do so, we combine the PAC-Bayes toolbox with Poincar\\'e and Log-Sobolev inequalities, avoiding an explicit dependency on dimension of the predictor space. Our results highlight the positive influence of \\emph{flat minima} (being minima with a neighbourhood nearly minimising the learning problem as well) on generalisation performances, involving directly the benefits of the optimisation phase.","sentences":["Modern machine learning usually involves predictors in the overparametrised setting (number of trained parameters greater than dataset size), and their training yield not only good performances on training data, but also good generalisation capacity.","This phenomenon challenges many theoretical results, and remains an open problem.","To reach a better understanding, we provide novel generalisation bounds involving gradient terms.","To do so, we combine the PAC-Bayes toolbox with Poincar\\'e and Log-Sobolev inequalities, avoiding an explicit dependency on dimension of the predictor space.","Our results highlight the positive influence of \\emph{flat minima} (being minima with a neighbourhood nearly minimising the learning problem as well) on generalisation performances, involving directly the benefits of the optimisation phase."],"url":"http://arxiv.org/abs/2402.08508v1","category":"stat.ML"}
{"created":"2024-02-13 14:48:52","title":"Deep learning enhanced cost-aware multi-fidelity uncertainty quantification of a computational model for radiotherapy","abstract":"Forward uncertainty quantification (UQ) for partial differential equations is a many-query task that requires a significant number of model evaluations. The objective of this work is to mitigate the computational cost of UQ for a 3D-1D multiscale computational model of microcirculation. To this purpose, we present a deep learning enhanced multi-fidelity Monte Carlo (DL-MFMC) method that integrates the information of a multiscale full-order model (FOM) with that coming from a deep learning enhanced non-intrusive projection-based reduced order model (ROM). The latter is constructed by leveraging on proper orthogonal decomposition (POD) and mesh-informed neural networks (previously developed by the authors and co-workers), integrating diverse architectures that approximate POD coefficients while introducing fine-scale corrections for the microstructures. The DL-MFMC approach provides a robust estimator of specific quantities of interest and their associated uncertainties, with optimal management of computational resources. In particular, the computational budget is efficiently divided between training and sampling, ensuring a reliable estimation process suitably exploiting the ROM speed-up. Here, we apply the DL-MFMC technique to accelerate the estimation of biophysical quantities regarding oxygen transfer and radiotherapy outcomes. Compared to classical Monte Carlo methods, the proposed approach shows remarkable speed-ups and a substantial reduction of the overall computational cost.","sentences":["Forward uncertainty quantification (UQ) for partial differential equations is a many-query task that requires a significant number of model evaluations.","The objective of this work is to mitigate the computational cost of UQ for a 3D-1D multiscale computational model of microcirculation.","To this purpose, we present a deep learning enhanced multi-fidelity Monte Carlo (DL-MFMC) method that integrates the information of a multiscale full-order model (FOM) with that coming from a deep learning enhanced non-intrusive projection-based reduced order model (ROM).","The latter is constructed by leveraging on proper orthogonal decomposition (POD) and mesh-informed neural networks (previously developed by the authors and co-workers), integrating diverse architectures that approximate POD coefficients while introducing fine-scale corrections for the microstructures.","The DL-MFMC approach provides a robust estimator of specific quantities of interest and their associated uncertainties, with optimal management of computational resources.","In particular, the computational budget is efficiently divided between training and sampling, ensuring a reliable estimation process suitably exploiting the ROM speed-up.","Here, we apply the DL-MFMC technique to accelerate the estimation of biophysical quantities regarding oxygen transfer and radiotherapy outcomes.","Compared to classical Monte Carlo methods, the proposed approach shows remarkable speed-ups and a substantial reduction of the overall computational cost."],"url":"http://arxiv.org/abs/2402.08494v1","category":"math.NA"}
{"created":"2024-02-13 14:20:09","title":"High Resolution Imaging Spectroscopy of a Tiny Sigmoidal Mini-filament Eruption","abstract":"Minifilament (MF) eruption producing small jets and micro-flares is regarded as an important source for coronal heating and the solar wind transients through studies mostly based on coronal observations in the extreme ultraviolet (EUV) and X-ray wavelengths. In this study, we focus on the chromospheric plasma diagnostics of a tiny minifilament in quiet Sun located at [71'', 450''] on 2021--08--07 at 19:11 UT observed as part of the ninth encounter of the PSP campaign. Main data obtained are the high cadence, high resolution spectroscopy from the Fast Imaging Solar Spectrograph (FISS) and high-resolution magnetograms from the Near InfraRed Imaging Spectropolarimeter (NIRIS) on the 1.6~m Goode Solar Telescope (GST) at Big Bear Solar Observatory (BBSO). The mini-filament with size $\\sim$1''$\\times$5'' and a micro-flare are detected in both the H$\\alpha$ line center and SDO/AIA 193, 304~\\AA\\ images. On the NIRIS magnetogram, we found that the cancellation of a magnetic bipole in the footpoints of the minifilament triggered its eruption in a sigmoidal shape. By inversion of the \\ha\\ and Ca {\\sc ii} spectra under the embedded cloud model, we found a temperature increase of 3,800 K in the brightening region, associated with rising speed average of MF increased by 18~$km~s^{-1}$. This cool plasma is also found in the EUV images. We estimate the kinetic energy change of the rising filament as 1.5$\\times$$10^{25}$~ergs, and thermal energy accumulation in the MF, 1.4$\\times$$10^{25}$~ergs. From the photospheric magnetograms, we find the magnetic energy change is 1.6$\\times$$10^{26}$~ergs across the PIL of converging opposite magnetic elements, which amounts to the energy release in the chromosphere in this smallest two-ribbon flare ever observed.","sentences":["Minifilament (MF) eruption producing small jets and micro-flares is regarded as an important source for coronal heating and the solar wind transients through studies mostly based on coronal observations in the extreme ultraviolet (EUV) and X-ray wavelengths.","In this study, we focus on the chromospheric plasma diagnostics of a tiny minifilament in quiet Sun located at [71'', 450''] on 2021--08--07 at 19:11 UT observed as part of the ninth encounter of the PSP campaign.","Main data obtained are the high cadence, high resolution spectroscopy from the Fast Imaging Solar Spectrograph (FISS) and high-resolution magnetograms from the Near InfraRed Imaging Spectropolarimeter (NIRIS) on the 1.6~m Goode Solar Telescope (GST) at Big Bear Solar Observatory (BBSO).","The mini-filament with size $\\sim$1''$\\times$5'' and a micro-flare are detected in both the H$\\alpha$ line center and SDO/AIA 193, 304~\\AA\\ images.","On the NIRIS magnetogram, we found that the cancellation of a magnetic bipole in the footpoints of the minifilament triggered its eruption in a sigmoidal shape.","By inversion of the \\ha\\ and Ca {\\sc ii} spectra under the embedded cloud model, we found a temperature increase of 3,800 K in the brightening region, associated with rising speed average of MF increased by 18~$km~s^{-1}$. This cool plasma is also found in the EUV images.","We estimate the kinetic energy change of the rising filament as 1.5$\\times$$10^{25}$~ergs, and thermal energy accumulation in the MF, 1.4$\\times$$10^{25}$~ergs.","From the photospheric magnetograms, we find the magnetic energy change is 1.6$\\times$$10^{26}$~ergs across the PIL of converging opposite magnetic elements, which amounts to the energy release in the chromosphere in this smallest two-ribbon flare ever observed."],"url":"http://arxiv.org/abs/2402.08483v1","category":"astro-ph.SR"}
{"created":"2024-02-13 14:08:18","title":"HQNET: Harnessing Quantum Noise for Effective Training of Quantum Neural Networks in NISQ Era","abstract":"This paper delves into the intricate dynamics of quantum noise and its influence on the onset and mitigation of barren plateaus (BPs) - a phenomenon that critically impedes the scalability of QNNs. We find that BPs appear earlier in noisy quantum environments compared to ideal, noise-free conditions.However, strategic selection of qubit measurement observables can effectively tackle this issue. To this end, we examine a variety of observables, such as PauliZ,PauliX, PauliY, and a specially designed arbitrary Hermitian observable, tailored to the requirements of the cost function and the desired outputs of quantum circuits. Our analysis encompasses both global and local cost function definitions, with the former involving measurements across all qubits and the latter focusing on single-qubit measurements within the QNN framework. Our findings indicate that in a global cost function scenario, PauliX and PauliY observables lead to flatter optimization landscapes, signaling BPs with increasing qubits, especially in noisy conditions. Conversely, the PauliZ observable maintains trainability up to 8 qubits but encounters BPs at 10 qubits. Notably, the arbitrary Hermitian observable, when used with a global cost function, shows a unique advantage as it benefits from noise, facilitating effective training up to 10 qubits. Furthermore, with a local cost function, out of the three conventional observables (PauliX, PauliY and PauliZ), PauliZ is more effective, sustaining training efficiency under noisy conditions for up to 10 qubits, while PauliX and PauliY do not show similar benefits and remain susceptible to BPs. Our results highlight the importance of noise consideration in QNN training and propose a strategic approach to observable selection to improve QNN performance in noisy quantum computing environments thus contributing to the advancement of quantum machine learning research.","sentences":["This paper delves into the intricate dynamics of quantum noise and its influence on the onset and mitigation of barren plateaus (BPs) - a phenomenon that critically impedes the scalability of QNNs.","We find that BPs appear earlier in noisy quantum environments compared to ideal, noise-free conditions.","However, strategic selection of qubit measurement observables can effectively tackle this issue.","To this end, we examine a variety of observables, such as PauliZ,PauliX, PauliY, and a specially designed arbitrary Hermitian observable, tailored to the requirements of the cost function and the desired outputs of quantum circuits.","Our analysis encompasses both global and local cost function definitions, with the former involving measurements across all qubits and the latter focusing on single-qubit measurements within the QNN framework.","Our findings indicate that in a global cost function scenario, PauliX and PauliY observables lead to flatter optimization landscapes, signaling BPs with increasing qubits, especially in noisy conditions.","Conversely, the PauliZ observable maintains trainability up to 8 qubits but encounters BPs at 10 qubits.","Notably, the arbitrary Hermitian observable, when used with a global cost function, shows a unique advantage as it benefits from noise, facilitating effective training up to 10 qubits.","Furthermore, with a local cost function, out of the three conventional observables (PauliX, PauliY and PauliZ), PauliZ is more effective, sustaining training efficiency under noisy conditions for up to 10 qubits, while PauliX and PauliY do not show similar benefits and remain susceptible to BPs.","Our results highlight the importance of noise consideration in QNN training and propose a strategic approach to observable selection to improve QNN performance in noisy quantum computing environments thus contributing to the advancement of quantum machine learning research."],"url":"http://arxiv.org/abs/2402.08475v1","category":"quant-ph"}
{"created":"2024-02-13 13:39:41","title":"The SRG/eROSITA All-Sky Survey: Weak-Lensing of eRASS1 Galaxy Clusters in KiDS-1000 and Consistency Checks with DES Y3 & HSC-Y3","abstract":"We aim to participate in the calibration of the X-ray photon count rate to halo mass scaling relation of galaxy clusters selected in the first eROSITA All-Sky Survey on the Western Galactic Hemisphere (eRASS1) using KiDS-1000 weak-lensing (WL) data. We measure the radial shear profiles around eRASS1 galaxy clusters using background galaxies in KiDS-1000, as well as the cluster member contamination. Furthermore we provide consistency checks with the other stage-III WL surveys who take part in the eRASS1 mass calibration, DES Y3 and HSC-Y3. We determine the cluster member contamination of eRASS1 clusters present in KiDS-1000 based on source number density profiles, where we account for the obscuration caused by cluster galaxies. The extracted shear profiles, together with the contamination model and the lens sample selection, are then analysed through a Bayesian population model. We calibrate the WL mass bias parameter by analysing realistic synthetic shear profiles from mock cluster catalogues. Our consistency checks between KiDS-1000 and DES Y3 & HSC-Y3 include the comparison of contamination-corrected density contrast profiles employing the union of background sources around common clusters, as well as the individual scaling relation results. We present a global contamination model for eRASS1 clusters in KiDS-1000 and the calibration results of the X-ray photon count rate to halo mass relation. The results of the WL mass bias parameter show that the uncertainty of the multiplicative shear bias dominates the systematic error budget at low clusters redshifts while the uncertainty of our contamination model does at high ones. The cross-checks between the three WL surveys show that they are statistically consistent with each other. This enables for the first time cosmological constraints from clusters calibrated by three state-of-the-art WL surveys. (abridged)","sentences":["We aim to participate in the calibration of the X-ray photon count rate to halo mass scaling relation of galaxy clusters selected in the first eROSITA All-Sky Survey on the Western Galactic Hemisphere (eRASS1) using KiDS-1000 weak-lensing (WL) data.","We measure the radial shear profiles around eRASS1 galaxy clusters using background galaxies in KiDS-1000, as well as the cluster member contamination.","Furthermore we provide consistency checks with the other stage-III WL surveys who take part in the eRASS1 mass calibration, DES Y3 and HSC-Y3.","We determine the cluster member contamination of eRASS1 clusters present in KiDS-1000 based on source number density profiles, where we account for the obscuration caused by cluster galaxies.","The extracted shear profiles, together with the contamination model and the lens sample selection, are then analysed through a Bayesian population model.","We calibrate the WL mass bias parameter by analysing realistic synthetic shear profiles from mock cluster catalogues.","Our consistency checks between KiDS-1000 and DES Y3 & HSC-Y3 include the comparison of contamination-corrected density contrast profiles employing the union of background sources around common clusters, as well as the individual scaling relation results.","We present a global contamination model for eRASS1 clusters in KiDS-1000 and the calibration results of the X-ray photon count rate to halo mass relation.","The results of the WL mass bias parameter show that the uncertainty of the multiplicative shear bias dominates the systematic error budget at low clusters redshifts while the uncertainty of our contamination model does at high ones.","The cross-checks between the three WL surveys show that they are statistically consistent with each other.","This enables for the first time cosmological constraints from clusters calibrated by three state-of-the-art WL surveys.","(abridged)"],"url":"http://arxiv.org/abs/2402.08456v1","category":"astro-ph.CO"}
{"created":"2024-02-13 13:39:40","title":"The SRG/eROSITA All-Sky Survey: Dark Energy Survey Year 3 Weak Gravitational Lensing by eRASS1 selected Galaxy Clusters","abstract":"Number counts of galaxy clusters across redshift are a powerful cosmological probe, if a precise and accurate reconstruction of the underlying mass distribution is performed -- a challenge called mass calibration. With the advent of wide and deep photometric surveys, weak gravitational lensing by clusters has become the method of choice to perform this measurement. We measure and validate the weak gravitational lensing (WL) signature in the shape of galaxies observed in the first 3 years of the DES Y3 caused by galaxy clusters selected in the first all-sky survey performed by SRG/eROSITA. These data are then used to determine the scaling between X-ray photon count rate of the clusters and their halo mass and redshift. We empirically determine the degree of cluster member contamination in our background source sample. The individual cluster shear profiles are then analysed with a Bayesian population model that self-consistently accounts for the lens sample selection and contamination, and includes marginalization over a host of instrumental and astrophysical systematics. To quantify the accuracy of the mass extraction of that model, we perform mass measurements on mock cluster catalogs with realistic synthetic shear profiles. This allows us to establish that hydro-dynamical modelling uncertainties at low lens redshifts ($z<0.6$) are the dominant systematic limitation. At high lens redshift the uncertainties of the sources' photometric redshift calibration dominate. With regard to the X-ray count rate to halo mass relation, we constrain all its parameters. This work sets the stage for a joint analysis with the number counts of eRASS1 clusters to constrain a host of cosmological parameters. We demonstrate that WL mass calibration of galaxy clusters can be performed successfully with source galaxies whose calibration was performed primarily for cosmic shear experiments.","sentences":["Number counts of galaxy clusters across redshift are a powerful cosmological probe, if a precise and accurate reconstruction of the underlying mass distribution is performed -- a challenge called mass calibration.","With the advent of wide and deep photometric surveys, weak gravitational lensing by clusters has become the method of choice to perform this measurement.","We measure and validate the weak gravitational lensing (WL) signature in the shape of galaxies observed in the first 3 years of the DES Y3 caused by galaxy clusters selected in the first all-sky survey performed by SRG/eROSITA.","These data are then used to determine the scaling between X-ray photon count rate of the clusters and their halo mass and redshift.","We empirically determine the degree of cluster member contamination in our background source sample.","The individual cluster shear profiles are then analysed with a Bayesian population model that self-consistently accounts for the lens sample selection and contamination, and includes marginalization over a host of instrumental and astrophysical systematics.","To quantify the accuracy of the mass extraction of that model, we perform mass measurements on mock cluster catalogs with realistic synthetic shear profiles.","This allows us to establish that hydro-dynamical modelling uncertainties at low lens redshifts ($z<0.6$) are the dominant systematic limitation.","At high lens redshift the uncertainties of the sources' photometric redshift calibration dominate.","With regard to the X-ray count rate to halo mass relation, we constrain all its parameters.","This work sets the stage for a joint analysis with the number counts of eRASS1 clusters to constrain a host of cosmological parameters.","We demonstrate that WL mass calibration of galaxy clusters can be performed successfully with source galaxies whose calibration was performed primarily for cosmic shear experiments."],"url":"http://arxiv.org/abs/2402.08455v1","category":"astro-ph.CO"}
{"created":"2024-02-13 13:38:06","title":"Moonwalk: Advancing Gait-Based User Recognition on Wearable Devices with Metric Learning","abstract":"Personal devices have adopted diverse authentication methods, including biometric recognition and passcodes. In contrast, headphones have limited input mechanisms, depending solely on the authentication of connected devices. We present Moonwalk, a novel method for passive user recognition utilizing the built-in headphone accelerometer. Our approach centers on gait recognition; enabling users to establish their identity simply by walking for a brief interval, despite the sensor's placement away from the feet. We employ self-supervised metric learning to train a model that yields a highly discriminative representation of a user's 3D acceleration, with no retraining required. We tested our method in a study involving 50 participants, achieving an average F1 score of 92.9% and equal error rate of 2.3%. We extend our evaluation by assessing performance under various conditions (e.g. shoe types and surfaces). We discuss the opportunities and challenges these variations introduce and propose new directions for advancing passive authentication for wearable devices.","sentences":["Personal devices have adopted diverse authentication methods, including biometric recognition and passcodes.","In contrast, headphones have limited input mechanisms, depending solely on the authentication of connected devices.","We present Moonwalk, a novel method for passive user recognition utilizing the built-in headphone accelerometer.","Our approach centers on gait recognition; enabling users to establish their identity simply by walking for a brief interval, despite the sensor's placement away from the feet.","We employ self-supervised metric learning to train a model that yields a highly discriminative representation of a user's 3D acceleration, with no retraining required.","We tested our method in a study involving 50 participants, achieving an average F1 score of 92.9% and equal error rate of 2.3%.","We extend our evaluation by assessing performance under various conditions (e.g. shoe types and surfaces).","We discuss the opportunities and challenges these variations introduce and propose new directions for advancing passive authentication for wearable devices."],"url":"http://arxiv.org/abs/2402.08451v1","category":"cs.HC"}
{"created":"2024-02-13 13:13:13","title":"T-semidefinite programming relaxation with third-order tensors for constrained polynomial optimization","abstract":"We study T-semidefinite programming (SDP) relaxation for constrained polynomial optimization problems (POPs). T-SDP relaxation for unconstrained POPs was introduced by Zheng, Huang and Hu in 2022. In this work, we propose a T-SDP relaxation for POPs with polynomial inequality constraints and show that the resulting T-SDP relaxation formulated with third-order tensors can be transformed into the standard SDP relaxation with block-diagonal structures. The convergence of the T-SDP relaxation to the optimal value of a given constrained POP is established under moderate assumptions as the relaxation level increases. Additionally, the feasibility and optimality of the T-SDP relaxation are discussed. Numerical results illustrate that the proposed T-SDP relaxation enhances numerical efficiency.","sentences":["We study T-semidefinite programming (SDP) relaxation for constrained polynomial optimization problems (POPs).","T-SDP relaxation for unconstrained POPs was introduced by Zheng, Huang and Hu in 2022.","In this work, we propose a T-SDP relaxation for POPs with polynomial inequality constraints and show that the resulting T-SDP relaxation formulated with third-order tensors can be transformed into the standard SDP relaxation with block-diagonal structures.","The convergence of the T-SDP relaxation to the optimal value of a given constrained POP is established under moderate assumptions as the relaxation level increases.","Additionally, the feasibility and optimality of the T-SDP relaxation are discussed.","Numerical results illustrate that the proposed T-SDP relaxation enhances numerical efficiency."],"url":"http://arxiv.org/abs/2402.08438v1","category":"math.OC"}
{"created":"2024-02-13 12:49:50","title":"Distribution Estimation under the Infinity Norm","abstract":"We present novel bounds for estimating discrete probability distributions under the $\\ell_\\infty$ norm. These are nearly optimal in various precise senses, including a kind of instance-optimality. Our data-dependent convergence guarantees for the maximum likelihood estimator significantly improve upon the currently known results. A variety of techniques are utilized and innovated upon, including Chernoff-type inequalities and empirical Bernstein bounds. We illustrate our results in synthetic and real-world experiments. Finally, we apply our proposed framework to a basic selective inference problem, where we estimate the most frequent probabilities in a sample.","sentences":["We present novel bounds for estimating discrete probability distributions under the $\\ell_\\infty$ norm.","These are nearly optimal in various precise senses, including a kind of instance-optimality.","Our data-dependent convergence guarantees for the maximum likelihood estimator significantly improve upon the currently known results.","A variety of techniques are utilized and innovated upon, including Chernoff-type inequalities and empirical Bernstein bounds.","We illustrate our results in synthetic and real-world experiments.","Finally, we apply our proposed framework to a basic selective inference problem, where we estimate the most frequent probabilities in a sample."],"url":"http://arxiv.org/abs/2402.08422v1","category":"math.ST"}
{"created":"2024-02-13 12:46:58","title":"Variations on Sidorenko's conjecture in tournaments","abstract":"We study variants of Sidorenko's conjecture in tournaments, where new phenomena arise that do not have clear analogues in the setting of undirected graphs. We first consider oriented graphs that are systematically under-represented in tournaments (called tournament anti-Sidorenko). We prove that such oriented graphs must be quite sparse; specifically, the maximum number of edges of a $k$-vertex oriented graph which is tournament anti-Sidorenko is $(1+o(1))k\\log_2 k$.   We also give several novel constructions of oriented graphs that are systematically over-represented in tournaments (tournament Sidorenko); as a representative example, we show that most ways to delete an edge from a transitive tournament yield a tournament Sidorenko oriented graph. As an illustration of our methods, we characterize which orientations of stars are tournament Sidorenko and which are tournament anti-Sidorenko.","sentences":["We study variants of Sidorenko's conjecture in tournaments, where new phenomena arise that do not have clear analogues in the setting of undirected graphs.","We first consider oriented graphs that are systematically under-represented in tournaments (called tournament anti-Sidorenko).","We prove that such oriented graphs must be quite sparse; specifically, the maximum number of edges of a $k$-vertex oriented graph which is tournament anti-Sidorenko is $(1+o(1))k\\log_2 k$.   ","We also give several novel constructions of oriented graphs that are systematically over-represented in tournaments (tournament Sidorenko); as a representative example, we show that most ways to delete an edge from a transitive tournament yield a tournament Sidorenko oriented graph.","As an illustration of our methods, we characterize which orientations of stars are tournament Sidorenko and which are tournament anti-Sidorenko."],"url":"http://arxiv.org/abs/2402.08418v1","category":"math.CO"}
{"created":"2024-02-13 12:12:39","title":"Coding-Based Hybrid Post-Quantum Cryptosystem for Non-Uniform Information","abstract":"We introduce for non-uniform messages a novel hybrid universal network coding cryptosystem (NU-HUNCC) in the finite blocklength regime that provides Post-Quantum (PQ) security at high communication rates. Recently, hybrid cryptosystems offered PQ security by premixing the data using secure coding schemes and encrypting only a small portion of it, assuming the data is uniformly distributed. An assumption that is often challenging to enforce. Standard fixed-length lossless source coding and compression schemes guarantee a uniform output in normalized divergence. Yet, his is not sufficient to guarantee security. We consider an efficient almost uniform compression scheme in non-normalized variational distance for the proposed hybrid cryptosystem, that by utilizing uniform sub-linear shared seed, guarantees PQ security. Specifically, for the proposed PQ cryptosystem, first, we provide an end-to-end coding scheme, NU-HUNCC, for non-uniform messages. Second, we show that NU-HUNCC is information-theoretic individually secured (IS) against an eavesdropper with access to any subset of the links. Third, we introduce a modified security definition, individually semantically secure under a chosen ciphertext attack (ISS-CCA1), and show that against an all-observing eavesdropper, NU-HUNCC satisfies its conditions. Finally, we provide an analysis that shows the high communication rate of NU-HUNCC and the negligibility of the shared seed size.","sentences":["We introduce for non-uniform messages a novel hybrid universal network coding cryptosystem (NU-HUNCC) in the finite blocklength regime that provides Post-Quantum (PQ) security at high communication rates.","Recently, hybrid cryptosystems offered PQ security by premixing the data using secure coding schemes and encrypting only a small portion of it, assuming the data is uniformly distributed.","An assumption that is often challenging to enforce.","Standard fixed-length lossless source coding and compression schemes guarantee a uniform output in normalized divergence.","Yet, his is not sufficient to guarantee security.","We consider an efficient almost uniform compression scheme in non-normalized variational distance for the proposed hybrid cryptosystem, that by utilizing uniform sub-linear shared seed, guarantees PQ security.","Specifically, for the proposed PQ cryptosystem, first, we provide an end-to-end coding scheme, NU-HUNCC, for non-uniform messages.","Second, we show that NU-HUNCC is information-theoretic individually secured (IS) against an eavesdropper with access to any subset of the links.","Third, we introduce a modified security definition, individually semantically secure under a chosen ciphertext attack (ISS-CCA1), and show that against an all-observing eavesdropper, NU-HUNCC satisfies its conditions.","Finally, we provide an analysis that shows the high communication rate of NU-HUNCC and the negligibility of the shared seed size."],"url":"http://arxiv.org/abs/2402.08407v1","category":"cs.CR"}
{"created":"2024-02-13 12:11:40","title":"Transition Constrained Bayesian Optimization via Markov Decision Processes","abstract":"Bayesian optimization is a methodology to optimize black-box functions. Traditionally, it focuses on the setting where you can arbitrarily query the search space. However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones. Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements. Altogether, such transition constraints necessitate a form of planning. This work extends Bayesian optimization via the framework of Markov Decision Processes, iteratively solving a tractable linearization of our objective using reinforcement learning to obtain a policy that plans ahead over long horizons. The resulting policy is potentially history-dependent and non-Markovian. We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other synthetic examples.","sentences":["Bayesian optimization is a methodology to optimize black-box functions.","Traditionally, it focuses on the setting where you can arbitrarily query the search space.","However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones.","Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements.","Altogether, such transition constraints necessitate a form of planning.","This work extends Bayesian optimization via the framework of Markov Decision Processes, iteratively solving a tractable linearization of our objective using reinforcement learning to obtain a policy that plans ahead over long horizons.","The resulting policy is potentially history-dependent and non-Markovian.","We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other synthetic examples."],"url":"http://arxiv.org/abs/2402.08406v1","category":"cs.LG"}
{"created":"2024-02-13 11:56:15","title":"A Neural-network Enhanced Video Coding Framework beyond ECM","abstract":"In this paper, a hybrid video compression framework is proposed that serves as a demonstrative showcase of deep learning-based approaches extending beyond the confines of traditional coding methodologies. The proposed hybrid framework is founded upon the Enhanced Compression Model (ECM), which is a further enhancement of the Versatile Video Coding (VVC) standard. We have augmented the latest ECM reference software with well-designed coding techniques, including block partitioning, deep learning-based loop filter, and the activation of block importance mapping (BIM) which was integrated but previously inactive within ECM, further enhancing coding performance. Compared with ECM-10.0, our method achieves 6.26, 13.33, and 12.33 BD-rate savings for the Y, U, and V components under random access (RA) configuration, respectively.","sentences":["In this paper, a hybrid video compression framework is proposed that serves as a demonstrative showcase of deep learning-based approaches extending beyond the confines of traditional coding methodologies.","The proposed hybrid framework is founded upon the Enhanced Compression Model (ECM), which is a further enhancement of the Versatile Video Coding (VVC) standard.","We have augmented the latest ECM reference software with well-designed coding techniques, including block partitioning, deep learning-based loop filter, and the activation of block importance mapping (BIM) which was integrated but previously inactive within ECM, further enhancing coding performance.","Compared with ECM-10.0, our method achieves 6.26, 13.33, and 12.33 BD-rate savings for the Y, U, and V components under random access (RA) configuration, respectively."],"url":"http://arxiv.org/abs/2402.08397v1","category":"cs.CV"}
{"created":"2024-02-13 11:43:11","title":"Domestic Competitive Balance and International Success: The Case of The Football Industry","abstract":"This paper examines the interdependence of international success and competitive balance of domestic sports competitions. More specifically, we apply the notion of the Herfindahl-Hirschman index to examine the effect of international rewards on distortion of competitive balance in domestic competitions and derive conditions under which the level of domestic competitive balance raises or falls. Our results yield interesting policy implications for the regulation of prize schemes in international competitions.","sentences":["This paper examines the interdependence of international success and competitive balance of domestic sports competitions.","More specifically, we apply the notion of the Herfindahl-Hirschman index to examine the effect of international rewards on distortion of competitive balance in domestic competitions and derive conditions under which the level of domestic competitive balance raises or falls.","Our results yield interesting policy implications for the regulation of prize schemes in international competitions."],"url":"http://arxiv.org/abs/2402.08396v1","category":"econ.TH"}
{"created":"2024-02-13 10:49:53","title":"Astronomy potential of KM3NeT/ARCA","abstract":"The KM3NeT/ARCA neutrino detector is currently under construction at 3500 m depth offshore Capo Passero, Sicily, in the Mediterranean Sea. The main science objectives are the detection of high-energy cosmic neutrinos and the discovery of their sources. Simulations were conducted for the full KM3NeT/ARCA detector, instrumenting a volume of 1 km$^3$, to estimate the sensitivity and discovery potential to point-like neutrino sources and an all-sky diffuse neutrino flux. This paper covers the reconstruction of track- and shower-like signatures, as well as the criteria employed for neutrino event selection. By leveraging both the track and shower observation channels, the KM3NeT/ARCA detector demonstrates the capability to detect the diffuse astrophysical neutrino flux within half a year of operation, achieving a 5$\\sigma$ statistical significance. With an angular resolution below 0.1$^\\circ$ for tracks and under 2$^\\circ$ for showers, the sensitivity to point-like neutrino sources surpasses existing observed limits across the entire sky.","sentences":["The KM3NeT/ARCA neutrino detector is currently under construction at 3500 m depth offshore Capo Passero, Sicily, in the Mediterranean Sea.","The main science objectives are the detection of high-energy cosmic neutrinos and the discovery of their sources.","Simulations were conducted for the full KM3NeT/ARCA detector, instrumenting a volume of 1 km$^3$, to estimate the sensitivity and discovery potential to point-like neutrino sources and an all-sky diffuse neutrino flux.","This paper covers the reconstruction of track- and shower-like signatures, as well as the criteria employed for neutrino event selection.","By leveraging both the track and shower observation channels, the KM3NeT/","ARCA detector demonstrates the capability to detect the diffuse astrophysical neutrino flux within half a year of operation, achieving a 5$\\sigma$ statistical significance.","With an angular resolution below 0.1$^\\circ$ for tracks and under 2$^\\circ$ for showers, the sensitivity to point-like neutrino sources surpasses existing observed limits across the entire sky."],"url":"http://arxiv.org/abs/2402.08363v1","category":"astro-ph.HE"}
{"created":"2024-02-13 10:38:35","title":"De la Vall\u00e9e Poussin filtered polynomial approximation on the half line","abstract":"On the half line we introduce a new sequence of near--best uniform approximation polynomials, easily computable by the values of the approximated function at a truncated number of Laguerre zeros. Such approximation polynomials come from a discretization of filtered Fourier--Laguerre partial sums, which are filtered by using a de la Vall\\'ee Poussin (VP) filter. They have the peculiarity of depending on two parameters: a truncation parameter that determines how many of the $n$ Laguerre zeros are considered, and a localization parameter, which determines the range of action of the VP filter that we are going to apply. As $n\\to\\infty$, under simple assumptions on such parameters and on the Laguerre exponents of the involved weights, we prove that the new VP filtered approximation polynomials have uniformly bounded Lebesgue constants and uniformly convergence at a near--best approximation rate, for any locally continuous function on the semiaxis. \\newline The theoretical results have been validated by the numerical experiments. In particular, they show a better performance of the proposed VP filtered approximation versus the truncated Lagrange interpolation at the same nodes, especially for functions a.e. very smooth with isolated singularities. In such cases we see a more localized approximation as well as a good reduction of the Gibbs phenomenon.","sentences":["On the half line we introduce a new sequence of near--best uniform approximation polynomials, easily computable by the values of the approximated function at a truncated number of Laguerre zeros.","Such approximation polynomials come from a discretization of filtered Fourier--Laguerre partial sums, which are filtered by using a de la Vall\\'ee Poussin (VP) filter.","They have the peculiarity of depending on two parameters: a truncation parameter that determines how many of the $n$ Laguerre zeros are considered, and a localization parameter, which determines the range of action of the VP filter that we are going to apply.","As $n\\to\\infty$, under simple assumptions on such parameters and on the Laguerre exponents of the involved weights, we prove that the new VP filtered approximation polynomials have uniformly bounded Lebesgue constants and uniformly convergence at a near--best approximation rate, for any locally continuous function on the semiaxis.","\\newline The theoretical results have been validated by the numerical experiments.","In particular, they show a better performance of the proposed VP filtered approximation versus the truncated Lagrange interpolation at the same nodes, especially for functions a.e. very smooth with isolated singularities.","In such cases we see a more localized approximation as well as a good reduction of the Gibbs phenomenon."],"url":"http://arxiv.org/abs/2402.08358v1","category":"math.NA"}
{"created":"2024-02-13 10:03:58","title":"A two-step approach for analyzing time to event data under non-proportional hazards","abstract":"The log-rank test and the Cox proportional hazards model are commonly used to compare time-to-event data in clinical trials, as they are most powerful under proportional hazards. But there is a loss of power if this assumption is violated, which is the case for some new oncology drugs like immunotherapies. We consider a two-stage test procedure, in which the weighting of the log-rank test statistic depends on a pre-test of the proportional hazards assumption. I.e., depending on the pre-test either the log-rank or an alternative test is used to compare the survival probabilities. We show that if naively implemented this can lead to a substantial inflation of the type-I error rate. To address this, we embed the two-stage test in a permutation test framework to keep the nominal level alpha. We compare the operating characteristics of the two-stage test with the log-rank test and other tests by clinical trial simulations.","sentences":["The log-rank test and the Cox proportional hazards model are commonly used to compare time-to-event data in clinical trials, as they are most powerful under proportional hazards.","But there is a loss of power if this assumption is violated, which is the case for some new oncology drugs like immunotherapies.","We consider a two-stage test procedure, in which the weighting of the log-rank test statistic depends on a pre-test of the proportional hazards assumption.","I.e., depending on the pre-test either the log-rank or an alternative test is used to compare the survival probabilities.","We show that if naively implemented this can lead to a substantial inflation of the type-I error rate.","To address this, we embed the two-stage test in a permutation test framework to keep the nominal level alpha.","We compare the operating characteristics of the two-stage test with the log-rank test and other tests by clinical trial simulations."],"url":"http://arxiv.org/abs/2402.08336v1","category":"stat.ME"}
{"created":"2024-02-13 09:56:33","title":"Beatty Sequences for a Quadratic Irrational: Decidability and Applications","abstract":"Let $\\alpha$ and $\\beta$ belong to the same quadratic field. We show that the inhomogeneous Beatty sequence $(\\lfloor n \\alpha + \\beta \\rfloor)_{n \\geq 1}$ is synchronized, in the sense that there is a finite automaton that takes as input the Ostrowski representations of $n$ and $y$ in parallel, and accepts if and only if $y = \\lfloor n \\alpha + \\beta \\rfloor$. Since it is already known that the addition relation is computable for Ostrowski representations based on a quadratic number, a consequence is a new and rather simple proof that the first-order logical theory of these sequences with addition is decidable. The decision procedure is easily implemented in the free software Walnut.   As an application, we show that for each $r \\geq 1$ it is decidable whether the set $\\{ \\lfloor n \\alpha + \\beta \\rfloor \\, : \\, n \\geq 1 \\}$ forms an additive basis (or asymptotic additive basis) of order $r$. Using our techniques, we also solve some open problems of Reble and Kimberling, and give an explicit characterization of a sequence of Hildebrand et al.","sentences":["Let $\\alpha$ and $\\beta$ belong to the same quadratic field.","We show that the inhomogeneous Beatty sequence $(\\lfloor n \\alpha + \\beta \\rfloor)_{n \\geq 1}$ is synchronized, in the sense that there is a finite automaton that takes as input the Ostrowski representations of $n$ and $y$ in parallel, and accepts if and only if $y = \\lfloor n \\alpha + \\beta \\rfloor$. Since it is already known that the addition relation is computable for Ostrowski representations based on a quadratic number, a consequence is a new and rather simple proof that the first-order logical theory of these sequences with addition is decidable.","The decision procedure is easily implemented in the free software Walnut.   ","As an application, we show that for each $r \\geq 1$ it is decidable whether the set $\\{ \\lfloor n \\alpha +","\\beta \\rfloor \\, : \\, n \\geq 1 \\}$ forms an additive basis (or asymptotic additive basis) of order $r$. Using our techniques, we also solve some open problems of Reble and Kimberling, and give an explicit characterization of a sequence of Hildebrand et al."],"url":"http://arxiv.org/abs/2402.08331v1","category":"math.NT"}
{"created":"2024-02-13 09:48:57","title":"Making Closed-Shell Lead-Pthalocyanine Paramagnetic on Pb(100)","abstract":"Lead phthalocyanine (PbPc), a non-planar molecule, is studied on Pb(100) using scanning tunneling spectroscopy. A rigid shift of the molecular orbitals is found between molecules with the central Pb ion pointing to (PbPc$\\downarrow$) or away (PbPc$\\uparrow$) from the substrate and understood from the interaction between the molecules and their image charges. Inside the superconducting energy gap, Yu-Shiba-Rusinov (YSR) resonances are observed for PbPc$\\uparrow$ molecules in islands indicating the presence of a magnetic moment. Such bound states are neither present on PbPc$\\downarrow$ molecules nor isolated PbPc$\\uparrow$ or molecules that lost the Pb ion during deposition (H$_0$Pc). The YSR energies vary depending on the orientation and type of the molecular neighbors. We analyze the role of the out-of-plane dipole moment of PbPc.","sentences":["Lead phthalocyanine (PbPc), a non-planar molecule, is studied on Pb(100) using scanning tunneling spectroscopy.","A rigid shift of the molecular orbitals is found between molecules with the central Pb ion pointing to (PbPc$\\downarrow$) or away (PbPc$\\uparrow$) from the substrate and understood from the interaction between the molecules and their image charges.","Inside the superconducting energy gap, Yu-Shiba-Rusinov (YSR) resonances are observed for PbPc$\\uparrow$ molecules in islands indicating the presence of a magnetic moment.","Such bound states are neither present on PbPc$\\downarrow$ molecules nor isolated PbPc$\\uparrow$ or molecules that lost the Pb ion during deposition (H$_0$Pc).","The YSR energies vary depending on the orientation and type of the molecular neighbors.","We analyze the role of the out-of-plane dipole moment of PbPc."],"url":"http://arxiv.org/abs/2402.08330v1","category":"cond-mat.supr-con"}
{"created":"2024-02-13 08:56:57","title":"Zero Trust Score-based Network-level Access Control in Enterprise Networks","abstract":"Zero Trust security has recently gained attention in enterprise network security. One of its key ideas is making network-level access decisions based on trust scores. However, score-based access control in the enterprise domain still lacks essential elements in our understanding, and in this paper, we contribute with respect to three crucial aspects. First, we provide a comprehensive list of 29 trust attributes that can be used to calculate a trust score. By introducing a novel mathematical approach, we demonstrate how to quantify these attributes. Second, we describe a dynamic risk-based method to calculate the trust threshold the trust score must meet for permitted access. Third, we introduce a novel trust algorithm based on Subjective Logic that incorporates the first two contributions and offers fine-grained decision possibilities. We discuss how this algorithm shows a higher expressiveness compared to a lightweight additive trust algorithm. Performance-wise, a prototype of the Subjective Logic-based approach showed similar calculation times for making an access decision as the additive approach. In addition, the dynamic threshold calculation showed only 7% increased decision-making times compared to a static threshold.","sentences":["Zero Trust security has recently gained attention in enterprise network security.","One of its key ideas is making network-level access decisions based on trust scores.","However, score-based access control in the enterprise domain still lacks essential elements in our understanding, and in this paper, we contribute with respect to three crucial aspects.","First, we provide a comprehensive list of 29 trust attributes that can be used to calculate a trust score.","By introducing a novel mathematical approach, we demonstrate how to quantify these attributes.","Second, we describe a dynamic risk-based method to calculate the trust threshold the trust score must meet for permitted access.","Third, we introduce a novel trust algorithm based on Subjective Logic that incorporates the first two contributions and offers fine-grained decision possibilities.","We discuss how this algorithm shows a higher expressiveness compared to a lightweight additive trust algorithm.","Performance-wise, a prototype of the Subjective Logic-based approach showed similar calculation times for making an access decision as the additive approach.","In addition, the dynamic threshold calculation showed only 7% increased decision-making times compared to a static threshold."],"url":"http://arxiv.org/abs/2402.08299v1","category":"cs.CR"}
{"created":"2024-02-13 08:48:16","title":"Learning semantic image quality for fetal ultrasound from noisy ranking annotation","abstract":"We introduce the notion of semantic image quality for applications where image quality relies on semantic requirements. Working in fetal ultrasound, where ranking is challenging and annotations are noisy, we design a robust coarse-to-fine model that ranks images based on their semantic image quality and endow our predicted rankings with an uncertainty estimate. To annotate rankings on training data, we design an efficient ranking annotation scheme based on the merge sort algorithm. Finally, we compare our ranking algorithm to a number of state-of-the-art ranking algorithms on a challenging fetal ultrasound quality assessment task, showing the superior performance of our method on the majority of rank correlation metrics.","sentences":["We introduce the notion of semantic image quality for applications where image quality relies on semantic requirements.","Working in fetal ultrasound, where ranking is challenging and annotations are noisy, we design a robust coarse-to-fine model that ranks images based on their semantic image quality and endow our predicted rankings with an uncertainty estimate.","To annotate rankings on training data, we design an efficient ranking annotation scheme based on the merge sort algorithm.","Finally, we compare our ranking algorithm to a number of state-of-the-art ranking algorithms on a challenging fetal ultrasound quality assessment task, showing the superior performance of our method on the majority of rank correlation metrics."],"url":"http://arxiv.org/abs/2402.08294v1","category":"cs.CV"}
{"created":"2024-02-13 08:33:29","title":"Covariance estimation with direction dependence accuracy","abstract":"We construct an estimator $\\widehat{\\Sigma}$ for covariance matrices of unknown, centred random vectors X, with the given data consisting of N independent measurements $X_1,...,X_N$ of X and the wanted confidence level. We show under minimal assumptions on X, the estimator performs with the optimal accuracy with respect to the operator norm. In addition, the estimator is also optimal with respect to direction dependence accuracy: $\\langle \\widehat{\\Sigma}u,u\\rangle$ is an optimal estimator for $\\sigma^2(u)=\\mathbb{E}\\langle X,u\\rangle^2$ when $\\sigma^2(u)$ is ``large\".","sentences":["We construct an estimator $\\widehat{\\Sigma}$ for covariance matrices of unknown, centred random vectors X, with the given data consisting of N independent measurements $X_1,...,X_N$ of X and the wanted confidence level.","We show under minimal assumptions on X, the estimator performs with the optimal accuracy with respect to the operator norm.","In addition, the estimator is also optimal with respect to direction dependence accuracy: $\\langle \\widehat{\\Sigma}u,u\\rangle$ is an optimal estimator for $\\sigma^2(u)=\\mathbb{E}\\langle X,u\\rangle^2$ when $\\sigma^2(u)$ is ``large\"."],"url":"http://arxiv.org/abs/2402.08288v1","category":"math.ST"}
{"created":"2024-02-13 05:43:49","title":"Causal Discovery under Off-Target Interventions","abstract":"Causal graph discovery is a significant problem with applications across various disciplines. However, with observational data alone, the underlying causal graph can only be recovered up to its Markov equivalence class, and further assumptions or interventions are necessary to narrow down the true graph. This work addresses the causal discovery problem under the setting of stochastic interventions with the natural goal of minimizing the number of interventions performed. We propose the following stochastic intervention model which subsumes existing adaptive noiseless interventions in the literature while capturing scenarios such as fat-hand interventions and CRISPR gene knockouts: any intervention attempt results in an actual intervention on a random subset of vertices, drawn from a distribution dependent on attempted action. Under this model, we study the two fundamental problems in causal discovery of verification and search and provide approximation algorithms with polylogarithmic competitive ratios and provide some preliminary experimental results.","sentences":["Causal graph discovery is a significant problem with applications across various disciplines.","However, with observational data alone, the underlying causal graph can only be recovered up to its Markov equivalence class, and further assumptions or interventions are necessary to narrow down the true graph.","This work addresses the causal discovery problem under the setting of stochastic interventions with the natural goal of minimizing the number of interventions performed.","We propose the following stochastic intervention model which subsumes existing adaptive noiseless interventions in the literature while capturing scenarios such as fat-hand interventions and CRISPR gene knockouts: any intervention attempt results in an actual intervention on a random subset of vertices, drawn from a distribution dependent on attempted action.","Under this model, we study the two fundamental problems in causal discovery of verification and search and provide approximation algorithms with polylogarithmic competitive ratios and provide some preliminary experimental results."],"url":"http://arxiv.org/abs/2402.08229v1","category":"cs.LG"}
{"created":"2024-02-13 05:36:54","title":"Privacy-Preserving Language Model Inference with Instance Obfuscation","abstract":"Language Models as a Service (LMaaS) offers convenient access for developers and researchers to perform inference using pre-trained language models. Nonetheless, the input data and the inference results containing private information are exposed as plaintext during the service call, leading to privacy issues. Recent studies have started tackling the privacy issue by transforming input data into privacy-preserving representation from the user-end with the techniques such as noise addition and content perturbation, while the exploration of inference result protection, namely decision privacy, is still a blank page. In order to maintain the black-box manner of LMaaS, conducting data privacy protection, especially for the decision, is a challenging task because the process has to be seamless to the models and accompanied by limited communication and computation overhead. We thus propose Instance-Obfuscated Inference (IOI) method, which focuses on addressing the decision privacy issue of natural language understanding tasks in their complete life-cycle. Besides, we conduct comprehensive experiments to evaluate the performance as well as the privacy-protection strength of the proposed method on various benchmarking tasks.","sentences":["Language Models as a Service (LMaaS) offers convenient access for developers and researchers to perform inference using pre-trained language models.","Nonetheless, the input data and the inference results containing private information are exposed as plaintext during the service call, leading to privacy issues.","Recent studies have started tackling the privacy issue by transforming input data into privacy-preserving representation from the user-end with the techniques such as noise addition and content perturbation, while the exploration of inference result protection, namely decision privacy, is still a blank page.","In order to maintain the black-box manner of LMaaS, conducting data privacy protection, especially for the decision, is a challenging task because the process has to be seamless to the models and accompanied by limited communication and computation overhead.","We thus propose Instance-Obfuscated Inference (IOI) method, which focuses on addressing the decision privacy issue of natural language understanding tasks in their complete life-cycle.","Besides, we conduct comprehensive experiments to evaluate the performance as well as the privacy-protection strength of the proposed method on various benchmarking tasks."],"url":"http://arxiv.org/abs/2402.08227v1","category":"cs.CL"}
{"created":"2024-02-13 05:28:38","title":"The Limits of Price Discrimination Under Privacy Constraints","abstract":"We consider a producer's problem of selling a product to a continuum of privacy-conscious consumers, where the producer can implement third-degree price discrimination, offering different prices to different market segments. In the absence of privacy constraints, Bergemann, Brooks, and Morris [2015] characterize the set of all possible consumer-producer utilities, showing that it is a triangle. We consider a privacy mechanism that provides a degree of protection by probabilistically masking each market segment, and we establish that the resultant set of all consumer-producer utilities forms a convex polygon, characterized explicitly as a linear mapping of a certain high-dimensional convex polytope into $\\mathbb{R}^2$. This characterization enables us to investigate the impact of the privacy mechanism on both producer and consumer utilities. In particular, we establish that the privacy constraint always hurts the producer by reducing both the maximum and minimum utility achievable. From the consumer's perspective, although the privacy mechanism ensures an increase in the minimum utility compared to the non-private scenario, interestingly, it may reduce the maximum utility. Finally, we demonstrate that increasing the privacy level does not necessarily intensify these effects. For instance, the maximum utility for the producer or the minimum utility for the consumer may exhibit nonmonotonic behavior in response to an increase of the privacy level.","sentences":["We consider a producer's problem of selling a product to a continuum of privacy-conscious consumers, where the producer can implement third-degree price discrimination, offering different prices to different market segments.","In the absence of privacy constraints, Bergemann, Brooks, and Morris [2015] characterize the set of all possible consumer-producer utilities, showing that it is a triangle.","We consider a privacy mechanism that provides a degree of protection by probabilistically masking each market segment, and we establish that the resultant set of all consumer-producer utilities forms a convex polygon, characterized explicitly as a linear mapping of a certain high-dimensional convex polytope into $\\mathbb{R}^2$. This characterization enables us to investigate the impact of the privacy mechanism on both producer and consumer utilities.","In particular, we establish that the privacy constraint always hurts the producer by reducing both the maximum and minimum utility achievable.","From the consumer's perspective, although the privacy mechanism ensures an increase in the minimum utility compared to the non-private scenario, interestingly, it may reduce the maximum utility.","Finally, we demonstrate that increasing the privacy level does not necessarily intensify these effects.","For instance, the maximum utility for the producer or the minimum utility for the consumer may exhibit nonmonotonic behavior in response to an increase of the privacy level."],"url":"http://arxiv.org/abs/2402.08223v1","category":"econ.TH"}
{"created":"2024-02-13 04:05:50","title":"Subsystem surface and compass code sensitivities to non-identical infidelity distributions on heavy-hex lattice","abstract":"Logical qubits encoded into a quantum code exhibit improved error rates when the physical error rates are sufficiently low, below the pseudothreshold. Logical error rates and pseudothresholds can be estimated for specific circuits and noise models, and these estimates provide approximate goals for qubit performance. However, estimates often assume uniform error rates, while real devices have static and/or dynamic distributions of non-identical error rates and may exhibit outliers. These distributions make it more challenging to evaluate, compare, and rank the expected performance of quantum processors. We numerically investigate how the logical error rate depends on parameters of the noise distribution for the subsystem surface code and the compass code on a subdivided hexagonal lattice. Three notable observations are found: (1) the average logical error rate depends on the average of the physical qubit infidelity distribution without sensitivity to higher moments (e.g., variance or outliers) for a wide parameter range; (2) the logical error rate saturates as errors increase at one or a few \"bad\" locations; and (3) a decoder that is aware of location specific error rates modestly improves the logical error rate. We discuss the implications of these results in the context of several different practical sources of outliers and non-uniform qubit error rates.","sentences":["Logical qubits encoded into a quantum code exhibit improved error rates when the physical error rates are sufficiently low, below the pseudothreshold.","Logical error rates and pseudothresholds can be estimated for specific circuits and noise models, and these estimates provide approximate goals for qubit performance.","However, estimates often assume uniform error rates, while real devices have static and/or dynamic distributions of non-identical error rates and may exhibit outliers.","These distributions make it more challenging to evaluate, compare, and rank the expected performance of quantum processors.","We numerically investigate how the logical error rate depends on parameters of the noise distribution for the subsystem surface code and the compass code on a subdivided hexagonal lattice.","Three notable observations are found: (1) the average logical error rate depends on the average of the physical qubit infidelity distribution without sensitivity to higher moments (e.g., variance or outliers) for a wide parameter range; (2) the logical error rate saturates as errors increase at one or a few \"bad\" locations; and (3) a decoder that is aware of location specific error rates modestly improves the logical error rate.","We discuss the implications of these results in the context of several different practical sources of outliers and non-uniform qubit error rates."],"url":"http://arxiv.org/abs/2402.08203v1","category":"quant-ph"}
{"created":"2024-02-13 04:03:09","title":"Confronting Discrimination in Classification: Smote Based on Marginalized Minorities in the Kernel Space for Imbalanced Data","abstract":"Financial fraud detection poses a typical challenge characterized by class imbalance, where instances of fraud are extremely rare but can lead to unpredictable economic losses if misidentified. Precisely classifying these critical minority samples represents a challenging task within the classification. The primary difficulty arises from mainstream classifiers, which often exhibit \"implicit discrimination\" against minority samples in evaluation metrics, which results in frequent misclassifications, and the key to the problem lies in the overlap of feature spaces between majority and minority samples. To address these challenges, oversampling is a feasible solution, yet current classical oversampling methods often lack the necessary caution in sample selection, exacerbating feature space overlap. In response, we propose a novel classification oversampling approach based on the decision boundary and sample proximity relationships. This method carefully considers the distance between critical samples and the decision hyperplane, as well as the density of surrounding samples, resulting in an adaptive oversampling strategy in the kernel space. Finally, we test the proposed method on a classic financial fraud dataset, and the results show that our proposed method provides an effective and robust solution that can improve the classification accuracy of minorities.","sentences":["Financial fraud detection poses a typical challenge characterized by class imbalance, where instances of fraud are extremely rare but can lead to unpredictable economic losses if misidentified.","Precisely classifying these critical minority samples represents a challenging task within the classification.","The primary difficulty arises from mainstream classifiers, which often exhibit \"implicit discrimination\" against minority samples in evaluation metrics, which results in frequent misclassifications, and the key to the problem lies in the overlap of feature spaces between majority and minority samples.","To address these challenges, oversampling is a feasible solution, yet current classical oversampling methods often lack the necessary caution in sample selection, exacerbating feature space overlap.","In response, we propose a novel classification oversampling approach based on the decision boundary and sample proximity relationships.","This method carefully considers the distance between critical samples and the decision hyperplane, as well as the density of surrounding samples, resulting in an adaptive oversampling strategy in the kernel space.","Finally, we test the proposed method on a classic financial fraud dataset, and the results show that our proposed method provides an effective and robust solution that can improve the classification accuracy of minorities."],"url":"http://arxiv.org/abs/2402.08202v1","category":"cs.LG"}
{"created":"2024-02-13 03:51:47","title":"A=3 (e,e') $x_B \\geq 1$ cross-section ratios and the isospin structure of short-range correlations","abstract":"We study the relation between measured high-$x_B$, high-$Q^2$, Helium-3 to Tritium, $(e,e')$ inclusive-scattering cross-section ratios and the relative abundance of high-momentum neutron-proton ($np$) and proton-proton ($pp$) short-range correlated (SRC) nucleon pairs in three-body ($A=3$) nuclei. Analysis of this data using a simple pair-counting cross-section model suggested a much smaller $np/pp$ ratio than previously measured in heavier nuclei, questioning our understanding of $A=3$ nuclei and, by extension, all other nuclei. Here we examine this finding using spectral-function-based cross-section calculations, with both an \\textit{ab initio} $A=3$ spectral function and effective Generalized Contact Formalism (GCF) spectral functions using different nucleon-nucleon interaction models. The \\textit{ab initio} calculation agrees with the data, showing good understanding of the structure of $A=3$ nuclei. An 8\\% uncertainty on the simple pair-counting model, as implied by the difference between it and the \\textit{ab initio} calculation, gives a factor of 5 uncertainty in the extracted $np/pp$ ratio. Thus we see no evidence for the claimed ``unexpected structure in the high-momentum wavefunction for hydrogen-3 and helium-3''.","sentences":["We study the relation between measured high-$x_B$, high-$Q^2$, Helium-3 to Tritium, $(e,e')$ inclusive-scattering cross-section ratios and the relative abundance of high-momentum neutron-proton ($np$) and proton-proton ($pp$) short-range correlated (SRC) nucleon pairs in three-body ($A=3$) nuclei.","Analysis of this data using a simple pair-counting cross-section model suggested a much smaller $np/pp$ ratio than previously measured in heavier nuclei, questioning our understanding of $A=3$ nuclei and, by extension, all other nuclei.","Here we examine this finding using spectral-function-based cross-section calculations, with both an \\textit{ab initio} $A=3$ spectral function and effective Generalized Contact Formalism (GCF) spectral functions using different nucleon-nucleon interaction models.","The \\textit{ab initio} calculation agrees with the data, showing good understanding of the structure of $A=3$ nuclei.","An 8\\% uncertainty on the simple pair-counting model, as implied by the difference between it and the \\textit{ab initio} calculation, gives a factor of 5 uncertainty in the extracted $np/pp$ ratio.","Thus we see no evidence for the claimed ``unexpected structure in the high-momentum wavefunction for hydrogen-3 and helium-3''."],"url":"http://arxiv.org/abs/2402.08199v1","category":"nucl-th"}
{"created":"2024-02-13 02:41:56","title":"Variational Continual Test-Time Adaptation","abstract":"The prior drift is crucial in Continual Test-Time Adaptation (CTTA) methods that only use unlabeled test data, as it can cause significant error propagation. In this paper, we introduce VCoTTA, a variational Bayesian approach to measure uncertainties in CTTA. At the source stage, we transform a pre-trained deterministic model into a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting uncertainties into the model. During the testing time, we employ a mean-teacher update strategy using variational inference for the student model and exponential moving average for the teacher model. Our novel approach updates the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating prior drift within the CTTA framework.","sentences":["The prior drift is crucial in Continual Test-Time Adaptation (CTTA) methods that only use unlabeled test data, as it can cause significant error propagation.","In this paper, we introduce VCoTTA, a variational Bayesian approach to measure uncertainties in CTTA.","At the source stage, we transform a pre-trained deterministic model into a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting uncertainties into the model.","During the testing time, we employ a mean-teacher update strategy using variational inference for the student model and exponential moving average for the teacher model.","Our novel approach updates the student model by combining priors from both the source and teacher models.","The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture.","Experimental results on three datasets demonstrate the method's effectiveness in mitigating prior drift within the CTTA framework."],"url":"http://arxiv.org/abs/2402.08182v1","category":"cs.LG"}
{"created":"2024-02-13 02:07:15","title":"A Projection-Based Time-Divided Reduced Order Model for Fluid-Structure Interactions","abstract":"In this paper, a type of novel projection-based, time-divided reduced order model (ROM) is proposed for dynamic fluid-structure interaction (FSI) problems, where spatial and temporal dimensions are partitioned as follows: spatially, each kind of variable is separated from others in terms of its attribution (fluid/structure), its category (velocity/pressure) and its component (horizontal/vertical); temporally, basis functions are deliberately adopted in small time windows tailored through extensive numerical trials. By the combination of space and time decompositions, the proposed ROM enables prolonged simulations under prescribed accuracy thresholds. Numerical experiments are carried out by means of a numerical comparison between the proposed ROM and corresponding full-order model (FOM) on solving a benchmark problem of FSI with a vibrating elastic beam in the fluid flow, where the representation of basis function sets on perturbation parameters is investigated as well. Extensive numerical results demonstrate the accuracy and efficiency of the proposed ROM. The developed numerical techniques are dimension-independent, which can be seamlessly extended to high dimensional FSI problems.","sentences":["In this paper, a type of novel projection-based, time-divided reduced order model (ROM) is proposed for dynamic fluid-structure interaction (FSI) problems, where spatial and temporal dimensions are partitioned as follows: spatially, each kind of variable is separated from others in terms of its attribution (fluid/structure), its category (velocity/pressure) and its component (horizontal/vertical); temporally, basis functions are deliberately adopted in small time windows tailored through extensive numerical trials.","By the combination of space and time decompositions, the proposed ROM enables prolonged simulations under prescribed accuracy thresholds.","Numerical experiments are carried out by means of a numerical comparison between the proposed ROM and corresponding full-order model (FOM) on solving a benchmark problem of FSI with a vibrating elastic beam in the fluid flow, where the representation of basis function sets on perturbation parameters is investigated as well.","Extensive numerical results demonstrate the accuracy and efficiency of the proposed ROM.","The developed numerical techniques are dimension-independent, which can be seamlessly extended to high dimensional FSI problems."],"url":"http://arxiv.org/abs/2402.08172v1","category":"cs.CE"}
{"created":"2024-02-13 02:07:03","title":"Epistemic Power, Objectivity and Gender in AI Ethics Labor: Legitimizing Located Complaints","abstract":"What counts as legitimate AI ethics labor, and consequently, what are the epistemic terms on which AI ethics claims are rendered legitimate? Based on 75 interviews with technologists including researchers, developers, open source contributors, artists, and activists, this paper explores various epistemic bases from which AI ethics is practiced. In the context of outside attacks on AI ethics as an impediment to \"progress,\" I show how some AI ethics practices have reached toward scholarly authority, automation and quantification and achieved some legitimacy, while those based on richly embodied and situated lived experience have not. This paper draws the works of feminist Anthropology and Science and Technology Studies (STS) scholars Diana Forsythe and Lucy Suchman together with the works of postcolonial feminist theorist Sara Ahmed and Black feminist theorist Kristie Dotson to examine the implications of dominant AI ethics practices. I argue that by entrenching the epistemic power of quantification, dominant AI ethics practices risk legitimizing AI ethics as a project in equal and opposite measure to the extent that they delegitimize and marginalize embodied and lived experiences as legitimate parts of the same project. In response, I propose and sketch the idea of humble technical practices: quantified or technical practices which specifically seek to make their epistemic limits clear, with a view to flattening hierarchies of epistemic power.","sentences":["What counts as legitimate AI ethics labor, and consequently, what are the epistemic terms on which AI ethics claims are rendered legitimate?","Based on 75 interviews with technologists including researchers, developers, open source contributors, artists, and activists, this paper explores various epistemic bases from which AI ethics is practiced.","In the context of outside attacks on AI ethics as an impediment to \"progress,\" I show how some AI ethics practices have reached toward scholarly authority, automation and quantification and achieved some legitimacy, while those based on richly embodied and situated lived experience have not.","This paper draws the works of feminist Anthropology and Science and Technology Studies (STS) scholars Diana Forsythe and Lucy Suchman together with the works of postcolonial feminist theorist Sara Ahmed and Black feminist theorist Kristie Dotson to examine the implications of dominant AI ethics practices.","I argue that by entrenching the epistemic power of quantification, dominant AI ethics practices risk legitimizing AI ethics as a project in equal and opposite measure to the extent that they delegitimize and marginalize embodied and lived experiences as legitimate parts of the same project.","In response, I propose and sketch the idea of humble technical practices: quantified or technical practices which specifically seek to make their epistemic limits clear, with a view to flattening hierarchies of epistemic power."],"url":"http://arxiv.org/abs/2402.08171v1","category":"cs.CY"}
{"created":"2024-02-13 01:53:48","title":"On convertibility among bipartite 2x2 entangled states","abstract":"Some progress is reported on conditions for convertibility among bipartite 2x2 entangled states: An inconvertibility condition related to the rank of an entangled state is given that it is impossible to convert to an entangled state with lower rank under separable operations; a particular set of local operations and classical communication (LOCC) is used to analyze convertibility of three subclasses of states - Werner states, Bell diagonal states and maximally entangled mixed states (MEMS). It is conjectured that MEMS may lie on the bottom of entangled state ordering for given entanglement of formation. A plausible way is suggested of systematically calculating convertibility in a general subclass of bipartite states whose density matrices are defined to be diagonal in a common basis. The set of LOCC adopted in this work is argued to be generalizable to provide sufficient conditions for convertibility among a large range of general 2x2 entangled states.","sentences":["Some progress is reported on conditions for convertibility among bipartite 2x2 entangled states: An inconvertibility condition related to the rank of an entangled state is given that it is impossible to convert to an entangled state with lower rank under separable operations; a particular set of local operations and classical communication (LOCC) is used to analyze convertibility of three subclasses of states - Werner states, Bell diagonal states and maximally entangled mixed states (MEMS).","It is conjectured that MEMS may lie on the bottom of entangled state ordering for given entanglement of formation.","A plausible way is suggested of systematically calculating convertibility in a general subclass of bipartite states whose density matrices are defined to be diagonal in a common basis.","The set of LOCC adopted in this work is argued to be generalizable to provide sufficient conditions for convertibility among a large range of general 2x2 entangled states."],"url":"http://arxiv.org/abs/2402.08166v1","category":"quant-ph"}
{"created":"2024-02-13 01:43:34","title":"A Search for Magnetized Quark Nuggets (MQNs), a Candidate for Dark Matter, Accumulating in Iron Ore","abstract":"A search has been carried out for Magnetized Quark Nuggets (MQNs) accumulating in iron ore over geologic time. MQNs, which are theoretically consistent with the Standard Models of Physics and of Cosmology, have been suggested as dark-matter candidates. Indirect evidence of MQNs has been previously inferred from observations of magnetars and of non-meteorite impact craters. It is shown in this paper that MQNs can accumulate in taconite (iron ore) and be transferred into ferromagnetic rod-mill liners during processing of the ore. When the liners are recycled to make fresh steel, they are heated to higher than the Curie temperature so that their ferromagnetic properties are destroyed. The MQNs would then be released and fall into the ferromagnetic furnace bottom where they would be trapped. Three such furnace bottoms have been magnetically scanned to search for the magnetic anomalies consistent with trapped MQNs. The observed magnetic anomalies are equivalent to an accumulation rate of ~1 kg of MQNs per 1.2 x $10^8$ kg of taconite ore processed. The results are consistent with MQNs but there could be other, unknown explanations. We propose an experiment and calculations to definitively test the MQN hypothesis for dark matter.","sentences":["A search has been carried out for Magnetized Quark Nuggets (MQNs) accumulating in iron ore over geologic time.","MQNs, which are theoretically consistent with the Standard Models of Physics and of Cosmology, have been suggested as dark-matter candidates.","Indirect evidence of MQNs has been previously inferred from observations of magnetars and of non-meteorite impact craters.","It is shown in this paper that MQNs can accumulate in taconite (iron ore) and be transferred into ferromagnetic rod-mill liners during processing of the ore.","When the liners are recycled to make fresh steel, they are heated to higher than the Curie temperature so that their ferromagnetic properties are destroyed.","The MQNs would then be released and fall into the ferromagnetic furnace bottom where they would be trapped.","Three such furnace bottoms have been magnetically scanned to search for the magnetic anomalies consistent with trapped MQNs.","The observed magnetic anomalies are equivalent to an accumulation rate of ~1 kg of MQNs per 1.2 x $10^8$ kg of taconite ore processed.","The results are consistent with MQNs but there could be other, unknown explanations.","We propose an experiment and calculations to definitively test the MQN hypothesis for dark matter."],"url":"http://arxiv.org/abs/2402.08163v1","category":"astro-ph.CO"}
{"created":"2024-02-13 01:41:38","title":"Quiver Heisenberg algebras: a cubic analogue of preprojective algebras","abstract":"In this paper we study a certain class of central extensions of preprojective algebras of quivers under the name quiver Heisenberg algebras (QHA). There are several classes of algebras introduced before by different researchers from different view points, which have the QHA as a special case. While these have mainly been studied in characteristic zero, we also study the case of positive characteristic. Our results show that the QHA is closely related to the representation theory of the corresponding path algebra in a similar way to the preprojective algebra.   Among other things, one of our main results is that the QHA provides an exact sequence of bimodules over the path algebra of a quiver, which can be called the universal Auslander-Reiten sequence. Moreover, we show that the QHA provides minimal left and right approximations with respect to the powers of the radical functor. Consequently, we obtain a description of the QHA as a module over the path algebra, which in the Dynkin case, gives a categorification (as well as a generalization to the positive characteristic case) of the dimension formula by Etingof-Rains.","sentences":["In this paper we study a certain class of central extensions of preprojective algebras of quivers under the name quiver Heisenberg algebras (QHA).","There are several classes of algebras introduced before by different researchers from different view points, which have the QHA as a special case.","While these have mainly been studied in characteristic zero, we also study the case of positive characteristic.","Our results show that the QHA is closely related to the representation theory of the corresponding path algebra in a similar way to the preprojective algebra.   ","Among other things, one of our main results is that the QHA provides an exact sequence of bimodules over the path algebra of a quiver, which can be called the universal Auslander-Reiten sequence.","Moreover, we show that the QHA provides minimal left and right approximations with respect to the powers of the radical functor.","Consequently, we obtain a description of the QHA as a module over the path algebra, which in the Dynkin case, gives a categorification (as well as a generalization to the positive characteristic case) of the dimension formula by Etingof-Rains."],"url":"http://arxiv.org/abs/2402.08162v1","category":"math.RT"}
{"created":"2024-02-12 23:50:47","title":"Efficient Contextual Bandits with Uninformed Feedback Graphs","abstract":"Bandits with feedback graphs are powerful online learning models that interpolate between the full information and classic bandit problems, capturing many real-life applications. A recent work by Zhang et al. (2023) studies the contextual version of this problem and proposes an efficient and optimal algorithm via a reduction to online regression. However, their algorithm crucially relies on seeing the feedback graph before making each decision, while in many applications, the feedback graph is uninformed, meaning that it is either only revealed after the learner makes her decision or even never fully revealed at all. This work develops the first contextual algorithm for such uninformed settings, via an efficient reduction to online regression over both the losses and the graphs. Importantly, we show that it is critical to learn the graphs using log loss instead of squared loss to obtain favorable regret guarantees. We also demonstrate the empirical effectiveness of our algorithm on a bidding application using both synthetic and real-world data.","sentences":["Bandits with feedback graphs are powerful online learning models that interpolate between the full information and classic bandit problems, capturing many real-life applications.","A recent work by Zhang et al. (2023) studies the contextual version of this problem and proposes an efficient and optimal algorithm via a reduction to online regression.","However, their algorithm crucially relies on seeing the feedback graph before making each decision, while in many applications, the feedback graph is uninformed, meaning that it is either only revealed after the learner makes her decision or even never fully revealed at all.","This work develops the first contextual algorithm for such uninformed settings, via an efficient reduction to online regression over both the losses and the graphs.","Importantly, we show that it is critical to learn the graphs using log loss instead of squared loss to obtain favorable regret guarantees.","We also demonstrate the empirical effectiveness of our algorithm on a bidding application using both synthetic and real-world data."],"url":"http://arxiv.org/abs/2402.08127v1","category":"cs.LG"}
{"created":"2024-02-12 23:00:46","title":"Estimating Lagged (Cross-)Covariance Operators of $L^p$-$m$-approximable Processes in Cartesian Product Hilbert Spaces","abstract":"When estimating the parameters in functional ARMA, GARCH and invertible, linear processes, covariance and lagged cross-covariance operators of processes in Cartesian product spaces appear. Such operators have been consistenly estimated in recent years, either less generally or under a strong condition. This article extends the existing literature by deriving explicit upper bounds for estimation errors for lagged covariance and lagged cross-covariance operators of processes in general Cartesian product Hilbert spaces, based on the mild weak dependence condition $L^p$-$m$-approximability. The upper bounds are stated for each lag, Cartesian power(s) and sample size, where the two processes in the context of lagged cross-covariance operators can take values in different spaces. General consequences of our results are also mentioned.","sentences":["When estimating the parameters in functional ARMA, GARCH and invertible, linear processes, covariance and lagged cross-covariance operators of processes in Cartesian product spaces appear.","Such operators have been consistenly estimated in recent years, either less generally or under a strong condition.","This article extends the existing literature by deriving explicit upper bounds for estimation errors for lagged covariance and lagged cross-covariance operators of processes in general Cartesian product Hilbert spaces, based on the mild weak dependence condition $L^p$-$m$-approximability.","The upper bounds are stated for each lag, Cartesian power(s) and sample size, where the two processes in the context of lagged cross-covariance operators can take values in different spaces.","General consequences of our results are also mentioned."],"url":"http://arxiv.org/abs/2402.08110v1","category":"math.ST"}
{"created":"2024-02-12 22:52:32","title":"Mirror Descent-Ascent for mean-field min-max problems","abstract":"We study two variants of the mirror descent-ascent algorithm for solving min-max problems on the space of measures: simultaneous and sequential. We work under assumptions of convexity-concavity and relative smoothness of the payoff function with respect to a suitable Bregman divergence, defined on the space of measures via flat derivatives. We show that the convergence rates to mixed Nash equilibria, measured in the Nikaid\\`o-Isoda error, are of order $\\mathcal{O}\\left(N^{-1/2}\\right)$ and $\\mathcal{O}\\left(N^{-2/3}\\right)$ for the simultaneous and sequential schemes, respectively, which is in line with the state-of-the-art results for related finite-dimensional algorithms.","sentences":["We study two variants of the mirror descent-ascent algorithm for solving min-max problems on the space of measures: simultaneous and sequential.","We work under assumptions of convexity-concavity and relative smoothness of the payoff function with respect to a suitable Bregman divergence, defined on the space of measures via flat derivatives.","We show that the convergence rates to mixed Nash equilibria, measured in the Nikaid\\`o-Isoda error, are of order $\\mathcal{O}\\left(N^{-1/2}\\right)$ and $\\mathcal{O}\\left(N^{-2/3}\\right)$ for the simultaneous and sequential schemes, respectively, which is in line with the state-of-the-art results for related finite-dimensional algorithms."],"url":"http://arxiv.org/abs/2402.08106v1","category":"math.OC"}
{"created":"2024-02-12 22:26:52","title":"Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization","abstract":"Diffusion models have achieved huge empirical success in data generation tasks. Recently, some efforts have been made to adapt the framework of diffusion models to discrete state space, providing a more natural approach for modeling intrinsically discrete data, such as language and graphs. This is achieved by formulating both the forward noising process and the corresponding reversed process as Continuous Time Markov Chains (CTMCs). In this paper, we investigate the theoretical properties of the discrete diffusion model. Specifically, we introduce an algorithm leveraging the uniformization of continuous Markov chains, implementing transitions on random time points. Under reasonable assumptions on the learning of the discrete score function, we derive Total Variation distance and KL divergence guarantees for sampling from any distribution on a hypercube. Our results align with state-of-the-art achievements for diffusion models in $\\mathbb{R}^d$ and further underscore the advantages of discrete diffusion models in comparison to the $\\mathbb{R}^d$ setting.","sentences":["Diffusion models have achieved huge empirical success in data generation tasks.","Recently, some efforts have been made to adapt the framework of diffusion models to discrete state space, providing a more natural approach for modeling intrinsically discrete data, such as language and graphs.","This is achieved by formulating both the forward noising process and the corresponding reversed process as Continuous Time Markov Chains (CTMCs).","In this paper, we investigate the theoretical properties of the discrete diffusion model.","Specifically, we introduce an algorithm leveraging the uniformization of continuous Markov chains, implementing transitions on random time points.","Under reasonable assumptions on the learning of the discrete score function, we derive Total Variation distance and KL divergence guarantees for sampling from any distribution on a hypercube.","Our results align with state-of-the-art achievements for diffusion models in $\\mathbb{R}^d$ and further underscore the advantages of discrete diffusion models in comparison to the $\\mathbb{R}^d$ setting."],"url":"http://arxiv.org/abs/2402.08095v1","category":"stat.ML"}
{"created":"2024-02-12 22:04:04","title":"CycPUF: Cyclic Physical Unclonable Function","abstract":"Physical Unclonable Functions (PUFs) leverage manufacturing process imperfections that cause propagation delay discrepancies for the signals traveling along these paths. While PUFs can be used for device authentication and chip-specific key generation, strong PUFs have been shown to be vulnerable to machine learning modeling attacks. Although there is an impression that combinational circuits must be designed without any loops, cyclic combinational circuits have been shown to increase design security against hardware intellectual property theft. In this paper, we introduce feedback signals into traditional delay-based PUF designs such as arbiter PUF, ring oscillator PUF, and butterfly PUF to give them a wider range of possible output behaviors and thus an edge against modeling attacks. Based on our analysis, cyclic PUFs produce responses that can be binary, steady-state, oscillating, or pseudo-random under fixed challenges. The proposed cyclic PUFs are implemented in field programmable gate arrays, and their power and area overhead, in addition to functional metrics, are reported compared with their traditional counterparts. The security gain of the proposed cyclic PUFs is also shown against state-of-the-art attacks.","sentences":["Physical Unclonable Functions (PUFs) leverage manufacturing process imperfections that cause propagation delay discrepancies for the signals traveling along these paths.","While PUFs can be used for device authentication and chip-specific key generation, strong PUFs have been shown to be vulnerable to machine learning modeling attacks.","Although there is an impression that combinational circuits must be designed without any loops, cyclic combinational circuits have been shown to increase design security against hardware intellectual property theft.","In this paper, we introduce feedback signals into traditional delay-based PUF designs such as arbiter PUF, ring oscillator PUF, and butterfly PUF to give them a wider range of possible output behaviors and thus an edge against modeling attacks.","Based on our analysis, cyclic PUFs produce responses that can be binary, steady-state, oscillating, or pseudo-random under fixed challenges.","The proposed cyclic PUFs are implemented in field programmable gate arrays, and their power and area overhead, in addition to functional metrics, are reported compared with their traditional counterparts.","The security gain of the proposed cyclic PUFs is also shown against state-of-the-art attacks."],"url":"http://arxiv.org/abs/2402.08084v1","category":"cs.CR"}
{"created":"2024-02-12 21:30:12","title":"Interrater agreement statistics under the two-rater dichotomous-response case with correlated decisions","abstract":"Measurement of the interrater agreement (IRA) is critical in various disciplines. To correct for potential confounding chance agreement in IRA, Cohen's kappa and many other methods have been proposed. However, owing to the varied strategies and assumptions across these methods, there is a lack of practical guidelines on how these methods should be preferred even for the common two-rater dichotomous rating. To fill the gaps in the literature, we systematically review nine IRA methods and propose a generalized framework that can simulate the correlated decision processes behind the two raters to compare those reviewed methods under comprehensive practical scenarios. Based on the new framework, an estimand of \"true\" chance-corrected IRA is defined by accounting for the \"probabilistic certainty\" and serves as the comparison benchmark. We carry out extensive simulations to evaluate the performance of the reviewed IRA measures, and an agglomerative hierarchical clustering analysis is conducted to assess the inter-relationships among the included methods and the benchmark metric. Recommendations for selecting appropriate IRA statistics in different practical conditions are provided and the needs for further advancements in IRA estimation methodologies are emphasized.","sentences":["Measurement of the interrater agreement (IRA) is critical in various disciplines.","To correct for potential confounding chance agreement in IRA, Cohen's kappa and many other methods have been proposed.","However, owing to the varied strategies and assumptions across these methods, there is a lack of practical guidelines on how these methods should be preferred even for the common two-rater dichotomous rating.","To fill the gaps in the literature, we systematically review nine IRA methods and propose a generalized framework that can simulate the correlated decision processes behind the two raters to compare those reviewed methods under comprehensive practical scenarios.","Based on the new framework, an estimand of \"true\" chance-corrected IRA is defined by accounting for the \"probabilistic certainty\" and serves as the comparison benchmark.","We carry out extensive simulations to evaluate the performance of the reviewed IRA measures, and an agglomerative hierarchical clustering analysis is conducted to assess the inter-relationships among the included methods and the benchmark metric.","Recommendations for selecting appropriate IRA statistics in different practical conditions are provided and the needs for further advancements in IRA estimation methodologies are emphasized."],"url":"http://arxiv.org/abs/2402.08069v1","category":"stat.ME"}
{"created":"2024-02-12 21:27:32","title":"The Blocklace: A Universal, Byzantine Fault-Tolerant, Conflict-free Replicated Data Type","abstract":"Conflict-free Replicated Data Types (CRDTs) are designed for replica convergence without global coordination or consensus. Recent work has achieves the same in a Byzantine environment, through DAG-like structures based on cryptographic hashes of content.   The blocklace is a partially-ordered generalization of the blockchain in which each block has any finite number of signed hash pointers to preceding blocks. We show that the blocklace datatype, with the sole operation of adding a single block, is a CRDT: it is both a pure operation-based CRDT, with self-tagging; and a delta-state CRDT, under a slight generalization of the delta framework. Allowing arbitrary values as payload, the blocklace can also be seen as a universal Byzantine fault-tolerant implementation for arbitrary CRDTs, under the operation-based approach.   Current approaches only care about CRDT convergence, being equivocation-tolerant (they do not detect or prevent equivocations), allowing a Byzantine node to cause an arbitrary amount of harm by polluting the CRDT state with an infinite number of equivocations.   We show that a blocklace can be used not only in an equivocation-tolerant way, but also so as to detect and eventually exclude Byzantine behavior, namely equivocations, even under the presence of collusion. The blocklace CRDT protocol ensures that the Byzantine nodes may harm only a finite prefix of the computation.","sentences":["Conflict-free Replicated Data Types (CRDTs) are designed for replica convergence without global coordination or consensus.","Recent work has achieves the same in a Byzantine environment, through DAG-like structures based on cryptographic hashes of content.   ","The blocklace is a partially-ordered generalization of the blockchain in which each block has any finite number of signed hash pointers to preceding blocks.","We show that the blocklace datatype, with the sole operation of adding a single block, is a CRDT: it is both a pure operation-based CRDT, with self-tagging; and a delta-state CRDT, under a slight generalization of the delta framework.","Allowing arbitrary values as payload, the blocklace can also be seen as a universal Byzantine fault-tolerant implementation for arbitrary CRDTs, under the operation-based approach.   ","Current approaches only care about CRDT convergence, being equivocation-tolerant (they do not detect or prevent equivocations), allowing a Byzantine node to cause an arbitrary amount of harm by polluting the CRDT state with an infinite number of equivocations.   ","We show that a blocklace can be used not only in an equivocation-tolerant way, but also so as to detect and eventually exclude Byzantine behavior, namely equivocations, even under the presence of collusion.","The blocklace CRDT protocol ensures that the Byzantine nodes may harm only a finite prefix of the computation."],"url":"http://arxiv.org/abs/2402.08068v1","category":"cs.DC"}
{"created":"2024-02-12 21:03:20","title":"Information gain and measurement disturbance for quantum agents","abstract":"The traditional formalism of quantum measurement (hereafter ``TQM'') describes processes where some properties of quantum states are extracted and stored as classical information. While TQM is a natural and appropriate description of how humans interact with quantum systems, it is silent on the question of how a more general, quantum, agent would do so. How do we describe the observation of a system by an observer with the ability to store not only classical information but quantum states in its memory? In this paper, we extend the idea of measurement to a more general class of sensors for quantum agents which interact with a system in such a way that the agent's memory stores information (classical or quantum) about the system under study. For appropriate sensory interactions, the quantum agent may ``learn'' more about the system than would be possible under any set of classical measurements -- but as we show, this comes at the cost of additional measurement disturbance. We experimentally demonstrate such a system and characterize the tradeoffs, which can be done by considering the information required to erase the effects of a measurement.","sentences":["The traditional formalism of quantum measurement (hereafter ``TQM'') describes processes where some properties of quantum states are extracted and stored as classical information.","While TQM is a natural and appropriate description of how humans interact with quantum systems, it is silent on the question of how a more general, quantum, agent would do so.","How do we describe the observation of a system by an observer with the ability to store not only classical information but quantum states in its memory?","In this paper, we extend the idea of measurement to a more general class of sensors for quantum agents which interact with a system in such a way that the agent's memory stores information (classical or quantum) about the system under study.","For appropriate sensory interactions, the quantum agent may ``learn'' more about the system than would be possible under any set of classical measurements -- but as we show, this comes at the cost of additional measurement disturbance.","We experimentally demonstrate such a system and characterize the tradeoffs, which can be done by considering the information required to erase the effects of a measurement."],"url":"http://arxiv.org/abs/2402.08060v1","category":"quant-ph"}
{"created":"2024-02-12 20:46:47","title":"MIML library: a Modular and Flexible Library for Multi-instance Multi-label Learning","abstract":"MIML library is a Java software tool to develop, test, and compare classification algorithms for multi-instance multi-label (MIML) learning. The library includes 43 algorithms and provides a specific format and facilities for data managing and partitioning, holdout and cross-validation methods, standard metrics for performance evaluation, and generation of reports. In addition, algorithms can be executed through $xml$ configuration files without needing to program. It is platform-independent, extensible, free, open-source, and available on GitHub under the GNU General Public License.","sentences":["MIML library is a Java software tool to develop, test, and compare classification algorithms for multi-instance multi-label (MIML) learning.","The library includes 43 algorithms and provides a specific format and facilities for data managing and partitioning, holdout and cross-validation methods, standard metrics for performance evaluation, and generation of reports.","In addition, algorithms can be executed through $xml$ configuration files without needing to program.","It is platform-independent, extensible, free, open-source, and available on GitHub under the GNU General Public License."],"url":"http://arxiv.org/abs/2402.08056v1","category":"cs.LG"}
{"created":"2024-02-12 20:44:46","title":"A Quantum Algorithm Based Heuristic to Hide Sensitive Itemsets","abstract":"Quantum devices use qubits to represent information, which allows them to exploit important properties from quantum physics, specifically superposition and entanglement. As a result, quantum computers have the potential to outperform the most advanced classical computers. In recent years, quantum algorithms have shown hints of this promise, and many algorithms have been proposed for the quantum domain. There are two key hurdles to solving difficult real-world problems on quantum computers. The first is on the hardware front -- the number of qubits in the most advanced quantum systems is too small to make the solution of large problems practical. The second involves the algorithms themselves -- as quantum computers use qubits, the algorithms that work there are fundamentally different from those that work on traditional computers. As a result of these constraints, research has focused on developing approaches to solve small versions of problems as proofs of concept -- recognizing that it would be possible to scale these up once quantum devices with enough qubits become available. Our objective in this paper is along the same lines. We present a quantum approach to solve a well-studied problem in the context of data sharing. This heuristic uses the well-known Quantum Approximate Optimization Algorithm (QAOA). We present results on experiments involving small datasets to illustrate how the problem could be solved using quantum algorithms. The results show that the method has potential and provide answers close to optimal. At the same time, we realize there are opportunities for improving the method further.","sentences":["Quantum devices use qubits to represent information, which allows them to exploit important properties from quantum physics, specifically superposition and entanglement.","As a result, quantum computers have the potential to outperform the most advanced classical computers.","In recent years, quantum algorithms have shown hints of this promise, and many algorithms have been proposed for the quantum domain.","There are two key hurdles to solving difficult real-world problems on quantum computers.","The first is on the hardware front -- the number of qubits in the most advanced quantum systems is too small to make the solution of large problems practical.","The second involves the algorithms themselves -- as quantum computers use qubits, the algorithms that work there are fundamentally different from those that work on traditional computers.","As a result of these constraints, research has focused on developing approaches to solve small versions of problems as proofs of concept -- recognizing that it would be possible to scale these up once quantum devices with enough qubits become available.","Our objective in this paper is along the same lines.","We present a quantum approach to solve a well-studied problem in the context of data sharing.","This heuristic uses the well-known Quantum Approximate Optimization Algorithm (QAOA).","We present results on experiments involving small datasets to illustrate how the problem could be solved using quantum algorithms.","The results show that the method has potential and provide answers close to optimal.","At the same time, we realize there are opportunities for improving the method further."],"url":"http://arxiv.org/abs/2402.08055v1","category":"quant-ph"}
{"created":"2024-02-12 20:43:39","title":"Probing the interaction energy of two $^{85}$Rb atoms in an optical tweezer via spin-motion coupling","abstract":"The inherent polarization gradients in tight optical tweezers can be used to couple the atomic spins to the two-body motion under the action of a microwave spin-flip transition, so that such a spin-motion coupling offers an important control knob on the motional states of optically trapped two colliding atoms. Here, after preparing two elastically scattering $^{85}$Rb atoms in the three-dimensional ground-state in the optical tweezer, we employed this control in order to probe the colliding energies of elastic and inelastic channels. The combination of microwave spectra and corresponding s-wave pseudopotential model allows us to infer the effect of the state-dependent trapping potentials on the elastics colliding energies, as well as to reveal how the presence of inelastic interactions affects elastic part of the relative potential. Our work shows that the spin-motion coupling in a tight optical tweezer expand the experimental toolbox for fundamental studies of ultracold collisions in the two body systems with reactive collisions, and potentially for that of more complex interactions, such as optically trapped atom-molecule and molecule-molecule interactions.","sentences":["The inherent polarization gradients in tight optical tweezers can be used to couple the atomic spins to the two-body motion under the action of a microwave spin-flip transition, so that such a spin-motion coupling offers an important control knob on the motional states of optically trapped two colliding atoms.","Here, after preparing two elastically scattering $^{85}$Rb atoms in the three-dimensional ground-state in the optical tweezer, we employed this control in order to probe the colliding energies of elastic and inelastic channels.","The combination of microwave spectra and corresponding s-wave pseudopotential model allows us to infer the effect of the state-dependent trapping potentials on the elastics colliding energies, as well as to reveal how the presence of inelastic interactions affects elastic part of the relative potential.","Our work shows that the spin-motion coupling in a tight optical tweezer expand the experimental toolbox for fundamental studies of ultracold collisions in the two body systems with reactive collisions, and potentially for that of more complex interactions, such as optically trapped atom-molecule and molecule-molecule interactions."],"url":"http://arxiv.org/abs/2402.08054v1","category":"physics.atom-ph"}
{"created":"2024-02-12 20:31:49","title":"Reconstruction of Polarization Properties of Whistler Waves From Two Magnetic and Two Electric Field Components: Application to Parker Solar Probe Measurements","abstract":"The search-coil magnetometer (SCM) aboard Parker Solar Probe (PSP) measures the 3 Hz to 1 MHz magnetic field fluctuations. During Encounter 1, the SCM operated as expected; however, in March 2019, technical issues limited subsequent encounters to two components for frequencies below 1 kHz. Detrimentally, most whistler waves are observed in the affected frequency band where established techniques cannot extract the wave polarization properties under these conditions. Fortunately, the Electric Field Instrument aboard PSP measures two electric field components and covers the affected bandwidth. We propose a technique using the available electromagnetic fields to reconstruct the missing components by neglecting the electric field parallel to the background magnetic field. This technique is applicable with the assumptions of (a) low-frequency whistlers in the plasma frame relative to the electron cyclotron frequency; (b) a small propagation angle with respect to the background magnetic field; and (c) a large wave phase speed relative to the cross-field solar wind velocity. Critically, the method cannot be applied if the background magnetic field is aligned with the affected SCM coil. We have validated our method using burst mode measurements made before March 2019. The reconstruction conditions are satisfied for 80% of the burst mode whistlers detected during Encounter 1. We apply the method to determine the polarization of a whistler event observed after March 2019 during Encounter 2. Our novel method is an encouraging step toward analyzing whistler properties in affected encounters and improving our understanding of wave-particle interactions in the young solar wind.","sentences":["The search-coil magnetometer (SCM) aboard Parker Solar Probe (PSP) measures the 3 Hz to 1 MHz magnetic field fluctuations.","During Encounter 1, the SCM operated as expected; however, in March 2019, technical issues limited subsequent encounters to two components for frequencies below 1 kHz.","Detrimentally, most whistler waves are observed in the affected frequency band where established techniques cannot extract the wave polarization properties under these conditions.","Fortunately, the Electric Field Instrument aboard PSP measures two electric field components and covers the affected bandwidth.","We propose a technique using the available electromagnetic fields to reconstruct the missing components by neglecting the electric field parallel to the background magnetic field.","This technique is applicable with the assumptions of (a) low-frequency whistlers in the plasma frame relative to the electron cyclotron frequency; (b) a small propagation angle with respect to the background magnetic field; and (c) a large wave phase speed relative to the cross-field solar wind velocity.","Critically, the method cannot be applied if the background magnetic field is aligned with the affected SCM coil.","We have validated our method using burst mode measurements made before March 2019.","The reconstruction conditions are satisfied for 80% of the burst mode whistlers detected during Encounter 1.","We apply the method to determine the polarization of a whistler event observed after March 2019 during Encounter 2.","Our novel method is an encouraging step toward analyzing whistler properties in affected encounters and improving our understanding of wave-particle interactions in the young solar wind."],"url":"http://arxiv.org/abs/2402.08050v1","category":"astro-ph.SR"}
{"created":"2024-02-12 19:43:19","title":"On the Stability of Undesirable Equilibria in the Quadratic Program Framework for Safety-Critical Control","abstract":"Control Lyapunov functions (CLFs) and Control Barrier Functions (CBFs) have been used to develop provably safe controllers by means of quadratic programs (QPs). This framework guarantees safety in the form of trajectory invariance with respect to a given set, but it can introduce undesirable equilibrium points to the closed loop system, which can be asymptotically stable. In this work, we present a detailed study of the formation and stability of equilibrium points with the QP framework for a class of nonlinear systems. We introduce the useful concept of compatibility between a CLF and a family of CBFs, regarding the number of stable equilibrium points other than the CLF minimum. Using this concept, we derive a set of compatibility conditions on the parameters of a quadratic CLF and a family of quadratic CBFs that guarantee that all undesirable equilibrium points are not attractive. Furthermore, we propose an extension to the QP-based controller that dynamically modifies the CLF geometry in order to satisfy the compatibility conditions, guaranteeing safety and quasi-global convergence of the system state to the CLF minimum. Numeric simulations illustrate the applicability of the proposed method for safety-critical, deadlock-free robotic navigation tasks.","sentences":["Control Lyapunov functions (CLFs) and Control Barrier Functions (CBFs) have been used to develop provably safe controllers by means of quadratic programs (QPs).","This framework guarantees safety in the form of trajectory invariance with respect to a given set, but it can introduce undesirable equilibrium points to the closed loop system, which can be asymptotically stable.","In this work, we present a detailed study of the formation and stability of equilibrium points with the QP framework for a class of nonlinear systems.","We introduce the useful concept of compatibility between a CLF and a family of CBFs, regarding the number of stable equilibrium points other than the CLF minimum.","Using this concept, we derive a set of compatibility conditions on the parameters of a quadratic CLF and a family of quadratic CBFs that guarantee that all undesirable equilibrium points are not attractive.","Furthermore, we propose an extension to the QP-based controller that dynamically modifies the CLF geometry in order to satisfy the compatibility conditions, guaranteeing safety and quasi-global convergence of the system state to the CLF minimum.","Numeric simulations illustrate the applicability of the proposed method for safety-critical, deadlock-free robotic navigation tasks."],"url":"http://arxiv.org/abs/2402.08027v1","category":"eess.SY"}
{"created":"2024-02-12 19:42:46","title":"The Influence of Laser Focusing Conditions on the Direct Laser Acceleration of Electrons","abstract":"Direct Laser Acceleration (DLA) of electrons during a high-energy, picosecond laser interaction with an underdense plasma has been demonstrated to be substantially enhanced by controlling the laser focusing geometry. Experiments using the OMEGA EP facility measured electrons accelerated to maximum energies exceeding 120 times the ponderomotive energy under certain laser focusing, pulse energy, and plasma density conditions. Two-dimensional particle-in-cell simulations show that the laser focusing conditions alter the laser field evolution, channel fields generation, and electron oscillation, all of which contribute to the final electron energies. The optimal laser focusing condition occurs when the transverse oscillation amplitude of the accelerated electron in the channel fields matches the laser beam width, resulting in efficient energy gain. Through this observation, a simple model was developed to calculate the optimal laser focal spot size in more general conditions and is validated by experimental data.","sentences":["Direct Laser Acceleration (DLA) of electrons during a high-energy, picosecond laser interaction with an underdense plasma has been demonstrated to be substantially enhanced by controlling the laser focusing geometry.","Experiments using the OMEGA EP facility measured electrons accelerated to maximum energies exceeding 120 times the ponderomotive energy under certain laser focusing, pulse energy, and plasma density conditions.","Two-dimensional particle-in-cell simulations show that the laser focusing conditions alter the laser field evolution, channel fields generation, and electron oscillation, all of which contribute to the final electron energies.","The optimal laser focusing condition occurs when the transverse oscillation amplitude of the accelerated electron in the channel fields matches the laser beam width, resulting in efficient energy gain.","Through this observation, a simple model was developed to calculate the optimal laser focal spot size in more general conditions and is validated by experimental data."],"url":"http://arxiv.org/abs/2402.08026v1","category":"physics.plasm-ph"}
{"created":"2024-02-12 19:42:05","title":"Beyond the Mud: Datasets and Benchmarks for Computer Vision in Off-Road Racing","abstract":"Despite significant progress in optical character recognition (OCR) and computer vision systems, robustly recognizing text and identifying people in images taken in unconstrained \\emph{in-the-wild} environments remain an ongoing challenge. However, such obstacles must be overcome in practical applications of vision systems, such as identifying racers in photos taken during off-road racing events. To this end, we introduce two new challenging real-world datasets - the off-road motorcycle Racer Number Dataset (RND) and the Muddy Racer re-iDentification Dataset (MUDD) - to highlight the shortcomings of current methods and drive advances in OCR and person re-identification (ReID) under extreme conditions. These two datasets feature over 6,300 images taken during off-road competitions which exhibit a variety of factors that undermine even modern vision systems, namely mud, complex poses, and motion blur. We establish benchmark performance on both datasets using state-of-the-art models. Off-the-shelf models transfer poorly, reaching only 15% end-to-end (E2E) F1 score on text spotting, and 33% rank-1 accuracy on ReID. Fine-tuning yields major improvements, bringing model performance to 53% F1 score for E2E text spotting and 79% rank-1 accuracy on ReID, but still falls short of good performance. Our analysis exposes open problems in real-world OCR and ReID that necessitate domain-targeted techniques. With these datasets and analysis of model limitations, we aim to foster innovations in handling real-world conditions like mud and complex poses to drive progress in robust computer vision. All data was sourced from PerformancePhoto.co, a website used by professional motorsports photographers, racers, and fans. The top-performing text spotting and ReID models are deployed on this platform to power real-time race photo search.","sentences":["Despite significant progress in optical character recognition (OCR) and computer vision systems, robustly recognizing text and identifying people in images taken in unconstrained \\emph{in-the-wild} environments remain an ongoing challenge.","However, such obstacles must be overcome in practical applications of vision systems, such as identifying racers in photos taken during off-road racing events.","To this end, we introduce two new challenging real-world datasets - the off-road motorcycle Racer Number Dataset (RND) and the Muddy Racer re-iDentification Dataset (MUDD) - to highlight the shortcomings of current methods and drive advances in OCR and person re-identification (ReID) under extreme conditions.","These two datasets feature over 6,300 images taken during off-road competitions which exhibit a variety of factors that undermine even modern vision systems, namely mud, complex poses, and motion blur.","We establish benchmark performance on both datasets using state-of-the-art models.","Off-the-shelf models transfer poorly, reaching only 15% end-to-end (E2E) F1 score on text spotting, and 33% rank-1 accuracy on ReID.","Fine-tuning yields major improvements, bringing model performance to 53% F1 score for E2E text spotting and 79% rank-1 accuracy on ReID, but still falls short of good performance.","Our analysis exposes open problems in real-world OCR and ReID that necessitate domain-targeted techniques.","With these datasets and analysis of model limitations, we aim to foster innovations in handling real-world conditions like mud and complex poses to drive progress in robust computer vision.","All data was sourced from PerformancePhoto.co, a website used by professional motorsports photographers, racers, and fans.","The top-performing text spotting and ReID models are deployed on this platform to power real-time race photo search."],"url":"http://arxiv.org/abs/2402.08025v1","category":"cs.CV"}
{"created":"2024-02-12 19:39:07","title":"Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks","abstract":"Optimizing large-scale wireless networks, including optimal resource management, power allocation, and throughput maximization, is inherently challenging due to their non-observable system dynamics and heterogeneous and complex nature. Herein, a novel ensemble Q-learning algorithm that addresses the performance and complexity challenges of the traditional Q-learning algorithm for optimizing wireless networks is presented. Ensemble learning with synthetic Markov Decision Processes is tailored to wireless networks via new models for approximating large state-space observable wireless networks. In particular, digital cousins are proposed as an extension of the traditional digital twin concept wherein multiple Q-learning algorithms on multiple synthetic Markovian environments are run in parallel and their outputs are fused into a single Q-function. Convergence analyses of key statistics and Q-functions and derivations of upper bounds on the estimation bias and variance are provided. Numerical results across a variety of real-world wireless networks show that the proposed algorithm can achieve up to 50% less average policy error with up to 40% less runtime complexity than the state-of-the-art reinforcement learning algorithms. It is also shown that theoretical results properly predict trends in the experimental results.","sentences":["Optimizing large-scale wireless networks, including optimal resource management, power allocation, and throughput maximization, is inherently challenging due to their non-observable system dynamics and heterogeneous and complex nature.","Herein, a novel ensemble Q-learning algorithm that addresses the performance and complexity challenges of the traditional Q-learning algorithm for optimizing wireless networks is presented.","Ensemble learning with synthetic Markov Decision Processes is tailored to wireless networks via new models for approximating large state-space observable wireless networks.","In particular, digital cousins are proposed as an extension of the traditional digital twin concept wherein multiple Q-learning algorithms on multiple synthetic Markovian environments are run in parallel and their outputs are fused into a single Q-function.","Convergence analyses of key statistics and Q-functions and derivations of upper bounds on the estimation bias and variance are provided.","Numerical results across a variety of real-world wireless networks show that the proposed algorithm can achieve up to 50% less average policy error with up to 40% less runtime complexity than the state-of-the-art reinforcement learning algorithms.","It is also shown that theoretical results properly predict trends in the experimental results."],"url":"http://arxiv.org/abs/2402.08022v1","category":"cs.LG"}
{"created":"2024-02-12 19:19:29","title":"On The Nature Of The Phenotype In Tree Genetic Programming","abstract":"In this contribution, we discuss the basic concepts of genotypes and phenotypes in tree-based GP (TGP), and then analyze their behavior using five benchmark datasets. We show that TGP exhibits the same behavior that we can observe in other GP representations: At the genotypic level trees show frequently unchecked growth with seemingly ineffective code, but on the phenotypic level, much smaller trees can be observed. To generate phenotypes, we provide a unique technique for removing semantically ineffective code from GP trees. The approach extracts considerably simpler phenotypes while not being limited to local operations in the genotype. We generalize this transformation based on a problem-independent parameter that enables a further simplification of the exact phenotype by coarse-graining to produce approximate phenotypes. The concept of these phenotypes (exact and approximate) allows us to clarify what evolved solutions truly predict, making GP models considered at the phenotypic level much better interpretable.","sentences":["In this contribution, we discuss the basic concepts of genotypes and phenotypes in tree-based GP (TGP), and then analyze their behavior using five benchmark datasets.","We show that TGP exhibits the same behavior that we can observe in other GP representations: At the genotypic level trees show frequently unchecked growth with seemingly ineffective code, but on the phenotypic level, much smaller trees can be observed.","To generate phenotypes, we provide a unique technique for removing semantically ineffective code from GP trees.","The approach extracts considerably simpler phenotypes while not being limited to local operations in the genotype.","We generalize this transformation based on a problem-independent parameter that enables a further simplification of the exact phenotype by coarse-graining to produce approximate phenotypes.","The concept of these phenotypes (exact and approximate) allows us to clarify what evolved solutions truly predict, making GP models considered at the phenotypic level much better interpretable."],"url":"http://arxiv.org/abs/2402.08011v1","category":"cs.NE"}
{"created":"2024-02-12 19:10:13","title":"Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs","abstract":"In this paper, we introduce \\emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data. The method involves creating synthetic data using self-critique prompting by a teacher LLM and then utilising a generalized DPO loss function to distil to a student LLM. The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset. rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness against role-playing, and reduced sycophancy. Code to be released at https://github.com/vicgalle/refined-dpo.","sentences":["In this paper, we introduce \\emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data.","The method involves creating synthetic data using self-critique prompting by a teacher LLM and then utilising a generalized DPO loss function to distil to a student LLM.","The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset.","rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness against role-playing, and reduced sycophancy.","Code to be released at https://github.com/vicgalle/refined-dpo."],"url":"http://arxiv.org/abs/2402.08005v1","category":"cs.CL"}
{"created":"2024-02-12 19:04:54","title":"An Automated Catalog of Long Period Variables using Infrared Lightcurves from Palomar Gattini-IR","abstract":"Stars in the Asymptotic Giant Branch (AGB) phase, dominated by low to intermediate-mass stars in the late stage of evolution, undergo periodic pulsations, with periods of several hundred days, earning them the name Long Period Variables (LPVs). These stars gradually shed their mass through stellar winds and mass ejections, enveloping themselves in dust. Infrared (IR) surveys can probe these dust-enshrouded phases and uncover populations of LPV stars in the Milky Way. In this paper, we present a catalog of 159,696 Long Period Variables using near-IR lightcurves from the Palomar Gattini - IR (PGIR) survey. PGIR has been surveying the entire accessible northern sky ($\\delta > -28^{\\circ}$) in the J-band at a cadence of 2-3 days since September 2018, and has produced J-band lightcurves for more than 60 million sources. We used a gradient-boosted decision tree classifier trained on a comprehensive feature set extracted from PGIR lightcurves to search for LPVs in this dataset. We developed a parallelized and optimized code to extract features at a rate of ~0.1 seconds per lightcurve. Our model can successfully distinguish LPVs from other stars with a true positive rate and weighted g-mean of 0.95. 73,346 (~46%) of the sources in our catalog are new, previously unknown LPVs.","sentences":["Stars in the Asymptotic Giant Branch (AGB) phase, dominated by low to intermediate-mass stars in the late stage of evolution, undergo periodic pulsations, with periods of several hundred days, earning them the name Long Period Variables (LPVs).","These stars gradually shed their mass through stellar winds and mass ejections, enveloping themselves in dust.","Infrared (IR) surveys can probe these dust-enshrouded phases and uncover populations of LPV stars in the Milky Way.","In this paper, we present a catalog of 159,696 Long Period Variables using near-IR lightcurves from the Palomar Gattini - IR (PGIR) survey.","PGIR has been surveying the entire accessible northern sky ($\\delta > -28^{\\circ}$) in the J-band at a cadence of 2-3 days since September 2018, and has produced J-band lightcurves for more than 60 million sources.","We used a gradient-boosted decision tree classifier trained on a comprehensive feature set extracted from PGIR lightcurves to search for LPVs in this dataset.","We developed a parallelized and optimized code to extract features at a rate of ~0.1 seconds per lightcurve.","Our model can successfully distinguish LPVs from other stars with a true positive rate and weighted g-mean of 0.95. 73,346 (~46%) of the sources in our catalog are new, previously unknown LPVs."],"url":"http://arxiv.org/abs/2402.08000v1","category":"astro-ph.SR"}
{"created":"2024-02-12 19:04:24","title":"On the linear stability of nonrelativistic selfinteracting boson stars","abstract":"In this paper we study the linear stability of selfinteracting boson stars in the nonrelativistic limit of the Einstein-Klein-Gordon theory. For this purpose, based on a combination of analytic and numerical methods, we determine the behavior of general linear perturbations around the stationary and spherically symmetric solutions of the Gross-Pitaevskii-Poisson system. In particular, we conclude that ground state configurations are linearly stable if the selfinteraction is repulsive, whereas there exist a state of maximum mass that divides the stable and the unstable branches in case the selfinteraction is attractive. Regarding the excited states, they are in general unstable under generic perturbations, although we identify a stability band in the first excited states of the repulsive theory. This result is independent of the mass of the scalar field and the details of the selfinteraction potential, and it is in contrast to the situation of vanishing selfinteraction, in which excited states are always unstable.","sentences":["In this paper we study the linear stability of selfinteracting boson stars in the nonrelativistic limit of the Einstein-Klein-Gordon theory.","For this purpose, based on a combination of analytic and numerical methods, we determine the behavior of general linear perturbations around the stationary and spherically symmetric solutions of the Gross-Pitaevskii-Poisson system.","In particular, we conclude that ground state configurations are linearly stable if the selfinteraction is repulsive, whereas there exist a state of maximum mass that divides the stable and the unstable branches in case the selfinteraction is attractive.","Regarding the excited states, they are in general unstable under generic perturbations, although we identify a stability band in the first excited states of the repulsive theory.","This result is independent of the mass of the scalar field and the details of the selfinteraction potential, and it is in contrast to the situation of vanishing selfinteraction, in which excited states are always unstable."],"url":"http://arxiv.org/abs/2402.07998v1","category":"gr-qc"}
{"created":"2024-02-12 19:02:34","title":"Performance enhancement of electrocatalytic hydrogen evolution through coalescence-induced bubble dynamics","abstract":"The evolution of electrogenerated gas bubbles during water electrolysis can significantly hamper the overall process efficiency. Promoting the departure of electrochemically generated bubbles during (water) electrolysis is therefore beneficial. For a single bubble, a departure from the electrode surface occurs when buoyancy wins over the downward-acting forces (e.g. contact, Marangoni, and electric forces). In this work, the dynamics of a pair of H$_2$ bubbles produced during hydrogen evolution reaction in 0.5 M H$_2$SO$_4$ using dual platinum micro-electrode system is systematically studied by varying the electrode distance and the cathodic potential. By combining high-speed imaging and electrochemical analysis, we demonstrate the importance of bubble-bubble interactions for the departure process. We show that bubble coalescence may lead to substantially earlier bubble departure as compared to buoyancy effects alone, resulting in considerably higher reaction rates at constant potential. However, due to continued mass input and conservation of momentum repeated coalescence events with bubbles close to the electrode may drive departed bubbles back to the surface beyond a critical current, which increases with the electrode spacing. The latter leads to the resumption of bubble growth near the electrode surface, followed by buoyancy-driven departure. While less favourable at small electrode spacing, this configuration proves to be very beneficial at larger separations increasing the mean current up to 2.4 times compared to a single electrode under the conditions explored in this study.","sentences":["The evolution of electrogenerated gas bubbles during water electrolysis can significantly hamper the overall process efficiency.","Promoting the departure of electrochemically generated bubbles during (water) electrolysis is therefore beneficial.","For a single bubble, a departure from the electrode surface occurs when buoyancy wins over the downward-acting forces (e.g. contact, Marangoni, and electric forces).","In this work, the dynamics of a pair of H$_2$ bubbles produced during hydrogen evolution reaction in 0.5 M H$_2$SO$_4$ using dual platinum micro-electrode system is systematically studied by varying the electrode distance and the cathodic potential.","By combining high-speed imaging and electrochemical analysis, we demonstrate the importance of bubble-bubble interactions for the departure process.","We show that bubble coalescence may lead to substantially earlier bubble departure as compared to buoyancy effects alone, resulting in considerably higher reaction rates at constant potential.","However, due to continued mass input and conservation of momentum repeated coalescence events with bubbles close to the electrode may drive departed bubbles back to the surface beyond a critical current, which increases with the electrode spacing.","The latter leads to the resumption of bubble growth near the electrode surface, followed by buoyancy-driven departure.","While less favourable at small electrode spacing, this configuration proves to be very beneficial at larger separations increasing the mean current up to 2.4 times compared to a single electrode under the conditions explored in this study."],"url":"http://arxiv.org/abs/2402.07996v1","category":"physics.flu-dyn"}
{"created":"2024-02-12 19:00:05","title":"Disentangling the Galaxy's Gordian knot: evidence from $APOGEE-Gaia$ for a knotted and slower bar in the Milky Way","abstract":"The inner $\\sim5$ kiloparsec (kpc) region of the Milky Way is complex. Unravelling the evolution of the Galaxy requires precise understanding of the formation of this region. We report a study focused on disentangling the inner Galaxy ($r < 5$ kpc) using the measured positions, velocities, and element abundance ratios of red giant stars from the $APOGEE-Gaia$ surveys. After removing the stellar halo, inner Galaxy populations can be grouped into three main components based on their angular momentum: bar, disc, and a previously unreported ``knot'' component. The knot has a spheroidal shape, is concentrated in the inner $\\sim1.5$ kpc, is comprised of stars on nearly-radial orbits, and contains stars with super-solar [Fe/H] element abundances. The chemical compositions of the knot are qualitatively similar to the Galactic bar and inner disc, suggestive that these three populations share a common genesis; the chemical/dynamic properties of the knot suggest it could constitute a classical bulge formed via secular evolution. Moreover, our results show that the bar is more slowly rotating than previously thought, with a pattern speed of $\\Omega_{\\mathrm{bar}}=24\\pm3$ km s$^{-1}$ kpc$^{-1}$. This new estimate suggests that the influence of the bar extends beyond the solar radius, with $R_{\\mathrm{CR}}\\sim9.4-9.8$ kpc, depending on the adopted Milky Way rotation curve; it also suggests a ratio of corotation to bar length of $\\mathcal{R}\\sim1.8-2$. Our findings help place constraints on the formation and evolution of inner Galaxy populations, and directly constrain dynamical studies of the Milky Way bar and stars in the solar neighbourhood.","sentences":["The inner $\\sim5$ kiloparsec (kpc) region of the Milky Way is complex.","Unravelling the evolution of the Galaxy requires precise understanding of the formation of this region.","We report a study focused on disentangling the inner Galaxy ($r < 5$ kpc) using the measured positions, velocities, and element abundance ratios of red giant stars from the $APOGEE-Gaia$ surveys.","After removing the stellar halo, inner Galaxy populations can be grouped into three main components based on their angular momentum: bar, disc, and a previously unreported ``knot'' component.","The knot has a spheroidal shape, is concentrated in the inner $\\sim1.5$ kpc, is comprised of stars on nearly-radial orbits, and contains stars with super-solar [Fe/H] element abundances.","The chemical compositions of the knot are qualitatively similar to the Galactic bar and inner disc, suggestive that these three populations share a common genesis; the chemical/dynamic properties of the knot suggest it could constitute a classical bulge formed via secular evolution.","Moreover, our results show that the bar is more slowly rotating than previously thought, with a pattern speed of $\\Omega_{\\mathrm{bar}}=24\\pm3$ km s$^{-1}$ kpc$^{-1}$. This new estimate suggests that the influence of the bar extends beyond the solar radius, with $R_{\\mathrm{CR}}\\sim9.4-9.8$ kpc, depending on the adopted Milky Way rotation curve; it also suggests a ratio of corotation to bar length of $\\mathcal{R}\\sim1.8-2$. Our findings help place constraints on the formation and evolution of inner Galaxy populations, and directly constrain dynamical studies of the Milky Way bar and stars in the solar neighbourhood."],"url":"http://arxiv.org/abs/2402.07986v1","category":"astro-ph.GA"}
{"created":"2024-02-12 19:00:02","title":"The ALPINE-ALMA [C II] survey: Characterisation of Spatial Offsets in Main-Sequence Galaxies at $z \\sim$ 4-6","abstract":"Galaxy morphology is shaped by stellar activity, feedback, gas and dust properties, and interactions with surroundings, and can therefore provide insight into these processes. In this paper, we study the spatial offsets between stellar and interstellar medium emission in a sample of 54 main-sequence star-forming galaxies at $z\\sim4-6$ observed with the Atacama Large Millimeter/submillimeter Array (ALMA) and drawn from the ALMA Large Program to INvestigate C$^+$ at Early times (ALPINE). We find no significant spatial offset for the majority ($\\sim$ 70 percent) of galaxies in the sample among any combination of [C II], far-infrared continuum, optical, and ultraviolet emission. However, a fraction of the sample ($\\sim$ 30 percent) shows offsets larger than the median by more than 3$\\sigma$ significance (compared to the uncertainty on the offsets), especially between [C II] and ultraviolet emission. We find that these significant offsets are of the order of $\\sim$0.5-0.7 arcsec, corresponding to $\\sim$3.5-4.5 kiloparsecs. The offsets could be caused by a complex dust geometry, strong feedback from stars and active galactic nuclei, large-scale gas inflow and outflow, or a combination of these phenomena. However, our current analysis does not definitively constrain the origin. Future, higher resolution ALMA and JWST observations may help resolve the ambiguity. Regardless, since there exist at least some galaxies that display such large offsets, galaxy models and spectral energy distribution fitting codes cannot assume co-spatial emission in all main-sequence galaxies, and must take into account that the observed emission across wavelengths may be spatially segregated.","sentences":["Galaxy morphology is shaped by stellar activity, feedback, gas and dust properties, and interactions with surroundings, and can therefore provide insight into these processes.","In this paper, we study the spatial offsets between stellar and interstellar medium emission in a sample of 54 main-sequence star-forming galaxies at $z\\sim4-6$ observed with the Atacama Large Millimeter/submillimeter Array (ALMA) and drawn from the ALMA Large Program to INvestigate C$^+$ at Early times (ALPINE).","We find no significant spatial offset for the majority ($\\sim$ 70 percent) of galaxies in the sample among any combination of [C II], far-infrared continuum, optical, and ultraviolet emission.","However, a fraction of the sample ($\\sim$ 30 percent) shows offsets larger than the median by more than 3$\\sigma$ significance (compared to the uncertainty on the offsets), especially between [C II] and ultraviolet emission.","We find that these significant offsets are of the order of $\\sim$0.5-0.7 arcsec, corresponding to $\\sim$3.5-4.5 kiloparsecs.","The offsets could be caused by a complex dust geometry, strong feedback from stars and active galactic nuclei, large-scale gas inflow and outflow, or a combination of these phenomena.","However, our current analysis does not definitively constrain the origin.","Future, higher resolution ALMA and JWST observations may help resolve the ambiguity.","Regardless, since there exist at least some galaxies that display such large offsets, galaxy models and spectral energy distribution fitting codes cannot assume co-spatial emission in all main-sequence galaxies, and must take into account that the observed emission across wavelengths may be spatially segregated."],"url":"http://arxiv.org/abs/2402.07982v1","category":"astro-ph.GA"}
{"created":"2024-02-12 19:00:02","title":"Cyclotron line formation in the radiative shock of an accreting magnetized neutron star","abstract":"Magnetic neutron stars (NSs) often exhibit a cyclotron resonant scattering feature (CRSF) in their X-ray spectra, but the site of the CRSF formation is still an open puzzle. A promising candidate for high-luminosity sources has always been the radiative shock (RS) in the accretion column. Yet, no quantitative calculations of spectral formation at the RS have been performed so far. Here we explore the scenario where the shock is the site of the CRSF formation. We study spectral formation at the RS and the emergent spectral shape across a wide range of the parameter space. We developed a Monte Carlo code to conduct radiation transfer simulations at the RS, adopting a fully relativistic scheme for the interaction between radiation and electrons. We properly treated bulk-motion Comptonization in the pre-shock region, thermal Comptonization in the post-shock region, and resonant Compton scattering in both regions. We calculated the angle- and energy-dependent emergent X-ray spectrum from the RS, focusing on both the CRSF and the X-ray continuum, under diverse conditions. We find that a power law, hard X-ray continuum and a CRSF are naturally produced by the first-order Fermi energization as the photons criss-cross the shock. The CRSF shape depends mainly on the transverse optical depth and the post-shock temperature. We show that the CRSF energy centroid is shifted by ~(20-30)% to lower energies compared to the classical cyclotron energy, due to the Doppler boosting between the shock frame and the bulk-motion frame. We demonstrate that a \"bump\" feature arises in the right wing of the CRSF due to upscattering of photons by the accreting plasma and extends to higher energies for larger optical depths and post-shock temperatures. The implications of the Doppler effect on the centroid of the emergent CRSF must be considered if an accurate determination of the magnetic field strength is desired.","sentences":["Magnetic neutron stars (NSs) often exhibit a cyclotron resonant scattering feature (CRSF) in their X-ray spectra, but the site of the CRSF formation is still an open puzzle.","A promising candidate for high-luminosity sources has always been the radiative shock (RS) in the accretion column.","Yet, no quantitative calculations of spectral formation at the RS have been performed so far.","Here we explore the scenario where the shock is the site of the CRSF formation.","We study spectral formation at the RS and the emergent spectral shape across a wide range of the parameter space.","We developed a Monte Carlo code to conduct radiation transfer simulations at the RS, adopting a fully relativistic scheme for the interaction between radiation and electrons.","We properly treated bulk-motion Comptonization in the pre-shock region, thermal Comptonization in the post-shock region, and resonant Compton scattering in both regions.","We calculated the angle- and energy-dependent emergent X-ray spectrum from the RS, focusing on both the CRSF and the X-ray continuum, under diverse conditions.","We find that a power law, hard X-ray continuum and a CRSF are naturally produced by the first-order Fermi energization as the photons criss-cross the shock.","The CRSF shape depends mainly on the transverse optical depth and the post-shock temperature.","We show that the CRSF energy centroid is shifted by ~(20-30)% to lower energies compared to the classical cyclotron energy, due to the Doppler boosting between the shock frame and the bulk-motion frame.","We demonstrate that a \"bump\" feature arises in the right wing of the CRSF due to upscattering of photons by the accreting plasma and extends to higher energies for larger optical depths and post-shock temperatures.","The implications of the Doppler effect on the centroid of the emergent CRSF must be considered if an accurate determination of the magnetic field strength is desired."],"url":"http://arxiv.org/abs/2402.07983v1","category":"astro-ph.HE"}
{"created":"2024-02-12 17:21:24","title":"Total Roman {2}-Dominating functions in Graphs","abstract":"A Roman $\\{2\\}$-dominating function (R2F) is a function $f:V\\rightarrow \\{0,1,2\\}$ with the property that for every vertex $v\\in V$ with $f(v)=0$ there is a neighbor $u$ of $v$ with $f(u)=2$, or there are two neighbors $x,y$ of $v$ with $f(x)=f(y)=1$. A total Roman $\\{2\\}$-dominating function (TR2DF) is an R2F $f$ such that the set of vertices with $f(v)>0$ induce a subgraph with no isolated vertices. The weight of a TR2DF is the sum of its function values over all vertices, and the minimum weight of a TR2DF of $G$ is the total Roman $\\{2\\}$-domination number $\\gamma_{tR2}(G).$ In this paper, we initiate the study of total Roman $\\{2\\}$-dominating functions, where properties are established. Moreover, we present various bounds on the total Roman $\\{2\\}$-domination number. We also show that the decision problem associated with $\\gamma_{tR2}(G)$ is NP-complete for bipartite and chordal graphs. {Moreover, we show that it is possible to compute this parameter in linear time for bounded clique-width graphs (including tres).}","sentences":["A Roman $\\{2\\}$-dominating function (R2F) is a function $f:V\\rightarrow \\{0,1,2\\}$ with the property that for every vertex $v\\in V$ with $f(v)=0$ there is a neighbor $u$ of $v$ with $f(u)=2$, or there are two neighbors $x,y$ of $v$ with $f(x)=f(y)=1$. A total Roman $\\{2\\}$-dominating function (TR2DF) is an R2F $f$ such that the set of vertices with $f(v)>0$ induce a subgraph with no isolated vertices.","The weight of a TR2DF is the sum of its function values over all vertices, and the minimum weight of a TR2DF of $G$ is the total Roman $\\{2\\}$-domination number $\\gamma_{tR2}(G).$ In this paper, we initiate the study of total Roman $\\{2\\}$-dominating functions, where properties are established.","Moreover, we present various bounds on the total Roman $\\{2\\}$-domination number.","We also show that the decision problem associated with $\\gamma_{tR2}(G)$ is NP-complete for bipartite and chordal graphs.","{Moreover, we show that it is possible to compute this parameter in linear time for bounded clique-width graphs (including tres).}"],"url":"http://arxiv.org/abs/2402.07968v1","category":"math.CO"}
{"created":"2024-02-13 18:47:02","title":"Tunable Shape Oscillations of Adaptive Droplets","abstract":"Living materials adapt their shape to signals from the environment, yet the impact of shape changes on signal processing and the associated feedback dynamics remain unclear. We derive coarse-grained equations for droplets that adjust their interfacial tension in response to signals exchanged at contact surfaces, from the microscopic biophysics of adhesion and signaling. We find that droplet pairs exhibit symmetry-breaking, excitability, and oscillations. The underlying critical points reveal novel mechanisms for physical signal processing through shape adaptation in soft active materials.","sentences":["Living materials adapt their shape to signals from the environment, yet the impact of shape changes on signal processing and the associated feedback dynamics remain unclear.","We derive coarse-grained equations for droplets that adjust their interfacial tension in response to signals exchanged at contact surfaces, from the microscopic biophysics of adhesion and signaling.","We find that droplet pairs exhibit symmetry-breaking, excitability, and oscillations.","The underlying critical points reveal novel mechanisms for physical signal processing through shape adaptation in soft active materials."],"url":"http://arxiv.org/abs/2402.08664v1","category":"cond-mat.soft"}
{"created":"2024-02-13 18:24:10","title":"Peeking Behind the Curtains of Residual Learning","abstract":"The utilization of residual learning has become widespread in deep and scalable neural nets. However, the fundamental principles that contribute to the success of residual learning remain elusive, thus hindering effective training of plain nets with depth scalability. In this paper, we peek behind the curtains of residual learning by uncovering the \"dissipating inputs\" phenomenon that leads to convergence failure in plain neural nets: the input is gradually compromised through plain layers due to non-linearities, resulting in challenges of learning feature representations. We theoretically demonstrate how plain neural nets degenerate the input to random noise and emphasize the significance of a residual connection that maintains a better lower bound of surviving neurons as a solution. With our theoretical discoveries, we propose \"The Plain Neural Net Hypothesis\" (PNNH) that identifies the internal path across non-linear layers as the most critical part in residual learning, and establishes a paradigm to support the training of deep plain neural nets devoid of residual connections. We thoroughly evaluate PNNH-enabled CNN architectures and Transformers on popular vision benchmarks, showing on-par accuracy, up to 0.3% higher training throughput, and 2x better parameter efficiency compared to ResNets and vision Transformers.","sentences":["The utilization of residual learning has become widespread in deep and scalable neural nets.","However, the fundamental principles that contribute to the success of residual learning remain elusive, thus hindering effective training of plain nets with depth scalability.","In this paper, we peek behind the curtains of residual learning by uncovering the \"dissipating inputs\" phenomenon that leads to convergence failure in plain neural nets: the input is gradually compromised through plain layers due to non-linearities, resulting in challenges of learning feature representations.","We theoretically demonstrate how plain neural nets degenerate the input to random noise and emphasize the significance of a residual connection that maintains a better lower bound of surviving neurons as a solution.","With our theoretical discoveries, we propose \"The Plain Neural Net Hypothesis\" (PNNH) that identifies the internal path across non-linear layers as the most critical part in residual learning, and establishes a paradigm to support the training of deep plain neural nets devoid of residual connections.","We thoroughly evaluate PNNH-enabled CNN architectures and Transformers on popular vision benchmarks, showing on-par accuracy, up to 0.3% higher training throughput, and 2x better parameter efficiency compared to ResNets and vision Transformers."],"url":"http://arxiv.org/abs/2402.08645v1","category":"cs.CV"}
{"created":"2024-02-13 17:13:55","title":"A robust second-order low-rank BUG integrator based on the midpoint rule","abstract":"Dynamical low-rank approximation has become a valuable tool to perform an on-the-fly model order reduction for prohibitively large matrix differential equations. A core ingredient is the construction of integrators that are robust to the presence of small singular values and the resulting large time derivatives of the orthogonal factors in the low-rank matrix representation. Recently, the robust basis-update & Galerkin (BUG) class of integrators has been introduced. These methods require no steps that evolve the solution backward in time, often have favourable structure-preserving properties, and allow for parallel time-updates of the low-rank factors. The BUG framework is flexible enough to allow for adaptations to these and further requirements. However, the BUG methods presented so far have only first-order robust error bounds. This work proposes a second-order BUG integrator for dynamical low-rank approximation based on the midpoint rule. The integrator first performs a half-step with a first-order BUG integrator, followed by a Galerkin update with a suitably augmented basis. We prove a robust second-order error bound which in addition shows an improved dependence on the normal component of the vector field. These rigorous results are illustrated and complemented by a number of numerical experiments.","sentences":["Dynamical low-rank approximation has become a valuable tool to perform an on-the-fly model order reduction for prohibitively large matrix differential equations.","A core ingredient is the construction of integrators that are robust to the presence of small singular values and the resulting large time derivatives of the orthogonal factors in the low-rank matrix representation.","Recently, the robust basis-update & Galerkin (BUG) class of integrators has been introduced.","These methods require no steps that evolve the solution backward in time, often have favourable structure-preserving properties, and allow for parallel time-updates of the low-rank factors.","The BUG framework is flexible enough to allow for adaptations to these and further requirements.","However, the BUG methods presented so far have only first-order robust error bounds.","This work proposes a second-order BUG integrator for dynamical low-rank approximation based on the midpoint rule.","The integrator first performs a half-step with a first-order BUG integrator, followed by a Galerkin update with a suitably augmented basis.","We prove a robust second-order error bound which in addition shows an improved dependence on the normal component of the vector field.","These rigorous results are illustrated and complemented by a number of numerical experiments."],"url":"http://arxiv.org/abs/2402.08607v1","category":"math.NA"}
{"created":"2024-02-13 15:26:19","title":"Investigating the Effect of Noise on the Training Performance of Hybrid Quantum Neural Networks","abstract":"In this paper, we conduct a comprehensively analyze the influence of different quantum noise gates, including Phase Flip, Bit Flip, Phase Damping, Amplitude Damping, and the Depolarizing Channel, on the performance of HyQNNs. Our results reveal distinct and significant effects on HyQNNs training and validation accuracies across different probabilities of noise. For instance, the Phase Flip gate introduces phase errors, and we observe that HyQNNs exhibit resilience at higher probability (p = 1.0), adapting effectively to consistent noise patterns, whereas at intermediate probabilities, the performance declines. Bit Flip errors, represented by the PauliX gate, impact HyQNNs in a similar way to that Phase Flip error gate. The HyQNNs, can adapt such kind of errors at maximum probability (p = 1.0). Unlike Phase and Bit Flip error gates, Phase Damping and Amplitude Damping gates disrupt quantum information, with HyQNNs demonstrating resilience at lower probabilities but facing challenges at higher probabilities. Amplitude Damping error gate, in particular, poses efficiency and accuracy issues at higher probabilities however with lowest probability (p = 0.1),it has the least effect and the HyQNNs, however not very effectively, but still tends to learn. The Depolarizing Channel proves most detrimental to HyQNNs performance, with limited or no training improvements. There was no training potential observed regardless of the probability of this noise gate. These findings underscore the critical need for advanced quantum error mitigation and resilience strategies in the design and training of HyQNNs, especially in environments prone to depolarizing noise. This paper quantitatively investigate that understanding the impact of quantum noise gates is essential for harnessing the full potential of quantum computing in practical applications.","sentences":["In this paper, we conduct a comprehensively analyze the influence of different quantum noise gates, including Phase Flip, Bit Flip, Phase Damping, Amplitude Damping, and the Depolarizing Channel, on the performance of HyQNNs.","Our results reveal distinct and significant effects on HyQNNs training and validation accuracies across different probabilities of noise.","For instance, the Phase Flip gate introduces phase errors, and we observe that HyQNNs exhibit resilience at higher probability (p = 1.0), adapting effectively to consistent noise patterns, whereas at intermediate probabilities, the performance declines.","Bit Flip errors, represented by the PauliX gate, impact HyQNNs in a similar way to that Phase Flip error gate.","The HyQNNs, can adapt such kind of errors at maximum probability (p = 1.0).","Unlike Phase and Bit Flip error gates, Phase Damping and Amplitude Damping gates disrupt quantum information, with HyQNNs demonstrating resilience at lower probabilities but facing challenges at higher probabilities.","Amplitude Damping error gate, in particular, poses efficiency and accuracy issues at higher probabilities however with lowest probability (p = 0.1),it has the least effect and the HyQNNs, however not very effectively, but still tends to learn.","The Depolarizing Channel proves most detrimental to HyQNNs performance, with limited or no training improvements.","There was no training potential observed regardless of the probability of this noise gate.","These findings underscore the critical need for advanced quantum error mitigation and resilience strategies in the design and training of HyQNNs, especially in environments prone to depolarizing noise.","This paper quantitatively investigate that understanding the impact of quantum noise gates is essential for harnessing the full potential of quantum computing in practical applications."],"url":"http://arxiv.org/abs/2402.08523v1","category":"quant-ph"}
{"created":"2024-02-13 15:05:07","title":"Clouds dissipate quickly during solar eclipses as the land surface cools","abstract":"Clouds affected by solar eclipses could influence the reflection of sunlight back into space and might change local precipitation patterns. Satellite cloud retrievals have so far not taken into account the lunar shadow, hindering a reliable spaceborne assessment of the eclipse-induced cloud evolution. Here we use satellite cloud measurements during three solar eclipses between 2005 and 2016 that have been corrected for the partial lunar shadow together with large-eddy simulations to analyze the eclipse-induced cloud evolution. Our corrected data reveal that, over cooling land surfaces, shallow cumulus clouds start to disappear at very small solar obscurations. Our simulations explain that the cloud response was delayed and was initiated at even smaller solar obscurations. We demonstrate that neglecting the disappearance of clouds during a solar eclipse could lead to a considerable overestimation of the eclipse-related reduction of net incoming solar radiation. These findings should spur cloud model simulations of the direct consequences of sunlight-intercepting geoengineering proposals, for which our results serve as a unique benchmark.","sentences":["Clouds affected by solar eclipses could influence the reflection of sunlight back into space and might change local precipitation patterns.","Satellite cloud retrievals have so far not taken into account the lunar shadow, hindering a reliable spaceborne assessment of the eclipse-induced cloud evolution.","Here we use satellite cloud measurements during three solar eclipses between 2005 and 2016 that have been corrected for the partial lunar shadow together with large-eddy simulations to analyze the eclipse-induced cloud evolution.","Our corrected data reveal that, over cooling land surfaces, shallow cumulus clouds start to disappear at very small solar obscurations.","Our simulations explain that the cloud response was delayed and was initiated at even smaller solar obscurations.","We demonstrate that neglecting the disappearance of clouds during a solar eclipse could lead to a considerable overestimation of the eclipse-related reduction of net incoming solar radiation.","These findings should spur cloud model simulations of the direct consequences of sunlight-intercepting geoengineering proposals, for which our results serve as a unique benchmark."],"url":"http://arxiv.org/abs/2402.08510v1","category":"physics.ao-ph"}
{"created":"2024-02-13 14:34:58","title":"Ground state energy of dense gases of strongly interacting fermions","abstract":"We study the ground state energy of a gas of $N$ fermions confined to a unit box in $d$ dimensions. The particles interact through a 2-body potential with strength scaled in an $N$-dependent way as $N^{-\\alpha}v$, where $\\alpha\\in \\mathbb R$ and $v$ is a function of positive type satisfying a mild regularity assumption. Our focus is on the strongly interacting case $\\alpha<1-\\frac2d$. We contrast our result with existing results in the weakly interacting case $\\alpha>1-\\frac2d$, and the transition happening at the mean-field scaling $\\alpha=1-\\frac2d$. Our proof is an adaptation of the bosonization technique used to treat the mean-field case.","sentences":["We study the ground state energy of a gas of $N$ fermions confined to a unit box in $d$ dimensions.","The particles interact through a 2-body potential with strength scaled in an $N$-dependent way as $N^{-\\alpha}v$, where $\\alpha\\in \\mathbb R$ and $v$ is a function of positive type satisfying a mild regularity assumption.","Our focus is on the strongly interacting case $\\alpha<1-\\frac2d$. We contrast our result with existing results in the weakly interacting case $\\alpha>1-\\frac2d$, and the transition happening at the mean-field scaling $\\alpha=1-\\frac2d$. Our proof is an adaptation of the bosonization technique used to treat the mean-field case."],"url":"http://arxiv.org/abs/2402.08490v1","category":"math-ph"}
{"created":"2024-02-13 14:08:55","title":"Risk-neutral limit of adaptive importance sampling of random stopping times","abstract":"We discuss importance sampling of exit problems that involve unbounded stopping times; examples are mean first passage times, transition rates or committor probabilities in molecular dynamics. The naive application of variance minimization techniques can lead to pathologies here, including proposal measures that are not absolutely continuous to the reference measure or importance sampling estimators that formally have zero variance, but that produce infinitely long trajectories. We illustrate these issues with simple examples and discuss a possible solution that is based on a risk-sensitive optimal control framework of importance sampling.","sentences":["We discuss importance sampling of exit problems that involve unbounded stopping times; examples are mean first passage times, transition rates or committor probabilities in molecular dynamics.","The naive application of variance minimization techniques can lead to pathologies here, including proposal measures that are not absolutely continuous to the reference measure or importance sampling estimators that formally have zero variance, but that produce infinitely long trajectories.","We illustrate these issues with simple examples and discuss a possible solution that is based on a risk-sensitive optimal control framework of importance sampling."],"url":"http://arxiv.org/abs/2402.08476v1","category":"math.PR"}
{"created":"2024-02-13 13:40:03","title":"Average dissipation for stochastic transport equations with L\u00e9vy noise","abstract":"We show that, in one spatial and arbitrary jump dimension, the averaged solution of a Marcustype SPDE with pure jump L\\'evy transport noise satisfies a dissipative deterministic equation involving a fractional Laplace-type operator. To this end, we identify the correct associated L\\'evy measure for the driving noise. We consider this a first step in the direction of a non-local version of enhanced dissipation, a phenomenon recently proven to occur for Brownian transport noise and the associated local parabolic PDE by the first author. Moreover, we present numerical simulations, supporting the fact that dissipation occurs for the averaged solution, with a behavior akin to the diffusion due to a fractional Laplacian, but not in a pathwise sense.","sentences":["We show that, in one spatial and arbitrary jump dimension, the averaged solution of a Marcustype SPDE with pure jump L\\'evy transport noise satisfies a dissipative deterministic equation involving a fractional Laplace-type operator.","To this end, we identify the correct associated L\\'evy measure for the driving noise.","We consider this a first step in the direction of a non-local version of enhanced dissipation, a phenomenon recently proven to occur for Brownian transport noise and the associated local parabolic PDE by the first author.","Moreover, we present numerical simulations, supporting the fact that dissipation occurs for the averaged solution, with a behavior akin to the diffusion due to a fractional Laplacian, but not in a pathwise sense."],"url":"http://arxiv.org/abs/2402.08461v1","category":"math.PR"}
{"created":"2024-02-13 13:30:06","title":"Energy-aware Dynamic Resource Allocation in Virtual Sensor Networks","abstract":"Sensor network virtualization enables the possibility of sharing common physical resources to multiple stakeholder applications. This paper focuses on addressing the dynamic adaptation of already assigned virtual sensor network resources to respond to time varying application demands. We propose an optimization framework that dynamically allocate applications into sensor nodes while accounting for the characteristics and limitations of the wireless sensor environment. It takes also into account the additional energy consumption related to activating new nodes and/or moving already active applications. Different objective functions related to the available energy in the nodes are analyzed. The proposed framework is evaluated by simulation considering realistic parameters from actual sensor nodes and deployed applications to assess the efficiency of the proposals.","sentences":["Sensor network virtualization enables the possibility of sharing common physical resources to multiple stakeholder applications.","This paper focuses on addressing the dynamic adaptation of already assigned virtual sensor network resources to respond to time varying application demands.","We propose an optimization framework that dynamically allocate applications into sensor nodes while accounting for the characteristics and limitations of the wireless sensor environment.","It takes also into account the additional energy consumption related to activating new nodes and/or moving already active applications.","Different objective functions related to the available energy in the nodes are analyzed.","The proposed framework is evaluated by simulation considering realistic parameters from actual sensor nodes and deployed applications to assess the efficiency of the proposals."],"url":"http://arxiv.org/abs/2402.08443v1","category":"cs.NI"}
{"created":"2024-02-13 10:33:26","title":"Nonparametric velocity estimation in stochastic convection-diffusion equations from multiple local measurements","abstract":"We investigate pointwise estimation of the function-valued velocity field of a second-order linear SPDE. Based on multiple spatially localised measurements, we construct a weighted augmented MLE and study its convergence properties as the spatial resolution of the observations tends to zero and the number of measurements increases. By imposing H\\\"older smoothness conditions, we recover the pointwise convergence rate known to be minimax-optimal in the linear regression framework. The optimality of the rate in the current setting is verified by adapting the lower bound ansatz based on the RKHS of local measurements to the nonparametric situation.","sentences":["We investigate pointwise estimation of the function-valued velocity field of a second-order linear SPDE.","Based on multiple spatially localised measurements, we construct a weighted augmented MLE and study its convergence properties as the spatial resolution of the observations tends to zero and the number of measurements increases.","By imposing H\\\"older smoothness conditions, we recover the pointwise convergence rate known to be minimax-optimal in the linear regression framework.","The optimality of the rate in the current setting is verified by adapting the lower bound ansatz based on the RKHS of local measurements to the nonparametric situation."],"url":"http://arxiv.org/abs/2402.08353v1","category":"math.ST"}
{"created":"2024-02-13 10:31:38","title":"A refinement of Horn's conjecture","abstract":"We provide a refinement of Horn's conjecture by considering spectra with repetitions. To do this we adapt P. Belkale's techniques to our context, in the form proposed by N. Berline, M. Vergne and M. Walter.","sentences":["We provide a refinement of Horn's conjecture by considering spectra with repetitions.","To do this we adapt P. Belkale's techniques to our context, in the form proposed by N. Berline, M. Vergne and M. Walter."],"url":"http://arxiv.org/abs/2402.08350v1","category":"math.AG"}
{"created":"2024-02-13 09:20:26","title":"CrossGaze: A Strong Method for 3D Gaze Estimation in the Wild","abstract":"Gaze estimation, the task of predicting where an individual is looking, is a critical task with direct applications in areas such as human-computer interaction and virtual reality. Estimating the direction of looking in unconstrained environments is difficult, due to the many factors that can obscure the face and eye regions. In this work we propose CrossGaze, a strong baseline for gaze estimation, that leverages recent developments in computer vision architectures and attention-based modules. Unlike previous approaches, our method does not require a specialised architecture, utilizing already established models that we integrate in our architecture and adapt for the task of 3D gaze estimation. This approach allows for seamless updates to the architecture as any module can be replaced with more powerful feature extractors. On the Gaze360 benchmark, our model surpasses several state-of-the-art methods, achieving a mean angular error of 9.94 degrees. Our proposed model serves as a strong foundation for future research and development in gaze estimation, paving the way for practical and accurate gaze prediction in real-world scenarios.","sentences":["Gaze estimation, the task of predicting where an individual is looking, is a critical task with direct applications in areas such as human-computer interaction and virtual reality.","Estimating the direction of looking in unconstrained environments is difficult, due to the many factors that can obscure the face and eye regions.","In this work we propose CrossGaze, a strong baseline for gaze estimation, that leverages recent developments in computer vision architectures and attention-based modules.","Unlike previous approaches, our method does not require a specialised architecture, utilizing already established models that we integrate in our architecture and adapt for the task of 3D gaze estimation.","This approach allows for seamless updates to the architecture as any module can be replaced with more powerful feature extractors.","On the Gaze360 benchmark, our model surpasses several state-of-the-art methods, achieving a mean angular error of 9.94 degrees.","Our proposed model serves as a strong foundation for future research and development in gaze estimation, paving the way for practical and accurate gaze prediction in real-world scenarios."],"url":"http://arxiv.org/abs/2402.08316v1","category":"cs.CV"}
{"created":"2024-02-13 03:14:32","title":"Learning time-dependent PDE via graph neural networks and deep operator network for robust accuracy on irregular grids","abstract":"Scientific computing using deep learning has seen significant advancements in recent years. There has been growing interest in models that learn the operator from the parameters of a partial differential equation (PDE) to the corresponding solutions. Deep Operator Network (DeepONet) and Fourier Neural operator, among other models, have been designed with structures suitable for handling functions as inputs and outputs, enabling real-time predictions as surrogate models for solution operators. There has also been significant progress in the research on surrogate models based on graph neural networks (GNNs), specifically targeting the dynamics in time-dependent PDEs. In this paper, we propose GraphDeepONet, an autoregressive model based on GNNs, to effectively adapt DeepONet, which is well-known for successful operator learning. GraphDeepONet exhibits robust accuracy in predicting solutions compared to existing GNN-based PDE solver models. It maintains consistent performance even on irregular grids, leveraging the advantages inherited from DeepONet and enabling predictions on arbitrary grids. Additionally, unlike traditional DeepONet and its variants, GraphDeepONet enables time extrapolation for time-dependent PDE solutions. We also provide theoretical analysis of the universal approximation capability of GraphDeepONet in approximating continuous operators across arbitrary time intervals.","sentences":["Scientific computing using deep learning has seen significant advancements in recent years.","There has been growing interest in models that learn the operator from the parameters of a partial differential equation (PDE) to the corresponding solutions.","Deep Operator Network (DeepONet) and Fourier Neural operator, among other models, have been designed with structures suitable for handling functions as inputs and outputs, enabling real-time predictions as surrogate models for solution operators.","There has also been significant progress in the research on surrogate models based on graph neural networks (GNNs), specifically targeting the dynamics in time-dependent PDEs.","In this paper, we propose GraphDeepONet, an autoregressive model based on GNNs, to effectively adapt DeepONet, which is well-known for successful operator learning.","GraphDeepONet exhibits robust accuracy in predicting solutions compared to existing GNN-based PDE solver models.","It maintains consistent performance even on irregular grids, leveraging the advantages inherited from DeepONet and enabling predictions on arbitrary grids.","Additionally, unlike traditional DeepONet and its variants, GraphDeepONet enables time extrapolation for time-dependent PDE solutions.","We also provide theoretical analysis of the universal approximation capability of GraphDeepONet in approximating continuous operators across arbitrary time intervals."],"url":"http://arxiv.org/abs/2402.08187v1","category":"cs.LG"}
{"created":"2024-02-13 02:16:28","title":"Vector Modulator Based Active Compensation of Direct Feedthrough in Magnetic Particle Imaging","abstract":"In magnetic particle imaging (MPI), simultaneous excitation and signal acquisition leads to direct feedthrough interference. While this interference can be mitigated up to some extent with passive compensation, its time-varying nature necessitates active compensation methods to achieve the sensitivity levels needed for applications such as stem cell tracking. We have recently proposed an active compensation technique for MRI, which uses a vector modulator and a lookup-table-based algorithm for reducing the direct feedthrough in the analog domain. Here, we adapt this technique to MPI, demonstrating a successful recovery of the fundamental frequency and a significant increase in detection sensitivity.","sentences":["In magnetic particle imaging (MPI), simultaneous excitation and signal acquisition leads to direct feedthrough interference.","While this interference can be mitigated up to some extent with passive compensation, its time-varying nature necessitates active compensation methods to achieve the sensitivity levels needed for applications such as stem cell tracking.","We have recently proposed an active compensation technique for MRI, which uses a vector modulator and a lookup-table-based algorithm for reducing the direct feedthrough in the analog domain.","Here, we adapt this technique to MPI, demonstrating a successful recovery of the fundamental frequency and a significant increase in detection sensitivity."],"url":"http://arxiv.org/abs/2402.08175v1","category":"physics.ins-det"}
{"created":"2024-02-12 21:31:13","title":"Multi-Attribute Vision Transformers are Efficient and Robust Learners","abstract":"Since their inception, Vision Transformers (ViTs) have emerged as a compelling alternative to Convolutional Neural Networks (CNNs) across a wide spectrum of tasks. ViTs exhibit notable characteristics, including global attention, resilience against occlusions, and adaptability to distribution shifts. One underexplored aspect of ViTs is their potential for multi-attribute learning, referring to their ability to simultaneously grasp multiple attribute-related tasks. In this paper, we delve into the multi-attribute learning capability of ViTs, presenting a straightforward yet effective strategy for training various attributes through a single ViT network as distinct tasks. We assess the resilience of multi-attribute ViTs against adversarial attacks and compare their performance against ViTs designed for single attributes. Moreover, we further evaluate the robustness of multi-attribute ViTs against a recent transformer based attack called Patch-Fool. Our empirical findings on the CelebA dataset provide validation for our assertion.","sentences":["Since their inception, Vision Transformers (ViTs) have emerged as a compelling alternative to Convolutional Neural Networks (CNNs) across a wide spectrum of tasks.","ViTs exhibit notable characteristics, including global attention, resilience against occlusions, and adaptability to distribution shifts.","One underexplored aspect of ViTs is their potential for multi-attribute learning, referring to their ability to simultaneously grasp multiple attribute-related tasks.","In this paper, we delve into the multi-attribute learning capability of ViTs, presenting a straightforward yet effective strategy for training various attributes through a single ViT network as distinct tasks.","We assess the resilience of multi-attribute ViTs against adversarial attacks and compare their performance against ViTs designed for single attributes.","Moreover, we further evaluate the robustness of multi-attribute ViTs against a recent transformer based attack called Patch-Fool.","Our empirical findings on the CelebA dataset provide validation for our assertion."],"url":"http://arxiv.org/abs/2402.08070v1","category":"cs.CV"}
{"created":"2024-02-12 20:48:51","title":"Colimits of Heyting Algebras through Esakia Duality","abstract":"In this note we generalize the construction, due to Ghilardi, of the free Heyting algebra generated by a finite distributive lattice, to the case of arbitrary distributive lattices. Categorically, this provides an explicit construction of a left adjoint to the inclusion of Heyting algebras in the category of distributive lattices This is shown to have several applications, both old and new, in the study of Heyting algebras: (1) it allows a more concrete description of colimits of Heyting algebras, as well as, via duality theory, limits of Esakia spaces, by knowing their description over distributive lattices and Priestley spaces; (2) It allows a direct proof of the amalgamation property for Heyting algebras, and of related facts; (3) it allows a proof of the fact that the category of Heyting algebras is co-distributive. We also study some generalizations and variations of this construction to different settings. First, we analyse some subvarieties of Heyting algebras -- such as Boolean algebras, $\\mathsf{KC}$ and $\\mathsf{LC}$ algebras, and show how the construction can be adapted to this setting. Second, we study the relationship between the category of image-finite posets with p-morphisms and the category of posets with monotone maps, showing that a variation of the above ideas provides us with an appropriate general idea.","sentences":["In this note we generalize the construction, due to Ghilardi, of the free Heyting algebra generated by a finite distributive lattice, to the case of arbitrary distributive lattices.","Categorically, this provides an explicit construction of a left adjoint to the inclusion of Heyting algebras in the category of distributive lattices This is shown to have several applications, both old and new, in the study of Heyting algebras: (1) it allows a more concrete description of colimits of Heyting algebras, as well as, via duality theory, limits of Esakia spaces, by knowing their description over distributive lattices and Priestley spaces; (2) It allows a direct proof of the amalgamation property for Heyting algebras, and of related facts; (3) it allows a proof of the fact that the category of Heyting algebras is co-distributive.","We also study some generalizations and variations of this construction to different settings.","First, we analyse some subvarieties of Heyting algebras -- such as Boolean algebras, $\\mathsf{KC}$ and $\\mathsf{LC}$ algebras, and show how the construction can be adapted to this setting.","Second, we study the relationship between the category of image-finite posets with p-morphisms and the category of posets with monotone maps, showing that a variation of the above ideas provides us with an appropriate general idea."],"url":"http://arxiv.org/abs/2402.08058v1","category":"math.LO"}
{"created":"2024-02-12 20:30:37","title":"An algorithm for dynamic vehicle-track-structure interaction analysis for high-speed trains","abstract":"The objective of the present work is to develop a robust, yet simple-to-implement algorithm for dynamic vehicle-track-structure-interaction (VTSI) analysis, applicable to trains passing over bridges. The algorithm can be readily implemented in existing bridge analysis software with minimal code modifications. It is based on modeling the bridge and train separately, and coupling them together by means of kinematic constraints. The contact forces between the wheels and the track become Lagrange multipliers in this approach. A direct implementation of such an approach results in spurious oscillations in the contact forces. Two approaches are presented to mitigate these spurious oscillations - (a) a cubic B-spline interpolation of the kinematic constraints in time, and (b) an adaptation of an alternate time-integration scheme originally developed by Bathe. Solutions obtained using this algorithm are verified using a generic differential algebraic equation (DAE) solver. Due to high train speeds and possible track irregularities, wheels can momentarily lose contact with the track. This contact separation is formulated as a Linear Complementarity Problem (LCP). With this formulation, including contact separation in the analysis amounts to replacing a call to a linear equation solver by a call to an LCP solver, a modification of only two steps of the procedure. The focus of this paper is on the computational procedure of VTSI analysis. The main contribution of this paper is recognizing computational issues associated with time-varying kinematic constraints, clearly identifying their cause and developing remedies.","sentences":["The objective of the present work is to develop a robust, yet simple-to-implement algorithm for dynamic vehicle-track-structure-interaction (VTSI) analysis, applicable to trains passing over bridges.","The algorithm can be readily implemented in existing bridge analysis software with minimal code modifications.","It is based on modeling the bridge and train separately, and coupling them together by means of kinematic constraints.","The contact forces between the wheels and the track become Lagrange multipliers in this approach.","A direct implementation of such an approach results in spurious oscillations in the contact forces.","Two approaches are presented to mitigate these spurious oscillations - (a) a cubic B-spline interpolation of the kinematic constraints in time, and (b) an adaptation of an alternate time-integration scheme originally developed by Bathe.","Solutions obtained using this algorithm are verified using a generic differential algebraic equation (DAE) solver.","Due to high train speeds and possible track irregularities, wheels can momentarily lose contact with the track.","This contact separation is formulated as a Linear Complementarity Problem (LCP).","With this formulation, including contact separation in the analysis amounts to replacing a call to a linear equation solver by a call to an LCP solver, a modification of only two steps of the procedure.","The focus of this paper is on the computational procedure of VTSI analysis.","The main contribution of this paper is recognizing computational issues associated with time-varying kinematic constraints, clearly identifying their cause and developing remedies."],"url":"http://arxiv.org/abs/2402.08049v1","category":"math.NA"}
{"created":"2024-02-12 20:27:36","title":"Adaptive quantum accelerated imaging for space domain awareness","abstract":"The growth in space activity has increased the need for Space Domain Awareness (SDA) to ensure safe space operations. Imaging and detecting space targets is, however, challenging due to their dim appearance, small angular size/separation, dense distribution, and atmospheric turbulence. These challenges render space targets in ground-based imaging observations as point-like objects in the sub-Rayleigh regime, with extreme brightness contrast but a low photon budget. Here, we propose to use the recently developed quantum-accelerated imaging (QAI) for the SDA challenge. We mainly focus on three SDA challenges (1) minimal a priori assumptions (2) many-object problem (3) extreme brightness ratio. We also present results on source estimation and localization in the presence of atmospheric turbulence. QAI shows significantly improved estimation in position, brightness, and number of targets for all SDA challenges. In particular, we demonstrate up to 2.5 times better performance in source detection than highly optimized direct imaging in extreme scenarios like stars with a 1000 times brightness ratio. With over 10,000 simulations, we verify the increased resolution of our approach compared to conventional state-of-the-art direct imaging paving the way towards quantum optics approaches for SDA.","sentences":["The growth in space activity has increased the need for Space Domain Awareness (SDA) to ensure safe space operations.","Imaging and detecting space targets is, however, challenging due to their dim appearance, small angular size/separation, dense distribution, and atmospheric turbulence.","These challenges render space targets in ground-based imaging observations as point-like objects in the sub-Rayleigh regime, with extreme brightness contrast but a low photon budget.","Here, we propose to use the recently developed quantum-accelerated imaging (QAI) for the SDA challenge.","We mainly focus on three SDA challenges (1) minimal a priori assumptions (2) many-object problem (3) extreme brightness ratio.","We also present results on source estimation and localization in the presence of atmospheric turbulence.","QAI shows significantly improved estimation in position, brightness, and number of targets for all SDA challenges.","In particular, we demonstrate up to 2.5 times better performance in source detection than highly optimized direct imaging in extreme scenarios like stars with a 1000 times brightness ratio.","With over 10,000 simulations, we verify the increased resolution of our approach compared to conventional state-of-the-art direct imaging paving the way towards quantum optics approaches for SDA."],"url":"http://arxiv.org/abs/2402.08047v1","category":"physics.optics"}
{"created":"2024-02-12 19:11:03","title":"Extending 3D body pose estimation for robotic-assistive therapies of autistic children","abstract":"Robotic-assistive therapy has demonstrated very encouraging results for children with Autism. Accurate estimation of the child's pose is essential both for human-robot interaction and for therapy assessment purposes. Non-intrusive methods are the sole viable option since these children are sensitive to touch.   While depth cameras have been used extensively, existing methods face two major limitations: (i) they are usually trained with adult-only data and do not correctly estimate a child's pose, and (ii) they fail in scenarios with a high number of occlusions. Therefore, our goal was to develop a 3D pose estimator for children, by adapting an existing state-of-the-art 3D body modelling method and incorporating a linear regression model to fine-tune one of its inputs, thereby correcting the pose of children's 3D meshes.   In controlled settings, our method has an error below $0.3m$, which is considered acceptable for this kind of application and lower than current state-of-the-art methods. In real-world settings, the proposed model performs similarly to a Kinect depth camera and manages to successfully estimate the 3D body poses in a much higher number of frames.","sentences":["Robotic-assistive therapy has demonstrated very encouraging results for children with Autism.","Accurate estimation of the child's pose is essential both for human-robot interaction and for therapy assessment purposes.","Non-intrusive methods are the sole viable option since these children are sensitive to touch.   ","While depth cameras have been used extensively, existing methods face two major limitations: (i) they are usually trained with adult-only data and do not correctly estimate a child's pose, and (ii) they fail in scenarios with a high number of occlusions.","Therefore, our goal was to develop a 3D pose estimator for children, by adapting an existing state-of-the-art 3D body modelling method and incorporating a linear regression model to fine-tune one of its inputs, thereby correcting the pose of children's 3D meshes.   ","In controlled settings, our method has an error below $0.3m$, which is considered acceptable for this kind of application and lower than current state-of-the-art methods.","In real-world settings, the proposed model performs similarly to a Kinect depth camera and manages to successfully estimate the 3D body poses in a much higher number of frames."],"url":"http://arxiv.org/abs/2402.08006v1","category":"cs.RO"}
{"created":"2024-02-13 18:59:47","title":"Chain Reaction of Ideas: Can Radioactive Decay Predict Technological Innovation?","abstract":"This work demonstrates the application of a birth-death Markov process, inspired by radioactive decay, to capture the dynamics of innovation processes. Leveraging the Bass diffusion model, we derive a Gompertz-like function explaining the long-term innovation trends. The validity of our model is confirmed using citation data, Google trends, and a recurrent neural network, which also reveals short-term fluctuations. Further analysis through an automaton model suggests these fluctuations can arise from the inherent stochastic nature of the underlying physics.","sentences":["This work demonstrates the application of a birth-death Markov process, inspired by radioactive decay, to capture the dynamics of innovation processes.","Leveraging the Bass diffusion model, we derive a Gompertz-like function explaining the long-term innovation trends.","The validity of our model is confirmed using citation data, Google trends, and a recurrent neural network, which also reveals short-term fluctuations.","Further analysis through an automaton model suggests these fluctuations can arise from the inherent stochastic nature of the underlying physics."],"url":"http://arxiv.org/abs/2402.08681v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-13 18:54:25","title":"On reduced expressions for core double cosets","abstract":"The notion of a reduced expression for a double coset in a Coxeter group was introduced by Williamson, and recent work of Elias and Ko has made this theory more accessible and combinatorial. One result of Elias-Ko is that any coset admits a reduced expression which factors through a reduced expression for a related coset called its core. In this paper we define a class of cosets called atomic cosets, and prove that every core coset admits a reduced expression as a composition of atomic cosets. This leads to an algorithmic construction of a reduced expression for any coset. In types $A$ and $B$ we prove that the combinatorics of compositions of atomic cosets matches the combinatorics of ordinary expressions in a smaller group. In other types the combinatorics is new, as explored in a sequel by Ko.","sentences":["The notion of a reduced expression for a double coset in a Coxeter group was introduced by Williamson, and recent work of Elias and Ko has made this theory more accessible and combinatorial.","One result of Elias-Ko is that any coset admits a reduced expression which factors through a reduced expression for a related coset called its core.","In this paper we define a class of cosets called atomic cosets, and prove that every core coset admits a reduced expression as a composition of atomic cosets.","This leads to an algorithmic construction of a reduced expression for any coset.","In types $A$ and $B$ we prove that the combinatorics of compositions of atomic cosets matches the combinatorics of ordinary expressions in a smaller group.","In other types the combinatorics is new, as explored in a sequel by Ko."],"url":"http://arxiv.org/abs/2402.08673v1","category":"math.CO"}
{"created":"2024-02-13 18:03:29","title":"Finite density QCD equation of state: critical point and lattice-based $T'$-expansion","abstract":"We present a novel construction of the QCD equation of state (EoS) at finite baryon density. Our work combines a recently proposed resummation scheme for lattice QCD results with the universal critical behavior at the QCD critical point. This allows us to obtain a family of equations of state in the range $0 \\leq \\mu_B \\leq 700$ MeV and 25 MeV $\\leq T \\leq 800$ MeV, which match lattice QCD results near $\\mu_B=0$ while featuring a critical point in the 3D Ising model universality class. The position of the critical point can be chosen within the range accessible to beam-energy scan heavy-ion collision experiments. The strength of the singularity and the shape of the critical region are parameterized using a standard parameter set. We impose stability and causality constraints and discuss the available ranges of critical point parameter choices, finding that they extend beyond earlier parametric QCD EoS proposals. We present thermodynamic observables, including baryon density, pressure, entropy density, energy density, baryon susceptibility and speed of sound, that cover a wide range in the QCD phase diagram relevant for experimental exploration.","sentences":["We present a novel construction of the QCD equation of state (EoS) at finite baryon density.","Our work combines a recently proposed resummation scheme for lattice QCD results with the universal critical behavior at the QCD critical point.","This allows us to obtain a family of equations of state in the range $0 \\leq \\mu_B \\leq 700$ MeV and 25 MeV $\\leq T \\leq 800$ MeV, which match lattice QCD results near $\\mu_B=0$ while featuring a critical point in the 3D Ising model universality class.","The position of the critical point can be chosen within the range accessible to beam-energy scan heavy-ion collision experiments.","The strength of the singularity and the shape of the critical region are parameterized using a standard parameter set.","We impose stability and causality constraints and discuss the available ranges of critical point parameter choices, finding that they extend beyond earlier parametric QCD EoS proposals.","We present thermodynamic observables, including baryon density, pressure, entropy density, energy density, baryon susceptibility and speed of sound, that cover a wide range in the QCD phase diagram relevant for experimental exploration."],"url":"http://arxiv.org/abs/2402.08636v1","category":"nucl-th"}
{"created":"2024-02-13 17:07:29","title":"Bipolar electron waveguides in two-dimensional materials with tilted Dirac cones","abstract":"We show that the (2+1)-dimensional massless Dirac equation, which includes a tilt term, can be reduced to the biconfluent Heun equation for a broad range of scalar confining potentials, including the well-known Morse potential. Applying these solutions, we investigate a bipolar electron waveguide in 8-$Pmmn$ borophene, formed by a well and barrier, both described by the Morse potential. We demonstrate that the ability of two-dimensional materials with tilted Dirac cones to localize electrons in both a barrier and a well can be harnessed to create pseudogaps in their electronic spectrum. These pseudogaps can be tuned through varying the applied top-gate voltage. Potential opto-valleytronic and terahertz applications are discussed.","sentences":["We show that the (2+1)-dimensional massless Dirac equation, which includes a tilt term, can be reduced to the biconfluent Heun equation for a broad range of scalar confining potentials, including the well-known Morse potential.","Applying these solutions, we investigate a bipolar electron waveguide in 8-$Pmmn$ borophene, formed by a well and barrier, both described by the Morse potential.","We demonstrate that the ability of two-dimensional materials with tilted Dirac cones to localize electrons in both a barrier and a well can be harnessed to create pseudogaps in their electronic spectrum.","These pseudogaps can be tuned through varying the applied top-gate voltage.","Potential opto-valleytronic and terahertz applications are discussed."],"url":"http://arxiv.org/abs/2402.08600v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-13 16:45:03","title":"Cauchy--Schwarz-type inequalities for additive functions","abstract":"The main goal of this paper is to show that if a real valued function defined on a groupoid satisfies a certain Levi--Civita-type functional equation, then it also fulfills a Cauchy--Schwarz-type functional inequality. In particular, if the groupoid is the multiplicative structure of commutative ring, then we can establish the existence of nontrivial additive functions satisfying inequalities connected to the multiplicative structure.","sentences":["The main goal of this paper is to show that if a real valued function defined on a groupoid satisfies a certain Levi--Civita-type functional equation, then it also fulfills a Cauchy--Schwarz-type functional inequality.","In particular, if the groupoid is the multiplicative structure of commutative ring, then we can establish the existence of nontrivial additive functions satisfying inequalities connected to the multiplicative structure."],"url":"http://arxiv.org/abs/2402.08587v1","category":"math.RA"}
{"created":"2024-02-13 16:36:50","title":"Mixture of Link Predictors","abstract":"Link prediction, which aims to forecast unseen connections in graphs, is a fundamental task in graph machine learning. Heuristic methods, leveraging a range of different pairwise measures such as common neighbors and shortest paths, often rival the performance of vanilla Graph Neural Networks (GNNs). Therefore, recent advancements in GNNs for link prediction (GNN4LP) have primarily focused on integrating one or a few types of pairwise information. In this work, we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance. As a result, we propose a simple mixture of experts model Link-MoE for link prediction. Link-MoE utilizes various GNNs as experts and strategically selects the appropriate expert for each node pair based on various types of pairwise information. Experimental results across diverse real-world datasets demonstrate substantial performance improvement from Link-MoE. Notably, Link-MoE achieves a relative improvement of 18.82\\% on the MRR metric for the Pubmed dataset and 10.8\\% on the Hits@100 metric for the ogbl-ppa dataset, compared to the best baselines.","sentences":["Link prediction, which aims to forecast unseen connections in graphs, is a fundamental task in graph machine learning.","Heuristic methods, leveraging a range of different pairwise measures such as common neighbors and shortest paths, often rival the performance of vanilla Graph Neural Networks (GNNs).","Therefore, recent advancements in GNNs for link prediction (GNN4LP) have primarily focused on integrating one or a few types of pairwise information.","In this work, we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance.","As a result, we propose a simple mixture of experts model Link-MoE for link prediction.","Link-MoE utilizes various GNNs as experts and strategically selects the appropriate expert for each node pair based on various types of pairwise information.","Experimental results across diverse real-world datasets demonstrate substantial performance improvement from Link-MoE. Notably, Link-MoE achieves a relative improvement of 18.82\\% on the MRR metric for the Pubmed dataset and 10.8\\% on the Hits@100 metric for the ogbl-ppa dataset, compared to the best baselines."],"url":"http://arxiv.org/abs/2402.08583v1","category":"cs.LG"}
{"created":"2024-02-13 15:55:58","title":"Incidence of the Brownian relaxation process on the magnetic properties of ferrofluids","abstract":"Ferrofluids containing magnetic nanoparticles represent a special class of magnetic materials due to the added freedom of particle tumbling in the fluids. We studied this process, known as Brownian relaxation, and its effect on the magnetic properties of ferrofluids with controlled magnetite nanoparticle sizes. For small nanoparticles (below 10 nm diameter) the N\\'eel process is expected to dominate the magnetic response, whereas for larger particles, Brownian relaxation becomes important. Temperature- and magnetic field-dependent magnetization studies, differential scanning calorimetry, and AC susceptibility measurements were carried out for 6, 8, 10.6, and 13.5 nm diameter magnetite nanoparticles suspended in water. We identify clear fingerprints of the Brownian relaxation for the sample of the large diameter nanoparticles as both magnetic and thermal hysteresis develop at the water freezing temperature, whereas the samples of small diameter nanoparticles remain hysteresis-free down to the magnetic blocking temperature. This is supported by the temperature-dependent AC susceptibility measurements: above 273 K, the data show a low-frequency Debye peak, which is characteristic of the Brownian relaxation. This peak vanishes below 273 K.","sentences":["Ferrofluids containing magnetic nanoparticles represent a special class of magnetic materials due to the added freedom of particle tumbling in the fluids.","We studied this process, known as Brownian relaxation, and its effect on the magnetic properties of ferrofluids with controlled magnetite nanoparticle sizes.","For small nanoparticles (below 10 nm diameter)","the N\\'eel process is expected to dominate the magnetic response, whereas for larger particles, Brownian relaxation becomes important.","Temperature- and magnetic field-dependent magnetization studies, differential scanning calorimetry, and AC susceptibility measurements were carried out for 6, 8, 10.6, and 13.5 nm diameter magnetite nanoparticles suspended in water.","We identify clear fingerprints of the Brownian relaxation for the sample of the large diameter nanoparticles as both magnetic and thermal hysteresis develop at the water freezing temperature, whereas the samples of small diameter nanoparticles remain hysteresis-free down to the magnetic blocking temperature.","This is supported by the temperature-dependent AC susceptibility measurements: above 273 K, the data show a low-frequency Debye peak, which is characteristic of the Brownian relaxation.","This peak vanishes below 273 K."],"url":"http://arxiv.org/abs/2402.08553v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-13 15:06:44","title":"Asymptotic Error Distribution of the Euler Scheme for Fractional Stochastic Delay Differential Equations with Additive Noise","abstract":"In this paper we consider the Euler scheme for a class of stochastic delay differential equations driven by a linear fractional $\\alpha$-stable L\\'evy motion with index $H\\in(0,1)$. We establish the consistency of the scheme and study the limit distribution of the normalized error process. We show that in the rough case, i.e. when $H<1/\\alpha$, the rate of convergence of the simulation error is of order $\\Delta_{n}^{H+1-1/\\alpha}$ and that the limit process is once again the solution of an (semi-linear) SDDE.","sentences":["In this paper we consider the Euler scheme for a class of stochastic delay differential equations driven by a linear fractional $\\alpha$-stable L\\'evy motion with index $H\\in(0,1)$.","We establish the consistency of the scheme and study the limit distribution of the normalized error process.","We show that in the rough case, i.e. when $H<1/\\alpha$, the rate of convergence of the simulation error is of order $\\Delta_{n}^{H+1-1/\\alpha}$ and that the limit process is once again the solution of an (semi-linear) SDDE."],"url":"http://arxiv.org/abs/2402.08513v1","category":"math.PR"}
{"created":"2024-02-13 14:50:24","title":"A Physics-Informed Deep Learning Description of Knudsen Layer Reactivity Reduction","abstract":"A physics-informed neural network (PINN) is used to evaluate the fast ion distribution in the hot spot of an inertial confinement fusion target. The employment of customized input and output layers to the neural network are shown to enable a PINN to learn the parametric solution to the Vlasov-Fokker-Planck equation in the absence of any synthetic or experimental data. While the offline training of the PINN is computationally intensive, online deployment is rapid, thus enabling an efficient surrogate of the fast ion tail across a broad range of hot spot conditions. As an explicit demonstration of the approach, the specific problem of Knudsen layer fusion yield reduction is treated. Here, predictions from the Vlasov-Fokker-Planck PINN are used to provide a non-perturbative solution of the fast ion tail in the vicinity of the hot spot thus allowing the spatial profile of the fusion reactivity to be evaluated for a range of collisionalities and hot spot conditions. Excellent agreement is found between the predictions of the Vlasov-Fokker-Planck PINN and results from traditional numerical solvers with respect to both the energy and spatial distribution of fast ions and the fusion reactivity profile demonstrating that the Vlasov-Fokker-Planck PINN provides an accurate and efficient means of determining the impact of Knudsen layer yield reduction across a broad range of plasma conditions.","sentences":["A physics-informed neural network (PINN) is used to evaluate the fast ion distribution in the hot spot of an inertial confinement fusion target.","The employment of customized input and output layers to the neural network are shown to enable a PINN to learn the parametric solution to the Vlasov-Fokker-Planck equation in the absence of any synthetic or experimental data.","While the offline training of the PINN is computationally intensive, online deployment is rapid, thus enabling an efficient surrogate of the fast ion tail across a broad range of hot spot conditions.","As an explicit demonstration of the approach, the specific problem of Knudsen layer fusion yield reduction is treated.","Here, predictions from the Vlasov-Fokker-Planck PINN are used to provide a non-perturbative solution of the fast ion tail in the vicinity of the hot spot thus allowing the spatial profile of the fusion reactivity to be evaluated for a range of collisionalities and hot spot conditions.","Excellent agreement is found between the predictions of the Vlasov-Fokker-Planck PINN and results from traditional numerical solvers with respect to both the energy and spatial distribution of fast ions and the fusion reactivity profile demonstrating that the Vlasov-Fokker-Planck PINN provides an accurate and efficient means of determining the impact of Knudsen layer yield reduction across a broad range of plasma conditions."],"url":"http://arxiv.org/abs/2402.08495v1","category":"physics.plasm-ph"}
{"created":"2024-02-13 13:37:13","title":"Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products","abstract":"In the realm of Graph Neural Networks (GNNs), two exciting research directions have recently emerged: Subgraph GNNs and Graph Transformers. In this paper, we propose an architecture that integrates both approaches, dubbed Subgraphormer, which combines the enhanced expressive power, message-passing mechanisms, and aggregation schemes from Subgraph GNNs with attention and positional encodings, arguably the most important components in Graph Transformers. Our method is based on an intriguing new connection we reveal between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be formulated as Message Passing Neural Networks (MPNNs) operating on a product of the graph with itself. We use this formulation to design our architecture: first, we devise an attention mechanism based on the connectivity of the product graph. Following this, we propose a novel and efficient positional encoding scheme for Subgraph GNNs, which we derive as a positional encoding for the product graph. Our experimental results demonstrate significant performance improvements over both Subgraph GNNs and Graph Transformers on a wide range of datasets.","sentences":["In the realm of Graph Neural Networks (GNNs), two exciting research directions have recently emerged: Subgraph GNNs and Graph Transformers.","In this paper, we propose an architecture that integrates both approaches, dubbed Subgraphormer, which combines the enhanced expressive power, message-passing mechanisms, and aggregation schemes from Subgraph GNNs with attention and positional encodings, arguably the most important components in Graph Transformers.","Our method is based on an intriguing new connection we reveal between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be formulated as Message Passing Neural Networks (MPNNs) operating on a product of the graph with itself.","We use this formulation to design our architecture: first, we devise an attention mechanism based on the connectivity of the product graph.","Following this, we propose a novel and efficient positional encoding scheme for Subgraph GNNs, which we derive as a positional encoding for the product graph.","Our experimental results demonstrate significant performance improvements over both Subgraph GNNs and Graph Transformers on a wide range of datasets."],"url":"http://arxiv.org/abs/2402.08450v1","category":"cs.LG"}
{"created":"2024-02-13 13:07:34","title":"Camera Calibration through Geometric Constraints from Rotation and Projection Matrices","abstract":"The process of camera calibration involves estimating the intrinsic and extrinsic parameters, which are essential for accurately performing tasks such as 3D reconstruction, object tracking and augmented reality. In this work, we propose a novel constraints-based loss for measuring the intrinsic (focal length: $(f_x, f_y)$ and principal point: $(p_x, p_y)$) and extrinsic (baseline: ($b$), disparity: ($d$), translation: $(t_x, t_y, t_z)$, and rotation specifically pitch: $(\\theta_p)$) camera parameters. Our novel constraints are based on geometric properties inherent in the camera model, including the anatomy of the projection matrix (vanishing points, image of world origin, axis planes) and the orthonormality of the rotation matrix. Thus we proposed a novel Unsupervised Geometric Constraint Loss (UGCL) via a multitask learning framework. Our methodology is a hybrid approach that employs the learning power of a neural network to estimate the desired parameters along with the underlying mathematical properties inherent in the camera projection matrix. This distinctive approach not only enhances the interpretability of the model but also facilitates a more informed learning process. Additionally, we introduce a new CVGL Camera Calibration dataset, featuring over 900 configurations of camera parameters, incorporating 63,600 image pairs that closely mirror real-world conditions. By training and testing on both synthetic and real-world datasets, our proposed approach demonstrates improvements across all parameters when compared to the state-of-the-art (SOTA) benchmarks. The code and the updated dataset can be found here: https://github.com/CVLABLUMS/CVGL-Camera-Calibration","sentences":["The process of camera calibration involves estimating the intrinsic and extrinsic parameters, which are essential for accurately performing tasks such as 3D reconstruction, object tracking and augmented reality.","In this work, we propose a novel constraints-based loss for measuring the intrinsic (focal length: $(f_x, f_y)$ and principal point: $(p_x, p_y)$) and extrinsic (baseline: ($b$), disparity: ($d$), translation: $(t_x, t_y, t_z)$, and rotation specifically pitch: $(\\theta_p)$) camera parameters.","Our novel constraints are based on geometric properties inherent in the camera model, including the anatomy of the projection matrix (vanishing points, image of world origin, axis planes) and the orthonormality of the rotation matrix.","Thus we proposed a novel Unsupervised Geometric Constraint Loss (UGCL) via a multitask learning framework.","Our methodology is a hybrid approach that employs the learning power of a neural network to estimate the desired parameters along with the underlying mathematical properties inherent in the camera projection matrix.","This distinctive approach not only enhances the interpretability of the model but also facilitates a more informed learning process.","Additionally, we introduce a new CVGL Camera Calibration dataset, featuring over 900 configurations of camera parameters, incorporating 63,600 image pairs that closely mirror real-world conditions.","By training and testing on both synthetic and real-world datasets, our proposed approach demonstrates improvements across all parameters when compared to the state-of-the-art (SOTA) benchmarks.","The code and the updated dataset can be found here: https://github.com/CVLABLUMS/CVGL-Camera-Calibration"],"url":"http://arxiv.org/abs/2402.08437v1","category":"cs.CV"}
{"created":"2024-02-13 12:37:41","title":"$\\mathcal{N}=2$ SYK models with dynamical bosons and fermions","abstract":"We study a class of SYK models with $\\mathcal{N}=2$ supersymmetry, described by $N$ fermions in chiral Fermi multiplets, as well as $\\alpha N$ first-order bosons in chiral multiplets. The interactions are characterised by two integers $(p,q)$. We focus on the large $N$ and low energy limit of these models. Despite the presence of dynamical bosons, we find conformal behaviour akin to the standard SYK model. We use $\\mathcal{I}$-extremization of a Witten index to study the supersymmetric solutions. In particular, we find an exact expression for the entropy, which matches the numerical solutions to the Schwinger-Dyson equations. We further solve the model both in the large $p$ and large $p,q$ limits. Numerically, we verify our analytical results and obtain estimates for the Schwarzian coupling in the near zero-temperature limit. We also study the low-lying spectrum of operators to determine the parameter ranges where the Schwarzian mode dominates the IR dynamics. Lastly, we study out-of-time-ordered correlators to show that the model is maximally chaotic.","sentences":["We study a class of SYK models with $\\mathcal{N}=2$ supersymmetry, described by $N$ fermions in chiral Fermi multiplets, as well as $\\alpha N$ first-order bosons in chiral multiplets.","The interactions are characterised by two integers $(p,q)$. We focus on the large $N$ and low energy limit of these models.","Despite the presence of dynamical bosons, we find conformal behaviour akin to the standard SYK model.","We use $\\mathcal{I}$-extremization of a Witten index to study the supersymmetric solutions.","In particular, we find an exact expression for the entropy, which matches the numerical solutions to the Schwinger-Dyson equations.","We further solve the model both in the large $p$ and large $p,q$ limits.","Numerically, we verify our analytical results and obtain estimates for the Schwarzian coupling in the near zero-temperature limit.","We also study the low-lying spectrum of operators to determine the parameter ranges where the Schwarzian mode dominates the IR dynamics.","Lastly, we study out-of-time-ordered correlators to show that the model is maximally chaotic."],"url":"http://arxiv.org/abs/2402.08414v1","category":"hep-th"}
{"created":"2024-02-13 11:02:10","title":"Breakup of liquid jets: Thermodynamic perspectives","abstract":"Breakup of a liquid jet into a chain of droplets is common in nature and industry. Previous researchers developed profound mathematic and fluid dynamic models to address this breakup phenomenon starting from tiny perturbations. However, the morphological evolution of the jets at large amplitude perturbations is still an open question. Here, we report a concise thermodynamic model based on the surface area minimization principle. Our results demonstrate a reversible breakup transition from a continuous jet via droplets towards a uniform-radius cylinder, which has not been found previously, but is observed in our simulations. This new observation is attributed to a geometric constraint, which was often overlooked in former studies. We anticipate our model to be a shortcut to tackle many similar highly nonlinear morphological evolutions without solving abstruse fluid dynamic equations for inviscid fluids.","sentences":["Breakup of a liquid jet into a chain of droplets is common in nature and industry.","Previous researchers developed profound mathematic and fluid dynamic models to address this breakup phenomenon starting from tiny perturbations.","However, the morphological evolution of the jets at large amplitude perturbations is still an open question.","Here, we report a concise thermodynamic model based on the surface area minimization principle.","Our results demonstrate a reversible breakup transition from a continuous jet via droplets towards a uniform-radius cylinder, which has not been found previously, but is observed in our simulations.","This new observation is attributed to a geometric constraint, which was often overlooked in former studies.","We anticipate our model to be a shortcut to tackle many similar highly nonlinear morphological evolutions without solving abstruse fluid dynamic equations for inviscid fluids."],"url":"http://arxiv.org/abs/2402.08370v1","category":"physics.flu-dyn"}
{"created":"2024-02-13 10:54:55","title":"On solitary waves for the Korteweg--de Vries equation on metric star graphs","abstract":"We study the Korteweg--de Vries equation on a metric star graph and investigate existence of solitary waves on the metric graph in terms of the coefficients of the equation on each edge, the coupling condition at the central vertex of the star and the speeds of the travelling wave. We show that, with a continuity condition at the vertex, solitary waves can occur exactly when the parameters are chosen in a fairly special manner. We also consider coupling conditions beyond continuity.","sentences":["We study the Korteweg--de Vries equation on a metric star graph and investigate existence of solitary waves on the metric graph in terms of the coefficients of the equation on each edge, the coupling condition at the central vertex of the star and the speeds of the travelling wave.","We show that, with a continuity condition at the vertex, solitary waves can occur exactly when the parameters are chosen in a fairly special manner.","We also consider coupling conditions beyond continuity."],"url":"http://arxiv.org/abs/2402.08368v1","category":"math.AP"}
{"created":"2024-02-13 10:54:43","title":"RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks","abstract":"While many recent Physics-Informed Neural Networks (PINNs) variants have had considerable success in solving Partial Differential Equations, the empirical benefits of feature mapping drawn from the broader Neural Representations research have been largely overlooked. We highlight the limitations of widely used Fourier-based feature mapping in certain situations and suggest the use of the conditionally positive definite Radial Basis Function. The empirical findings demonstrate the effectiveness of our approach across a variety of forward and inverse problem cases. Our method can be seamlessly integrated into coordinate-based input neural networks and contribute to the wider field of PINNs research.","sentences":["While many recent Physics-Informed Neural Networks (PINNs) variants have had considerable success in solving Partial Differential Equations, the empirical benefits of feature mapping drawn from the broader Neural Representations research have been largely overlooked.","We highlight the limitations of widely used Fourier-based feature mapping in certain situations and suggest the use of the conditionally positive definite Radial Basis Function.","The empirical findings demonstrate the effectiveness of our approach across a variety of forward and inverse problem cases.","Our method can be seamlessly integrated into coordinate-based input neural networks and contribute to the wider field of PINNs research."],"url":"http://arxiv.org/abs/2402.08367v1","category":"cs.LG"}
{"created":"2024-02-13 10:53:26","title":"Small-scale dynamo with nonzero correlation time","abstract":"The small-scale dynamo is typically studied by assuming that the correlation time of the velocity field is zero. Some authors have used a smooth renovating flow model to study how the properties of the dynamo are affected by the correlation time being nonzero. Here, we assume the velocity is an incompressible Gaussian random field (which need not be smooth), and derive the lowest-order corrections to the evolution equation for the two-point correlation of the magnetic field in Fourier space. Using this, we obtain the evolution equation for the longitudinal correlation function of the magnetic field in nonhelical turbulence, valid for arbitrary Prandtl number. Even at high Prandtl number, the derived evolution equation is qualitatively different from that obtained from the renovating flow model. Further, the growth rate of the magnetic energy is much smaller. Nevertheless, the magnetic power spectrum still retains the Kazantsev form at high Prandtl number.","sentences":["The small-scale dynamo is typically studied by assuming that the correlation time of the velocity field is zero.","Some authors have used a smooth renovating flow model to study how the properties of the dynamo are affected by the correlation time being nonzero.","Here, we assume the velocity is an incompressible Gaussian random field (which need not be smooth), and derive the lowest-order corrections to the evolution equation for the two-point correlation of the magnetic field in Fourier space.","Using this, we obtain the evolution equation for the longitudinal correlation function of the magnetic field in nonhelical turbulence, valid for arbitrary Prandtl number.","Even at high Prandtl number, the derived evolution equation is qualitatively different from that obtained from the renovating flow model.","Further, the growth rate of the magnetic energy is much smaller.","Nevertheless, the magnetic power spectrum still retains the Kazantsev form at high Prandtl number."],"url":"http://arxiv.org/abs/2402.08366v1","category":"astro-ph.GA"}
{"created":"2024-02-13 10:19:33","title":"Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training","abstract":"Training Deep Neural Networks (DNNs) with small batches using Stochastic Gradient Descent (SGD) yields superior test performance compared to larger batches. The specific noise structure inherent to SGD is known to be responsible for this implicit bias. DP-SGD, used to ensure differential privacy (DP) in DNNs' training, adds Gaussian noise to the clipped gradients. Surprisingly, large-batch training still results in a significant decrease in performance, which poses an important challenge because strong DP guarantees necessitate the use of massive batches. We first show that the phenomenon extends to Noisy-SGD (DP-SGD without clipping), suggesting that the stochasticity (and not the clipping) is the cause of this implicit bias, even with additional isotropic Gaussian noise. We theoretically analyse the solutions obtained with continuous versions of Noisy-SGD for the Linear Least Square and Diagonal Linear Network settings, and reveal that the implicit bias is indeed amplified by the additional noise. Thus, the performance issues of large-batch DP-SGD training are rooted in the same underlying principles as SGD, offering hope for potential improvements in large batch training strategies.","sentences":["Training Deep Neural Networks (DNNs) with small batches using Stochastic Gradient Descent (SGD) yields superior test performance compared to larger batches.","The specific noise structure inherent to SGD is known to be responsible for this implicit bias.","DP-SGD, used to ensure differential privacy (DP) in DNNs' training, adds Gaussian noise to the clipped gradients.","Surprisingly, large-batch training still results in a significant decrease in performance, which poses an important challenge because strong DP guarantees necessitate the use of massive batches.","We first show that the phenomenon extends to Noisy-SGD (DP-SGD without clipping), suggesting that the stochasticity (and not the clipping) is the cause of this implicit bias, even with additional isotropic Gaussian noise.","We theoretically analyse the solutions obtained with continuous versions of Noisy-SGD for the Linear Least Square and Diagonal Linear Network settings, and reveal that the implicit bias is indeed amplified by the additional noise.","Thus, the performance issues of large-batch DP-SGD training are rooted in the same underlying principles as SGD, offering hope for potential improvements in large batch training strategies."],"url":"http://arxiv.org/abs/2402.08344v1","category":"stat.ML"}
{"created":"2024-02-13 09:19:33","title":"Invariant Monge-Amp\u00e8re equations on contactified para-K\u00e4hler manifolds","abstract":"We develop a method for describing invariant Monge-Amp\\`ere equations in the sense of V. Lychagin and T. Morimoto (MAE) on a homogeneous contact manifold $N$ of a semisimple Lie group $G$, which is the contactification of the homogeneous symplectic manifold $M = G/H = \\mathrm{Ad}_G Z \\subset \\mathfrak{g}$, where $M$ is the adjoint orbit of a splittable closed element $Z $ of the Lie algebra $\\mathfrak{g} = \\mathrm{Lie}(G)$. The method is then applied to a ten-dimensional semisimple orbit $M$ of the exceptional Lie group $\\mathsf{G}_2$ and a complete list of mutually non-equivalent MAEs on $N$ is obtained.","sentences":["We develop a method for describing invariant Monge-Amp\\`ere equations in the sense of V. Lychagin and T. Morimoto (MAE) on a homogeneous contact manifold $N$ of a semisimple Lie group $G$, which is the contactification of the homogeneous symplectic manifold $M = G/H = \\mathrm{Ad}_G Z \\subset \\mathfrak{g}$, where $M$ is the adjoint orbit of a splittable closed element $Z $ of the Lie algebra $\\mathfrak{g} = \\mathrm{Lie}(G)$. The method is then applied to a ten-dimensional semisimple orbit $M$ of the exceptional Lie group $\\mathsf{G}_2$ and a complete list of mutually non-equivalent MAEs on $N$ is obtained."],"url":"http://arxiv.org/abs/2402.08315v1","category":"math.DG"}
{"created":"2024-02-13 09:09:57","title":"The Method of Formal Series: Applications to Nonlinear Beam Dynamics and Invariants of Motion","abstract":"A novel technique to determine invariant curves in nonlinear beam dynamics based on the method of formal series has been developed. It is first shown how the solution of the Hamilton equations of motion describing nonlinear betatron oscillations in the presence of a single sextupole can be represented in a nonperturbative form. Further, the solution of the Hamilton-Jacobi equation is obtained in a closed symbolic form as a ratio of two series in the perturbation parameter (and the nonlinear action invariant), rather than a conventional power series according to canonical perturbation theory. It is well behaved even for large values of the perturbation parameter close to strong structural resonances. The relationship between existing invariant curves and the so-called scattering orbits in classical scattering theory has been revealed.","sentences":["A novel technique to determine invariant curves in nonlinear beam dynamics based on the method of formal series has been developed.","It is first shown how the solution of the Hamilton equations of motion describing nonlinear betatron oscillations in the presence of a single sextupole can be represented in a nonperturbative form.","Further, the solution of the Hamilton-Jacobi equation is obtained in a closed symbolic form as a ratio of two series in the perturbation parameter (and the nonlinear action invariant), rather than a conventional power series according to canonical perturbation theory.","It is well behaved even for large values of the perturbation parameter close to strong structural resonances.","The relationship between existing invariant curves and the so-called scattering orbits in classical scattering theory has been revealed."],"url":"http://arxiv.org/abs/2402.08308v1","category":"physics.acc-ph"}
{"created":"2024-02-13 08:42:06","title":"On a modified Hilbert transformation, the discrete inf-sup condition, and error estimates","abstract":"In this paper, we analyze the discrete inf-sup condition and related error estimates for a modified Hilbert transformation as used in the space-time discretization of time-dependent partial differential equations. It turns out that the stability constant depends linearly on the finite element mesh parameter, but in most cases, we can show optimal convergence. We present a series of numerical experiments which illustrate the theoretical findings.","sentences":["In this paper, we analyze the discrete inf-sup condition and related error estimates for a modified Hilbert transformation as used in the space-time discretization of time-dependent partial differential equations.","It turns out that the stability constant depends linearly on the finite element mesh parameter, but in most cases, we can show optimal convergence.","We present a series of numerical experiments which illustrate the theoretical findings."],"url":"http://arxiv.org/abs/2402.08291v1","category":"math.NA"}
{"created":"2024-02-13 08:07:14","title":"Rethinking U-net Skip Connections for Biomedical Image Segmentation","abstract":"The U-net architecture has significantly impacted deep learning-based segmentation of medical images. Through the integration of long-range skip connections, it facilitated the preservation of high-resolution features. Out-of-distribution data can, however, substantially impede the performance of neural networks. Previous works showed that the trained network layers differ in their susceptibility to this domain shift, e.g., shallow layers are more affected than deeper layers. In this work, we investigate the implications of this observation of layer sensitivity to domain shifts of U-net-style segmentation networks. By copying features of shallow layers to corresponding decoder blocks, these bear the risk of re-introducing domain-specific information. We used a synthetic dataset to model different levels of data distribution shifts and evaluated the impact on downstream segmentation performance. We quantified the inherent domain susceptibility of each network layer, using the Hellinger distance. These experiments confirmed the higher domain susceptibility of earlier network layers. When gradually removing skip connections, a decrease in domain susceptibility of deeper layers could be observed. For downstream segmentation performance, the original U-net outperformed the variant without any skip connections. The best performance, however, was achieved when removing the uppermost skip connection - not only in the presence of domain shifts but also for in-domain test data. We validated our results on three clinical datasets - two histopathology datasets and one magnetic resonance dataset - with performance increases of up to 10% in-domain and 13% cross-domain when removing the uppermost skip connection.","sentences":["The U-net architecture has significantly impacted deep learning-based segmentation of medical images.","Through the integration of long-range skip connections, it facilitated the preservation of high-resolution features.","Out-of-distribution data can, however, substantially impede the performance of neural networks.","Previous works showed that the trained network layers differ in their susceptibility to this domain shift, e.g., shallow layers are more affected than deeper layers.","In this work, we investigate the implications of this observation of layer sensitivity to domain shifts of U-net-style segmentation networks.","By copying features of shallow layers to corresponding decoder blocks, these bear the risk of re-introducing domain-specific information.","We used a synthetic dataset to model different levels of data distribution shifts and evaluated the impact on downstream segmentation performance.","We quantified the inherent domain susceptibility of each network layer, using the Hellinger distance.","These experiments confirmed the higher domain susceptibility of earlier network layers.","When gradually removing skip connections, a decrease in domain susceptibility of deeper layers could be observed.","For downstream segmentation performance, the original U-net outperformed the variant without any skip connections.","The best performance, however, was achieved when removing the uppermost skip connection - not only in the presence of domain shifts but also for in-domain test data.","We validated our results on three clinical datasets - two histopathology datasets and one magnetic resonance dataset - with performance increases of up to 10% in-domain and 13% cross-domain when removing the uppermost skip connection."],"url":"http://arxiv.org/abs/2402.08276v1","category":"eess.IV"}
{"created":"2024-02-13 04:42:34","title":"Dispersive and Strichartz estimates for 3D wave equation with a class of many-electric potentials","abstract":"We prove the dispersive and Strichartz estimates for solutions to the wave equation with a class of many-electric potentials in spatial dimension three. To obtain the desired dispersive estimate, based on the spectral properties of the Schr\\\"odinger operator involved, we subsequently prove the dispersive estimate for the corresponding Schr\\\"odinger semigroup, obtain a Gaussian type upper bound, establish Bernstein type inequalities, and finally pass to the M\\\"uller-Seeger's subordination formula. The desired Strichartz estimates follow by the established dispersive estimate and the standard argument of Keel-Tao.","sentences":["We prove the dispersive and Strichartz estimates for solutions to the wave equation with a class of many-electric potentials in spatial dimension three.","To obtain the desired dispersive estimate, based on the spectral properties of the Schr\\\"odinger operator involved, we subsequently prove the dispersive estimate for the corresponding Schr\\\"odinger semigroup, obtain a Gaussian type upper bound, establish Bernstein type inequalities, and finally pass to the M\\\"uller-Seeger's subordination formula.","The desired Strichartz estimates follow by the established dispersive estimate and the standard argument of Keel-Tao."],"url":"http://arxiv.org/abs/2402.08213v1","category":"math.AP"}
{"created":"2024-02-13 03:55:09","title":"Fine-Tuning Text-To-Image Diffusion Models for Class-Wise Spurious Feature Generation","abstract":"We propose a method for generating spurious features by leveraging large-scale text-to-image diffusion models. Although the previous work detects spurious features in a large-scale dataset like ImageNet and introduces Spurious ImageNet, we found that not all spurious images are spurious across different classifiers. Although spurious images help measure the reliance of a classifier, filtering many images from the Internet to find more spurious features is time-consuming. To this end, we utilize an existing approach of personalizing large-scale text-to-image diffusion models with available discovered spurious images and propose a new spurious feature similarity loss based on neural features of an adversarially robust model. Precisely, we fine-tune Stable Diffusion with several reference images from Spurious ImageNet with a modified objective incorporating the proposed spurious-feature similarity loss. Experiment results show that our method can generate spurious images that are consistently spurious across different classifiers. Moreover, the generated spurious images are visually similar to reference images from Spurious ImageNet.","sentences":["We propose a method for generating spurious features by leveraging large-scale text-to-image diffusion models.","Although the previous work detects spurious features in a large-scale dataset like ImageNet and introduces Spurious ImageNet, we found that not all spurious images are spurious across different classifiers.","Although spurious images help measure the reliance of a classifier, filtering many images from the Internet to find more spurious features is time-consuming.","To this end, we utilize an existing approach of personalizing large-scale text-to-image diffusion models with available discovered spurious images and propose a new spurious feature similarity loss based on neural features of an adversarially robust model.","Precisely, we fine-tune Stable Diffusion with several reference images from Spurious ImageNet with a modified objective incorporating the proposed spurious-feature similarity loss.","Experiment results show that our method can generate spurious images that are consistently spurious across different classifiers.","Moreover, the generated spurious images are visually similar to reference images from Spurious ImageNet."],"url":"http://arxiv.org/abs/2402.08200v1","category":"cs.CV"}
{"created":"2024-02-13 02:41:20","title":"Algebraic approach to maximum likelihood factor analysis","abstract":"In exploratory factor analysis, model parameters are usually estimated by maximum likelihood method. The maximum likelihood estimate is obtained by solving a complicated multivariate algebraic equation. Since the solution to the equation is usually intractable, it is typically computed with continuous optimization methods, such as Newton-Raphson methods. With this procedure, however, the solution is inevitably dependent on the estimation algorithm and initial value since the log-likelihood function is highly non-concave. Particularly, the estimates of unique variances can result in zero or negative, referred to as improper solutions; in this case, the maximum likelihood estimate can be severely unstable. To delve into the issue of the instability of the maximum likelihood estimate, we compute exact solutions to the multivariate algebraic equation by using algebraic computations. We provide a computationally efficient algorithm based on the algebraic computations specifically optimized for maximum likelihood factor analysis. To be specific, Gr\\\"oebner basis and cylindrical decomposition are employed, powerful tools for solving the multivariate algebraic equation. Our proposed procedure produces all exact solutions to the algebraic equation; therefore, these solutions are independent of the initial value and estimation algorithm. We conduct Monte Carlo simulations to investigate the characteristics of the maximum likelihood solutions.","sentences":["In exploratory factor analysis, model parameters are usually estimated by maximum likelihood method.","The maximum likelihood estimate is obtained by solving a complicated multivariate algebraic equation.","Since the solution to the equation is usually intractable, it is typically computed with continuous optimization methods, such as Newton-Raphson methods.","With this procedure, however, the solution is inevitably dependent on the estimation algorithm and initial value since the log-likelihood function is highly non-concave.","Particularly, the estimates of unique variances can result in zero or negative, referred to as improper solutions; in this case, the maximum likelihood estimate can be severely unstable.","To delve into the issue of the instability of the maximum likelihood estimate, we compute exact solutions to the multivariate algebraic equation by using algebraic computations.","We provide a computationally efficient algorithm based on the algebraic computations specifically optimized for maximum likelihood factor analysis.","To be specific, Gr\\\"oebner basis and cylindrical decomposition are employed, powerful tools for solving the multivariate algebraic equation.","Our proposed procedure produces all exact solutions to the algebraic equation; therefore, these solutions are independent of the initial value and estimation algorithm.","We conduct Monte Carlo simulations to investigate the characteristics of the maximum likelihood solutions."],"url":"http://arxiv.org/abs/2402.08181v1","category":"math.ST"}
{"created":"2024-02-13 01:01:46","title":"Effect of electron-phonon coupling on thermal transport in metals: a Monte Carlo approach for solving the coupled electron-phonon Boltzmann transport equations","abstract":"In this work, the effect of electron-phonon (e-ph) coupling on both electron and phonon transport of metals is investigated via first principles calculations. A Monte-Carlo (MC) approach for solving the coupled electron-phonon Boltzmann transport equations is developed to investigate thermal conductivity of metals. In this approach, the anisotropic electron band structure, phonon dispersion in the full Brillouin zone, and mode-dependent thermal relaxation time of electrons and phonons are calculated from first principles. Using this approach, MC simulations of coupled e-ph thermal transport at different temperatures in {\\alpha}-U and Ag are performed. These two materials were selected as a way to demonstrate the applicability of the method in different fields. The results indicate that the electron relaxation time due to phonon scattering is orders of magnitude smaller than the phonon relaxation time due to electron scattering. The results also show that, in phonon thermal transport, the impact of ph-e scattering is almost negligible and the ph-ph scattering dominates phonon transport. At high temperature, the electrons dominate thermal transport in both {\\alpha}-U and Ag. However, at low temperature, the phonon contribution to the total thermal conductivity of {\\alpha}-U is significant. Moreover, the Lorenz ratio deviates from the Sommerfeld value at low to intermediate temperatures, where the Wiedemann-Franz law is not applicable. Finally, we show that the Ag electronic thermal conductivity shows a stronger size effect than its phonon thermal conductivity.","sentences":["In this work, the effect of electron-phonon (e-ph) coupling on both electron and phonon transport of metals is investigated via first principles calculations.","A Monte-Carlo (MC) approach for solving the coupled electron-phonon Boltzmann transport equations is developed to investigate thermal conductivity of metals.","In this approach, the anisotropic electron band structure, phonon dispersion in the full Brillouin zone, and mode-dependent thermal relaxation time of electrons and phonons are calculated from first principles.","Using this approach, MC simulations of coupled e-ph thermal transport at different temperatures in {\\alpha}-U and Ag are performed.","These two materials were selected as a way to demonstrate the applicability of the method in different fields.","The results indicate that the electron relaxation time due to phonon scattering is orders of magnitude smaller than the phonon relaxation time due to electron scattering.","The results also show that, in phonon thermal transport, the impact of ph-e scattering is almost negligible and the ph-ph scattering dominates phonon transport.","At high temperature, the electrons dominate thermal transport in both {\\alpha}-U and Ag.","However, at low temperature, the phonon contribution to the total thermal conductivity of {\\alpha}-U is significant.","Moreover, the Lorenz ratio deviates from the Sommerfeld value at low to intermediate temperatures, where the Wiedemann-Franz law is not applicable.","Finally, we show that the Ag electronic thermal conductivity shows a stronger size effect than its phonon thermal conductivity."],"url":"http://arxiv.org/abs/2402.08150v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-13 00:57:19","title":"Constraints on Dark Matter-Dark Energy Scattering from ACT DR6 CMB Lensing","abstract":"The predicted present-day amplitude of matter fluctuations based on cosmic microwave background (CMB) anisotropy data has sometimes been found discrepant with more direct measurements of late-time structure. This has motivated many extensions to the standard cosmological model, including kinetic interactions between dark matter and dark energy that introduce a drag force slowing the growth of structure at late times. Exploring this scenario, we develop a model for quasi-linear scales in the matter power spectrum by calculating the critical overdensity in the presence of this interaction and a varying dark energy equation of state. We explicitly avoid modeling or interpretation of data on non-linear scales in this model (such as use of $\\Lambda$CDM-calibrated priors), which would require numerical simulations. We find that the presence of the drag force hinders halo formation, thus increasing the deviation from $\\Lambda$CDM in the mildly non-linear regime. We use CMB lensing observations from the sixth data release of the Atacama Cosmology Telescope up to $L=1250$ (in combination with Planck, Sloan Digital Sky Survey, and 6dFGS data) to derive the strongest constraints to date on the amplitude of the drag term, finding the dimensionless interaction strength $\\Gamma_\\mathrm{DMDE}/(H_0\\rho_\\mathrm{c})<0.831\\; (2.81)$ at the 68\\% (95\\%) confidence level. The inclusion of non-linear corrections improves our constraints by about 35\\% compared to linear theory. Our results do not exclude the best-fit values of $\\Gamma_\\mathrm{DMDE}$ found in previous studies using information from galaxy weak lensing, though we find no statistical preference for the dark matter-dark energy kinetic interactions over $\\Lambda$CDM. We implement our model in a publicly available fork of the Boltzmann code CLASS at https://github.com/fmccarthy/Class_DMDE.","sentences":["The predicted present-day amplitude of matter fluctuations based on cosmic microwave background (CMB) anisotropy data has sometimes been found discrepant with more direct measurements of late-time structure.","This has motivated many extensions to the standard cosmological model, including kinetic interactions between dark matter and dark energy that introduce a drag force slowing the growth of structure at late times.","Exploring this scenario, we develop a model for quasi-linear scales in the matter power spectrum by calculating the critical overdensity in the presence of this interaction and a varying dark energy equation of state.","We explicitly avoid modeling or interpretation of data on non-linear scales in this model (such as use of $\\Lambda$CDM-calibrated priors), which would require numerical simulations.","We find that the presence of the drag force hinders halo formation, thus increasing the deviation from $\\Lambda$CDM in the mildly non-linear regime.","We use CMB lensing observations from the sixth data release of the Atacama Cosmology Telescope up to $L=1250$ (in combination with Planck, Sloan Digital Sky Survey, and 6dFGS data) to derive the strongest constraints to date on the amplitude of the drag term, finding the dimensionless interaction strength $\\Gamma_\\mathrm{DMDE}/(H_0\\rho_\\mathrm{c})<0.831\\; (2.81)$ at the 68\\% (95\\%) confidence level.","The inclusion of non-linear corrections improves our constraints by about 35\\% compared to linear theory.","Our results do not exclude the best-fit values of $\\Gamma_\\mathrm{DMDE}$ found in previous studies using information from galaxy weak lensing, though we find no statistical preference for the dark matter-dark energy kinetic interactions over $\\Lambda$CDM.","We implement our model in a publicly available fork of the Boltzmann code CLASS at https://github.com/fmccarthy/Class_DMDE."],"url":"http://arxiv.org/abs/2402.08149v1","category":"astro-ph.CO"}
{"created":"2024-02-13 00:23:31","title":"H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields","abstract":"Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D indoor scene reconstruction. We introduce a novel two-phase learning approach, H2O-SDF, that discriminates between object and non-object regions within indoor environments. This method achieves a nuanced balance, carefully preserving the geometric integrity of room layouts while also capturing intricate surface details of specific objects. A cornerstone of our two-phase learning framework is the introduction of the Object Surface Field (OSF), a novel concept designed to mitigate the persistent vanishing gradient problem that has previously hindered the capture of high-frequency details in other methods. Our proposed approach is validated through several experiments that include ablation studies.","sentences":["Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D indoor scene reconstruction.","We introduce a novel two-phase learning approach, H2O-SDF, that discriminates between object and non-object regions within indoor environments.","This method achieves a nuanced balance, carefully preserving the geometric integrity of room layouts while also capturing intricate surface details of specific objects.","A cornerstone of our two-phase learning framework is the introduction of the Object Surface Field (OSF), a novel concept designed to mitigate the persistent vanishing gradient problem that has previously hindered the capture of high-frequency details in other methods.","Our proposed approach is validated through several experiments that include ablation studies."],"url":"http://arxiv.org/abs/2402.08138v1","category":"cs.CV"}
{"created":"2024-02-12 23:29:20","title":"Transport property predictions for laser resonance chromatography on Rf$^+$ (Z = 104)","abstract":"We propose a theoretically designed laser resonance chromatography (LRC) experiment on Rf$^+$ (Z = 104) drifting in He buffer gas. To this end, we first developed a four-level rate equation model that simulates the optical pumping of Rf$^+$ from its ground state, $^2$D$_{3/2}$ (7s$^2$6d$^1$), to the metastable $^4$F$_{3/2}$ (7s$^1$6d$^2$) state via laser resonant excitation of the intermediate $^4$F$_{3/2}$ (7s$^1$6d$^1$7p$^1$) state prior to electronic state chromatography. This model predicts a 93\\% pumping efficiency that suffices to enable efficient laser resonance chromatography of this ion. We then performed accurate relativistic Multi-Reference Configuration-Interaction (MRCI) calculations to model the interaction of Rf$^+$ with He in the ground $^2$D$_{3/2}$ (7s$^2$6d$^1$), low-lying $^2$D$_{5/2}$ (7s$^2$6d$^1$), and metastable $^4$F$_{3/2}$ (7s$^1$6d$^2$) states. These ion-atom interaction potentials were used to calculate the state-specific ion mobilities. For gas temperatures above 100 K and small applied electric fields, the reduced ion mobilities of the ground and metastable states differ significantly. In particular, at room temperature the difference between the reduced ion mobilities of these states is larger than 11%, and therefore should be sufficiently large to ensure LRC of this ion.","sentences":["We propose a theoretically designed laser resonance chromatography (LRC) experiment on Rf$^+$","(","Z = 104) drifting in He buffer gas.","To this end, we first developed a four-level rate equation model that simulates the optical pumping of Rf$^+$ from its ground state, $^2$D$_{3/2}$ (7s$^2$6d$^1$), to the metastable $^4$F$_{3/2}$ (7s$^1$6d$^2$) state via laser resonant excitation of the intermediate $^4$F$_{3/2}$ (7s$^1$6d$^1$7p$^1$) state prior to electronic state chromatography.","This model predicts a 93\\% pumping efficiency that suffices to enable efficient laser resonance chromatography of this ion.","We then performed accurate relativistic Multi-Reference Configuration-Interaction (MRCI) calculations to model the interaction of Rf$^+$ with He in the ground $^2$D$_{3/2}$ (7s$^2$6d$^1$), low-lying $^2$D$_{5/2}$ (7s$^2$6d$^1$), and metastable $^4$F$_{3/2}$ (7s$^1$6d$^2$) states.","These ion-atom interaction potentials were used to calculate the state-specific ion mobilities.","For gas temperatures above 100 K and small applied electric fields, the reduced ion mobilities of the ground and metastable states differ significantly.","In particular, at room temperature the difference between the reduced ion mobilities of these states is larger than 11%, and therefore should be sufficiently large to ensure LRC of this ion."],"url":"http://arxiv.org/abs/2402.08120v1","category":"physics.atom-ph"}
{"created":"2024-02-12 23:07:36","title":"On timelike Bonnet surfaces in Lorentzian 3-manifold","abstract":"In this paper we generalized a result of Soley Ersoy and Kemal Eren [10] about Bonnet timelike surface in Minkowski 3-space. We give a necessary and sufficient condition for a surface M in a Lorentzian 3-space to be timelike Bonnet surface. At the end, a theorem of classification of timelike Bonnet surface in a Lorentzian 3-space is given.","sentences":["In this paper we generalized a result of Soley Ersoy and Kemal Eren","[10] about Bonnet timelike surface in Minkowski 3-space.","We give a necessary and sufficient condition for a surface M in a Lorentzian 3-space to be timelike Bonnet surface.","At the end, a theorem of classification of timelike Bonnet surface in a Lorentzian 3-space is given."],"url":"http://arxiv.org/abs/2402.08111v1","category":"math.DG"}
{"created":"2024-02-12 22:41:08","title":"Simultaneous real and momentum space electron diffraction from a fullerene molecule","abstract":"Plane-wave electrons undergo momentum transfer as they scatter off a target in overlapping spherical waves. The transferred momentum leads to target structural information to be encoded in angle and energy differential scattering. For symmetric, periodic or structured targets this can engender diffraction in the electron intensity both in real (angular) and in momentum space. With the example of elastic scattering from C60 we show this simultaneous manifestation of diffraction signatures. The simulated angle-momentum diffractograms can be imaged in experiments with a two-dimensional detector and an energy-tunable electron gun. The result may inspire invention of technology to extend scopes of electron diffraction from molecules and nanostructures, open a direction of electron crystallography using the momentum-differential diffraction, and motivate an approach to control the time delay between the pump laser-pulse and the probe electron-pulse by tuning the electron impact-speed in ultrafast electron diffraction experiments.","sentences":["Plane-wave electrons undergo momentum transfer as they scatter off a target in overlapping spherical waves.","The transferred momentum leads to target structural information to be encoded in angle and energy differential scattering.","For symmetric, periodic or structured targets this can engender diffraction in the electron intensity both in real (angular) and in momentum space.","With the example of elastic scattering from C60 we show this simultaneous manifestation of diffraction signatures.","The simulated angle-momentum diffractograms can be imaged in experiments with a two-dimensional detector and an energy-tunable electron gun.","The result may inspire invention of technology to extend scopes of electron diffraction from molecules and nanostructures, open a direction of electron crystallography using the momentum-differential diffraction, and motivate an approach to control the time delay between the pump laser-pulse and the probe electron-pulse by tuning the electron impact-speed in ultrafast electron diffraction experiments."],"url":"http://arxiv.org/abs/2402.08103v1","category":"physics.atm-clus"}
{"created":"2024-02-12 22:34:57","title":"Automated Classification of Body MRI Sequence Type Using Convolutional Neural Networks","abstract":"Multi-parametric MRI of the body is routinely acquired for the identification of abnormalities and diagnosis of diseases. However, a standard naming convention for the MRI protocols and associated sequences does not exist due to wide variations in imaging practice at institutions and myriad MRI scanners from various manufacturers being used for imaging. The intensity distributions of MRI sequences differ widely as a result, and there also exists information conflicts related to the sequence type in the DICOM headers. At present, clinician oversight is necessary to ensure that the correct sequence is being read and used for diagnosis. This poses a challenge when specific series need to be considered for building a cohort for a large clinical study or for developing AI algorithms. In order to reduce clinician oversight and ensure the validity of the DICOM headers, we propose an automated method to classify the 3D MRI sequence acquired at the levels of the chest, abdomen, and pelvis. In our pilot work, our 3D DenseNet-121 model achieved an F1 score of 99.5% at differentiating 5 common MRI sequences obtained by three Siemens scanners (Aera, Verio, Biograph mMR). To the best of our knowledge, we are the first to develop an automated method for the 3D classification of MRI sequences in the chest, abdomen, and pelvis, and our work has outperformed the previous state-of-the-art MRI series classifiers.","sentences":["Multi-parametric MRI of the body is routinely acquired for the identification of abnormalities and diagnosis of diseases.","However, a standard naming convention for the MRI protocols and associated sequences does not exist due to wide variations in imaging practice at institutions and myriad MRI scanners from various manufacturers being used for imaging.","The intensity distributions of MRI sequences differ widely as a result, and there also exists information conflicts related to the sequence type in the DICOM headers.","At present, clinician oversight is necessary to ensure that the correct sequence is being read and used for diagnosis.","This poses a challenge when specific series need to be considered for building a cohort for a large clinical study or for developing AI algorithms.","In order to reduce clinician oversight and ensure the validity of the DICOM headers, we propose an automated method to classify the 3D MRI sequence acquired at the levels of the chest, abdomen, and pelvis.","In our pilot work, our 3D DenseNet-121 model achieved an F1 score of 99.5% at differentiating 5 common MRI sequences obtained by three Siemens scanners (Aera, Verio, Biograph mMR).","To the best of our knowledge, we are the first to develop an automated method for the 3D classification of MRI sequences in the chest, abdomen, and pelvis, and our work has outperformed the previous state-of-the-art MRI series classifiers."],"url":"http://arxiv.org/abs/2402.08098v1","category":"eess.IV"}
{"created":"2024-02-12 21:59:11","title":"Reverse design of the ideal pulse for hollow capillary fiber post-compression schemes","abstract":"The countless applications of ultrashort laser pulses in very different scientific areas explain the ongoing efforts to develop new strategies for the generation of light pulses with increasingly better characteristics. In this work, we theoretically study the application of the nonlinear reverse propagation method to produce few-cycle pulses with clean temporal profiles from standard post-compression setups based in hollow capillary fibers. By numerically solving the propagation of a desired goal pulse in the backward direction, we are able to predict the structure of the ideal input pulse that could be perfectly compressed in a given setup. Although the goal pulse cannot be chosen in a simple manner due to the fundamental symmetries of the nonlinear propagation equation, our analysis shows that the ideal pulse presents a recurring form and that, in general, both its intensity profile and phase must be shaped to recover the optimized goal output.","sentences":["The countless applications of ultrashort laser pulses in very different scientific areas explain the ongoing efforts to develop new strategies for the generation of light pulses with increasingly better characteristics.","In this work, we theoretically study the application of the nonlinear reverse propagation method to produce few-cycle pulses with clean temporal profiles from standard post-compression setups based in hollow capillary fibers.","By numerically solving the propagation of a desired goal pulse in the backward direction, we are able to predict the structure of the ideal input pulse that could be perfectly compressed in a given setup.","Although the goal pulse cannot be chosen in a simple manner due to the fundamental symmetries of the nonlinear propagation equation, our analysis shows that the ideal pulse presents a recurring form and that, in general, both its intensity profile and phase must be shaped to recover the optimized goal output."],"url":"http://arxiv.org/abs/2402.08081v1","category":"physics.optics"}
{"created":"2024-02-12 21:47:24","title":"ReNeLiB: Real-time Neural Listening Behavior Generation for Socially Interactive Agents","abstract":"Flexible and natural nonverbal reactions to human behavior remain a challenge for socially interactive agents (SIAs) that are predominantly animated using hand-crafted rules. While recently proposed machine learning based approaches to conversational behavior generation are a promising way to address this challenge, they have not yet been employed in SIAs. The primary reason for this is the lack of a software toolkit integrating such approaches with SIA frameworks that conforms to the challenging real-time requirements of human-agent interaction scenarios. In our work, we for the first time present such a toolkit consisting of three main components: (1) real-time feature extraction capturing multi-modal social cues from the user; (2) behavior generation based on a recent state-of-the-art neural network approach; (3) visualization of the generated behavior supporting both FLAME-based and Apple ARKit-based interactive agents. We comprehensively evaluate the real-time performance of the whole framework and its components. In addition, we introduce pre-trained behavioral generation models derived from psychotherapy sessions for domain-specific listening behaviors. Our software toolkit, pivotal for deploying and assessing SIAs' listening behavior in real-time, is publicly available. Resources, including code, behavioural multi-modal features extracted from therapeutic interactions, are hosted at \\url{https://daksitha.github.io/ReNeLib}","sentences":["Flexible and natural nonverbal reactions to human behavior remain a challenge for socially interactive agents (SIAs) that are predominantly animated using hand-crafted rules.","While recently proposed machine learning based approaches to conversational behavior generation are a promising way to address this challenge, they have not yet been employed in SIAs.","The primary reason for this is the lack of a software toolkit integrating such approaches with SIA frameworks that conforms to the challenging real-time requirements of human-agent interaction scenarios.","In our work, we for the first time present such a toolkit consisting of three main components: (1) real-time feature extraction capturing multi-modal social cues from the user; (2) behavior generation based on a recent state-of-the-art neural network approach; (3) visualization of the generated behavior supporting both FLAME-based and Apple ARKit-based interactive agents.","We comprehensively evaluate the real-time performance of the whole framework and its components.","In addition, we introduce pre-trained behavioral generation models derived from psychotherapy sessions for domain-specific listening behaviors.","Our software toolkit, pivotal for deploying and assessing SIAs' listening behavior in real-time, is publicly available.","Resources, including code, behavioural multi-modal features extracted from therapeutic interactions, are hosted at \\url{https://daksitha.github.io/ReNeLib}"],"url":"http://arxiv.org/abs/2402.08079v1","category":"cs.HC"}
{"created":"2024-02-12 20:49:27","title":"Learned infinite elements for helioseismology -- Learning transparent boundary conditions for the solar atmosphere","abstract":"Context. Acoustic waves in the Sun are affected by the atmospheric layers, but this region is often ignored in forward models due to the increase in computational cost. Aims. The purpose of this work is to take into account the solar atmosphere without increasing significantly the computational cost. Methods. We solve a scalar wave equation that describes the propagation of acoustic modes inside the Sun using a finite element method. The boundary conditions used to truncate the computational domain are learned from the Dirichlet-to-Neumann operator, that is the relation between the solution and its normal derivative at the computational boundary. These boundary conditions may be applied at any height above which the background medium is assumed to be radially symmetric. Results. Taking into account the atmosphere is important even for wave frequencies below the acoustic cut-off. In particular, the mode frequencies computed for an isothermal atmosphere differ by up 10 {\\mu}Hz from those computed for the VAL-C atmospheric model. We show that learned infinite elements lead to a numerical accuracy similar to that obtained for a traditional radiation boundary condition. Its main advantage is to reproduce the solution for any radially symmetric atmosphere to a very good accuracy at a low computational cost. Conclusions. This work emphasizes the importance of including atmospheric layers in helioseismology and proposes a computationally efficient method to do so.","sentences":["Context.","Acoustic waves in the Sun are affected by the atmospheric layers, but this region is often ignored in forward models due to the increase in computational cost.","Aims.","The purpose of this work is to take into account the solar atmosphere without increasing significantly the computational cost.","Methods.","We solve a scalar wave equation that describes the propagation of acoustic modes inside the Sun using a finite element method.","The boundary conditions used to truncate the computational domain are learned from the Dirichlet-to-Neumann operator, that is the relation between the solution and its normal derivative at the computational boundary.","These boundary conditions may be applied at any height above which the background medium is assumed to be radially symmetric.","Results.","Taking into account the atmosphere is important even for wave frequencies below the acoustic cut-off.","In particular, the mode frequencies computed for an isothermal atmosphere differ by up 10 {\\mu}Hz from those computed for the VAL-C atmospheric model.","We show that learned infinite elements lead to a numerical accuracy similar to that obtained for a traditional radiation boundary condition.","Its main advantage is to reproduce the solution for any radially symmetric atmosphere to a very good accuracy at a low computational cost.","Conclusions.","This work emphasizes the importance of including atmospheric layers in helioseismology and proposes a computationally efficient method to do so."],"url":"http://arxiv.org/abs/2402.08059v1","category":"astro-ph.SR"}
{"created":"2024-02-12 20:15:14","title":"Elasticity between manifolds: the weakly-incompatible limit","abstract":"Elasticity between manifolds concerns the study of the \"most isometric\" way of immersing one Riemannian manifold $(M,g)$ (the elastic body) into another $(S,s)$ (ambient space). We study the limit problem of weak-incompatibility: We obtain, via $\\Gamma$-convergence, a limit model for a sequence of bodies $(M,g_\\varepsilon)$ when $g_\\varepsilon \\to s$, and relate the minimum of the limit energy to a linearized curvature discrepancy between $g_\\varepsilon$ and $s$, using recent results of Kupferman and Leder. The main technical challenge is obtaining the correct notion of displacement for manifold-valued configurations, using Sobolev truncations and parallel transport. We further show that the associated compactness result is obtained if $(S,s)$ satisfies a quantitative rigidity property, analogous to the Friesecke--James--M\\\"uller rigidity estimate in Euclidean space, and show that this property holds when $(S,s)$ is a round sphere.","sentences":["Elasticity between manifolds concerns the study of the \"most isometric\" way of immersing one Riemannian manifold $(M,g)$ (the elastic body) into another $(S,s)$ (ambient space).","We study the limit problem of weak-incompatibility: We obtain, via $\\Gamma$-convergence, a limit model for a sequence of bodies $(M,g_\\varepsilon)$ when $g_\\varepsilon \\to s$, and relate the minimum of the limit energy to a linearized curvature discrepancy between $g_\\varepsilon$ and $s$, using recent results of Kupferman and Leder.","The main technical challenge is obtaining the correct notion of displacement for manifold-valued configurations, using Sobolev truncations and parallel transport.","We further show that the associated compactness result is obtained if $(S,s)$ satisfies a quantitative rigidity property, analogous to the Friesecke--James--M\\\"uller rigidity estimate in Euclidean space, and show that this property holds when $(S,s)$ is a round sphere."],"url":"http://arxiv.org/abs/2402.08041v1","category":"math.AP"}
{"created":"2024-02-12 20:11:40","title":"Physics informed data-driven near-wall modelling for lattice Boltzmann simulation of high Reynolds number turbulent flows","abstract":"Data-driven approaches offer novel opportunities for improving the performance of turbulent flow simulations, which are critical to wide-ranging applications from wind farms and aerodynamic designs to weather and climate forecasting. While conventional continuum Navier-Stokes solvers have been the subject of a significant amount of work in this domain, there has hitherto been very limited effort in the same direction for the more scalable and highly performant lattice Boltzmann method (LBM), even though it has been successfully applied to a variety of turbulent flow simulations using large-eddy simulation (LES) techniques. In this work, we establish a data-driven framework for the LES-based lattice Boltzmann simulation of near-wall turbulent flow fields. We do this by training neural networks using improved delayed detached eddy simulation data. Crucially, this is done in combination with physics-based information that substantially constrains the data-driven predictions. Using data from turbulent channel flow at a friction Reynolds number at $5200$, our simulations accurately predict the behaviour of the wall model at arbitrary friction Reynolds numbers up to $1.0 \\times 10^6$. In contradistinction with other models that use direct numerical simulation datasets, our physics-informed model requires data from very limited regions within the wall-bounded plane, reducing by three orders of magnitude the quantity of data needed for training. We also demonstrate that our model can handle data configurations when the near-wall grid is sparse. Our physics-informed neural network approach opens up the possibility of employing LBM in combination with highly specific and therefore much more limited quantities of macroscopic data, substantially facilitating the investigation of a wide-range of turbulent flow applications at very large scale.","sentences":["Data-driven approaches offer novel opportunities for improving the performance of turbulent flow simulations, which are critical to wide-ranging applications from wind farms and aerodynamic designs to weather and climate forecasting.","While conventional continuum Navier-Stokes solvers have been the subject of a significant amount of work in this domain, there has hitherto been very limited effort in the same direction for the more scalable and highly performant lattice Boltzmann method (LBM), even though it has been successfully applied to a variety of turbulent flow simulations using large-eddy simulation (LES) techniques.","In this work, we establish a data-driven framework for the LES-based lattice Boltzmann simulation of near-wall turbulent flow fields.","We do this by training neural networks using improved delayed detached eddy simulation data.","Crucially, this is done in combination with physics-based information that substantially constrains the data-driven predictions.","Using data from turbulent channel flow at a friction Reynolds number at $5200$, our simulations accurately predict the behaviour of the wall model at arbitrary friction Reynolds numbers up to $1.0 \\times 10^6$.","In contradistinction with other models that use direct numerical simulation datasets, our physics-informed model requires data from very limited regions within the wall-bounded plane, reducing by three orders of magnitude the quantity of data needed for training.","We also demonstrate that our model can handle data configurations when the near-wall grid is sparse.","Our physics-informed neural network approach opens up the possibility of employing LBM in combination with highly specific and therefore much more limited quantities of macroscopic data, substantially facilitating the investigation of a wide-range of turbulent flow applications at very large scale."],"url":"http://arxiv.org/abs/2402.08037v1","category":"physics.flu-dyn"}
{"created":"2024-02-12 20:06:08","title":"Stochastic Integration of the Cahn-Hilliard Phase Field Equations","abstract":"In this work we develop a stochastic algorithm to integrate the Cahn-Hilliard equations. The algorithm is based on Gillespie's stochastic simulation algorithm, also known as kinetic Monte Carlo. The deterministic integration of the phase field equations leads to the closest minimum of free energy and does not overcome free energy barriers. However, in the nucleation and growth regime of the phase diagram, free energy barriers need to be thermally overcome for the system to phase separate to reach equilibrium. We show in this work that the proposed stochastic integration algorithm indeed allows the system to overcome free energy barriers. We discuss the results in terms of fluctuation distributions, grid sizes and efficiency.","sentences":["In this work we develop a stochastic algorithm to integrate the Cahn-Hilliard equations.","The algorithm is based on Gillespie's stochastic simulation algorithm, also known as kinetic Monte Carlo.","The deterministic integration of the phase field equations leads to the closest minimum of free energy and does not overcome free energy barriers.","However, in the nucleation and growth regime of the phase diagram, free energy barriers need to be thermally overcome for the system to phase separate to reach equilibrium.","We show in this work that the proposed stochastic integration algorithm indeed allows the system to overcome free energy barriers.","We discuss the results in terms of fluctuation distributions, grid sizes and efficiency."],"url":"http://arxiv.org/abs/2402.08034v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-12 19:57:15","title":"Dumviri: Detecting Trackers and Mixed Trackers with a Breakage Detector","abstract":"Previous automatic tracker detection work lacks features to recognize web page breakage and often resort to manual analysis to assess the breakage caused by blocking trackers.   We introduce Dumviri, which incorporates a breakage detector that can automatically detect web page breakage caused by erroneously blocking a resource that is needed by the page to function properly. This addition allows Dumviri to prevent functional resources from being misclassified as trackers and increases overall detection accuracy. We designed Dumviri to take differential features. We further find that these features are agnostic to analysis granularity and enable Dumviri to predict tracking resources at the request field granularity, allowing Dumviri to handle some mixed trackers.   Evaluating Dumviri on 15K pages shows its ability to replicate the labels of human-generated filter lists with an accuracy of 97.44%. Through a manual analysis, we found that Dumviri identified previously unreported trackers and its breakage detector can identify rules that cause web page breakage in commonly used filter lists like EasyPrivacy. In the case of mixed trackers, Dumviri, being the first automated mixed tracker detector, achieves a 79.09% accuracy. We have confirmed 22 previously unreported unique trackers and 26 unique mixed trackers. We promptly reported these findings to privacy developers, and we will publish our filter lists in uBlock Origin's extended syntax.","sentences":["Previous automatic tracker detection work lacks features to recognize web page breakage and often resort to manual analysis to assess the breakage caused by blocking trackers.   ","We introduce Dumviri, which incorporates a breakage detector that can automatically detect web page breakage caused by erroneously blocking a resource that is needed by the page to function properly.","This addition allows Dumviri to prevent functional resources from being misclassified as trackers and increases overall detection accuracy.","We designed Dumviri to take differential features.","We further find that these features are agnostic to analysis granularity and enable Dumviri to predict tracking resources at the request field granularity, allowing Dumviri to handle some mixed trackers.   ","Evaluating Dumviri on 15K pages shows its ability to replicate the labels of human-generated filter lists with an accuracy of 97.44%.","Through a manual analysis, we found that Dumviri identified previously unreported trackers and its breakage detector can identify rules that cause web page breakage in commonly used filter lists like EasyPrivacy.","In the case of mixed trackers, Dumviri, being the first automated mixed tracker detector, achieves a 79.09% accuracy.","We have confirmed 22 previously unreported unique trackers and 26 unique mixed trackers.","We promptly reported these findings to privacy developers, and we will publish our filter lists in uBlock Origin's extended syntax."],"url":"http://arxiv.org/abs/2402.08031v1","category":"cs.CR"}
{"created":"2024-02-12 19:27:30","title":"Nearest Neighbour Score Estimators for Diffusion Generative Models","abstract":"Score function estimation is the cornerstone of both training and sampling from diffusion generative models. Despite this fact, the most commonly used estimators are either biased neural network approximations or high variance Monte Carlo estimators based on the conditional score. We introduce a novel nearest neighbour score function estimator which utilizes multiple samples from the training set to dramatically decrease estimator variance. We leverage our low variance estimator in two compelling applications. Training consistency models with our estimator, we report a significant increase in both convergence speed and sample quality. In diffusion models, we show that our estimator can replace a learned network for probability-flow ODE integration, opening promising new avenues of future research.","sentences":["Score function estimation is the cornerstone of both training and sampling from diffusion generative models.","Despite this fact, the most commonly used estimators are either biased neural network approximations or high variance Monte Carlo estimators based on the conditional score.","We introduce a novel nearest neighbour score function estimator which utilizes multiple samples from the training set to dramatically decrease estimator variance.","We leverage our low variance estimator in two compelling applications.","Training consistency models with our estimator, we report a significant increase in both convergence speed and sample quality.","In diffusion models, we show that our estimator can replace a learned network for probability-flow ODE integration, opening promising new avenues of future research."],"url":"http://arxiv.org/abs/2402.08018v1","category":"cs.LG"}
{"created":"2024-02-12 19:21:14","title":"Online Differentially Private Synthetic Data Generation","abstract":"We present a polynomial-time algorithm for online differentially private synthetic data generation. For a data stream within the hypercube $[0,1]^d$ and an infinite time horizon, we develop an online algorithm that generates a differentially private synthetic dataset at each time $t$. This algorithm achieves a near-optimal accuracy bound of $O(t^{-1/d}\\log(t))$ for $d\\geq 2$ and $O(t^{-1}\\log^{4.5}(t))$ for $d=1$ in the 1-Wasserstein distance. This result generalizes the previous work on the continual release model for counting queries to include Lipschitz queries. Compared to the offline case, where the entire dataset is available at once, our approach requires only an extra polylog factor in the accuracy bound.","sentences":["We present a polynomial-time algorithm for online differentially private synthetic data generation.","For a data stream within the hypercube $[0,1]^d$ and an infinite time horizon, we develop an online algorithm that generates a differentially private synthetic dataset at each time $t$. This algorithm achieves a near-optimal accuracy bound of $O(t^{-1/d}\\log(t))$ for $d\\geq 2$ and $O(t^{-1}\\log^{4.5}(t))$ for $d=1$ in the 1-Wasserstein distance.","This result generalizes the previous work on the continual release model for counting queries to include Lipschitz queries.","Compared to the offline case, where the entire dataset is available at once, our approach requires only an extra polylog factor in the accuracy bound."],"url":"http://arxiv.org/abs/2402.08012v1","category":"math.ST"}
{"created":"2024-02-12 19:04:32","title":"NetInfoF Framework: Measuring and Exploiting Network Usable Information","abstract":"Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network (GNN) will perform well? More specifically, do the graph structure and the node features carry enough usable information for the task? Our goals are (1) to develop a fast tool to measure how much information is in the graph structure and in the node features, and (2) to exploit the information to solve the task, if there is enough. We propose NetInfoF, a framework including NetInfoF_Probe and NetInfoF_Act, for the measurement and the exploitation of network usable information (NUI), respectively. Given a graph data, NetInfoF_Probe measures NUI without any model training, and NetInfoF_Act solves link prediction and node classification, while two modules share the same backbone. In summary, NetInfoF has following notable advantages: (a) General, handling both link prediction and node classification; (b) Principled, with theoretical guarantee and closed-form solution; (c) Effective, thanks to the proposed adjustment to node similarity; (d) Scalable, scaling linearly with the input size. In our carefully designed synthetic datasets, NetInfoF correctly identifies the ground truth of NUI and is the only method being robust to all graph scenarios. Applied on real-world datasets, NetInfoF wins in 11 out of 12 times on link prediction compared to general GNN baselines.","sentences":["Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network (GNN) will perform well?","More specifically, do the graph structure and the node features carry enough usable information for the task?","Our goals are (1) to develop a fast tool to measure how much information is in the graph structure and in the node features, and (2) to exploit the information to solve the task, if there is enough.","We propose NetInfoF, a framework including NetInfoF_Probe and NetInfoF_Act, for the measurement and the exploitation of network usable information (NUI), respectively.","Given a graph data, NetInfoF_Probe measures NUI without any model training, and NetInfoF_Act solves link prediction and node classification, while two modules share the same backbone.","In summary, NetInfoF has following notable advantages: (a) General, handling both link prediction and node classification; (b) Principled, with theoretical guarantee and closed-form solution; (c) Effective, thanks to the proposed adjustment to node similarity; (d) Scalable, scaling linearly with the input size.","In our carefully designed synthetic datasets, NetInfoF correctly identifies the ground truth of NUI and is the only method being robust to all graph scenarios.","Applied on real-world datasets, NetInfoF wins in 11 out of 12 times on link prediction compared to general GNN baselines."],"url":"http://arxiv.org/abs/2402.07999v1","category":"cs.LG"}
{"created":"2024-02-12 19:01:57","title":"How the Galaxy-Halo Connection Depends on Large-Scale Environment","abstract":"We investigate the connection between galaxies, dark matter halos, and their large-scale environments with Illustris TNG300 hydrodynamic simulation data. We predict stellar masses from subhalo properties to test two types of machine learning (ML) models: Explainable Boosting Machines (EBMs) with simple galaxy environment features and E$(3)$-invariant graph neural networks (GNNs). The best-performing EBM models leverage spherically averaged overdensity features on $3$ Mpc scales. Interpretations via SHapley Additive exPlanations (SHAP) also suggest that, in the context of the TNG300 galaxy--halo connection, simple spherical overdensity on $\\sim 3$ Mpc scales is more important than cosmic web distance features measured using the DisPerSE algorithm. Meanwhile, a GNN with connectivity defined by a fixed linking length, $L$, outperforms the EBM models by a significant margin. As we increase the linking length scale, GNNs learn important environmental contributions up to the largest scales we probe ($L = 10$ Mpc). We conclude that $3$ Mpc distance scales are most critical for describing the TNG galaxy--halo connection using the spherical overdensity parameterization but that information on larger scales, which is not captured by simple environmental parameters or cosmic web features, can further augment these models. Our study highlights the benefits of using interpretable ML algorithms to explain models of astrophysical phenomena, and the power of using GNNs to flexibly learn complex relationships directly from data while imposing constraints from physical symmetries.","sentences":["We investigate the connection between galaxies, dark matter halos, and their large-scale environments with Illustris TNG300 hydrodynamic simulation data.","We predict stellar masses from subhalo properties to test two types of machine learning (ML) models: Explainable Boosting Machines (EBMs) with simple galaxy environment features and E$(3)$-invariant graph neural networks (GNNs).","The best-performing EBM models leverage spherically averaged overdensity features on $3$ Mpc scales.","Interpretations via SHapley Additive exPlanations (SHAP) also suggest that, in the context of the TNG300 galaxy--halo connection, simple spherical overdensity on $\\sim 3$ Mpc scales is more important than cosmic web distance features measured using the DisPerSE algorithm.","Meanwhile, a GNN with connectivity defined by a fixed linking length, $L$, outperforms the EBM models by a significant margin.","As we increase the linking length scale, GNNs learn important environmental contributions up to the largest scales we probe ($L = 10$ Mpc).","We conclude that $3$ Mpc distance scales are most critical for describing the TNG galaxy--halo connection using the spherical overdensity parameterization but that information on larger scales, which is not captured by simple environmental parameters or cosmic web features, can further augment these models.","Our study highlights the benefits of using interpretable ML algorithms to explain models of astrophysical phenomena, and the power of using GNNs to flexibly learn complex relationships directly from data while imposing constraints from physical symmetries."],"url":"http://arxiv.org/abs/2402.07995v1","category":"astro-ph.GA"}
{"created":"2024-02-12 19:00:03","title":"Cosmological Dynamics from Covariant Loop Quantum Gravity with Scalar Matter","abstract":"We study homogenous and isotropic quantum cosmology using the spinfoam formalism of Loop Quantum Gravity (LQG). We define a coupling of a scalar field to the 4-dimensional Lorentzian Engle-Pereira-Rovelli-Livine (EPRL) spinfoam model. We employ the numerical method of complex critical points to investigate the model on two different simplicial complexes: the triangulations of a single hypercube and two connected hypercubes. We find nontrivial implications for the effective cosmological dynamics. In the single-hypercube model, the numerical results suggest an effective Friedmann equation with a scalar density that contains higher-order derivatives and a scalar potential. The scalar potential plays a role similar to a positive cosmological constant and drives an accelerated expansion of the universe. The double-hypercubes model resembles a symmetric cosmic bounce, and a similar effective Friedmann equation emerges with higher-order derivative terms in the effective scalar density, whereas the scalar potential becomes negligible.","sentences":["We study homogenous and isotropic quantum cosmology using the spinfoam formalism of Loop Quantum Gravity (LQG).","We define a coupling of a scalar field to the 4-dimensional Lorentzian Engle-Pereira-Rovelli-Livine (EPRL) spinfoam model.","We employ the numerical method of complex critical points to investigate the model on two different simplicial complexes: the triangulations of a single hypercube and two connected hypercubes.","We find nontrivial implications for the effective cosmological dynamics.","In the single-hypercube model, the numerical results suggest an effective Friedmann equation with a scalar density that contains higher-order derivatives and a scalar potential.","The scalar potential plays a role similar to a positive cosmological constant and drives an accelerated expansion of the universe.","The double-hypercubes model resembles a symmetric cosmic bounce, and a similar effective Friedmann equation emerges with higher-order derivative terms in the effective scalar density, whereas the scalar potential becomes negligible."],"url":"http://arxiv.org/abs/2402.07984v1","category":"gr-qc"}
{"created":"2024-02-12 19:00:01","title":"Self-similarity of the third type in ultra relativistic blastwaves","abstract":"A new type of self-similarity is found in the problem of a plane-parallel, ultra-relativistic blast wave, propagating in a powerlaw density profile of the form $\\rho \\propto z^{-k}$. Self-similar solutions of the first kind can be found for $k<7/4$ using dimensional considerations. For steeper density gradients with $k>2$, second type solutions are obtained by eliminating a singularity from the equations. However, for intermediate powerlaw indices $7/4<k<2$ the flow does not obey any of the known types of self-similarity. Instead, the solutions belong to a new class in which the self-similar dynamics are dictated by the non self-similar part of the flow. We obtain an exact solution to the ultra-relativistic fluid equations and find that the non self-similar flow is described by relativistic expansion into vacuum, composed of (1) an accelerating piston that contains most of the energy and (2) a leading edge of fast material that coincides with the interiors of the blastwave and terminates at the shock. The dynamics of the piston itself are self-similar and universal, and do not depend on the external medium. The exact solution of the non self-similar flow is used to solve for the shock in the new class of solutions.","sentences":["A new type of self-similarity is found in the problem of a plane-parallel, ultra-relativistic blast wave, propagating in a powerlaw density profile of the form $\\rho \\propto z^{-k}$. Self-similar solutions of the first kind can be found for $k<7/4$ using dimensional considerations.","For steeper density gradients with $k>2$, second type solutions are obtained by eliminating a singularity from the equations.","However, for intermediate powerlaw indices $7/4<k<2$ the flow does not obey any of the known types of self-similarity.","Instead, the solutions belong to a new class in which the self-similar dynamics are dictated by the non self-similar part of the flow.","We obtain an exact solution to the ultra-relativistic fluid equations and find that the non self-similar flow is described by relativistic expansion into vacuum, composed of (1) an accelerating piston that contains most of the energy and (2) a leading edge of fast material that coincides with the interiors of the blastwave and terminates at the shock.","The dynamics of the piston itself are self-similar and universal, and do not depend on the external medium.","The exact solution of the non self-similar flow is used to solve for the shock in the new class of solutions."],"url":"http://arxiv.org/abs/2402.07978v1","category":"astro-ph.HE"}
{"created":"2024-02-12 19:00:00","title":"Quasicrystalline Spin Liquid","abstract":"The interplay of electronic interactions and frustration in crystalline systems leads to a panoply of correlated phases, including exotic Mott insulators with non-trivial patterns of entanglement. Disorder introduces additional quantum interference effects that can drive localization phenomena. Quasicrystals, which are neither disordered nor perfectly crystalline, are interesting playgrounds for studying the effects of interaction, frustration, and quantum interference. Here we consider a solvable example of a quantum spin liquid on a tri-coordinated quasicrystal. We extend Kitaev's original construction for the spin model to our quasicrystalline setting and perform a large scale flux-sampling to find the ground-state configuration in terms of the emergent majorana fermions and flux excitations. This reveals a fully gapped quantum spin liquid, regardless of the exchange anisotropies, accompanied by a tendency towards non-trivial (de-)localization at the edge and the bulk. The advent of moir\\'e materials and a variety of quantum simulators provide a new platform to bring phases of quasicrystalline quantum matter to life in a controlled fashion.","sentences":["The interplay of electronic interactions and frustration in crystalline systems leads to a panoply of correlated phases, including exotic Mott insulators with non-trivial patterns of entanglement.","Disorder introduces additional quantum interference effects that can drive localization phenomena.","Quasicrystals, which are neither disordered nor perfectly crystalline, are interesting playgrounds for studying the effects of interaction, frustration, and quantum interference.","Here we consider a solvable example of a quantum spin liquid on a tri-coordinated quasicrystal.","We extend Kitaev's original construction for the spin model to our quasicrystalline setting and perform a large scale flux-sampling to find the ground-state configuration in terms of the emergent majorana fermions and flux excitations.","This reveals a fully gapped quantum spin liquid, regardless of the exchange anisotropies, accompanied by a tendency towards non-trivial (de-)localization at the edge and the bulk.","The advent of moir\\'e materials and a variety of quantum simulators provide a new platform to bring phases of quasicrystalline quantum matter to life in a controlled fashion."],"url":"http://arxiv.org/abs/2402.07971v1","category":"cond-mat.str-el"}
{"created":"2024-02-13 18:56:55","title":"A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification","abstract":"Motivated by the recent application of approximate message passing (AMP) to the analysis of convex optimizations in multi-class classifications [Loureiro, et. al., 2021], we present a convergence analysis of AMP dynamics with non-separable multivariate nonlinearities. As an application, we present a complete (and independent) analysis of the motivated convex optimization problem.","sentences":["Motivated by the recent application of approximate message passing (AMP) to the analysis of convex optimizations in multi-class classifications [Loureiro, et. al., 2021], we present a convergence analysis of AMP dynamics with non-separable multivariate nonlinearities.","As an application, we present a complete (and independent) analysis of the motivated convex optimization problem."],"url":"http://arxiv.org/abs/2402.08676v1","category":"cs.LG"}
{"created":"2024-02-13 18:48:28","title":"Target Score Matching","abstract":"Denoising Score Matching estimates the score of a noised version of a target distribution by minimizing a regression loss and is widely used to train the popular class of Denoising Diffusion Models. A well known limitation of Denoising Score Matching, however, is that it yields poor estimates of the score at low noise levels. This issue is particularly unfavourable for problems in the physical sciences and for Monte Carlo sampling tasks for which the score of the clean original target is known. Intuitively, estimating the score of a slightly noised version of the target should be a simple task in such cases. In this paper, we address this shortcoming and show that it is indeed possible to leverage knowledge of the target score. We present a Target Score Identity and corresponding Target Score Matching regression loss which allows us to obtain score estimates admitting favourable properties at low noise levels.","sentences":["Denoising Score Matching estimates the score of a noised version of a target distribution by minimizing a regression loss and is widely used to train the popular class of Denoising Diffusion Models.","A well known limitation of Denoising Score Matching, however, is that it yields poor estimates of the score at low noise levels.","This issue is particularly unfavourable for problems in the physical sciences and for Monte Carlo sampling tasks for which the score of the clean original target is known.","Intuitively, estimating the score of a slightly noised version of the target should be a simple task in such cases.","In this paper, we address this shortcoming and show that it is indeed possible to leverage knowledge of the target score.","We present a Target Score Identity and corresponding Target Score Matching regression loss which allows us to obtain score estimates admitting favourable properties at low noise levels."],"url":"http://arxiv.org/abs/2402.08667v1","category":"cs.LG"}
{"created":"2024-02-13 18:46:10","title":"Learning Emergent Gaits with Decentralized Phase Oscillators: on the role of Observations, Rewards, and Feedback","abstract":"We present a minimal phase oscillator model for learning quadrupedal locomotion. Each of the four oscillators is coupled only to itself and its corresponding leg through local feedback of the ground reaction force, which can be interpreted as an observer feedback gain. We interpret the oscillator itself as a latent contact state-estimator. Through a systematic ablation study, we show that the combination of phase observations, simple phase-based rewards, and the local feedback dynamics induces policies that exhibit emergent gait preferences, while using a reduced set of simple rewards, and without prescribing a specific gait. The code is open-source, and a video synopsis available at https://youtu.be/1NKQ0rSV3jU.","sentences":["We present a minimal phase oscillator model for learning quadrupedal locomotion.","Each of the four oscillators is coupled only to itself and its corresponding leg through local feedback of the ground reaction force, which can be interpreted as an observer feedback gain.","We interpret the oscillator itself as a latent contact state-estimator.","Through a systematic ablation study, we show that the combination of phase observations, simple phase-based rewards, and the local feedback dynamics induces policies that exhibit emergent gait preferences, while using a reduced set of simple rewards, and without prescribing a specific gait.","The code is open-source, and a video synopsis available at https://youtu.be/1NKQ0rSV3jU."],"url":"http://arxiv.org/abs/2402.08662v1","category":"cs.RO"}
{"created":"2024-02-13 18:20:04","title":"Learned Image Compression with Text Quality Enhancement","abstract":"Learned image compression has gained widespread popularity for their efficiency in achieving ultra-low bit-rates. Yet, images containing substantial textual content, particularly screen-content images (SCI), often suffers from text distortion at such compressed levels. To address this, we propose to minimize a novel text logit loss designed to quantify the disparity in text between the original and reconstructed images, thereby improving the perceptual quality of the reconstructed text. Through rigorous experimentation across diverse datasets and employing state-of-the-art algorithms, our findings reveal significant enhancements in the quality of reconstructed text upon integration of the proposed loss function with appropriate weighting. Notably, we achieve a Bjontegaard delta (BD) rate of -32.64% for Character Error Rate (CER) and -28.03% for Word Error Rate (WER) on average by applying the text logit loss for two screenshot datasets. Additionally, we present quantitative metrics tailored for evaluating text quality in image compression tasks. Our findings underscore the efficacy and potential applicability of our proposed text logit loss function across various text-aware image compression contexts.","sentences":["Learned image compression has gained widespread popularity for their efficiency in achieving ultra-low bit-rates.","Yet, images containing substantial textual content, particularly screen-content images (SCI), often suffers from text distortion at such compressed levels.","To address this, we propose to minimize a novel text logit loss designed to quantify the disparity in text between the original and reconstructed images, thereby improving the perceptual quality of the reconstructed text.","Through rigorous experimentation across diverse datasets and employing state-of-the-art algorithms, our findings reveal significant enhancements in the quality of reconstructed text upon integration of the proposed loss function with appropriate weighting.","Notably, we achieve a Bjontegaard delta (BD) rate of -32.64% for Character Error Rate (CER) and -28.03% for Word Error Rate (WER) on average by applying the text logit loss for two screenshot datasets.","Additionally, we present quantitative metrics tailored for evaluating text quality in image compression tasks.","Our findings underscore the efficacy and potential applicability of our proposed text logit loss function across various text-aware image compression contexts."],"url":"http://arxiv.org/abs/2402.08643v1","category":"cs.CV"}
{"created":"2024-02-13 17:32:59","title":"Adjustment Identification Distance: A gadjid for Causal Structure Learning","abstract":"Evaluating graphs learned by causal discovery algorithms is difficult: The number of edges that differ between two graphs does not reflect how the graphs differ with respect to the identifying formulas they suggest for causal effects. We introduce a framework for developing causal distances between graphs which includes the structural intervention distance for directed acyclic graphs as a special case. We use this framework to develop improved adjustment-based distances as well as extensions to completed partially directed acyclic graphs and causal orders. We develop polynomial-time reachability algorithms to compute the distances efficiently. In our package gadjid (open source at https://github.com/CausalDisco/gadjid), we provide implementations of our distances; they are orders of magnitude faster than the structural intervention distance and thereby provide a success metric for causal discovery that scales to graph sizes that were previously prohibitive.","sentences":["Evaluating graphs learned by causal discovery algorithms is difficult: The number of edges that differ between two graphs does not reflect how the graphs differ with respect to the identifying formulas they suggest for causal effects.","We introduce a framework for developing causal distances between graphs which includes the structural intervention distance for directed acyclic graphs as a special case.","We use this framework to develop improved adjustment-based distances as well as extensions to completed partially directed acyclic graphs and causal orders.","We develop polynomial-time reachability algorithms to compute the distances efficiently.","In our package gadjid (open source at https://github.com/CausalDisco/gadjid), we provide implementations of our distances; they are orders of magnitude faster than the structural intervention distance and thereby provide a success metric for causal discovery that scales to graph sizes that were previously prohibitive."],"url":"http://arxiv.org/abs/2402.08616v1","category":"stat.ML"}
{"created":"2024-02-13 16:44:02","title":"Faster Repeated Evasion Attacks in Tree Ensembles","abstract":"Tree ensembles are one of the most widely used model classes. However, these models are susceptible to adversarial examples, i.e., slightly perturbed examples that elicit a misprediction. There has been significant research on designing approaches to construct such examples for tree ensembles. But this is a computationally challenging problem that often must be solved a large number of times (e.g., for all examples in a training set). This is compounded by the fact that current approaches attempt to find such examples from scratch. In contrast, we exploit the fact that multiple similar problems are being solved. Specifically, our approach exploits the insight that adversarial examples for tree ensembles tend to perturb a consistent but relatively small set of features. We show that we can quickly identify this set of features and use this knowledge to speedup constructing adversarial examples.","sentences":["Tree ensembles are one of the most widely used model classes.","However, these models are susceptible to adversarial examples, i.e., slightly perturbed examples that elicit a misprediction.","There has been significant research on designing approaches to construct such examples for tree ensembles.","But this is a computationally challenging problem that often must be solved a large number of times (e.g., for all examples in a training set).","This is compounded by the fact that current approaches attempt to find such examples from scratch.","In contrast, we exploit the fact that multiple similar problems are being solved.","Specifically, our approach exploits the insight that adversarial examples for tree ensembles tend to perturb a consistent but relatively small set of features.","We show that we can quickly identify this set of features and use this knowledge to speedup constructing adversarial examples."],"url":"http://arxiv.org/abs/2402.08586v1","category":"cs.LG"}
{"created":"2024-02-13 16:28:28","title":"Test-Time Backdoor Attacks on Multimodal Large Language Models","abstract":"Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing a new challenge for defending against backdoor attacks. Our project page is available at https://sail-sg.github.io/AnyDoor/.","sentences":["Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase.","In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data.","AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects.","In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies.","Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing a new challenge for defending against backdoor attacks.","Our project page is available at https://sail-sg.github.io/AnyDoor/."],"url":"http://arxiv.org/abs/2402.08577v1","category":"cs.CL"}
{"created":"2024-02-13 16:24:57","title":"Regret Minimization in Stackelberg Games with Side Information","abstract":"In its most basic form, a Stackelberg game is a two-player game in which a leader commits to a (mixed) strategy, and a follower best-responds. Stackelberg games are perhaps one of the biggest success stories of algorithmic game theory over the last decade, as algorithms for playing in Stackelberg games have been deployed in many real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader then commits to a (possibly context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on the online setting in which a sequence of followers arrive over time, and the context may change from round-to-round. In sharp contrast to the non-contextual version, we show that it is impossible for the leader to achieve good performance (measured by regret) in the full adversarial setting (i.e., when both the context and the follower are chosen by an adversary). However, it turns out that a little bit of randomness goes a long way. Motivated by our impossibility result, we show that no-regret learning is possible in two natural relaxations: the setting in which the sequence of followers is chosen stochastically and the sequence of contexts is adversarial, and the setting in which the sequence of contexts is stochastic and the sequence of followers is chosen by an adversary.","sentences":["In its most basic form, a Stackelberg game is a two-player game in which a leader commits to a (mixed) strategy, and a follower best-responds.","Stackelberg games are perhaps one of the biggest success stories of algorithmic game theory over the last decade, as algorithms for playing in Stackelberg games have been deployed in many real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention.","However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies.","We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing.","The leader then commits to a (possibly context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context.","We focus on the online setting in which a sequence of followers arrive over time, and the context may change from round-to-round.","In sharp contrast to the non-contextual version, we show that it is impossible for the leader to achieve good performance (measured by regret) in the full adversarial setting (i.e., when both the context and the follower are chosen by an adversary).","However, it turns out that a little bit of randomness goes a long way.","Motivated by our impossibility result, we show that no-regret learning is possible in two natural relaxations: the setting in which the sequence of followers is chosen stochastically and the sequence of contexts is adversarial, and the setting in which the sequence of contexts is stochastic and the sequence of followers is chosen by an adversary."],"url":"http://arxiv.org/abs/2402.08576v1","category":"cs.GT"}
{"created":"2024-02-13 15:24:19","title":"Benchmarking multi-component signal processing methods in the time-frequency plane","abstract":"Signal processing in the time-frequency plane has a long history and remains a field of methodological innovation. For instance, detection and denoising based on the zeros of the spectrogram have been proposed since 2015, contrasting with a long history of focusing on larger values of the spectrogram. Yet, unlike neighboring fields like optimization and machine learning, time-frequency signal processing lacks widely-adopted benchmarking tools. In this work, we contribute an open-source, Python-based toolbox termed MCSM-Benchs for benchmarking multi-component signal analysis methods, and we demonstrate our toolbox on three time-frequency benchmarks. First, we compare different methods for signal detection based on the zeros of the spectrogram, including unexplored variations of previously proposed detection tests. Second, we compare zero-based denoising methods to both classical and novel methods based on large values and ridges of the spectrogram. Finally, we compare the denoising performance of these methods against typical spectrogram thresholding strategies, in terms of post-processing artifacts commonly referred to as musical noise. At a low level, the obtained results provide new insight on the assessed approaches, and in particular research directions to further develop zero-based methods. At a higher level, our benchmarks exemplify the benefits of using a public, collaborative, common framework for benchmarking.","sentences":["Signal processing in the time-frequency plane has a long history and remains a field of methodological innovation.","For instance, detection and denoising based on the zeros of the spectrogram have been proposed since 2015, contrasting with a long history of focusing on larger values of the spectrogram.","Yet, unlike neighboring fields like optimization and machine learning, time-frequency signal processing lacks widely-adopted benchmarking tools.","In this work, we contribute an open-source, Python-based toolbox termed MCSM-Benchs for benchmarking multi-component signal analysis methods, and we demonstrate our toolbox on three time-frequency benchmarks.","First, we compare different methods for signal detection based on the zeros of the spectrogram, including unexplored variations of previously proposed detection tests.","Second, we compare zero-based denoising methods to both classical and novel methods based on large values and ridges of the spectrogram.","Finally, we compare the denoising performance of these methods against typical spectrogram thresholding strategies, in terms of post-processing artifacts commonly referred to as musical noise.","At a low level, the obtained results provide new insight on the assessed approaches, and in particular research directions to further develop zero-based methods.","At a higher level, our benchmarks exemplify the benefits of using a public, collaborative, common framework for benchmarking."],"url":"http://arxiv.org/abs/2402.08521v1","category":"eess.SP"}
{"created":"2024-02-13 13:43:02","title":"Indicators for characterising online hate speech and its automatic detection","abstract":"We examined four case studies in the context of hate speech on Twitter in Italian from 2019 to 2020, aiming at comparing the classification of the 3,600 tweets made by expert pedagogists with the automatic classification made by machine learning algorithms. Pedagogists used a novel classification scheme based on seven indicators that characterize hate. These indicators are: the content is public, it affects a target group, it contains hate speech in explicit verbal form, it will not redeem, it has intention to harm, it can have a possible violent response, it incites hatred and violence. The case studies refer to Jews, Muslims, Roma, and immigrants target groups. We find that not all the types of hateful content are equally detectable by the machine learning algorithms that we considered. In particular, algorithms perform better in identifying tweets that incite hatred and violence, and those that can have possible violent response.","sentences":["We examined four case studies in the context of hate speech on Twitter in Italian from 2019 to 2020, aiming at comparing the classification of the 3,600 tweets made by expert pedagogists with the automatic classification made by machine learning algorithms.","Pedagogists used a novel classification scheme based on seven indicators that characterize hate.","These indicators are: the content is public, it affects a target group, it contains hate speech in explicit verbal form, it will not redeem, it has intention to harm, it can have a possible violent response, it incites hatred and violence.","The case studies refer to Jews, Muslims, Roma, and immigrants target groups.","We find that not all the types of hateful content are equally detectable by the machine learning algorithms that we considered.","In particular, algorithms perform better in identifying tweets that incite hatred and violence, and those that can have possible violent response."],"url":"http://arxiv.org/abs/2402.08462v1","category":"cs.SI"}
{"created":"2024-02-13 12:53:18","title":"Frequency-aware Graph Signal Processing for Collaborative Filtering","abstract":"Graph Signal Processing (GSP) based recommendation algorithms have recently attracted lots of attention due to its high efficiency. However, these methods failed to consider the importance of various interactions that reflect unique user/item characteristics and failed to utilize user and item high-order neighborhood information to model user preference, thus leading to sub-optimal performance. To address the above issues, we propose a frequency-aware graph signal processing method (FaGSP) for collaborative filtering. Firstly, we design a Cascaded Filter Module, consisting of an ideal high-pass filter and an ideal low-pass filter that work in a successive manner, to capture both unique and common user/item characteristics to more accurately model user preference. Then, we devise a Parallel Filter Module, consisting of two low-pass filters that can easily capture the hierarchy of neighborhood, to fully utilize high-order neighborhood information of users/items for more accurate user preference modeling. Finally, we combine these two modules via a linear model to further improve recommendation accuracy. Extensive experiments on six public datasets demonstrate the superiority of our method from the perspectives of prediction accuracy and training efficiency compared with state-of-the-art GCN-based recommendation methods and GSP-based recommendation methods.","sentences":["Graph Signal Processing (GSP) based recommendation algorithms have recently attracted lots of attention due to its high efficiency.","However, these methods failed to consider the importance of various interactions that reflect unique user/item characteristics and failed to utilize user and item high-order neighborhood information to model user preference, thus leading to sub-optimal performance.","To address the above issues, we propose a frequency-aware graph signal processing method (FaGSP) for collaborative filtering.","Firstly, we design a Cascaded Filter Module, consisting of an ideal high-pass filter and an ideal low-pass filter that work in a successive manner, to capture both unique and common user/item characteristics to more accurately model user preference.","Then, we devise a Parallel Filter Module, consisting of two low-pass filters that can easily capture the hierarchy of neighborhood, to fully utilize high-order neighborhood information of users/items for more accurate user preference modeling.","Finally, we combine these two modules via a linear model to further improve recommendation accuracy.","Extensive experiments on six public datasets demonstrate the superiority of our method from the perspectives of prediction accuracy and training efficiency compared with state-of-the-art GCN-based recommendation methods and GSP-based recommendation methods."],"url":"http://arxiv.org/abs/2402.08426v1","category":"cs.IR"}
{"created":"2024-02-13 11:58:08","title":"Deep Learning-based Real-time Smartphone Pose Detection for Ultra-wideband Tagless Gate","abstract":"As commercial interest in proximity services increased, the development of various wireless localization techniques was promoted. In line with this trend, Ultra-wideband (UWB) is emerging as a promising solution that can realize proximity services thanks to centimeter-level localization accuracy. In addition, since the actual location of the mobile device (MD) on the human body, called pose, affects the localization accuracy, poses are also important to provide accurate proximity services, especially for the UWB tagless gate (UTG). In this paper, a real-time pose detector, termed D3, is proposed to estimate the pose of MD when users pass through UTG. D3 is based on line-of-sight (LOS) and non-LOS (NLOS) classification using UWB channel impulse response and utilizes the inertial measurement unit embedded in the smartphone to estimate the pose. D3 is implemented on Samsung Galaxy Note20 Ultra (i.e., SMN986B) and Qorvo UWB board to show the feasibility and applicability. D3 achieved an LOS/NLOS classification accuracy of 0.984, and ultimately detected four different poses of MD with an accuracy of 0.961 in real-time.","sentences":["As commercial interest in proximity services increased, the development of various wireless localization techniques was promoted.","In line with this trend, Ultra-wideband (UWB) is emerging as a promising solution that can realize proximity services thanks to centimeter-level localization accuracy.","In addition, since the actual location of the mobile device (MD) on the human body, called pose, affects the localization accuracy, poses are also important to provide accurate proximity services, especially for the UWB tagless gate (UTG).","In this paper, a real-time pose detector, termed D3, is proposed to estimate the pose of MD when users pass through UTG.","D3 is based on line-of-sight (LOS) and non-LOS (NLOS) classification using UWB channel impulse response and utilizes the inertial measurement unit embedded in the smartphone to estimate the pose.","D3 is implemented on Samsung Galaxy Note20 Ultra (i.e., SMN986B) and Qorvo UWB board to show the feasibility and applicability.","D3 achieved an LOS/NLOS classification accuracy of 0.984, and ultimately detected four different poses of MD with an accuracy of 0.961 in real-time."],"url":"http://arxiv.org/abs/2402.08399v1","category":"eess.SP"}
{"created":"2024-02-13 10:40:10","title":"Learning to Produce Semi-dense Correspondences for Visual Localization","abstract":"This study addresses the challenge of performing visual localization in demanding conditions such as night-time scenarios, adverse weather, and seasonal changes. While many prior studies have focused on improving image-matching performance to facilitate reliable dense keypoint matching between images, existing methods often heavily rely on predefined feature points on a reconstructed 3D model. Consequently, they tend to overlook unobserved keypoints during the matching process. Therefore, dense keypoint matches are not fully exploited, leading to a notable reduction in accuracy, particularly in noisy scenes. To tackle this issue, we propose a novel localization method that extracts reliable semi-dense 2D-3D matching points based on dense keypoint matches. This approach involves regressing semi-dense 2D keypoints into 3D scene coordinates using a point inference network. The network utilizes both geometric and visual cues to effectively infer 3D coordinates for unobserved keypoints from the observed ones. The abundance of matching information significantly enhances the accuracy of camera pose estimation, even in scenarios involving noisy or sparse 3D models. Comprehensive evaluations demonstrate that the proposed method outperforms other methods in challenging scenes and achieves competitive results in large-scale visual localization benchmarks. The code will be available.","sentences":["This study addresses the challenge of performing visual localization in demanding conditions such as night-time scenarios, adverse weather, and seasonal changes.","While many prior studies have focused on improving image-matching performance to facilitate reliable dense keypoint matching between images, existing methods often heavily rely on predefined feature points on a reconstructed 3D model.","Consequently, they tend to overlook unobserved keypoints during the matching process.","Therefore, dense keypoint matches are not fully exploited, leading to a notable reduction in accuracy, particularly in noisy scenes.","To tackle this issue, we propose a novel localization method that extracts reliable semi-dense 2D-3D matching points based on dense keypoint matches.","This approach involves regressing semi-dense 2D keypoints into 3D scene coordinates using a point inference network.","The network utilizes both geometric and visual cues to effectively infer 3D coordinates for unobserved keypoints from the observed ones.","The abundance of matching information significantly enhances the accuracy of camera pose estimation, even in scenarios involving noisy or sparse 3D models.","Comprehensive evaluations demonstrate that the proposed method outperforms other methods in challenging scenes and achieves competitive results in large-scale visual localization benchmarks.","The code will be available."],"url":"http://arxiv.org/abs/2402.08359v1","category":"cs.CV"}
{"created":"2024-02-13 09:26:19","title":"Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures","abstract":"The study of social values in fairy tales opens the possibility to learn about the communication of values across space and time. We propose to study the communication of values in fairy tales from Portugal, Italy and Germany using a technique called word embedding with a compass to quantify vocabulary differences and commonalities. We study how these three national traditions of fairy tales differ in their explicit references to values. To do this, we specify a list of value-charged tokens, consider their word stems and analyse the distance between these in a bespoke pre-trained Word2Vec model. We triangulate and critically discuss the validity of the resulting hypotheses emerging from this quantitative model. Our claim is that this is a reusable and reproducible method for the study of the values explicitly referenced in historical corpora. Finally, our preliminary findings hint at a shared cultural understanding and the expression of values such as Benevolence, Conformity, and Universalism across European societies, suggesting the existence of a pan-European cultural memory.","sentences":["The study of social values in fairy tales opens the possibility to learn about the communication of values across space and time.","We propose to study the communication of values in fairy tales from Portugal, Italy and Germany using a technique called word embedding with a compass to quantify vocabulary differences and commonalities.","We study how these three national traditions of fairy tales differ in their explicit references to values.","To do this, we specify a list of value-charged tokens, consider their word stems and analyse the distance between these in a bespoke pre-trained Word2Vec model.","We triangulate and critically discuss the validity of the resulting hypotheses emerging from this quantitative model.","Our claim is that this is a reusable and reproducible method for the study of the values explicitly referenced in historical corpora.","Finally, our preliminary findings hint at a shared cultural understanding and the expression of values such as Benevolence, Conformity, and Universalism across European societies, suggesting the existence of a pan-European cultural memory."],"url":"http://arxiv.org/abs/2402.08318v1","category":"cs.CL"}
{"created":"2024-02-13 07:51:36","title":"What the Fix? A Study of ASATs Rule Documentation","abstract":"Automatic Static Analysis Tools (ASATs) are widely used by software developers to diffuse and enforce coding practices. Yet, we know little about the documentation of ASATs, despite it being critical to learn about the coding practices in the first place. We shed light on this through several contributions. First, we analyze the documentation of more than 100 rules of 16 ASATs for multiple programming languages, and distill a taxonomy of the purposes of the documentation-What triggers a rule; Why it is important; and how to Fix an issue-and its types of contents. Then, we conduct a survey to assess the effectiveness of the documentation in terms of its goals and types of content. We highlight opportunities for improvement in ASAT documentation. In particular, we find that the Why purpose is missing in half of the rules we survey; moreover, when the Why is present, it is more likely to have quality issues than the What and the Fix.","sentences":["Automatic Static Analysis Tools (ASATs) are widely used by software developers to diffuse and enforce coding practices.","Yet, we know little about the documentation of ASATs, despite it being critical to learn about the coding practices in the first place.","We shed light on this through several contributions.","First, we analyze the documentation of more than 100 rules of 16 ASATs for multiple programming languages, and distill a taxonomy of the purposes of the documentation-What triggers a rule; Why it is important; and how to Fix an issue-and its types of contents.","Then, we conduct a survey to assess the effectiveness of the documentation in terms of its goals and types of content.","We highlight opportunities for improvement in ASAT documentation.","In particular, we find that the Why purpose is missing in half of the rules we survey; moreover, when the Why is present, it is more likely to have quality issues than the What and the Fix."],"url":"http://arxiv.org/abs/2402.08270v1","category":"cs.SE"}
{"created":"2024-02-13 05:33:35","title":"Improving Black-box Robustness with In-Context Rewriting","abstract":"Machine learning models often excel on in-distribution (ID) data but struggle with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD robustness are not applicable to settings where the model is effectively a black box, such as when the weights are frozen, retraining is costly, or the model is leveraged via an API. Test-time augmentation (TTA) is a simple post-hoc technique for improving robustness that sidesteps black-box constraints by aggregating predictions across multiple augmentations of the test input. TTA has seen limited use in NLP due to the challenge of generating effective natural language augmentations. In this work, we propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks for BERT and T5 models, with BERT's OOD robustness improving by an average of 4.30 percentage points without regressing average ID performance. We explore selectively augmenting inputs based on prediction entropy to reduce the rate of expensive LLM augmentations, allowing us to maintain performance gains while reducing the average number of generated augmentations by 57.76%. LLM-TTA is agnostic to the task model architecture, does not require OOD labels, and is effective across low and high-resource settings. We share our data, models, and code for reproducibility.","sentences":["Machine learning models often excel on in-distribution (ID) data but struggle with unseen out-of-distribution (OOD) inputs.","Most techniques for improving OOD robustness are not applicable to settings where the model is effectively a black box, such as when the weights are frozen, retraining is costly, or the model is leveraged via an API.","Test-time augmentation (TTA) is a simple post-hoc technique for improving robustness that sidesteps black-box constraints by aggregating predictions across multiple augmentations of the test input.","TTA has seen limited use in NLP due to the challenge of generating effective natural language augmentations.","In this work, we propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function.","LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks for BERT and T5 models, with BERT's OOD robustness improving by an average of 4.30 percentage points without regressing average ID performance.","We explore selectively augmenting inputs based on prediction entropy to reduce the rate of expensive LLM augmentations, allowing us to maintain performance gains while reducing the average number of generated augmentations by 57.76%.","LLM-TTA is agnostic to the task model architecture, does not require OOD labels, and is effective across low and high-resource settings.","We share our data, models, and code for reproducibility."],"url":"http://arxiv.org/abs/2402.08225v1","category":"cs.LG"}
{"created":"2024-02-13 05:25:37","title":"MetaTra: Meta-Learning for Generalized Trajectory Prediction in Unseen Domain","abstract":"Trajectory prediction has garnered widespread attention in different fields, such as autonomous driving and robotic navigation. However, due to the significant variations in trajectory patterns across different scenarios, models trained in known environments often falter in unseen ones. To learn a generalized model that can directly handle unseen domains without requiring any model updating, we propose a novel meta-learning-based trajectory prediction method called MetaTra. This approach incorporates a Dual Trajectory Transformer (Dual-TT), which enables a thorough exploration of the individual intention and the interactions within group motion patterns in diverse scenarios. Building on this, we propose a meta-learning framework to simulate the generalization process between source and target domains. Furthermore, to enhance the stability of our prediction outcomes, we propose a Serial and Parallel Training (SPT) strategy along with a feature augmentation method named MetaMix. Experimental results on several real-world datasets confirm that MetaTra not only surpasses other state-of-the-art methods but also exhibits plug-and-play capabilities, particularly in the realm of domain generalization.","sentences":["Trajectory prediction has garnered widespread attention in different fields, such as autonomous driving and robotic navigation.","However, due to the significant variations in trajectory patterns across different scenarios, models trained in known environments often falter in unseen ones.","To learn a generalized model that can directly handle unseen domains without requiring any model updating, we propose a novel meta-learning-based trajectory prediction method called MetaTra.","This approach incorporates a Dual Trajectory Transformer (Dual-TT), which enables a thorough exploration of the individual intention and the interactions within group motion patterns in diverse scenarios.","Building on this, we propose a meta-learning framework to simulate the generalization process between source and target domains.","Furthermore, to enhance the stability of our prediction outcomes, we propose a Serial and Parallel Training (SPT) strategy along with a feature augmentation method named MetaMix.","Experimental results on several real-world datasets confirm that MetaTra not only surpasses other state-of-the-art methods but also exhibits plug-and-play capabilities, particularly in the realm of domain generalization."],"url":"http://arxiv.org/abs/2402.08221v1","category":"cs.RO"}
{"created":"2024-02-13 04:19:06","title":"Quantum Computing-Enhanced Algorithm Unveils Novel Inhibitors for KRAS","abstract":"The discovery of small molecules with therapeutic potential is a long-standing challenge in chemistry and biology. Researchers have increasingly leveraged novel computational techniques to streamline the drug development process to increase hit rates and reduce the costs associated with bringing a drug to market. To this end, we introduce a quantum-classical generative model that seamlessly integrates the computational power of quantum algorithms trained on a 16-qubit IBM quantum computer with the established reliability of classical methods for designing small molecules. Our hybrid generative model was applied to designing new KRAS inhibitors, a crucial target in cancer therapy. We synthesized 15 promising molecules during our investigation and subjected them to experimental testing to assess their ability to engage with the target. Notably, among these candidates, two molecules, ISM061-018-2 and ISM061-22, each featuring unique scaffolds, stood out by demonstrating effective engagement with KRAS. ISM061-018-2 was identified as a broad-spectrum KRAS inhibitor, exhibiting a binding affinity to KRAS-G12D at $1.4 \\mu M$. Concurrently, ISM061-22 exhibited specific mutant selectivity, displaying heightened activity against KRAS G12R and Q61H mutants. To our knowledge, this work shows for the first time the use of a quantum-generative model to yield experimentally confirmed biological hits, showcasing the practical potential of quantum-assisted drug discovery to produce viable therapeutics. Moreover, our findings reveal that the efficacy of distribution learning correlates with the number of qubits utilized, underlining the scalability potential of quantum computing resources. Overall, we anticipate our results to be a stepping stone towards developing more advanced quantum generative models in drug discovery.","sentences":["The discovery of small molecules with therapeutic potential is a long-standing challenge in chemistry and biology.","Researchers have increasingly leveraged novel computational techniques to streamline the drug development process to increase hit rates and reduce the costs associated with bringing a drug to market.","To this end, we introduce a quantum-classical generative model that seamlessly integrates the computational power of quantum algorithms trained on a 16-qubit IBM quantum computer with the established reliability of classical methods for designing small molecules.","Our hybrid generative model was applied to designing new KRAS inhibitors, a crucial target in cancer therapy.","We synthesized 15 promising molecules during our investigation and subjected them to experimental testing to assess their ability to engage with the target.","Notably, among these candidates, two molecules, ISM061-018-2 and ISM061-22, each featuring unique scaffolds, stood out by demonstrating effective engagement with KRAS.","ISM061-018-2 was identified as a broad-spectrum KRAS inhibitor, exhibiting a binding affinity to KRAS-G12D at $1.4 \\mu M$. Concurrently, ISM061-22 exhibited specific mutant selectivity, displaying heightened activity against KRAS G12R and Q61H mutants.","To our knowledge, this work shows for the first time the use of a quantum-generative model to yield experimentally confirmed biological hits, showcasing the practical potential of quantum-assisted drug discovery to produce viable therapeutics.","Moreover, our findings reveal that the efficacy of distribution learning correlates with the number of qubits utilized, underlining the scalability potential of quantum computing resources.","Overall, we anticipate our results to be a stepping stone towards developing more advanced quantum generative models in drug discovery."],"url":"http://arxiv.org/abs/2402.08210v1","category":"quant-ph"}
{"created":"2024-02-13 02:46:45","title":"Pixel Sentence Representation Learning","abstract":"Pretrained language models are long known to be subpar in capturing sentence and document-level semantics. Though heavily investigated, transferring perturbation-based methods from unsupervised visual representation learning to NLP remains an unsolved problem. This is largely due to the discreteness of subword units brought by tokenization of language models, limiting small perturbations of inputs to form semantics-preserved positive pairs. In this work, we conceptualize the learning of sentence-level textual semantics as a visual representation learning process. Drawing from cognitive and linguistic sciences, we introduce an unsupervised visual sentence representation learning framework, employing visually-grounded text perturbation methods like typos and word order shuffling, resonating with human cognitive patterns, and enabling perturbation to texts to be perceived as continuous. Our approach is further bolstered by large-scale unsupervised topical alignment training and natural language inference supervision, achieving comparable performance in semantic textual similarity (STS) to existing state-of-the-art NLP methods. Additionally, we unveil our method's inherent zero-shot cross-lingual transferability and a unique leapfrogging pattern across languages during iterative training. To our knowledge, this is the first representation learning method devoid of traditional language models for understanding sentence and document semantics, marking a stride closer to human-like textual comprehension. Our code is available at https://github.com/gowitheflow-1998/Pixel-Linguist","sentences":["Pretrained language models are long known to be subpar in capturing sentence and document-level semantics.","Though heavily investigated, transferring perturbation-based methods from unsupervised visual representation learning to NLP remains an unsolved problem.","This is largely due to the discreteness of subword units brought by tokenization of language models, limiting small perturbations of inputs to form semantics-preserved positive pairs.","In this work, we conceptualize the learning of sentence-level textual semantics as a visual representation learning process.","Drawing from cognitive and linguistic sciences, we introduce an unsupervised visual sentence representation learning framework, employing visually-grounded text perturbation methods like typos and word order shuffling, resonating with human cognitive patterns, and enabling perturbation to texts to be perceived as continuous.","Our approach is further bolstered by large-scale unsupervised topical alignment training and natural language inference supervision, achieving comparable performance in semantic textual similarity (STS) to existing state-of-the-art NLP methods.","Additionally, we unveil our method's inherent zero-shot cross-lingual transferability and a unique leapfrogging pattern across languages during iterative training.","To our knowledge, this is the first representation learning method devoid of traditional language models for understanding sentence and document semantics, marking a stride closer to human-like textual comprehension.","Our code is available at https://github.com/gowitheflow-1998/Pixel-Linguist"],"url":"http://arxiv.org/abs/2402.08183v1","category":"cs.CL"}
{"created":"2024-02-13 02:36:41","title":"Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss","abstract":"This paper studies online structured prediction with full-information feedback. For online multiclass classification, van der Hoeven (2020) has obtained surrogate regret bounds independent of the time horizon, or \\emph{finite}, by introducing an elegant \\emph{exploit-the-surrogate-gap} framework. However, this framework has been limited to multiclass classification primarily because it relies on a classification-specific procedure for converting estimated scores to outputs. We extend the exploit-the-surrogate-gap framework to online structured prediction with \\emph{Fenchel--Young losses}, a large family of surrogate losses including the logistic loss for multiclass classification, obtaining finite surrogate regret bounds in various structured prediction problems. To this end, we propose and analyze \\emph{randomized decoding}, which converts estimated scores to general structured outputs. Moreover, by applying our decoding to online multiclass classification with the logistic loss, we obtain a surrogate regret bound of $O(B^2)$, where $B$ is the $\\ell_2$-diameter of the domain. This bound is tight up to logarithmic factors and improves the previous bound of $O(dB^2)$ due to van der Hoeven (2020) by a factor of $d$, the number of classes.","sentences":["This paper studies online structured prediction with full-information feedback.","For online multiclass classification, van der Hoeven (2020) has obtained surrogate regret bounds independent of the time horizon, or \\emph{finite}, by introducing an elegant \\emph{exploit-the-surrogate-gap} framework.","However, this framework has been limited to multiclass classification primarily because it relies on a classification-specific procedure for converting estimated scores to outputs.","We extend the exploit-the-surrogate-gap framework to online structured prediction with \\emph{Fenchel--Young losses}, a large family of surrogate losses including the logistic loss for multiclass classification, obtaining finite surrogate regret bounds in various structured prediction problems.","To this end, we propose and analyze \\emph{randomized decoding}, which converts estimated scores to general structured outputs.","Moreover, by applying our decoding to online multiclass classification with the logistic loss, we obtain a surrogate regret bound of $O(B^2)$, where $B$ is the $\\ell_2$-diameter of the domain.","This bound is tight up to logarithmic factors and improves the previous bound of $O(dB^2)$ due to van der Hoeven (2020) by a factor of $d$, the number of classes."],"url":"http://arxiv.org/abs/2402.08180v1","category":"cs.LG"}
{"created":"2024-02-13 01:39:56","title":"Poisson flow consistency models for low-dose CT image denoising","abstract":"Diffusion and Poisson flow models have demonstrated remarkable success for a wide range of generative tasks. Nevertheless, their iterative nature results in computationally expensive sampling and the number of function evaluations (NFE) required can be orders of magnitude larger than for single-step methods. Consistency models are a recent class of deep generative models which enable single-step sampling of high quality data without the need for adversarial training. In this paper, we introduce a novel image denoising technique which combines the flexibility afforded in Poisson flow generative models (PFGM)++ with the, high quality, single step sampling of consistency models. The proposed method first learns a trajectory between a noise distribution and the posterior distribution of interest by training PFGM++ in a supervised fashion. These pre-trained PFGM++ are subsequently \"distilled\" into Poisson flow consistency models (PFCM) via an updated version of consistency distillation. We call this approach posterior sampling Poisson flow consistency models (PS-PFCM). Our results indicate that the added flexibility of tuning the hyperparameter D, the dimensionality of the augmentation variables in PFGM++, allows us to outperform consistency models, a current state-of-the-art diffusion-style model with NFE=1 on clinical low-dose CT images. Notably, PFCM is in itself a novel family of deep generative models and we provide initial results on the CIFAR-10 dataset.","sentences":["Diffusion and Poisson flow models have demonstrated remarkable success for a wide range of generative tasks.","Nevertheless, their iterative nature results in computationally expensive sampling and the number of function evaluations (NFE) required can be orders of magnitude larger than for single-step methods.","Consistency models are a recent class of deep generative models which enable single-step sampling of high quality data without the need for adversarial training.","In this paper, we introduce a novel image denoising technique which combines the flexibility afforded in Poisson flow generative models (PFGM)++ with the, high quality, single step sampling of consistency models.","The proposed method first learns a trajectory between a noise distribution and the posterior distribution of interest by training PFGM++ in a supervised fashion.","These pre-trained PFGM++ are subsequently \"distilled\" into Poisson flow consistency models (PFCM) via an updated version of consistency distillation.","We call this approach posterior sampling Poisson flow consistency models (PS-PFCM).","Our results indicate that the added flexibility of tuning the hyperparameter D, the dimensionality of the augmentation variables in PFGM++, allows us to outperform consistency models, a current state-of-the-art diffusion-style model with NFE=1 on clinical low-dose CT images.","Notably, PFCM is in itself a novel family of deep generative models and we provide initial results on the CIFAR-10 dataset."],"url":"http://arxiv.org/abs/2402.08159v1","category":"eess.IV"}
{"created":"2024-02-13 00:02:05","title":"Randomized Algorithms for Symmetric Nonnegative Matrix Factorization","abstract":"Symmetric Nonnegative Matrix Factorization (SymNMF) is a technique in data analysis and machine learning that approximates a symmetric matrix with a product of a nonnegative, low-rank matrix and its transpose. To design faster and more scalable algorithms for SymNMF we develop two randomized algorithms for its computation. The first algorithm uses randomized matrix sketching to compute an initial low-rank input matrix and proceeds to use this input to rapidly compute a SymNMF. The second algorithm uses randomized leverage score sampling to approximately solve constrained least squares problems. Many successful methods for SymNMF rely on (approximately) solving sequences of constrained least squares problems. We prove theoretically that leverage score sampling can approximately solve nonnegative least squares problems to a chosen accuracy with high probability. Finally we demonstrate that both methods work well in practice by applying them to graph clustering tasks on large real world data sets. These experiments show that our methods approximately maintain solution quality and achieve significant speed ups for both large dense and large sparse problems.","sentences":["Symmetric Nonnegative Matrix Factorization (SymNMF) is a technique in data analysis and machine learning that approximates a symmetric matrix with a product of a nonnegative, low-rank matrix and its transpose.","To design faster and more scalable algorithms for SymNMF we develop two randomized algorithms for its computation.","The first algorithm uses randomized matrix sketching to compute an initial low-rank input matrix and proceeds to use this input to rapidly compute a SymNMF.","The second algorithm uses randomized leverage score sampling to approximately solve constrained least squares problems.","Many successful methods for SymNMF rely on (approximately) solving sequences of constrained least squares problems.","We prove theoretically that leverage score sampling can approximately solve nonnegative least squares problems to a chosen accuracy with high probability.","Finally we demonstrate that both methods work well in practice by applying them to graph clustering tasks on large real world data sets.","These experiments show that our methods approximately maintain solution quality and achieve significant speed ups for both large dense and large sparse problems."],"url":"http://arxiv.org/abs/2402.08134v1","category":"cs.LG"}
{"created":"2024-02-12 23:50:44","title":"Contextual Multinomial Logit Bandits with General Value Functions","abstract":"Contextual multinomial logit (MNL) bandits capture many real-world assortment recommendation problems such as online retailing/advertising. However, prior work has only considered (generalized) linear value functions, which greatly limits its applicability. Motivated by this fact, in this work, we consider contextual MNL bandits with a general value function class that contains the ground truth, borrowing ideas from a recent trend of studies on contextual bandits. Specifically, we consider both the stochastic and the adversarial settings, and propose a suite of algorithms, each with different computation-regret trade-off. When applied to the linear case, our results not only are the first ones with no dependence on a certain problem-dependent constant that can be exponentially large, but also enjoy other advantages such as computational efficiency, dimension-free regret bounds, or the ability to handle completely adversarial contexts and rewards.","sentences":["Contextual multinomial logit (MNL) bandits capture many real-world assortment recommendation problems such as online retailing/advertising.","However, prior work has only considered (generalized) linear value functions, which greatly limits its applicability.","Motivated by this fact, in this work, we consider contextual MNL bandits with a general value function class that contains the ground truth, borrowing ideas from a recent trend of studies on contextual bandits.","Specifically, we consider both the stochastic and the adversarial settings, and propose a suite of algorithms, each with different computation-regret trade-off.","When applied to the linear case, our results not only are the first ones with no dependence on a certain problem-dependent constant that can be exponentially large, but also enjoy other advantages such as computational efficiency, dimension-free regret bounds, or the ability to handle completely adversarial contexts and rewards."],"url":"http://arxiv.org/abs/2402.08126v1","category":"cs.LG"}
{"created":"2024-02-12 22:56:16","title":"Finding Moving-Band Statistical Arbitrages via Convex-Concave Optimization","abstract":"We propose a new method for finding statistical arbitrages that can contain more assets than just the traditional pair. We formulate the problem as seeking a portfolio with the highest volatility, subject to its price remaining in a band and a leverage limit. This optimization problem is not convex, but can be approximately solved using the convex-concave procedure, a specific sequential convex programming method. We show how the method generalizes to finding moving-band statistical arbitrages, where the price band midpoint varies over time.","sentences":["We propose a new method for finding statistical arbitrages that can contain more assets than just the traditional pair.","We formulate the problem as seeking a portfolio with the highest volatility, subject to its price remaining in a band and a leverage limit.","This optimization problem is not convex, but can be approximately solved using the convex-concave procedure, a specific sequential convex programming method.","We show how the method generalizes to finding moving-band statistical arbitrages, where the price band midpoint varies over time."],"url":"http://arxiv.org/abs/2402.08108v1","category":"econ.EM"}
{"created":"2024-02-12 22:35:40","title":"Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation","abstract":"Understanding textual description to generate code seems to be an achieved capability of instruction-following Large Language Models (LLMs) in zero-shot scenario. However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination.   In this study, we investigate the impact of Data Contamination on the performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we introduce a novel method to detect Data Contamination in GPTs and examine GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database. Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to-SQL translation tasks.","sentences":["Understanding textual description to generate code seems to be an achieved capability of instruction-following Large Language Models (LLMs) in zero-shot scenario.","However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code.","This effect is known as Data Contamination.   ","In this study, we investigate the impact of Data Contamination on the performance of GPT-3.5 in the Text-to-SQL code-generating tasks.","Hence, we introduce a novel method to detect Data Contamination in GPTs and examine GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite.","Furthermore, we analyze GPT-3.5's efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database.","Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to-SQL translation tasks."],"url":"http://arxiv.org/abs/2402.08100v1","category":"cs.CL"}
{"created":"2024-02-12 22:32:12","title":"Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?","abstract":"Fine-tuning pretrained foundational models on specific tasks is now the de facto approach for text and vision tasks. A known pitfall of this approach is the forgetting of pretraining knowledge that happens during finetuning. Rehearsing samples randomly from the pretrain dataset is a common approach to alleviate such forgetting. However, we find that random mixing unintentionally includes samples which are not (yet) forgotten or unlearnable by the model. We propose a novel sampling scheme, mix-cd, that identifies and prioritizes samples that actually face forgetting, which we call collateral damage. Since directly identifying collateral damage samples is computationally expensive, we propose a procedure to estimate the distribution of such samples by tracking the statistics of finetuned samples. Our approach is lightweight, easy to implement, and can be seamlessly integrated into existing models, offering an effective means to retain pretrain performance without additional computational costs.","sentences":["Fine-tuning pretrained foundational models on specific tasks is now the de facto approach for text and vision tasks.","A known pitfall of this approach is the forgetting of pretraining knowledge that happens during finetuning.","Rehearsing samples randomly from the pretrain dataset is a common approach to alleviate such forgetting.","However, we find that random mixing unintentionally includes samples which are not (yet) forgotten or unlearnable by the model.","We propose a novel sampling scheme, mix-cd, that identifies and prioritizes samples that actually face forgetting, which we call collateral damage.","Since directly identifying collateral damage samples is computationally expensive, we propose a procedure to estimate the distribution of such samples by tracking the statistics of finetuned samples.","Our approach is lightweight, easy to implement, and can be seamlessly integrated into existing models, offering an effective means to retain pretrain performance without additional computational costs."],"url":"http://arxiv.org/abs/2402.08096v1","category":"cs.LG"}
{"created":"2024-02-13 18:45:09","title":"Multidimensional Blockchain Fees are (Essentially) Optimal","abstract":"In this paper we show that, using only mild assumptions, previously proposed multidimensional blockchain fee markets are essentially optimal, even against worst-case adversaries. In particular, we show that the average welfare gap between the following two scenarios is at most $O(1/\\sqrt{T})$, where $T$ is the length of the time horizon considered. In the first scenario, the designer knows all future actions by users and is allowed to fix the optimal prices of resources ahead of time, based on the designer's oracular knowledge of those actions. In the second, the prices are updated by a very simple algorithm that does not have this oracular knowledge, a special case of which is similar to EIP-1559, the base fee mechanism used by the Ethereum blockchain. Roughly speaking, this means that, on average, over a reasonable timescale, there is no difference in welfare between 'correctly' fixing the prices, with oracular knowledge of the future, when compared to the proposed algorithm. We show a matching lower bound of $\\Omega(1/\\sqrt{T})$ for any implementable algorithm and also separately consider the case where the adversary is known to be stochastic.","sentences":["In this paper we show that, using only mild assumptions, previously proposed multidimensional blockchain fee markets are essentially optimal, even against worst-case adversaries.","In particular, we show that the average welfare gap between the following two scenarios is at most $O(1/\\sqrt{T})$, where $T$ is the length of the time horizon considered.","In the first scenario, the designer knows all future actions by users and is allowed to fix the optimal prices of resources ahead of time, based on the designer's oracular knowledge of those actions.","In the second, the prices are updated by a very simple algorithm that does not have this oracular knowledge, a special case of which is similar to EIP-1559, the base fee mechanism used by the Ethereum blockchain.","Roughly speaking, this means that, on average, over a reasonable timescale, there is no difference in welfare between 'correctly' fixing the prices, with oracular knowledge of the future, when compared to the proposed algorithm.","We show a matching lower bound of $\\Omega(1/\\sqrt{T})$ for any implementable algorithm and also separately consider the case where the adversary is known to be stochastic."],"url":"http://arxiv.org/abs/2402.08661v1","category":"cs.GT"}
{"created":"2024-02-13 17:57:42","title":"Efficient arc-flow formulations for makespan minimisation on parallel machines with a common server","abstract":"We consider the problem of scheduling non preemptively a set of jobs on parallel identical machines with prior setup operations on a single shared server, where the objective is to minimise the makespan. We develop an arc-flow formulation to the problem with two multigraphs, one for the machines and one for the server, with a same set of nodes representing points in time, and arcs associated with job execution, and with machines or server idleness. The resulting Flow-Flow formulation (FFF) and its tuned version (FFT) are compared with the best existing model in the literature, TIV I, on benchmark instances with up to 200 jobs and 7 machines. Computational results showed that our Flow-Flow models outperformed TIV I for instances with 100 jobs and produced optimal solutions to the vast majority of problems with 150 and 200 jobs for which TIV I failed to even find integer solutions within the standard limited runtime of 3600 s. When non optimal, solutions provided by our models exhibited very low gaps to best bound.","sentences":["We consider the problem of scheduling non preemptively a set of jobs on parallel identical machines with prior setup operations on a single shared server, where the objective is to minimise the makespan.","We develop an arc-flow formulation to the problem with two multigraphs, one for the machines and one for the server, with a same set of nodes representing points in time, and arcs associated with job execution, and with machines or server idleness.","The resulting Flow-Flow formulation (FFF) and its tuned version (FFT) are compared with the best existing model in the literature, TIV I, on benchmark instances with up to 200 jobs and 7 machines.","Computational results showed that our Flow-Flow models outperformed TIV I for instances with 100 jobs and produced optimal solutions to the vast majority of problems with 150 and 200 jobs for which TIV I failed to even find integer solutions within the standard limited runtime of 3600 s.","When non optimal, solutions provided by our models exhibited very low gaps to best bound."],"url":"http://arxiv.org/abs/2402.08629v1","category":"math.OC"}
{"created":"2024-02-13 15:41:29","title":"Retractions on closed sets","abstract":"On a manifold or a closed subset of a Euclidean vector space, a retraction enables to move in the direction of a tangent vector while staying on the set. Retractions are a versatile tool to perform computational tasks such as optimization, interpolation, and numerical integration. This paper studies two definitions of retraction on a closed subset of a Euclidean vector space, one being weaker than the other. Specifically, it shows that, in the context of constrained optimization, the weaker definition should be preferred as it inherits the main property of the other while being less restrictive.","sentences":["On a manifold or a closed subset of a Euclidean vector space, a retraction enables to move in the direction of a tangent vector while staying on the set.","Retractions are a versatile tool to perform computational tasks such as optimization, interpolation, and numerical integration.","This paper studies two definitions of retraction on a closed subset of a Euclidean vector space, one being weaker than the other.","Specifically, it shows that, in the context of constrained optimization, the weaker definition should be preferred as it inherits the main property of the other while being less restrictive."],"url":"http://arxiv.org/abs/2402.08536v1","category":"math.OC"}
{"created":"2024-02-13 15:29:50","title":"A short review on Improvements and stability for some interpolation inequalities","abstract":"In this paper, we present recent stability results with explicit and dimensionally sharp constants and optimal norms for the Sobolev inequality and for the Gaussian logarithmic Sobolev inequality obtained by the authors in [22]. The stability for the Gaussian logarithmic Sobolev inequality is obtained as a byproduct of the stability for the Sobolev inequality. Here we also give a new, direct, alternative proof of this result. We also discuss improved versions of interpolation inequalities based on the carr\\'e du champ method.","sentences":["In this paper, we present recent stability results with explicit and dimensionally sharp constants and optimal norms for the Sobolev inequality and for the Gaussian logarithmic Sobolev inequality obtained by the authors in [22].","The stability for the Gaussian logarithmic Sobolev inequality is obtained as a byproduct of the stability for the Sobolev inequality.","Here we also give a new, direct, alternative proof of this result.","We also discuss improved versions of interpolation inequalities based on the carr\\'e du champ method."],"url":"http://arxiv.org/abs/2402.08527v1","category":"math.AP"}
{"created":"2024-02-13 14:07:52","title":"P\u00f3lya-type estimates for the first Robin eigenvalue of elliptic operators","abstract":"The aim of this paper is to obtain optimal estimates for the first Robin eigenvalue of the anisotropic $p$-Laplace operator, namely: \\[   \\lambda_F(\\beta,\\Omega)=\\lambda_{F}(p,\\beta,\\Omega)= \\min_{\\psi\\in W^{1,p}(\\Omega)\\setminus\\{0\\} } \\frac{\\int_\\Omega F(\\nabla \\psi)^p dx +\\beta\\int_{\\partial\\Omega}|\\psi|^p F(\\nu_{\\Omega}) d\\mathcal H^{N-1} }{\\int_\\Omega|\\psi|^p dx} \\] where $p\\in]1,+\\infty[$, $\\Omega$ is a bounded, convex domain in $\\mathbb R^{N}$, $\\nu_{\\Omega}$ is its Euclidean outward normal, $\\beta$ is a real number, and $F$ is a sufficiently smooth norm on $\\mathbb R^{N}$. We show an upper bound for $\\lambda_{F}(\\beta,\\Omega)$ in terms of the first eigenvalue of a one-dimensional nonlinear problem, which depends on $\\beta$ and on the volume and the anisotropic perimeter of $\\Omega$, in the spirit of the classical estimates of P\\'olya \\cite{po61} for the Euclidean Dirichlet Laplacian.   We will also provide a lower bound for the torsional rigidity \\[ \\tau_p(\\beta,\\Omega)^{p-1} = \\max_{\\substack{\\psi\\in   W^{1,p}(\\Omega)\\setminus\\{0\\}}} \\dfrac{\\left(\\int_\\Omega |\\psi| \\,   dx\\right)^p}{\\int_\\Omega F(\\nabla\\psi)^p dx+\\beta \\int_{\\partial\\Omega}|\\psi|^p F(\\nu_{\\Omega}) d\\mathcal H^{N-1} }, \\] when $\\beta>0$. The obtained results are new also in the case of the classical Euclidean Laplacian.","sentences":["The aim of this paper is to obtain optimal estimates for the first Robin eigenvalue of the anisotropic $p$-Laplace operator, namely: \\[   \\lambda_F(\\beta,\\Omega)=\\lambda_{F}(p,\\beta,\\Omega)= \\min_{\\psi\\in W^{1,p}(\\Omega)\\setminus\\{0\\} } \\frac{\\int_\\Omega F(\\nabla \\psi)^p dx +\\beta\\int_{\\partial\\Omega}|\\psi|^p F(\\nu_{\\Omega}) d\\mathcal H^{N-1} }{\\int_\\Omega|\\psi|^p dx} \\] where $p\\in]1,+\\infty[$, $\\Omega$ is a bounded, convex domain in $\\mathbb R^{N}$, $\\nu_{\\Omega}$ is its Euclidean outward normal, $\\beta$ is a real number, and $F$ is a sufficiently smooth norm on $\\mathbb R^{N}$. We show an upper bound for $\\lambda_{F}(\\beta,\\Omega)$ in terms of the first eigenvalue of a one-dimensional nonlinear problem, which depends on $\\beta$ and on the volume and the anisotropic perimeter of $\\Omega$, in the spirit of the classical estimates of P\\'olya \\cite{po61} for the Euclidean Dirichlet Laplacian.   ","We will also provide a lower bound for the torsional rigidity \\[ \\tau_p(\\beta,\\Omega)^{p-1} = \\max_{\\substack{\\psi\\in   W^{1,p}(\\Omega)\\setminus\\{0\\}}} \\dfrac{\\left(\\int_\\Omega |\\psi| \\,   dx\\right)^p}{\\int_\\Omega F(\\nabla\\psi)^p","dx+\\beta \\int_{\\partial\\Omega}|\\psi|^p F(\\nu_{\\Omega}) d\\mathcal H^{N-1} }, \\] when $\\beta>0$. The obtained results are new also in the case of the classical Euclidean Laplacian."],"url":"http://arxiv.org/abs/2402.08474v1","category":"math.AP"}
{"created":"2024-02-13 10:32:21","title":"Wireless Channel Prediction via Gaussian Mixture Models","abstract":"In this work, we utilize a Gaussian mixture model (GMM) to capture the underlying probability density function (PDF) of the channel trajectories of moving mobile terminals (MTs) within the coverage area of a base station (BS) in an offline phase. We propose to leverage the same GMM for channel prediction in the online phase. Our proposed approach does not require signal-to-noise ratio (SNR)-specific training and allows for parallelization. Numerical simulations for both synthetic and measured channel data demonstrate the effectiveness of our proposed GMM-based channel predictor compared to state-ofthe-art channel prediction methods.","sentences":["In this work, we utilize a Gaussian mixture model (GMM) to capture the underlying probability density function (PDF) of the channel trajectories of moving mobile terminals (MTs) within the coverage area of a base station (BS) in an offline phase.","We propose to leverage the same GMM for channel prediction in the online phase.","Our proposed approach does not require signal-to-noise ratio (SNR)-specific training and allows for parallelization.","Numerical simulations for both synthetic and measured channel data demonstrate the effectiveness of our proposed GMM-based channel predictor compared to state-ofthe-art channel prediction methods."],"url":"http://arxiv.org/abs/2402.08351v1","category":"eess.SP"}
{"created":"2024-02-13 09:43:06","title":"Multi-Blade detector with VMM3a-ASIC-based readout: installation and commissioning at the reflectometer Amor at PSI","abstract":"The Multi-Blade (MB) Boron-10-based neutron detector is the chosen technology for three instruments at the European Spallation Source (ESS): the two ESS reflectometers, ESTIA and FREIA, and the Test Beam Line. A fourth MB detector has been built, installed and commissioned for the user operation of the reflectometer Amor at PSI (Switzerland). Amor can be considered a downscaled version of the ESS reflectometer ESTIA. They are based on the same Selene guide concept, optimized for performing focusing reflectometry on small samples. The experience gained at Amor is invaluable for the future deployment of the MB detector at the ESS. This manuscript describes the MB detector construction and installation at Amor along with the readout electronics chain based on the VMM3a ASIC. The readout chain deployed at Amor is equivalent of that of the ESS, including the readout master module (RMM), event-formation-units (EFUs), Kafka, FileWriter and live visualisation tools.","sentences":["The Multi-Blade (MB) Boron-10-based neutron detector is the chosen technology for three instruments at the European Spallation Source (ESS): the two ESS reflectometers, ESTIA and FREIA, and the Test Beam Line.","A fourth MB detector has been built, installed and commissioned for the user operation of the reflectometer Amor at PSI (Switzerland).","Amor can be considered a downscaled version of the ESS reflectometer ESTIA.","They are based on the same Selene guide concept, optimized for performing focusing reflectometry on small samples.","The experience gained at Amor is invaluable for the future deployment of the MB detector at the ESS.","This manuscript describes the MB detector construction and installation at Amor along with the readout electronics chain based on the VMM3a ASIC.","The readout chain deployed at Amor is equivalent of that of the ESS, including the readout master module (RMM), event-formation-units (EFUs), Kafka, FileWriter and live visualisation tools."],"url":"http://arxiv.org/abs/2402.08325v1","category":"physics.ins-det"}
{"created":"2024-02-13 07:56:45","title":"The gradient's limit of a definable family of functions is aconservative set-valued field","abstract":"It is well-known that the convergence of a family of smooth functions does not implythe convergence of its gradients. In this work, we show that if the family is definablein an o-minimal structure (for instance semialgebraic, subanalytic, or any compositionof the previous with exp, log), then the gradient's limit is a conservative set-valued fieldin the sense introduced by Bolte and Pauwels. Immediate implications of this resulton convergence guarantees of smoothing methods are discussed. Finally, a more generalresult is established, where the functions in the original family might be merely Lipschitzcontinuous, vector-valued and the gradients are replaced by their Clarke's Jacobians oran arbitrary definable conservative mapping.","sentences":["It is well-known that the convergence of a family of smooth functions does not implythe convergence of its gradients.","In this work, we show that if the family is definablein an o-minimal structure (for instance semialgebraic, subanalytic, or any compositionof the previous with exp, log), then the gradient's limit is a conservative set-valued fieldin the sense introduced by Bolte and Pauwels.","Immediate implications of this resulton convergence guarantees of smoothing methods are discussed.","Finally, a more generalresult is established, where the functions in the original family might be merely Lipschitzcontinuous, vector-valued and the gradients are replaced by their Clarke's Jacobians oran arbitrary definable conservative mapping."],"url":"http://arxiv.org/abs/2402.08272v1","category":"math.OC"}
{"created":"2024-02-13 05:36:19","title":"Noise Aware Utility Optimization of NISQ Devices","abstract":"In order to enter the era of utility, noisy intermediate-scale quantum (NISQ) devices need to enable long-range entanglement of large qubit chains. However, due to the limited connectivity of superconducting NISQ devices, long-range entangling gates are realized in linear depth. Furthermore, a time-dependent degradation of the average CNOT gate fidelity is observed. Likely due to aging, this phenomenon further degrades entanglement capabilities. Our aim is to help in the current efforts to achieve utility and provide an opportunity to extend the utility lifespan of current devices --albeit by selecting fewer, high quality resources. To achieve this, we provide a method to transform user-provided CNOT and readout error requirements into a compliant partition onto which circuits can be executed. We demonstrate an improvement of up to 52% in fidelity for a random CNOT chain of length 50 qubits and consistent improvements between 11.8% and 47.7% for chains between 10 and 40 in varying in increments of 10, respectively.","sentences":["In order to enter the era of utility, noisy intermediate-scale quantum (NISQ) devices need to enable long-range entanglement of large qubit chains.","However, due to the limited connectivity of superconducting NISQ devices, long-range entangling gates are realized in linear depth.","Furthermore, a time-dependent degradation of the average CNOT gate fidelity is observed.","Likely due to aging, this phenomenon further degrades entanglement capabilities.","Our aim is to help in the current efforts to achieve utility and provide an opportunity to extend the utility lifespan of current devices --albeit by selecting fewer, high quality resources.","To achieve this, we provide a method to transform user-provided CNOT and readout error requirements into a compliant partition onto which circuits can be executed.","We demonstrate an improvement of up to 52% in fidelity for a random CNOT chain of length 50 qubits and consistent improvements between 11.8% and 47.7% for chains between 10 and 40 in varying in increments of 10, respectively."],"url":"http://arxiv.org/abs/2402.08226v1","category":"quant-ph"}
{"created":"2024-02-13 03:39:15","title":"Optimized Information Flow for Transformer Tracking","abstract":"One-stream Transformer trackers have shown outstanding performance in challenging benchmark datasets over the last three years, as they enable interaction between the target template and search region tokens to extract target-oriented features with mutual guidance. Previous approaches allow free bidirectional information flow between template and search tokens without investigating their influence on the tracker's discriminative capability. In this study, we conducted a detailed study on the information flow of the tokens and based on the findings, we propose a novel Optimized Information Flow Tracking (OIFTrack) framework to enhance the discriminative capability of the tracker. The proposed OIFTrack blocks the interaction from all search tokens to target template tokens in early encoder layers, as the large number of non-target tokens in the search region diminishes the importance of target-specific features. In the deeper encoder layers of the proposed tracker, search tokens are partitioned into target search tokens and non-target search tokens, allowing bidirectional flow from target search tokens to template tokens to capture the appearance changes of the target. In addition, since the proposed tracker incorporates dynamic background cues, distractor objects are successfully avoided by capturing the surrounding information of the target. The OIFTrack demonstrated outstanding performance in challenging benchmarks, particularly excelling in the one-shot tracking benchmark GOT-10k, achieving an average overlap of 74.6\\%. The code, models, and results of this work are available at \\url{https://github.com/JananiKugaa/OIFTrack}","sentences":["One-stream Transformer trackers have shown outstanding performance in challenging benchmark datasets over the last three years, as they enable interaction between the target template and search region tokens to extract target-oriented features with mutual guidance.","Previous approaches allow free bidirectional information flow between template and search tokens without investigating their influence on the tracker's discriminative capability.","In this study, we conducted a detailed study on the information flow of the tokens and based on the findings, we propose a novel Optimized Information Flow Tracking (OIFTrack) framework to enhance the discriminative capability of the tracker.","The proposed OIFTrack blocks the interaction from all search tokens to target template tokens in early encoder layers, as the large number of non-target tokens in the search region diminishes the importance of target-specific features.","In the deeper encoder layers of the proposed tracker, search tokens are partitioned into target search tokens and non-target search tokens, allowing bidirectional flow from target search tokens to template tokens to capture the appearance changes of the target.","In addition, since the proposed tracker incorporates dynamic background cues, distractor objects are successfully avoided by capturing the surrounding information of the target.","The OIFTrack demonstrated outstanding performance in challenging benchmarks, particularly excelling in the one-shot tracking benchmark GOT-10k, achieving an average overlap of 74.6\\%.","The code, models, and results of this work are available at \\url{https://github.com/JananiKugaa/OIFTrack}"],"url":"http://arxiv.org/abs/2402.08195v1","category":"cs.CV"}
{"created":"2024-02-13 00:56:19","title":"Model Predictive Bang-Bang Controller Synthesis via Approximate Value Functions","abstract":"In this paper, we propose a novel method for addressing Optimal Control Problems (OCPs) with input-affine dynamics and cost functions. This approach adopts a Model Predictive Control (MPC) strategy, wherein a controller is synthesized to handle an approximated OCP within a finite time horizon. Upon reaching this horizon, the controller is re-calibrated to tackle another approximation of the OCP, with the approximation updated based on the final state and time information. To tackle each OCP instance, all non-polynomial terms are Taylor-expanded about the current time and state and the resulting Hamilton-Jacobi-Bellman (HJB) PDE is solved via Sum-of-Squares (SOS) programming, providing us with an approximate polynomial value function that can be used to synthesize a bang-bang controller.","sentences":["In this paper, we propose a novel method for addressing Optimal Control Problems (OCPs) with input-affine dynamics and cost functions.","This approach adopts a Model Predictive Control (MPC) strategy, wherein a controller is synthesized to handle an approximated OCP within a finite time horizon.","Upon reaching this horizon, the controller is re-calibrated to tackle another approximation of the OCP, with the approximation updated based on the final state and time information.","To tackle each OCP instance, all non-polynomial terms are Taylor-expanded about the current time and state and the resulting Hamilton-Jacobi-Bellman (HJB) PDE is solved via Sum-of-Squares (SOS) programming, providing us with an approximate polynomial value function that can be used to synthesize a bang-bang controller."],"url":"http://arxiv.org/abs/2402.08148v1","category":"math.OC"}
{"created":"2024-02-13 00:33:43","title":"Optimal Size-Aware Dispatching Rules via Value Iteration and Some Numerical Investigations","abstract":"This technical report explains how optimal size-aware dispatching policies can be determined numerically using value iteration. It also contains some numerical examples that shed light to the nature of the optimal policies itself. The report complements our ``Towards the Optimal Dynamic Size-aware Dispatching'' article that will appear in Elsevier's Performance Evaluation in 2024.","sentences":["This technical report explains how optimal size-aware dispatching policies can be determined numerically using value iteration.","It also contains some numerical examples that shed light to the nature of the optimal policies itself.","The report complements our ``Towards the Optimal Dynamic Size-aware Dispatching'' article that will appear in Elsevier's Performance Evaluation in 2024."],"url":"http://arxiv.org/abs/2402.08142v1","category":"math.OC"}
{"created":"2024-02-12 23:46:01","title":"Anisotropic molecular photoemission dynamics: Interpreting and accounting for the nuclear motion","abstract":"We investigate how vibration affects molecular photoemission dynamics, through simulations on two-dimension asymmetric model molecules including the electronic and nuclear motions in a fully correlated way. We show that a slight anisotropy in the electron-ion momentum sharing is sufficient to prevent one from unambigously characterizing the vibrationnaly averaged photoemission dynamics in terms of stereo Wigner delays. We further show that vibrational resolution can be retrieved in fixed-nuclei simulations, using effective molecular conformations that are specific to each vibrational channel. The optimal internuclear distances found empirically in 1-photon processes can be identified a priori using simple physical arguments. They also turn out to be efficient to simulate vibrationnally-resolved \\rabbit measurements and to account for interchannel coherences in broadband 1-photon ionization.","sentences":["We investigate how vibration affects molecular photoemission dynamics, through simulations on two-dimension asymmetric model molecules including the electronic and nuclear motions in a fully correlated way.","We show that a slight anisotropy in the electron-ion momentum sharing is sufficient to prevent one from unambigously characterizing the vibrationnaly averaged photoemission dynamics in terms of stereo Wigner delays.","We further show that vibrational resolution can be retrieved in fixed-nuclei simulations, using effective molecular conformations that are specific to each vibrational channel.","The optimal internuclear distances found empirically in 1-photon processes can be identified a priori using simple physical arguments.","They also turn out to be efficient to simulate vibrationnally-resolved \\rabbit measurements and to account for interchannel coherences in broadband 1-photon ionization."],"url":"http://arxiv.org/abs/2402.08124v1","category":"physics.chem-ph"}
{"created":"2024-02-12 22:35:10","title":"Semantic segmentation for recognition of epileptiform patterns recorded via Microelectrode Arrays in vitro","abstract":"Epilepsy is a prevalent neurological disorder that affects approximately 1% of the global population. Around 30-40% of patients do not respond to pharmacological treatment, leading to a significant negative impact on their quality of life. Closed-loop deep brain stimulation (DBS) is a promising treatment for individuals who do not respond to medical therapy. To achieve effective seizure control, algorithms play an important role in identifying relevant electrographic biomarkers from local field potentials (LFPs) to determine the optimal stimulation timing. In this regard, the detection and classification of events from ongoing brain activity, while achieving low power through computationally unexpensive implementations, represents a major challenge in the field. To address this challenge, we here present two lightweight algorithms, the ZdensityRODE and the AMPDE, for identifying relevant events from LFPs by utilizing semantic segmentation, which involves extracting different levels of information from the LFP and relevant events from it. The algorithms performance was validated against epileptiform activity induced by 4-minopyridine in mouse hippocampus-cortex (CTX) slices and recorded via microelectrode array, as a case study. The ZdensityRODE algorithm showcased a precision and recall of 93% for ictal event detection and 42% precision for interictal event detection, while the AMPDE algorithm attained a precision of 96% and recall of 90% for ictal event detection and 54% precision for interictal event detection. While initially trained specifically for detection of ictal activity, these algorithms can be fine-tuned for improved interictal detection, aiming at seizure prediction. Our results suggest that these algorithms can effectively capture epileptiform activity; their light weight opens new possibilities for real-time seizure detection and seizure prediction and control.","sentences":["Epilepsy is a prevalent neurological disorder that affects approximately 1% of the global population.","Around 30-40% of patients do not respond to pharmacological treatment, leading to a significant negative impact on their quality of life.","Closed-loop deep brain stimulation (DBS) is a promising treatment for individuals who do not respond to medical therapy.","To achieve effective seizure control, algorithms play an important role in identifying relevant electrographic biomarkers from local field potentials (LFPs) to determine the optimal stimulation timing.","In this regard, the detection and classification of events from ongoing brain activity, while achieving low power through computationally unexpensive implementations, represents a major challenge in the field.","To address this challenge, we here present two lightweight algorithms, the ZdensityRODE and the AMPDE, for identifying relevant events from LFPs by utilizing semantic segmentation, which involves extracting different levels of information from the LFP and relevant events from it.","The algorithms performance was validated against epileptiform activity induced by 4-minopyridine in mouse hippocampus-cortex (CTX) slices and recorded via microelectrode array, as a case study.","The ZdensityRODE algorithm showcased a precision and recall of 93% for ictal event detection and 42% precision for interictal event detection, while the AMPDE algorithm attained a precision of 96% and recall of 90% for ictal event detection and 54% precision for interictal event detection.","While initially trained specifically for detection of ictal activity, these algorithms can be fine-tuned for improved interictal detection, aiming at seizure prediction.","Our results suggest that these algorithms can effectively capture epileptiform activity; their light weight opens new possibilities for real-time seizure detection and seizure prediction and control."],"url":"http://arxiv.org/abs/2402.08099v1","category":"eess.SP"}
{"created":"2024-02-12 21:14:37","title":"Locality Sensitive Hashing for Network Traffic Fingerprinting","abstract":"The advent of the Internet of Things (IoT) has brought forth additional intricacies and difficulties to computer networks. These gadgets are particularly susceptible to cyber-attacks because of their simplistic design. Therefore, it is crucial to recognise these devices inside a network for the purpose of network administration and to identify any harmful actions. Network traffic fingerprinting is a crucial technique for identifying devices and detecting anomalies. Currently, the predominant methods for this depend heavily on machine learning (ML). Nevertheless, machine learning (ML) methods need the selection of features, adjustment of hyperparameters, and retraining of models to attain optimal outcomes and provide resilience to concept drifts detected in a network. In this research, we suggest using locality-sensitive hashing (LSH) for network traffic fingerprinting as a solution to these difficulties. Our study focuses on examining several design options for the Nilsimsa LSH function. We then use this function to create unique fingerprints for network data, which may be used to identify devices. We also compared it with ML-based traffic fingerprinting and observed that our method increases the accuracy of state-of-the-art by 12% achieving around 94% accuracy in identifying devices in a network.","sentences":["The advent of the Internet of Things (IoT) has brought forth additional intricacies and difficulties to computer networks.","These gadgets are particularly susceptible to cyber-attacks because of their simplistic design.","Therefore, it is crucial to recognise these devices inside a network for the purpose of network administration and to identify any harmful actions.","Network traffic fingerprinting is a crucial technique for identifying devices and detecting anomalies.","Currently, the predominant methods for this depend heavily on machine learning (ML).","Nevertheless, machine learning (ML) methods need the selection of features, adjustment of hyperparameters, and retraining of models to attain optimal outcomes and provide resilience to concept drifts detected in a network.","In this research, we suggest using locality-sensitive hashing (LSH) for network traffic fingerprinting as a solution to these difficulties.","Our study focuses on examining several design options for the Nilsimsa LSH function.","We then use this function to create unique fingerprints for network data, which may be used to identify devices.","We also compared it with ML-based traffic fingerprinting and observed that our method increases the accuracy of state-of-the-art by 12% achieving around 94% accuracy in identifying devices in a network."],"url":"http://arxiv.org/abs/2402.08063v1","category":"cs.NI"}
{"created":"2024-02-12 19:18:50","title":"Relativistic electrons from vacuum laser acceleration using tightly focused radially polarized beams","abstract":"We generate a tabletop pulsed relativistic electron beam at 100 Hz repetition rate from vacuum laser acceleration (VLA) by tightly focusing a radially polarized beam into a low-density gas. We demonstrate that strong longitudinal electric fields at the focus can accelerate electrons up to 1.43 MeV by using only 98 GW of peak laser power. The electron energy is measured as a function of laser intensity and gas species, revealing a strong dependence on the atomic ionization dynamics. These experimental results are supported by numerical simulations of particle dynamics in a tightly focused configuration that take ionization into consideration. For the range of intensities considered, it is demonstrated that atoms with higher atomic numbers like krypton can optimally inject electrons at the peak of the laser field, resulting in higher energies and an efficient acceleration mechanism that reaches a significant fraction of the theoretical energy gain limit.","sentences":["We generate a tabletop pulsed relativistic electron beam at 100 Hz repetition rate from vacuum laser acceleration (VLA) by tightly focusing a radially polarized beam into a low-density gas.","We demonstrate that strong longitudinal electric fields at the focus can accelerate electrons up to 1.43 MeV by using only 98 GW of peak laser power.","The electron energy is measured as a function of laser intensity and gas species, revealing a strong dependence on the atomic ionization dynamics.","These experimental results are supported by numerical simulations of particle dynamics in a tightly focused configuration that take ionization into consideration.","For the range of intensities considered, it is demonstrated that atoms with higher atomic numbers like krypton can optimally inject electrons at the peak of the laser field, resulting in higher energies and an efficient acceleration mechanism that reaches a significant fraction of the theoretical energy gain limit."],"url":"http://arxiv.org/abs/2402.08009v1","category":"physics.optics"}
{"created":"2024-02-12 19:00:00","title":"Experimental roadmap for optimal state transfer and entanglement generation in power-law systems","abstract":"Experimental systems with power-law interactions have recently garnered interest as promising platforms for quantum information processing. Such systems are capable of spreading entanglement superballistically and achieving an asymptotic speed-up over locally interacting systems. Recently, protocols developed by Eldredge et al. [Phys. Rev. Lett. 119, 170503 (2017)] and Tran et al. [Phys. Rev. X 11, 031016 (2021)] for the task of transferring a quantum state between distant particles quickly were shown to be optimal and saturate theoretical bounds. However, the implementation of these protocols in physical systems with long-range interactions remains to be fully realized. In this work, we provide an experimental roadmap towards realizing fast state-transfer protocols in three classes of atomic and molecular systems with dipolar interactions: polar molecules composed of alkali-metal dimers, neutral atoms in excited Rydberg states, and atoms with strong magnetic moments (e.g. dysprosium). As a guide to near-term experimental implementation, we numerically evaluate the tradeoffs between the two protocols for small system sizes and develop methods to address potential crosstalk errors that may arise during the execution of the protocols.","sentences":["Experimental systems with power-law interactions have recently garnered interest as promising platforms for quantum information processing.","Such systems are capable of spreading entanglement superballistically and achieving an asymptotic speed-up over locally interacting systems.","Recently, protocols developed by Eldredge et al.","[Phys. Rev. Lett.","119, 170503 (2017)] and Tran et al.","[Phys. Rev. X 11, 031016 (2021)] for the task of transferring a quantum state between distant particles quickly were shown to be optimal and saturate theoretical bounds.","However, the implementation of these protocols in physical systems with long-range interactions remains to be fully realized.","In this work, we provide an experimental roadmap towards realizing fast state-transfer protocols in three classes of atomic and molecular systems with dipolar interactions: polar molecules composed of alkali-metal dimers, neutral atoms in excited Rydberg states, and atoms with strong magnetic moments (e.g. dysprosium).","As a guide to near-term experimental implementation, we numerically evaluate the tradeoffs between the two protocols for small system sizes and develop methods to address potential crosstalk errors that may arise during the execution of the protocols."],"url":"http://arxiv.org/abs/2402.07974v1","category":"quant-ph"}
