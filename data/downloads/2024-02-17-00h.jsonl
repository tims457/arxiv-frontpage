{"created":"2024-02-15 18:59:18","title":"Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation","abstract":"Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (\"winner\" and \"loser\" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.","sentences":["Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs).","While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data.","Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (\"winner\" and \"loser\" images) for each text prompt.","In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process.","Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment.","Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration.","By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data."],"url":"http://arxiv.org/abs/2402.10210v1","category":"cs.LG"}
{"created":"2024-02-15 18:59:02","title":"Recovering the Pre-Fine-Tuning Weights of Generative Models","abstract":"The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.","sentences":["The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning.","This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights.","In this paper, we demonstrate that this assumption is often false.","Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models.","In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights.","Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral."],"url":"http://arxiv.org/abs/2402.10208v1","category":"cs.LG"}
{"created":"2024-02-15 18:58:31","title":"Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment","abstract":"We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around $10\\%$ GPU hours compared with multi-objective RL baseline.","sentences":["We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems.","However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process.","In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment.","The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time.","Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives.","Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around $10\\%$ GPU hours compared with multi-objective RL baseline."],"url":"http://arxiv.org/abs/2402.10207v1","category":"cs.LG"}
{"created":"2024-02-15 18:58:18","title":"Ising on the Graph: Task-specific Graph Subsampling via the Ising Model","abstract":"Reducing a graph while preserving its overall structure is an important problem with many applications. Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind. In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network. Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion. The utilized loss function of the task does not even have to be differentiable. We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination.","sentences":["Reducing a graph while preserving its overall structure is an important problem with many applications.","Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind.","In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network.","Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion.","The utilized loss function of the task does not even have to be differentiable.","We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination."],"url":"http://arxiv.org/abs/2402.10206v1","category":"cs.LG"}
{"created":"2024-02-15 18:57:24","title":"Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model","abstract":"Reconstructing sky models from dirty radio images for accurate source localization and flux estimation is crucial for studying galaxy evolution at high redshift, especially in deep fields using instruments like the Atacama Large Millimetre Array (ALMA). With new projects like the Square Kilometre Array (SKA), there's a growing need for better source extraction methods. Current techniques, such as CLEAN and PyBDSF, often fail to detect faint sources, highlighting the need for more accurate methods. This study proposes using stochastic neural networks to rebuild sky models directly from dirty images. This method can pinpoint radio sources and measure their fluxes with related uncertainties, marking a potential improvement in radio source characterization. We tested this approach on 10164 images simulated with the CASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied conditional Denoising Diffusion Probabilistic Models (DDPMs) for sky models reconstruction, then used Photutils to determine source coordinates and fluxes, assessing the model's performance across different water vapor levels. Our method showed excellence in source localization, achieving more than 90% completeness at a signal-to-noise ratio (SNR) as low as 2. It also surpassed PyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in the test set, a significant improvement over CLEAN+ PyBDSF's 57%. Conditional DDPMs is a powerful tool for image-to-image translation, yielding accurate and robust characterisation of radio sources, and outperforming existing methodologies. While this study underscores its significant potential for applications in radio astronomy, we also acknowledge certain limitations that accompany its usage, suggesting directions for further refinement and research.","sentences":["Reconstructing sky models from dirty radio images for accurate source localization and flux estimation is crucial for studying galaxy evolution at high redshift, especially in deep fields using instruments like the Atacama Large Millimetre Array (ALMA).","With new projects like the Square Kilometre Array (SKA), there's a growing need for better source extraction methods.","Current techniques, such as CLEAN and PyBDSF, often fail to detect faint sources, highlighting the need for more accurate methods.","This study proposes using stochastic neural networks to rebuild sky models directly from dirty images.","This method can pinpoint radio sources and measure their fluxes with related uncertainties, marking a potential improvement in radio source characterization.","We tested this approach on 10164 images simulated with the CASA tool simalma, based on ALMA's Cycle 5.3 antenna setup.","We applied conditional Denoising Diffusion Probabilistic Models (DDPMs) for sky models reconstruction, then used Photutils to determine source coordinates and fluxes, assessing the model's performance across different water vapor levels.","Our method showed excellence in source localization, achieving more than 90% completeness at a signal-to-noise ratio (SNR) as low as 2.","It also surpassed PyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in the test set, a significant improvement over CLEAN+ PyBDSF's 57%.","Conditional DDPMs is a powerful tool for image-to-image translation, yielding accurate and robust characterisation of radio sources, and outperforming existing methodologies.","While this study underscores its significant potential for applications in radio astronomy, we also acknowledge certain limitations that accompany its usage, suggesting directions for further refinement and research."],"url":"http://arxiv.org/abs/2402.10204v1","category":"astro-ph.IM"}
{"created":"2024-02-15 18:56:46","title":"Bridging Associative Memory and Probabilistic Modeling","abstract":"Associative memory and probabilistic modeling are two fundamental topics in artificial intelligence. The first studies recurrent neural networks designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions. Based on the observation that associative memory's energy functions can be seen as probabilistic modeling's negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions. We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new in-context datasets, an approach we term \\textit{in-context learning of energy functions}. Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the evidence lower bound. Third, using tools from associative memory, we analytically and numerically characterize the memory capacity of Gaussian kernel density estimators, a widespread tool in probababilistic modeling. Fourth, we study a widespread implementation choice in transformers -- normalization followed by self attention -- to show it performs clustering on the hypersphere. Altogether, this work urges further exchange of useful ideas between these two continents of artificial intelligence.","sentences":["Associative memory and probabilistic modeling are two fundamental topics in artificial intelligence.","The first studies recurrent neural networks designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions.","Based on the observation that associative memory's energy functions can be seen as probabilistic modeling's negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions.","We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new in-context datasets, an approach we term \\textit{in-context learning of energy functions}.","Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the evidence lower bound.","Third, using tools from associative memory, we analytically and numerically characterize the memory capacity of Gaussian kernel density estimators, a widespread tool in probababilistic modeling.","Fourth, we study a widespread implementation choice in transformers -- normalization followed by self attention -- to show it performs clustering on the hypersphere.","Altogether, this work urges further exchange of useful ideas between these two continents of artificial intelligence."],"url":"http://arxiv.org/abs/2402.10202v1","category":"cs.LG"}
{"created":"2024-02-15 18:55:05","title":"Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention","abstract":"Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times fewer parameters. The code is available at https://github.com/romilbert/samformer.","sentences":["Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting.","To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power.","We further identify the attention of transformers as being responsible for this low generalization capacity.","Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization.","We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets.","In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times fewer parameters.","The code is available at https://github.com/romilbert/samformer."],"url":"http://arxiv.org/abs/2402.10198v1","category":"cs.LG"}
{"created":"2024-02-15 18:51:32","title":"A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents","abstract":"Language agents powered by large language models (LLMs) have seen exploding development. Their capability of using language as a vehicle for thought and communication lends an incredible level of flexibility and versatility. People have quickly capitalized on this capability to connect LLMs to a wide range of external components and environments: databases, tools, the Internet, robotic embodiment, etc. Many believe an unprecedentedly powerful automation technology is emerging. However, new automation technologies come with new safety risks, especially for intricate systems like language agents. There is a surprisingly large gap between the speed and scale of their development and deployment and our understanding of their safety risks. Are we building a house of cards? In this position paper, we present the first systematic effort in mapping adversarial attacks against language agents. We first present a unified conceptual framework for agents with three major components: Perception, Brain, and Action. Under this framework, we present a comprehensive discussion and propose 12 potential attack scenarios against different components of an agent, covering different attack strategies (e.g., input manipulation, adversarial demonstrations, jailbreaking, backdoors). We also draw connections to successful attack strategies previously applied to LLMs. We emphasize the urgency to gain a thorough understanding of language agent risks before their widespread deployment.","sentences":["Language agents powered by large language models (LLMs) have seen exploding development.","Their capability of using language as a vehicle for thought and communication lends an incredible level of flexibility and versatility.","People have quickly capitalized on this capability to connect LLMs to a wide range of external components and environments: databases, tools, the Internet, robotic embodiment, etc.","Many believe an unprecedentedly powerful automation technology is emerging.","However, new automation technologies come with new safety risks, especially for intricate systems like language agents.","There is a surprisingly large gap between the speed and scale of their development and deployment and our understanding of their safety risks.","Are we building a house of cards?","In this position paper, we present the first systematic effort in mapping adversarial attacks against language agents.","We first present a unified conceptual framework for agents with three major components: Perception, Brain, and Action.","Under this framework, we present a comprehensive discussion and propose 12 potential attack scenarios against different components of an agent, covering different attack strategies (e.g., input manipulation, adversarial demonstrations, jailbreaking, backdoors).","We also draw connections to successful attack strategies previously applied to LLMs.","We emphasize the urgency to gain a thorough understanding of language agent risks before their widespread deployment."],"url":"http://arxiv.org/abs/2402.10196v1","category":"cs.CL"}
{"created":"2024-02-15 18:50:24","title":"Simple Tilings of Nilpotent Lie Groups","abstract":"We define simple tilings in the general context of a G-tiling on a Riemannian homogeneous space M to be tilings by \"almost linear\" simplices. As evidence that this definition is natural, we prove that a natural class of tilings of M are MLD to simple ones. We demonstrate the utility of this definition by generalizing previously known results about simple tilings of Euclidean space. In particular, it is shown that a simple tiling space of a connected, simply connected, rational, nilpotent Lie group is homeomorphic to a rational tiling space, and therefore a fiber bundle over a nilmanifold. We further sketch a proof of the fact that there is an isomorphism between \\v{C}ech cohomology and pattern equivariant cohomology of simple tilings in connected, simply connected, nilpotent Lie groups.","sentences":["We define simple tilings in the general context of a G-tiling on a Riemannian homogeneous space M to be tilings by \"almost linear\" simplices.","As evidence that this definition is natural, we prove that a natural class of tilings of M are MLD to simple ones.","We demonstrate the utility of this definition by generalizing previously known results about simple tilings of Euclidean space.","In particular, it is shown that a simple tiling space of a connected, simply connected, rational, nilpotent Lie group is homeomorphic to a rational tiling space, and therefore a fiber bundle over a nilmanifold.","We further sketch a proof of the fact that there is an isomorphism between \\v{C}ech cohomology and pattern equivariant cohomology of simple tilings in connected, simply connected, nilpotent Lie groups."],"url":"http://arxiv.org/abs/2402.10194v1","category":"math.DS"}
{"created":"2024-02-15 18:50:06","title":"BitDelta: Your Fine-Tune May Only Be Worth One Bit","abstract":"Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings.","sentences":["Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks.","Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible.","We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta.","We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance.","This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models.","By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings.","We validate BitDelta through experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings."],"url":"http://arxiv.org/abs/2402.10193v1","category":"cs.LG"}
{"created":"2024-02-15 18:48:32","title":"Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias","abstract":"With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them. While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for a dynamic hypergraph is put forward to describe the agent's training history along with applications to AI and hypergraph visualization. An inductive bias inspired by the remarkably successful few-body interaction models used in quantum many-body physics is formalized for our classical mePS framework and employed to tackle the exponential complexity associated with naive implementations of hypergraphs. We prove that our inductive bias reduces the complexity from exponential to polynomial, with the exponent representing the cutoff on how many particles can interact. We numerically apply our method to two toy environments and a more complex scenario modelling the diagnosis of a broken computer. These environments demonstrate the resource savings provided by an appropriate choice of inductive bias, as well as showcasing aspects of interpretability. A quantum model for mePS is also briefly outlined and some future directions for it are discussed.","sentences":["With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life.","However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions.","This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI).","One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them.","While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously.","To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph.","A definition for a dynamic hypergraph is put forward to describe the agent's training history along with applications to AI and hypergraph visualization.","An inductive bias inspired by the remarkably successful few-body interaction models used in quantum many-body physics is formalized for our classical mePS framework and employed to tackle the exponential complexity associated with naive implementations of hypergraphs.","We prove that our inductive bias reduces the complexity from exponential to polynomial, with the exponent representing the cutoff on how many particles can interact.","We numerically apply our method to two toy environments and a more complex scenario modelling the diagnosis of a broken computer.","These environments demonstrate the resource savings provided by an appropriate choice of inductive bias, as well as showcasing aspects of interpretability.","A quantum model for mePS is also briefly outlined and some future directions for it are discussed."],"url":"http://arxiv.org/abs/2402.10192v1","category":"cs.LG"}
{"created":"2024-02-15 18:48:21","title":"FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss for Unlabeled Clients","abstract":"Federated learning (FL) is a distributed learning paradigm that facilitates collaborative training of a shared global model across devices while keeping data localized. The deployment of FL in numerous real-world applications faces delays, primarily due to the prevalent reliance on supervised tasks. Generating detailed labels at edge devices, if feasible, is demanding, given resource constraints and the imperative for continuous data updates. In addressing these challenges, solutions such as federated semi-supervised learning (FSSL), which relies on unlabeled clients' data and a limited amount of labeled data on the server, become pivotal. In this paper, we propose FedAnchor, an innovative FSSL method that introduces a unique double-head structure, called anchor head, paired with the classification head trained exclusively on labeled anchor data on the server. The anchor head is empowered with a newly designed label contrastive loss based on the cosine similarity metric. Our approach mitigates the confirmation bias and overfitting issues associated with pseudo-labeling techniques based on high-confidence model prediction samples. Extensive experiments on CIFAR10/100 and SVHN datasets demonstrate that our method outperforms the state-of-the-art method by a significant margin in terms of convergence rate and model accuracy.","sentences":["Federated learning (FL) is a distributed learning paradigm that facilitates collaborative training of a shared global model across devices while keeping data localized.","The deployment of FL in numerous real-world applications faces delays, primarily due to the prevalent reliance on supervised tasks.","Generating detailed labels at edge devices, if feasible, is demanding, given resource constraints and the imperative for continuous data updates.","In addressing these challenges, solutions such as federated semi-supervised learning (FSSL), which relies on unlabeled clients' data and a limited amount of labeled data on the server, become pivotal.","In this paper, we propose FedAnchor, an innovative FSSL method that introduces a unique double-head structure, called anchor head, paired with the classification head trained exclusively on labeled anchor data on the server.","The anchor head is empowered with a newly designed label contrastive loss based on the cosine similarity metric.","Our approach mitigates the confirmation bias and overfitting issues associated with pseudo-labeling techniques based on high-confidence model prediction samples.","Extensive experiments on CIFAR10/100 and SVHN datasets demonstrate that our method outperforms the state-of-the-art method by a significant margin in terms of convergence rate and model accuracy."],"url":"http://arxiv.org/abs/2402.10191v1","category":"cs.LG"}
{"created":"2024-02-15 18:46:40","title":"Extremal black hole formation as a critical phenomenon","abstract":"In this paper, we prove that extremal black holes arise on the threshold of gravitational collapse. More precisely, we construct smooth one-parameter families of smooth, spherically symmetric solutions to the Einstein-Maxwell-Vlasov system which interpolate between dispersion and collapse and for which the critical solution is an extremal black hole. Physically, these solutions can be understood as beams of gravitationally self-interacting collisionless charged particles fired into Minkowski space from past infinity. Depending on the precise value of the parameter, we show that the Vlasov matter either disperses due to the combined effects of angular momentum and electromagnetic repulsion, or undergoes gravitational collapse. At the critical value of the parameter, an extremal Reissner-Nordstr\\\"om black hole is formed. No naked singularities occur as the extremal threshold is crossed. We call this critical phenomenon extremal critical collapse and the present work constitutes the first rigorous result on the black hole formation threshold in general relativity.","sentences":["In this paper, we prove that extremal black holes arise on the threshold of gravitational collapse.","More precisely, we construct smooth one-parameter families of smooth, spherically symmetric solutions to the Einstein-Maxwell-Vlasov system which interpolate between dispersion and collapse and for which the critical solution is an extremal black hole.","Physically, these solutions can be understood as beams of gravitationally self-interacting collisionless charged particles fired into Minkowski space from past infinity.","Depending on the precise value of the parameter, we show that the Vlasov matter either disperses due to the combined effects of angular momentum and electromagnetic repulsion, or undergoes gravitational collapse.","At the critical value of the parameter, an extremal Reissner-Nordstr\\\"om black hole is formed.","No naked singularities occur as the extremal threshold is crossed.","We call this critical phenomenon extremal critical collapse and the present work constitutes the first rigorous result on the black hole formation threshold in general relativity."],"url":"http://arxiv.org/abs/2402.10190v1","category":"gr-qc"}
{"created":"2024-02-15 18:43:53","title":"Euclid preparation. Measuring detailed galaxy morphologies for Euclid with Machine Learning","abstract":"The Euclid mission is expected to image millions of galaxies with high resolution, providing an extensive dataset to study galaxy evolution. We investigate the application of deep learning to predict the detailed morphologies of galaxies in Euclid using Zoobot a convolutional neural network pretrained with 450000 galaxies from the Galaxy Zoo project. We adapted Zoobot for emulated Euclid images, generated based on Hubble Space Telescope COSMOS images, and with labels provided by volunteers in the Galaxy Zoo: Hubble project. We demonstrate that the trained Zoobot model successfully measures detailed morphology for emulated Euclid images. It effectively predicts whether a galaxy has features and identifies and characterises various features such as spiral arms, clumps, bars, disks, and central bulges. When compared to volunteer classifications Zoobot achieves mean vote fraction deviations of less than 12% and an accuracy above 91% for the confident volunteer classifications across most morphology types. However, the performance varies depending on the specific morphological class. For the global classes such as disk or smooth galaxies, the mean deviations are less than 10%, with only 1000 training galaxies necessary to reach this performance. For more detailed structures and complex tasks like detecting and counting spiral arms or clumps, the deviations are slightly higher, around 12% with 60000 galaxies used for training. In order to enhance the performance on complex morphologies, we anticipate that a larger pool of labelled galaxies is needed, which could be obtained using crowdsourcing. Finally, our findings imply that the model can be effectively adapted to new morphological labels. We demonstrate this adaptability by applying Zoobot to peculiar galaxies. In summary, our trained Zoobot CNN can readily predict morphological catalogues for Euclid images.","sentences":["The Euclid mission is expected to image millions of galaxies with high resolution, providing an extensive dataset to study galaxy evolution.","We investigate the application of deep learning to predict the detailed morphologies of galaxies in Euclid using Zoobot a convolutional neural network pretrained with 450000 galaxies from the Galaxy Zoo project.","We adapted Zoobot for emulated Euclid images, generated based on Hubble Space Telescope COSMOS images, and with labels provided by volunteers in the Galaxy Zoo: Hubble project.","We demonstrate that the trained Zoobot model successfully measures detailed morphology for emulated Euclid images.","It effectively predicts whether a galaxy has features and identifies and characterises various features such as spiral arms, clumps, bars, disks, and central bulges.","When compared to volunteer classifications Zoobot achieves mean vote fraction deviations of less than 12% and an accuracy above 91% for the confident volunteer classifications across most morphology types.","However, the performance varies depending on the specific morphological class.","For the global classes such as disk or smooth galaxies, the mean deviations are less than 10%, with only 1000 training galaxies necessary to reach this performance.","For more detailed structures and complex tasks like detecting and counting spiral arms or clumps, the deviations are slightly higher, around 12% with 60000 galaxies used for training.","In order to enhance the performance on complex morphologies, we anticipate that a larger pool of labelled galaxies is needed, which could be obtained using crowdsourcing.","Finally, our findings imply that the model can be effectively adapted to new morphological labels.","We demonstrate this adaptability by applying Zoobot to peculiar galaxies.","In summary, our trained Zoobot CNN can readily predict morphological catalogues for Euclid images."],"url":"http://arxiv.org/abs/2402.10187v1","category":"astro-ph.GA"}
{"created":"2024-02-15 18:41:35","title":"Self-consistent Validation for Machine Learning Electronic Structure","abstract":"Machine learning has emerged as a significant approach to efficiently tackle electronic structure problems. Despite its potential, there is less guarantee for the model to generalize to unseen data that hinders its application in real-world scenarios. To address this issue, a technique has been proposed to estimate the accuracy of the predictions. This method integrates machine learning with self-consistent field methods to achieve both low validation cost and interpret-ability. This, in turn, enables exploration of the model's ability with active learning and instills confidence in its integration into real-world studies.","sentences":["Machine learning has emerged as a significant approach to efficiently tackle electronic structure problems.","Despite its potential, there is less guarantee for the model to generalize to unseen data that hinders its application in real-world scenarios.","To address this issue, a technique has been proposed to estimate the accuracy of the predictions.","This method integrates machine learning with self-consistent field methods to achieve both low validation cost and interpret-ability.","This, in turn, enables exploration of the model's ability with active learning and instills confidence in its integration into real-world studies."],"url":"http://arxiv.org/abs/2402.10186v1","category":"cs.LG"}
{"created":"2024-02-15 18:39:24","title":"Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective","abstract":"There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of our analysis is the superiority of the tree-based information structure in reward modeling, compared to chain-based baselines adopted by conventional RLHF methods. We derive that under highly complex contexts with limited data, the tree-based reward model (RM) induces up to $\\Theta(\\log n/\\log\\log n)$ times less variance than chain-based RM where $n$ is the dataset size. To validate our theoretical contribution, we demonstrate that on three different NLP tasks, the tree-based RM achieves 65% win rate on average against chain-based baselines. Looking forward, we hope our framework can serve as a step towards understanding goal misgeneralization.","sentences":["There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance.","Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling.","Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions.","Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior.","Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF.","To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space.","A key insight of our analysis is the superiority of the tree-based information structure in reward modeling, compared to chain-based baselines adopted by conventional RLHF methods.","We derive that under highly complex contexts with limited data, the tree-based reward model (RM) induces up to $\\Theta(\\log n/\\log\\log n)$ times less variance than chain-based RM where $n$ is the dataset size.","To validate our theoretical contribution, we demonstrate that on three different NLP tasks, the tree-based RM achieves 65% win rate on average against chain-based baselines.","Looking forward, we hope our framework can serve as a step towards understanding goal misgeneralization."],"url":"http://arxiv.org/abs/2402.10184v1","category":"cs.LG"}
{"created":"2024-02-15 18:34:17","title":"Intent Demonstration in General-Sum Dynamic Games via Iterative Linear-Quadratic Approximations","abstract":"Autonomous agents should be able to coordinate with other agents without knowing their intents ahead of time. While prior work has studied how agents can gather information about the intent of others, in this work we study the inverse problem: how agents can demonstrate their intent to others, within the framework of general-sum dynamic games. We first present a model of this intent demonstration problem and then propose an algorithm that enables an agent to trade off their task performance and intent demonstration to improve the overall system's performance. To scale to continuous states and action spaces as well as to nonlinear dynamics and costs, our algorithm leverages linear-quadratic approximations with an efficient intent teaching guarantee. Our empirical results show that intent demonstration accelerates other agents' learning and enables the demonstrating agent to balance task performance with intent expression.","sentences":["Autonomous agents should be able to coordinate with other agents without knowing their intents ahead of time.","While prior work has studied how agents can gather information about the intent of others, in this work we study the inverse problem: how agents can demonstrate their intent to others, within the framework of general-sum dynamic games.","We first present a model of this intent demonstration problem and then propose an algorithm that enables an agent to trade off their task performance and intent demonstration to improve the overall system's performance.","To scale to continuous states and action spaces as well as to nonlinear dynamics and costs, our algorithm leverages linear-quadratic approximations with an efficient intent teaching guarantee.","Our empirical results show that intent demonstration accelerates other agents' learning and enables the demonstrating agent to balance task performance with intent expression."],"url":"http://arxiv.org/abs/2402.10182v1","category":"eess.SY"}
{"created":"2024-02-15 18:33:21","title":"Extracting randomness from quantum 'magic'","abstract":"Magic is a critical property of quantum states that plays a pivotal role in fault-tolerant quantum computation. Simultaneously, random states have emerged as a key element in various randomized techniques within contemporary quantum science. In this study, we establish a direct connection between these two notions. More specifically, our research demonstrates that when a subsystem of a quantum state is measured, the resultant projected ensemble of the unmeasured subsystem can exhibit a high degree of randomness that is enhanced by the inherent 'magic' of the underlying state. We demonstrate this relationship rigorously for quantum state 2-designs, and present compelling numerical evidence to support its validity for higher-order quantum designs. Our findings suggest an efficient approach for leveraging magic as a resource to generate random quantum states.","sentences":["Magic is a critical property of quantum states that plays a pivotal role in fault-tolerant quantum computation.","Simultaneously, random states have emerged as a key element in various randomized techniques within contemporary quantum science.","In this study, we establish a direct connection between these two notions.","More specifically, our research demonstrates that when a subsystem of a quantum state is measured, the resultant projected ensemble of the unmeasured subsystem can exhibit a high degree of randomness that is enhanced by the inherent 'magic' of the underlying state.","We demonstrate this relationship rigorously for quantum state 2-designs, and present compelling numerical evidence to support its validity for higher-order quantum designs.","Our findings suggest an efficient approach for leveraging magic as a resource to generate random quantum states."],"url":"http://arxiv.org/abs/2402.10181v1","category":"quant-ph"}
{"created":"2024-02-15 18:31:39","title":"Approaching the absorption limit with monolayer semiconductor superlattices","abstract":"Optical absorption plays a central role in optoelectronic and photonic technologies. Strongly absorbing materials are thus needed for efficient and miniaturized devices. There exists, however, a fundamental limit of 50% absorptance for any ultrathin film in a symmetric environment. Although deviating from these conditions allows higher absorption, finding the thinnest possible material with the highest intrinsic absorption is still desirable. Here, we demonstrate strong absorption approaching the fundamental limit by artificially stacking WS$_2$ monolayers into superlattices. We compare three simple approaches based on different spacer materials to surpass the record peak absorptance of WS2 monolayers, which stands at 16% on ideal substrates. Through direct monolayer stacking without an intentional spacer, we reach a transmittance contrast of 30% for an artificial bilayer, although with limited control over interlayer distance. Using a molecular spacer via spin coating, we demonstrate controllable spacer thickness in a bilayer, reaching 28% transmittance contrast while increasing photoluminescence thanks to doping. Finally, we exploit atomic layer deposition of alumina spacers to boost the transmittance contrast to 36% for a 4-monolayer superlattice. Our results demonstrate that monolayer superlattices are a powerful platform directly applicable to improve exciton-polariton phenomena such as strong light-matter coupling and nanophotonic devices such as modulators and photodetectors.","sentences":["Optical absorption plays a central role in optoelectronic and photonic technologies.","Strongly absorbing materials are thus needed for efficient and miniaturized devices.","There exists, however, a fundamental limit of 50% absorptance for any ultrathin film in a symmetric environment.","Although deviating from these conditions allows higher absorption, finding the thinnest possible material with the highest intrinsic absorption is still desirable.","Here, we demonstrate strong absorption approaching the fundamental limit by artificially stacking WS$_2$ monolayers into superlattices.","We compare three simple approaches based on different spacer materials to surpass the record peak absorptance of WS2 monolayers, which stands at 16% on ideal substrates.","Through direct monolayer stacking without an intentional spacer, we reach a transmittance contrast of 30% for an artificial bilayer, although with limited control over interlayer distance.","Using a molecular spacer via spin coating, we demonstrate controllable spacer thickness in a bilayer, reaching 28% transmittance contrast while increasing photoluminescence thanks to doping.","Finally, we exploit atomic layer deposition of alumina spacers to boost the transmittance contrast to 36% for a 4-monolayer superlattice.","Our results demonstrate that monolayer superlattices are a powerful platform directly applicable to improve exciton-polariton phenomena such as strong light-matter coupling and nanophotonic devices such as modulators and photodetectors."],"url":"http://arxiv.org/abs/2402.10179v1","category":"physics.optics"}
{"created":"2024-02-15 18:27:37","title":"TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation","abstract":"The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench is designed to assess agents' abilities in memory, planning, and tool usage across tasks of varying complexity. Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios.","sentences":["The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks.","However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability.","To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG).","This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks.","Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks.","In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system.","ItineraryBench is designed to assess agents' abilities in memory, planning, and tool usage across tasks of varying complexity.","Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios."],"url":"http://arxiv.org/abs/2402.10178v1","category":"cs.CL"}
{"created":"2024-02-15 18:27:18","title":"Large Scale Constrained Clustering With Reinforcement Learning","abstract":"Given a network, allocating resources at clusters level, rather than at each node, enhances efficiency in resource allocation and usage. In this paper, we study the problem of finding fully connected disjoint clusters to minimize the intra-cluster distances and maximize the number of nodes assigned to the clusters, while also ensuring that no two nodes within a cluster exceed a threshold distance. While the problem can easily be formulated using a binary linear model, traditional combinatorial optimization solvers struggle when dealing with large-scale instances. We propose an approach to solve this constrained clustering problem via reinforcement learning. Our method involves training an agent to generate both feasible and (near) optimal solutions. The agent learns problem-specific heuristics, tailored to the instances encountered in this task. In the results section, we show that our algorithm finds near optimal solutions, even for large scale instances.","sentences":["Given a network, allocating resources at clusters level, rather than at each node, enhances efficiency in resource allocation and usage.","In this paper, we study the problem of finding fully connected disjoint clusters to minimize the intra-cluster distances and maximize the number of nodes assigned to the clusters, while also ensuring that no two nodes within a cluster exceed a threshold distance.","While the problem can easily be formulated using a binary linear model, traditional combinatorial optimization solvers struggle when dealing with large-scale instances.","We propose an approach to solve this constrained clustering problem via reinforcement learning.","Our method involves training an agent to generate both feasible and (near) optimal solutions.","The agent learns problem-specific heuristics, tailored to the instances encountered in this task.","In the results section, we show that our algorithm finds near optimal solutions, even for large scale instances."],"url":"http://arxiv.org/abs/2402.10177v1","category":"cs.LG"}
{"created":"2024-02-15 18:26:11","title":"OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset","abstract":"Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.","sentences":["Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills.","Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses.","A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs.","Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs.","The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model.","Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models.","We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license."],"url":"http://arxiv.org/abs/2402.10176v1","category":"cs.CL"}
{"created":"2024-02-15 18:23:39","title":"Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence","abstract":"Recent large language models (LLMs) have shown remarkable performance in aligning generated text with user intentions across various tasks. When it comes to long-form text generation, there has been a growing interest in generation from a discourse coherence perspective. However, existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence. The development of discourse-specific automatic evaluation methods for assessing the output of LLMs warrants greater focus and exploration. In this paper, we present a novel automatic metric designed to quantify the discourse divergence between two long-form articles. Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and GPT-4 coherence evaluation, outperforming existing evaluation methods.","sentences":["Recent large language models (LLMs) have shown remarkable performance in aligning generated text with user intentions across various tasks.","When it comes to long-form text generation, there has been a growing interest in generation from a discourse coherence perspective.","However, existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence.","The development of discourse-specific automatic evaluation methods for assessing the output of LLMs warrants greater focus and exploration.","In this paper, we present a novel automatic metric designed to quantify the discourse divergence between two long-form articles.","Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and GPT-4 coherence evaluation, outperforming existing evaluation methods."],"url":"http://arxiv.org/abs/2402.10175v1","category":"cs.CL"}
{"created":"2024-02-15 18:23:06","title":"Overapproximation of Non-Linear Integer Arithmetic for Smart Contract Verification","abstract":"The need to solve non-linear arithmetic constraints presents a major obstacle to the automatic verification of smart contracts. In this case study we focus on the two overapproximation techniques used by the industry verification tool Certora Prover: overapproximation of non-linear integer arithmetic using linear integer arithmetic and using non-linear real arithmetic. We compare the performance of contemporary SMT solvers on verification conditions produced by the Certora Prover using these two approximations against the natural non-linear integer arithmetic encoding. Our evaluation shows that the use of the overapproximation methods leads to solving a significant number of new problems.","sentences":["The need to solve non-linear arithmetic constraints presents a major obstacle to the automatic verification of smart contracts.","In this case study we focus on the two overapproximation techniques used by the industry verification tool Certora Prover: overapproximation of non-linear integer arithmetic using linear integer arithmetic and using non-linear real arithmetic.","We compare the performance of contemporary SMT solvers on verification conditions produced by the Certora Prover using these two approximations against the natural non-linear integer arithmetic encoding.","Our evaluation shows that the use of the overapproximation methods leads to solving a significant number of new problems."],"url":"http://arxiv.org/abs/2402.10174v1","category":"cs.LO"}
{"created":"2024-02-15 18:19:18","title":"OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models","abstract":"Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20\\%$ and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30\\%$.","sentences":["Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare.","However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques.","This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions.","OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations.","OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts.","Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20\\%$ and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30\\%$."],"url":"http://arxiv.org/abs/2402.10172v1","category":"cs.AI"}
{"created":"2024-02-15 18:19:16","title":"Data Engineering for Scaling Language Models to 128K Context","abstract":"We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \\textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \\textit{quantity} and \\textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \\textit{domain balance} and \\textit{length upsampling}. Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.","sentences":["We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering.","We hypothesize that long context modeling, in particular \\textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture.","We investigate the \\textit{quantity} and \\textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \\textit{domain balance} and \\textit{length upsampling}.","Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important.","We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K."],"url":"http://arxiv.org/abs/2402.10171v1","category":"cs.CL"}
{"created":"2024-02-15 18:11:02","title":"DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning","abstract":"A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music data base that are closely related to the presented query sequence.","sentences":["A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike.","Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections.","In this work, we propose a deep learning based approach to Raga recognition.","Our approach employs efficient pre possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN).","We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole.","Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task.","Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music data base that are closely related to the presented query sequence."],"url":"http://arxiv.org/abs/2402.10168v1","category":"cs.SD"}
{"created":"2024-02-15 18:10:29","title":"Marked length spectrum rigidity in groups with contracting elements","abstract":"This paper presents a study of the well-known marked length spectrum rigidity problem in the coarse-geometric setting. For any two (possibly non-proper) group actions $G\\curvearrowright X_1$ and $G\\curvearrowright X_2$ with contracting property, we prove that if the two actions have the same marked length spectrum, then the orbit map $Go_1\\to Go_2$ must be a rough isometry. In the special case of cusp-uniform actions, the rough isometry can be extended to the entire space. This generalizes the existing results in hyperbolic groups and relatively hyperbolic groups. In addition, we prove a finer marked length spectrum rigidity from confined subgroups and further, geometrically dense subgroups. Our proof is based on the Extension Lemma and uses purely elementary metric geometry. This study produces new results and recovers existing ones for many more interesting groups through a unified and elementary approach.","sentences":["This paper presents a study of the well-known marked length spectrum rigidity problem in the coarse-geometric setting.","For any two (possibly non-proper) group actions $G\\curvearrowright X_1$ and $G\\curvearrowright X_2$ with contracting property, we prove that if the two actions have the same marked length spectrum, then the orbit map $Go_1\\to Go_2$ must be a rough isometry.","In the special case of cusp-uniform actions, the rough isometry can be extended to the entire space.","This generalizes the existing results in hyperbolic groups and relatively hyperbolic groups.","In addition, we prove a finer marked length spectrum rigidity from confined subgroups and further, geometrically dense subgroups.","Our proof is based on the Extension Lemma and uses purely elementary metric geometry.","This study produces new results and recovers existing ones for many more interesting groups through a unified and elementary approach."],"url":"http://arxiv.org/abs/2402.10165v1","category":"math.GR"}
{"created":"2024-02-15 18:09:41","title":"Random features and polynomial rules","abstract":"Random features models play a distinguished role in the theory of deep learning, describing the behavior of neural networks close to their infinite-width limit. In this work, we present a thorough analysis of the generalization performance of random features models for generic supervised learning problems with Gaussian data. Our approach, built with tools from the statistical mechanics of disordered systems, maps the random features model to an equivalent polynomial model, and allows us to plot average generalization curves as functions of the two main control parameters of the problem: the number of random features $N$ and the size $P$ of the training set, both assumed to scale as powers in the input dimension $D$. Our results extend the case of proportional scaling between $N$, $P$ and $D$. They are in accordance with rigorous bounds known for certain particular learning tasks and are in quantitative agreement with numerical experiments performed over many order of magnitudes of $N$ and $P$. We find good agreement also far from the asymptotic limits where $D\\to \\infty$ and at least one between $P/D^K$, $N/D^L$ remains finite.","sentences":["Random features models play a distinguished role in the theory of deep learning, describing the behavior of neural networks close to their infinite-width limit.","In this work, we present a thorough analysis of the generalization performance of random features models for generic supervised learning problems with Gaussian data.","Our approach, built with tools from the statistical mechanics of disordered systems, maps the random features model to an equivalent polynomial model, and allows us to plot average generalization curves as functions of the two main control parameters of the problem: the number of random features $N$ and the size $P$ of the training set, both assumed to scale as powers in the input dimension $D$. Our results extend the case of proportional scaling between $N$, $P$ and $D$. They are in accordance with rigorous bounds known for certain particular learning tasks and are in quantitative agreement with numerical experiments performed over many order of magnitudes of $N$ and $P$. We find good agreement also far from the asymptotic limits where $D\\to \\infty$ and at least one between $P/D^K$, $N/D^L$ remains finite."],"url":"http://arxiv.org/abs/2402.10164v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-15 18:08:10","title":"Semi-classical dilaton gravity and the very blunt defect expansion","abstract":"We explore dilaton gravity with general dilaton potentials in the semi-classical limit viewed both as a gas of blunt defects and also as a semi-classical theory in its own right. We compare the exact defect gas picture with that obtained by naively canonically quantizing the theory in geodesic gauge. We find a subtlety in the canonical approach due to a non-perturbative ambiguity in geodesic gauge. Unlike in JT gravity, this ambiguity arises already at the disk level. This leads to a distinct mechanism from that in JT gravity by which the semi-classical approximation breaks down at low temperatures. Along the way, we propose that new, previously un-studied saddles contribute to the density of states of dilaton gravity. This in particular leads to a re-interpretation of the disk-level density of states in JT gravity in terms of two saddles with fixed energy boundary conditions: the disk, which caps off on the outer horizon, and another, sub-leading complex saddle which caps off on the inner horizon. When the theory is studied using a defect expansion, we show how the smooth classical geometries of dilaton gravity arise from a dense gas of very blunt defects in the $G_N \\to 0$ limit. The classical saddle points arise from a balance between the attractive force on the defects toward negative dilaton and a statistical pressure from the entropy of the configuration. We end with speculations on the nature of the space-like singularity present inside black holes described by certain dilaton potentials.","sentences":["We explore dilaton gravity with general dilaton potentials in the semi-classical limit viewed both as a gas of blunt defects and also as a semi-classical theory in its own right.","We compare the exact defect gas picture with that obtained by naively canonically quantizing the theory in geodesic gauge.","We find a subtlety in the canonical approach due to a non-perturbative ambiguity in geodesic gauge.","Unlike in JT gravity, this ambiguity arises already at the disk level.","This leads to a distinct mechanism from that in JT gravity by which the semi-classical approximation breaks down at low temperatures.","Along the way, we propose that new, previously un-studied saddles contribute to the density of states of dilaton gravity.","This in particular leads to a re-interpretation of the disk-level density of states in JT gravity in terms of two saddles with fixed energy boundary conditions: the disk, which caps off on the outer horizon, and another, sub-leading complex saddle which caps off on the inner horizon.","When the theory is studied using a defect expansion, we show how the smooth classical geometries of dilaton gravity arise from a dense gas of very blunt defects in the $G_N \\to 0$ limit.","The classical saddle points arise from a balance between the attractive force on the defects toward negative dilaton and a statistical pressure from the entropy of the configuration.","We end with speculations on the nature of the space-like singularity present inside black holes described by certain dilaton potentials."],"url":"http://arxiv.org/abs/2402.10162v1","category":"hep-th"}
{"created":"2024-02-15 18:06:24","title":"Robotic Exploration using Generalized Behavioral Entropy","abstract":"This work presents and evaluates a novel strategy for robotic exploration that leverages human models of uncertainty perception. To do this, we introduce a measure of uncertainty that we term ``Behavioral entropy'', which builds on Prelec's probability weighting from Behavioral Economics. We show that the new operator is an admissible generalized entropy, analyze its theoretical properties and compare it with other common formulations such as Shannon's and Renyi's. In particular, we discuss how the new formulation is more expressive in the sense of measures of sensitivity and perceptiveness to uncertainty introduced here. Then we use Behavioral entropy to define a new type of utility function that can guide a frontier-based environment exploration process. The approach's benefits are illustrated and compared in a Proof-of-Concept and ROS-unity simulation environment with a Clearpath Warthog robot. We show that the robot equipped with Behavioral entropy explores faster than Shannon and Renyi entropies.","sentences":["This work presents and evaluates a novel strategy for robotic exploration that leverages human models of uncertainty perception.","To do this, we introduce a measure of uncertainty that we term ``Behavioral entropy'', which builds on Prelec's probability weighting from Behavioral Economics.","We show that the new operator is an admissible generalized entropy, analyze its theoretical properties and compare it with other common formulations such as Shannon's and Renyi's.","In particular, we discuss how the new formulation is more expressive in the sense of measures of sensitivity and perceptiveness to uncertainty introduced here.","Then we use Behavioral entropy to define a new type of utility function that can guide a frontier-based environment exploration process.","The approach's benefits are illustrated and compared in a Proof-of-Concept and ROS-unity simulation environment with a Clearpath Warthog robot.","We show that the robot equipped with Behavioral entropy explores faster than Shannon and Renyi entropies."],"url":"http://arxiv.org/abs/2402.10161v1","category":"cs.RO"}
{"created":"2024-02-15 18:02:47","title":"InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization","abstract":"Estimating mutual correlations between random variables or data streams is essential for intelligent behavior and decision-making. As a fundamental quantity for measuring statistical relationships, mutual information has been extensively studied and utilized for its generality and equitability. However, existing methods often lack the efficiency needed for real-time applications, such as test-time optimization of a neural network, or the differentiability required for end-to-end learning, like histograms. We introduce a neural network called InfoNet, which directly outputs mutual information estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures. By maximizing a dual formulation of mutual information through large-scale simulated training, our approach circumvents time-consuming test-time optimization and offers generalization ability. We evaluate the effectiveness and generalization of our proposed mutual information estimation scheme on various families of distributions and applications. Our results demonstrate that InfoNet and its training process provide a graceful efficiency-accuracy trade-off and order-preserving properties. We will make the code and models available as a comprehensive toolbox to facilitate studies in different fields requiring real-time mutual information estimation.","sentences":["Estimating mutual correlations between random variables or data streams is essential for intelligent behavior and decision-making.","As a fundamental quantity for measuring statistical relationships, mutual information has been extensively studied and utilized for its generality and equitability.","However, existing methods often lack the efficiency needed for real-time applications, such as test-time optimization of a neural network, or the differentiability required for end-to-end learning, like histograms.","We introduce a neural network called InfoNet, which directly outputs mutual information estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures.","By maximizing a dual formulation of mutual information through large-scale simulated training, our approach circumvents time-consuming test-time optimization and offers generalization ability.","We evaluate the effectiveness and generalization of our proposed mutual information estimation scheme on various families of distributions and applications.","Our results demonstrate that InfoNet and its training process provide a graceful efficiency-accuracy trade-off and order-preserving properties.","We will make the code and models available as a comprehensive toolbox to facilitate studies in different fields requiring real-time mutual information estimation."],"url":"http://arxiv.org/abs/2402.10158v1","category":"cs.IT"}
{"created":"2024-02-15 18:01:09","title":"Empirically assessing the plausibility of unconfoundedness in observational studies","abstract":"The possibility of unmeasured confounding is one of the main limitations for causal inference from observational studies. There are different methods for partially empirically assessing the plausibility of unconfoundedness. However, most currently available methods require (at least partial) assumptions about the confounding structure, which may be difficult to know in practice. In this paper we describe a simple strategy for empirically assessing the plausibility of conditional unconfoundedness (i.e., whether the candidate set of covariates suffices for confounding adjustment) which does not require any assumptions about the confounding structure, requiring instead assumptions related to temporal ordering between covariates, exposure and outcome (which can be guaranteed by design), measurement error and selection into the study. The proposed method essentially relies on testing the association between a subset of covariates (those associated with the exposure given all other covariates) and the outcome conditional on the remaining covariates and the exposure. We describe the assumptions underlying the method, provide proofs, use simulations to corroborate the theory and illustrate the method with an applied example assessing the causal effect of length-for-age measured in childhood and intelligence quotient measured in adulthood using data from the 1982 Pelotas (Brazil) birth cohort. We also discuss the implications of measurement error and some important limitations.","sentences":["The possibility of unmeasured confounding is one of the main limitations for causal inference from observational studies.","There are different methods for partially empirically assessing the plausibility of unconfoundedness.","However, most currently available methods require (at least partial) assumptions about the confounding structure, which may be difficult to know in practice.","In this paper we describe a simple strategy for empirically assessing the plausibility of conditional unconfoundedness (i.e., whether the candidate set of covariates suffices for confounding adjustment) which does not require any assumptions about the confounding structure, requiring instead assumptions related to temporal ordering between covariates, exposure and outcome (which can be guaranteed by design), measurement error and selection into the study.","The proposed method essentially relies on testing the association between a subset of covariates (those associated with the exposure given all other covariates) and the outcome conditional on the remaining covariates and the exposure.","We describe the assumptions underlying the method, provide proofs, use simulations to corroborate the theory and illustrate the method with an applied example assessing the causal effect of length-for-age measured in childhood and intelligence quotient measured in adulthood using data from the 1982 Pelotas (Brazil) birth cohort.","We also discuss the implications of measurement error and some important limitations."],"url":"http://arxiv.org/abs/2402.10156v1","category":"stat.ME"}
{"created":"2024-02-15 18:00:33","title":"The Generalized Riemann Zeta heat flow","abstract":"We consider the PDE flow associated to Riemann zeta and general Dirichlet $L$-functions. These are models characterized by nonlinearities appearing in classical number theory problems, and generalizing the classical holomorphic Riemann flow studied by Broughan and Barnett. Each zero of a Dirichlet $L$-function is an exact solution of the model. In this paper, we first show local existence of bounded continuous solutions in the Duhamel sense to any Dirichlet $L$-function flow with initial condition far from the pole (as long as this exists). In a second result, we prove global existence in the case of nonlinearities of the form Dirichlet $L$-functions and data initially on the right of a possible pole. Additional global well-posedness and convergence results are proved in the case of the defocusing Riemann zeta nonlinearity and initial data located on the real line and close to the trivial zeros of the zeta. The asymptotic stability of any stable zero is also proved. Finally, in the Riemann zeta case, we consider the ``focusing'' model, and prove blow-up of solutions near the pole $s=1$.","sentences":["We consider the PDE flow associated to Riemann zeta and general Dirichlet $L$-functions.","These are models characterized by nonlinearities appearing in classical number theory problems, and generalizing the classical holomorphic Riemann flow studied by Broughan and Barnett.","Each zero of a Dirichlet $L$-function is an exact solution of the model.","In this paper, we first show local existence of bounded continuous solutions in the Duhamel sense to any Dirichlet $L$-function flow with initial condition far from the pole (as long as this exists).","In a second result, we prove global existence in the case of nonlinearities of the form Dirichlet $L$-functions and data initially on the right of a possible pole.","Additional global well-posedness and convergence results are proved in the case of the defocusing Riemann zeta nonlinearity and initial data located on the real line and close to the trivial zeros of the zeta.","The asymptotic stability of any stable zero is also proved.","Finally, in the Riemann zeta case, we consider the ``focusing'' model, and prove blow-up of solutions near the pole $s=1$."],"url":"http://arxiv.org/abs/2402.10154v1","category":"math.AP"}
{"created":"2024-02-15 18:00:02","title":"Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients","abstract":"Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabetes-related questions on daily meal choices and assessing the potential risks associated with the suggested diet. Our findings show that the proposed agent demonstrates superior performance in generating responses to manage essential nutrients.","sentences":["Effective diabetes management is crucial for maintaining health in diabetic patients.","Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy.","However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses.","In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients.","We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities.","This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines.","We compare the proposed CHA with GPT4.","Our evaluation includes 100 diabetes-related questions on daily meal choices and assessing the potential risks associated with the suggested diet.","Our findings show that the proposed agent demonstrates superior performance in generating responses to manage essential nutrients."],"url":"http://arxiv.org/abs/2402.10153v1","category":"cs.CL"}
{"created":"2024-02-15 17:58:50","title":"A new type of simplified inverse Lax-Wendroff boundary treatment I: hyperbolic conservation laws","abstract":"In this paper, we design a new kind of high order inverse Lax-Wendroff (ILW) boundary treatment for solving hyperbolic conservation laws with finite difference method on a Cartesian mesh. This new ILW method decomposes the construction of ghost point values near inflow boundary into two steps: interpolation and extrapolation. At first, we impose values of some artificial auxiliary points through a polynomial interpolating the interior points near the boundary. Then, we will construct a Hermite extrapolation based on those auxiliary point values and the spatial derivatives at boundary obtained via the ILW procedure. This polynomial will give us the approximation to the ghost point value. By an appropriate selection of those artificial auxiliary points, high-order accuracy and stable results can be achieved. Moreover, theoretical analysis indicates that comparing with the original ILW method, especially for higher order accuracy, the new proposed one would require fewer terms using the relatively complicated ILW procedure and thus improve computational efficiency on the premise of maintaining accuracy and stability. We perform numerical experiments on several benchmarks, including one- and two-dimensional scalar equations and systems. The robustness and efficiency of the proposed scheme is numerically verified.","sentences":["In this paper, we design a new kind of high order inverse Lax-Wendroff (ILW) boundary treatment for solving hyperbolic conservation laws with finite difference method on a Cartesian mesh.","This new ILW method decomposes the construction of ghost point values near inflow boundary into two steps: interpolation and extrapolation.","At first, we impose values of some artificial auxiliary points through a polynomial interpolating the interior points near the boundary.","Then, we will construct a Hermite extrapolation based on those auxiliary point values and the spatial derivatives at boundary obtained via the ILW procedure.","This polynomial will give us the approximation to the ghost point value.","By an appropriate selection of those artificial auxiliary points, high-order accuracy and stable results can be achieved.","Moreover, theoretical analysis indicates that comparing with the original ILW method, especially for higher order accuracy, the new proposed one would require fewer terms using the relatively complicated ILW procedure and thus improve computational efficiency on the premise of maintaining accuracy and stability.","We perform numerical experiments on several benchmarks, including one- and two-dimensional scalar equations and systems.","The robustness and efficiency of the proposed scheme is numerically verified."],"url":"http://arxiv.org/abs/2402.10152v1","category":"math.NA"}
{"created":"2024-02-15 17:57:54","title":"$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning","abstract":"In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based mutual information. In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based mutual information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel, we derive an $f$-Gaussian similarity with better interpretability and empirical performance. Finally, we identify close relationships between the $f$-MICL objective and several popular InfoNCE-based objectives. Using benchmark tasks from both vision and natural language, we empirically evaluate $f$-MICL with different $f$-divergences on various architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that $f$-MICL generally outperforms the benchmarks and the best-performing $f$-divergence is task and dataset dependent.","sentences":["In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based mutual information.","In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective?","(2) Besides the popular cosine similarity, can we design a better similarity function?","We provide answers to both questions by generalizing the KL-based mutual information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL) using the $f$-divergences.","To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance.","For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel, we derive an $f$-Gaussian similarity with better interpretability and empirical performance.","Finally, we identify close relationships between the $f$-MICL objective and several popular InfoNCE-based objectives.","Using benchmark tasks from both vision and natural language, we empirically evaluate $f$-MICL with different $f$-divergences on various architectures (SimCLR, MoCo, and MoCo v3) and datasets.","We observe that $f$-MICL generally outperforms the benchmarks and the best-performing $f$-divergence is task and dataset dependent."],"url":"http://arxiv.org/abs/2402.10150v1","category":"cs.LG"}
{"created":"2024-02-15 17:51:47","title":"Tracer dynamics in polymer networks: generalized Langevin description","abstract":"Tracer diffusion in polymer networks and hydrogels is relevant in biology and technology, while it also constitutes an interesting model process for the dynamics of molecules in fluctuating, heterogeneous soft matter. Here, we study systematically the time-dependent dynamics and (non-Markovian) memory effects of tracers in polymer networks based on (Markovian) implicit-solvent Langevin simulations. In particular, we consider spherical tracer solutes at high dilution in regular, tetrafunctional bead-spring polymer networks, and control the tracer-network Lennard-Jones (LJ) interactions and the polymer density. Based on the analysis of the memory (friction) kernels, we recover the expected long-time transport coefficients, and demonstrate how the short-time tracer dynamics, polymer fluctuations, and the viscoelastic response are interlinked. Further, we fit the characteristic memory modes of the tracers with damped harmonic oscillations and identify LJ contributions, bond vibrations, and slow network relaxations, which enter the kernel with an almost linear scaling with the LJ attractions. This procedure proposes a reduced functional form for the tracer memory, allowing for a convenient inter- and extrapolation of the memory kernels. This leads eventually to highly efficient simulations utilizing the generalized Langevin equation (GLE), in which the polymer network acts as an additional thermal bath with tuneable intensity.","sentences":["Tracer diffusion in polymer networks and hydrogels is relevant in biology and technology, while it also constitutes an interesting model process for the dynamics of molecules in fluctuating, heterogeneous soft matter.","Here, we study systematically the time-dependent dynamics and (non-Markovian) memory effects of tracers in polymer networks based on (Markovian) implicit-solvent Langevin simulations.","In particular, we consider spherical tracer solutes at high dilution in regular, tetrafunctional bead-spring polymer networks, and control the tracer-network Lennard-Jones (LJ) interactions and the polymer density.","Based on the analysis of the memory (friction) kernels, we recover the expected long-time transport coefficients, and demonstrate how the short-time tracer dynamics, polymer fluctuations, and the viscoelastic response are interlinked.","Further, we fit the characteristic memory modes of the tracers with damped harmonic oscillations and identify LJ contributions, bond vibrations, and slow network relaxations, which enter the kernel with an almost linear scaling with the LJ attractions.","This procedure proposes a reduced functional form for the tracer memory, allowing for a convenient inter- and extrapolation of the memory kernels.","This leads eventually to highly efficient simulations utilizing the generalized Langevin equation (GLE), in which the polymer network acts as an additional thermal bath with tuneable intensity."],"url":"http://arxiv.org/abs/2402.10148v1","category":"cond-mat.soft"}
{"created":"2024-02-15 17:51:03","title":"Thermodynamically Stable Knots in Semiflexible Polymers","abstract":"Semiflexible polymers are widely used as a paradigm for understanding structural phases in biomolecules including folding of proteins. Here, we compare bead-spring and bead-stick variants of coarse-grained semiflexible polymer models that cover the whole range from flexible to stiff by conducting extensive replica-exchange Monte Carlo computer simulations. In the data analysis we focus on knotted conformations whose stability is shown to depend on the ratio $r_b/r_{\\rm min}$ with $r_b$ denoting the equilibrium bond length and $r_{\\rm min}$ the distance of the strongest nonbonded interactions. For both models, our results provide evidence that at low temperatures for $r_b/r_{\\rm min}$ outside a small range around unity one always encounters knots as generic stable phases along with the usual frozen and bent-like structures. By varying the bending stiffness, we observe rather strong first-order-like structural transitions between the coexisting phases characterized by these geometrically different motifs. Through analyses of the energy distributions close to the transition point, we present exploratory estimates of the free-energy barriers between the coexisting phases.","sentences":["Semiflexible polymers are widely used as a paradigm for understanding structural phases in biomolecules including folding of proteins.","Here, we compare bead-spring and bead-stick variants of coarse-grained semiflexible polymer models that cover the whole range from flexible to stiff by conducting extensive replica-exchange Monte Carlo computer simulations.","In the data analysis we focus on knotted conformations whose stability is shown to depend on the ratio $r_b/r_{\\rm min}$ with $r_b$ denoting the equilibrium bond length and $r_{\\rm min}$ the distance of the strongest nonbonded interactions.","For both models, our results provide evidence that at low temperatures for $r_b/r_{\\rm min}$ outside a small range around unity one always encounters knots as generic stable phases along with the usual frozen and bent-like structures.","By varying the bending stiffness, we observe rather strong first-order-like structural transitions between the coexisting phases characterized by these geometrically different motifs.","Through analyses of the energy distributions close to the transition point, we present exploratory estimates of the free-energy barriers between the coexisting phases."],"url":"http://arxiv.org/abs/2402.10147v1","category":"cond-mat.soft"}
{"created":"2024-02-15 17:49:50","title":"A unified quantification of synchrony in globally coupled populations with the Wiener order parameter","abstract":"We tackle the quantification of synchrony in globally coupled populations. Furthermore, we treat the problem of incomplete observations when the population mean field is unavailable, but only a small subset of units is observed. We introduce a new order parameter and demonstrate its efficiency for quantifying synchrony via monitoring general observables, regardless of whether the oscillations can be characterized in terms of the phases. Under condition of a significant irregularity in the dynamics of the coupled units, this order parameter provides a unified description of synchrony in populations of units of various complexity. The main examples include noise-induced oscillations, coupled strongly chaotic systems, and noisy periodic oscillations. Furthermore, we explore how this parameter works for the standard Kuramoto model of coupled regular phase oscillators. The most significant advantage of our approach is its ability to infer and quantify synchrony from the observation of a small percentage of the units and even from a single unit, provided the observations are sufficiently long.","sentences":["We tackle the quantification of synchrony in globally coupled populations.","Furthermore, we treat the problem of incomplete observations when the population mean field is unavailable, but only a small subset of units is observed.","We introduce a new order parameter and demonstrate its efficiency for quantifying synchrony via monitoring general observables, regardless of whether the oscillations can be characterized in terms of the phases.","Under condition of a significant irregularity in the dynamics of the coupled units, this order parameter provides a unified description of synchrony in populations of units of various complexity.","The main examples include noise-induced oscillations, coupled strongly chaotic systems, and noisy periodic oscillations.","Furthermore, we explore how this parameter works for the standard Kuramoto model of coupled regular phase oscillators.","The most significant advantage of our approach is its ability to infer and quantify synchrony from the observation of a small percentage of the units and even from a single unit, provided the observations are sufficiently long."],"url":"http://arxiv.org/abs/2402.10144v1","category":"nlin.CD"}
{"created":"2024-02-15 17:48:58","title":"Tracking Changing Probabilities via Dynamic Learners","abstract":"Consider a predictor, a learner, whose input is a stream of discrete items. The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The predictor has constant (limited) space and we seek efficient prediction and update techniques: The stream is unbounded, the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few currently frequent items may cease to occur again. The predictor, being space-bounded, need only provide probabilities for those items with (currently) sufficiently high frequency, i.e., the salient items. This problem is motivated in the setting of prediction games, a self-supervised learning regime where concepts serve as both the predictors and the predictands, and the set of concepts grows over time, resulting in non-stationarities as new concepts are generated and used. We develop moving average techniques designed to respond to such non-stationarities in a timely manner, and explore their properties. One is a simple technique based on queuing of count snapshots, and another is a combination of queuing together with an extended version of sparse EMA. The latter combination supports predictand-specific dynamic learning rates. We find that this flexibility allows for a more accurate and timely convergence.","sentences":["Consider a predictor, a learner, whose input is a stream of discrete items.","The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation.","To output probabilities, the predictor keeps track of the proportions of the items it has seen.","The predictor has constant (limited) space and we seek efficient prediction and update techniques: The stream is unbounded, the set of items is unknown to the predictor and their totality can also grow unbounded.","Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time.","For instance, new items may start appearing and a few currently frequent items may cease to occur again.","The predictor, being space-bounded, need only provide probabilities for those items with (currently) sufficiently high frequency, i.e., the salient items.","This problem is motivated in the setting of prediction games, a self-supervised learning regime where concepts serve as both the predictors and the predictands, and the set of concepts grows over time, resulting in non-stationarities as new concepts are generated and used.","We develop moving average techniques designed to respond to such non-stationarities in a timely manner, and explore their properties.","One is a simple technique based on queuing of count snapshots, and another is a combination of queuing together with an extended version of sparse EMA.","The latter combination supports predictand-specific dynamic learning rates.","We find that this flexibility allows for a more accurate and timely convergence."],"url":"http://arxiv.org/abs/2402.10142v1","category":"cs.LG"}
{"created":"2024-02-15 17:44:12","title":"A Case for a Binary Black Hole System Revealed via Quasi-Periodic Outflows","abstract":"Binaries containing a compact object orbiting a supermassive black hole are thought to be precursors of gravitational wave events, but their identification has been extremely challenging. Here, we report quasi-periodic variability in X-ray absorption which we interpret as quasi-periodic outflows (QPOuts) from a previously low-luminosity active galactic nucleus after an outburst, likely caused by a stellar tidal disruption. We rule out several models based on observed properties and instead show using general relativistic magnetohydrodynamic simulations that QPOuts, separated by roughly 8.3 days, can be explained with an intermediate-mass black hole secondary on a mildly eccentric orbit at a mean distance of about 100 gravitational radii from the primary. Our work suggests that QPOuts could be a new way to identify intermediate/extreme-mass ratio binary candidates.","sentences":["Binaries containing a compact object orbiting a supermassive black hole are thought to be precursors of gravitational wave events, but their identification has been extremely challenging.","Here, we report quasi-periodic variability in X-ray absorption which we interpret as quasi-periodic outflows (QPOuts) from a previously low-luminosity active galactic nucleus after an outburst, likely caused by a stellar tidal disruption.","We rule out several models based on observed properties and instead show using general relativistic magnetohydrodynamic simulations that QPOuts, separated by roughly 8.3 days, can be explained with an intermediate-mass black hole secondary on a mildly eccentric orbit at a mean distance of about 100 gravitational radii from the primary.","Our work suggests that QPOuts could be a new way to identify intermediate/extreme-mass ratio binary candidates."],"url":"http://arxiv.org/abs/2402.10140v1","category":"astro-ph.HE"}
{"created":"2024-02-15 17:40:02","title":"TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles","abstract":"In light of recent advances in large language models~(LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios. However, the creation of high-quality annotated data for Task-Oriented Dialog~(TOD) is recognized to be slow and costly. To address these challenges, we introduce Task-Oriented Automatic Dialogs~(TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline. The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options. Two aspects of system response styles are considered, verbosity level and users' expression mirroring. We benchmark TOAD on two response generation tasks and the results show that modeling more verbose or responses without user expression mirroring is more challenging.","sentences":["In light of recent advances in large language models~(LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios.","However, the creation of high-quality annotated data for Task-Oriented Dialog~(TOD) is recognized to be slow and costly.","To address these challenges, we introduce Task-Oriented Automatic Dialogs~(TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline.","The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options.","Two aspects of system response styles are considered, verbosity level and users' expression mirroring.","We benchmark TOAD on two response generation tasks and the results show that modeling more verbose or responses without user expression mirroring is more challenging."],"url":"http://arxiv.org/abs/2402.10137v1","category":"cs.CL"}
{"created":"2024-02-15 17:38:32","title":"Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data","abstract":"The increasing requirements for data protection and privacy has attracted a huge research interest on distributed artificial intelligence and specifically on federated learning, an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data. In the initial proposal of federated learning the architecture was centralised and the aggregation was done with federated averaging, meaning that a central server will orchestrate the federation using the most straightforward averaging strategy. This research is focused on testing different federated strategies in a peer-to-peer environment. The authors propose various aggregation strategies for federated learning, including weighted averaging aggregation, using different factors and strategies based on participant contribution. The strategies are tested with varying data sizes to identify the most robust ones. This research tests the strategies with several biomedical datasets and the results of the experiments show that the accuracy-based weighted average outperforms the classical federated averaging method.","sentences":["The increasing requirements for data protection and privacy has attracted a huge research interest on distributed artificial intelligence and specifically on federated learning, an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data.","In the initial proposal of federated learning the architecture was centralised and the aggregation was done with federated averaging, meaning that a central server will orchestrate the federation using the most straightforward averaging strategy.","This research is focused on testing different federated strategies in a peer-to-peer environment.","The authors propose various aggregation strategies for federated learning, including weighted averaging aggregation, using different factors and strategies based on participant contribution.","The strategies are tested with varying data sizes to identify the most robust ones.","This research tests the strategies with several biomedical datasets and the results of the experiments show that the accuracy-based weighted average outperforms the classical federated averaging method."],"url":"http://arxiv.org/abs/2402.10135v1","category":"cs.LG"}
{"created":"2024-02-15 17:37:25","title":"Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem","abstract":"Procedural content generation uses algorithmic techniques to create large amounts of new content for games at much lower production costs. In newer approaches, procedural content generation utilizes machine learning. However, these methods usually require expensive collection of large amounts of data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive. The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with large language models. Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it. Therefore, this paper presents a novel approach to achieving personalization by using large language models to propose levels based on the gameplay data continuously collected from individual players. We compared the levels generated using our approach with levels generated with more traditional procedural generation techniques. Our easily reproducible method has proven viable in a production setting and outperformed levels generated by traditional methods in the probability that a player will not quit the game mid-level.","sentences":["Procedural content generation uses algorithmic techniques to create large amounts of new content for games at much lower production costs.","In newer approaches, procedural content generation utilizes machine learning.","However, these methods usually require expensive collection of large amounts of data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive.","The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with large language models.","Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it.","Therefore, this paper presents a novel approach to achieving personalization by using large language models to propose levels based on the gameplay data continuously collected from individual players.","We compared the levels generated using our approach with levels generated with more traditional procedural generation techniques.","Our easily reproducible method has proven viable in a production setting and outperformed levels generated by traditional methods in the probability that a player will not quit the game mid-level."],"url":"http://arxiv.org/abs/2402.10133v1","category":"cs.AI"}
{"created":"2024-02-15 17:37:23","title":"Quantum option pricing via the Karhunen-Lo\u00e8ve expansion","abstract":"We consider the problem of pricing discretely monitored Asian options over $T$ monitoring points where the underlying asset is modeled by a geometric Brownian motion. We provide two quantum algorithms with complexity poly-logarithmic in $T$ and polynomial in $1/\\epsilon$, where $\\epsilon$ is the additive approximation error. Our algorithms are obtained respectively by using an $O(\\log T)$-qubit semi-digital quantum encoding of the Brownian motion that allows for exponentiation of the stochastic process and by analyzing classical Monte Carlo algorithms inspired by the semi-digital encodings. The best quantum algorithm obtained using this approach has complexity $\\widetilde{O}(1/\\epsilon^{3})$ where the $\\widetilde{O}$ suppresses factors poly-logarithmic in $T$ and $1/\\epsilon$. The methods proposed in this work generalize to pricing options where the underlying asset price is modeled by a smooth function of a sub-Gaussian process and the payoff is dependent on the weighted time-average of the underlying asset price.","sentences":["We consider the problem of pricing discretely monitored Asian options over $T$ monitoring points where the underlying asset is modeled by a geometric Brownian motion.","We provide two quantum algorithms with complexity poly-logarithmic in $T$ and polynomial in $1/\\epsilon$, where $\\epsilon$ is the additive approximation error.","Our algorithms are obtained respectively by using an $O(\\log T)$-qubit semi-digital quantum encoding of the Brownian motion that allows for exponentiation of the stochastic process and by analyzing classical Monte Carlo algorithms inspired by the semi-digital encodings.","The best quantum algorithm obtained using this approach has complexity $\\widetilde{O}(1/\\epsilon^{3})$ where the $\\widetilde{O}$ suppresses factors poly-logarithmic in $T$ and $1/\\epsilon$. The methods proposed in this work generalize to pricing options where the underlying asset price is modeled by a smooth function of a sub-Gaussian process and the payoff is dependent on the weighted time-average of the underlying asset price."],"url":"http://arxiv.org/abs/2402.10132v1","category":"quant-ph"}
{"created":"2024-02-15 17:35:17","title":"A Study of monogenity of Binomial Composition","abstract":"Let $\\theta$ be a root of a monic polynomial $h(x) \\in \\Z[x]$ of degree $n \\geq 2$. We say $h(x)$ is monogenic if it is irreducible over $\\Q$ and $\\{ 1, \\theta, \\theta^2, \\ldots, \\theta^{n-1} \\}$ is a basis for the ring $\\Z_K$ of integers of $K = \\Q(\\theta)$. In this article, we study about the monogenity of number fields generated by a root of composition of two binomials. We characterise all the primes dividing the index of the subgroup $\\Z[\\theta]$ in $\\Z_K$ where $K = \\Q(\\theta)$ with $\\theta$ having minimal polynomial $F(x) = (x^m-b)^n - a \\in \\Z[x]$, $m\\geq 1$ and $n \\geq 2$. As an application, we provide a class of pairs of binomials $f(x)=x^n-a$ and $g(x)=x^m-b$ having the property that both $f(x)$ and $f(g(x))$ are monogenic.","sentences":["Let $\\theta$ be a root of a monic polynomial $h(x) \\in \\Z[x]$ of degree $n \\geq 2$.","We say $h(x)$ is monogenic if it is irreducible over $\\Q$ and $\\{ 1, \\theta, \\theta^2, \\ldots, \\theta^{n-1} \\}$ is a basis for the ring $\\Z_K$ of integers of $K = \\Q(\\theta)$.","In this article, we study about the monogenity of number fields generated by a root of composition of two binomials.","We characterise all the primes dividing the index of the subgroup $\\Z[\\theta]$ in $\\Z_K$ where $K = \\Q(\\theta)$ with $\\theta$ having minimal polynomial $F(x) = (x^m-b)^n - a \\in \\Z[x]$, $m\\geq 1$ and $n \\geq 2$.","As an application, we provide a class of pairs of binomials $f(x)=x^n-a$ and $g(x)=x^m-b$ having the property that both $f(x)$ and $f(g(x))$ are monogenic."],"url":"http://arxiv.org/abs/2402.10131v1","category":"math.NT"}
{"created":"2024-02-15 17:34:56","title":"Is Continual Learning Ready for Real-world Challenges?","abstract":"Despite continual learning's long and well-established academic history, its application in real-world scenarios remains rather limited. This paper contends that this gap is attributable to a misalignment between the actual challenges of continual learning and the evaluation protocols in use, rendering proposed solutions ineffective for addressing the complexities of real-world setups. We validate our hypothesis and assess progress to date, using a new 3D semantic segmentation benchmark, OCL-3DSS. We investigate various continual learning schemes from the literature by utilizing more realistic protocols that necessitate online and continual learning for dynamic, real-world scenarios (eg., in robotics and 3D vision applications). The outcomes are sobering: all considered methods perform poorly, significantly deviating from the upper bound of joint offline training. This raises questions about the applicability of existing methods in realistic settings. Our paper aims to initiate a paradigm shift, advocating for the adoption of continual learning methods through new experimental protocols that better emulate real-world conditions to facilitate breakthroughs in the field.","sentences":["Despite continual learning's long and well-established academic history, its application in real-world scenarios remains rather limited.","This paper contends that this gap is attributable to a misalignment between the actual challenges of continual learning and the evaluation protocols in use, rendering proposed solutions ineffective for addressing the complexities of real-world setups.","We validate our hypothesis and assess progress to date, using a new 3D semantic segmentation benchmark, OCL-3DSS.","We investigate various continual learning schemes from the literature by utilizing more realistic protocols that necessitate online and continual learning for dynamic, real-world scenarios (eg., in robotics and 3D vision applications).","The outcomes are sobering: all considered methods perform poorly, significantly deviating from the upper bound of joint offline training.","This raises questions about the applicability of existing methods in realistic settings.","Our paper aims to initiate a paradigm shift, advocating for the adoption of continual learning methods through new experimental protocols that better emulate real-world conditions to facilitate breakthroughs in the field."],"url":"http://arxiv.org/abs/2402.10130v1","category":"cs.LG"}
{"created":"2024-02-15 17:32:50","title":"GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering","abstract":"Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website https://abdullahamdi.com/ges .","sentences":["Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation.","However, it may require a large number of Gaussians, which creates a substantial memory footprint.","This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities.","GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   ","It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics.","Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting.","With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%.","The code is available on the project website https://abdullahamdi.com/ges ."],"url":"http://arxiv.org/abs/2402.10128v1","category":"cs.CV"}
{"created":"2024-02-15 17:31:19","title":"Nonlinear spiked covariance matrices and signal propagation in deep neural networks","abstract":"Many recent works have studied the eigenvalue spectrum of the Conjugate Kernel (CK) defined by the nonlinear feature map of a feedforward neural network. However, existing results only establish weak convergence of the empirical eigenvalue distribution, and fall short of providing precise quantitative characterizations of the ''spike'' eigenvalues and eigenvectors that often capture the low-dimensional signal structure of the learning problem. In this work, we characterize these signal eigenvalues and eigenvectors for a nonlinear version of the spiked covariance model, including the CK as a special case. Using this general result, we give a quantitative description of how spiked eigenstructure in the input data propagates through the hidden layers of a neural network with random weights. As a second application, we study a simple regime of representation learning where the weight matrix develops a rank-one signal component over training and characterize the alignment of the target function with the spike eigenvector of the CK on test data.","sentences":["Many recent works have studied the eigenvalue spectrum of the Conjugate Kernel (CK) defined by the nonlinear feature map of a feedforward neural network.","However, existing results only establish weak convergence of the empirical eigenvalue distribution, and fall short of providing precise quantitative characterizations of the ''spike'' eigenvalues and eigenvectors that often capture the low-dimensional signal structure of the learning problem.","In this work, we characterize these signal eigenvalues and eigenvectors for a nonlinear version of the spiked covariance model, including the CK as a special case.","Using this general result, we give a quantitative description of how spiked eigenstructure in the input data propagates through the hidden layers of a neural network with random weights.","As a second application, we study a simple regime of representation learning where the weight matrix develops a rank-one signal component over training and characterize the alignment of the target function with the spike eigenvector of the CK on test data."],"url":"http://arxiv.org/abs/2402.10127v1","category":"stat.ML"}
{"created":"2024-02-15 17:28:52","title":"Exchangeability, prediction and predictive modeling in Bayesian statistics","abstract":"Prediction is a central problem in Statistics, and there is currently a renewed interest for the so-called predictive approach in Bayesian statistics. What is the latter about? One has to return on foundational concepts, which we do in this paper, moving from the role of exchangeability and reviewing forms of partial exchangeability for more structured data, with the aim of discussing their use and implications in Bayesian statistics. There we show the underlying concept that, in Bayesian statistics, a predictive rule is meant as a learning rule - how one conveys past information to information on future events. This concept has implications on the use of exchangeability and generally invests all statistical problems, also in inference. It applies to classic contexts and to less explored situations, such as the use of predictive algorithms that can be read as Bayesian learning rules. The paper offers a historical overview, but also includes a few new results, presents some recent developments and poses some open questions.","sentences":["Prediction is a central problem in Statistics, and there is currently a renewed interest for the so-called predictive approach in Bayesian statistics.","What is the latter about?","One has to return on foundational concepts, which we do in this paper, moving from the role of exchangeability and reviewing forms of partial exchangeability for more structured data, with the aim of discussing their use and implications in Bayesian statistics.","There we show the underlying concept that, in Bayesian statistics, a predictive rule is meant as a learning rule - how one conveys past information to information on future events.","This concept has implications on the use of exchangeability and generally invests all statistical problems, also in inference.","It applies to classic contexts and to less explored situations, such as the use of predictive algorithms that can be read as Bayesian learning rules.","The paper offers a historical overview, but also includes a few new results, presents some recent developments and poses some open questions."],"url":"http://arxiv.org/abs/2402.10126v1","category":"math.ST"}
{"created":"2024-02-15 17:25:57","title":"Energy Flux Decomposition in Magnetohydrodynamic Turbulence","abstract":"In hydrodynamic (HD) turbulence an exact decomposition of the energy flux across scales has been derived that identifies the contributions associated with vortex stretching and strain self-amplification (P. Johnson, Phys. Rev. Lett., 124, 104501 (2020), J. Fluid Mech. 922, A3 (2021)) to the energy flux across scales. Here we extend this methodology to general coupled advection-diffusion equations, in particular to homogeneous magnetohydrodynamic (MHD) turbulence, and we show that several subfluxes are related to each other by kinematic constraints akin to the Betchov relation in HD. Applied to data from direct numerical simulations, this decomposition allows for an identification of physical processes and for the quantification of their respective contributions to the energy cascade, as well as a quantitative assessment of their multi-scale nature through a further decomposition into single- and multi-scale terms. We find that vortex stretching is strongly depleted in MHD compared to HD, and the kinetic energy is transferred from large to small scales almost exclusively by the generation of regions of small-scale intense strain induced by the Lorentz force. In regions of large strain, current sheets are stretched by large-scale straining motion into regions of magnetic shear. This magnetic shear in turn drives extensional flows at smaller scales. Magnetic energy is transferred from large to small scales, albeit with considerable backscatter, predominantly by the aforementioned current-sheet thinning in region of high strain while the contribution from current-filament stretching - the analogue to vortex stretching - is negligible. Consequences of these results to subgrid-scale turbulence modelling are discussed.","sentences":["In hydrodynamic (HD) turbulence an exact decomposition of the energy flux across scales has been derived that identifies the contributions associated with vortex stretching and strain self-amplification (P. Johnson, Phys. Rev. Lett., 124, 104501 (2020), J. Fluid Mech. 922, A3 (2021)) to the energy flux across scales.","Here we extend this methodology to general coupled advection-diffusion equations, in particular to homogeneous magnetohydrodynamic (MHD) turbulence, and we show that several subfluxes are related to each other by kinematic constraints akin to the Betchov relation in HD.","Applied to data from direct numerical simulations, this decomposition allows for an identification of physical processes and for the quantification of their respective contributions to the energy cascade, as well as a quantitative assessment of their multi-scale nature through a further decomposition into single- and multi-scale terms.","We find that vortex stretching is strongly depleted in MHD compared to HD, and the kinetic energy is transferred from large to small scales almost exclusively by the generation of regions of small-scale intense strain induced by the Lorentz force.","In regions of large strain, current sheets are stretched by large-scale straining motion into regions of magnetic shear.","This magnetic shear in turn drives extensional flows at smaller scales.","Magnetic energy is transferred from large to small scales, albeit with considerable backscatter, predominantly by the aforementioned current-sheet thinning in region of high strain while the contribution from current-filament stretching - the analogue to vortex stretching - is negligible.","Consequences of these results to subgrid-scale turbulence modelling are discussed."],"url":"http://arxiv.org/abs/2402.10125v1","category":"physics.flu-dyn"}
{"created":"2024-02-15 17:21:42","title":"A Blob Method for Mean Field Control With Terminal Constraints","abstract":"In the present work, we develop a novel particle method for a general class of mean field control problems, with source and terminal constraints. Specific examples of the problems we consider include the dynamic formulation of the p-Wasserstein metric, optimal transport around an obstacle, and measure transport subject to acceleration controls. Unlike existing numerical approaches, our particle method is meshfree and does not require global knowledge of an underlying cost function or of the terminal constraint. A key feature of our approach is a novel way of enforcing the terminal constraint via a soft, nonlocal approximation, inspired by recent work on blob methods for diffusion equations. We prove convergence of our particle approximation to solutions of the continuum mean-field control problem in the sense of Gamma-convergence. A byproduct of our result is an extension of existing discrete-to-continuum convergence results for mean field control problems to more general state and measure costs, as arise when modeling transport around obstacles, and more general constraint sets, including controllable linear time invariant systems. Finally, we conclude by implementing our method numerically and using it to compute solutions the example problems discussed above. We conduct a detailed numerical investigation of the convergence properties of our method, as well as its behavior in sampling applications and for approximation of optimal transport maps.","sentences":["In the present work, we develop a novel particle method for a general class of mean field control problems, with source and terminal constraints.","Specific examples of the problems we consider include the dynamic formulation of the p-Wasserstein metric, optimal transport around an obstacle, and measure transport subject to acceleration controls.","Unlike existing numerical approaches, our particle method is meshfree and does not require global knowledge of an underlying cost function or of the terminal constraint.","A key feature of our approach is a novel way of enforcing the terminal constraint via a soft, nonlocal approximation, inspired by recent work on blob methods for diffusion equations.","We prove convergence of our particle approximation to solutions of the continuum mean-field control problem in the sense of Gamma-convergence.","A byproduct of our result is an extension of existing discrete-to-continuum convergence results for mean field control problems to more general state and measure costs, as arise when modeling transport around obstacles, and more general constraint sets, including controllable linear time invariant systems.","Finally, we conclude by implementing our method numerically and using it to compute solutions the example problems discussed above.","We conduct a detailed numerical investigation of the convergence properties of our method, as well as its behavior in sampling applications and for approximation of optimal transport maps."],"url":"http://arxiv.org/abs/2402.10124v1","category":"math.OC"}
{"created":"2024-02-15 17:20:55","title":"Mitigating subjectivity and bias in AI development indices: A robust approach to redefining country rankings","abstract":"Countries worldwide have been implementing different actions national strategies for Artificial Intelligence (AI) to shape policy priorities and guide their development concerning AI. Several AI indices have emerged to assess countries' progress in AI development, aiding decision-making on investments and policy choices. Typically, these indices combine multiple indicators using linear additive methods such as weighted sums, although they are limited in their ability to account for interactions among indicators. Another limitation concerns the use of deterministic weights, which can be perceived as subjective and vulnerable to debate and scrutiny, especially by nations that feel disadvantaged. Aiming at mitigating these problems, we conduct a methodological analysis to derive AI indices based on multiple criteria decision analysis. Initially, we assess correlations between different AI dimensions and employ the Choquet integral to model them. Thus, we apply the Stochastic Multicriteria Acceptability Analysis (SMAA) to conduct a sensitivity analysis using both weighted sum and Choquet integral in order to evaluate the stability of the indices with regard the weights. Finally, we introduce a novel ranking methodology based on SMAA, which considers several sets of weights to derive the ranking of countries. As a result, instead of using predefined weights, in the proposed approach, the ranking is achieved based on the probabilities of countries in occupying a specific position. In the computational analysis, we utilize the data employed in The Global AI Index proposed by Tortoise. Results reveal correlations in the data, and our approach effectively mitigates bias. In the sensitivity analysis, we scrutinize changes in the ranking resulting from weight adjustments. We demonstrate that our proposal rankings closely align with those derived from weight variations, proving to be more robust.","sentences":["Countries worldwide have been implementing different actions national strategies for Artificial Intelligence (AI) to shape policy priorities and guide their development concerning AI.","Several AI indices have emerged to assess countries' progress in AI development, aiding decision-making on investments and policy choices.","Typically, these indices combine multiple indicators using linear additive methods such as weighted sums, although they are limited in their ability to account for interactions among indicators.","Another limitation concerns the use of deterministic weights, which can be perceived as subjective and vulnerable to debate and scrutiny, especially by nations that feel disadvantaged.","Aiming at mitigating these problems, we conduct a methodological analysis to derive AI indices based on multiple criteria decision analysis.","Initially, we assess correlations between different AI dimensions and employ the Choquet integral to model them.","Thus, we apply the Stochastic Multicriteria Acceptability Analysis (SMAA) to conduct a sensitivity analysis using both weighted sum and Choquet integral in order to evaluate the stability of the indices with regard the weights.","Finally, we introduce a novel ranking methodology based on SMAA, which considers several sets of weights to derive the ranking of countries.","As a result, instead of using predefined weights, in the proposed approach, the ranking is achieved based on the probabilities of countries in occupying a specific position.","In the computational analysis, we utilize the data employed in The Global AI Index proposed by Tortoise.","Results reveal correlations in the data, and our approach effectively mitigates bias.","In the sensitivity analysis, we scrutinize changes in the ranking resulting from weight adjustments.","We demonstrate that our proposal rankings closely align with those derived from weight variations, proving to be more robust."],"url":"http://arxiv.org/abs/2402.10122v1","category":"cs.CE"}
{"created":"2024-02-15 17:16:44","title":"Physics-Informed Neural Network Policy Iteration: Algorithms, Convergence, and Verification","abstract":"Solving nonlinear optimal control problems is a challenging task, particularly for high-dimensional problems. We propose algorithms for model-based policy iterations to solve nonlinear optimal control problems with convergence guarantees. The main component of our approach is an iterative procedure that utilizes neural approximations to solve linear partial differential equations (PDEs), ensuring convergence. We present two variants of the algorithms. The first variant formulates the optimization problem as a linear least square problem, drawing inspiration from extreme learning machine (ELM) for solving PDEs. This variant efficiently handles low-dimensional problems with high accuracy. The second variant is based on a physics-informed neural network (PINN) for solving PDEs and has the potential to address high-dimensional problems. We demonstrate that both algorithms outperform traditional approaches, such as Galerkin methods, by a significant margin. We provide a theoretical analysis of both algorithms in terms of convergence of neural approximations towards the true optimal solutions in a general setting. Furthermore, we employ formal verification techniques to demonstrate the verifiable stability of the resulting controllers.","sentences":["Solving nonlinear optimal control problems is a challenging task, particularly for high-dimensional problems.","We propose algorithms for model-based policy iterations to solve nonlinear optimal control problems with convergence guarantees.","The main component of our approach is an iterative procedure that utilizes neural approximations to solve linear partial differential equations (PDEs), ensuring convergence.","We present two variants of the algorithms.","The first variant formulates the optimization problem as a linear least square problem, drawing inspiration from extreme learning machine (ELM) for solving PDEs.","This variant efficiently handles low-dimensional problems with high accuracy.","The second variant is based on a physics-informed neural network (PINN) for solving PDEs and has the potential to address high-dimensional problems.","We demonstrate that both algorithms outperform traditional approaches, such as Galerkin methods, by a significant margin.","We provide a theoretical analysis of both algorithms in terms of convergence of neural approximations towards the true optimal solutions in a general setting.","Furthermore, we employ formal verification techniques to demonstrate the verifiable stability of the resulting controllers."],"url":"http://arxiv.org/abs/2402.10119v1","category":"eess.SY"}
{"created":"2024-02-15 17:10:27","title":"Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN","abstract":"In this study, we tackle a modern research challenge within the field of perceptual brain decoding, which revolves around synthesizing images from EEG signals using an adversarial deep learning framework. The specific objective is to recreate images belonging to various object categories by leveraging EEG recordings obtained while subjects view those images. To achieve this, we employ a Transformer-encoder based EEG encoder to produce EEG encodings, which serve as inputs to the generator component of the GAN network. Alongside the adversarial loss, we also incorporate perceptual loss to enhance the quality of the generated images.","sentences":["In this study, we tackle a modern research challenge within the field of perceptual brain decoding, which revolves around synthesizing images from EEG signals using an adversarial deep learning framework.","The specific objective is to recreate images belonging to various object categories by leveraging EEG recordings obtained while subjects view those images.","To achieve this, we employ a Transformer-encoder based EEG encoder to produce EEG encodings, which serve as inputs to the generator component of the GAN network.","Alongside the adversarial loss, we also incorporate perceptual loss to enhance the quality of the generated images."],"url":"http://arxiv.org/abs/2402.10115v1","category":"cs.AI"}
{"created":"2024-02-15 17:09:32","title":"Electromagnetic Scattering at an Arbitrarily Accelerated Interface","abstract":"We present a general analytical solution to the problem of electromagnetic scattering at a one-dimensional arbitrarily accelerated space-time engineered-modulation (ASTEM) interface in the subluminal regime. We show that such an interface fundamentally produces chirping, whose profile can be designed according to specifications. This work represents an important step in the development of ASTEM crystals and holds significant potential for applications in microwave and optical devices reliant on chirp-based functionalities.","sentences":["We present a general analytical solution to the problem of electromagnetic scattering at a one-dimensional arbitrarily accelerated space-time engineered-modulation (ASTEM) interface in the subluminal regime.","We show that such an interface fundamentally produces chirping, whose profile can be designed according to specifications.","This work represents an important step in the development of ASTEM crystals and holds significant potential for applications in microwave and optical devices reliant on chirp-based functionalities."],"url":"http://arxiv.org/abs/2402.10114v1","category":"physics.optics"}
{"created":"2024-02-15 17:06:21","title":"Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning","abstract":"Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data. We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs. Our codes, models, and data will be released at https://github.com/tianyi-lab/Reflection_Tuning.","sentences":["Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality.","Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned.","This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data.","This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance.","Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data.","We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs.","Our codes, models, and data will be released at https://github.com/tianyi-lab/Reflection_Tuning."],"url":"http://arxiv.org/abs/2402.10110v1","category":"cs.CL"}
{"created":"2024-02-15 17:05:48","title":"Towards Reducing Diagnostic Errors with Interpretable Risk Prediction","abstract":"Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual \"true\" diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.","sentences":["Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs).","In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors.","In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential.","To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual \"true\" diagnoses.","We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made.","We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model.","We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses."],"url":"http://arxiv.org/abs/2402.10109v1","category":"cs.AI"}
{"created":"2024-02-15 17:02:48","title":"Quantized Embedding Vectors for Controllable Diffusion Language Models","abstract":"Improving the controllability, portability, and inference speed of diffusion language models (DLMs) is a key challenge in natural language generation. While recent research has shown significant success in complex text generation with language models, the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models. To mitigate these issues, numerous well-established methods were proposed for neural network quantization. To further enhance their portability of independent deployment as well as improve their stability evaluated by language perplexity, we propose a novel approach called the Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via quantization. This leads to a gradient-based controller for the generation tasks, and more stable intermediate latent variables are obtained, which naturally brings in an accelerated convergence as well as better controllability. Additionally, the adaption fine-tuning method is employed to reduce tunable weights. Experimental results on five challenging fine-grained control tasks demonstrate that QE-CDLM compares favorably to existing methods in terms of quality and feasibility, achieving better perplexity and lightweight fine-tuning.","sentences":["Improving the controllability, portability, and inference speed of diffusion language models (DLMs) is a key challenge in natural language generation.","While recent research has shown significant success in complex text generation with language models, the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models.","To mitigate these issues, numerous well-established methods were proposed for neural network quantization.","To further enhance their portability of independent deployment as well as improve their stability evaluated by language perplexity, we propose a novel approach called the Quantized Embedding Controllable Diffusion Language Model (QE-CDLM).","QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via quantization.","This leads to a gradient-based controller for the generation tasks, and more stable intermediate latent variables are obtained, which naturally brings in an accelerated convergence as well as better controllability.","Additionally, the adaption fine-tuning method is employed to reduce tunable weights.","Experimental results on five challenging fine-grained control tasks demonstrate that QE-CDLM compares favorably to existing methods in terms of quality and feasibility, achieving better perplexity and lightweight fine-tuning."],"url":"http://arxiv.org/abs/2402.10107v1","category":"cs.CL"}
{"created":"2024-02-15 16:59:41","title":"GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving","abstract":"Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\\% accuracy rate on the main subset but only a 6.00\\% accuracy on the challenging subset. This highlights the critical need for testing models against datasets on which they have not been pre-trained. Additionally, our findings indicate that GPT-series models perform more effectively on problems they have rephrased, suggesting a promising method for enhancing model capabilities.","sentences":["Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving.","Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated.","To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems.","This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems.","Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\\% accuracy rate on the main subset but only a 6.00\\% accuracy on the challenging subset.","This highlights the critical need for testing models against datasets on which they have not been pre-trained.","Additionally, our findings indicate that GPT-series models perform more effectively on problems they have rephrased, suggesting a promising method for enhancing model capabilities."],"url":"http://arxiv.org/abs/2402.10104v1","category":"cs.AI"}
{"created":"2024-02-15 16:56:25","title":"A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research","abstract":"Distributed Artificial Intelligence is attracting interest day by day. In this paper, the authors introduce an innovative methodology for distributed learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a privacy-preserving way. The authors design a training scheme for collaborative FCM learning that offers data privacy compliant with the current regulation. This method is applied to a cancer detection problem, proving that the performance of the model is improved by the Federated Learning process, and obtaining similar results to the ones that can be found in the literature.","sentences":["Distributed Artificial Intelligence is attracting interest day by day.","In this paper, the authors introduce an innovative methodology for distributed learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a privacy-preserving way.","The authors design a training scheme for collaborative FCM learning that offers data privacy compliant with the current regulation.","This method is applied to a cancer detection problem, proving that the performance of the model is improved by the Federated Learning process, and obtaining similar results to the ones that can be found in the literature."],"url":"http://arxiv.org/abs/2402.10102v1","category":"cs.AI"}
{"created":"2024-02-15 16:53:42","title":"Any-Shift Prompting for Generalization over Distributions","abstract":"Image-language models with prompt learning have shown remarkable advances in numerous downstream vision tasks. Nevertheless, conventional prompt learning methods overfit their training distribution and lose the generalization ability on test distributions. To improve generalization across various distribution shifts, we propose any-shift prompting: a general probabilistic inference framework that considers the relationship between training and test distributions during prompt learning. We explicitly connect training and test distributions in the latent space by constructing training and test prompts in a hierarchical architecture. Within this framework, the test prompt exploits the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution. To effectively encode the distribution information and their relationships, we further introduce a transformer inference network with a pseudo-shift training mechanism. The network generates the tailored test prompt with both training and test information in a feedforward pass, avoiding extra training costs at test time. Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift prompting on the generalization over various distribution shifts.","sentences":["Image-language models with prompt learning have shown remarkable advances in numerous downstream vision tasks.","Nevertheless, conventional prompt learning methods overfit their training distribution and lose the generalization ability on test distributions.","To improve generalization across various distribution shifts, we propose any-shift prompting: a general probabilistic inference framework that considers the relationship between training and test distributions during prompt learning.","We explicitly connect training and test distributions in the latent space by constructing training and test prompts in a hierarchical architecture.","Within this framework, the test prompt exploits the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution.","To effectively encode the distribution information and their relationships, we further introduce a transformer inference network with a pseudo-shift training mechanism.","The network generates the tailored test prompt with both training and test information in a feedforward pass, avoiding extra training costs at test time.","Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift prompting on the generalization over various distribution shifts."],"url":"http://arxiv.org/abs/2402.10099v1","category":"cs.CV"}
{"created":"2024-02-15 16:49:42","title":"Classification Diffusion Models","abstract":"A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\\textit{classify}$ between data samples and samples from some reference distribution. These techniques are successful in simple low-dimensional settings but fail to achieve good results on complex high-dimensional data, like images. A different family of methods for learning distributions is that of denoising diffusion models (DDMs), in which a model is trained to $\\textit{denoise}$ data samples. These approaches achieve state-of-the-art results in image, video, and audio generation. In this work, we present $\\textit{Classification Diffusion Models}$ (CDMs), a generative technique that adopts the denoising-based formalism of DDMs while making use of a classifier that predicts the amount of noise added to a clean signal, similarly to DRE methods. Our approach is based on the observation that an MSE-optimal denoiser for white Gaussian noise can be expressed in terms of the gradient of a cross-entropy-optimal classifier for predicting the noise level. As we illustrate, CDM achieves better denoising results compared to DDM, and leads to at least comparable FID in image generation. CDM is also capable of highly efficient one-step exact likelihood estimation, achieving state-of-the-art results among methods that use a single step. Code is available on the project's webpage in https://shaharYadin.github.io/CDM/ .","sentences":["A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\\textit{classify}$ between data samples and samples from some reference distribution.","These techniques are successful in simple low-dimensional settings but fail to achieve good results on complex high-dimensional data, like images.","A different family of methods for learning distributions is that of denoising diffusion models (DDMs), in which a model is trained to $\\textit{denoise}$ data samples.","These approaches achieve state-of-the-art results in image, video, and audio generation.","In this work, we present $\\textit{Classification Diffusion Models}$ (CDMs), a generative technique that adopts the denoising-based formalism of DDMs while making use of a classifier that predicts the amount of noise added to a clean signal, similarly to DRE methods.","Our approach is based on the observation that an MSE-optimal denoiser for white Gaussian noise can be expressed in terms of the gradient of a cross-entropy-optimal classifier for predicting the noise level.","As we illustrate, CDM achieves better denoising results compared to DDM, and leads to at least comparable FID in image generation.","CDM is also capable of highly efficient one-step exact likelihood estimation, achieving state-of-the-art results among methods that use a single step.","Code is available on the project's webpage in https://shaharYadin.github.io/CDM/ ."],"url":"http://arxiv.org/abs/2402.10095v1","category":"cs.LG"}
{"created":"2024-02-15 16:46:16","title":"MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations","abstract":"We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger models that were trained on up to 2000x more data such as DINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page: https://ml-jku.github.io/MIM-Refiner","sentences":["We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models.","The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers.","Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers.","In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   ","The refinement process is short but effective.","Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features.","Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger models that were trained on up to 2000x more data such as DINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page: https://ml-jku.github.io/MIM-Refiner"],"url":"http://arxiv.org/abs/2402.10093v1","category":"cs.CV"}
{"created":"2024-02-15 16:45:15","title":"Cumulant Tensors in Partitioned Independent Component Analysis","abstract":"In this work, we explore Partitioned Independent Component Analysis (PICA), an extension of the well-established Independent Component Analysis (ICA) framework. Traditionally, ICA focuses on extracting a vector of independent source signals from a linear combination of them defined by a mixing matrix. We aim to provide a comprehensive understanding of the identifiability of this mixing matrix in ICA. Significant to our investigation, recent developments by Mesters and Zwiernik relax these strict independence requirements, studying the identifiability of the mixing matrix from zero restrictions on cumulant tensors. In this paper, we assume alternative independence conditions, in particular, the PICA case, where only partitions of the sources are mutually independent. We study this case from an algebraic perspective, and our primary result generalizes previous results on the identifiability of the mixing matrix.","sentences":["In this work, we explore Partitioned Independent Component Analysis (PICA), an extension of the well-established Independent Component Analysis (ICA) framework.","Traditionally, ICA focuses on extracting a vector of independent source signals from a linear combination of them defined by a mixing matrix.","We aim to provide a comprehensive understanding of the identifiability of this mixing matrix in ICA.","Significant to our investigation, recent developments by Mesters and Zwiernik relax these strict independence requirements, studying the identifiability of the mixing matrix from zero restrictions on cumulant tensors.","In this paper, we assume alternative independence conditions, in particular, the PICA case, where only partitions of the sources are mutually independent.","We study this case from an algebraic perspective, and our primary result generalizes previous results on the identifiability of the mixing matrix."],"url":"http://arxiv.org/abs/2402.10089v1","category":"math.ST"}
{"created":"2024-02-15 16:43:41","title":"Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4","abstract":"Purpose: To assess the alignment of GPT-4-based evaluation to human clinician experts, for the evaluation of responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology questions and paired answers were created by ophthalmologists to represent commonly asked patient questions, divided into fine-tuning (368; 92%), and testing (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset, additional 8 glaucoma QnA pairs were included. 200 responses to the testing dataset were generated by 5 fine-tuned LLMs for evaluation. A customized clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on clinical accuracy, relevance, patient safety, and ease of understanding. GPT-4 evaluation was then compared against ranking by 5 clinicians for clinical alignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest (87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%), LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4 evaluation demonstrated significant agreement with human clinician rankings, with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80 respectively; while correlation based on Cohen Kappa was more modest at 0.50. Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical inaccuracies in the LLM-generated responses, which were appropriately identified by the GPT-4 evaluation. Conclusion: The notable clinical alignment of GPT-4 evaluation highlighted its potential to streamline the clinical evaluation of LLM chatbot responses to healthcare-related queries. By complementing the existing clinician-dependent manual grading, this efficient and automated evaluation could assist the validation of future developments in LLM applications for healthcare.","sentences":["Purpose: To assess the alignment of GPT-4-based evaluation to human clinician experts, for the evaluation of responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots.","Methods: 400 ophthalmology questions and paired answers were created by ophthalmologists to represent commonly asked patient questions, divided into fine-tuning (368; 92%), and testing (40; 8%).","We find-tuned 5 different LLMs, including LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat.","For the testing dataset, additional 8 glaucoma QnA pairs were included.","200 responses to the testing dataset were generated by 5 fine-tuned LLMs for evaluation.","A customized clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on clinical accuracy, relevance, patient safety, and ease of understanding.","GPT-4 evaluation was then compared against ranking by 5 clinicians for clinical alignment.","Results:","Among all fine-tuned LLMs, GPT-3.5 scored the highest (87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%), LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation.","GPT-4 evaluation demonstrated significant agreement with human clinician rankings, with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80 respectively; while correlation based on Cohen Kappa was more modest at 0.50.","Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical inaccuracies in the LLM-generated responses, which were appropriately identified by the GPT-4 evaluation.","Conclusion: The notable clinical alignment of GPT-4 evaluation highlighted its potential to streamline the clinical evaluation of LLM chatbot responses to healthcare-related queries.","By complementing the existing clinician-dependent manual grading, this efficient and automated evaluation could assist the validation of future developments in LLM applications for healthcare."],"url":"http://arxiv.org/abs/2402.10083v1","category":"cs.AI"}
{"created":"2024-02-15 16:42:04","title":"On the Hamilton-Waterloo Problem with a single factor of 6-cycles","abstract":"The uniform Hamilton-Waterloo Problem (HWP) asks for a resolvable $(C_M, C_N)$-decomposition of $K_v$ into $\\alpha$ $C_M$-factors and $\\beta$ $C_N$-factors. We denote a solution to the uniform Hamilton Hamilton-Waterloo problem by $\\hbox{HWP}(v; M, N; \\alpha, \\beta)$. Our research concentrates on addressing some of the remaining unresolved cases, which pose a significant challenge to generalize. We place a particular emphasis on instances where the $\\gcd(M,N)=\\{2, 3\\}$, with a specific focus on the parameter $M=6$. We introduce modifications to some known structures, and develop new approaches to resolving these outstanding challenges in the construction of uniform $2$-factorizations. This innovative method not only extends the scope of solved cases, but also contributes to a deeper understanding of the complexity involved in solving the Hamilton-Waterloo Problem.","sentences":["The uniform Hamilton-Waterloo Problem (HWP) asks for a resolvable $(C_M, C_N)$-decomposition of $K_v$ into $\\alpha$ $C_M$-factors and $\\beta$ $C_N$-factors.","We denote a solution to the uniform Hamilton Hamilton-Waterloo problem by $\\hbox{HWP}(v; M, N; \\alpha, \\beta)$. Our research concentrates on addressing some of the remaining unresolved cases, which pose a significant challenge to generalize.","We place a particular emphasis on instances where the $\\gcd(M,N)=\\{2, 3\\}$, with a specific focus on the parameter $M=6$. We introduce modifications to some known structures, and develop new approaches to resolving these outstanding challenges in the construction of uniform $2$-factorizations.","This innovative method not only extends the scope of solved cases, but also contributes to a deeper understanding of the complexity involved in solving the Hamilton-Waterloo Problem."],"url":"http://arxiv.org/abs/2402.10081v1","category":"math.CO"}
{"created":"2024-02-15 16:42:04","title":"FedRDF: A Robust and Dynamic Aggregation Function against Poisoning Attacks in Federated Learning","abstract":"Federated Learning (FL) represents a promising approach to typical privacy concerns associated with centralized Machine Learning (ML) deployments. Despite its well-known advantages, FL is vulnerable to security attacks such as Byzantine behaviors and poisoning attacks, which can significantly degrade model performance and hinder convergence. The effectiveness of existing approaches to mitigate complex attacks, such as median, trimmed mean, or Krum aggregation functions, has been only partially demonstrated in the case of specific attacks. Our study introduces a novel robust aggregation mechanism utilizing the Fourier Transform (FT), which is able to effectively handling sophisticated attacks without prior knowledge of the number of attackers. Employing this data technique, weights generated by FL clients are projected into the frequency domain to ascertain their density function, selecting the one exhibiting the highest frequency. Consequently, malicious clients' weights are excluded. Our proposed approach was tested against various model poisoning attacks, demonstrating superior performance over state-of-the-art aggregation methods.","sentences":["Federated Learning (FL) represents a promising approach to typical privacy concerns associated with centralized Machine Learning (ML) deployments.","Despite its well-known advantages, FL is vulnerable to security attacks such as Byzantine behaviors and poisoning attacks, which can significantly degrade model performance and hinder convergence.","The effectiveness of existing approaches to mitigate complex attacks, such as median, trimmed mean, or Krum aggregation functions, has been only partially demonstrated in the case of specific attacks.","Our study introduces a novel robust aggregation mechanism utilizing the Fourier Transform (FT), which is able to effectively handling sophisticated attacks without prior knowledge of the number of attackers.","Employing this data technique, weights generated by FL clients are projected into the frequency domain to ascertain their density function, selecting the one exhibiting the highest frequency.","Consequently, malicious clients' weights are excluded.","Our proposed approach was tested against various model poisoning attacks, demonstrating superior performance over state-of-the-art aggregation methods."],"url":"http://arxiv.org/abs/2402.10082v1","category":"cs.LG"}
{"created":"2024-02-15 16:38:41","title":"QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference","abstract":"We introduce QUICK, a group of novel optimized CUDA kernels for the efficient inference of quantized Large Language Models (LLMs). QUICK addresses the shared memory bank-conflict problem of state-of-the-art mixed precision matrix multiplication kernels. Our method interleaves the quantized weight matrices of LLMs offline to skip the shared memory write-back after the dequantization. We demonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger batches and up to 1.94x throughput gain on representative LLM models on various NVIDIA GPU devices.","sentences":["We introduce QUICK, a group of novel optimized CUDA kernels for the efficient inference of quantized Large Language Models (LLMs).","QUICK addresses the shared memory bank-conflict problem of state-of-the-art mixed precision matrix multiplication kernels.","Our method interleaves the quantized weight matrices of LLMs offline to skip the shared memory write-back after the dequantization.","We demonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger batches and up to 1.94x throughput gain on representative LLM models on various NVIDIA GPU devices."],"url":"http://arxiv.org/abs/2402.10076v1","category":"cs.LG"}
{"created":"2024-02-15 16:36:04","title":"Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence","abstract":"Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \\textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \\underline{\\textbf{Mo}}dular \\underline{\\textbf{E}}motional \\underline{\\textbf{I}}ntelligence enhancement method (\\textbf{MoEI}), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs without compromise their GI. Extensive experiments on two representative LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness of MoEI to improving EI while maintain GI.","sentences":["Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants.","Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks.","However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI).","To this end, we first introduce \\textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs.","Then a novel \\underline{\\textbf{Mo}}dular \\underline{\\textbf{E}}motional \\underline{\\textbf{I}}ntelligence enhancement method (\\textbf{MoEI}), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs without compromise their GI.","Extensive experiments on two representative LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness of MoEI to improving EI while maintain GI."],"url":"http://arxiv.org/abs/2402.10073v1","category":"cs.CL"}
{"created":"2024-02-15 16:30:55","title":"How Much Does Each Datapoint Leak Your Privacy? Quantifying the Per-datum Membership Leakage","abstract":"We study the per-datum Membership Inference Attacks (MIAs), where an attacker aims to infer whether a fixed target datum has been included in the input dataset of an algorithm and thus, violates privacy. First, we define the membership leakage of a datum as the advantage of the optimal adversary targeting to identify it. Then, we quantify the per-datum membership leakage for the empirical mean, and show that it depends on the Mahalanobis distance between the target datum and the data-generating distribution. We further assess the effect of two privacy defences, i.e. adding Gaussian noise and sub-sampling. We quantify exactly how both of them decrease the per-datum membership leakage. Our analysis builds on a novel proof technique that combines an Edgeworth expansion of the likelihood ratio test and a Lindeberg-Feller central limit theorem. Our analysis connects the existing likelihood ratio and scalar product attacks, and also justifies different canary selection strategies used in the privacy auditing literature. Finally, our experiments demonstrate the impacts of the leakage score, the sub-sampling ratio and the noise scale on the per-datum membership leakage as indicated by the theory.","sentences":["We study the per-datum Membership Inference Attacks (MIAs), where an attacker aims to infer whether a fixed target datum has been included in the input dataset of an algorithm and thus, violates privacy.","First, we define the membership leakage of a datum as the advantage of the optimal adversary targeting to identify it.","Then, we quantify the per-datum membership leakage for the empirical mean, and show that it depends on the Mahalanobis distance between the target datum and the data-generating distribution.","We further assess the effect of two privacy defences, i.e. adding Gaussian noise and sub-sampling.","We quantify exactly how both of them decrease the per-datum membership leakage.","Our analysis builds on a novel proof technique that combines an Edgeworth expansion of the likelihood ratio test and a Lindeberg-Feller central limit theorem.","Our analysis connects the existing likelihood ratio and scalar product attacks, and also justifies different canary selection strategies used in the privacy auditing literature.","Finally, our experiments demonstrate the impacts of the leakage score, the sub-sampling ratio and the noise scale on the per-datum membership leakage as indicated by the theory."],"url":"http://arxiv.org/abs/2402.10065v1","category":"cs.LG"}
{"created":"2024-02-15 16:30:45","title":"Balancing the Causal Effects in Class-Incremental Learning","abstract":"Class-Incremental Learning (CIL) is a practical and challenging problem for achieving general artificial intelligence. Recently, Pre-Trained Models (PTMs) have led to breakthroughs in both visual and natural language processing tasks. Despite recent studies showing PTMs' potential ability to learn sequentially, a plethora of work indicates the necessity of alleviating the catastrophic forgetting of PTMs. Through a pilot study and a causal analysis of CIL, we reveal that the crux lies in the imbalanced causal effects between new and old data. Specifically, the new data encourage models to adapt to new classes while hindering the adaptation of old classes. Similarly, the old data encourages models to adapt to old classes while hindering the adaptation of new classes. In other words, the adaptation process between new and old classes conflicts from the causal perspective. To alleviate this problem, we propose Balancing the Causal Effects (BaCE) in CIL. Concretely, BaCE proposes two objectives for building causal paths from both new and old data to the prediction of new and classes, respectively. In this way, the model is encouraged to adapt to all classes with causal effects from both new and old data and thus alleviates the causal imbalance problem. We conduct extensive experiments on continual image classification, continual text classification, and continual named entity recognition. Empirical results show that BaCE outperforms a series of CIL methods on different tasks and settings.","sentences":["Class-Incremental Learning (CIL) is a practical and challenging problem for achieving general artificial intelligence.","Recently, Pre-Trained Models (PTMs) have led to breakthroughs in both visual and natural language processing tasks.","Despite recent studies showing PTMs' potential ability to learn sequentially, a plethora of work indicates the necessity of alleviating the catastrophic forgetting of PTMs.","Through a pilot study and a causal analysis of CIL, we reveal that the crux lies in the imbalanced causal effects between new and old data.","Specifically, the new data encourage models to adapt to new classes while hindering the adaptation of old classes.","Similarly, the old data encourages models to adapt to old classes while hindering the adaptation of new classes.","In other words, the adaptation process between new and old classes conflicts from the causal perspective.","To alleviate this problem, we propose Balancing the Causal Effects (BaCE) in CIL.","Concretely, BaCE proposes two objectives for building causal paths from both new and old data to the prediction of new and classes, respectively.","In this way, the model is encouraged to adapt to all classes with causal effects from both new and old data and thus alleviates the causal imbalance problem.","We conduct extensive experiments on continual image classification, continual text classification, and continual named entity recognition.","Empirical results show that BaCE outperforms a series of CIL methods on different tasks and settings."],"url":"http://arxiv.org/abs/2402.10063v1","category":"cs.LG"}
{"created":"2024-02-15 16:29:44","title":"Quantum Backtracking in Qrisp Applied to Sudoku Problems","abstract":"The quantum backtracking algorithm proposed by Ashley Montanaro raised considerable interest, as it provides a quantum speed-up for a large class of classical optimization algorithms. It does not suffer from Barren-Plateaus and transfers well into the fault-tolerant era, as it requires only a limited number of arbitrary angle gates. Despite its potential, the algorithm has seen limited implementation efforts, presumably due to its abstract formulation. In this work, we provide a detailed instruction on implementing the quantum step operator for arbitrary backtracking instances. For a single controlled diffuser of a binary backtracking tree with depth n, our implementation requires only $6n+14$ CX gates. We detail the process of constructing accept and reject oracles for Sudoku problems using our interface to quantum backtracking. The presented code is written using Qrisp, a high-level quantum programming language, making it executable on most current physical backends and simulators. Subsequently, we perform several simulator based experiments and demonstrate solving 4x4 Sudoku instances with up to 9 empty fields. This is, to the best of our knowledge, the first instance of a compilable implementation of this generality, marking a significant and exciting step forward in quantum software engineering.","sentences":["The quantum backtracking algorithm proposed by Ashley Montanaro raised considerable interest, as it provides a quantum speed-up for a large class of classical optimization algorithms.","It does not suffer from Barren-Plateaus and transfers well into the fault-tolerant era, as it requires only a limited number of arbitrary angle gates.","Despite its potential, the algorithm has seen limited implementation efforts, presumably due to its abstract formulation.","In this work, we provide a detailed instruction on implementing the quantum step operator for arbitrary backtracking instances.","For a single controlled diffuser of a binary backtracking tree with depth n, our implementation requires only $6n+14$ CX gates.","We detail the process of constructing accept and reject oracles for Sudoku problems using our interface to quantum backtracking.","The presented code is written using Qrisp, a high-level quantum programming language, making it executable on most current physical backends and simulators.","Subsequently, we perform several simulator based experiments and demonstrate solving 4x4 Sudoku instances with up to 9 empty fields.","This is, to the best of our knowledge, the first instance of a compilable implementation of this generality, marking a significant and exciting step forward in quantum software engineering."],"url":"http://arxiv.org/abs/2402.10060v1","category":"quant-ph"}
{"created":"2024-02-15 16:29:24","title":"Partial synchrony for free? New bounds for Byzantine agreement via a generic transformation across network models","abstract":"Byzantine consensus allows n processes to decide on a common value, in spite of arbitrary failures. The seminal Dolev-Reischuk bound states that any deterministic solution to Byzantine consensus exchanges Omega(n^2) bits. In recent years, great advances have been made in deterministic Byzantine agreement for partially synchronous networks, with state-of-the-art cryptographic solutions achieving O(n^2 \\kappa) bits (where $\\kappa$ is the security parameter) and nearly matching the lower bound. In contrast, for synchronous networks, optimal solutions with O(n^2) bits, with no cryptography and the same failure tolerance, have been known for more than three decades. Can this gap in network models be closed?   In this paper, we present Repeater, the first generic transformation of Byzantine agreement algorithms from synchrony to partial synchrony. Repeater is modular, relying on existing and novel algorithms for its sub-modules. With the right choice of modules, Repeater requires no additional cryptography, is optimally resilient (n = 3t+1, where t is the maximum number of failures) and, for constant-size inputs, preserves the worst-case per-process bit complexity of the transformed synchronous algorithm. Leveraging Repeater, we present the first partially synchronous algorithm that (1) achieves optimal bit complexity (O(n^2) bits), (2) resists a computationally unbounded adversary (no cryptography), and (3) is optimally-resilient (n = 3t+1), thus showing that the Dolev-Reischuk bound is tight in partial synchrony. Moreover, we adapt Repeater for long inputs, introducing several new algorithms with improved complexity and weaker (or completely absent) cryptographic assumptions.","sentences":["Byzantine consensus allows n processes to decide on a common value, in spite of arbitrary failures.","The seminal Dolev-Reischuk bound states that any deterministic solution to Byzantine consensus exchanges Omega(n^2) bits.","In recent years, great advances have been made in deterministic Byzantine agreement for partially synchronous networks, with state-of-the-art cryptographic solutions achieving O(n^2 \\kappa) bits (where $\\kappa$ is the security parameter) and nearly matching the lower bound.","In contrast, for synchronous networks, optimal solutions with O(n^2) bits, with no cryptography and the same failure tolerance, have been known for more than three decades.","Can this gap in network models be closed?   ","In this paper, we present Repeater, the first generic transformation of Byzantine agreement algorithms from synchrony to partial synchrony.","Repeater is modular, relying on existing and novel algorithms for its sub-modules.","With the right choice of modules, Repeater requires no additional cryptography, is optimally resilient (n = 3t+1, where t is the maximum number of failures) and, for constant-size inputs, preserves the worst-case per-process bit complexity of the transformed synchronous algorithm.","Leveraging Repeater, we present the first partially synchronous algorithm that (1) achieves optimal bit complexity (O(n^2) bits), (2) resists a computationally unbounded adversary (no cryptography), and (3) is optimally-resilient (n = 3t+1), thus showing that the Dolev-Reischuk bound is tight in partial synchrony.","Moreover, we adapt Repeater for long inputs, introducing several new algorithms with improved complexity and weaker (or completely absent) cryptographic assumptions."],"url":"http://arxiv.org/abs/2402.10059v1","category":"cs.DC"}
{"created":"2024-02-15 16:28:34","title":"Towards Safer Large Language Models through Machine Unlearning","abstract":"The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.","sentences":["The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability.","However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts.","To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output.","While these methods can be effective, they frequently impact the model utility in responding to normal prompts.","To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts.","Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage.","The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge.","SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts.","Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility."],"url":"http://arxiv.org/abs/2402.10058v1","category":"cs.CL"}
{"created":"2024-02-15 16:28:29","title":"Phase coexistence in the Non-reciprocal Cahn-Hilliard model","abstract":"We establish the criterion for the phase coexistence in a mixture of nonreciprocally interacting scalar densities. For an arbitrary number of components the active pressure exists for a specific class of interactions, and when the free energy receives no contribution from cross couplings between spatial gradients of two different species. In this case, the pressure can be used to determine phase equilibrium, i.e. to construct binodals, and the active mixture can be mapped to a passive system with an effective free energy. For general interfacial tension, the pressure changes discontinuously across a flat interface which assumes the form of an active Laplace pressure in two dimensions.","sentences":["We establish the criterion for the phase coexistence in a mixture of nonreciprocally interacting scalar densities.","For an arbitrary number of components the active pressure exists for a specific class of interactions, and when the free energy receives no contribution from cross couplings between spatial gradients of two different species.","In this case, the pressure can be used to determine phase equilibrium, i.e. to construct binodals, and the active mixture can be mapped to a passive system with an effective free energy.","For general interfacial tension, the pressure changes discontinuously across a flat interface which assumes the form of an active Laplace pressure in two dimensions."],"url":"http://arxiv.org/abs/2402.10057v1","category":"cond-mat.soft"}
{"created":"2024-02-15 16:25:28","title":"Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network","abstract":"The morphology and hierarchy of the vascular systems are essential for perfusion in supporting metabolism. In human retina, one of the most energy-demanding organs, retinal circulation nourishes the entire inner retina by an intricate vasculature emerging and remerging at the optic nerve head (ONH). Thus, tracing the vascular branching from ONH through the vascular tree can illustrate vascular hierarchy and allow detailed morphological quantification, and yet remains a challenging task. Here, we presented a novel approach for a robust semi-automatic vessel tracing algorithm on human fundus images by an instance segmentation neural network (InSegNN). Distinct from semantic segmentation, InSegNN separates and labels different vascular trees individually and therefore enable tracing each tree throughout its branching. We have built-in three strategies to improve robustness and accuracy with temporal learning, spatial multi-sampling, and dynamic probability map. We achieved 83% specificity, and 50% improvement in Symmetric Best Dice (SBD) compared to literature, and outperformed baseline U-net. We have demonstrated tracing individual vessel trees from fundus images, and simultaneously retain the vessel hierarchy information. InSegNN paves a way for any subsequent morphological analysis of vascular morphology in relation to retinal diseases.","sentences":["The morphology and hierarchy of the vascular systems are essential for perfusion in supporting metabolism.","In human retina, one of the most energy-demanding organs, retinal circulation nourishes the entire inner retina by an intricate vasculature emerging and remerging at the optic nerve head (ONH).","Thus, tracing the vascular branching from ONH through the vascular tree can illustrate vascular hierarchy and allow detailed morphological quantification, and yet remains a challenging task.","Here, we presented a novel approach for a robust semi-automatic vessel tracing algorithm on human fundus images by an instance segmentation neural network (InSegNN).","Distinct from semantic segmentation, InSegNN separates and labels different vascular trees individually and therefore enable tracing each tree throughout its branching.","We have built-in three strategies to improve robustness and accuracy with temporal learning, spatial multi-sampling, and dynamic probability map.","We achieved 83% specificity, and 50% improvement in Symmetric Best Dice (SBD) compared to literature, and outperformed baseline U-net.","We have demonstrated tracing individual vessel trees from fundus images, and simultaneously retain the vessel hierarchy information.","InSegNN paves a way for any subsequent morphological analysis of vascular morphology in relation to retinal diseases."],"url":"http://arxiv.org/abs/2402.10055v1","category":"eess.IV"}
{"created":"2024-02-15 16:21:14","title":"Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination","abstract":"While displaying impressive generation capabilities across many tasks, Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data. This raises an essential question: how should we prevent such undesired behavior of LLMs while maintaining their strong generation and natural language understanding (NLU) capabilities? In this work, we introduce a novel approach termed deliberate imagination in the context of LLM unlearning. Instead of trying to forget memorized data, we employ a self-distillation framework, guiding LLMs to deliberately imagine alternative scenarios. As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks. Our results demonstrate the usefulness of this approach across different models and sizes, and also with parameter-efficient fine-tuning, offering a novel pathway to addressing the challenges with private and sensitive data in LLM applications.","sentences":["While displaying impressive generation capabilities across many tasks, Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data.","This raises an essential question: how should we prevent such undesired behavior of LLMs while maintaining their strong generation and natural language understanding (NLU) capabilities?","In this work, we introduce a novel approach termed deliberate imagination in the context of LLM unlearning.","Instead of trying to forget memorized data, we employ a self-distillation framework, guiding LLMs to deliberately imagine alternative scenarios.","As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks.","Our results demonstrate the usefulness of this approach across different models and sizes, and also with parameter-efficient fine-tuning, offering a novel pathway to addressing the challenges with private and sensitive data in LLM applications."],"url":"http://arxiv.org/abs/2402.10052v1","category":"cs.CL"}
{"created":"2024-02-15 16:15:38","title":"SwissNYF: Tool Grounded LLM Agents for Black Box Setting","abstract":"While Large Language Models (LLMs) have demonstrated enhanced capabilities in function-calling, these advancements primarily rely on accessing the functions' responses. This methodology is practical for simpler APIs but faces scalability issues with irreversible APIs that significantly impact the system, such as a database deletion API. Similarly, processes requiring extensive time for each API call and those necessitating forward planning, like automated action pipelines, present complex challenges. Furthermore, scenarios often arise where a generalized approach is needed because algorithms lack direct access to the specific implementations of these functions or secrets to use them. Traditional tool planning methods are inadequate in these cases, compelling the need to operate within black-box environments. Unlike their performance in tool manipulation, LLMs excel in black-box tasks, such as program synthesis. Therefore, we harness the program synthesis capabilities of LLMs to strategize tool usage in black-box settings, ensuring solutions are verified prior to implementation. We introduce TOPGUN, an ingeniously crafted approach leveraging program synthesis for black box tool planning. Accompanied by SwissNYF, a comprehensive suite that integrates black-box algorithms for planning and verification tasks, addressing the aforementioned challenges and enhancing the versatility and effectiveness of LLMs in complex API interactions. The public code for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.","sentences":["While Large Language Models (LLMs) have demonstrated enhanced capabilities in function-calling, these advancements primarily rely on accessing the functions' responses.","This methodology is practical for simpler APIs but faces scalability issues with irreversible APIs that significantly impact the system, such as a database deletion API.","Similarly, processes requiring extensive time for each API call and those necessitating forward planning, like automated action pipelines, present complex challenges.","Furthermore, scenarios often arise where a generalized approach is needed because algorithms lack direct access to the specific implementations of these functions or secrets to use them.","Traditional tool planning methods are inadequate in these cases, compelling the need to operate within black-box environments.","Unlike their performance in tool manipulation, LLMs excel in black-box tasks, such as program synthesis.","Therefore, we harness the program synthesis capabilities of LLMs to strategize tool usage in black-box settings, ensuring solutions are verified prior to implementation.","We introduce TOPGUN, an ingeniously crafted approach leveraging program synthesis for black box tool planning.","Accompanied by SwissNYF, a comprehensive suite that integrates black-box algorithms for planning and verification tasks, addressing the aforementioned challenges and enhancing the versatility and effectiveness of LLMs in complex API interactions.","The public code for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF."],"url":"http://arxiv.org/abs/2402.10051v1","category":"cs.AI"}
{"created":"2024-02-15 16:11:47","title":"On-Demand Myoelectric Control Using Wake Gestures to Eliminate False Activations During Activities of Daily Living","abstract":"While myoelectric control has recently become a focus of increased research as a possible flexible hands-free input modality, current control approaches are prone to inadvertent false activations in real-world conditions. In this work, a novel myoelectric control paradigm -- on-demand myoelectric control -- is proposed, designed, and evaluated, to reduce the number of unrelated muscle movements that are incorrectly interpreted as input gestures . By leveraging the concept of wake gestures, users were able to switch between a dedicated control mode and a sleep mode, effectively eliminating inadvertent activations during activities of daily living (ADLs). The feasibility of wake gestures was demonstrated in this work through two online ubiquitous EMG control tasks with varying difficulty levels; dismissing an alarm and controlling a robot. The proposed control scheme was able to appropriately ignore almost all non-targeted muscular inputs during ADLs (>99.9%) while maintaining sufficient sensitivity for reliable mode switching during intentional wake gesture elicitation. These results highlight the potential of wake gestures as a critical step towards enabling ubiquitous myoelectric control-based on-demand input for a wide range of applications.","sentences":["While myoelectric control has recently become a focus of increased research as a possible flexible hands-free input modality, current control approaches are prone to inadvertent false activations in real-world conditions.","In this work, a novel myoelectric control paradigm -- on-demand myoelectric control -- is proposed, designed, and evaluated, to reduce the number of unrelated muscle movements that are incorrectly interpreted as input gestures .","By leveraging the concept of wake gestures, users were able to switch between a dedicated control mode and a sleep mode, effectively eliminating inadvertent activations during activities of daily living (ADLs).","The feasibility of wake gestures was demonstrated in this work through two online ubiquitous EMG control tasks with varying difficulty levels; dismissing an alarm and controlling a robot.","The proposed control scheme was able to appropriately ignore almost all non-targeted muscular inputs during ADLs (>99.9%) while maintaining sufficient sensitivity for reliable mode switching during intentional wake gesture elicitation.","These results highlight the potential of wake gestures as a critical step towards enabling ubiquitous myoelectric control-based on-demand input for a wide range of applications."],"url":"http://arxiv.org/abs/2402.10050v1","category":"cs.HC"}
{"created":"2024-02-15 16:07:56","title":"How Flawed is ECE? An Analysis via Logit Smoothing","abstract":"Informally, a model is calibrated if its predictions are correct with a probability that matches the confidence of the prediction. By far the most common method in the literature for measuring calibration is the expected calibration error (ECE). Recent work, however, has pointed out drawbacks of ECE, such as the fact that it is discontinuous in the space of predictors. In this work, we ask: how fundamental are these issues, and what are their impacts on existing results? Towards this end, we completely characterize the discontinuities of ECE with respect to general probability measures on Polish spaces. We then use the nature of these discontinuities to motivate a novel continuous, easily estimated miscalibration metric, which we term Logit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained image classification models, we show in initial experiments that binned ECE closely tracks LS-ECE, indicating that the theoretical pathologies of ECE may be avoidable in practice.","sentences":["Informally, a model is calibrated if its predictions are correct with a probability that matches the confidence of the prediction.","By far the most common method in the literature for measuring calibration is the expected calibration error (ECE).","Recent work, however, has pointed out drawbacks of ECE, such as the fact that it is discontinuous in the space of predictors.","In this work, we ask: how fundamental are these issues, and what are their impacts on existing results?","Towards this end, we completely characterize the discontinuities of ECE with respect to general probability measures on Polish spaces.","We then use the nature of these discontinuities to motivate a novel continuous, easily estimated miscalibration metric, which we term Logit-Smoothed ECE (LS-ECE).","By comparing the ECE and LS-ECE of pre-trained image classification models, we show in initial experiments that binned ECE closely tracks LS-ECE, indicating that the theoretical pathologies of ECE may be avoidable in practice."],"url":"http://arxiv.org/abs/2402.10046v1","category":"cs.LG"}
{"created":"2024-02-15 16:00:58","title":"RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models","abstract":"Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.","sentences":["Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent.","However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment.","Recently, direct preference optimization (DPO) is proposed to address those challenges.","However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF.","In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO.","Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT).","A varied set of k responses per prompt are sampled directly from the SFT model.","RS-DPO identifies pairs of contrastive samples based on their reward distribution.","Finally, we apply DPO with the contrastive samples to align the model to human preference.","Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent.","Furthermore, it outperforms existing methods, including RS, PPO, and DPO."],"url":"http://arxiv.org/abs/2402.10038v1","category":"cs.CL"}
{"created":"2024-02-15 16:00:33","title":"Gaining insight into molecular tunnel junctions with a pocket calculator without I-V data fitting. Five-thirds protocol","abstract":"The proposed protocol is an attempt to meet the experimentalists' legitimate desire of reliably and easily extracting microscopic parameters from current-voltage measurements on molecular junctions. It applies to junctions wherein charge transport dominated by a single level (molecular orbital, MO) occurs via off-resonant tunneling. The recipe is simple. The measured current-voltage curve $I = I(V)$ should be recast as a curve of $V^{5/3}/I$ versus $V$. This curve exhibits two maxima: one at positive bias ($V = V_{p+}$), another at negative bias ($V = V_{p-}$). The values $V_{p +} > 0$ and $V_{p -} < 0$ at the two peaks of the curve for $V^{5/3}/I$ at positive and negative bias and the corresponding values $I_{p +} = I(V_{p+}) > 0$ and $I_{p -} = I(V_{p-}) < 0$ of the current is all information needed as input. The arithmetic average of $V_{p +}$ and $\\vert V_{p -}\\vert$ in volt provides the value in electronvolt of the MO energy offset $\\varepsilon_0 = E_{MO} - E_F$ relative to the electrode Fermi level ($\\vert \\varepsilon_0\\vert = e (V_{p +} + \\vert V_{p -}\\vert )/2$). The value of the (Stark) strength of the bias-driven MO shift is obtained as $\\gamma = (4/5) (V_{p +} - \\vert V_{p -} \\vert) / (V_{p +} + \\vert V_{p -} \\vert) $. Even the low-bias conductance estimate, $ G = (3/8) (I_{p +} / V_{p +} + I_{p -} / V_{p -})$, can be a preferable alternative to that deduced from fitting the $I$-$V$ slope in situations of noisy curves at low bias. To demonstrate the reliability and the generality of this ``five-thirds'' protocol, I illustrate its wide applicability for molecular tunnel junctions fabricated using metallic and nonmetallic electrodes, molecular species possessing localized $\\sigma$ and delocalized $\\pi$ electrons, and} various techniques (mechanically controlled break junctions, STM break junctions, conducting probe AFM junctions, and large area junctions).","sentences":["The proposed protocol is an attempt to meet the experimentalists' legitimate desire of reliably and easily extracting microscopic parameters from current-voltage measurements on molecular junctions.","It applies to junctions wherein charge transport dominated by a single level (molecular orbital, MO) occurs via off-resonant tunneling.","The recipe is simple.","The measured current-voltage curve $I = I(V)$ should be recast as a curve of $V^{5/3}/I$ versus $V$.","This curve exhibits two maxima: one at positive bias ($V = V_{p+}$), another at negative bias ($V = V_{p-}$).","The values $V_{p +} > 0$ and $V_{p -} < 0$ at the two peaks of the curve for $V^{5/3}/I$ at positive and negative bias and the corresponding values $I_{p +} = I(V_{p+}) > 0$ and $I_{p -} = I(V_{p-}) < 0$ of the current is all information needed as input.","The arithmetic average of $V_{p +}$ and $\\vert V_{p -}\\vert$ in volt provides the value in electronvolt of the MO energy offset $\\varepsilon_0 = E_{MO} - E_F$ relative to the electrode Fermi level ($\\vert \\varepsilon_0\\vert = e (V_{p +} + \\vert V_{p -}\\vert )/2$).","The value of the (Stark) strength of the bias-driven MO shift is obtained as $\\gamma = (4/5) (V_{p +} - \\vert V_{p -} \\vert) / (V_{p +} + \\vert V_{p -} \\vert) $.","Even the low-bias conductance estimate, $ G = (3/8) (I_{p +} / V_{p +} + I_{p -} / V_{p -})$, can be a preferable alternative to that deduced from fitting the $I$-$V$ slope in situations of noisy curves at low bias.","To demonstrate the reliability and the generality of this ``five-thirds'' protocol, I illustrate its wide applicability for molecular tunnel junctions fabricated using metallic and nonmetallic electrodes, molecular species possessing localized $\\sigma$ and delocalized $\\pi$ electrons, and} various techniques (mechanically controlled break junctions, STM break junctions, conducting probe AFM junctions, and large area junctions)."],"url":"http://arxiv.org/abs/2402.10037v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-15 15:48:55","title":"Diffusion Models Meet Contextual Bandits with Large Action Spaces","abstract":"Efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS). Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance.","sentences":["Efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies.","Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently.","In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS).","Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance."],"url":"http://arxiv.org/abs/2402.10028v1","category":"cs.LG"}
{"created":"2024-02-15 15:43:05","title":"Self-Augmented In-Context Learning for Unsupervised Word Translation","abstract":"Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised BLI performance, we also conduct comprehensive analyses on SAIL and discuss its limitations.","sentences":["Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages.","To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion.","Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board.","In addition to achieving state-of-the-art unsupervised BLI performance, we also conduct comprehensive analyses on SAIL and discuss its limitations."],"url":"http://arxiv.org/abs/2402.10024v1","category":"cs.CL"}
{"created":"2024-02-15 15:31:56","title":"Multi-Stage Algorithm for Group Testing with Prior Statistics","abstract":"In this paper, we propose an efficient multi-stage algorithm for non-adaptive Group Testing (GT) with general correlated prior statistics. The proposed solution can be applied to any correlated statistical prior represented in trellis, e.g., finite state machines and Markov processes. We introduce a variation of List Viterbi Algorithm (LVA) to enable accurate recovery using much fewer tests than objectives, which efficiently gains from the correlated prior statistics structure. Our numerical results demonstrate that the proposed Multi-Stage GT (MSGT) algorithm can obtain the optimal Maximum A Posteriori (MAP) performance with feasible complexity in practical regimes, such as with COVID-19 and sparse signal recovery applications, and reduce in the scenarios tested the number of pooled tests by at least $25\\%$ compared to existing classical low complexity GT algorithms. Moreover, we analytically characterize the complexity of the proposed MSGT algorithm that guarantees its efficiency.","sentences":["In this paper, we propose an efficient multi-stage algorithm for non-adaptive Group Testing (GT) with general correlated prior statistics.","The proposed solution can be applied to any correlated statistical prior represented in trellis, e.g., finite state machines and Markov processes.","We introduce a variation of List Viterbi Algorithm (LVA) to enable accurate recovery using much fewer tests than objectives, which efficiently gains from the correlated prior statistics structure.","Our numerical results demonstrate that the proposed Multi-Stage GT (MSGT) algorithm can obtain the optimal Maximum A Posteriori (MAP) performance with feasible complexity in practical regimes, such as with COVID-19 and sparse signal recovery applications, and reduce in the scenarios tested the number of pooled tests by at least $25\\%$ compared to existing classical low complexity GT algorithms.","Moreover, we analytically characterize the complexity of the proposed MSGT algorithm that guarantees its efficiency."],"url":"http://arxiv.org/abs/2402.10018v1","category":"cs.IT"}
{"created":"2024-02-15 15:28:10","title":"A Piecewise Approach for the Analysis of Exact Algorithms","abstract":"To analyze the worst-case running time of branching algorithms, the majority of work in exponential time algorithms focuses on designing complicated branching rules over developing better analysis methods for simple algorithms. In the mid-$2000$s, Fomin et al. [2005] introduced measure & conquer, an advanced general analysis method, sparking widespread adoption for obtaining tighter worst-case running time upper bounds for many fundamental NP-complete problems. Yet, much potential in this direction remains untapped, as most subsequent work applied it without further advancement. Motivated by this, we present piecewise analysis, a new general method that analyzes the running time of branching algorithms. Our approach is to define a similarity ratio that divides instances into groups and then analyze the running time within each group separately. The similarity ratio is a scale between two parameters of an instance I. Instead of relying on a single measure and a single analysis for the whole instance space, our method allows to take advantage of different intrinsic properties of instances with different similarity ratios. To showcase its potential, we reanalyze two $17$-year-old algorithms from Fomin et al. [2007] that solve $4$-Coloring and #$3$-Coloring respectively. The original analysis in their paper gave running times of $O(1.7272^n)$ and $O(1.6262^n)$ respectively for these algorithms, our analysis improves these running times to $O(1.7215^n)$ and $O(1.6232^n)$. Among the two improvements, our new running time $O(1.7215^n)$ is the first improvement in the best known running time for the 4-Coloring problem since 2007.","sentences":["To analyze the worst-case running time of branching algorithms, the majority of work in exponential time algorithms focuses on designing complicated branching rules over developing better analysis methods for simple algorithms.","In the mid-$2000$s, Fomin et al.","[2005] introduced measure & conquer, an advanced general analysis method, sparking widespread adoption for obtaining tighter worst-case running time upper bounds for many fundamental NP-complete problems.","Yet, much potential in this direction remains untapped, as most subsequent work applied it without further advancement.","Motivated by this, we present piecewise analysis, a new general method that analyzes the running time of branching algorithms.","Our approach is to define a similarity ratio that divides instances into groups and then analyze the running time within each group separately.","The similarity ratio is a scale between two parameters of an instance I. Instead of relying on a single measure and a single analysis for the whole instance space, our method allows to take advantage of different intrinsic properties of instances with different similarity ratios.","To showcase its potential, we reanalyze two $17$-year-old algorithms from Fomin et al.","[2007] that solve $4$-Coloring and #$3$-Coloring respectively.","The original analysis in their paper gave running times of $O(1.7272^n)$ and $O(1.6262^n)$ respectively for these algorithms, our analysis improves these running times to $O(1.7215^n)$ and $O(1.6232^n)$. Among the two improvements, our new running time $O(1.7215^n)$ is the first improvement in the best known running time for the 4-Coloring problem since 2007."],"url":"http://arxiv.org/abs/2402.10015v1","category":"cs.DS"}
{"created":"2024-02-15 15:25:35","title":"Trajectory Guidance: Enhanced Remote Driving of highly-automated Vehicles","abstract":"Despite the rapid technological progress, autonomous vehicles still face a wide range of complex driving situations that require human intervention. Teleoperation technology offers a versatile and effective way to address these challenges. The following work puts existing ideas into a modern context and introduces a novel technical implementation of the trajectory guidance teleoperation concept. The presented system was developed within a high-fidelity simulation environment and experimentally validated, demonstrating a realistic ride-hailing mission with prototype autonomous vehicles and onboard passengers. The results indicate that the proposed concept can be a viable alternative to the existing remote driving options, offering a promising way to enhance teleoperation technology and improve overall operation safety.","sentences":["Despite the rapid technological progress, autonomous vehicles still face a wide range of complex driving situations that require human intervention.","Teleoperation technology offers a versatile and effective way to address these challenges.","The following work puts existing ideas into a modern context and introduces a novel technical implementation of the trajectory guidance teleoperation concept.","The presented system was developed within a high-fidelity simulation environment and experimentally validated, demonstrating a realistic ride-hailing mission with prototype autonomous vehicles and onboard passengers.","The results indicate that the proposed concept can be a viable alternative to the existing remote driving options, offering a promising way to enhance teleoperation technology and improve overall operation safety."],"url":"http://arxiv.org/abs/2402.10014v1","category":"cs.RO"}
{"created":"2024-02-15 15:25:30","title":"Bridging the Empirical-Theoretical Gap in Neural Network Formal Language Learning Using Minimum Description Length","abstract":"Neural networks offer good approximation to many tasks but consistently fail to reach perfect generalization, even when theoretical work shows that such perfect solutions can be expressed by certain architectures. Using the task of formal language learning, we focus on one simple formal language and show that the theoretically correct solution is in fact not an optimum of commonly used objectives -- even with regularization techniques that according to common wisdom should lead to simple weights and good generalization (L1, L2) or other meta-heuristics (early-stopping, dropout). However, replacing standard targets with the Minimum Description Length objective (MDL) results in the correct solution being an optimum.","sentences":["Neural networks offer good approximation to many tasks but consistently fail to reach perfect generalization, even when theoretical work shows that such perfect solutions can be expressed by certain architectures.","Using the task of formal language learning, we focus on one simple formal language and show that the theoretically correct solution is in fact not an optimum of commonly used objectives -- even with regularization techniques that according to common wisdom should lead to simple weights and good generalization (L1, L2) or other meta-heuristics (early-stopping, dropout).","However, replacing standard targets with the Minimum Description Length objective (MDL) results in the correct solution being an optimum."],"url":"http://arxiv.org/abs/2402.10013v1","category":"cs.CL"}
{"created":"2024-02-15 15:18:53","title":"Clifford Group Equivariant Simplicial Message Passing Networks","abstract":"We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing. Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors. Using this knowledge, we represent simplex features through geometric products of their vertices. To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions. Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term shared simplicial message passing. Experimental results show that our method is able to outperform both equivariant and simplicial graph neural networks on a variety of geometric tasks.","sentences":["We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes.","Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing.","Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors.","Using this knowledge, we represent simplex features through geometric products of their vertices.","To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions.","Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term shared simplicial message passing.","Experimental results show that our method is able to outperform both equivariant and simplicial graph neural networks on a variety of geometric tasks."],"url":"http://arxiv.org/abs/2402.10011v1","category":"cs.AI"}
{"created":"2024-02-15 15:12:53","title":"Gauge-Independent Metric Reconstruction of Perturbations of Vacuum Spherically-Symmetric Spacetimes","abstract":"Perturbation theory of vacuum spherically-symmetric spacetimes (including the cosmological constant) has greatly contributed to the understanding of black holes, relativistic compact stars and even inhomogeneous cosmological models. The perturbative equations can be decoupled in terms of (gauge-invariant) master functions satisfying $1+1$ wave equations. In this work, building on previous work on the structure of the space of master functions and equations, we study the reconstruction of the metric perturbations in terms of the master functions. To that end, we consider the general situation in which the perturbations are driven by an arbitrary energy-momentum tensor. Then, we perform the metric reconstruction in a completely general perturbative gauge. In doing so, we investigate the role of Darboux transformations and Darboux covariance, responsible for the isospectrality between odd and even parity in the absence of matter sources and also of the physical equivalence between the descriptions based on all the possible master equations. We also show that the metric reconstruction can be carried out in terms of any of the possible master functions and that the expressions admit an explicitly covariant form.","sentences":["Perturbation theory of vacuum spherically-symmetric spacetimes (including the cosmological constant) has greatly contributed to the understanding of black holes, relativistic compact stars and even inhomogeneous cosmological models.","The perturbative equations can be decoupled in terms of (gauge-invariant) master functions satisfying $1+1$ wave equations.","In this work, building on previous work on the structure of the space of master functions and equations, we study the reconstruction of the metric perturbations in terms of the master functions.","To that end, we consider the general situation in which the perturbations are driven by an arbitrary energy-momentum tensor.","Then, we perform the metric reconstruction in a completely general perturbative gauge.","In doing so, we investigate the role of Darboux transformations and Darboux covariance, responsible for the isospectrality between odd and even parity in the absence of matter sources and also of the physical equivalence between the descriptions based on all the possible master equations.","We also show that the metric reconstruction can be carried out in terms of any of the possible master functions and that the expressions admit an explicitly covariant form."],"url":"http://arxiv.org/abs/2402.10004v1","category":"gr-qc"}
{"created":"2024-02-15 15:10:17","title":"MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding","abstract":"In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully designed transformation strategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point demonstrates state-of-the-art (SOTA) performance in various downstream tasks. For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN, comparable to fully supervised methods. Additionally, we demonstrate its effectiveness in tasks such as few-shot classification, 3D part segmentation and 3D semantic segmentation.","sentences":["In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments.","But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.","The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects.","In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives.","The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time.","In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies.","Through carefully designed transformation strategies, we further learn Multi-level invariance in 2D Multi-views.","MM-Point demonstrates state-of-the-art (SOTA) performance in various downstream tasks.","For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN, comparable to fully supervised methods.","Additionally, we demonstrate its effectiveness in tasks such as few-shot classification, 3D part segmentation and 3D semantic segmentation."],"url":"http://arxiv.org/abs/2402.10002v1","category":"cs.CV"}
{"created":"2024-02-15 15:05:40","title":"Davenport constant for finite abelian groups with higher rank","abstract":"For a finite abelian group $G,$ the Davenport Constant, denoted by $D(G)$, is defined to be the least positive integer $k$ such that every sequence of length at least $k$ has a non-trivial zero-sum subsequence. A long-standing conjecture is that the Davenport constant of a finite abelian group $G =C_{n_1}\\times\\cdots\\times C_{n_d}$ of rank $d \\in \\mathbb{N}$ is $1+\\displaystyle\\sum_{i=1}^d (n_i-1) $. This conjecture is false in general, but it remains to know for which groups it is true. In this paper, we consider groups of the form $G = (C_p)^{d-1} \\times C_{pq},$ where $p$ is a prime and $q\\in \\mathbb{N}$ and provide sufficient condition when the conjecture holds true.","sentences":["For a finite abelian group $G,$ the Davenport Constant, denoted by $D(G)$, is defined to be the least positive integer $k$ such that every sequence of length at least $k$ has a non-trivial zero-sum subsequence.","A long-standing conjecture is that the Davenport constant of a finite abelian group $G =C_{n_1}\\times\\cdots\\times C_{n_d}$ of rank $d \\in \\mathbb{N}$ is $1+\\displaystyle\\sum_{i=1}^d (n_i-1) $.","This conjecture is false in general, but it remains to know for which groups it is true.","In this paper, we consider groups of the form $G = (C_p)^{d-1} \\times C_{pq},$ where $p$ is a prime and $q\\in \\mathbb{N}$ and provide sufficient condition when the conjecture holds true."],"url":"http://arxiv.org/abs/2402.09999v1","category":"math.NT"}
{"created":"2024-02-15 15:02:46","title":"LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild","abstract":"Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing efficient batch inference to accommodate heterogeneous requests. Experimental results indicate that LoraRetriever consistently outperforms the baselines, highlighting its practical effectiveness and versatility.","sentences":["Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM).","The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs.","Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training.","However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated.","To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts.","LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing efficient batch inference to accommodate heterogeneous requests.","Experimental results indicate that LoraRetriever consistently outperforms the baselines, highlighting its practical effectiveness and versatility."],"url":"http://arxiv.org/abs/2402.09997v1","category":"cs.AI"}
{"created":"2024-02-15 15:02:23","title":"Effect of crystallographic twins on the elastoplastic response of polycrystals","abstract":"We investigate the influence of crystallographic twins on the elastoplastic response of $\\gamma$-TiAl intermetallics via full-field FFT-based computations. We first introduce a hierarchical stochastic model, which is used to simulate synthetic polycrystalline microstructures containing twin grains with certain morphologies, and apply it to generate representative volume elements. Second, we develop a Fourier-based method with regularization for solving the effective and local mechanical response of polycrystalline media using the M\\'eric-Cailletaud crystal plasticity constitutive law. Numerical results show that, across configurations of twinning, the corresponding average effective response is similar. Although differences were quantified, the effect of twins on the yield stress is negligible in practice (less than $1\\%$).","sentences":["We investigate the influence of crystallographic twins on the elastoplastic response of $\\gamma$-TiAl intermetallics via full-field FFT-based computations.","We first introduce a hierarchical stochastic model, which is used to simulate synthetic polycrystalline microstructures containing twin grains with certain morphologies, and apply it to generate representative volume elements.","Second, we develop a Fourier-based method with regularization for solving the effective and local mechanical response of polycrystalline media using the M\\'eric-Cailletaud crystal plasticity constitutive law.","Numerical results show that, across configurations of twinning, the corresponding average effective response is similar.","Although differences were quantified, the effect of twins on the yield stress is negligible in practice (less than $1\\%$)."],"url":"http://arxiv.org/abs/2402.09996v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-15 14:58:00","title":"Approximating Competitive Equilibrium by Nash Welfare","abstract":"We explore the relationship between two popular concepts on allocating divisible items: competitive equilibrium (CE) and allocations with maximum Nash welfare, i.e., allocations where the weighted geometric mean of the utilities is maximal. When agents have homogeneous concave utility functions, these two concepts coincide: the classical Eisenberg-Gale convex program that maximizes Nash welfare over feasible allocations yields a competitive equilibrium. However, these two concepts diverge for non-homogeneous utilities. From a computational perspective, maximizing Nash welfare amounts to solving a convex program for any concave utility functions, computing CE becomes PPAD-hard already for separable piecewise linear concave (SPLC) utilities.   We introduce the concept of Gale-substitute utility functions, an analogue of the weak gross substitutes (WGS) property for the so-called Gale demand system. For Gale-substitutes utilities, we show that any allocation maximizing Nash welfare provides an approximate-CE with surprisingly strong guarantees, where every agent gets at least half the maximum utility they can get at any CE, and is approximately envy-free. Gale-substitutes include examples of utilities where computing CE is PPAD hard: in particular, all separable concave utilities, and the previously studied non-separable class of Leontief-free utilities. We introduce a new, general class of utility functions called generalized network utilities based on the generalized flow model; this class includes SPLC and Leontief-free utilities. We show that all such utilities are Gale-substitutes.   Conversely, although some agents may get much higher utility at a Nash welfare maximizing allocation than at a CE, we show a price of anarchy type result: for general concave utilities, every CE achieves at least $(1/e)^{1/e} > 0.69$ fraction of the maximum Nash welfare, and this factor is tight.","sentences":["We explore the relationship between two popular concepts on allocating divisible items: competitive equilibrium (CE) and allocations with maximum Nash welfare, i.e., allocations where the weighted geometric mean of the utilities is maximal.","When agents have homogeneous concave utility functions, these two concepts coincide: the classical Eisenberg-Gale convex program that maximizes Nash welfare over feasible allocations yields a competitive equilibrium.","However, these two concepts diverge for non-homogeneous utilities.","From a computational perspective, maximizing Nash welfare amounts to solving a convex program for any concave utility functions, computing CE becomes PPAD-hard already for separable piecewise linear concave (SPLC) utilities.   ","We introduce the concept of Gale-substitute utility functions, an analogue of the weak gross substitutes (WGS) property for the so-called Gale demand system.","For Gale-substitutes utilities, we show that any allocation maximizing Nash welfare provides an approximate-CE with surprisingly strong guarantees, where every agent gets at least half the maximum utility they can get at any CE, and is approximately envy-free.","Gale-substitutes include examples of utilities where computing CE is PPAD hard: in particular, all separable concave utilities, and the previously studied non-separable class of Leontief-free utilities.","We introduce a new, general class of utility functions called generalized network utilities based on the generalized flow model; this class includes SPLC and Leontief-free utilities.","We show that all such utilities are Gale-substitutes.   ","Conversely, although some agents may get much higher utility at a Nash welfare maximizing allocation than at a CE, we show a price of anarchy type result: for general concave utilities, every CE achieves at least $(1/e)^{1/e} > 0.69$ fraction of the maximum Nash welfare, and this factor is tight."],"url":"http://arxiv.org/abs/2402.09994v1","category":"cs.GT"}
{"created":"2024-02-15 14:55:38","title":"Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts","abstract":"We study the robustness of deep reinforcement learning algorithms against distribution shifts within contextual multi-stage stochastic combinatorial optimization problems from the operations research domain. In this context, risk-sensitive algorithms promise to learn robust policies. While this field is of general interest to the reinforcement learning community, most studies up-to-date focus on theoretical results rather than real-world performance. With this work, we aim to bridge this gap by formally deriving a novel risk-sensitive deep reinforcement learning algorithm while providing numerical evidence for its efficacy. Specifically, we introduce discrete Soft Actor-Critic for the entropic risk measure by deriving a version of the Bellman equation for the respective Q-values. We establish a corresponding policy improvement result and infer a practical algorithm. We introduce an environment that represents typical contextual multi-stage stochastic combinatorial optimization problems and perform numerical experiments to empirically validate our algorithm's robustness against realistic distribution shifts, without compromising performance on the training distribution. We show that our algorithm is superior to risk-neutral Soft Actor-Critic as well as to two benchmark approaches for robust deep reinforcement learning. Thereby, we provide the first structured analysis on the robustness of reinforcement learning under distribution shifts in the realm of contextual multi-stage stochastic combinatorial optimization problems.","sentences":["We study the robustness of deep reinforcement learning algorithms against distribution shifts within contextual multi-stage stochastic combinatorial optimization problems from the operations research domain.","In this context, risk-sensitive algorithms promise to learn robust policies.","While this field is of general interest to the reinforcement learning community, most studies up-to-date focus on theoretical results rather than real-world performance.","With this work, we aim to bridge this gap by formally deriving a novel risk-sensitive deep reinforcement learning algorithm while providing numerical evidence for its efficacy.","Specifically, we introduce discrete Soft Actor-Critic for the entropic risk measure by deriving a version of the Bellman equation for the respective Q-values.","We establish a corresponding policy improvement result and infer a practical algorithm.","We introduce an environment that represents typical contextual multi-stage stochastic combinatorial optimization problems and perform numerical experiments to empirically validate our algorithm's robustness against realistic distribution shifts, without compromising performance on the training distribution.","We show that our algorithm is superior to risk-neutral Soft Actor-Critic as well as to two benchmark approaches for robust deep reinforcement learning.","Thereby, we provide the first structured analysis on the robustness of reinforcement learning under distribution shifts in the realm of contextual multi-stage stochastic combinatorial optimization problems."],"url":"http://arxiv.org/abs/2402.09992v1","category":"cs.LG"}
{"created":"2024-02-15 14:49:28","title":"Symmetry-Breaking Augmentations for Ad Hoc Teamwork","abstract":"In many collaborative settings, artificial intelligence (AI) agents must be able to adapt to new teammates that use unknown or previously unobserved strategies. While often simple for humans, this can be challenging for AI agents. For example, if an AI agent learns to drive alongside others (a training set) that only drive on one side of the road, it may struggle to adapt this experience to coordinate with drivers on the opposite side, even if their behaviours are simply flipped along the left-right symmetry. To address this we introduce symmetry-breaking augmentations (SBA), which increases diversity in the behaviour of training teammates by applying a symmetry-flipping operation. By learning a best-response to the augmented set of teammates, our agent is exposed to a wider range of behavioural conventions, improving performance when deployed with novel teammates. We demonstrate this experimentally in two settings, and show that our approach improves upon previous ad hoc teamwork results in the challenging card game Hanabi. We also propose a general metric for estimating symmetry-dependency amongst a given set of policies.","sentences":["In many collaborative settings, artificial intelligence (AI) agents must be able to adapt to new teammates that use unknown or previously unobserved strategies.","While often simple for humans, this can be challenging for AI agents.","For example, if an AI agent learns to drive alongside others (a training set) that only drive on one side of the road, it may struggle to adapt this experience to coordinate with drivers on the opposite side, even if their behaviours are simply flipped along the left-right symmetry.","To address this we introduce symmetry-breaking augmentations (SBA), which increases diversity in the behaviour of training teammates by applying a symmetry-flipping operation.","By learning a best-response to the augmented set of teammates, our agent is exposed to a wider range of behavioural conventions, improving performance when deployed with novel teammates.","We demonstrate this experimentally in two settings, and show that our approach improves upon previous ad hoc teamwork results in the challenging card game Hanabi.","We also propose a general metric for estimating symmetry-dependency amongst a given set of policies."],"url":"http://arxiv.org/abs/2402.09984v1","category":"cs.LG"}
{"created":"2024-02-15 14:46:03","title":"Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition","abstract":"The face expression is the first thing we pay attention to when we want to understand a person's state of mind. Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field. In this paper, because the small size of available training datasets, we propose a novel data augmentation technique that improves the performances in the recognition task. We apply geometrical transformations and build from scratch GAN models able to generate new synthetic images for each emotion type. Thus, on the augmented datasets we fine tune pretrained convolutional neural networks with different architectures. To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases. The combination of these techniques allows to reach average accuracy values of the order of 85\\% for the InceptionResNetV2 model.","sentences":["The face expression is the first thing we pay attention to when we want to understand a person's state of mind.","Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field.","In this paper, because the small size of available training datasets, we propose a novel data augmentation technique that improves the performances in the recognition task.","We apply geometrical transformations and build from scratch GAN models able to generate new synthetic images for each emotion type.","Thus, on the augmented datasets we fine tune pretrained convolutional neural networks with different architectures.","To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases.","The combination of these techniques allows to reach average accuracy values of the order of 85\\% for the InceptionResNetV2 model."],"url":"http://arxiv.org/abs/2402.09982v1","category":"cs.CV"}
{"created":"2024-02-15 14:44:30","title":"Resurgence in Lorentzian quantum cosmology: no-boundary saddles and resummation of quantum gravity corrections around tunneling saddles","abstract":"We revisit the path-integral approach to the wave function of the universe by utilizing Lefschetz thimble analyses and resurgence theory. The traditional Euclidean path-integral of gravity has the notorious ambiguity of the direction of Wick rotation. In contrast, the Lorentzian method can be formulated concretely with the Picard-Lefschetz theory. Yet, a challenge remains: the physical parameter space lies on a Stokes line, meaning that the Lefschetz-thimble structure is still unclear. Through complex deformations, we resolve this issue by uniquely identifying the thimble structure. This leads to the tunneling wave function, as opposed to the no-boundary wave function, offering a more rigorous proof of the previous results. Further exploring the parameter space, we discover rich structures: the ambiguity of the Borel resummation of perturbative series around the tunneling saddle points is exactly cancelled by the ambiguity of the contributions from no-boundary saddle points. This indicates that resurgence works also in quantum cosmology, particularly in the minisuperspace model.","sentences":["We revisit the path-integral approach to the wave function of the universe by utilizing Lefschetz thimble analyses and resurgence theory.","The traditional Euclidean path-integral of gravity has the notorious ambiguity of the direction of Wick rotation.","In contrast, the Lorentzian method can be formulated concretely with the Picard-Lefschetz theory.","Yet, a challenge remains: the physical parameter space lies on a Stokes line, meaning that the Lefschetz-thimble structure is still unclear.","Through complex deformations, we resolve this issue by uniquely identifying the thimble structure.","This leads to the tunneling wave function, as opposed to the no-boundary wave function, offering a more rigorous proof of the previous results.","Further exploring the parameter space, we discover rich structures: the ambiguity of the Borel resummation of perturbative series around the tunneling saddle points is exactly cancelled by the ambiguity of the contributions from no-boundary saddle points.","This indicates that resurgence works also in quantum cosmology, particularly in the minisuperspace model."],"url":"http://arxiv.org/abs/2402.09981v1","category":"gr-qc"}
{"created":"2024-02-15 14:37:07","title":"Fast Vocabulary Transfer for Language Model Compression","abstract":"Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.","sentences":["Real-world business applications require a trade-off between language model performance and size.","We propose a new method for model compression that relies on vocabulary transfer.","We evaluate the method on various vertical domains and downstream tasks.","Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance."],"url":"http://arxiv.org/abs/2402.09977v1","category":"cs.CL"}
{"created":"2024-02-15 14:35:02","title":"Current and future roles of artificial intelligence in retinopathy of prematurity","abstract":"Retinopathy of prematurity (ROP) is a severe condition affecting premature infants, leading to abnormal retinal blood vessel growth, retinal detachment, and potential blindness. While semi-automated systems have been used in the past to diagnose ROP-related plus disease by quantifying retinal vessel features, traditional machine learning (ML) models face challenges like accuracy and overfitting. Recent advancements in deep learning (DL), especially convolutional neural networks (CNNs), have significantly improved ROP detection and classification. The i-ROP deep learning (i-ROP-DL) system also shows promise in detecting plus disease, offering reliable ROP diagnosis potential. This research comprehensively examines the contemporary progress and challenges associated with using retinal imaging and artificial intelligence (AI) to detect ROP, offering valuable insights that can guide further investigation in this domain. Based on 89 original studies in this field (out of 1487 studies that were comprehensively reviewed), we concluded that traditional methods for ROP diagnosis suffer from subjectivity and manual analysis, leading to inconsistent clinical decisions. AI holds great promise for improving ROP management. This review explores AI's potential in ROP detection, classification, diagnosis, and prognosis.","sentences":["Retinopathy of prematurity (ROP) is a severe condition affecting premature infants, leading to abnormal retinal blood vessel growth, retinal detachment, and potential blindness.","While semi-automated systems have been used in the past to diagnose ROP-related plus disease by quantifying retinal vessel features, traditional machine learning (ML) models face challenges like accuracy and overfitting.","Recent advancements in deep learning (DL), especially convolutional neural networks (CNNs), have significantly improved ROP detection and classification.","The i-ROP deep learning (i-ROP-DL) system also shows promise in detecting plus disease, offering reliable ROP diagnosis potential.","This research comprehensively examines the contemporary progress and challenges associated with using retinal imaging and artificial intelligence (AI) to detect ROP, offering valuable insights that can guide further investigation in this domain.","Based on 89 original studies in this field (out of 1487 studies that were comprehensively reviewed), we concluded that traditional methods for ROP diagnosis suffer from subjectivity and manual analysis, leading to inconsistent clinical decisions.","AI holds great promise for improving ROP management.","This review explores AI's potential in ROP detection, classification, diagnosis, and prognosis."],"url":"http://arxiv.org/abs/2402.09975v1","category":"eess.IV"}
{"created":"2024-02-15 14:29:21","title":"TSTEM: A Cognitive Platform for Collecting Cyber Threat Intelligence in the Wild","abstract":"The extraction of cyber threat intelligence (CTI) from open sources is a rapidly expanding defensive strategy that enhances the resilience of both Information Technology (IT) and Operational Technology (OT) environments against large-scale cyber-attacks. While previous research has focused on improving individual components of the extraction process, the community lacks open-source platforms for deploying streaming CTI data pipelines in the wild. To address this gap, the study describes the implementation of an efficient and well-performing platform capable of processing compute-intensive data pipelines based on the cloud computing paradigm for real-time detection, collecting, and sharing CTI from different online sources. We developed a prototype platform (TSTEM), a containerized microservice architecture that uses Tweepy, Scrapy, Terraform, ELK, Kafka, and MLOps to autonomously search, extract, and index IOCs in the wild. Moreover, the provisioning, monitoring, and management of the TSTEM platform are achieved through infrastructure as a code (IaC). Custom focus crawlers collect web content, which is then processed by a first-level classifier to identify potential indicators of compromise (IOCs). If deemed relevant, the content advances to a second level of extraction for further examination. Throughout this process, state-of-the-art NLP models are utilized for classification and entity extraction, enhancing the overall IOC extraction methodology. Our experimental results indicate that these models exhibit high accuracy (exceeding 98%) in the classification and extraction tasks, achieving this performance within a time frame of less than a minute. The effectiveness of our system can be attributed to a finely-tuned IOC extraction method that operates at multiple stages, ensuring precise identification of relevant information with low false positives.","sentences":["The extraction of cyber threat intelligence (CTI) from open sources is a rapidly expanding defensive strategy that enhances the resilience of both Information Technology (IT) and Operational Technology (OT) environments against large-scale cyber-attacks.","While previous research has focused on improving individual components of the extraction process, the community lacks open-source platforms for deploying streaming CTI data pipelines in the wild.","To address this gap, the study describes the implementation of an efficient and well-performing platform capable of processing compute-intensive data pipelines based on the cloud computing paradigm for real-time detection, collecting, and sharing CTI from different online sources.","We developed a prototype platform (TSTEM), a containerized microservice architecture that uses Tweepy, Scrapy, Terraform, ELK, Kafka, and MLOps to autonomously search, extract, and index IOCs in the wild.","Moreover, the provisioning, monitoring, and management of the TSTEM platform are achieved through infrastructure as a code (IaC).","Custom focus crawlers collect web content, which is then processed by a first-level classifier to identify potential indicators of compromise (IOCs).","If deemed relevant, the content advances to a second level of extraction for further examination.","Throughout this process, state-of-the-art NLP models are utilized for classification and entity extraction, enhancing the overall IOC extraction methodology.","Our experimental results indicate that these models exhibit high accuracy (exceeding 98%) in the classification and extraction tasks, achieving this performance within a time frame of less than a minute.","The effectiveness of our system can be attributed to a finely-tuned IOC extraction method that operates at multiple stages, ensuring precise identification of relevant information with low false positives."],"url":"http://arxiv.org/abs/2402.09973v1","category":"cs.CR"}
{"created":"2024-02-15 14:27:58","title":"Accelerating Parallel Sampling of Diffusion Models","abstract":"Diffusion models have emerged as state-of-the-art generative models for image generation. However, sampling from diffusion models is usually time-consuming due to the inherent autoregressive nature of their sampling process. In this work, we propose a novel approach that accelerates the sampling of diffusion models by parallelizing the autoregressive process. Specifically, we reformulate the sampling process as solving a system of triangular nonlinear equations through fixed-point iteration. With this innovative formulation, we explore several systematic techniques to further reduce the iteration steps required by the solving process. Applying these techniques, we introduce ParaTAA, a universal and training-free parallel sampling algorithm that can leverage extra computational and memory resources to increase the sampling speed. Our experiments demonstrate that ParaTAA can decrease the inference steps required by common sequential sampling algorithms such as DDIM and DDPM by a factor of 4~14 times. Notably, when applying ParaTAA with 100 steps DDIM for Stable Diffusion, a widely-used text-to-image diffusion model, it can produce the same images as the sequential sampling in only 7 inference steps.","sentences":["Diffusion models have emerged as state-of-the-art generative models for image generation.","However, sampling from diffusion models is usually time-consuming due to the inherent autoregressive nature of their sampling process.","In this work, we propose a novel approach that accelerates the sampling of diffusion models by parallelizing the autoregressive process.","Specifically, we reformulate the sampling process as solving a system of triangular nonlinear equations through fixed-point iteration.","With this innovative formulation, we explore several systematic techniques to further reduce the iteration steps required by the solving process.","Applying these techniques, we introduce ParaTAA, a universal and training-free parallel sampling algorithm that can leverage extra computational and memory resources to increase the sampling speed.","Our experiments demonstrate that ParaTAA can decrease the inference steps required by common sequential sampling algorithms such as DDIM and DDPM by a factor of 4~14 times.","Notably, when applying ParaTAA with 100 steps DDIM for Stable Diffusion, a widely-used text-to-image diffusion model, it can produce the same images as the sequential sampling in only 7 inference steps."],"url":"http://arxiv.org/abs/2402.09970v1","category":"cs.LG"}
{"created":"2024-02-15 14:23:23","title":"On the kernel of actions on asymptotic cones","abstract":"Any finitely generated group $G$ acts on its asymptotic cones. The purpose of this paper is to calculate the kernel of such actions. First, we show that when $G$ is acylindrically hyperbolic, the following subgroups all coincide: (1) the kernel of the natural action on asymptotic cones, (2) the kernel of the group action on the Gromov boundary $\\partial X$ of a $\\delta$-hyperbolic space $X$ on which $G$ acts non-elementarily and acylindrically, (3) the set $\\mathrm{FC}(G)$ of elements with finite conjugacy classes, (4) the unique maximal finite normal subgroup $U(G)$ of $G$, and (5) the amenable radical $\\mathcal{A}(G)$ of $G$. Secondly, we use this equivalence to interpret the kernel of the actions on asymptotic cones as the kernel of the actions on many spaces at \"infinity\". For instance, if $G \\curvearrowright M$ is a non-elementary convergence group, then we show that the kernel of actions on the limit set $L(G)$ coincides with the kernel of the action on asymptotic cones. Similar results can also be established for the non-trivial Floyd boundary and the $\\mathrm{CAT}(0)$ groups with the visual boundary, contracting boundary, and sublinearly Morse boundary. Additionally, the results are extended to another action on asymptotic cones, called Paulin's construction. One direct corollary of these results is that the kernels of actions on asymptotic cones are invariant under the choice of the ultrafilter and non-decreasing sequence used to define the asymptotic cone. As an application, we show that the cardinality of the kernel can determine whether the group admits non-elementary action under some mild assumptions.","sentences":["Any finitely generated group $G$ acts on its asymptotic cones.","The purpose of this paper is to calculate the kernel of such actions.","First, we show that when $G$ is acylindrically hyperbolic, the following subgroups all coincide: (1) the kernel of the natural action on asymptotic cones, (2) the kernel of the group action on the Gromov boundary $\\partial X$ of a $\\delta$-hyperbolic space $X$ on which $G$ acts non-elementarily and acylindrically, (3) the set $\\mathrm{FC}(G)$ of elements with finite conjugacy classes, (4) the unique maximal finite normal subgroup $U(G)$ of $G$, and (5) the amenable radical $\\mathcal{A}(G)$ of $G$. Secondly, we use this equivalence to interpret the kernel of the actions on asymptotic cones as the kernel of the actions on many spaces at \"infinity\".","For instance, if $G \\curvearrowright M$ is a non-elementary convergence group, then we show that the kernel of actions on the limit set $L(G)$ coincides with the kernel of the action on asymptotic cones.","Similar results can also be established for the non-trivial Floyd boundary and the $\\mathrm{CAT}(0)$ groups with the visual boundary, contracting boundary, and sublinearly Morse boundary.","Additionally, the results are extended to another action on asymptotic cones, called Paulin's construction.","One direct corollary of these results is that the kernels of actions on asymptotic cones are invariant under the choice of the ultrafilter and non-decreasing sequence used to define the asymptotic cone.","As an application, we show that the cardinality of the kernel can determine whether the group admits non-elementary action under some mild assumptions."],"url":"http://arxiv.org/abs/2402.09969v1","category":"math.GR"}
{"created":"2024-02-15 14:21:30","title":"Case Study: Testing Model Capabilities in Some Reasoning Tasks","abstract":"Large Language Models (LLMs) excel in generating personalized content and facilitating interactive dialogues, showcasing their remarkable aptitude for a myriad of applications. However, their capabilities in reasoning and providing explainable outputs, especially within the context of reasoning abilities, remain areas for improvement. In this study, we delve into the reasoning abilities of LLMs, highlighting the current challenges and limitations that hinder their effectiveness in complex reasoning scenarios.","sentences":["Large Language Models (LLMs) excel in generating personalized content and facilitating interactive dialogues, showcasing their remarkable aptitude for a myriad of applications.","However, their capabilities in reasoning and providing explainable outputs, especially within the context of reasoning abilities, remain areas for improvement.","In this study, we delve into the reasoning abilities of LLMs, highlighting the current challenges and limitations that hinder their effectiveness in complex reasoning scenarios."],"url":"http://arxiv.org/abs/2402.09967v1","category":"cs.CL"}
{"created":"2024-02-15 14:19:42","title":"Textual Localization: Decomposing Multi-concept Images for Subject-Driven Text-to-Image Generation","abstract":"Subject-driven text-to-image diffusion models empower users to tailor the model to new concepts absent in the pre-training dataset using a few sample images. However, prevalent subject-driven models primarily rely on single-concept input images, facing challenges in specifying the target concept when dealing with multi-concept input images. To this end, we introduce a textual localized text-to-image model (Texual Localization) to handle multi-concept input images. During fine-tuning, our method incorporates a novel cross-attention guidance to decompose multiple concepts, establishing distinct connections between the visual representation of the target concept and the identifier token in the text prompt. Experimental results reveal that our method outperforms or performs comparably to the baseline models in terms of image fidelity and image-text alignment on multi-concept input images. In comparison to Custom Diffusion, our method with hard guidance achieves CLIP-I scores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85% higher in single-concept and multi-concept generation, respectively. Notably, our method generates cross-attention maps consistent with the target concept in the generated images, a capability absent in existing models.","sentences":["Subject-driven text-to-image diffusion models empower users to tailor the model to new concepts absent in the pre-training dataset using a few sample images.","However, prevalent subject-driven models primarily rely on single-concept input images, facing challenges in specifying the target concept when dealing with multi-concept input images.","To this end, we introduce a textual localized text-to-image model (Texual Localization) to handle multi-concept input images.","During fine-tuning, our method incorporates a novel cross-attention guidance to decompose multiple concepts, establishing distinct connections between the visual representation of the target concept and the identifier token in the text prompt.","Experimental results reveal that our method outperforms or performs comparably to the baseline models in terms of image fidelity and image-text alignment on multi-concept input images.","In comparison to Custom Diffusion, our method with hard guidance achieves CLIP-I scores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85% higher in single-concept and multi-concept generation, respectively.","Notably, our method generates cross-attention maps consistent with the target concept in the generated images, a capability absent in existing models."],"url":"http://arxiv.org/abs/2402.09966v1","category":"cs.CV"}
{"created":"2024-02-15 14:17:51","title":"Why are Sensitive Functions Hard for Transformers?","abstract":"Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY. This shows that understanding transformers' inductive biases requires studying not just their in-principle expressivity, but also their loss landscape.","sentences":["Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions.","However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities.","We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization.","We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY.","This shows that understanding transformers' inductive biases requires studying not just their in-principle expressivity, but also their loss landscape."],"url":"http://arxiv.org/abs/2402.09963v1","category":"cs.LG"}
{"created":"2024-02-15 14:08:39","title":"Optimal design of fast topological pumping","abstract":"Utilizing synthetic dimensions generated by spatial or temporal modulation, topological pumping enables the exploration of higher-dimensional topological phenomena through lower-dimensional physical systems. In this letter, we propose a rational design paradigm of fast topological pumping based on 1D and 2D time-modulated discrete elastic lattices for the first time. Firstly, the realization of topological pumping is ensured by introducing quantitative indicators to drive a transition of the edge or corner state in the lattice spectrum. Meanwhile, with the help of limiting speed for adiabaticity to calculate the modulation time, a mathematical formulation of designing topological pumping with the fastest modulation speed is presented. By applying the proposed design paradigm, topological edge-bulk-edge and corner-bulk-corner energy transport are successfully achieved, with 11.2 and 4.0 times of improvement in modulation speed compared to classical pumping systems in the literature. In addition, applying to 1D and 2D space-modulated systems, the optimized modulation schemes can reduce the number of stacks to 5.3% and 26.8% of the classical systems while ensuring highly concentrated energy transport. This design paradigm is expected to be extended to the rational design of fast topological pumping in other physical fields.","sentences":["Utilizing synthetic dimensions generated by spatial or temporal modulation, topological pumping enables the exploration of higher-dimensional topological phenomena through lower-dimensional physical systems.","In this letter, we propose a rational design paradigm of fast topological pumping based on 1D and 2D time-modulated discrete elastic lattices for the first time.","Firstly, the realization of topological pumping is ensured by introducing quantitative indicators to drive a transition of the edge or corner state in the lattice spectrum.","Meanwhile, with the help of limiting speed for adiabaticity to calculate the modulation time, a mathematical formulation of designing topological pumping with the fastest modulation speed is presented.","By applying the proposed design paradigm, topological edge-bulk-edge and corner-bulk-corner energy transport are successfully achieved, with 11.2 and 4.0 times of improvement in modulation speed compared to classical pumping systems in the literature.","In addition, applying to 1D and 2D space-modulated systems, the optimized modulation schemes can reduce the number of stacks to 5.3% and 26.8% of the classical systems while ensuring highly concentrated energy transport.","This design paradigm is expected to be extended to the rational design of fast topological pumping in other physical fields."],"url":"http://arxiv.org/abs/2402.09958v1","category":"cond-mat.other"}
{"created":"2024-02-15 14:05:14","title":"A Quantum Approach to News Verification from the Perspective of a News Aggregator","abstract":"In the dynamic landscape of digital information, the rise of misinformation and fake news presents a pressing challenge. This paper takes a completely new approach to verifying news, inspired by how quantum actors can reach agreement even when they are spatially spread out. We propose a radically new, to the best of our knowledge, algorithm that uses quantum ``entanglement'' (think of it as a special connection) to help news aggregators sniff out bad actors, whether they be other news sources or even fact-checkers trying to spread misinformation. This algorithm doesn't rely on quantum signatures, it just uses basic quantum technology we already have, in particular, special pairs of particles called ``EPR pairs'' that are much easier to create than other options. More complex entangled states are like juggling too many balls - they're hard to make and slow things down, especially when many players are involved. For instance, bigger, more complex states like ``GHZ states'' work for small groups, but they become messy with larger numbers. So, we stick with Bell states, the simplest form of entanglement, which are easy to generate no matter how many players are in the game. This means our algorithm is faster to set up, works for any number of participants, and is more practical for real-world use. Bonus points: it finishes in a fixed number of steps, regardless of how many players are involved, making it even more scalable. This new approach may lead to a powerful and efficient way to fight misinformation in the digital age, using the weird and wonderful world of quantum mechanics.","sentences":["In the dynamic landscape of digital information, the rise of misinformation and fake news presents a pressing challenge.","This paper takes a completely new approach to verifying news, inspired by how quantum actors can reach agreement even when they are spatially spread out.","We propose a radically new, to the best of our knowledge, algorithm that uses quantum ``entanglement'' (think of it as a special connection) to help news aggregators sniff out bad actors, whether they be other news sources or even fact-checkers trying to spread misinformation.","This algorithm doesn't rely on quantum signatures, it just uses basic quantum technology we already have, in particular, special pairs of particles called ``EPR pairs'' that are much easier to create than other options.","More complex entangled states are like juggling too many balls - they're hard to make and slow things down, especially when many players are involved.","For instance, bigger, more complex states like ``GHZ states'' work for small groups, but they become messy with larger numbers.","So, we stick with Bell states, the simplest form of entanglement, which are easy to generate no matter how many players are in the game.","This means our algorithm is faster to set up, works for any number of participants, and is more practical for real-world use.","Bonus points: it finishes in a fixed number of steps, regardless of how many players are involved, making it even more scalable.","This new approach may lead to a powerful and efficient way to fight misinformation in the digital age, using the weird and wonderful world of quantum mechanics."],"url":"http://arxiv.org/abs/2402.09956v1","category":"quant-ph"}
{"created":"2024-02-15 14:03:33","title":"Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation","abstract":"Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demos, increasing the number of demos still improves dialogue performance, proving that LLMs can learn from corrupted dialogue demos. Previous explanations of the ICL mechanism, such as $n$-gram induction head, cannot fully account for this phenomenon.","sentences":["Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce.","Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets.","From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demos, increasing the number of demos still improves dialogue performance, proving that LLMs can learn from corrupted dialogue demos.","Previous explanations of the ICL mechanism, such as $n$-gram induction head, cannot fully account for this phenomenon."],"url":"http://arxiv.org/abs/2402.09954v1","category":"cs.CL"}
{"created":"2024-02-15 13:58:57","title":"Strict width for Constraint Satisfaction Problems over homogeneous strucures of finite duality","abstract":"We investigate the `local consistency implies global consistency' principle of strict width among structures within the scope of the Bodirsky-Pinsker dichotomy conjecture for infinite-domain Constraint Satisfaction Problems (CSPs). Our main result implies that for certain CSP templates within the scope of that conjecture, having bounded strict width has a concrete consequence on the expressive power of the template called implicational simplicity. This in turn yields an explicit bound on the relational width of the CSP, i.e., the amount of local consistency needed to ensure the satisfiability of any instance. Our result applies to first-order expansions of any homogeneous $k$-uniform hypergraph, but more generally to any CSP template under the assumption of finite duality and general abstract conditions mainly on its automorphism group. In particular, it overcomes the restriction to binary signatures in the pioneering work of Wrona.","sentences":["We investigate the `local consistency implies global consistency' principle of strict width among structures within the scope of the Bodirsky-Pinsker dichotomy conjecture for infinite-domain Constraint Satisfaction Problems (CSPs).","Our main result implies that for certain CSP templates within the scope of that conjecture, having bounded strict width has a concrete consequence on the expressive power of the template called implicational simplicity.","This in turn yields an explicit bound on the relational width of the CSP, i.e., the amount of local consistency needed to ensure the satisfiability of any instance.","Our result applies to first-order expansions of any homogeneous $k$-uniform hypergraph, but more generally to any CSP template under the assumption of finite duality and general abstract conditions mainly on its automorphism group.","In particular, it overcomes the restriction to binary signatures in the pioneering work of Wrona."],"url":"http://arxiv.org/abs/2402.09951v1","category":"cs.LO"}
{"created":"2024-02-15 13:47:55","title":"NGTS-28Ab: A short period transiting brown dwarf","abstract":"We report the discovery of a brown dwarf orbiting a M1 host star. We first identified the brown dwarf within the Next Generation Transit Survey data, with supporting observations found in TESS sectors 11 and 38. We confirmed the discovery with follow-up photometry from the South African Astronomical Observatory, SPECULOOS-S, and TRAPPIST-S, and radial velocity measurements from HARPS, which allowed us to characterise the system. We find an orbital period of ~1.25 d, a mass of 69.0+5.3-4.8 MJ, close to the Hydrogen burning limit, and a radius of 0.95 +- 0.05 RJ. We determine the age to be >0.5 Gyr, using model isochrones, which is found to be in agreement with SED fitting within errors. NGTS-28Ab is one of the shortest period systems found within the brown dwarf desert, as well as one of the highest mass brown dwarfs that transits an M dwarf. This makes NGTS-28Ab another important discovery within this scarcely populated region.","sentences":["We report the discovery of a brown dwarf orbiting a M1 host star.","We first identified the brown dwarf within the Next Generation Transit Survey data, with supporting observations found in TESS sectors 11 and 38.","We confirmed the discovery with follow-up photometry from the South African Astronomical Observatory, SPECULOOS-S, and TRAPPIST-S, and radial velocity measurements from HARPS, which allowed us to characterise the system.","We find an orbital period of ~1.25 d, a mass of 69.0+5.3-4.8 MJ, close to the Hydrogen burning limit, and a radius of 0.95 +- 0.05 RJ.","We determine the age to be >0.5 Gyr, using model isochrones, which is found to be in agreement with SED fitting within errors.","NGTS-28Ab is one of the shortest period systems found within the brown dwarf desert, as well as one of the highest mass brown dwarfs that transits an M dwarf.","This makes NGTS-28Ab another important discovery within this scarcely populated region."],"url":"http://arxiv.org/abs/2402.09943v1","category":"astro-ph.EP"}
{"created":"2024-02-15 13:41:23","title":"FedLion: Faster Adaptive Federated Optimization with Fewer Communication","abstract":"In Federated Learning (FL), a framework to train machine learning models across distributed data, well-known algorithms like FedAvg tend to have slow convergence rates, resulting in high communication costs during training. To address this challenge, we introduce FedLion, an adaptive federated optimization algorithm that seamlessly incorporates key elements from the recently proposed centralized adaptive algorithm, Lion (Chen et al. 2o23), into the FL framework. Through comprehensive evaluations on two widely adopted FL benchmarks, we demonstrate that FedLion outperforms previous state-of-the-art adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA. Moreover, thanks to the use of signed gradients in local training, FedLion substantially reduces data transmission requirements during uplink communication when compared to existing adaptive algorithms, further reducing communication costs. Last but not least, this work also includes a novel theoretical analysis, showcasing that FedLion attains faster convergence rate than established FL algorithms like FedAvg.","sentences":["In Federated Learning (FL), a framework to train machine learning models across distributed data, well-known algorithms like FedAvg tend to have slow convergence rates, resulting in high communication costs during training.","To address this challenge, we introduce FedLion, an adaptive federated optimization algorithm that seamlessly incorporates key elements from the recently proposed centralized adaptive algorithm, Lion (Chen et al.","2o23)",", into the FL framework.","Through comprehensive evaluations on two widely adopted FL benchmarks, we demonstrate that FedLion outperforms previous state-of-the-art adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA.","Moreover, thanks to the use of signed gradients in local training, FedLion substantially reduces data transmission requirements during uplink communication when compared to existing adaptive algorithms, further reducing communication costs.","Last but not least, this work also includes a novel theoretical analysis, showcasing that FedLion attains faster convergence rate than established FL algorithms like FedAvg."],"url":"http://arxiv.org/abs/2402.09941v1","category":"cs.LG"}
{"created":"2024-02-15 13:39:55","title":"Generative AI in the Construction Industry: A State-of-the-art Analysis","abstract":"The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their own data, comprising steps such as data collection, dataset curation, training custom large language model (LLM), model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a generative model for querying contract documents. The results show that retrieval augmented generation (RAG) improves the baseline LLM by 5.2, 9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study provides academics and construction professionals with a comprehensive analysis and practical framework to guide the adoption of generative AI techniques to enhance productivity, quality, safety, and sustainability across the construction industry.","sentences":["The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance.","Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges.","However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry.","This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their own data, comprising steps such as data collection, dataset curation, training custom large language model (LLM), model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a generative model for querying contract documents.","The results show that retrieval augmented generation (RAG) improves the baseline LLM by 5.2, 9.4, and 4.8% in terms of quality, relevance, and reproducibility.","This study provides academics and construction professionals with a comprehensive analysis and practical framework to guide the adoption of generative AI techniques to enhance productivity, quality, safety, and sustainability across the construction industry."],"url":"http://arxiv.org/abs/2402.09939v1","category":"cs.AI"}
{"created":"2024-02-15 13:39:00","title":"Conformally invariant free parafermionic quantum chains with multispin interactions","abstract":"We calculated the spectral properties of two related families of non-Hermitian free-particle quantum chains with $N$-multispin interactions ($N=2,3,\\ldots$). The first family have a $Z(N)$ symmetry and are described by free parafermions. The second one has a $U(1)$ symmetry and are generalizations of $XX$ quantum chains described by free fermions. The eigenspectra of both free-particle families are formed by the combination of the same pseudo-energies. The models have a multicritical point with dynamical critical exponent $z=1$. The finite-size behavior of their eigenspectra, as well as the entanglement properties of their ground state wave function, indicate the models are conformally invariant. The models with open and periodic boundary conditions show quite distinct physics due to their non-Hermiticity. The models defined with open boundaries have a single conformal invariant phase while the $XX$ multispin models show multiple phases with distinct conformal central charges in the periodic case. The critical exponents of the models are calculated for $N=3,4,5$ and $6$.","sentences":["We calculated the spectral properties of two related families of non-Hermitian free-particle quantum chains with $N$-multispin interactions ($N=2,3,\\ldots$).","The first family have a $Z(N)$ symmetry and are described by free parafermions.","The second one has a $U(1)$ symmetry and are generalizations of $XX$ quantum chains described by free fermions.","The eigenspectra of both free-particle families are formed by the combination of the same pseudo-energies.","The models have a multicritical point with dynamical critical exponent $z=1$. The finite-size behavior of their eigenspectra, as well as the entanglement properties of their ground state wave function, indicate the models are conformally invariant.","The models with open and periodic boundary conditions show quite distinct physics due to their non-Hermiticity.","The models defined with open boundaries have a single conformal invariant phase while the $XX$ multispin models show multiple phases with distinct conformal central charges in the periodic case.","The critical exponents of the models are calculated for $N=3,4,5$ and $6$."],"url":"http://arxiv.org/abs/2402.09936v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-15 13:34:19","title":"Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse","abstract":"Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.","sentences":["Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research.","Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing.","We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy.","Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism.","Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining.","We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively."],"url":"http://arxiv.org/abs/2402.09934v1","category":"cs.CL"}
{"created":"2024-02-15 13:29:48","title":"Prediction Through Quantum Dynamics Simulations: Photo-excited Cyclobutanone","abstract":"Quantum dynamics simulations are becoming a standard tool for simulating photo-excited molecular systems involving a manifold of coupled states, known as non-adiabatic dynamics. While these simulations have had many successes in explaining experiments and giving details of non-adiabatic transitions, the question remains as to their predictive power. In this work, we present a set of quantum dynamics simulations on cyclobutanone, using both grid-based multi-configuration time-dependent Hartree (MCTDH) and direct dynamics variational multi-configuration Gaussian (DD-vMCG) methods. The former used a parameterised vibronic coupling model Hamiltonian and the latter generated the potential energy surfaces on-the-fly. The results give a picture of the non-adiabatic behaviour of this molecule and were used to calculate the signal from a gas-phase ultrafast electron diffraction (GUED) experiment. Corresponding experimental results will be obtained and presented at a later stage for comparison to test the predictive power of the methods. The results show that over the first 500 fs after photo-excitation to the S$_2$ state, cyclobutanone relaxes quickly to the S$_1$ state, but only a small population relaxes further to the S$_0$ state. No significant transfer of population to the triplet manifold is found. It is predicted that the GUED experiments over this time scale will see s signal related mostly to the C-O stretch motion and elongation of the molecular ring along the C-C-O axis.","sentences":["Quantum dynamics simulations are becoming a standard tool for simulating photo-excited molecular systems involving a manifold of coupled states, known as non-adiabatic dynamics.","While these simulations have had many successes in explaining experiments and giving details of non-adiabatic transitions, the question remains as to their predictive power.","In this work, we present a set of quantum dynamics simulations on cyclobutanone, using both grid-based multi-configuration time-dependent Hartree (MCTDH) and direct dynamics variational multi-configuration Gaussian (DD-vMCG) methods.","The former used a parameterised vibronic coupling model Hamiltonian and the latter generated the potential energy surfaces on-the-fly.","The results give a picture of the non-adiabatic behaviour of this molecule and were used to calculate the signal from a gas-phase ultrafast electron diffraction (GUED) experiment.","Corresponding experimental results will be obtained and presented at a later stage for comparison to test the predictive power of the methods.","The results show that over the first 500 fs after photo-excitation to the S$_2$ state, cyclobutanone relaxes quickly to the S$_1$ state, but only a small population relaxes further to the S$_0$ state.","No significant transfer of population to the triplet manifold is found.","It is predicted that the GUED experiments over this time scale will see s signal related mostly to the C-O stretch motion and elongation of the molecular ring along the C-C-O axis."],"url":"http://arxiv.org/abs/2402.09933v1","category":"physics.chem-ph"}
{"created":"2024-02-15 13:22:52","title":"Non-linear media in weakly curved spacetime: optical solitons and probe pulses for gravimetry","abstract":"That light propagating in a gravitational field gets frequency-shifted is one of the basic consequences of any metric theory of gravity rooted in the equivalence principle. At the same time, also a time dependent material's refractive index can frequency-shift light propagating in it. The mathematical analogy between the two effects is such that the latter has been used to study the optical analogue of a black-hole spacetime. Here, we combine these two effects by showing that light propagation in non-linear media in the presence of a moving refractive index perturbation can lead to a gravity-dependent blueshift. We find that the predicted blueshift surpasses the gravitational redshift even if the medium is considered to be perfectly stiff. In realistic scenarios, by far the strongest frequency shift arises due to the deformation of the dielectric medium and the corresponding photoelastic change of refractive index. This has the potential to facilitate optical sensing of small gravity gradients.","sentences":["That light propagating in a gravitational field gets frequency-shifted is one of the basic consequences of any metric theory of gravity rooted in the equivalence principle.","At the same time, also a time dependent material's refractive index can frequency-shift light propagating in it.","The mathematical analogy between the two effects is such that the latter has been used to study the optical analogue of a black-hole spacetime.","Here, we combine these two effects by showing that light propagation in non-linear media in the presence of a moving refractive index perturbation can lead to a gravity-dependent blueshift.","We find that the predicted blueshift surpasses the gravitational redshift even if the medium is considered to be perfectly stiff.","In realistic scenarios, by far the strongest frequency shift arises due to the deformation of the dielectric medium and the corresponding photoelastic change of refractive index.","This has the potential to facilitate optical sensing of small gravity gradients."],"url":"http://arxiv.org/abs/2402.09930v1","category":"gr-qc"}
{"created":"2024-02-15 13:17:43","title":"When Can We Use Two-Way Fixed-Effects (TWFE): A Comparison of TWFE and Novel Dynamic Difference-in-Differences Estimators","abstract":"The conventional Two-Way Fixed-Effects (TWFE) estimator has been under strain lately. Recent literature has revealed potential shortcomings of TWFE when the treatment effects are heterogeneous. Scholars have developed new advanced dynamic Difference-in-Differences (DiD) estimators to tackle these potential shortcomings. However, confusion remains in applied research as to when the conventional TWFE is biased and what issues the novel estimators can and cannot address. In this study, we first provide an intuitive explanation of the problems of TWFE and elucidate the key features of the novel alternative DiD estimators. We then systematically demonstrate the conditions under which the conventional TWFE is inconsistent. We employ Monte Carlo simulations to assess the performance of dynamic DiD estimators under violations of key assumptions, which likely happens in applied cases. While the new dynamic DiD estimators offer notable advantages in capturing heterogeneous treatment effects, we show that the conventional TWFE performs generally well if the model specifies an event-time function. All estimators are equally sensitive to violations of the parallel trends assumption, anticipation effects or violations of time-varying exogeneity. Despite their advantages, the new dynamic DiD estimators tackle a very specific problem and they do not serve as a universal remedy for violations of the most critical assumptions. We finally derive, based on our simulations, recommendations for how and when to use TWFE and the new DiD estimators in applied research.","sentences":["The conventional Two-Way Fixed-Effects (TWFE) estimator has been under strain lately.","Recent literature has revealed potential shortcomings of TWFE when the treatment effects are heterogeneous.","Scholars have developed new advanced dynamic Difference-in-Differences (DiD) estimators to tackle these potential shortcomings.","However, confusion remains in applied research as to when the conventional TWFE is biased and what issues the novel estimators can and cannot address.","In this study, we first provide an intuitive explanation of the problems of TWFE and elucidate the key features of the novel alternative DiD estimators.","We then systematically demonstrate the conditions under which the conventional TWFE is inconsistent.","We employ Monte Carlo simulations to assess the performance of dynamic DiD estimators under violations of key assumptions, which likely happens in applied cases.","While the new dynamic DiD estimators offer notable advantages in capturing heterogeneous treatment effects, we show that the conventional TWFE performs generally well if the model specifies an event-time function.","All estimators are equally sensitive to violations of the parallel trends assumption, anticipation effects or violations of time-varying exogeneity.","Despite their advantages, the new dynamic DiD estimators tackle a very specific problem and they do not serve as a universal remedy for violations of the most critical assumptions.","We finally derive, based on our simulations, recommendations for how and when to use TWFE and the new DiD estimators in applied research."],"url":"http://arxiv.org/abs/2402.09928v1","category":"econ.EM"}
{"created":"2024-02-15 13:17:32","title":"Quantum algorithm for bioinformatics to compute the similarity between proteins","abstract":"Drug discovery has become a main challenge in our society, following the Covid-19 pandemic. Even pharmaceutical companies are already using computing to accelerate drug discovery. They are increasingly interested in Quantum Computing with a view to improve the speed of research and development process for new drugs. Here, the authors propose a quantum method to generate random sequences based on the occurrence in a protein database and another quantum process to compute a similarity rate between proteins. The aim is to find proteins that are closest to the generated protein and to have an ordering of these proteins. First, the authors will present the construction of a quantum generator of proteins who define a protein, called the test protein. The aim is to have a randomly defined amino-acids sequence according to a proteins database given. The authors will then describe two different methods to compute the similarity's rate between the test protein and each protein of the database and present results obtained for the test protein and for a case study, the elafin.","sentences":["Drug discovery has become a main challenge in our society, following the Covid-19 pandemic.","Even pharmaceutical companies are already using computing to accelerate drug discovery.","They are increasingly interested in Quantum Computing with a view to improve the speed of research and development process for new drugs.","Here, the authors propose a quantum method to generate random sequences based on the occurrence in a protein database and another quantum process to compute a similarity rate between proteins.","The aim is to find proteins that are closest to the generated protein and to have an ordering of these proteins.","First, the authors will present the construction of a quantum generator of proteins who define a protein, called the test protein.","The aim is to have a randomly defined amino-acids sequence according to a proteins database given.","The authors will then describe two different methods to compute the similarity's rate between the test protein and each protein of the database and present results obtained for the test protein and for a case study, the elafin."],"url":"http://arxiv.org/abs/2402.09927v1","category":"quant-ph"}
{"created":"2024-02-15 13:07:06","title":"Understanding the phenomenological and intrinsic blazar sequence using a simple scaling model","abstract":"The blazar sequence, including negative correlations between radiative luminosity $L_{\\rm rad}$ and synchrotron peak frequency $\\nu$, and between Compton dominance $Y$ and $\\nu$, is widely adopted as a phenomenological description of spectral energy distributions (SEDs) of blazars, although its underlying cause is hotly debated. In particular, these correlations turn positive after correcting Doppler boosting effect. In this work, we revisit the phenomenological and intrinsic blazar sequence with three samples, which are historical sample (SEDs are built with historical data), quasi-simultaneous sample (SEDs are built with quasi-simultaneous data) and Doppler factor corrected sample (a sample with available Doppler factors), selected from literature. We find that phenomenological blazar sequence holds in historical sample, but does not exist in quasi-simultaneous sample, and intrinsic correlation between $L_{\\rm rad}$ and $\\nu$ becomes positive in Doppler factor corrected sample. We also analyze if the blazar sequence still exists in subclasses of blazars, i.e., flat-spectrum radio quasars and BL Lacertae objects, with different values of $Y$. To interpret these correlations, we apply a simple scaling model, in which physical parameters of the dissipation region are connected to the location of the dissipation region. We find that the model generated results are highly sensitive to the chosen ranges and distributions of physical parameters. Therefore, we suggest that even though the simple scaling model can reproduce the blazar sequence under specific conditions that have been fine-tuned, such results may not have universal applicability. Further consideration of a more realistic emission model is expected.","sentences":["The blazar sequence, including negative correlations between radiative luminosity $L_{\\rm rad}$ and synchrotron peak frequency $\\nu$, and between Compton dominance $Y$ and $\\nu$, is widely adopted as a phenomenological description of spectral energy distributions (SEDs) of blazars, although its underlying cause is hotly debated.","In particular, these correlations turn positive after correcting Doppler boosting effect.","In this work, we revisit the phenomenological and intrinsic blazar sequence with three samples, which are historical sample (SEDs are built with historical data), quasi-simultaneous sample (SEDs are built with quasi-simultaneous data) and Doppler factor corrected sample (a sample with available Doppler factors), selected from literature.","We find that phenomenological blazar sequence holds in historical sample, but does not exist in quasi-simultaneous sample, and intrinsic correlation between $L_{\\rm rad}$ and $\\nu$ becomes positive in Doppler factor corrected sample.","We also analyze if the blazar sequence still exists in subclasses of blazars, i.e., flat-spectrum radio quasars and","BL Lacertae objects, with different values of $Y$. To interpret these correlations, we apply a simple scaling model, in which physical parameters of the dissipation region are connected to the location of the dissipation region.","We find that the model generated results are highly sensitive to the chosen ranges and distributions of physical parameters.","Therefore, we suggest that even though the simple scaling model can reproduce the blazar sequence under specific conditions that have been fine-tuned, such results may not have universal applicability.","Further consideration of a more realistic emission model is expected."],"url":"http://arxiv.org/abs/2402.09924v1","category":"astro-ph.HE"}
{"created":"2024-02-15 13:03:57","title":"A Dataset of Open-Domain Question Answering with Multiple-Span Answers","abstract":"Multi-span answer extraction, also known as the task of multi-span question answering (MSQA), is critical for real-world applications, as it requires extracting multiple pieces of information from a text to answer complex questions. Despite the active studies and rapid progress in English MSQA research, there is a notable lack of publicly available MSQA benchmark in Chinese. Previous efforts for constructing MSQA datasets predominantly emphasized entity-centric contextualization, resulting in a bias towards collecting factoid questions and potentially overlooking questions requiring more detailed descriptive responses. To overcome these limitations, we present CLEAN, a comprehensive Chinese multi-span question answering dataset that involves a wide range of open-domain subjects with a substantial number of instances requiring descriptive answers. Additionally, we provide established models from relevant literature as baselines for CLEAN. Experimental results and analysis show the characteristics and challenge of the newly proposed CLEAN dataset for the community. Our dataset, CLEAN, will be publicly released at zhiyiluo.site/misc/clean_v1.0_ sample.json.","sentences":["Multi-span answer extraction, also known as the task of multi-span question answering (MSQA), is critical for real-world applications, as it requires extracting multiple pieces of information from a text to answer complex questions.","Despite the active studies and rapid progress in English MSQA research, there is a notable lack of publicly available MSQA benchmark in Chinese.","Previous efforts for constructing MSQA datasets predominantly emphasized entity-centric contextualization, resulting in a bias towards collecting factoid questions and potentially overlooking questions requiring more detailed descriptive responses.","To overcome these limitations, we present CLEAN, a comprehensive Chinese multi-span question answering dataset that involves a wide range of open-domain subjects with a substantial number of instances requiring descriptive answers.","Additionally, we provide established models from relevant literature as baselines for CLEAN.","Experimental results and analysis show the characteristics and challenge of the newly proposed CLEAN dataset for the community.","Our dataset, CLEAN, will be publicly released at zhiyiluo.site/misc/clean_v1.0_ sample.json."],"url":"http://arxiv.org/abs/2402.09923v1","category":"cs.CL"}
{"created":"2024-02-15 13:01:26","title":"Interpreting symplectic linear transformations in a two-qubit phase space","abstract":"For the continuous Wigner function and for the simplest discrete Wigner function in odd-prime-power dimensions, permuting the values of the Wigner function in accordance with a symplectic linear transformation is equivalent to performing a certain unitary transformation on the state. That is, performing this unitary transformation is simply a matter of moving Wigner-function values around in phase space. This simple correspondence does not hold for dimensions that are powers of two. Here we show, though, that a generalized version of this correspondence does hold for the case of a two-qubit phase space. The phase space we consider is the two-dimensional vector space over the four-element field. In this case, a symplectic linear permutation of the points of phase space, together with a certain reinterpretation of the Wigner function, is equivalent to a unitary transformation.","sentences":["For the continuous Wigner function and for the simplest discrete Wigner function in odd-prime-power dimensions, permuting the values of the Wigner function in accordance with a symplectic linear transformation is equivalent to performing a certain unitary transformation on the state.","That is, performing this unitary transformation is simply a matter of moving Wigner-function values around in phase space.","This simple correspondence does not hold for dimensions that are powers of two.","Here we show, though, that a generalized version of this correspondence does hold for the case of a two-qubit phase space.","The phase space we consider is the two-dimensional vector space over the four-element field.","In this case, a symplectic linear permutation of the points of phase space, together with a certain reinterpretation of the Wigner function, is equivalent to a unitary transformation."],"url":"http://arxiv.org/abs/2402.09922v1","category":"quant-ph"}
{"created":"2024-02-15 12:58:27","title":"Identifying and modelling cognitive biases in mobility choices","abstract":"This report presents results from an M1 internship dedicated to agent-based modelling and simulation of daily mobility choices. This simulation is intended to be realistic enough to serve as a basis for a serious game about the mobility transition. In order to ensure this level of realism, we conducted a survey to measure if real mobility choices are made rationally, or how biased they are. Results analysed here show that various biases could play a role in decisions. We then propose an implementation in a GAMA agent-based simulation.","sentences":["This report presents results from an M1 internship dedicated to agent-based modelling and simulation of daily mobility choices.","This simulation is intended to be realistic enough to serve as a basis for a serious game about the mobility transition.","In order to ensure this level of realism, we conducted a survey to measure if real mobility choices are made rationally, or how biased they are.","Results analysed here show that various biases could play a role in decisions.","We then propose an implementation in a GAMA agent-based simulation."],"url":"http://arxiv.org/abs/2402.09921v1","category":"cs.CY"}
{"created":"2024-02-15 12:53:25","title":"Road Graph Generator: Mapping roads at construction sites from GPS data","abstract":"We present a method for road inference from GPS trajectories to map construction sites. This task introduces a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which diverge significantly from typical vehicular traffic on established roads. Our method first identifies intersections in the road network that serve as critical decision points, and later connects them with edges, producing a graph, which subsequently can be used for planning and task-allocation. We demonstrate the effectiveness of our approach by mapping roads at a real-life construction site in Norway.","sentences":["We present a method for road inference from GPS trajectories to map construction sites.","This task introduces a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which diverge significantly from typical vehicular traffic on established roads.","Our method first identifies intersections in the road network that serve as critical decision points, and later connects them with edges, producing a graph, which subsequently can be used for planning and task-allocation.","We demonstrate the effectiveness of our approach by mapping roads at a real-life construction site in Norway."],"url":"http://arxiv.org/abs/2402.09919v1","category":"cs.AI"}
{"created":"2024-02-15 12:47:04","title":"Quantum gravity phenomenology and the blackbody radiation","abstract":"We analyze the problem of blackbody radiation in the presence of quantum gravity effects encoded in modified dispersion relations. In this context, the spectral radiance and the generalized Stefan-Boltzmann law are studied. Furthermore, the regime of low temperatures is contemplated as well, where features related to the blackbody thermal laws and the thermodynamics quantities such as energy, pressure, entropy, and specific heat are obtained. Possible implications in compact objects such as neutron stars are also discussed.","sentences":["We analyze the problem of blackbody radiation in the presence of quantum gravity effects encoded in modified dispersion relations.","In this context, the spectral radiance and the generalized Stefan-Boltzmann law are studied.","Furthermore, the regime of low temperatures is contemplated as well, where features related to the blackbody thermal laws and the thermodynamics quantities such as energy, pressure, entropy, and specific heat are obtained.","Possible implications in compact objects such as neutron stars are also discussed."],"url":"http://arxiv.org/abs/2402.09918v1","category":"gr-qc"}
{"created":"2024-02-15 12:39:57","title":"BUSTER: a \"BUSiness Transaction Entity Recognition\" dataset","abstract":"Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. One of the reasons resides in the displacement between popular benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health. To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists of 3779 manually annotated documents on financial transactions. We establish several baselines exploiting both general-purpose and domain-specific language models. The best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to BUSTER.","sentences":["Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging.","One of the reasons resides in the displacement between popular benchmarks and actual data.","Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health.","To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity Recognition dataset.","The dataset consists of 3779 manually annotated documents on financial transactions.","We establish several baselines exploiting both general-purpose and domain-specific language models.","The best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to BUSTER."],"url":"http://arxiv.org/abs/2402.09916v1","category":"cs.CL"}
{"created":"2024-02-15 12:33:41","title":"Radio spectra of pulsars fitted with the spectral distribution function of the emission from their current sheet","abstract":"In their catalogue of pulsars' radio spectra, Swainston et al. (2022, PASA, 39, e056) distinguish between five different forms of these spectra: those that can be fitted with (i) a simple power law, (ii) a broken power law, (iii) a low-frequency turn-over, (iv) a high-frequency turn-over or (v) a double turn-over spectrum. Here, we choose two examples from each of these categories and fit them with the spectral distribution function of the caustics that are generated by the superluminally moving current sheet in the magnetosphere of a non-aligned neutron star. In contrast to the prevailing view that the curved features of pulsars' radio spectra arise from the absorption of the observed radiation in high-density environments, our results imply that these features are intrinsic to the emission mechanism. We find that all observed features of pulsar spectra (including those that are normally fitted with simple or broken power laws) can be described by a single spectral distribution function and regarded as manifestations of a single emission mechanism. From the results of an earlier analysis of the emission from a pulsar's current sheet and the values of the fit parameters for each spectrum, we also determine the physical characteristics of the central neutron star of each considered example and its magnetosphere.","sentences":["In their catalogue of pulsars' radio spectra, Swainston et al. (2022, PASA, 39, e056) distinguish between five different forms of these spectra: those that can be fitted with (i) a simple power law, (ii) a broken power law, (iii) a low-frequency turn-over, (iv) a high-frequency turn-over or (v) a double turn-over spectrum.","Here, we choose two examples from each of these categories and fit them with the spectral distribution function of the caustics that are generated by the superluminally moving current sheet in the magnetosphere of a non-aligned neutron star.","In contrast to the prevailing view that the curved features of pulsars' radio spectra arise from the absorption of the observed radiation in high-density environments, our results imply that these features are intrinsic to the emission mechanism.","We find that all observed features of pulsar spectra (including those that are normally fitted with simple or broken power laws) can be described by a single spectral distribution function and regarded as manifestations of a single emission mechanism.","From the results of an earlier analysis of the emission from a pulsar's current sheet and the values of the fit parameters for each spectrum, we also determine the physical characteristics of the central neutron star of each considered example and its magnetosphere."],"url":"http://arxiv.org/abs/2402.09913v1","category":"astro-ph.HE"}
{"created":"2024-02-15 12:20:02","title":"Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering","abstract":"Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ended questions. For precise questions, we observe a minimum accuracy improvement of 7.5. Moreover, there is also demonstration that this framework exhibits generalizability across different KG sources. In summary, our results pave the way for enhancing LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the context of open-ended questions.","sentences":["Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task.","Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations.","Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously.","To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed.","The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation.","Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources.","Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ended questions.","For precise questions, we observe a minimum accuracy improvement of 7.5.","Moreover, there is also demonstration that this framework exhibits generalizability across different KG sources.","In summary, our results pave the way for enhancing LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the context of open-ended questions."],"url":"http://arxiv.org/abs/2402.09911v1","category":"cs.CL"}
{"created":"2024-02-15 12:12:19","title":"Generative Representational Instruction Tuning","abstract":"All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.","sentences":["All text-based language problems can be reduced to either generation or embedding.","Current models only perform well at one or the other.","We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions.","Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks.","By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models.","Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss.","Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models.","Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm."],"url":"http://arxiv.org/abs/2402.09906v1","category":"cs.CL"}
{"created":"2024-02-15 12:05:57","title":"Effective yields as tracers of feedback effects on metallicity scaling relations in the EAGLE cosmological simulations","abstract":"Effective yields, $y_{\\rm eff}$, are defined by fundamental galaxy properties (i.e., stellar mass -$M_{\\star}$-, gas mass -$M_{\\rm gas}$- and gas-phase metallicity). For a closed-box model, $y_{\\rm eff}$ is constant and equivalent to the mass in metals returned to the gas per unit mass locked in long-lived stars. Deviations from such behaviour have been often considered observational signatures of past feedback events. By analysing EAGLE simulations with different feedback models, we evaluate the impact of supernovae (SN) and active galactic nuclei (AGN) feedback on $y_{\\rm eff}$ at redshift $z=0$. When removing supermassive black holes (BH) and, hence, AGN effects, in simulations, galaxies are located around a plane in the $M_{\\star} - M_{\\rm gas} - {\\rm O/H}$ parameter space (being O/H a proxy for gas metallicity, as usual), with such a plane roughly describing a surface of constant $y_{\\rm eff}$. As the ratio between BH mass and $M_{\\star}$ increases, galaxies deviate from that plane towards lower $y_{\\rm eff}$ as a consequence of AGN feedback. For galaxies not strongly affected by AGN feedback, a stronger SN feedback efficiency generates deviations towards lower $y_{\\rm eff}$, while galaxies move towards the opposite side of the plane (i.e., towards higher values of $y_{\\rm eff}$) as SN feedback becomes weaker. Star-forming galaxies observed in the Local Universe are located around a similar 3D plane. Our results suggest that the features of the scatter around the observed plane are related to the different feedback histories of galaxies, which might be traced by $y_{\\rm eff}$.","sentences":["Effective yields, $y_{\\rm eff}$, are defined by fundamental galaxy properties (i.e., stellar mass -$M_{\\star}$-, gas mass -$M_{\\rm gas}$- and gas-phase metallicity).","For a closed-box model, $y_{\\rm eff}$ is constant and equivalent to the mass in metals returned to the gas per unit mass locked in long-lived stars.","Deviations from such behaviour have been often considered observational signatures of past feedback events.","By analysing EAGLE simulations with different feedback models, we evaluate the impact of supernovae (SN) and active galactic nuclei (AGN) feedback on $y_{\\rm eff}$ at redshift $z=0$. When removing supermassive black holes (BH) and, hence, AGN effects, in simulations, galaxies are located around a plane in the $M_{\\star} - M_{\\rm gas} - {\\rm O/H}$ parameter space (being O/H a proxy for gas metallicity, as usual), with such a plane roughly describing a surface of constant $y_{\\rm eff}$. As the ratio between BH mass and $M_{\\star}$ increases, galaxies deviate from that plane towards lower $y_{\\rm eff}$ as a consequence of AGN feedback.","For galaxies not strongly affected by AGN feedback, a stronger SN feedback efficiency generates deviations towards lower $y_{\\rm eff}$, while galaxies move towards the opposite side of the plane (i.e., towards higher values of $y_{\\rm eff}$) as SN feedback becomes weaker.","Star-forming galaxies observed in the Local Universe are located around a similar 3D plane.","Our results suggest that the features of the scatter around the observed plane are related to the different feedback histories of galaxies, which might be traced by $y_{\\rm eff}$."],"url":"http://arxiv.org/abs/2402.09904v1","category":"astro-ph.GA"}
{"created":"2024-02-15 12:05:14","title":"Enumeration of multiplex juggling card sequences using generalized q-derivatives","abstract":"In 2019, Butler, Choi, Kim, and Seo introduced a new type of juggling card that represents multiplex juggling patterns in a natural bijective way. They conjectured a formula for the generating function for the number of multiplex juggling cards with capacity 2. In this paper we prove their conjecture. More generally, we find an explicit formula for the generating function with any capacity. We also find an expression for the generating function for multiplex juggling card sequences by introducing a generalization of the q-derivative operator. As a consequence, we show that this generating function is a rational function.","sentences":["In 2019, Butler, Choi, Kim, and Seo introduced a new type of juggling card that represents multiplex juggling patterns in a natural bijective way.","They conjectured a formula for the generating function for the number of multiplex juggling cards with capacity 2.","In this paper we prove their conjecture.","More generally, we find an explicit formula for the generating function with any capacity.","We also find an expression for the generating function for multiplex juggling card sequences by introducing a generalization of the q-derivative operator.","As a consequence, we show that this generating function is a rational function."],"url":"http://arxiv.org/abs/2402.09903v1","category":"math.CO"}
{"created":"2024-02-15 11:56:53","title":"Revisiting Recurrent Reinforcement Learning with Memory Monoids","abstract":"In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.","sentences":["In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states.","Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models.","We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework.","We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies.","Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL."],"url":"http://arxiv.org/abs/2402.09900v1","category":"cs.LG"}
{"created":"2024-02-15 11:45:19","title":"Two-Timescale Design for Active STAR-RIS Aided Massive MIMO Systems","abstract":"Simultaneously transmitting and reflecting \\textcolor{black}{reconfigurable intelligent surface} (STAR-RIS) is a promising implementation of RIS-assisted systems that enables full-space coverage. However, STAR-RIS as well as conventional RIS suffer from the double-fading effect. Thus, in this paper, we propose the marriage of active RIS and STAR-RIS, denoted as ASTARS for massive multiple-input multiple-output (mMIMO) systems, and we focus on the energy splitting (ES) and mode switching (MS) protocols. Compared to prior literature, we consider the impact of correlated fading, and we rely our analysis on the two timescale protocol, being dependent on statistical channel state information (CSI). On this ground, we propose a channel estimation method for ASTARS with reduced overhead that accounts for its architecture. Next, we derive a \\textcolor{black}{closed-form expression} for the achievable sum-rate for both types of users in the transmission and reflection regions in a unified approach with significant practical advantages such as reduced complexity and overhead, which result in a lower number of required iterations for convergence compared to an alternating optimization (AO) approach. Notably, we maximize simultaneously the amplitudes, the phase shifts, and the active amplifying coefficients of the ASTARS by applying the projected gradient ascent method (PGAM). Remarkably, the proposed optimization can be executed at every several coherence intervals that reduces the processing burden considerably. Simulations corroborate the analytical results, provide insight into the effects of fundamental variables on the sum achievable SE, and present the superiority of 16 ASTARS compared to passive STAR-RIS for a practical number of surface elements.","sentences":["Simultaneously transmitting and reflecting \\textcolor{black}{reconfigurable intelligent surface} (STAR-RIS) is a promising implementation of RIS-assisted systems that enables full-space coverage.","However, STAR-RIS as well as conventional RIS suffer from the double-fading effect.","Thus, in this paper, we propose the marriage of active RIS and STAR-RIS, denoted as ASTARS for massive multiple-input multiple-output (mMIMO) systems, and we focus on the energy splitting (ES) and mode switching (MS) protocols.","Compared to prior literature, we consider the impact of correlated fading, and we rely our analysis on the two timescale protocol, being dependent on statistical channel state information (CSI).","On this ground, we propose a channel estimation method for ASTARS with reduced overhead that accounts for its architecture.","Next, we derive a \\textcolor{black}{closed-form expression} for the achievable sum-rate for both types of users in the transmission and reflection regions in a unified approach with significant practical advantages such as reduced complexity and overhead, which result in a lower number of required iterations for convergence compared to an alternating optimization (AO) approach.","Notably, we maximize simultaneously the amplitudes, the phase shifts, and the active amplifying coefficients of the ASTARS by applying the projected gradient ascent method (PGAM).","Remarkably, the proposed optimization can be executed at every several coherence intervals that reduces the processing burden considerably.","Simulations corroborate the analytical results, provide insight into the effects of fundamental variables on the sum achievable SE, and present the superiority of 16 ASTARS compared to passive STAR-RIS for a practical number of surface elements."],"url":"http://arxiv.org/abs/2402.09896v1","category":"cs.IT"}
{"created":"2024-02-15 11:39:11","title":"Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows","abstract":"Generative AI brings novel and impressive abilities to help people in everyday tasks. There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction. Although there is an undeniable lure of AI, it's uncertain how useful generative AI workflows are after the novelty wears off. Additionally, tools built with generative AI have the potential to be personalized and adapted quickly and easily, but do users take advantage of the potential to customize? We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of generative AI tools for science communication. Our study revealed that the familiarization phase lasts for 4.3 sessions, where users explore the capabilities of the workflow and which aspects they find useful. After familiarization, the perceived utility of the system is rated higher than before, indicating that the perceived utility of AI is not just a novelty effect. The increase in benefits mainly comes from end-users' ability to customize prompts, and thus appropriate the system to their own needs. This points to a future where generative AI systems can allow us to design for appropriation.","sentences":["Generative AI brings novel and impressive abilities to help people in everyday tasks.","There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction.","Although there is an undeniable lure of AI, it's uncertain how useful generative AI workflows are after the novelty wears off.","Additionally, tools built with generative AI have the potential to be personalized and adapted quickly and easily, but do users take advantage of the potential to customize?","We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of generative AI tools for science communication.","Our study revealed that the familiarization phase lasts for 4.3 sessions, where users explore the capabilities of the workflow and which aspects they find useful.","After familiarization, the perceived utility of the system is rated higher than before, indicating that the perceived utility of AI is not just a novelty effect.","The increase in benefits mainly comes from end-users' ability to customize prompts, and thus appropriate the system to their own needs.","This points to a future where generative AI systems can allow us to design for appropriation."],"url":"http://arxiv.org/abs/2402.09894v1","category":"cs.HC"}
{"created":"2024-02-15 11:34:38","title":"Predictors from causal features do not generalize better to new domains","abstract":"We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features. If the goal is to generalize to new domains, practitioners might as well train the best possible model on all available features.","sentences":["We study how well machine learning models trained on causal features generalize across domains.","We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics.","Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another.","For each prediction task, we select features that have a causal influence on the target of prediction.","Our goal is to test the hypothesis that models trained on causal features generalize better across domains.","Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features.","Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features.","If the goal is to generalize to new domains, practitioners might as well train the best possible model on all available features."],"url":"http://arxiv.org/abs/2402.09891v1","category":"cs.LG"}
{"created":"2024-02-15 11:24:45","title":"Jones--Wenzl projections and Dyck tilings: type $A$ and $B$","abstract":"The Jones--Wenzl projections are a special class of elements of the Temperley--Lieb algebra. We prove that the coefficient appearing in the Jones--Wenzl projection is given by a generating function of combinatorial objects, called Dyck tilings. We also show that this correspondence holds for type $B$ case.","sentences":["The Jones--Wenzl projections are a special class of elements of the Temperley--Lieb algebra.","We prove that the coefficient appearing in the Jones--Wenzl projection is given by a generating function of combinatorial objects, called Dyck tilings.","We also show that this correspondence holds for type $B$ case."],"url":"http://arxiv.org/abs/2402.09887v1","category":"math.CO"}
{"created":"2024-02-15 11:15:54","title":"Lester: rotoscope animation through video object segmentation and tracking","abstract":"This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer from temporal consistency problems and do not cope well with pixelated and schematic outputs. The method is also much most practical than techniques based on 3D human pose estimation, which require custom handcrafted 3D models and are very limited with respect to the type of scenes they can process.","sentences":["This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos.","The method approaches the challenge mainly as an object segmentation and tracking problem.","Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation.","The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm.","Finally, facial traits, pixelation and a basic shadow effect can be optionally added.","The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds.","The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer from temporal consistency problems and do not cope well with pixelated and schematic outputs.","The method is also much most practical than techniques based on 3D human pose estimation, which require custom handcrafted 3D models and are very limited with respect to the type of scenes they can process."],"url":"http://arxiv.org/abs/2402.09883v1","category":"cs.CV"}
{"created":"2024-02-15 11:08:54","title":"Variability Modeling of Products, Processes, and Resources in Cyber-Physical Production Systems Engineering","abstract":"Cyber-Physical Production Systems (CPPSs), such as automated car manufacturing plants, execute a configurable sequence of production steps to manufacture products from a product portfolio. In CPPS engineering, domain experts start with manually determining feasible production step sequences and resources based on implicit knowledge. This process is hard to reproduce and highly inefficient. In this paper, we present the Extended Iterative Process Sequence Exploration (eIPSE) approach to derive variability models for products, processes, and resources from a domain-specific description. To automate the integrated exploration and configuration process for a CPPS, we provide a toolchain which automatically reduces the configuration space and allows to generate CPPS artifacts, such as control code for resources. We evaluate the approach with four real-world use cases, including the generation of control code artifacts, and an observational user study to collect feedback from engineers with different backgrounds. The results confirm the usefulness of the eIPSE approach and accompanying prototype to straightforwardly configure a desired CPPS.","sentences":["Cyber-Physical Production Systems (CPPSs), such as automated car manufacturing plants, execute a configurable sequence of production steps to manufacture products from a product portfolio.","In CPPS engineering, domain experts start with manually determining feasible production step sequences and resources based on implicit knowledge.","This process is hard to reproduce and highly inefficient.","In this paper, we present the Extended Iterative Process Sequence Exploration (eIPSE) approach to derive variability models for products, processes, and resources from a domain-specific description.","To automate the integrated exploration and configuration process for a CPPS, we provide a toolchain which automatically reduces the configuration space and allows to generate CPPS artifacts, such as control code for resources.","We evaluate the approach with four real-world use cases, including the generation of control code artifacts, and an observational user study to collect feedback from engineers with different backgrounds.","The results confirm the usefulness of the eIPSE approach and accompanying prototype to straightforwardly configure a desired CPPS."],"url":"http://arxiv.org/abs/2402.09882v1","category":"cs.SE"}
{"created":"2024-02-15 11:08:10","title":"Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence","abstract":"The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral profiling to accurately capture LLMs' complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems' integration into society.","sentences":["The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their LLM benchmarks.","Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security.","Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment.","Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral profiling to accurately capture LLMs' complex behaviors and potential risks.","Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems' integration into society."],"url":"http://arxiv.org/abs/2402.09880v1","category":"cs.AI"}
{"created":"2024-02-15 11:01:33","title":"Explaining all-optical switching in ferrimagnets with heavy rare-earth elements by varying the spin-flip scattering probability of Gd in Co$_x$Gd$_{100-x}$ alloys and Co/Gd bilayers","abstract":"Using the microscopic three temperature model, we simulate single-pulse all-optical switching (AOS) in alloys and bilayers consisting of Co and Gd. In particular, we investigate its dependence on the spin-flip probability of Gd $a_\\mathrm{sf,Gd}$, a material parameter describing the strength of spin-phonon coupling. We do so to elucidate the mechanisms behind all-optical switching in systems where Co is coupled to heavy rare-earth elements with higher damping such as Tb. In alloys, our observations are twofold. First, an increase of $a_\\mathrm{sf,Gd}$ leads to a broadening of the range of compositions for which AOS is observed. Second, the ideal Co content is decreased as $a_\\mathrm{sf,Gd}$ is varied. For bilayers, our analysis indicates that switching is most efficient when $a_\\mathrm{sf,Gd}$ takes on small values. Conversely, increasing the value of $a_\\mathrm{sf,Gd}$ leads to a general suppression of AOS. Comparing alloys to bilayers, we find that AOS in alloys exhibits greater resilience to variations in $a_\\mathrm{sf,Gd}$ than it does in bilayers.","sentences":["Using the microscopic three temperature model, we simulate single-pulse all-optical switching (AOS) in alloys and bilayers consisting of Co and Gd.","In particular, we investigate its dependence on the spin-flip probability of Gd $a_\\mathrm{sf,Gd}$, a material parameter describing the strength of spin-phonon coupling.","We do so to elucidate the mechanisms behind all-optical switching in systems where Co is coupled to heavy rare-earth elements with higher damping such as Tb.","In alloys, our observations are twofold.","First, an increase of $a_\\mathrm{sf,Gd}$ leads to a broadening of the range of compositions for which AOS is observed.","Second, the ideal Co content is decreased as $a_\\mathrm{sf,Gd}$ is varied.","For bilayers, our analysis indicates that switching is most efficient when $a_\\mathrm{sf,Gd}$ takes on small values.","Conversely, increasing the value of $a_\\mathrm{sf,Gd}$ leads to a general suppression of AOS.","Comparing alloys to bilayers, we find that AOS in alloys exhibits greater resilience to variations in $a_\\mathrm{sf,Gd}$ than it does in bilayers."],"url":"http://arxiv.org/abs/2402.09878v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-15 11:00:28","title":"On Computing Plans with Uniform Action Costs","abstract":"In many real-world planning applications, agents might be interested in finding plans whose actions have costs that are as uniform as possible. Such plans provide agents with a sense of stability and predictability, which are key features when humans are the agents executing plans suggested by planning tools. This paper adapts three uniformity metrics to automated planning, and introduce planning-based compilations that allow to lexicographically optimize sum of action costs and action costs uniformity. Experimental results both in well-known and novel planning benchmarks show that the reformulated tasks can be effectively solved in practice to generate uniform plans.","sentences":["In many real-world planning applications, agents might be interested in finding plans whose actions have costs that are as uniform as possible.","Such plans provide agents with a sense of stability and predictability, which are key features when humans are the agents executing plans suggested by planning tools.","This paper adapts three uniformity metrics to automated planning, and introduce planning-based compilations that allow to lexicographically optimize sum of action costs and action costs uniformity.","Experimental results both in well-known and novel planning benchmarks show that the reformulated tasks can be effectively solved in practice to generate uniform plans."],"url":"http://arxiv.org/abs/2402.09877v1","category":"cs.AI"}
{"created":"2024-02-15 10:58:22","title":"Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks","abstract":"Adversarial attacks represent a substantial challenge in Natural Language Processing (NLP). This study undertakes a systematic exploration of this challenge in two distinct phases: vulnerability evaluation and resilience enhancement of Transformer-based models under adversarial attacks.   In the evaluation phase, we assess the susceptibility of three Transformer configurations, encoder-decoder, encoder-only, and decoder-only setups, to adversarial attacks of escalating complexity across datasets containing offensive language and misinformation. Encoder-only models manifest a 14% and 21% performance drop in offensive language detection and misinformation detection tasks, respectively. Decoder-only models register a 16% decrease in both tasks, while encoder-decoder models exhibit a maximum performance drop of 14% and 26% in the respective tasks.   The resilience-enhancement phase employs adversarial training, integrating pre-camouflaged and dynamically altered data. This approach effectively reduces the performance drop in encoder-only models to an average of 5% in offensive language detection and 2% in misinformation detection tasks. Decoder-only models, occasionally exceeding original performance, limit the performance drop to 7% and 2% in the respective tasks. Although not surpassing the original performance, Encoder-decoder models can reduce the drop to an average of 6% and 2% respectively.   Results suggest a trade-off between performance and robustness, with some models maintaining similar performance while gaining robustness. Our study and adversarial training techniques have been incorporated into an open-source tool for generating camouflaged datasets. However, methodology effectiveness depends on the specific camouflage technique and data encountered, emphasizing the need for continued exploration.","sentences":["Adversarial attacks represent a substantial challenge in Natural Language Processing (NLP).","This study undertakes a systematic exploration of this challenge in two distinct phases: vulnerability evaluation and resilience enhancement of Transformer-based models under adversarial attacks.   ","In the evaluation phase, we assess the susceptibility of three Transformer configurations, encoder-decoder, encoder-only, and decoder-only setups, to adversarial attacks of escalating complexity across datasets containing offensive language and misinformation.","Encoder-only models manifest a 14% and 21% performance drop in offensive language detection and misinformation detection tasks, respectively.","Decoder-only models register a 16% decrease in both tasks, while encoder-decoder models exhibit a maximum performance drop of 14% and 26% in the respective tasks.   ","The resilience-enhancement phase employs adversarial training, integrating pre-camouflaged and dynamically altered data.","This approach effectively reduces the performance drop in encoder-only models to an average of 5% in offensive language detection and 2% in misinformation detection tasks.","Decoder-only models, occasionally exceeding original performance, limit the performance drop to 7% and 2% in the respective tasks.","Although not surpassing the original performance, Encoder-decoder models can reduce the drop to an average of 6% and 2% respectively.   ","Results suggest a trade-off between performance and robustness, with some models maintaining similar performance while gaining robustness.","Our study and adversarial training techniques have been incorporated into an open-source tool for generating camouflaged datasets.","However, methodology effectiveness depends on the specific camouflage technique and data encountered, emphasizing the need for continued exploration."],"url":"http://arxiv.org/abs/2402.09874v1","category":"cs.CL"}
{"created":"2024-02-15 10:56:31","title":"Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community","abstract":"Social reward as a form of community recognition provides a strong source of motivation for users of online platforms to engage and contribute with content. The recent progress of text-conditioned image synthesis has ushered in a collaborative era where AI empowers users to craft original visual artworks seeking community validation. Nevertheless, assessing these models in the context of collective community preference introduces distinct challenges. Existing evaluation methods predominantly center on limited size user studies guided by image quality and prompt alignment. This work pioneers a paradigm shift, unveiling Social Reward - an innovative reward modeling framework that leverages implicit feedback from social network users engaged in creative editing of generated images. We embark on an extensive journey of dataset curation and refinement, drawing from Picsart: an online visual creation and editing platform, yielding a first million-user-scale dataset of implicit human preferences for user-generated visual art named Picsart Image-Social. Our analysis exposes the shortcomings of current metrics in modeling community creative preference of text-to-image models' outputs, compelling us to introduce a novel predictive model explicitly tailored to address these limitations. Rigorous quantitative experiments and user study show that our Social Reward model aligns better with social popularity than existing metrics. Furthermore, we utilize Social Reward to fine-tune text-to-image models, yielding images that are more favored by not only Social Reward, but also other established metrics. These findings highlight the relevance and effectiveness of Social Reward in assessing community appreciation for AI-generated artworks, establishing a closer alignment with users' creative goals: creating popular visual art. Codes can be accessed at https://github.com/Picsart-AI-Research/Social-Reward","sentences":["Social reward as a form of community recognition provides a strong source of motivation for users of online platforms to engage and contribute with content.","The recent progress of text-conditioned image synthesis has ushered in a collaborative era where AI empowers users to craft original visual artworks seeking community validation.","Nevertheless, assessing these models in the context of collective community preference introduces distinct challenges.","Existing evaluation methods predominantly center on limited size user studies guided by image quality and prompt alignment.","This work pioneers a paradigm shift, unveiling Social Reward - an innovative reward modeling framework that leverages implicit feedback from social network users engaged in creative editing of generated images.","We embark on an extensive journey of dataset curation and refinement, drawing from Picsart: an online visual creation and editing platform, yielding a first million-user-scale dataset of implicit human preferences for user-generated visual art named Picsart Image-Social.","Our analysis exposes the shortcomings of current metrics in modeling community creative preference of text-to-image models' outputs, compelling us to introduce a novel predictive model explicitly tailored to address these limitations.","Rigorous quantitative experiments and user study show that our Social Reward model aligns better with social popularity than existing metrics.","Furthermore, we utilize Social Reward to fine-tune text-to-image models, yielding images that are more favored by not only Social Reward, but also other established metrics.","These findings highlight the relevance and effectiveness of Social Reward in assessing community appreciation for AI-generated artworks, establishing a closer alignment with users' creative goals: creating popular visual art.","Codes can be accessed at https://github.com/Picsart-AI-Research/Social-Reward"],"url":"http://arxiv.org/abs/2402.09872v1","category":"cs.CV"}
{"created":"2024-02-15 10:55:01","title":"MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music","abstract":"The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music. However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks. To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music. We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics. Utilizing this method, we built a dataset with multi-dimensional, high-precision music annotations, the Caichong Music Dataset (CaiMD), and carefully selected 1,000 high-quality entries to serve as the test set for MuChin. Based on MuChin, we analyzed the discrepancies between professionals and amateurs in terms of music description, and empirically demonstrated the effectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed MuChin to evaluate existing music understanding models on their ability to provide colloquial descriptions of music. All data related to the benchmark and the code for scoring have been open-sourced.","sentences":["The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music.","However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks.","To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music.","We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics.","Utilizing this method, we built a dataset with multi-dimensional, high-precision music annotations, the Caichong Music Dataset (CaiMD), and carefully selected 1,000 high-quality entries to serve as the test set for MuChin.","Based on MuChin, we analyzed the discrepancies between professionals and amateurs in terms of music description, and empirically demonstrated the effectiveness of annotated data for fine-tuning LLMs.","Ultimately, we employed MuChin to evaluate existing music understanding models on their ability to provide colloquial descriptions of music.","All data related to the benchmark and the code for scoring have been open-sourced."],"url":"http://arxiv.org/abs/2402.09871v1","category":"cs.SD"}
{"created":"2024-02-15 10:54:02","title":"Convex Equilibrium-Free Stability and Performance Analysis of Discrete-Time Nonlinear Systems","abstract":"This paper considers the equilibrium-free stability and performance analysis of discrete-time nonlinear systems. We consider two types of equilibrium-free notions. Namely, the universal shifted concept, which considers stability and performance w.r.t. all equilibrium points of the system, and the incremental concept, which considers stability and performance between trajectories of the system. In this paper, we show how universal shifted stability and performance of discrete-time systems can be analyzed by making use of the time-difference dynamics. Moreover, we extend the existing results for incremental dissipativity for discrete-time systems based on dissipativity analysis of the differential dynamics to more general state-dependent storage functions for less conservative results. Finally, we show how both these equilibrium-free notions can be cast as a convex analysis problem by making use of the linear parameter-varying framework, which is also demonstrated by means of an example.","sentences":["This paper considers the equilibrium-free stability and performance analysis of discrete-time nonlinear systems.","We consider two types of equilibrium-free notions.","Namely, the universal shifted concept, which considers stability and performance w.r.t.","all equilibrium points of the system, and the incremental concept, which considers stability and performance between trajectories of the system.","In this paper, we show how universal shifted stability and performance of discrete-time systems can be analyzed by making use of the time-difference dynamics.","Moreover, we extend the existing results for incremental dissipativity for discrete-time systems based on dissipativity analysis of the differential dynamics to more general state-dependent storage functions for less conservative results.","Finally, we show how both these equilibrium-free notions can be cast as a convex analysis problem by making use of the linear parameter-varying framework, which is also demonstrated by means of an example."],"url":"http://arxiv.org/abs/2402.09870v1","category":"eess.SY"}
{"created":"2024-02-15 10:50:42","title":"Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs","abstract":"Electroencephalography (EEG) recordings are analyzed using battery-powered wearable devices to monitor brain activities and neurological disorders. These applications require long and continuous processing to generate feasible results. However, wearable devices are constrained with limited energy and computation resources, owing to their small sizes for practical use cases. Embedded heterogeneous multi-core platforms (HMPs) can provide better performance within limited energy budgets for EEG applications. Error resilience of the EEG application pipeline can be exploited further to maximize the performance and energy gains with HMPs. However, disciplined tuning of approximation on embedded HMPs requires a thorough exploration of the accuracy-performance-power trade-off space. In this work, we characterize the error resilience of three EEG applications, including Epileptic Seizure Detection, Sleep Stage Classification, and Stress Detection on the real-world embedded HMP test-bed of the Odroid XU3 platform. We present a combinatorial evaluation of power-performance-accuracy trade-offs of EEG applications at different approximation, power, and performance levels to provide insights into the disciplined tuning of approximation in EEG applications on embedded platforms.","sentences":["Electroencephalography (EEG) recordings are analyzed using battery-powered wearable devices to monitor brain activities and neurological disorders.","These applications require long and continuous processing to generate feasible results.","However, wearable devices are constrained with limited energy and computation resources, owing to their small sizes for practical use cases.","Embedded heterogeneous multi-core platforms (HMPs) can provide better performance within limited energy budgets for EEG applications.","Error resilience of the EEG application pipeline can be exploited further to maximize the performance and energy gains with HMPs.","However, disciplined tuning of approximation on embedded HMPs requires a thorough exploration of the accuracy-performance-power trade-off space.","In this work, we characterize the error resilience of three EEG applications, including Epileptic Seizure Detection, Sleep Stage Classification, and Stress Detection on the real-world embedded HMP test-bed of the Odroid XU3 platform.","We present a combinatorial evaluation of power-performance-accuracy trade-offs of EEG applications at different approximation, power, and performance levels to provide insights into the disciplined tuning of approximation in EEG applications on embedded platforms."],"url":"http://arxiv.org/abs/2402.09867v1","category":"eess.SP"}
{"created":"2024-02-15 10:34:45","title":"Double-well instantons in finite volume","abstract":"Assuming a toroidal space with finite volume, we derive analytically the full one-loop vacuum energy for a scalar field tunnelling between two degenerate vacua, taking into account discrete momentum. The Casimir energy is computed for an arbitrary number of dimensions using the Abel-Plana formula, while the one-loop instanton functional determinant is evaluated using the Green's functions for the fluctuation operators. The resulting energetic properties are non-trivial: both the Casimir effect and tunnelling contribute to the Null Energy Condition violation, arising from a non-extensive true vacuum energy. We discuss the relevance of this mechanism to induce a cosmic bounce, requiring no modified gravity or exotic matter.","sentences":["Assuming a toroidal space with finite volume, we derive analytically the full one-loop vacuum energy for a scalar field tunnelling between two degenerate vacua, taking into account discrete momentum.","The Casimir energy is computed for an arbitrary number of dimensions using the Abel-Plana formula, while the one-loop instanton functional determinant is evaluated using the Green's functions for the fluctuation operators.","The resulting energetic properties are non-trivial: both the Casimir effect and tunnelling contribute to the Null Energy Condition violation, arising from a non-extensive true vacuum energy.","We discuss the relevance of this mechanism to induce a cosmic bounce, requiring no modified gravity or exotic matter."],"url":"http://arxiv.org/abs/2402.09863v1","category":"hep-th"}
{"created":"2024-02-15 10:29:06","title":"Testing mirror symmetry in the Universe with LIGO-Virgo black-hole mergers","abstract":"Precessing black-hole mergers can produce gravitational waves with net circular polarization, understood as an imbalance between right- and left-handed amplitudes. According to the Cosmological Principle, such emission must average to zero across all binary mergers in our Universe to preserve mirror-reflection symmetry at very large scales. We present a new, independent gravitational-wave test of this hypothesis. Using a novel observable based on the Chern-Pontryagin pseudo-scalar, we measure the emission of net circular polarization across 47 black-hole mergers recently analyzed by Islam. et. al. with a state-of-the art model for precessing black-hole mergers. The average value obtained is consistent with zero. Remarkably, however, we find that at least $82\\%$ of the analysed sources must have produced net circular polarization, which requires orbital precession. Of these, GW200129 shows strong evidence for mirror asymmetry, with a Bayes Factor of 12.6 or, equivalently, $93.1\\%$ probability. We obtain consistent (although stronger) results of $97.5\\%$ and $94.3\\%$ respectively using public results on this event from Hannam et. al. and performing our own parameter inference. This finding further implies indirect evidence for spontaneous emission of circularly polarized photons out of the quantum vacuum. Forthcoming black-hole merger detections will enable stronger constraints on large-scale mirror asymmetry and the Cosmological Principle.","sentences":["Precessing black-hole mergers can produce gravitational waves with net circular polarization, understood as an imbalance between right- and left-handed amplitudes.","According to the Cosmological Principle, such emission must average to zero across all binary mergers in our Universe to preserve mirror-reflection symmetry at very large scales.","We present a new, independent gravitational-wave test of this hypothesis.","Using a novel observable based on the Chern-Pontryagin pseudo-scalar, we measure the emission of net circular polarization across 47 black-hole mergers recently analyzed by Islam.","et.","al.","with a state-of-the art model for precessing black-hole mergers.","The average value obtained is consistent with zero.","Remarkably, however, we find that at least $82\\%$ of the analysed sources must have produced net circular polarization, which requires orbital precession.","Of these, GW200129 shows strong evidence for mirror asymmetry, with a Bayes Factor of 12.6 or, equivalently, $93.1\\%$ probability.","We obtain consistent (although stronger) results of $97.5\\%$ and $94.3\\%$ respectively using public results on this event from Hannam et.","al. and performing our own parameter inference.","This finding further implies indirect evidence for spontaneous emission of circularly polarized photons out of the quantum vacuum.","Forthcoming black-hole merger detections will enable stronger constraints on large-scale mirror asymmetry and the Cosmological Principle."],"url":"http://arxiv.org/abs/2402.09861v1","category":"gr-qc"}
{"created":"2024-02-15 10:22:48","title":"Predicting the Ages of Galaxies with an Artificial Neural Network","abstract":"We present a new method of predicting the ages of galaxies using a machine learning (ML) algorithm with the goal of providing an alternative to traditional methods. We aim to match the ability of traditional models to predict the ages of galaxies by training an artificial neural network (ANN) to recognise the relationships between the equivalent widths of spectral indices and the mass-weighted ages of galaxies estimated by the MAGPHYS model in data release 3 (DR3) of the Galaxy and Mass Assembly (GAMA) survey. We discuss the optimisation of our hyperparameters extensively and investigate the application of a custom loss function to reduce the influence of errors in our input data. To quantify the quality of our predictions we calculate the mean squared error (MSE), mean absolute error (MAE) and R^2 score for which we find MSE = 0.020, MAE = 0.108 and R^2 = 0.530. We find our predicted ages have a similar distribution with standard deviation sigma_p = 0.182 compared with the GAMA true ages sigma_t = 0.207. This is achieved in approximately 23s to train our ANN on an 11th Gen Intel Core i9-11900H running at 2.50GHz using 32GB of RAM. We report our results for when light-weighted ages are used to train the ANN, which improves the accuracy of the predictions. Finally, we detail an evaluation of our method relating to physical properties and compare with other ML techniques to encourage future applications of ML techniques in Astronomy.","sentences":["We present a new method of predicting the ages of galaxies using a machine learning (ML) algorithm with the goal of providing an alternative to traditional methods.","We aim to match the ability of traditional models to predict the ages of galaxies by training an artificial neural network (ANN) to recognise the relationships between the equivalent widths of spectral indices and the mass-weighted ages of galaxies estimated by the MAGPHYS model in data release 3 (DR3) of the Galaxy and Mass Assembly (GAMA) survey.","We discuss the optimisation of our hyperparameters extensively and investigate the application of a custom loss function to reduce the influence of errors in our input data.","To quantify the quality of our predictions we calculate the mean squared error (MSE), mean absolute error (MAE) and R^2 score for which we find MSE = 0.020, MAE = 0.108 and R^2 = 0.530.","We find our predicted ages have a similar distribution with standard deviation sigma_p = 0.182 compared with the GAMA true ages sigma_t = 0.207.","This is achieved in approximately 23s to train our ANN on an 11th Gen Intel Core i9-11900H running at 2.50GHz using 32GB of RAM.","We report our results for when light-weighted ages are used to train the ANN, which improves the accuracy of the predictions.","Finally, we detail an evaluation of our method relating to physical properties and compare with other ML techniques to encourage future applications of ML techniques in Astronomy."],"url":"http://arxiv.org/abs/2402.09857v1","category":"astro-ph.GA"}
{"created":"2024-02-15 10:20:40","title":"Improving the efficiency of GP-GOMEA for higher-arity operators","abstract":"Deploying machine learning models into sensitive domains in our society requires these models to be explainable. Genetic Programming (GP) can offer a way to evolve inherently interpretable expressions. GP-GOMEA is a form of GP that has been found particularly effective at evolving expressions that are accurate yet of limited size and, thus, promote interpretability. Despite this strength, a limitation of GP-GOMEA is template-based. This negatively affects its scalability regarding the arity of operators that can be used, since with increasing operator arity, an increasingly large part of the template tends to go unused. In this paper, we therefore propose two enhancements to GP-GOMEA: (i) semantic subtree inheritance, which performs additional variation steps that consider the semantic context of a subtree, and (ii) greedy child selection, which explicitly considers parts of the template that in standard GP-GOMEA remain unused. We compare different versions of GP-GOMEA regarding search enhancements on a set of continuous and discontinuous regression problems, with varying tree depths and operator sets. Experimental results show that both proposed search enhancements have a generally positive impact on the performance of GP-GOMEA, especially when the set of operators to choose from is large and contains higher-arity operators.","sentences":["Deploying machine learning models into sensitive domains in our society requires these models to be explainable.","Genetic Programming (GP) can offer a way to evolve inherently interpretable expressions.","GP-GOMEA is a form of GP that has been found particularly effective at evolving expressions that are accurate yet of limited size and, thus, promote interpretability.","Despite this strength, a limitation of GP-GOMEA is template-based.","This negatively affects its scalability regarding the arity of operators that can be used, since with increasing operator arity, an increasingly large part of the template tends to go unused.","In this paper, we therefore propose two enhancements to GP-GOMEA: (i) semantic subtree inheritance, which performs additional variation steps that consider the semantic context of a subtree, and (ii) greedy child selection, which explicitly considers parts of the template that in standard GP-GOMEA remain unused.","We compare different versions of GP-GOMEA regarding search enhancements on a set of continuous and discontinuous regression problems, with varying tree depths and operator sets.","Experimental results show that both proposed search enhancements have a generally positive impact on the performance of GP-GOMEA, especially when the set of operators to choose from is large and contains higher-arity operators."],"url":"http://arxiv.org/abs/2402.09854v1","category":"cs.NE"}
{"created":"2024-02-15 10:18:23","title":"The stack of $G$-zips is a Mori dream space","abstract":"We first extend previous results of the author with T. Wedhorn and W. Goldring regarding the existence of $\\mu$-ordinary Hasse invariants for Hodge-type Shimura varieties to other automorphic line bundles. We also determine exactly which line bundles admit nonzero sections on the stack of $G$-zips of Pink--Wedhorn--Ziegler. Then, we define and study the Cox ring of the stack of $G$-zips and show that it is always finitely generated. Finally, beyond the case of line bundles, we define a ring of vector-valued automorphic forms on the stack of $G$-zips and study its properties. We prove that it is finitely generated in certain cases.","sentences":["We first extend previous results of the author with T. Wedhorn and W. Goldring regarding the existence of $\\mu$-ordinary Hasse invariants for Hodge-type Shimura varieties to other automorphic line bundles.","We also determine exactly which line bundles admit nonzero sections on the stack of $G$-zips of Pink--Wedhorn--Ziegler.","Then, we define and study the Cox ring of the stack of $G$-zips and show that it is always finitely generated.","Finally, beyond the case of line bundles, we define a ring of vector-valued automorphic forms on the stack of $G$-zips and study its properties.","We prove that it is finitely generated in certain cases."],"url":"http://arxiv.org/abs/2402.09852v1","category":"math.NT"}
{"created":"2024-02-15 10:15:06","title":"A categorification for the characteristic polynomial of matroids","abstract":"In the present paper, we provide a cohomology group as a categorification of the characteristic polynomial of matroids. The construction depends on the ``quasi-representation'' of a matroid. For a certain choice of the quasi-representation, we show that our cohomology theory gives a generalization of the chromatic cohomology introduced by L. Helme-Guizon and Y. Rong, and also the characteristic cohomology introduced by Z. Dancso and A. Licata.","sentences":["In the present paper, we provide a cohomology group as a categorification of the characteristic polynomial of matroids.","The construction depends on the ``quasi-representation'' of a matroid.","For a certain choice of the quasi-representation, we show that our cohomology theory gives a generalization of the chromatic cohomology introduced by L. Helme-Guizon and Y. Rong, and also the characteristic cohomology introduced by Z. Dancso and A. Licata."],"url":"http://arxiv.org/abs/2402.09851v1","category":"math.CO"}
{"created":"2024-02-15 10:12:14","title":"Application of a metric for complex polynomials to bounded modification of planar Pythagorean-hodograph curves","abstract":"By interpreting planar polynomial curves as complex-valued functions of a real parameter, an inner product, norm, metric function, and the notion of orthogonality may be defined for such curves. This approach is applied to the complex pre-image polynomials that generate planar Pythagorean-hodograph (PH) curves, to facilitate the implementation of bounded modifications of them that preserve their PH nature. The problems of bounded modifications under the constraint of fixed curve end points and end tangent directions, and of increasing the arc length of a PH curve by a prescribed amount, are also addressed.","sentences":["By interpreting planar polynomial curves as complex-valued functions of a real parameter, an inner product, norm, metric function, and the notion of orthogonality may be defined for such curves.","This approach is applied to the complex pre-image polynomials that generate planar Pythagorean-hodograph (PH) curves, to facilitate the implementation of bounded modifications of them that preserve their PH nature.","The problems of bounded modifications under the constraint of fixed curve end points and end tangent directions, and of increasing the arc length of a PH curve by a prescribed amount, are also addressed."],"url":"http://arxiv.org/abs/2402.09850v1","category":"math.NA"}
{"created":"2024-02-15 10:08:31","title":"Expressivity of parameterized quantum circuits for generative modeling of continuous multivariate distributions","abstract":"Parameterized quantum circuits have been extensively used as the basis for machine learning models in regression, classification, and generative tasks. For supervised learning their expressivity has been thoroughly investigated and several universality properties have been proven. However, in the case of quantum generative modeling, the situation is less clear, especially when the task is to model distributions over continuous variables. In this work, we focus on expectation value sampling-based models; models where random variables are sampled classically, encoded with a parametrized quantum circuit, and the expectation value of fixed observables is measured and returned as a sample. We prove the universality of such variational quantum algorithms for the generation of multivariate distributions. Additionally, we provide a detailed analysis of these models, including fundamental upper bounds on the dimensionality of the distributions these models can represent. We further present a tight trade-off result connecting the needed number of measurements and qubit numbers in order to have universality for a desired dimension of output distribution within an error tolerance. Finally we also show that the data encoding strategy relates to the so-called polynomial chaos expansion, which is an analog of the Fourier expansion. Our results may help guide the design of future quantum circuits in generative modeling tasks.","sentences":["Parameterized quantum circuits have been extensively used as the basis for machine learning models in regression, classification, and generative tasks.","For supervised learning their expressivity has been thoroughly investigated and several universality properties have been proven.","However, in the case of quantum generative modeling, the situation is less clear, especially when the task is to model distributions over continuous variables.","In this work, we focus on expectation value sampling-based models; models where random variables are sampled classically, encoded with a parametrized quantum circuit, and the expectation value of fixed observables is measured and returned as a sample.","We prove the universality of such variational quantum algorithms for the generation of multivariate distributions.","Additionally, we provide a detailed analysis of these models, including fundamental upper bounds on the dimensionality of the distributions these models can represent.","We further present a tight trade-off result connecting the needed number of measurements and qubit numbers in order to have universality for a desired dimension of output distribution within an error tolerance.","Finally we also show that the data encoding strategy relates to the so-called polynomial chaos expansion, which is an analog of the Fourier expansion.","Our results may help guide the design of future quantum circuits in generative modeling tasks."],"url":"http://arxiv.org/abs/2402.09848v1","category":"quant-ph"}
{"created":"2024-02-15 10:05:18","title":"A Deep Learning Approach to Radar-based QPE","abstract":"In this study, we propose a volume-to-point framework for quantitative precipitation estimation (QPE) based on the Quantitative Precipitation Estimation and Segregation Using Multiple Sensor (QPESUMS) Mosaic Radar data set. With a data volume consisting of the time series of gridded radar reflectivities over the Taiwan area, we used machine learning algorithms to establish a statistical model for QPE in weather stations. The model extracts spatial and temporal features from the input data volume and then associates these features with the location-specific precipitations. In contrast to QPE methods based on the Z-R relation, we leverage the machine learning algorithms to automatically detect the evolution and movement of weather systems and associate these patterns to a location with specific topographic attributes. Specifically, we evaluated this framework with the hourly precipitation data of 45 weather stations in Taipei during 2013-2016. In comparison to the operational QPE scheme used by the Central Weather Bureau, the volume-to-point framework performed comparably well in general cases and excelled in detecting heavy-rainfall events. By using the current results as the reference benchmark, the proposed method can integrate the heterogeneous data sources and potentially improve the forecast in extreme precipitation scenarios.","sentences":["In this study, we propose a volume-to-point framework for quantitative precipitation estimation (QPE) based on the Quantitative Precipitation Estimation and Segregation Using Multiple Sensor (QPESUMS)","Mosaic Radar data set.","With a data volume consisting of the time series of gridded radar reflectivities over the Taiwan area, we used machine learning algorithms to establish a statistical model for QPE in weather stations.","The model extracts spatial and temporal features from the input data volume and then associates these features with the location-specific precipitations.","In contrast to QPE methods based on the Z-R relation, we leverage the machine learning algorithms to automatically detect the evolution and movement of weather systems and associate these patterns to a location with specific topographic attributes.","Specifically, we evaluated this framework with the hourly precipitation data of 45 weather stations in Taipei during 2013-2016.","In comparison to the operational QPE scheme used by the Central Weather Bureau, the volume-to-point framework performed comparably well in general cases and excelled in detecting heavy-rainfall events.","By using the current results as the reference benchmark, the proposed method can integrate the heterogeneous data sources and potentially improve the forecast in extreme precipitation scenarios."],"url":"http://arxiv.org/abs/2402.09846v1","category":"physics.ao-ph"}
{"created":"2024-02-15 10:01:55","title":"Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent","abstract":"The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. In this paper, we present Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-sourced (see https://huggingface.co/jat-project/jat), including a pioneering general-purpose dataset.","sentences":["The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research.","The prevailing methodology in Reinforcement Learning (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model.","In this paper, we present Jack of All Trades (JAT), a transformer-based model with a unique design optimized for handling sequential decision-making tasks and multimodal data types.","The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL benchmarks, along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights.","The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-sourced (see https://huggingface.co/jat-project/jat), including a pioneering general-purpose dataset."],"url":"http://arxiv.org/abs/2402.09844v1","category":"cs.AI"}
{"created":"2024-02-15 10:00:49","title":"LAPDoc: Layout-Aware Prompting for Documents","abstract":"Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks. Opposed to that there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout. This involves a separate fine-tuning step for which additional training data is required. At present, no document transformers with comparable generalization to LLMs are available That raises the question which type of model is to be preferred for document understanding tasks. In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment. We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information. In our experiments we investigate the effects on the commercial ChatGPT model and the open-source LLM Solar. We demonstrate that using our approach both LLMs show improved performance on various standard document benchmarks. In addition, we study the impact of noisy OCR and layout errors, as well as the limitations of LLMs when it comes to utilizing document layout. Our results indicate that layout enrichment can improve the performance of purely text-based LLMs for document understanding by up to 15% compared to just using plain document text. In conclusion, this approach should be considered for the best model choice between text-based LLM or multi-modal document transformers.","sentences":["Recent advances in training large language models (LLMs) using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks.","Opposed to that there is a trend to train multi-modal transformer architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout.","This involves a separate fine-tuning step for which additional training data is required.","At present, no document transformers with comparable generalization to LLMs are available That raises the question which type of model is to be preferred for document understanding tasks.","In this paper we investigate the possibility to use purely text-based LLMs for document-specific tasks by using layout enrichment.","We explore drop-in modifications and rule-based methods to enrich purely textual LLM prompts with layout information.","In our experiments we investigate the effects on the commercial ChatGPT model and the open-source LLM Solar.","We demonstrate that using our approach both LLMs show improved performance on various standard document benchmarks.","In addition, we study the impact of noisy OCR and layout errors, as well as the limitations of LLMs when it comes to utilizing document layout.","Our results indicate that layout enrichment can improve the performance of purely text-based LLMs for document understanding by up to 15% compared to just using plain document text.","In conclusion, this approach should be considered for the best model choice between text-based LLM or multi-modal document transformers."],"url":"http://arxiv.org/abs/2402.09841v1","category":"cs.CL"}
{"created":"2024-02-15 10:00:34","title":"On the Chemical Composition of the Evolved Very Bright Metal-Poor Star HD 1936","abstract":"We present chemical abundances of the very bright metal-poor star HD~1936 based on high-resolution and high SNR spectra from AUKR. We obtain the abundances of 29 atomic species with atomic numbers between 3 and 63. In this context, the derived lithium abundance of 1.01 is consistent with the thin Li plateau observed in lower red giant branch stars. The star is a carbon-normal with the ratio of -0.31, just like other low-luminosity red giants on the thin Li plateau. We find the ratios of [Eu/Fe]=0.43 and [Ba/Eu]=-0.64, indicating very little s-process contamination. These ratios allow us to classify the star as a moderately r-process-enhanced (r-I) metal-poor star for the first time. It is worth mentioning that the star has a metallicity of -1.74, a [Cu/Fe] of -0.74, a [Zn/Fe] of 0.04, and a [Mg/C] of 0.69. The results suggest that it may be a second-generation star formed in a multi-enriched environment, rather than being a descendant of very massive first-generation stars. A last point worth mentioning is the possibility that HD~1936 may host a sub-stellar component with a mass of 18.35$M_{\\rm J}$. Although our study does not confirm or deny this, we briefly discuss the possibility of the star hosting a planet.","sentences":["We present chemical abundances of the very bright metal-poor star HD~1936 based on high-resolution and high SNR spectra from AUKR.","We obtain the abundances of 29 atomic species with atomic numbers between 3 and 63.","In this context, the derived lithium abundance of 1.01 is consistent with the thin Li plateau observed in lower red giant branch stars.","The star is a carbon-normal with the ratio of -0.31, just like other low-luminosity red giants on the thin Li plateau.","We find the ratios of [Eu/Fe]=0.43 and [Ba/Eu]=-0.64, indicating very little s-process contamination.","These ratios allow us to classify the star as a moderately r-process-enhanced (r-I) metal-poor star for the first time.","It is worth mentioning that the star has a metallicity of -1.74, a [Cu/Fe] of -0.74, a [Zn/Fe] of 0.04, and a [Mg/C] of 0.69.","The results suggest that it may be a second-generation star formed in a multi-enriched environment, rather than being a descendant of very massive first-generation stars.","A last point worth mentioning is the possibility that HD~1936 may host a sub-stellar component with a mass of 18.35$M_{\\rm J}$.","Although our study does not confirm or deny this, we briefly discuss the possibility of the star hosting a planet."],"url":"http://arxiv.org/abs/2402.09840v1","category":"astro-ph.SR"}
{"created":"2024-02-15 10:00:18","title":"Three-state $p$-SOS models on binary Cayley trees","abstract":"We consider a version of the solid-on-solid model on the Cayley tree of order two in which vertices carry spins of value $0,1$ or $2$ and the pairwise interaction of neighboring vertices is given by their spin difference to the power $p>0$. We exhibit all translation-invariant splitting Gibbs measures (TISGMs) of the model and demonstrate the existence of up to seven such measures, depending on the parameters. We further establish general conditions for extremality and non-extremality of TISGMs in the set of all Gibbs measures and use them to examine selected TISGMs for a small and a large $p$.   Notably, our analysis reveals that extremality properties are similar for large $p$ compared to the case $p=1$, a case that has been explored already in previous work. However, for the small $p$, certain measures that were consistently non-extremal for $p=1$ do exhibit transitions between extremality and non-extremality.","sentences":["We consider a version of the solid-on-solid model on the Cayley tree of order two in which vertices carry spins of value $0,1$ or $2$ and the pairwise interaction of neighboring vertices is given by their spin difference to the power $p>0$. We exhibit all translation-invariant splitting Gibbs measures (TISGMs) of the model and demonstrate the existence of up to seven such measures, depending on the parameters.","We further establish general conditions for extremality and non-extremality of TISGMs in the set of all Gibbs measures and use them to examine selected TISGMs for a small and a large $p$.   Notably, our analysis reveals that extremality properties are similar for large $p$ compared to the case $p=1$, a case that has been explored already in previous work.","However, for the small $p$, certain measures that were consistently non-extremal for $p=1$ do exhibit transitions between extremality and non-extremality."],"url":"http://arxiv.org/abs/2402.09839v1","category":"math-ph"}
{"created":"2024-02-15 10:00:13","title":"Performative Reinforcement Learning in Gradually Shifting Environments","abstract":"When Reinforcement Learning (RL) agents are deployed in practice, they might impact their environment and change its dynamics. Ongoing research attempts to formally model this phenomenon and to analyze learning algorithms in these models. To this end, we propose a framework where the current environment depends on the deployed policy as well as its previous dynamics. This is a generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy. We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR). We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment. Unlike previous approaches, MDRR combines samples from multiple deployments in its training. This makes MDRR particularly suitable for scenarios where the environment's response strongly depends on its previous dynamics, which are common in practice. We experimentally compare the algorithms using a simulation-based testbed and our results show that MDRR converges significantly faster than previous approaches.","sentences":["When Reinforcement Learning (RL) agents are deployed in practice, they might impact their environment and change its dynamics.","Ongoing research attempts to formally model this phenomenon and to analyze learning algorithms in these models.","To this end, we propose a framework where the current environment depends on the deployed policy as well as its previous dynamics.","This is a generalization of Performative RL (PRL)","[Mandal et al., 2023].","Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy.","We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR).","We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment.","Unlike previous approaches, MDRR combines samples from multiple deployments in its training.","This makes MDRR particularly suitable for scenarios where the environment's response strongly depends on its previous dynamics, which are common in practice.","We experimentally compare the algorithms using a simulation-based testbed and our results show that MDRR converges significantly faster than previous approaches."],"url":"http://arxiv.org/abs/2402.09838v1","category":"cs.LG"}
{"created":"2024-02-15 09:59:23","title":"Conjugacy properties of multivariate unified skew-elliptical distributions","abstract":"The broad class of multivariate unified skew-normal (SUN) distributions has been recently shown to possess fundamental conjugacy properties. When used as priors for the vector of parameters in general probit, tobit, and multinomial probit models, these distributions yield posteriors that still belong to the SUN family. Although such a core result has led to important advancements in Bayesian inference and computation, its applicability beyond likelihoods associated with fully-observed, discretized, or censored realizations from multivariate Gaussian models remains yet unexplored. This article covers such an important gap by proving that the wider family of multivariate unified skew-elliptical (SUE) distributions, which extends SUNs to more general perturbations of elliptical densities, guarantees conjugacy for broader classes of models, beyond those relying on fully-observed, discretized or censored Gaussians. Such a result leverages the closure under linear combinations, conditioning and marginalization of SUE to prove that such a family is conjugate to the likelihood induced by general multivariate regression models for fully-observed, censored or dichotomized realizations from skew-elliptical distributions. This advancement substantially enlarges the set of models that enable conjugate Bayesian inference to general formulations arising from elliptical and skew-elliptical families, including the multivariate Student's t and skew-t, among others.","sentences":["The broad class of multivariate unified skew-normal (SUN) distributions has been recently shown to possess fundamental conjugacy properties.","When used as priors for the vector of parameters in general probit, tobit, and multinomial probit models, these distributions yield posteriors that still belong to the SUN family.","Although such a core result has led to important advancements in Bayesian inference and computation, its applicability beyond likelihoods associated with fully-observed, discretized, or censored realizations from multivariate Gaussian models remains yet unexplored.","This article covers such an important gap by proving that the wider family of multivariate unified skew-elliptical (SUE) distributions, which extends SUNs to more general perturbations of elliptical densities, guarantees conjugacy for broader classes of models, beyond those relying on fully-observed, discretized or censored Gaussians.","Such a result leverages the closure under linear combinations, conditioning and marginalization of SUE to prove that such a family is conjugate to the likelihood induced by general multivariate regression models for fully-observed, censored or dichotomized realizations from skew-elliptical distributions.","This advancement substantially enlarges the set of models that enable conjugate Bayesian inference to general formulations arising from elliptical and skew-elliptical families, including the multivariate Student's t and skew-t, among others."],"url":"http://arxiv.org/abs/2402.09837v1","category":"stat.ME"}
{"created":"2024-02-15 09:58:23","title":"Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models","abstract":"Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control. However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models. Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions. They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness. Inspired by the emergent reasoning ability in LLMs, we propose a radical perspective shift that reformulates mobility generation as a commonsense reasoning problem. In this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour. Specifically, we design a context-aware chain-of-thoughts prompting technique to align LLMs with context-aware mobility behaviour by few-shot in-context learning. Besides, MobiGeaR employ a divide-and-coordinate mechanism to exploit the synergistic effect between LLM reasoning and mechanistic gravity model. It leverages the step-by-step LLM reasoning to recursively generate a temporal template of activity intentions, which are then mapped to physical locations with a mechanistic gravity model. Experiments on two real-world datasets show MobiGeaR achieves state-of-the-art performance across all metrics, and substantially reduces the size of training samples at the same time. Besides, MobiGeaR also significantly improves the semantic-awareness of mobility generation by improving the intention accuracy by 62.23% and the generated mobility data is proven effective in boosting the performance of downstream applications. The implementation of our approach is available in the paper.","sentences":["Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control.","However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models.","Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions.","They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness.","Inspired by the emergent reasoning ability in LLMs, we propose a radical perspective shift that reformulates mobility generation as a commonsense reasoning problem.","In this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour.","Specifically, we design a context-aware chain-of-thoughts prompting technique to align LLMs with context-aware mobility behaviour by few-shot in-context learning.","Besides, MobiGeaR employ a divide-and-coordinate mechanism to exploit the synergistic effect between LLM reasoning and mechanistic gravity model.","It leverages the step-by-step LLM reasoning to recursively generate a temporal template of activity intentions, which are then mapped to physical locations with a mechanistic gravity model.","Experiments on two real-world datasets show MobiGeaR achieves state-of-the-art performance across all metrics, and substantially reduces the size of training samples at the same time.","Besides, MobiGeaR also significantly improves the semantic-awareness of mobility generation by improving the intention accuracy by 62.23% and the generated mobility data is proven effective in boosting the performance of downstream applications.","The implementation of our approach is available in the paper."],"url":"http://arxiv.org/abs/2402.09836v1","category":"cs.AI"}
{"created":"2024-02-15 09:55:51","title":"Parameterized Algorithms for Steiner Forest in Bounded Width Graphs","abstract":"In this paper we reassess the parameterized complexity and approximability of the well-studied Steiner Forest problem in several graph classes of bounded width. The problem takes an edge-weighted graph and pairs of vertices as input, and the aim is to find a minimum cost subgraph in which each given vertex pair lies in the same connected component. It is known that this problem is APX-hard in general, and NP-hard on graphs of treewidth 3, treedepth 4, and feedback vertex set size 2. However, Bateni, Hajiaghayi and Marx [JACM, 2011] gave an approximation scheme with a runtime of $n^{O(\\frac{k^2}{\\varepsilon})}$ on graphs of treewidth $k$. Our main result is a much faster efficient parameterized approximation scheme (EPAS) with a runtime of $2^{O(\\frac{k^2}{\\varepsilon} \\log \\frac{k^2}{\\varepsilon})} \\cdot n^{O(1)}$. If $k$ instead is the vertex cover number of the input graph, we show how to compute the optimum solution in $2^{O(k \\log k)} \\cdot n^{O(1)}$ time, and we also prove that this runtime dependence on $k$ is asymptotically best possible, under ETH. Furthermore, if $k$ is the size of a feedback edge set, then we obtain a faster $2^{O(k)} \\cdot n^{O(1)}$ time algorithm, which again cannot be improved under ETH.","sentences":["In this paper we reassess the parameterized complexity and approximability of the well-studied Steiner Forest problem in several graph classes of bounded width.","The problem takes an edge-weighted graph and pairs of vertices as input, and the aim is to find a minimum cost subgraph in which each given vertex pair lies in the same connected component.","It is known that this problem is APX-hard in general, and NP-hard on graphs of treewidth 3, treedepth 4, and feedback vertex set size 2.","However, Bateni, Hajiaghayi and Marx","[JACM, 2011] gave an approximation scheme with a runtime of $n^{O(\\frac{k^2}{\\varepsilon})}$ on graphs of treewidth $k$.","Our main result is a much faster efficient parameterized approximation scheme (EPAS) with a runtime of $2^{O(\\frac{k^2}{\\varepsilon} \\log \\frac{k^2}{\\varepsilon})}","\\cdot n^{O(1)}$.","If $k$ instead is the vertex cover number of the input graph, we show how to compute the optimum solution in $2^{O(k \\log k)} \\cdot n^{O(1)}$ time, and we also prove that this runtime dependence on $k$ is asymptotically best possible, under ETH.","Furthermore, if $k$ is the size of a feedback edge set, then we obtain a faster $2^{O(k)} \\cdot n^{O(1)}$ time algorithm, which again cannot be improved under ETH."],"url":"http://arxiv.org/abs/2402.09835v1","category":"cs.DS"}
{"created":"2024-02-15 09:55:39","title":"All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining","abstract":"Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.","sentences":["Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP).","One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'.","This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions.","Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'.","However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer.","This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources.","In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning.","Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks.","Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach.","By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model."],"url":"http://arxiv.org/abs/2402.09834v1","category":"cs.LG"}
{"created":"2024-02-15 09:48:59","title":"Generic Fr{\u00e9}chet stationarity in constrained optimization","abstract":"Minimizing a smooth function f on a closed subset C leads to different notions of stationarity: Fr{\\'e}chet stationarity, which carries a strong variational meaning, and criticallity, which is defined through a closure process. The latter is an optimality condition which may loose the variational meaning of Fr{\\'e}chet stationarity in some settings. We show that, while criticality is the appropriate notion in full generality, Fr{\\'e}chet stationarity is typical in practical scenarios. This is illustrated with two main results, first we show that if C is semi-algebraic, then for a generic smooth semi-algebraic function f , all critical points of f on C are actually Fr{\\'e}chet stationary. Second we prove that for small step-sizes, all the accumulation points of the projected gradient algorithm are Fr{\\'e}chet stationary, with an explicit global quadratic estimate of the remainder, avoiding potential critical points which are not Fr{\\'e}chet stationary, and some bad local minima.","sentences":["Minimizing a smooth function f on a closed subset C leads to different notions of stationarity: Fr{\\'e}chet stationarity, which carries a strong variational meaning, and criticallity, which is defined through a closure process.","The latter is an optimality condition which may loose the variational meaning of Fr{\\'e}chet stationarity in some settings.","We show that, while criticality is the appropriate notion in full generality, Fr{\\'e}chet stationarity is typical in practical scenarios.","This is illustrated with two main results, first we show that if C is semi-algebraic, then for a generic smooth semi-algebraic function f , all critical points of f on C are actually Fr{\\'e}chet stationary.","Second we prove that for small step-sizes, all the accumulation points of the projected gradient algorithm are Fr{\\'e}chet stationary, with an explicit global quadratic estimate of the remainder, avoiding potential critical points which are not Fr{\\'e}chet stationary, and some bad local minima."],"url":"http://arxiv.org/abs/2402.09831v1","category":"math.OC"}
{"created":"2024-02-15 09:48:20","title":"Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data","abstract":"Anomaly detection is a critical challenge across various research domains, aiming to identify instances that deviate from normal data distributions. This paper explores the application of Generative Adversarial Networks (GANs) in fraud detection, comparing their advantages with traditional methods. GANs, a type of Artificial Neural Network (ANN), have shown promise in modeling complex data distributions, making them effective tools for anomaly detection. The paper systematically describes the principles of GANs and their derivative models, emphasizing their application in fraud detection across different datasets. And by building a collection of adversarial verification graphs, we will effectively prevent fraud caused by bots or automated systems and ensure that the users in the transaction are real. The objective of the experiment is to design and implement a fake face verification code and fraud detection system based on Generative Adversarial network (GANs) algorithm to enhance the security of the transaction process.The study demonstrates the potential of GANs in enhancing transaction security through deep learning techniques.","sentences":["Anomaly detection is a critical challenge across various research domains, aiming to identify instances that deviate from normal data distributions.","This paper explores the application of Generative Adversarial Networks (GANs) in fraud detection, comparing their advantages with traditional methods.","GANs, a type of Artificial Neural Network (ANN), have shown promise in modeling complex data distributions, making them effective tools for anomaly detection.","The paper systematically describes the principles of GANs and their derivative models, emphasizing their application in fraud detection across different datasets.","And by building a collection of adversarial verification graphs, we will effectively prevent fraud caused by bots or automated systems and ensure that the users in the transaction are real.","The objective of the experiment is to design and implement a fake face verification code and fraud detection system based on Generative Adversarial network (GANs) algorithm to enhance the security of the transaction process.","The study demonstrates the potential of GANs in enhancing transaction security through deep learning techniques."],"url":"http://arxiv.org/abs/2402.09830v1","category":"cs.LG"}
{"created":"2024-02-15 09:45:54","title":"Validation of homogenized finite element models of human metastatic vertebrae using digital volume correlation","abstract":"The incidence of vertebral fragility fracture is increased by the presence of preexisting pathologies such as metastatic disease. Computational tools could support the fracture prediction and consequently the decision of the best medical treatment. Anyway, validation is required to use these tools in clinical practice. To address this necessity, in this study subject-specific homogenized finite element models of single vertebrae were generated from micro CT images for both healthy and metastatic vertebrae and validated against experimental data. More in detail, spine segments were tested under compression and imaged with micro CT. The displacements field could be extracted for each vertebra singularly using the digital volume correlation full-field technique. Homogenized finite element models of each vertebra could hence be built from the micro CT images, applying boundary conditions consistent with the experimental displacements at the endplates. Numerical and experimental displacements and strains fields were eventually compared. In addition, the outcomes of a micro CT based homogenized model were compared to the ones of a clinical-CT based model. Good agreement between experimental and computational displacement fields, both for healthy and metastatic vertebrae, was found. Comparison between micro CT based and clinical-CT based outcomes showed strong correlations. Furthermore, models were able to qualitatively identify the regions which experimentally showed the highest strain concentration. In conclusion, the combination of experimental full-field technique and the in-silico modelling allowed the development of a promising pipeline for validation of fracture risk predictors, although further improvements in both fields are needed to better analyse quantitatively the post-yield behaviour of the vertebra.","sentences":["The incidence of vertebral fragility fracture is increased by the presence of preexisting pathologies such as metastatic disease.","Computational tools could support the fracture prediction and consequently the decision of the best medical treatment.","Anyway, validation is required to use these tools in clinical practice.","To address this necessity, in this study subject-specific homogenized finite element models of single vertebrae were generated from micro CT images for both healthy and metastatic vertebrae and validated against experimental data.","More in detail, spine segments were tested under compression and imaged with micro CT.","The displacements field could be extracted for each vertebra singularly using the digital volume correlation full-field technique.","Homogenized finite element models of each vertebra could hence be built from the micro CT images, applying boundary conditions consistent with the experimental displacements at the endplates.","Numerical and experimental displacements and strains fields were eventually compared.","In addition, the outcomes of a micro CT based homogenized model were compared to the ones of a clinical-CT based model.","Good agreement between experimental and computational displacement fields, both for healthy and metastatic vertebrae, was found.","Comparison between micro CT based and clinical-CT based outcomes showed strong correlations.","Furthermore, models were able to qualitatively identify the regions which experimentally showed the highest strain concentration.","In conclusion, the combination of experimental full-field technique and the in-silico modelling allowed the development of a promising pipeline for validation of fracture risk predictors, although further improvements in both fields are needed to better analyse quantitatively the post-yield behaviour of the vertebra."],"url":"http://arxiv.org/abs/2402.09828v1","category":"cs.CE"}
{"created":"2024-02-15 09:36:36","title":"Diffusion Models for Audio Restoration","abstract":"With the development of audio playback devices and fast data transmission, the demand for high sound quality is rising, for both entertainment and communications. In this quest for better sound quality, challenges emerge from distortions and interferences originating at the recording side or caused by an imperfect transmission pipeline. To address this problem, audio restoration methods aim to recover clean sound signals from the corrupted input data. We present here audio restoration algorithms based on diffusion models, with a focus on speech enhancement and music restoration tasks. Traditional approaches, often grounded in handcrafted rules and statistical heuristics, have shaped our understanding of audio signals. In the past decades, there has been a notable shift towards data-driven methods that exploit the modeling capabilities of deep neural networks (DNNs). Deep generative models, and among them diffusion models, have emerged as powerful techniques for learning complex data distributions. However, relying solely on DNN-based learning approaches carries the risk of reducing interpretability, particularly when employing end-to-end models. Nonetheless, data-driven approaches allow more flexibility in comparison to statistical model-based frameworks whose performance depends on distributional and statistical assumptions that can be difficult to guarantee. Here, we aim to show that diffusion models can combine the best of both worlds and offer the opportunity to design audio restoration algorithms with a good degree of interpretability and a remarkable performance in terms of sound quality.","sentences":["With the development of audio playback devices and fast data transmission, the demand for high sound quality is rising, for both entertainment and communications.","In this quest for better sound quality, challenges emerge from distortions and interferences originating at the recording side or caused by an imperfect transmission pipeline.","To address this problem, audio restoration methods aim to recover clean sound signals from the corrupted input data.","We present here audio restoration algorithms based on diffusion models, with a focus on speech enhancement and music restoration tasks.","Traditional approaches, often grounded in handcrafted rules and statistical heuristics, have shaped our understanding of audio signals.","In the past decades, there has been a notable shift towards data-driven methods that exploit the modeling capabilities of deep neural networks (DNNs).","Deep generative models, and among them diffusion models, have emerged as powerful techniques for learning complex data distributions.","However, relying solely on DNN-based learning approaches carries the risk of reducing interpretability, particularly when employing end-to-end models.","Nonetheless, data-driven approaches allow more flexibility in comparison to statistical model-based frameworks whose performance depends on distributional and statistical assumptions that can be difficult to guarantee.","Here, we aim to show that diffusion models can combine the best of both worlds and offer the opportunity to design audio restoration algorithms with a good degree of interpretability and a remarkable performance in terms of sound quality."],"url":"http://arxiv.org/abs/2402.09821v1","category":"eess.AS"}
{"created":"2024-02-15 09:35:57","title":"Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection","abstract":"In the age of the Internet, people's lives are increasingly dependent on today's network technology. However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges. Maintaining network security and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. In the field of network information security, the technical update of network attack and network protection is spiraling. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the original model to decline. With this in mind, this paper uses deep learning for advanced threat detection to improve cybersecurity resilienc e in the financial industry. Many network security researchers have shifted their focus to exceptio n-based intrusion detection techniques. The detection technology mainly uses statistical machine learning methods - collecting normal program and network behavior data, extracting multidimensional features, and training decision machine learning models on this basis (commonly used include naive Bayes, decision trees, support vector machines, random forests, etc.). In the detection phase, program code or network behavior that deviates from the normal value beyond the tolerance is considered malicious code or network attack behavior.","sentences":["In the age of the Internet, people's lives are increasingly dependent on today's network technology.","However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges.","Maintaining network security and protecting the legitimate interests of users is at the heart of network construction.","Threat detection is an important part of a complete and effective defense system.","In the field of network information security, the technical update of network attack and network protection is spiraling.","How to effectively detect unknown threats is one of the concerns of network protection.","Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the original model to decline.","With this in mind, this paper uses deep learning for advanced threat detection to improve cybersecurity resilienc e in the financial industry.","Many network security researchers have shifted their focus to exceptio n-based intrusion detection techniques.","The detection technology mainly uses statistical machine learning methods - collecting normal program and network behavior data, extracting multidimensional features, and training decision machine learning models on this basis (commonly used include naive Bayes, decision trees, support vector machines, random forests, etc.).","In the detection phase, program code or network behavior that deviates from the normal value beyond the tolerance is considered malicious code or network attack behavior."],"url":"http://arxiv.org/abs/2402.09820v1","category":"cs.CR"}
{"created":"2024-02-15 09:32:13","title":"On a Serrin-type overdetermined problem","abstract":"In this paper, we prove a Serrin-type result for an elliptic system of equations, overdetermined with both Dirichlet and a generalized Neumann conditions. With this tool, we characterize the critical shapes under volume constraint of some domain functionals.","sentences":["In this paper, we prove a Serrin-type result for an elliptic system of equations, overdetermined with both Dirichlet and a generalized Neumann conditions.","With this tool, we characterize the critical shapes under volume constraint of some domain functionals."],"url":"http://arxiv.org/abs/2402.09817v1","category":"math.AP"}
{"created":"2024-02-15 09:21:16","title":"DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization","abstract":"The objective of text-to-image (T2I) personalization is to customize a diffusion model to a user-provided reference concept, generating diverse images of the concept aligned with the target prompts. Conventional methods representing the reference concepts using unique text embeddings often fail to accurately mimic the appearance of the reference. To address this, one solution may be explicitly conditioning the reference images into the target denoising process, known as key-value replacement. However, prior works are constrained to local editing since they disrupt the structure path of the pre-trained T2I model. To overcome this, we propose a novel plug-in method, called DreamMatcher, which reformulates T2I personalization as semantic matching. Specifically, DreamMatcher replaces the target values with reference values aligned by semantic matching, while leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also introduce a semantic-consistent masking strategy to isolate the personalized concept from irrelevant regions introduced by the target prompts. Compatible with existing T2I models, DreamMatcher shows significant improvements in complex scenarios. Intensive analyses demonstrate the effectiveness of our approach.","sentences":["The objective of text-to-image (T2I) personalization is to customize a diffusion model to a user-provided reference concept, generating diverse images of the concept aligned with the target prompts.","Conventional methods representing the reference concepts using unique text embeddings often fail to accurately mimic the appearance of the reference.","To address this, one solution may be explicitly conditioning the reference images into the target denoising process, known as key-value replacement.","However, prior works are constrained to local editing since they disrupt the structure path of the pre-trained T2I model.","To overcome this, we propose a novel plug-in method, called DreamMatcher, which reformulates T2I personalization as semantic matching.","Specifically, DreamMatcher replaces the target values with reference values aligned by semantic matching, while leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures.","We also introduce a semantic-consistent masking strategy to isolate the personalized concept from irrelevant regions introduced by the target prompts.","Compatible with existing T2I models, DreamMatcher shows significant improvements in complex scenarios.","Intensive analyses demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2402.09812v1","category":"cs.CV"}
{"created":"2024-02-15 09:15:09","title":"Effective and Scalable Math Support: Evidence on the Impact of an AI- Tutor on Math Achievement in Ghana","abstract":"This study evaluates the impact of Rori, an AI powered conversational math tutor accessible via WhatsApp, on the math performance of approximately 1,000 students in grades 3-9 across 11 schools in Ghana. Each school was assigned to a treatment group or control group; the students in the control group continued their regular math instruction, while students in the treatment group engaged with Rori, for two 30-minute sessions per week over 8 months in addition to regular math instruction. We find that the math growth scores were substantially higher for the treatment group with an effect size of 0.37, and that the results were statistically significant (p < 0.001). The fact that Rori works with basic mobile devices on low-bandwidth data networks gives the intervention strong potential to support personalized learning on other low-and-middle-income countries (LMICs), where laptop ownership and high-speed internet - prerequisite for many video-centered learning platforms - remain extremely limited. While the results should be interpreted judiciously, as they only report on year 1 of the intervention, and future research is necessary to better understand which conditions are necessary for successful implementation, they do suggest that chat-based tutoring solutions leveraging artificial intelligence could offer a costeffective approach to enhancing learning outcomes for millions of students globally.","sentences":["This study evaluates the impact of Rori, an AI powered conversational math tutor accessible via WhatsApp, on the math performance of approximately 1,000 students in grades 3-9 across 11 schools in Ghana.","Each school was assigned to a treatment group or control group; the students in the control group continued their regular math instruction, while students in the treatment group engaged with Rori, for two 30-minute sessions per week over 8 months in addition to regular math instruction.","We find that the math growth scores were substantially higher for the treatment group with an effect size of 0.37, and that the results were statistically significant (p < 0.001).","The fact that Rori works with basic mobile devices on low-bandwidth data networks gives the intervention strong potential to support personalized learning on other low-and-middle-income countries (LMICs), where laptop ownership and high-speed internet - prerequisite for many video-centered learning platforms - remain extremely limited.","While the results should be interpreted judiciously, as they only report on year 1 of the intervention, and future research is necessary to better understand which conditions are necessary for successful implementation, they do suggest that chat-based tutoring solutions leveraging artificial intelligence could offer a costeffective approach to enhancing learning outcomes for millions of students globally."],"url":"http://arxiv.org/abs/2402.09809v1","category":"cs.HC"}
{"created":"2024-02-15 09:14:53","title":"Knowledge of Pretrained Language Models on Surface Information of Tokens","abstract":"Do pretrained language models have knowledge regarding the surface information of tokens? We examined the surface information stored in word or subword embeddings acquired by pretrained language models from the perspectives of token length, substrings, and token constitution. Additionally, we evaluated the ability of models to generate knowledge regarding token surfaces. We focused on 12 pretrained language models that were mainly trained on English and Japanese corpora. Experimental results demonstrate that pretrained language models have knowledge regarding token length and substrings but not token constitution. Additionally, the results imply that there is a bottleneck on the decoder side in terms of effectively utilizing acquired knowledge.","sentences":["Do pretrained language models have knowledge regarding the surface information of tokens?","We examined the surface information stored in word or subword embeddings acquired by pretrained language models from the perspectives of token length, substrings, and token constitution.","Additionally, we evaluated the ability of models to generate knowledge regarding token surfaces.","We focused on 12 pretrained language models that were mainly trained on English and Japanese corpora.","Experimental results demonstrate that pretrained language models have knowledge regarding token length and substrings but not token constitution.","Additionally, the results imply that there is a bottleneck on the decoder side in terms of effectively utilizing acquired knowledge."],"url":"http://arxiv.org/abs/2402.09808v1","category":"cs.CL"}
{"created":"2024-02-15 09:06:25","title":"Enabling Edge processing on LoRaWAN architecture","abstract":"LoRaWAN is a wireless technology that enables high-density deployments of IoT devices. Designed for Low Power Wide Area Networks (LPWAN), LoRaWAN employs large cells to service a potentially extremely high number of devices. The technology enforces a centralized architecture, directing all data generated by the devices to a single network server for data processing. End-to-end encryption is used to guarantee the confidentiality and security of data. In this demo, we present \\edgelora, a system architecture designed to incorporate edge processing in LoRaWAN without compromising security and confidentiality of data. \\edgelora maintains backward compatibility and addresses scalability issues arising from handling large amounts of data sourced from a diverse range of devices. The demo provides evidence on the advantages in terms of reduced latency, lower network bandwidth requirements, higher scalability, and improved security and privacy resulting from the application of the Edge processing paradigm to LoRaWAN.","sentences":["LoRaWAN is a wireless technology that enables high-density deployments of IoT devices.","Designed for Low Power Wide Area Networks (LPWAN), LoRaWAN employs large cells to service a potentially extremely high number of devices.","The technology enforces a centralized architecture, directing all data generated by the devices to a single network server for data processing.","End-to-end encryption is used to guarantee the confidentiality and security of data.","In this demo, we present \\edgelora, a system architecture designed to incorporate edge processing in LoRaWAN without compromising security and confidentiality of data.","\\edgelora maintains backward compatibility and addresses scalability issues arising from handling large amounts of data sourced from a diverse range of devices.","The demo provides evidence on the advantages in terms of reduced latency, lower network bandwidth requirements, higher scalability, and improved security and privacy resulting from the application of the Edge processing paradigm to LoRaWAN."],"url":"http://arxiv.org/abs/2402.09805v1","category":"cs.NI"}
{"created":"2024-02-15 09:03:11","title":"Coevolution of relationship and interaction in cooperative dynamical multiplex networks","abstract":"While actors in a population can interact with anyone else freely, social relations significantly influence our inclination towards particular individuals. The consequence of such interactions, however, may also form the intensity of our relations established earlier. These dynamical processes are captured via a coevolutionary model staged in multiplex networks with two distinct layers. In a so-called relationship layer the weights of edges among players may change in time as a consequence of games played in the alternative interaction layer. As an reasonable assumption, bilateral cooperation confirms while mutual defection weakens these weight factors. Importantly, the fitness of a player, which basically determines the success of a strategy imitation, depends not only on the payoff collected from interactions, but also on the individual relationship index calculated from the mentioned weight factors of related edges. Within the framework of weak prisoner's dilemma situation we explore the potential outcomes of the mentioned coevolutionary process where we assume different topologies for relationship layer. We find that higher average degree of the relationship graph is more beneficial to maintain cooperation in regular graphs, but the randomness of links could be a decisive factor in harsh situations. Surprisingly, a stronger coupling between relationship index and fitness discourage the evolution of cooperation by weakening the direct consequence of a strategy change. To complete our study we also monitor how the distribution of relationship index vary and detect a strong relation between its polarization and the general cooperation level.","sentences":["While actors in a population can interact with anyone else freely, social relations significantly influence our inclination towards particular individuals.","The consequence of such interactions, however, may also form the intensity of our relations established earlier.","These dynamical processes are captured via a coevolutionary model staged in multiplex networks with two distinct layers.","In a so-called relationship layer the weights of edges among players may change in time as a consequence of games played in the alternative interaction layer.","As an reasonable assumption, bilateral cooperation confirms while mutual defection weakens these weight factors.","Importantly, the fitness of a player, which basically determines the success of a strategy imitation, depends not only on the payoff collected from interactions, but also on the individual relationship index calculated from the mentioned weight factors of related edges.","Within the framework of weak prisoner's dilemma situation we explore the potential outcomes of the mentioned coevolutionary process where we assume different topologies for relationship layer.","We find that higher average degree of the relationship graph is more beneficial to maintain cooperation in regular graphs, but the randomness of links could be a decisive factor in harsh situations.","Surprisingly, a stronger coupling between relationship index and fitness discourage the evolution of cooperation by weakening the direct consequence of a strategy change.","To complete our study we also monitor how the distribution of relationship index vary and detect a strong relation between its polarization and the general cooperation level."],"url":"http://arxiv.org/abs/2402.09804v1","category":"physics.soc-ph"}
{"created":"2024-02-15 09:01:05","title":"An Inverse Problems Approach to Pulse Wave Analysis in the Human Brain","abstract":"Cardiac pulsations in the human brain have received recent interest due to their possible role in the pathogenesis of neurodegenerative diseases. Further interest stems from their possible application as an endogenous signal source that can be utilized for brain imaging in general. The (pulse-)wave describing the blood flow velocity along an intracranial artery consists of a forward (anterograde) and a backward (retrograde, reflected) part, but measurements of this wave usually consist of a superposition of these components. In this paper, we provide a mathematical framework for the inverse problem of estimating the pulse wave velocity, as well as the forward and backward component of the pulse wave separately from MRI measurements on the middle cerebral artery. After a mathematical analysis of this problem, we consider possible reconstruction approaches, and derive an alternate direction approach for its solution. The resulting methods provide estimates for anterograde/retrograde wave forms and the pulse wave velocity under specified assumptions on a cerebrovascular model system.","sentences":["Cardiac pulsations in the human brain have received recent interest due to their possible role in the pathogenesis of neurodegenerative diseases.","Further interest stems from their possible application as an endogenous signal source that can be utilized for brain imaging in general.","The (pulse-)wave describing the blood flow velocity along an intracranial artery consists of a forward (anterograde) and a backward (retrograde, reflected) part, but measurements of this wave usually consist of a superposition of these components.","In this paper, we provide a mathematical framework for the inverse problem of estimating the pulse wave velocity, as well as the forward and backward component of the pulse wave separately from MRI measurements on the middle cerebral artery.","After a mathematical analysis of this problem, we consider possible reconstruction approaches, and derive an alternate direction approach for its solution.","The resulting methods provide estimates for anterograde/retrograde wave forms and the pulse wave velocity under specified assumptions on a cerebrovascular model system."],"url":"http://arxiv.org/abs/2402.09803v1","category":"math.NA"}
{"created":"2024-02-15 08:58:03","title":"EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models","abstract":"Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.","sentences":["Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination.","To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text.","However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms.","To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data.","Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead.","Our code and datasets will be publicly available."],"url":"http://arxiv.org/abs/2402.09801v1","category":"cs.CL"}
{"created":"2024-02-15 08:52:31","title":"A cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings","abstract":"In this work, we propose a novel cross-talk rejection framework for a multi-channel multi-talker setup for a live multiparty interactive show. Our far-field audio setup is required to be hands-free during live interaction and comprises four adjacent talkers with directional microphones in the same space. Such setups often introduce heavy cross-talk between channels, resulting in reduced automatic speech recognition (ASR) and natural language understanding (NLU) performance. To address this problem, we propose voice activity detection (VAD) model for all talkers using multichannel information, which is then used to filter audio for downstream tasks. We adopt a synthetic training data generation approach through playback and re-recording for such scenarios, simulating challenging speech overlap conditions. We train our models on this synthetic data and demonstrate that our approach outperforms single-channel VAD models and energy-based multi-channel VAD algorithm in various acoustic environments. In addition to VAD results, we also present multiparty ASR evaluation results to highlight the impact of using our VAD model for filtering audio in downstream tasks by significantly reducing the insertion error.","sentences":["In this work, we propose a novel cross-talk rejection framework for a multi-channel multi-talker setup for a live multiparty interactive show.","Our far-field audio setup is required to be hands-free during live interaction and comprises four adjacent talkers with directional microphones in the same space.","Such setups often introduce heavy cross-talk between channels, resulting in reduced automatic speech recognition (ASR) and natural language understanding (NLU) performance.","To address this problem, we propose voice activity detection (VAD) model for all talkers using multichannel information, which is then used to filter audio for downstream tasks.","We adopt a synthetic training data generation approach through playback and re-recording for such scenarios, simulating challenging speech overlap conditions.","We train our models on this synthetic data and demonstrate that our approach outperforms single-channel VAD models and energy-based multi-channel VAD algorithm in various acoustic environments.","In addition to VAD results, we also present multiparty ASR evaluation results to highlight the impact of using our VAD model for filtering audio in downstream tasks by significantly reducing the insertion error."],"url":"http://arxiv.org/abs/2402.09797v1","category":"cs.SD"}
{"created":"2024-02-15 08:50:36","title":"Electron-Photon Exchange-Correlation Approximation for QEDFT","abstract":"Quantum-electrodynamical density-functional theory (QEDFT) provides a promising avenue for exploring complex light-matter interactions in optical cavities for real materials. Similar to conventional density-functional theory, the Kohn-Sham formulation of QEDFT needs approximations for the generally unknown exchange-correlation functional. In addition to the usual electron-electron exchange-correlation potential, an approximation for the electron-photon exchange-correlation potential is needed. A recent electron-photon exchange functional [C. Sch\\\"afer et al., Proc. Natl. Acad. Sci. USA, 118, e2110464118 (2021), https://www.pnas.org/doi/abs/10.1073/pnas.2110464118], derived from the equation of motion of the non-relativistic Pauli-Fierz Hamiltonian, shows robust performance in one-dimensional systems across weak- and strong-coupling regimes. Yet, its performance in reproducing electron densities in higher dimensions remains unexplored. Here we consider this QEDFT functional approximation from one to three-dimensional finite systems and across weak to strong light-matter couplings. The electron-photon exchange approximation provides excellent results in the ultra-strong-coupling regime. However, to ensure accuracy also in the weak-coupling regime across higher dimensions, we introduce a computationally efficient renormalization factor for the electron-photon exchange functional, which accounts for part of the electron-photon correlation contribution. These findings extend the applicability of photon-exchange-based functionals to realistic cavity-matter systems, fostering the field of cavity QED (quantum electrodynamics) materials engineering.","sentences":["Quantum-electrodynamical density-functional theory (QEDFT) provides a promising avenue for exploring complex light-matter interactions in optical cavities for real materials.","Similar to conventional density-functional theory, the Kohn-Sham formulation of QEDFT needs approximations for the generally unknown exchange-correlation functional.","In addition to the usual electron-electron exchange-correlation potential, an approximation for the electron-photon exchange-correlation potential is needed.","A recent electron-photon exchange functional [C. Sch\\\"afer et al., Proc.","Natl.","Acad.","Sci. USA, 118, e2110464118 (2021), https://www.pnas.org/doi/abs/10.1073/pnas.2110464118], derived from the equation of motion of the non-relativistic Pauli-Fierz Hamiltonian, shows robust performance in one-dimensional systems across weak- and strong-coupling regimes.","Yet, its performance in reproducing electron densities in higher dimensions remains unexplored.","Here we consider this QEDFT functional approximation from one to three-dimensional finite systems and across weak to strong light-matter couplings.","The electron-photon exchange approximation provides excellent results in the ultra-strong-coupling regime.","However, to ensure accuracy also in the weak-coupling regime across higher dimensions, we introduce a computationally efficient renormalization factor for the electron-photon exchange functional, which accounts for part of the electron-photon correlation contribution.","These findings extend the applicability of photon-exchange-based functionals to realistic cavity-matter systems, fostering the field of cavity QED (quantum electrodynamics) materials engineering."],"url":"http://arxiv.org/abs/2402.09794v1","category":"physics.comp-ph"}
{"created":"2024-02-15 08:50:36","title":"An advanced data fabric architecture leveraging homomorphic encryption and federated learning","abstract":"Data fabric is an automated and AI-driven data fusion approach to accomplish data management unification without moving data to a centralized location for solving complex data problems. In a Federated learning architecture, the global model is trained based on the learned parameters of several local models that eliminate the necessity of moving data to a centralized repository for machine learning. This paper introduces a secure approach for medical image analysis using federated learning and partially homomorphic encryption within a distributed data fabric architecture. With this method, multiple parties can collaborate in training a machine-learning model without exchanging raw data but using the learned or fused features. The approach complies with laws and regulations such as HIPAA and GDPR, ensuring the privacy and security of the data. The study demonstrates the method's effectiveness through a case study on pituitary tumor classification, achieving a significant level of accuracy. However, the primary focus of the study is on the development and evaluation of federated learning and partially homomorphic encryption as tools for secure medical image analysis. The results highlight the potential of these techniques to be applied to other privacy-sensitive domains and contribute to the growing body of research on secure and privacy-preserving machine learning.","sentences":["Data fabric is an automated and AI-driven data fusion approach to accomplish data management unification without moving data to a centralized location for solving complex data problems.","In a Federated learning architecture, the global model is trained based on the learned parameters of several local models that eliminate the necessity of moving data to a centralized repository for machine learning.","This paper introduces a secure approach for medical image analysis using federated learning and partially homomorphic encryption within a distributed data fabric architecture.","With this method, multiple parties can collaborate in training a machine-learning model without exchanging raw data but using the learned or fused features.","The approach complies with laws and regulations such as HIPAA and GDPR, ensuring the privacy and security of the data.","The study demonstrates the method's effectiveness through a case study on pituitary tumor classification, achieving a significant level of accuracy.","However, the primary focus of the study is on the development and evaluation of federated learning and partially homomorphic encryption as tools for secure medical image analysis.","The results highlight the potential of these techniques to be applied to other privacy-sensitive domains and contribute to the growing body of research on secure and privacy-preserving machine learning."],"url":"http://arxiv.org/abs/2402.09795v1","category":"cs.CR"}
{"created":"2024-02-15 08:47:35","title":"System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network","abstract":"Learning of deep neural networks (DNN) using Resistive Processing Unit (RPU) architecture is energy-efficient as it utilizes dedicated neuromorphic hardware and stochastic computation of weight updates for in-memory computing. Charge Trap Flash (CTF) devices can implement RPU-based weight updates in DNNs. However, prior work has shown that the weight updates (V_T) in CTF-based RPU are impacted by the non-ideal program time of CTF. The non-ideal program time is affected by two factors of CTF. Firstly, the effects of the number of input pulses (N) or pulse width (pw), and secondly, the gap between successive update pulses (t_gap) used for the stochastic computation of weight updates. Therefore, the impact of this non-ideal program time must be studied for neural network training simulations. In this study, Firstly, we propose a pulse-train design compensation technique to reduce the total error caused by non-ideal program time of CTF and stochastic variance of a network. Secondly, we simulate RPU-based DNN with non-ideal program time of CTF on MNIST and Fashion-MNIST datasets. We find that for larger N (~1000), learning performance approaches the ideal (software-level) training level and, therefore, is not much impacted by the choice of t_gap used to implement RPU-based weight updates. However, for lower N (<500), learning performance depends on T_gap of the pulses. Finally, we also performed an ablation study to isolate the causal factor of the improved learning performance. We conclude that the lower noise level in the weight updates is the most likely significant factor to improve the learning performance of DNN. Thus, our study attempts to compensate for the error caused by non-ideal program time and standardize the pulse length (N) and pulse gap (t_gap) specifications for CTF-based RPUs for accurate system-level on-chip training.","sentences":["Learning of deep neural networks (DNN) using Resistive Processing Unit (RPU) architecture is energy-efficient as it utilizes dedicated neuromorphic hardware and stochastic computation of weight updates for in-memory computing.","Charge Trap Flash (CTF) devices can implement RPU-based weight updates in DNNs.","However, prior work has shown that the weight updates (V_T) in CTF-based RPU are impacted by the non-ideal program time of CTF.","The non-ideal program time is affected by two factors of CTF.","Firstly, the effects of the number of input pulses (N) or pulse width (pw), and secondly, the gap between successive update pulses (t_gap) used for the stochastic computation of weight updates.","Therefore, the impact of this non-ideal program time must be studied for neural network training simulations.","In this study, Firstly, we propose a pulse-train design compensation technique to reduce the total error caused by non-ideal program time of CTF and stochastic variance of a network.","Secondly, we simulate RPU-based DNN with non-ideal program time of CTF on MNIST and Fashion-MNIST datasets.","We find that for larger N (~1000), learning performance approaches the ideal (software-level) training level and, therefore, is not much impacted by the choice of t_gap used to implement RPU-based weight updates.","However, for lower N (<500), learning performance depends on T_gap of the pulses.","Finally, we also performed an ablation study to isolate the causal factor of the improved learning performance.","We conclude that the lower noise level in the weight updates is the most likely significant factor to improve the learning performance of DNN.","Thus, our study attempts to compensate for the error caused by non-ideal program time and standardize the pulse length (N) and pulse gap (t_gap) specifications for CTF-based RPUs for accurate system-level on-chip training."],"url":"http://arxiv.org/abs/2402.09792v1","category":"cs.NE"}
{"created":"2024-02-15 08:43:17","title":"Multi-vertebral CT-based FE models implementing linear isotropic population-based material properties for the intervertebral discs cannot accurately predict strains","abstract":"Vertebral fractures prediction in clinics lacks of accuracy. The most used scores have limitations in distinguishing between subjects at risk or not. Finite element (FE) models generated from computed tomography (CT) of these patients may improve the predictive capability. Many models have already been proposed but the most of them considered the single vertebral body, excluding from the analysis the role of the inter-vertebral discs in the distribution of the load through the spine. Multi-vertebral models instead allow to examine more complex boundary condition. However, CT scans do not provide subject-specif information about the material properties of the disc. Consequently, the goal of the study was to validate a multi-vertebral FE model with subject specific modelling of the vertebral bone and population-based properties assigned to the disc, idealizing them with a linear isotropic material. Boundary condition were assigned in order to reproduce an experimental test performed on the same specimen and recorded using digital image correlation technique (DIC). FE and DIC strains on the vertebral surfaces are compared point-wise. Young's modulus values in the range 25-30 MPa allowed to achieve a comparable order of magnitude between experimental and computational data. However, the two distribution remained strongly different. To conclude, subject-specific material properties need to be assigned also to the discs as well as to the vertebrae to achieve acceptable accuracy in the assessment of the fracture risk.","sentences":["Vertebral fractures prediction in clinics lacks of accuracy.","The most used scores have limitations in distinguishing between subjects at risk or not.","Finite element (FE) models generated from computed tomography (CT) of these patients may improve the predictive capability.","Many models have already been proposed but the most of them considered the single vertebral body, excluding from the analysis the role of the inter-vertebral discs in the distribution of the load through the spine.","Multi-vertebral models instead allow to examine more complex boundary condition.","However, CT scans do not provide subject-specif information about the material properties of the disc.","Consequently, the goal of the study was to validate a multi-vertebral FE model with subject specific modelling of the vertebral bone and population-based properties assigned to the disc, idealizing them with a linear isotropic material.","Boundary condition were assigned in order to reproduce an experimental test performed on the same specimen and recorded using digital image correlation technique (DIC).","FE and DIC strains on the vertebral surfaces are compared point-wise.","Young's modulus values in the range 25-30 MPa allowed to achieve a comparable order of magnitude between experimental and computational data.","However, the two distribution remained strongly different.","To conclude, subject-specific material properties need to be assigned also to the discs as well as to the vertebrae to achieve acceptable accuracy in the assessment of the fracture risk."],"url":"http://arxiv.org/abs/2402.09790v1","category":"cs.CE"}
{"created":"2024-02-15 08:36:43","title":"An extension of sine-skewed circular distributions","abstract":"Sine-skewed circular distributions are identifiable and have easily-computable trigonometric moments and a simple random number generation algorithm, whereas they are known to have relatively low levels of asymmetry. This study proposes a new family of circular distributions that can be skewed more significantly than that of existing models. It is shown that a subfamily of the proposed distributions is identifiable with respect to parameters and all distributions in the subfamily have explicit trigonometric moments and a simple random number generation algorithm. The maximum likelihood estimation for model parameters is considered and its finite sample performances are investigated by numerical simulations. Some real data applications are illustrated for practical purposes.","sentences":["Sine-skewed circular distributions are identifiable and have easily-computable trigonometric moments and a simple random number generation algorithm, whereas they are known to have relatively low levels of asymmetry.","This study proposes a new family of circular distributions that can be skewed more significantly than that of existing models.","It is shown that a subfamily of the proposed distributions is identifiable with respect to parameters and all distributions in the subfamily have explicit trigonometric moments and a simple random number generation algorithm.","The maximum likelihood estimation for model parameters is considered and its finite sample performances are investigated by numerical simulations.","Some real data applications are illustrated for practical purposes."],"url":"http://arxiv.org/abs/2402.09788v1","category":"stat.ME"}
{"created":"2024-02-15 08:34:21","title":"Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model","abstract":"Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.","sentences":["Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces.","We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories.","We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology."],"url":"http://arxiv.org/abs/2402.09786v1","category":"cs.CV"}
{"created":"2024-02-15 08:33:16","title":"Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention","abstract":"Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a month. To address these gaps, we propose a sequential recommendation model called TemProxRec, which includes contrastive learning and self-attention methods to consider temporal proximities both across and within user-item interactions. The proposed contrastive learning method learns representations of items selected in close temporal periods across different users to be close. Simultaneously, the proposed self-attention mechanism encodes temporal and positional contexts in a user sequence using both absolute and relative embeddings. This way, our TemProxRec accurately predicts the relevant items based on the user-item interactions within a specific timeframe. We validate this work through comprehensive experiments on TemProxRec, consistently outperforming existing models on benchmark datasets as well as showing the significance of considering the vertical and horizontal temporal proximities into sequential recommendation.","sentences":["Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally.","Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored.","Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity.","These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions.","Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a month.","To address these gaps, we propose a sequential recommendation model called TemProxRec, which includes contrastive learning and self-attention methods to consider temporal proximities both across and within user-item interactions.","The proposed contrastive learning method learns representations of items selected in close temporal periods across different users to be close.","Simultaneously, the proposed self-attention mechanism encodes temporal and positional contexts in a user sequence using both absolute and relative embeddings.","This way, our TemProxRec accurately predicts the relevant items based on the user-item interactions within a specific timeframe.","We validate this work through comprehensive experiments on TemProxRec, consistently outperforming existing models on benchmark datasets as well as showing the significance of considering the vertical and horizontal temporal proximities into sequential recommendation."],"url":"http://arxiv.org/abs/2402.09784v1","category":"cs.IR"}
{"created":"2024-02-15 08:21:50","title":"MC-DBN: A Deep Belief Network-Based Model for Modality Completion","abstract":"Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate monitoring domains. Comprehensive experiments showcase the model's capacity to bridge the semantic divide present in multi-modal data, subsequently enhancing its performance. The source code is available at: https://github.com/logan-0623/DBN-generate","sentences":["Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring.","Utilizing diverse data sources can substantially improve prediction accuracy.","Nonetheless, additional data may not always align with the original dataset.","Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information.","Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN).","This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data.","It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model.","We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate monitoring domains.","Comprehensive experiments showcase the model's capacity to bridge the semantic divide present in multi-modal data, subsequently enhancing its performance.","The source code is available at: https://github.com/logan-0623/DBN-generate"],"url":"http://arxiv.org/abs/2402.09782v1","category":"cs.LG"}
{"created":"2024-02-15 08:06:34","title":"Switching waves-induced broadband Kerr frequency comb in fiber Fabry-Perot resonators","abstract":"We report the generation of broadband frequency combs in fiber Fabry-Perot resonators in the normal dispersion regime enabled by the excitation of switching waves. We theoretically characterise the process by means of a transverse linear stability analysis of the Lugiato-Lefever equation, enabling precise prediction of the switching waves' frequencies. Experimentally, we employed a pulsed-pump fiber Fabry-Perot resonator operating in the normal dispersion regime, integrated into an all-fiber experimental setup. The synchronisation mismatch and the influence of dispersion is thoroughly discussed, unveiling the potential to generate a frequency comb spanning over 15 THz bandwidth, specifically leveraging a flattened low dispersion cavity.","sentences":["We report the generation of broadband frequency combs in fiber Fabry-Perot resonators in the normal dispersion regime enabled by the excitation of switching waves.","We theoretically characterise the process by means of a transverse linear stability analysis of the Lugiato-Lefever equation, enabling precise prediction of the switching waves' frequencies.","Experimentally, we employed a pulsed-pump fiber Fabry-Perot resonator operating in the normal dispersion regime, integrated into an all-fiber experimental setup.","The synchronisation mismatch and the influence of dispersion is thoroughly discussed, unveiling the potential to generate a frequency comb spanning over 15 THz bandwidth, specifically leveraging a flattened low dispersion cavity."],"url":"http://arxiv.org/abs/2402.09777v1","category":"physics.optics"}
{"created":"2024-02-15 08:05:40","title":"Chemotactic particles as strong electrolytes: Debye-H\u00fcckel approximation and effective mobility law","abstract":"We consider a binary mixture of chemically active particles, that produce or consume solute molecules, and that interact with each other through the long-range concentrations fields they generate. We analytically calculate the effective phoretic mobility of these particles when the mixture is submitted to a constant, external concentration gradient, at leading order in the overall concentration. Relying on a analogy with the modeling of strong electrolytes, we show that the effective phoretic mobility decays with the square-root of the concentration: our result is therefore a nonequilibrium counterpart to the celebrated Kohlrausch and Debye-H\\\"uckel-Onsager conductivity laws for electrolytes, which are extended here to particles with long-range nonreciprocal interactions. The effective mobility law we derive reveals the existence of a regime of maximal mobility, and could find applications in the description of nanoscale transport phenomena in living cells.","sentences":["We consider a binary mixture of chemically active particles, that produce or consume solute molecules, and that interact with each other through the long-range concentrations fields they generate.","We analytically calculate the effective phoretic mobility of these particles when the mixture is submitted to a constant, external concentration gradient, at leading order in the overall concentration.","Relying on a analogy with the modeling of strong electrolytes, we show that the effective phoretic mobility decays with the square-root of the concentration: our result is therefore a nonequilibrium counterpart to the celebrated Kohlrausch and Debye-H\\\"uckel-Onsager conductivity laws for electrolytes, which are extended here to particles with long-range nonreciprocal interactions.","The effective mobility law we derive reveals the existence of a regime of maximal mobility, and could find applications in the description of nanoscale transport phenomena in living cells."],"url":"http://arxiv.org/abs/2402.09775v1","category":"cond-mat.soft"}
{"created":"2024-02-15 07:52:12","title":"Optimal Experimental Design for Partially Observable Pure Birth Processes","abstract":"We develop an efficient algorithm to find optimal observation times by maximizing the Fisher information for the birth rate of a partially observable pure birth process involving $n$ observations. Partially observable implies that at each of the $n$ observation time points for counting the number of individuals present in the pure birth process, each individual is observed independently with a fixed probability $p$, modeling detection difficulties or constraints on resources. We apply concepts and techniques from generating functions, using a combination of symbolic and numeric computation, to establish a recursion for evaluating and optimizing the Fisher information. Our numerical results reveal the efficacy of this new method. An implementation of the algorithm is available publicly.","sentences":["We develop an efficient algorithm to find optimal observation times by maximizing the Fisher information for the birth rate of a partially observable pure birth process involving $n$ observations.","Partially observable implies that at each of the $n$ observation time points for counting the number of individuals present in the pure birth process, each individual is observed independently with a fixed probability $p$, modeling detection difficulties or constraints on resources.","We apply concepts and techniques from generating functions, using a combination of symbolic and numeric computation, to establish a recursion for evaluating and optimizing the Fisher information.","Our numerical results reveal the efficacy of this new method.","An implementation of the algorithm is available publicly."],"url":"http://arxiv.org/abs/2402.09772v1","category":"math.ST"}
{"created":"2024-02-15 07:51:47","title":"Detailed analysis of the direct piezo-response of AlN nanowire-based vertically integrated nanogenerators","abstract":"In this work, detailed analysis of the direct piezo-response of AlN nanowire-based vertically integrated nanogenerators (VINGs) is undertaken as a function of mechanical excitation frequency. We demonstrate that the piezo-charge, piezo-voltage, and impedance of the devices can be directly correlated using an equivalent circuit model. The performance figure of merit (FoM) of nanowire-based VINGs, namely the piezoelectric voltage constant (g) for sensing, and the product gxd for energy harvesting, where d is a piezoelectric charge constant, are extracted from our presented results and compared with that of bulk single crystal GaN and quartz substrates, as well as sputtered AlN thin films. The measured FoM values suggest that the nanowires can outperform their rigid counterparts in terms of mechanical sensing applications, while further improvements are required to enhance their capability for energy generation. This work not only provides new experimental guidelines for understanding the direct piezo-characteristics of VINGs, but it also enables a quantitative comparison between nanostructured piezoelectric devices fabricated using different materials or architectures.","sentences":["In this work, detailed analysis of the direct piezo-response of AlN nanowire-based vertically integrated nanogenerators (VINGs) is undertaken as a function of mechanical excitation frequency.","We demonstrate that the piezo-charge, piezo-voltage, and impedance of the devices can be directly correlated using an equivalent circuit model.","The performance figure of merit (FoM) of nanowire-based VINGs, namely the piezoelectric voltage constant (g) for sensing, and the product gxd for energy harvesting, where d is a piezoelectric charge constant, are extracted from our presented results and compared with that of bulk single crystal GaN and quartz substrates, as well as sputtered AlN thin films.","The measured FoM values suggest that the nanowires can outperform their rigid counterparts in terms of mechanical sensing applications, while further improvements are required to enhance their capability for energy generation.","This work not only provides new experimental guidelines for understanding the direct piezo-characteristics of VINGs, but it also enables a quantitative comparison between nanostructured piezoelectric devices fabricated using different materials or architectures."],"url":"http://arxiv.org/abs/2402.09771v1","category":"physics.app-ph"}
{"created":"2024-02-15 07:47:10","title":"Representation Learning Using a Single Forward Pass","abstract":"We propose a neuroscience-inspired Solo Pass Embedded Learning Algorithm (SPELA). SPELA is a prime candidate for training and inference applications in Edge AI devices. At the same time, SPELA can optimally cater to the need for a framework to study perceptual representation learning and formation. SPELA has distinctive features such as neural priors (in the form of embedded vectors), no weight transport, no update locking of weights, complete local Hebbian learning, single forward pass with no storage of activations, and single weight update per sample. Juxtaposed with traditional approaches, SPELA operates without the need for backpropagation. We show that our algorithm can perform nonlinear classification on a noisy boolean operation dataset. Additionally, we exhibit high performance using SPELA across MNIST, KMNIST, and Fashion MNIST. Lastly, we show the few-shot and 1-epoch learning capabilities of SPELA on MNIST, KMNIST, and Fashion MNIST, where it consistently outperforms backpropagation.","sentences":["We propose a neuroscience-inspired Solo Pass Embedded Learning Algorithm (SPELA).","SPELA is a prime candidate for training and inference applications in Edge AI devices.","At the same time, SPELA can optimally cater to the need for a framework to study perceptual representation learning and formation.","SPELA has distinctive features such as neural priors (in the form of embedded vectors), no weight transport, no update locking of weights, complete local Hebbian learning, single forward pass with no storage of activations, and single weight update per sample.","Juxtaposed with traditional approaches, SPELA operates without the need for backpropagation.","We show that our algorithm can perform nonlinear classification on a noisy boolean operation dataset.","Additionally, we exhibit high performance using SPELA across MNIST, KMNIST, and Fashion MNIST.","Lastly, we show the few-shot and 1-epoch learning capabilities of SPELA on MNIST, KMNIST, and Fashion MNIST, where it consistently outperforms backpropagation."],"url":"http://arxiv.org/abs/2402.09769v1","category":"cs.AI"}
{"created":"2024-02-15 07:35:52","title":"From Variability to Stability: Advancing RecSys Benchmarking Practices","abstract":"In the rapidly evolving domain of Recommender Systems (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets. However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance. Addressing this deficiency, this paper introduces a novel benchmarking methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices. By utilizing a diverse set of $30$ open datasets, including two introduced in this work, and evaluating $11$ collaborative filtering algorithms across $9$ metrics, we critically examine the influence of dataset characteristics on algorithm performance. We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking. Through rigorous experimental analysis, we validate the reliability of our methodology under the variability of datasets, offering a benchmarking strategy that balances quality and computational demands. This methodology enables a fair yet effective means of evaluating RecSys algorithms, providing valuable guidance for future research endeavors.","sentences":["In the rapidly evolving domain of Recommender Systems (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets.","However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance.","Addressing this deficiency, this paper introduces a novel benchmarking methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices.","By utilizing a diverse set of $30$ open datasets, including two introduced in this work, and evaluating $11$ collaborative filtering algorithms across $9$ metrics, we critically examine the influence of dataset characteristics on algorithm performance.","We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking.","Through rigorous experimental analysis, we validate the reliability of our methodology under the variability of datasets, offering a benchmarking strategy that balances quality and computational demands.","This methodology enables a fair yet effective means of evaluating RecSys algorithms, providing valuable guidance for future research endeavors."],"url":"http://arxiv.org/abs/2402.09766v1","category":"cs.IR"}
{"created":"2024-02-15 07:35:29","title":"Reinforcement Learning for Solving Stochastic Vehicle Routing Problem with Time Windows","abstract":"This paper introduces a reinforcement learning approach to optimize the Stochastic Vehicle Routing Problem with Time Windows (SVRP), focusing on reducing travel costs in goods delivery. We develop a novel SVRP formulation that accounts for uncertain travel costs and demands, alongside specific customer time windows. An attention-based neural network trained through reinforcement learning is employed to minimize routing costs. Our approach addresses a gap in SVRP research, which traditionally relies on heuristic methods, by leveraging machine learning. The model outperforms the Ant-Colony Optimization algorithm, achieving a 1.73% reduction in travel costs. It uniquely integrates external information, demonstrating robustness in diverse environments, making it a valuable benchmark for future SVRP studies and industry application.","sentences":["This paper introduces a reinforcement learning approach to optimize the Stochastic Vehicle Routing Problem with Time Windows (SVRP), focusing on reducing travel costs in goods delivery.","We develop a novel SVRP formulation that accounts for uncertain travel costs and demands, alongside specific customer time windows.","An attention-based neural network trained through reinforcement learning is employed to minimize routing costs.","Our approach addresses a gap in SVRP research, which traditionally relies on heuristic methods, by leveraging machine learning.","The model outperforms the Ant-Colony Optimization algorithm, achieving a 1.73% reduction in travel costs.","It uniquely integrates external information, demonstrating robustness in diverse environments, making it a valuable benchmark for future SVRP studies and industry application."],"url":"http://arxiv.org/abs/2402.09765v1","category":"cs.AI"}
{"created":"2024-02-15 07:29:43","title":"Aligning Crowd Feedback via Distributional Preference Reward Modeling","abstract":"Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately. In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with a diverse set of human preferences. To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM policy to generate responses favoured by the population. Our experiments show that DPRM significantly enhances the alignment of LLMs with population preference, yielding more accurate, unbiased, and contextually appropriate responses.","sentences":["Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference.","However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals.","Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately.","In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with a diverse set of human preferences.","To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends.","On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution.","Finally, the expected reward is utilized to fine-tune an LLM policy to generate responses favoured by the population.","Our experiments show that DPRM significantly enhances the alignment of LLMs with population preference, yielding more accurate, unbiased, and contextually appropriate responses."],"url":"http://arxiv.org/abs/2402.09764v1","category":"cs.AI"}
{"created":"2024-02-15 07:22:04","title":"Grounding Language Model with Chunking-Free In-Context Retrieval","abstract":"This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval.   CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained. Our evaluations of CFIC on a range of open QA datasets demonstrate its superiority in retrieving relevant and accurate evidence, offering a significant improvement over traditional methods. By doing away with the need for document chunking, CFIC presents a more streamlined, effective, and efficient retrieval solution, making it a valuable advancement in the field of RAG systems.","sentences":["This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems.","Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content.","Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations.","These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval.   ","CFIC addresses these challenges by circumventing the conventional chunking process.","It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking.","CFIC is further enhanced by incorporating two decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding.","These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained.","Our evaluations of CFIC on a range of open QA datasets demonstrate its superiority in retrieving relevant and accurate evidence, offering a significant improvement over traditional methods.","By doing away with the need for document chunking, CFIC presents a more streamlined, effective, and efficient retrieval solution, making it a valuable advancement in the field of RAG systems."],"url":"http://arxiv.org/abs/2402.09760v1","category":"cs.CL"}
{"created":"2024-02-15 07:17:10","title":"Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish","abstract":"This study explores the potential of fine-tuning foundational English Large Language Models (LLMs) for generating Polish text. The first step involves Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB, consisting of 276 million Polish tokens. The LAPT is followed by additional fine-tuning aimed at solving nine KLEJ challenges. Our trained model Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02 among decoder-based Polish models but also closely rivals the performance of the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9 tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn Polish. The LAPT was completed in less than five days using a consumer GPU, highlighting the method's efficiency. The proficiency of the model in Polish was significantly enhanced, demonstrating the viability of this approach for adding new languages to existing LLMs by training just 1.2% of its parameters. To contribute to the community's collaborative progress, the model has been released as open-source.","sentences":["This study explores the potential of fine-tuning foundational English Large Language Models (LLMs) for generating Polish text.","The first step involves Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB, consisting of 276 million Polish tokens.","The LAPT is followed by additional fine-tuning aimed at solving nine KLEJ challenges.","Our trained model Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02 among decoder-based Polish models but also closely rivals the performance of the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9 tasks.","Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn Polish.","The LAPT was completed in less than five days using a consumer GPU, highlighting the method's efficiency.","The proficiency of the model in Polish was significantly enhanced, demonstrating the viability of this approach for adding new languages to existing LLMs by training just 1.2% of its parameters.","To contribute to the community's collaborative progress, the model has been released as open-source."],"url":"http://arxiv.org/abs/2402.09759v1","category":"cs.CL"}
{"created":"2024-02-15 07:11:10","title":"Construction of CCC and ZCCS Through Additive Characters Over Galois Field","abstract":"The rapid progression in wireless communication technologies, especially in multicarrier code-division multiple access (MC-CDMA), there is a need of advanced code construction methods. Traditional approaches, mainly based on generalized Boolean functions, have limitations in code length versatility. This paper introduces a novel approach to constructing complete complementary codes (CCC) and Z-complementary code sets (ZCCS), for reducing interference in MC-CDMA systems. The proposed construction, distinct from Boolean function-based approaches, employs additive characters over Galois fields GF($p^{r}$), where $p$ is prime and $r$ is a positive integer. First, we develop CCCs with lengths of $p^{r}$, which are then extended to construct ZCCS with both unreported lengths and sizes of $np^{r}$, where $n$ are arbitrary positive integers. The versatility of this method is further highlighted as it includes the lengths of ZCCS reported in prior studies as special cases, underscoring the method's comprehensive nature and superiority.","sentences":["The rapid progression in wireless communication technologies, especially in multicarrier code-division multiple access (MC-CDMA), there is a need of advanced code construction methods.","Traditional approaches, mainly based on generalized Boolean functions, have limitations in code length versatility.","This paper introduces a novel approach to constructing complete complementary codes (CCC) and Z-complementary code sets (ZCCS), for reducing interference in MC-CDMA systems.","The proposed construction, distinct from Boolean function-based approaches, employs additive characters over Galois fields GF($p^{r}$), where $p$ is prime and $r$ is a positive integer.","First, we develop CCCs with lengths of $p^{r}$, which are then extended to construct ZCCS with both unreported lengths and sizes of $np^{r}$, where $n$ are arbitrary positive integers.","The versatility of this method is further highlighted as it includes the lengths of ZCCS reported in prior studies as special cases, underscoring the method's comprehensive nature and superiority."],"url":"http://arxiv.org/abs/2402.09757v1","category":"cs.IT"}
{"created":"2024-02-15 18:19:45","title":"Universal Quantum Computing with Field-Mediated Unruh--DeWitt Qubits","abstract":"A set of universal quantum gates is a vital part of the theory of quantum computing, but is absent in the developing theory of Relativistic Quantum Information (RQI). Yet, the Unruh--DeWitt (UDW) detector formalism can be elevated to unitary gates between qubits and quantum fields and has allowed RQI applications in quantum Shannon theory, such as mutual information, coherent information, and quantum capacity in field-mediated quantum channels. Recently, experimental realizations of UDW-style qubits have been proposed in two-dimensional quantum materials, but their value as a quantum technology, including quantum communication and computation, is not yet clear, especially since fields introduce many avenues for decoherence. We introduce controlled-unitary UDW logic gates between qubit and field that are comparable to the two-qubit CNOT gate. We then extend this formalism to demonstrate Quantum State Transfer (QST) (two CNOT gates) and SWAP (three CNOT gates) channels. We illustrate the performance of these quantum operation gates with the diamond distance, a measure of distinguishability between quantum channels. Distinguishability measures like diamond distance allow for a rigorous comparison between field-mediated transduction through UDW detectors and local quantum mechanical operations and so quantify the performance of UDW detectors in quantum technological applications. Using the controlled-unitary qubit-field interactions we define an exact form of the CNOT gate. With this technique we also define quantum field-mediated single qubit operations associated with the Hadamard $H$, the $S$, and $T$ gates. Thus, UDW detectors in simple settings enable a collection of gates known to provide universal quantum computing.","sentences":["A set of universal quantum gates is a vital part of the theory of quantum computing, but is absent in the developing theory of Relativistic Quantum Information (RQI).","Yet, the Unruh--DeWitt (UDW) detector formalism can be elevated to unitary gates between qubits and quantum fields and has allowed RQI applications in quantum Shannon theory, such as mutual information, coherent information, and quantum capacity in field-mediated quantum channels.","Recently, experimental realizations of UDW-style qubits have been proposed in two-dimensional quantum materials, but their value as a quantum technology, including quantum communication and computation, is not yet clear, especially since fields introduce many avenues for decoherence.","We introduce controlled-unitary UDW logic gates between qubit and field that are comparable to the two-qubit CNOT gate.","We then extend this formalism to demonstrate Quantum State Transfer (QST) (two CNOT gates) and SWAP (three CNOT gates) channels.","We illustrate the performance of these quantum operation gates with the diamond distance, a measure of distinguishability between quantum channels.","Distinguishability measures like diamond distance allow for a rigorous comparison between field-mediated transduction through UDW detectors and local quantum mechanical operations and so quantify the performance of UDW detectors in quantum technological applications.","Using the controlled-unitary qubit-field interactions we define an exact form of the CNOT gate.","With this technique we also define quantum field-mediated single qubit operations associated with the Hadamard $H$, the $S$, and $T$ gates.","Thus, UDW detectors in simple settings enable a collection of gates known to provide universal quantum computing."],"url":"http://arxiv.org/abs/2402.10173v1","category":"quant-ph"}
{"created":"2024-02-15 18:08:58","title":"Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks","abstract":"Traveling waves are a fundamental phenomenon in the brain, playing a crucial role in short-term information storage. In this study, we leverage the concept of traveling wave dynamics within a neural lattice to formulate a theoretical model of neural working memory, study its properties, and its real world implications in AI. The proposed model diverges from traditional approaches, which assume information storage in static, register-like locations updated by interference. Instead, the model stores data as waves that is updated by the wave's boundary conditions. We rigorously examine the model's capabilities in representing and learning state histories, which are vital for learning history-dependent dynamical systems. The findings reveal that the model reliably stores external information and enhances the learning process by addressing the diminishing gradient problem. To understand the model's real-world applicability, we explore two cases: linear boundary condition and non-linear, self-attention-driven boundary condition. The experiments reveal that the linear scenario is effectively learned by Recurrent Neural Networks (RNNs) through backpropagation when modeling history-dependent dynamical systems. Conversely, the non-linear scenario parallels the autoregressive loop of an attention-only transformer. Collectively, our findings suggest the broader relevance of traveling waves in AI and its potential in advancing neural network architectures.","sentences":["Traveling waves are a fundamental phenomenon in the brain, playing a crucial role in short-term information storage.","In this study, we leverage the concept of traveling wave dynamics within a neural lattice to formulate a theoretical model of neural working memory, study its properties, and its real world implications in AI.","The proposed model diverges from traditional approaches, which assume information storage in static, register-like locations updated by interference.","Instead, the model stores data as waves that is updated by the wave's boundary conditions.","We rigorously examine the model's capabilities in representing and learning state histories, which are vital for learning history-dependent dynamical systems.","The findings reveal that the model reliably stores external information and enhances the learning process by addressing the diminishing gradient problem.","To understand the model's real-world applicability, we explore two cases: linear boundary condition and non-linear, self-attention-driven boundary condition.","The experiments reveal that the linear scenario is effectively learned by Recurrent Neural Networks (RNNs) through backpropagation when modeling history-dependent dynamical systems.","Conversely, the non-linear scenario parallels the autoregressive loop of an attention-only transformer.","Collectively, our findings suggest the broader relevance of traveling waves in AI and its potential in advancing neural network architectures."],"url":"http://arxiv.org/abs/2402.10163v1","category":"cs.NE"}
{"created":"2024-02-15 17:14:12","title":"Collision efficiency of droplets across diffusive, electrostatic and inertial regimes","abstract":"Rain drops form in clouds by collision of submillimetric droplets falling under gravity: larger drops fall faster than smaller ones and collect them on their path. The puzzling stability of fogs and non-precipitating warm clouds with respect to this avalanching mechanism has been a longstanding problem. How to explain that droplets of diameter around $10~{\\rm \\mu m}$ have a low probability of collision, inhibiting the cascade towards larger and larger drops? Here we review the dynamical mechanisms that have been proposed in the literature and quantitatively investigate the frequency of drop collisions induced by Brownian diffusion, electrostatics and gravity, using an open-source Monte-Carlo code that takes all of them into account. Inertia dominates over aerodynamic forces for large drops, when the Stokes number is larger than $1$. Thermal diffusion dominates over aerodynamic forces for small drops, when the P\\'eclet number is smaller than $1$. We show that there exists a range of size (typically $1-10~{\\rm \\mu m}$ for water drops in air) for which neither inertia nor Brownian diffusion are significant, leading to a gap in the collision rate. The effect is particularly important, due to the lubrication film forming between the drops immediately before collision, and secondarily to the long-range aerodynamic interaction. Two different mechanisms regularise the divergence of the lubrication force at vanishing gap: the transition to a noncontinuum regime in the lubrication film, when the gap is comparable to the mean free path of air, and the induction of a flow inside the drops due to shear at their surfaces. In the gap between inertia-dominated and diffusion-dominated regimes, dipole-dipole electrostatic interactions becomes the major effect controlling the efficiency of drop collisions.","sentences":["Rain drops form in clouds by collision of submillimetric droplets falling under gravity: larger drops fall faster than smaller ones and collect them on their path.","The puzzling stability of fogs and non-precipitating warm clouds with respect to this avalanching mechanism has been a longstanding problem.","How to explain that droplets of diameter around $10~{\\rm \\mu m}$ have a low probability of collision, inhibiting the cascade towards larger and larger drops?","Here we review the dynamical mechanisms that have been proposed in the literature and quantitatively investigate the frequency of drop collisions induced by Brownian diffusion, electrostatics and gravity, using an open-source Monte-Carlo code that takes all of them into account.","Inertia dominates over aerodynamic forces for large drops, when the Stokes number is larger than $1$. Thermal diffusion dominates over aerodynamic forces for small drops, when the P\\'eclet number is smaller than $1$. We show that there exists a range of size (typically $1-10~{\\rm \\mu m}$ for water drops in air) for which neither inertia nor Brownian diffusion are significant, leading to a gap in the collision rate.","The effect is particularly important, due to the lubrication film forming between the drops immediately before collision, and secondarily to the long-range aerodynamic interaction.","Two different mechanisms regularise the divergence of the lubrication force at vanishing gap: the transition to a noncontinuum regime in the lubrication film, when the gap is comparable to the mean free path of air, and the induction of a flow inside the drops due to shear at their surfaces.","In the gap between inertia-dominated and diffusion-dominated regimes, dipole-dipole electrostatic interactions becomes the major effect controlling the efficiency of drop collisions."],"url":"http://arxiv.org/abs/2402.10117v1","category":"physics.flu-dyn"}
{"created":"2024-02-15 16:07:35","title":"On the Domain Generalizability of RF Fingerprints Through Multifractal Dimension Representation","abstract":"RF data-driven device fingerprinting through the use of deep learning has recently surfaced as a possible method for enabling secure device identification and authentication. Traditional approaches are commonly susceptible to the domain adaptation problem where a model trained on data collected under one domain performs badly when tested on data collected under a different domain. Some examples of a domain change include varying the location or environment of the device and varying the time or day of the data collection. In this work, we propose using multifractal analysis and the variance fractal dimension trajectory (VFDT) as a data representation input to the deep neural network to extract device fingerprints that are domain generalizable. We analyze the effectiveness of the proposed VFDT representation in detecting device-specific signatures from hardware-impaired IQ (in-phase and quadrature) signals, and we evaluate its robustness in real-world settings, using an experimental testbed of 30 WiFi-enabled Pycom devices. Our experimental results show that the proposed VFDT representation improves the scalability, robustness and generalizability of the deep learning models significantly compared to when using IQ data samples.","sentences":["RF data-driven device fingerprinting through the use of deep learning has recently surfaced as a possible method for enabling secure device identification and authentication.","Traditional approaches are commonly susceptible to the domain adaptation problem where a model trained on data collected under one domain performs badly when tested on data collected under a different domain.","Some examples of a domain change include varying the location or environment of the device and varying the time or day of the data collection.","In this work, we propose using multifractal analysis and the variance fractal dimension trajectory (VFDT) as a data representation input to the deep neural network to extract device fingerprints that are domain generalizable.","We analyze the effectiveness of the proposed VFDT representation in detecting device-specific signatures from hardware-impaired IQ (in-phase and quadrature) signals, and we evaluate its robustness in real-world settings, using an experimental testbed of 30 WiFi-enabled Pycom devices.","Our experimental results show that the proposed VFDT representation improves the scalability, robustness and generalizability of the deep learning models significantly compared to when using IQ data samples."],"url":"http://arxiv.org/abs/2402.10044v1","category":"cs.CR"}
{"created":"2024-02-15 15:39:54","title":"Reproducing, Extending, and Analyzing Naming Experiments","abstract":"Naming is very important in software development, as names are often the only vehicle of meaning about what the code is intended to do. A recent study on how developers choose names collected the names given by different developers for the same objects. This enabled a study of these names' diversity and structure, and the construction of a model of how names are created. We reproduce different parts of this study in three independent experiments. Importantly, we employ methodological variations rather than striving of an exact replication. When the same results are obtained this then boosts our confidence in their validity by demonstrating that they do not depend on the methodology.   Our results indeed corroborate those of the original study in terms of the diversity of names, the low probability of two developers choosing the same name, and the finding that experienced developers tend to use slightly longer names than inexperienced students. We explain name diversity by performing a new analysis of the names, classifying the concepts represented in them as universal (agreed upon), alternative (reflecting divergent views on a topic), or optional (reflecting divergent opinions on whether to include this concept at all). This classification enables new research directions concerning the considerations involved in naming decisions. We also show that explicitly using the model proposed in the original study to guide naming leads to the creation of better names, whereas the simpler approach of just asking participants to use longer and more detailed names does not.","sentences":["Naming is very important in software development, as names are often the only vehicle of meaning about what the code is intended to do.","A recent study on how developers choose names collected the names given by different developers for the same objects.","This enabled a study of these names' diversity and structure, and the construction of a model of how names are created.","We reproduce different parts of this study in three independent experiments.","Importantly, we employ methodological variations rather than striving of an exact replication.","When the same results are obtained this then boosts our confidence in their validity by demonstrating that they do not depend on the methodology.   ","Our results indeed corroborate those of the original study in terms of the diversity of names, the low probability of two developers choosing the same name, and the finding that experienced developers tend to use slightly longer names than inexperienced students.","We explain name diversity by performing a new analysis of the names, classifying the concepts represented in them as universal (agreed upon), alternative (reflecting divergent views on a topic), or optional (reflecting divergent opinions on whether to include this concept at all).","This classification enables new research directions concerning the considerations involved in naming decisions.","We also show that explicitly using the model proposed in the original study to guide naming leads to the creation of better names, whereas the simpler approach of just asking participants to use longer and more detailed names does not."],"url":"http://arxiv.org/abs/2402.10022v1","category":"cs.SE"}
{"created":"2024-02-15 15:19:49","title":"Countably Colorful Hyperplane Transversal","abstract":"Let $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ be an infinite sequence of families of compact connected sets in $\\mathbb{R}^{d}$. An infinite sequence of compact connected sets $\\left\\{ B_{n} \\right\\}_{n\\in \\mathbb{N}}$ is called heterochromatic sequence from $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ if there exists an infinite sequence $\\left\\{ i_{n} \\right\\}_{n\\in \\mathbb{N}}$ of natural numbers satisfying the following two properties: (a) $\\{i_{n}\\}_{n\\in \\mathbb{N}}$ is a monotonically increasing sequence, and (b) for all $n \\in \\mathbb{N}$, we have $B_{n} \\in \\mathcal{F}_{i_n}$. We show that if every heterochromatic sequence from $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ contains $d+1$ sets that can be pierced by a single hyperplane then there exists a finite collection $\\mathcal{H}$ of hyperplanes from $\\mathbb{R}^{d}$ that pierces all but finitely many families from $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$. As a direct consequence of our result, we get that if every countable subcollection from an infinite family $\\mathcal{F}$ of compact connected sets in $\\mathbb{R}^{d}$ contains $d+1$ sets that can be pierced by a single hyperplane then $\\mathcal{F}$ can be pierced by finitely many hyperplanes. To establish the optimality of our result we show that, for all $d \\in \\mathbb{N}$, there exists an infinite sequence $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ of families of compact connected sets satisfying the following two conditions: (1) for all $n \\in \\mathbb{N}$, $\\mathcal{F}_{n}$ is not pierceable by finitely many hyperplanes, and (2) for any $m \\in \\mathbb{N}$ and every sequence $\\left\\{B_n\\right\\}_{n=m}^{\\infty}$ of compact connected sets in $\\mathbb{R}^d$, where $B_i\\in\\mathcal{F}_i$ for all $i \\geq m$, there exists a hyperplane in $\\mathbb{R}^d$ that pierces at least $d+1$ sets in the sequence.","sentences":["Let $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ be an infinite sequence of families of compact connected sets in $\\mathbb{R}^{d}$. An infinite sequence of compact connected sets $\\left\\{ B_{n} \\right\\}_{n\\in \\mathbb{N}}$ is called heterochromatic sequence from $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ if there exists an infinite sequence $\\left\\{ i_{n} \\right\\}_{n\\in \\mathbb{N}}$ of natural numbers satisfying the following two properties: (a) $\\{i_{n}\\}_{n\\in \\mathbb{N}}$ is a monotonically increasing sequence, and (b) for all $n \\in \\mathbb{N}$, we have $B_{n} \\in \\mathcal{F}_{i_n}$.","We show that if every heterochromatic sequence from $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ contains $d+1$ sets that can be pierced by a single hyperplane then there exists a finite collection $\\mathcal{H}$ of hyperplanes from $\\mathbb{R}^{d}$ that pierces all but finitely many families from $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$. As a direct consequence of our result, we get that if every countable subcollection from an infinite family $\\mathcal{F}$ of compact connected sets in $\\mathbb{R}^{d}$ contains $d+1$ sets that can be pierced by a single hyperplane then $\\mathcal{F}$ can be pierced by finitely many hyperplanes.","To establish the optimality of our result we show that, for all $d \\in \\mathbb{N}$, there exists an infinite sequence $\\left\\{ \\mathcal{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ of families of compact connected sets satisfying the following two conditions: (1) for all $n \\in \\mathbb{N}$, $\\mathcal{F}_{n}$ is not pierceable by finitely many hyperplanes, and (2) for any $m \\in \\mathbb{N}$ and every sequence $\\left\\{B_n\\right\\}_{n=m}^{\\infty}$ of compact connected sets in $\\mathbb{R}^d$, where $B_i\\in\\mathcal{F}_i$ for all $i \\geq m$, there exists a hyperplane in $\\mathbb{R}^d$ that pierces at least $d+1$ sets in the sequence."],"url":"http://arxiv.org/abs/2402.10012v1","category":"math.CO"}
{"created":"2024-02-15 15:15:58","title":"The challenge of identifying INTEGRAL sources on the Galactic plane","abstract":"The International Gamma-ray Astrophysics Laboratory (INTEGRAL) has been surveying the sky above 20 keV since its launch in 2002 providing new insights into the nature of the sources that populate our Universe at soft gamma-ray energies. The latest IBIS/ISGRI survey lists 929 hard X-ray sources, of which 113 are reported as unidentified, i.e. lacking a lower energy counterpart or simply not studied in other wavebands. To overcome this lack of information, we either browsed the X-ray archives, or, if no data in the X-ray band were available, we requested Target of Opportunity (ToO) observations with the X-ray Telescope (XRT) on-board the Neil Gehrels Swift Observatory. Following this approach, we selected a sample of 10 objects for which X-ray data were key to investigate their nature. We found a single X-ray association for all of the sources, except for IGR J16267-3303, for which two X-ray detections were spotted within the IBIS positional uncertainty. We then browsed multi-waveband archives to search for counterparts to these X-ray detections at other wavelengths and analysed X-ray spectral properties to determine their nature and association with the high-energy emitter. As a result of our analysis, we identified the most likely counterpart for 7 sources, although in some cases its nature/class could not be definitely assessed on the basis of the information collected. Interestingly, SWIFT J2221.6+5952, first reported in the 105-month Swift/Burst Alert Telescope (BAT) survey, is the only source of the sample for which we did not find any counterpart at radio/optical/IR wavebands. Finally, we found that two IBIS source, IGR J17449-3037 and IGR J17596-2315 are positionally associated with a Fermi Large Area Telescope (LAT) object.","sentences":["The International Gamma-ray Astrophysics Laboratory (INTEGRAL) has been surveying the sky above 20 keV since its launch in 2002 providing new insights into the nature of the sources that populate our Universe at soft gamma-ray energies.","The latest IBIS/ISGRI survey lists 929 hard X-ray sources, of which 113 are reported as unidentified, i.e. lacking a lower energy counterpart or simply not studied in other wavebands.","To overcome this lack of information, we either browsed the X-ray archives, or, if no data in the X-ray band were available, we requested Target of Opportunity (ToO) observations with the X-ray Telescope (XRT) on-board the Neil Gehrels Swift Observatory.","Following this approach, we selected a sample of 10 objects for which X-ray data were key to investigate their nature.","We found a single X-ray association for all of the sources, except for IGR J16267-3303, for which two X-ray detections were spotted within the IBIS positional uncertainty.","We then browsed multi-waveband archives to search for counterparts to these X-ray detections at other wavelengths and analysed X-ray spectral properties to determine their nature and association with the high-energy emitter.","As a result of our analysis, we identified the most likely counterpart for 7 sources, although in some cases its nature/class could not be definitely assessed on the basis of the information collected.","Interestingly, SWIFT J2221.6+5952, first reported in the 105-month Swift/Burst Alert Telescope (BAT) survey, is the only source of the sample for which we did not find any counterpart at radio/optical/IR wavebands.","Finally, we found that two IBIS source, IGR J17449-3037 and IGR J17596-2315 are positionally associated with a Fermi Large Area Telescope (LAT) object."],"url":"http://arxiv.org/abs/2402.10007v1","category":"astro-ph.HE"}
{"created":"2024-02-15 14:16:59","title":"ViGEO: an Assessment of Vision GNNs in Earth Observation","abstract":"Satellite missions and Earth Observation (EO) systems represent fundamental assets for environmental monitoring and the timely identification of catastrophic events, long-term monitoring of both natural resources and human-made assets, such as vegetation, water bodies, forests as well as buildings. Different EO missions enables the collection of information on several spectral bandwidths, such as MODIS, Sentinel-1 and Sentinel-2. Thus, given the recent advances of machine learning, computer vision and the availability of labeled data, researchers demonstrated the feasibility and the precision of land-use monitoring systems and remote sensing image classification through the use of deep neural networks. Such systems may help domain experts and governments in constant environmental monitoring, enabling timely intervention in case of catastrophic events (e.g., forest wildfire in a remote area). Despite the recent advances in the field of computer vision, many works limit their analysis on Convolutional Neural Networks (CNNs) and, more recently, to vision transformers (ViTs). Given the recent successes of Graph Neural Networks (GNNs) on non-graph data, such as time-series and images, we investigate the performances of a recent Vision GNN architecture (ViG) applied to the task of land cover classification. The experimental results show that ViG achieves state-of-the-art performances in multiclass and multilabel classification contexts, surpassing both ViT and ResNet on large-scale benchmarks.","sentences":["Satellite missions and Earth Observation (EO) systems represent fundamental assets for environmental monitoring and the timely identification of catastrophic events, long-term monitoring of both natural resources and human-made assets, such as vegetation, water bodies, forests as well as buildings.","Different EO missions enables the collection of information on several spectral bandwidths, such as MODIS, Sentinel-1 and Sentinel-2.","Thus, given the recent advances of machine learning, computer vision and the availability of labeled data, researchers demonstrated the feasibility and the precision of land-use monitoring systems and remote sensing image classification through the use of deep neural networks.","Such systems may help domain experts and governments in constant environmental monitoring, enabling timely intervention in case of catastrophic events (e.g., forest wildfire in a remote area).","Despite the recent advances in the field of computer vision, many works limit their analysis on Convolutional Neural Networks (CNNs) and, more recently, to vision transformers (ViTs).","Given the recent successes of Graph Neural Networks (GNNs) on non-graph data, such as time-series and images, we investigate the performances of a recent Vision GNN architecture (ViG) applied to the task of land cover classification.","The experimental results show that ViG achieves state-of-the-art performances in multiclass and multilabel classification contexts, surpassing both ViT and ResNet on large-scale benchmarks."],"url":"http://arxiv.org/abs/2402.09962v1","category":"cs.CV"}
{"created":"2024-02-15 13:28:10","title":"Search for fractionally charged particles in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search is presented for fractionally charged particles with charge below 1$e$, using their small energy loss in the tracking detector as a key variable to observe a signal. The analyzed data set corresponds to an integrated luminosity of 138 fb$^{-1}$ of proton-proton collisions collected at $\\sqrt{s}$ = 13 TeV in 2016-2018 at the CERN LHC. This is the first search at the LHC for new particles with charges between $e/$3 and $e$. Masses up to 640 GeV and charges as low as $e/$3 are excluded at 95% confidence level. These are the most stringent limits to date for the considered Drell-Yan-like production mode.","sentences":["A search is presented for fractionally charged particles with charge below 1$e$, using their small energy loss in the tracking detector as a key variable to observe a signal.","The analyzed data set corresponds to an integrated luminosity of 138 fb$^{-1}$ of proton-proton collisions collected at $\\sqrt{s}$ = 13 TeV in 2016-2018 at the CERN LHC.","This is the first search at the LHC for new particles with charges between $e/$3 and $e$. Masses up to 640 GeV and charges as low as $e/$3 are excluded at 95% confidence level.","These are the most stringent limits to date for the considered Drell-Yan-like production mode."],"url":"http://arxiv.org/abs/2402.09932v1","category":"hep-ex"}
{"created":"2024-02-15 13:11:07","title":"Characterizing Role Models in Software Practitioners' Career: An Interview Study","abstract":"A role model is a person who serves as an example for others to follow, especially in terms of values, behavior, achievements, and personal characteristics. In this paper, authors study how role models influence software practitioners careers, an aspect not studied in the literature before. By means of this study, authors aim to understand if there are any salient role model archetypes and what characteristics are valued by participants in their role models. To do so, authors use a thematic coding approach to analyze the data collected from interviewing ten Latin American software practitioners. Findings reveal that role models were perceived as sources of knowledge, yet the majority of participants, regardless of their career stage, displayed a stronger interest in the human side and the moral values that their role models embodied. This study also shows that any practitioner can be viewed as a role model.","sentences":["A role model is a person who serves as an example for others to follow, especially in terms of values, behavior, achievements, and personal characteristics.","In this paper, authors study how role models influence software practitioners careers, an aspect not studied in the literature before.","By means of this study, authors aim to understand if there are any salient role model archetypes and what characteristics are valued by participants in their role models.","To do so, authors use a thematic coding approach to analyze the data collected from interviewing ten Latin American software practitioners.","Findings reveal that role models were perceived as sources of knowledge, yet the majority of participants, regardless of their career stage, displayed a stronger interest in the human side and the moral values that their role models embodied.","This study also shows that any practitioner can be viewed as a role model."],"url":"http://arxiv.org/abs/2402.09925v1","category":"cs.SE"}
{"created":"2024-02-15 12:09:16","title":"Operadic Kazhdan-Lusztig-Stanley theory","abstract":"We introduce a new type of operad-like structure called a P-operad, which depends on the choice of some collection of posets P, and which is governed by chains in posets of P. We introduce several examples of such structures which are related to classical poset theoretic notions such as poset homology, Cohen--Macaulayness and lexicographic shellability. We then show that P-operads form a satisfactory framework to categorify Kazhdan--Lusztig polynomials of geometric lattices and their kernel. In particular, this leads to a new proof of the positivity of the coefficients of Kazhdan--Lusztig polynomials of geometric lattices.","sentences":["We introduce a new type of operad-like structure called a P-operad, which depends on the choice of some collection of posets P, and which is governed by chains in posets of P. We introduce several examples of such structures which are related to classical poset theoretic notions such as poset homology, Cohen--Macaulayness and lexicographic shellability.","We then show that P-operads form a satisfactory framework to categorify Kazhdan--Lusztig polynomials of geometric lattices and their kernel.","In particular, this leads to a new proof of the positivity of the coefficients of Kazhdan--Lusztig polynomials of geometric lattices."],"url":"http://arxiv.org/abs/2402.09905v1","category":"math.CO"}
{"created":"2024-02-15 11:45:34","title":"COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web Application for Classifying COVID-19 Discussions","abstract":"The COVID-19 pandemic has had adverse effects on both physical and mental health. During this pandemic, numerous studies have focused on gaining insights into health-related perspectives from social media. In this study, our primary objective is to develop a machine learning-based web application for automatically classifying COVID-19-related discussions on social media. To achieve this, we label COVID-19-related Twitter data, provide benchmark classification results, and develop a web application. We collected data using the Twitter API and labeled a total of 6,667 tweets into five different classes: health risks, prevention, symptoms, transmission, and treatment. We extracted features using various feature extraction methods and applied them to seven different traditional machine learning algorithms, including Decision Tree, Random Forest, Stochastic Gradient Descent, Adaboost, K-Nearest Neighbour, Logistic Regression, and Linear SVC. Additionally, we used four deep learning algorithms: LSTM, CNN, RNN, and BERT, for classification. Overall, we achieved a maximum F1 score of 90.43% with the CNN algorithm in deep learning. The Linear SVC algorithm exhibited the highest F1 score at 86.13%, surpassing other traditional machine learning approaches. Our study not only contributes to the field of health-related data analysis but also provides a valuable resource in the form of a web-based tool for efficient data classification, which can aid in addressing public health challenges and increasing awareness during pandemics. We made the dataset and application publicly available, which can be downloaded from this link https://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website.","sentences":["The COVID-19 pandemic has had adverse effects on both physical and mental health.","During this pandemic, numerous studies have focused on gaining insights into health-related perspectives from social media.","In this study, our primary objective is to develop a machine learning-based web application for automatically classifying COVID-19-related discussions on social media.","To achieve this, we label COVID-19-related Twitter data, provide benchmark classification results, and develop a web application.","We collected data using the Twitter API and labeled a total of 6,667 tweets into five different classes: health risks, prevention, symptoms, transmission, and treatment.","We extracted features using various feature extraction methods and applied them to seven different traditional machine learning algorithms, including Decision Tree, Random Forest, Stochastic Gradient Descent, Adaboost, K-Nearest Neighbour, Logistic Regression, and Linear SVC.","Additionally, we used four deep learning algorithms: LSTM, CNN, RNN, and BERT, for classification.","Overall, we achieved a maximum F1 score of 90.43% with the CNN algorithm in deep learning.","The Linear SVC algorithm exhibited the highest F1 score at 86.13%, surpassing other traditional machine learning approaches.","Our study not only contributes to the field of health-related data analysis but also provides a valuable resource in the form of a web-based tool for efficient data classification, which can aid in addressing public health challenges and increasing awareness during pandemics.","We made the dataset and application publicly available, which can be downloaded from this link https://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website."],"url":"http://arxiv.org/abs/2402.09897v1","category":"cs.LG"}
{"created":"2024-02-15 11:21:57","title":"Self-Supervised Learning of Visual Robot Localization Using LED State Prediction as a Pretext Task","abstract":"We propose a novel self-supervised approach for learning to visually localize robots equipped with controllable LEDs. We rely on a few training samples labeled with position ground truth and many training samples in which only the LED state is known, whose collection is cheap. We show that using LED state prediction as a pretext task significantly helps to learn the visual localization end task. The resulting model does not require knowledge of LED states during inference. We instantiate the approach to visual relative localization of nano-quadrotors: experimental results show that using our pretext task significantly improves localization accuracy (from 68.3% to 76.2%) and outperforms alternative strategies, such as a supervised baseline, model pre-training, and an autoencoding pretext task. We deploy our model aboard a 27-g Crazyflie nano-drone, running at 21 fps, in a position-tracking task of a peer nano-drone. Our approach, relying on position labels for only 300 images, yields a mean tracking error of 4.2 cm versus 11.9 cm of a supervised baseline model trained without our pretext task. Videos and code of the proposed approach are available at https://github.com/idsia-robotics/leds-as-pretext","sentences":["We propose a novel self-supervised approach for learning to visually localize robots equipped with controllable LEDs.","We rely on a few training samples labeled with position ground truth and many training samples in which only the LED state is known, whose collection is cheap.","We show that using LED state prediction as a pretext task significantly helps to learn the visual localization end task.","The resulting model does not require knowledge of LED states during inference.","We instantiate the approach to visual relative localization of nano-quadrotors: experimental results show that using our pretext task significantly improves localization accuracy (from 68.3% to 76.2%) and outperforms alternative strategies, such as a supervised baseline, model pre-training, and an autoencoding pretext task.","We deploy our model aboard a 27-g Crazyflie nano-drone, running at 21 fps, in a position-tracking task of a peer nano-drone.","Our approach, relying on position labels for only 300 images, yields a mean tracking error of 4.2 cm versus 11.9 cm of a supervised baseline model trained without our pretext task.","Videos and code of the proposed approach are available at https://github.com/idsia-robotics/leds-as-pretext"],"url":"http://arxiv.org/abs/2402.09886v1","category":"cs.RO"}
{"created":"2024-02-15 10:18:52","title":"Besov spaces in operator theory","abstract":"The survey is devoted to diverse applications of Besov classes in operator theory. It is illustrated how Besov classes are used to describe Hankel operators of Schatten--von Neumann classes; various applications of this description are considered. Next, we discuss the role of Besov classes in norm estimates of polynomials of power bounded operators on Hilbert space and related estimates of Hankel matrices in tensor products of the spaces $\\ell^1$ and $\\ell^\\infty$. An essential part of the survey is devoted to the role of Besov spaces in various problems of perturbation theory when studying the behavior of functions of a single operator or of a collection of operators under their perturbation.","sentences":["The survey is devoted to diverse applications of Besov classes in operator theory.","It is illustrated how Besov classes are used to describe Hankel operators of Schatten--von Neumann classes; various applications of this description are considered.","Next, we discuss the role of Besov classes in norm estimates of polynomials of power bounded operators on Hilbert space and related estimates of Hankel matrices in tensor products of the spaces $\\ell^1$ and $\\ell^\\infty$. An essential part of the survey is devoted to the role of Besov spaces in various problems of perturbation theory when studying the behavior of functions of a single operator or of a collection of operators under their perturbation."],"url":"http://arxiv.org/abs/2402.09853v1","category":"math.FA"}
{"created":"2024-02-15 07:09:21","title":"Mixture of Experts for Network Optimization: A Large Language Model-enabled Approach","abstract":"Optimizing various wireless user tasks poses a significant challenge for networking systems because of the expanding range of user requirements. Despite advancements in Deep Reinforcement Learning (DRL), the need for customized optimization tasks for individual users complicates developing and applying numerous DRL models, leading to substantial computation resource and energy consumption and can lead to inconsistent outcomes. To address this issue, we propose a novel approach utilizing a Mixture of Experts (MoE) framework, augmented with Large Language Models (LLMs), to analyze user objectives and constraints effectively, select specialized DRL experts, and weigh each decision from the participating experts. Specifically, we develop a gate network to oversee the expert models, allowing a collective of experts to tackle a wide array of new tasks. Furthermore, we innovatively substitute the traditional gate network with an LLM, leveraging its advanced reasoning capabilities to manage expert model selection for joint decisions. Our proposed method reduces the need to train new DRL models for each unique optimization problem, decreasing energy consumption and AI model implementation costs. The LLM-enabled MoE approach is validated through a general maze navigation task and a specific network service provider utility maximization task, demonstrating its effectiveness and practical applicability in optimizing complex networking systems.","sentences":["Optimizing various wireless user tasks poses a significant challenge for networking systems because of the expanding range of user requirements.","Despite advancements in Deep Reinforcement Learning (DRL), the need for customized optimization tasks for individual users complicates developing and applying numerous DRL models, leading to substantial computation resource and energy consumption and can lead to inconsistent outcomes.","To address this issue, we propose a novel approach utilizing a Mixture of Experts (MoE) framework, augmented with Large Language Models (LLMs), to analyze user objectives and constraints effectively, select specialized DRL experts, and weigh each decision from the participating experts.","Specifically, we develop a gate network to oversee the expert models, allowing a collective of experts to tackle a wide array of new tasks.","Furthermore, we innovatively substitute the traditional gate network with an LLM, leveraging its advanced reasoning capabilities to manage expert model selection for joint decisions.","Our proposed method reduces the need to train new DRL models for each unique optimization problem, decreasing energy consumption and AI model implementation costs.","The LLM-enabled MoE approach is validated through a general maze navigation task and a specific network service provider utility maximization task, demonstrating its effectiveness and practical applicability in optimizing complex networking systems."],"url":"http://arxiv.org/abs/2402.09756v1","category":"cs.NI"}
{"created":"2024-02-15 07:00:06","title":"Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming","abstract":"Recently, the potential of large language models (LLMs) has been widely used in assisting programming. However, current research does not explore the artist potential of LLMs in creative coding within artist and AI collaboration. Our work probes the reflection type of artists in the creation process with such collaboration. We compare two common collaboration approaches: invoking the entire program and multiple subtasks. Our findings exhibit artists' different stimulated reflections in two different methods. Our finding also shows the correlation of reflection type with user performance, user satisfaction, and subjective experience in two collaborations through conducting two methods, including experimental data and qualitative interviews. In this sense, our work reveals the artistic potential of LLM in creative coding. Meanwhile, we provide a critical lens of human-AI collaboration from the artists' perspective and expound design suggestions for future work of AI-assisted creative tasks.","sentences":["Recently, the potential of large language models (LLMs) has been widely used in assisting programming.","However, current research does not explore the artist potential of LLMs in creative coding within artist and AI collaboration.","Our work probes the reflection type of artists in the creation process with such collaboration.","We compare two common collaboration approaches: invoking the entire program and multiple subtasks.","Our findings exhibit artists' different stimulated reflections in two different methods.","Our finding also shows the correlation of reflection type with user performance, user satisfaction, and subjective experience in two collaborations through conducting two methods, including experimental data and qualitative interviews.","In this sense, our work reveals the artistic potential of LLM in creative coding.","Meanwhile, we provide a critical lens of human-AI collaboration from the artists' perspective and expound design suggestions for future work of AI-assisted creative tasks."],"url":"http://arxiv.org/abs/2402.09750v1","category":"cs.HC"}
{"created":"2024-02-15 06:58:30","title":"Model Compression and Efficient Inference for Large Language Models: A Survey","abstract":"Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large models, such as quantization and pruning, start to explore tuning-free algorithms. (2) Large models emphasize versatility and generalization rather than performance on a single task. Hence, many algorithms, such as knowledge distillation, focus on how to preserving their versatility and generalization after compression. Since these two characteristics were not very pronounced in early large models, we further distinguish large language models into medium models and ``real'' large models. Additionally, we also provide an introduction to some mature frameworks for efficient inference of large models, which can support basic compression or acceleration algorithms, greatly facilitating model deployment for users.","sentences":["Transformer based large language models have achieved tremendous success.","However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices.","In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective.","Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks.","However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression.","The most notable aspect of large models is the very high cost associated with model finetuning or training.","Therefore, many algorithms for large models, such as quantization and pruning, start to explore tuning-free algorithms.","(2) Large models emphasize versatility and generalization rather than performance on a single task.","Hence, many algorithms, such as knowledge distillation, focus on how to preserving their versatility and generalization after compression.","Since these two characteristics were not very pronounced in early large models, we further distinguish large language models into medium models and ``real'' large models.","Additionally, we also provide an introduction to some mature frameworks for efficient inference of large models, which can support basic compression or acceleration algorithms, greatly facilitating model deployment for users."],"url":"http://arxiv.org/abs/2402.09748v1","category":"cs.CL"}
{"created":"2024-02-15 06:58:25","title":"Less is more: Ensemble Learning for Retinal Disease Recognition Under Limited Resources","abstract":"Retinal optical coherence tomography (OCT) images provide crucial insights into the health of the posterior ocular segment. Therefore, the advancement of automated image analysis methods is imperative to equip clinicians and researchers with quantitative data, thereby facilitating informed decision-making. The application of deep learning (DL)-based approaches has gained extensive traction for executing these analysis tasks, demonstrating remarkable performance compared to labor-intensive manual analyses. However, the acquisition of Retinal OCT images often presents challenges stemming from privacy concerns and the resource-intensive labeling procedures, which contradicts the prevailing notion that DL models necessitate substantial data volumes for achieving superior performance. Moreover, limitations in available computational resources constrain the progress of high-performance medical artificial intelligence, particularly in less developed regions and countries. This paper introduces a novel ensemble learning mechanism designed for recognizing retinal diseases under limited resources (e.g., data, computation). The mechanism leverages insights from multiple pre-trained models, facilitating the transfer and adaptation of their knowledge to Retinal OCT images. This approach establishes a robust model even when confronted with limited labeled data, eliminating the need for an extensive array of parameters, as required in learning from scratch. Comprehensive experimentation on real-world datasets demonstrates that the proposed approach can achieve superior performance in recognizing Retinal OCT images, even when dealing with exceedingly restricted labeled datasets. Furthermore, this method obviates the necessity of learning extensive-scale parameters, making it well-suited for deployment in low-resource scenarios.","sentences":["Retinal optical coherence tomography (OCT) images provide crucial insights into the health of the posterior ocular segment.","Therefore, the advancement of automated image analysis methods is imperative to equip clinicians and researchers with quantitative data, thereby facilitating informed decision-making.","The application of deep learning (DL)-based approaches has gained extensive traction for executing these analysis tasks, demonstrating remarkable performance compared to labor-intensive manual analyses.","However, the acquisition of Retinal OCT images often presents challenges stemming from privacy concerns and the resource-intensive labeling procedures, which contradicts the prevailing notion that DL models necessitate substantial data volumes for achieving superior performance.","Moreover, limitations in available computational resources constrain the progress of high-performance medical artificial intelligence, particularly in less developed regions and countries.","This paper introduces a novel ensemble learning mechanism designed for recognizing retinal diseases under limited resources (e.g., data, computation).","The mechanism leverages insights from multiple pre-trained models, facilitating the transfer and adaptation of their knowledge to Retinal OCT images.","This approach establishes a robust model even when confronted with limited labeled data, eliminating the need for an extensive array of parameters, as required in learning from scratch.","Comprehensive experimentation on real-world datasets demonstrates that the proposed approach can achieve superior performance in recognizing Retinal OCT images, even when dealing with exceedingly restricted labeled datasets.","Furthermore, this method obviates the necessity of learning extensive-scale parameters, making it well-suited for deployment in low-resource scenarios."],"url":"http://arxiv.org/abs/2402.09747v1","category":"eess.IV"}
{"created":"2024-02-15 06:51:53","title":"WEFix: Intelligent Automatic Generation of Explicit Waits for Efficient Web End-to-End Flaky Tests","abstract":"Web end-to-end (e2e) testing evaluates the workflow of a web application. It simulates real-world user scenarios to ensure the application flows behave as expected. However, web e2e tests are notorious for being flaky, i.e., the tests can produce inconsistent results despite no changes to the code. One common type of flakiness is caused by nondeterministic execution orders between the test code and the client-side code under test. In particular, UI-based flakiness emerges as a notably prevalent and challenging issue to fix because the test code has limited knowledge about the client-side code execution. In this paper, we propose WEFix, a technique that can automatically generate fix code for UI-based flakiness in web e2e testing. The core of our approach is to leverage browser UI changes to predict the client-side code execution and generate proper wait oracles. We evaluate the effectiveness and efficiency of WEFix against 122 web e2e flaky tests from seven popular real-world projects. Our results show that WEFix dramatically reduces the overhead (from 3.7$\\times$ to 1.25$\\times$) while achieving a high correctness (98%).","sentences":["Web end-to-end (e2e) testing evaluates the workflow of a web application.","It simulates real-world user scenarios to ensure the application flows behave as expected.","However, web e2e tests are notorious for being flaky, i.e., the tests can produce inconsistent results despite no changes to the code.","One common type of flakiness is caused by nondeterministic execution orders between the test code and the client-side code under test.","In particular, UI-based flakiness emerges as a notably prevalent and challenging issue to fix because the test code has limited knowledge about the client-side code execution.","In this paper, we propose WEFix, a technique that can automatically generate fix code for UI-based flakiness in web e2e testing.","The core of our approach is to leverage browser UI changes to predict the client-side code execution and generate proper wait oracles.","We evaluate the effectiveness and efficiency of WEFix against 122 web e2e flaky tests from seven popular real-world projects.","Our results show that WEFix dramatically reduces the overhead (from 3.7$\\times$ to 1.25$\\times$) while achieving a high correctness (98%)."],"url":"http://arxiv.org/abs/2402.09745v1","category":"cs.SE"}
{"created":"2024-02-15 06:46:48","title":"AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis","abstract":"The incorporation of Large Language Models (LLMs) in healthcare marks a significant advancement. However, the application has predominantly been limited to discriminative and question-answering tasks, which does not fully leverage their interactive potential. To address this limitation, our paper presents AI Hospital, a framework designed to build a real-time interactive diagnosis environment. To simulate the procedure, we collect high-quality medical records to create patient, examiner, and medical director agents. AI Hospital is then utilized for the interactive evaluation and collaboration of LLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark where various LLMs serve as intern doctors for interactive diagnosis. Subsequently, to improve diagnostic accuracy, we introduce a collaborative mechanism that involves iterative discussions and a dispute resolution process under the supervision of the medical director. In our experiments, we validate the reliability of AI Hospital. The results not only explore the feasibility of apply LLMs in clinical consultation but also confirm the effectiveness of the dispute resolution focused collaboration method.","sentences":["The incorporation of Large Language Models (LLMs) in healthcare marks a significant advancement.","However, the application has predominantly been limited to discriminative and question-answering tasks, which does not fully leverage their interactive potential.","To address this limitation, our paper presents AI Hospital, a framework designed to build a real-time interactive diagnosis environment.","To simulate the procedure, we collect high-quality medical records to create patient, examiner, and medical director agents.","AI Hospital is then utilized for the interactive evaluation and collaboration of LLMs.","Initially, we create a Multi-View Medical Evaluation (MVME) benchmark where various LLMs serve as intern doctors for interactive diagnosis.","Subsequently, to improve diagnostic accuracy, we introduce a collaborative mechanism that involves iterative discussions and a dispute resolution process under the supervision of the medical director.","In our experiments, we validate the reliability of AI Hospital.","The results not only explore the feasibility of apply LLMs in clinical consultation but also confirm the effectiveness of the dispute resolution focused collaboration method."],"url":"http://arxiv.org/abs/2402.09742v1","category":"cs.CL"}
{"created":"2024-02-15 06:15:46","title":"Agents Need Not Know Their Purpose","abstract":"Ensuring artificial intelligence behaves in such a way that is aligned with human values is commonly referred to as the alignment challenge. Prior work has shown that rational agents, behaving in such a way that maximizes a utility function, will inevitably behave in such a way that is not aligned with human values, especially as their level of intelligence goes up. Prior work has also shown that there is no \"one true utility function\"; solutions must include a more holistic approach to alignment. This paper describes oblivious agents: agents that are architected in such a way that their effective utility function is an aggregation of a known and hidden sub-functions. The hidden component, to be maximized, is internally implemented as a black box, preventing the agent from examining it. The known component, to be minimized, is knowledge of the hidden sub-function. Architectural constraints further influence how agent actions can evolve its internal environment model. We show that an oblivious agent, behaving rationally, constructs an internal approximation of designers' intentions (i.e., infers alignment), and, as a consequence of its architecture and effective utility function, behaves in such a way that maximizes alignment; i.e., maximizing the approximated intention function. We show that, paradoxically, it does this for whatever utility function is used as the hidden component and, in contrast with extant techniques, chances of alignment actually improve as agent intelligence grows.","sentences":["Ensuring artificial intelligence behaves in such a way that is aligned with human values is commonly referred to as the alignment challenge.","Prior work has shown that rational agents, behaving in such a way that maximizes a utility function, will inevitably behave in such a way that is not aligned with human values, especially as their level of intelligence goes up.","Prior work has also shown that there is no \"one true utility function\"; solutions must include a more holistic approach to alignment.","This paper describes oblivious agents: agents that are architected in such a way that their effective utility function is an aggregation of a known and hidden sub-functions.","The hidden component, to be maximized, is internally implemented as a black box, preventing the agent from examining it.","The known component, to be minimized, is knowledge of the hidden sub-function.","Architectural constraints further influence how agent actions can evolve its internal environment model.","We show that an oblivious agent, behaving rationally, constructs an internal approximation of designers' intentions (i.e., infers alignment), and, as a consequence of its architecture and effective utility function, behaves in such a way that maximizes alignment; i.e., maximizing the approximated intention function.","We show that, paradoxically, it does this for whatever utility function is used as the hidden component and, in contrast with extant techniques, chances of alignment actually improve as agent intelligence grows."],"url":"http://arxiv.org/abs/2402.09734v1","category":"cs.AI"}
{"created":"2024-02-15 05:56:35","title":"Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System","abstract":"This paper investigates resource allocation to provide heterogeneous users with customized virtual reality (VR) services in a mobile edge computing (MEC) system. We first introduce a quality of experience (QoE) metric to measure user experience, which considers the MEC system's latency, user attention levels, and preferred resolutions. Then, a QoE maximization problem is formulated for resource allocation to ensure the highest possible user experience,which is cast as a reinforcement learning problem, aiming to learn a generalized policy applicable across diverse user environments for all MEC servers. To learn the generalized policy, we propose a framework that employs federated learning (FL) and prompt-based sequence modeling to pre-train a common decision model across MEC servers, which is named FedPromptDT. Using FL solves the problem of insufficient local MEC data while protecting user privacy during offline training. The design of prompts integrating user-environment cues and user-preferred allocation improves the model's adaptability to various user environments during online execution.","sentences":["This paper investigates resource allocation to provide heterogeneous users with customized virtual reality (VR) services in a mobile edge computing (MEC) system.","We first introduce a quality of experience (QoE) metric to measure user experience, which considers the MEC system's latency, user attention levels, and preferred resolutions.","Then, a QoE maximization problem is formulated for resource allocation to ensure the highest possible user experience,which is cast as a reinforcement learning problem, aiming to learn a generalized policy applicable across diverse user environments for all MEC servers.","To learn the generalized policy, we propose a framework that employs federated learning (FL) and prompt-based sequence modeling to pre-train a common decision model across MEC servers, which is named FedPromptDT.","Using FL solves the problem of insufficient local MEC data while protecting user privacy during offline training.","The design of prompts integrating user-environment cues and user-preferred allocation improves the model's adaptability to various user environments during online execution."],"url":"http://arxiv.org/abs/2402.09729v1","category":"cs.AI"}
{"created":"2024-02-15 05:49:22","title":"AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns","abstract":"SMS phishing, also known as \"smishing\", is a growing threat that tricks users into disclosing private information or clicking into URLs with malicious content through fraudulent mobile text messages. In recent past, we have also observed a rapid advancement of conversational generative AI chatbot services (e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large language models (LLMs). These AI chatbots certainly have a lot of utilities but it is not systematically understood how they can play a role in creating threats and attacks. In this paper, we propose AbuseGPT method to show how the existing generative AI-based chatbot services can be exploited by attackers in real world to create smishing texts and eventually lead to craftier smishing campaigns. To the best of our knowledge, there is no pre-existing work that evidently shows the impacts of these generative text-based models on creating SMS phishing. Thus, we believe this study is the first of its kind to shed light on this emerging cybersecurity threat. We have found strong empirical evidences to show that attackers can exploit ethical standards in the existing generative AI-based chatbot services by crafting prompt injection attacks to create newer smishing campaigns. We also discuss some future research directions and guidelines to protect the abuse of generative AI-based services and safeguard users from smishing attacks.","sentences":["SMS phishing, also known as \"smishing\", is a growing threat that tricks users into disclosing private information or clicking into URLs with malicious content through fraudulent mobile text messages.","In recent past, we have also observed a rapid advancement of conversational generative AI chatbot services (e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large language models (LLMs).","These AI chatbots certainly have a lot of utilities but it is not systematically understood how they can play a role in creating threats and attacks.","In this paper, we propose AbuseGPT method to show how the existing generative AI-based chatbot services can be exploited by attackers in real world to create smishing texts and eventually lead to craftier smishing campaigns.","To the best of our knowledge, there is no pre-existing work that evidently shows the impacts of these generative text-based models on creating SMS phishing.","Thus, we believe this study is the first of its kind to shed light on this emerging cybersecurity threat.","We have found strong empirical evidences to show that attackers can exploit ethical standards in the existing generative AI-based chatbot services by crafting prompt injection attacks to create newer smishing campaigns.","We also discuss some future research directions and guidelines to protect the abuse of generative AI-based services and safeguard users from smishing attacks."],"url":"http://arxiv.org/abs/2402.09728v1","category":"cs.CR"}
{"created":"2024-02-15 05:40:21","title":"A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts","abstract":"Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3-20x.","sentences":["Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs.","To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments.","Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task.","We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories.","These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.","ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3-20x."],"url":"http://arxiv.org/abs/2402.09727v1","category":"cs.CL"}
{"created":"2024-02-15 05:35:04","title":"Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization","abstract":"Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens. However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference. The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores compared to the base models, respectively, and our CMLMC-EECR achieves the best performance with a comparable translation quality with the Transformer. The experiments results demonstrate the effectiveness of our method.","sentences":["Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens.","However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases.","In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR).","We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions.","We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference.","The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores compared to the base models, respectively, and our CMLMC-EECR achieves the best performance with a comparable translation quality with the Transformer.","The experiments results demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.09725v1","category":"cs.CL"}
{"created":"2024-02-15 05:31:13","title":"Best Arm Identification for Prompt Learning under a Limited Budget","abstract":"The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and function approximation. Extensive experiments on multiple well-adopted tasks using both GPT 3.5 and Llama2 demonstrate the significant performance improvement of TRIPLE over the previous baselines while satisfying the limited budget constraints.","sentences":["The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts.","However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered.","To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning.","Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB).","Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically.","Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and function approximation.","Extensive experiments on multiple well-adopted tasks using both GPT 3.5 and Llama2 demonstrate the significant performance improvement of TRIPLE over the previous baselines while satisfying the limited budget constraints."],"url":"http://arxiv.org/abs/2402.09723v1","category":"stat.ML"}
{"created":"2024-02-15 05:31:03","title":"Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields","abstract":"Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene. In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D geometry and appearance in a way which is compact and ideal for robotics applications. However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations. In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors. Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs). We showcase our approach on a new neural field dataset for evaluating registration problems. We provide an exhaustive set of experiments and ablation studies to identify the performance of our approach, while also discussing limitations to provide future direction to the research community on open challenges in utilizing neural fields in unconstrained environments.","sentences":["Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene.","In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D geometry and appearance in a way which is compact and ideal for robotics applications.","However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations.","In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors.","Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs).","We showcase our approach on a new neural field dataset for evaluating registration problems.","We provide an exhaustive set of experiments and ablation studies to identify the performance of our approach, while also discussing limitations to provide future direction to the research community on open challenges in utilizing neural fields in unconstrained environments."],"url":"http://arxiv.org/abs/2402.09722v1","category":"cs.RO"}
{"created":"2024-02-15 05:30:47","title":"Persuading a Learning Agent","abstract":"We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bounded by the agent's regret (swap-regret). If the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can do significantly better than the non-learning model. These conclusions hold not only for Bayesian persuasion, but also for any generalized principal-agent problem with complete information, including Stackelberg games and contract design.","sentences":["We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals.","We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent.","This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment.","The difference between the principal's obtainable utility in the learning model and the non-learning model is bounded by the agent's regret (swap-regret).","If the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can do significantly better than the non-learning model.","These conclusions hold not only for Bayesian persuasion, but also for any generalized principal-agent problem with complete information, including Stackelberg games and contract design."],"url":"http://arxiv.org/abs/2402.09721v1","category":"cs.GT"}
{"created":"2024-02-15 05:07:54","title":"Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement","abstract":"Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analysis, shedding light on the functioning of this model. This is the first work to reveal the potent disentanglement capability of diffusion models with cross-attention, requiring no complex designs. We anticipate that our findings will inspire more investigation on exploring diffusion for disentangled representation learning towards more sophisticated data analysis and understanding.","sentences":["Disentangled representation learning strives to extract the intrinsic factors within observed data.","Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs.","In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations.","We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion.","Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs.","We have conducted comprehensive ablation studies and visualization analysis, shedding light on the functioning of this model.","This is the first work to reveal the potent disentanglement capability of diffusion models with cross-attention, requiring no complex designs.","We anticipate that our findings will inspire more investigation on exploring diffusion for disentangled representation learning towards more sophisticated data analysis and understanding."],"url":"http://arxiv.org/abs/2402.09712v1","category":"cs.CV"}
{"created":"2024-02-15 05:06:53","title":"Preserving Data Privacy for ML-driven Applications in Open Radio Access Networks","abstract":"Deep learning offers a promising solution to improve spectrum access techniques by utilizing data-driven approaches to manage and share limited spectrum resources for emerging applications. For several of these applications, the sensitive wireless data (such as spectrograms) are stored in a shared database or multistakeholder cloud environment and are therefore prone to privacy leaks. This paper aims to address such privacy concerns by examining the representative case study of shared database scenarios in 5G Open Radio Access Network (O-RAN) networks where we have a shared database within the near-real-time (near-RT) RAN intelligent controller. We focus on securing the data that can be used by machine learning (ML) models for spectrum sharing and interference mitigation applications without compromising the model and network performances. The underlying idea is to leverage a (i) Shuffling-based learnable encryption technique to encrypt the data, following which, (ii) employ a custom Vision transformer (ViT) as the trained ML model that is capable of performing accurate inferences on such encrypted data. The paper offers a thorough analysis and comparisons with analogous convolutional neural networks (CNN) as well as deeper architectures (such as ResNet-50) as baselines. Our experiments showcase that the proposed approach significantly outperforms the baseline CNN with an improvement of 24.5% and 23.9% for the percent accuracy and F1-Score respectively when operated on encrypted data. Though deeper ResNet-50 architecture is obtained as a slightly more accurate model, with an increase of 4.4%, the proposed approach boasts a reduction of parameters by 99.32%, and thus, offers a much-improved prediction time by nearly 60%.","sentences":["Deep learning offers a promising solution to improve spectrum access techniques by utilizing data-driven approaches to manage and share limited spectrum resources for emerging applications.","For several of these applications, the sensitive wireless data (such as spectrograms) are stored in a shared database or multistakeholder cloud environment and are therefore prone to privacy leaks.","This paper aims to address such privacy concerns by examining the representative case study of shared database scenarios in 5G Open Radio Access Network (O-RAN) networks where we have a shared database within the near-real-time (near-RT) RAN intelligent controller.","We focus on securing the data that can be used by machine learning (ML) models for spectrum sharing and interference mitigation applications without compromising the model and network performances.","The underlying idea is to leverage a (i) Shuffling-based learnable encryption technique to encrypt the data, following which, (ii) employ a custom Vision transformer (ViT) as the trained ML model that is capable of performing accurate inferences on such encrypted data.","The paper offers a thorough analysis and comparisons with analogous convolutional neural networks (CNN) as well as deeper architectures (such as ResNet-50) as baselines.","Our experiments showcase that the proposed approach significantly outperforms the baseline CNN with an improvement of 24.5% and 23.9% for the percent accuracy and F1-Score respectively when operated on encrypted data.","Though deeper ResNet-50 architecture is obtained as a slightly more accurate model, with an increase of 4.4%, the proposed approach boasts a reduction of parameters by 99.32%, and thus, offers a much-improved prediction time by nearly 60%."],"url":"http://arxiv.org/abs/2402.09710v1","category":"cs.CR"}
{"created":"2024-02-15 04:08:49","title":"Reward Poisoning Attack Against Offline Reinforcement Learning","abstract":"We study the problem of reward poisoning attacks against general offline reinforcement learning with deep neural networks for function approximation. We consider a black-box threat model where the attacker is completely oblivious to the learning algorithm and its budget is limited by constraining both the amount of corruption at each data point, and the total perturbation. We propose an attack strategy called `policy contrast attack'. The high-level idea is to make some low-performing policies appear as high-performing while making high-performing policies appear as low-performing. To the best of our knowledge, we propose the first black-box reward poisoning attack in the general offline RL setting. We provide theoretical insights on the attack design and empirically show that our attack is efficient against current state-of-the-art offline RL algorithms in different kinds of learning datasets.","sentences":["We study the problem of reward poisoning attacks against general offline reinforcement learning with deep neural networks for function approximation.","We consider a black-box threat model where the attacker is completely oblivious to the learning algorithm and its budget is limited by constraining both the amount of corruption at each data point, and the total perturbation.","We propose an attack strategy called `policy contrast attack'.","The high-level idea is to make some low-performing policies appear as high-performing while making high-performing policies appear as low-performing.","To the best of our knowledge, we propose the first black-box reward poisoning attack in the general offline RL setting.","We provide theoretical insights on the attack design and empirically show that our attack is efficient against current state-of-the-art offline RL algorithms in different kinds of learning datasets."],"url":"http://arxiv.org/abs/2402.09695v1","category":"cs.LG"}
{"created":"2024-02-15 03:43:00","title":"Pheno-Robot: An Auto-Digital Modelling System for In-Situ Phenotyping in the Field","abstract":"Accurate reconstruction of plant models for phenotyping analysis is critical for optimising sustainable agricultural practices in precision agriculture. Traditional laboratory-based phenotyping, while valuable, falls short of understanding how plants grow under uncontrolled conditions. Robotic technologies offer a promising avenue for large-scale, direct phenotyping in real-world environments. This study explores the deployment of emerging robotics and digital technology in plant phenotyping to improve performance and efficiency. Three critical functional modules: environmental understanding, robotic motion planning, and in-situ phenotyping, are introduced to automate the entire process. Experimental results demonstrate the effectiveness of the system in agricultural environments. The pheno-robot system autonomously collects high-quality data by navigating around plants. In addition, the in-situ modelling model reconstructs high-quality plant models from the data collected by the robot. The developed robotic system shows high efficiency and robustness, demonstrating its potential to advance plant science in real-world agricultural environments.","sentences":["Accurate reconstruction of plant models for phenotyping analysis is critical for optimising sustainable agricultural practices in precision agriculture.","Traditional laboratory-based phenotyping, while valuable, falls short of understanding how plants grow under uncontrolled conditions.","Robotic technologies offer a promising avenue for large-scale, direct phenotyping in real-world environments.","This study explores the deployment of emerging robotics and digital technology in plant phenotyping to improve performance and efficiency.","Three critical functional modules: environmental understanding, robotic motion planning, and in-situ phenotyping, are introduced to automate the entire process.","Experimental results demonstrate the effectiveness of the system in agricultural environments.","The pheno-robot system autonomously collects high-quality data by navigating around plants.","In addition, the in-situ modelling model reconstructs high-quality plant models from the data collected by the robot.","The developed robotic system shows high efficiency and robustness, demonstrating its potential to advance plant science in real-world agricultural environments."],"url":"http://arxiv.org/abs/2402.09685v1","category":"cs.RO"}
{"created":"2024-02-15 03:39:55","title":"Exploring a Behavioral Model of \"Positive Friction\" in Human-AI Interaction","abstract":"Designing seamless, frictionless user experiences has long been a dominant trend in both applied behavioral science and artificial intelligence (AI), in which the goal of making desirable actions easy and efficient informs efforts to minimize friction in user experiences. However, in some settings, friction can be genuinely beneficial, such as the insertion of deliberate delays to increase reflection, preventing individuals from resorting to automatic or biased behaviors, and enhancing opportunities for unexpected discoveries. More recently, the popularization and availability of AI on a widespread scale has only increased the need to examine how friction can help or hinder users of AI; it also suggests a need to consider how positive friction can benefit AI practitioners, both during development processes (e.g., working with diverse teams) and to inform how AI is designed into offerings. This paper first proposes a \"positive friction\" model that can help characterize how friction is currently beneficial in user and developer experiences with AI, diagnose the potential need for friction where it may not yet exist in these contexts, and inform how positive friction can be used to generate solutions, especially as advances in AI continue to be progress and new opportunities emerge. It then explores this model in the context of AI users and developers by proposing the value of taking a hybrid \"AI+human\" lens, and concludes by suggesting questions for further exploration.","sentences":["Designing seamless, frictionless user experiences has long been a dominant trend in both applied behavioral science and artificial intelligence (AI), in which the goal of making desirable actions easy and efficient informs efforts to minimize friction in user experiences.","However, in some settings, friction can be genuinely beneficial, such as the insertion of deliberate delays to increase reflection, preventing individuals from resorting to automatic or biased behaviors, and enhancing opportunities for unexpected discoveries.","More recently, the popularization and availability of AI on a widespread scale has only increased the need to examine how friction can help or hinder users of AI; it also suggests a need to consider how positive friction can benefit AI practitioners, both during development processes (e.g., working with diverse teams) and to inform how AI is designed into offerings.","This paper first proposes a \"positive friction\" model that can help characterize how friction is currently beneficial in user and developer experiences with AI, diagnose the potential need for friction where it may not yet exist in these contexts, and inform how positive friction can be used to generate solutions, especially as advances in AI continue to be progress and new opportunities emerge.","It then explores this model in the context of AI users and developers by proposing the value of taking a hybrid \"AI+human\" lens, and concludes by suggesting questions for further exploration."],"url":"http://arxiv.org/abs/2402.09683v1","category":"cs.HC"}
{"created":"2024-02-15 02:54:49","title":"PAL: Proxy-Guided Black-Box Attack on Large Language Models","abstract":"Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs (PAL), the first optimization-based attack on LLMs in a black-box query-only setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs. Our attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on Llama-2-7B, compared to 4% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple baseline for query-based attacks. We believe the techniques proposed in this work will enable more comprehensive safety testing of LLMs and, in the long term, the development of better security guardrails. The code can be found at https://github.com/chawins/pal.","sentences":["Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated.","While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses.","In this work, we introduce the Proxy-Guided Attack on LLMs (PAL), the first optimization-based attack on LLMs in a black-box query-only setting.","In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs.","Our attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on Llama-2-7B, compared to 4% for the current state of the art.","We also propose GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple baseline for query-based attacks.","We believe the techniques proposed in this work will enable more comprehensive safety testing of LLMs and, in the long term, the development of better security guardrails.","The code can be found at https://github.com/chawins/pal."],"url":"http://arxiv.org/abs/2402.09674v1","category":"cs.CL"}
{"created":"2024-02-15 02:27:57","title":"How to Train Data-Efficient LLMs","abstract":"The training of large language models (LLMs) is expensive. In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample. In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover the performance of the full data, while models trained on Ask-LLM data consistently outperform full-data training -- even when we reject 90% of the original dataset, while converging up to 70% faster.","sentences":["The training of large language models (LLMs) is expensive.","In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption.","We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space.","Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example.","To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample.","In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories.","Coverage sampling can recover the performance of the full data, while models trained on Ask-LLM data consistently outperform full-data training -- even when we reject 90% of the original dataset, while converging up to 70% faster."],"url":"http://arxiv.org/abs/2402.09668v1","category":"cs.LG"}
{"created":"2024-02-15 02:24:46","title":"CodeMind: A Framework to Challenge Large Language Models for Code Reasoning","abstract":"Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize. However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls. Furthermore, we observe that, while correlated, specification reasoning (essential for code synthesis) does not imply execution reasoning (essential for broader programming tasks such as testing and debugging): ranking LLMs based on test passing can be different compared to code reasoning.","sentences":["Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage.","As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs.","CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR).","The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize.","The third one evaluates the extent to which LLMs implement the specified expected behavior.","Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize.","However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls.","Furthermore, we observe that, while correlated, specification reasoning (essential for code synthesis) does not imply execution reasoning (essential for broader programming tasks such as testing and debugging): ranking LLMs based on test passing can be different compared to code reasoning."],"url":"http://arxiv.org/abs/2402.09664v1","category":"cs.SE"}
{"created":"2024-02-15 02:06:06","title":"User Modeling and User Profiling: A Comprehensive Survey","abstract":"The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences. These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems. This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research. We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends. Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data structures. We also address the critical need for privacy-preserving techniques and the push towards explainability and fairness in user modeling approaches. By examining the definitions of core terminology, we aim to clarify ambiguities and foster a clearer understanding of the field by proposing two novel encyclopedic definitions of the main terms. Furthermore, we explore the application of user modeling in various domains, such as fake news detection, cybersecurity, and personalized education. This survey serves as a comprehensive resource for researchers and practitioners, offering insights into the evolution of user modeling and profiling and guiding the development of more personalized, ethical, and effective AI systems.","sentences":["The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences.","These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems.","This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research.","We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends.","Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data structures.","We also address the critical need for privacy-preserving techniques and the push towards explainability and fairness in user modeling approaches.","By examining the definitions of core terminology, we aim to clarify ambiguities and foster a clearer understanding of the field by proposing two novel encyclopedic definitions of the main terms.","Furthermore, we explore the application of user modeling in various domains, such as fake news detection, cybersecurity, and personalized education.","This survey serves as a comprehensive resource for researchers and practitioners, offering insights into the evolution of user modeling and profiling and guiding the development of more personalized, ethical, and effective AI systems."],"url":"http://arxiv.org/abs/2402.09660v1","category":"cs.AI"}
{"created":"2024-02-15 01:50:38","title":"The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse","abstract":"Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse after only few edits. To facilitate further research, we have utilized ChatGPT to develop a new dataset, HardCF, based on those hard cases. This dataset aims to establish the foundation for pioneering research in reliable model editing and the mechanisms underlying editing-induced model collapse. We hope this work can draw the community's attention to the potential risks inherent in model editing practices.","sentences":["Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked.","In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks.","However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive.","To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance.","We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies.","The results indicate that nearly all examined editing methods result in model collapse after only few edits.","To facilitate further research, we have utilized ChatGPT to develop a new dataset, HardCF, based on those hard cases.","This dataset aims to establish the foundation for pioneering research in reliable model editing and the mechanisms underlying editing-induced model collapse.","We hope this work can draw the community's attention to the potential risks inherent in model editing practices."],"url":"http://arxiv.org/abs/2402.09656v1","category":"cs.AI"}
{"created":"2024-02-15 01:38:50","title":"GPT-4's assessment of its performance in a USMLE-based case study","abstract":"This study investigates GPT-4's assessment of its performance in healthcare applications. A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it. Understanding the performance of LLM is paramount in exploring its utility in sensitive areas like healthcare. This study contributes to the ongoing discourse on the reliability of AI, particularly of LLMs like GPT-4, within healthcare, offering insights into how feedback mechanisms might be optimized to enhance AI-assisted medical education and decision support.","sentences":["This study investigates GPT-4's assessment of its performance in healthcare applications.","A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question.","The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question.","The model was asked to provide absolute and relative confidence scores before and after each question.","The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups.","Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups.","Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it.","Understanding the performance of LLM is paramount in exploring its utility in sensitive areas like healthcare.","This study contributes to the ongoing discourse on the reliability of AI, particularly of LLMs like GPT-4, within healthcare, offering insights into how feedback mechanisms might be optimized to enhance AI-assisted medical education and decision support."],"url":"http://arxiv.org/abs/2402.09654v1","category":"cs.AI"}
{"created":"2024-02-15 01:22:30","title":"ProtChatGPT: Towards Understanding Proteins with Large Language Models","abstract":"Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM. The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM. The LLM finally combines user questions with projected embeddings to generate informative answers. Experiments show that ProtChatGPT can produce promising responses to proteins and their corresponding questions. We hope that ProtChatGPT could form the basis for further exploration and application in protein research. Code and our pre-trained model will be publicly available.","sentences":["Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging.","Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research.","In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages.","ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers.","The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM.","The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM.","The LLM finally combines user questions with projected embeddings to generate informative answers.","Experiments show that ProtChatGPT can produce promising responses to proteins and their corresponding questions.","We hope that ProtChatGPT could form the basis for further exploration and application in protein research.","Code and our pre-trained model will be publicly available."],"url":"http://arxiv.org/abs/2402.09649v1","category":"cs.CE"}
{"created":"2024-02-15 01:11:22","title":"Characterizing the Modification Space of Signature IDS Rules","abstract":"Signature-based Intrusion Detection Systems (SIDSs) are traditionally used to detect malicious activity in networks. A notable example of such a system is Snort, which compares network traffic against a series of rules that match known exploits. Current SIDS rules are designed to minimize the amount of legitimate traffic flagged incorrectly, reducing the burden on network administrators. However, different use cases than the traditional one--such as researchers studying trends or analyzing modified versions of known exploits--may require SIDSs to be less constrained in their operation. In this paper, we demonstrate that applying modifications to real-world SIDS rules allow for relaxing some constraints and characterizing the performance space of modified rules. We develop an iterative approach for exploring the space of modifications to SIDS rules. By taking the modifications that expand the ROC curve of performance and altering them further, we show how to modify rules in a directed manner. Using traffic collected and identified as benign or malicious from a cloud telescope, we find that the removal of a single component from SIDS rules has the largest impact on the performance space. Effectively modifying SIDS rules to reduce constraints can enable a broader range of detection for various objectives, from increased security to research purposes.","sentences":["Signature-based Intrusion Detection Systems (SIDSs) are traditionally used to detect malicious activity in networks.","A notable example of such a system is Snort, which compares network traffic against a series of rules that match known exploits.","Current SIDS rules are designed to minimize the amount of legitimate traffic flagged incorrectly, reducing the burden on network administrators.","However, different use cases than the traditional one--such as researchers studying trends or analyzing modified versions of known exploits--may require SIDSs to be less constrained in their operation.","In this paper, we demonstrate that applying modifications to real-world SIDS rules allow for relaxing some constraints and characterizing the performance space of modified rules.","We develop an iterative approach for exploring the space of modifications to SIDS rules.","By taking the modifications that expand the ROC curve of performance and altering them further, we show how to modify rules in a directed manner.","Using traffic collected and identified as benign or malicious from a cloud telescope, we find that the removal of a single component from SIDS rules has the largest impact on the performance space.","Effectively modifying SIDS rules to reduce constraints can enable a broader range of detection for various objectives, from increased security to research purposes."],"url":"http://arxiv.org/abs/2402.09644v1","category":"cs.CR"}
{"created":"2024-02-15 00:31:14","title":"Orthogonal Time Frequency Space for Integrated Sensing and Communication: A Survey","abstract":"Sixth-generation (6G) wireless communication systems, as stated in the European 6G flagship project Hexa-X, are anticipated to feature the integration of intelligence, communication, sensing, positioning, and computation. An important aspect of this integration is integrated sensing and communication (ISAC), in which the same waveform is used for both systems both sensing and communication, to address the challenge of spectrum scarcity. Recently, the orthogonal time frequency space (OTFS) waveform has been proposed to address OFDM's limitations due to the high Doppler spread in some future wireless communication systems. In this paper, we review existing OTFS waveforms for ISAC systems and provide some insights into future research. Firstly, we introduce the basic principles and a system model of OTFS and provide a foundational understanding of this innovative technology's core concepts and architecture. Subsequently, we present an overview of OTFS-based ISAC system frameworks. We provide a comprehensive review of recent research developments and the current state of the art in the field of OTFS-assisted ISAC systems to gain a thorough understanding of the current landscape and advancements. Furthermore, we perform a thorough comparison between OTFS-enabled ISAC operations and traditional OFDM, highlighting the distinctive advantages of OTFS, especially in high Doppler spread scenarios. Subsequently, we address the primary challenges facing OTFS-based ISAC systems, identifying potential limitations and drawbacks. Then, finally, we suggest future research directions, aiming to inspire further innovation in the 6G wireless communication landscape.","sentences":["Sixth-generation (6G) wireless communication systems, as stated in the European 6G flagship project Hexa-X, are anticipated to feature the integration of intelligence, communication, sensing, positioning, and computation.","An important aspect of this integration is integrated sensing and communication (ISAC), in which the same waveform is used for both systems both sensing and communication, to address the challenge of spectrum scarcity.","Recently, the orthogonal time frequency space (OTFS) waveform has been proposed to address OFDM's limitations due to the high Doppler spread in some future wireless communication systems.","In this paper, we review existing OTFS waveforms for ISAC systems and provide some insights into future research.","Firstly, we introduce the basic principles and a system model of OTFS and provide a foundational understanding of this innovative technology's core concepts and architecture.","Subsequently, we present an overview of OTFS-based ISAC system frameworks.","We provide a comprehensive review of recent research developments and the current state of the art in the field of OTFS-assisted ISAC systems to gain a thorough understanding of the current landscape and advancements.","Furthermore, we perform a thorough comparison between OTFS-enabled ISAC operations and traditional OFDM, highlighting the distinctive advantages of OTFS, especially in high Doppler spread scenarios.","Subsequently, we address the primary challenges facing OTFS-based ISAC systems, identifying potential limitations and drawbacks.","Then, finally, we suggest future research directions, aiming to inspire further innovation in the 6G wireless communication landscape."],"url":"http://arxiv.org/abs/2402.09637v1","category":"cs.IT"}
{"created":"2024-02-14 23:12:09","title":"LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations","abstract":"The extraordinary performance of large language models has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains. However, the potential of these models in mining relationships from graph data remains under-explored. Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining. Yet, current cutting-edge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks. A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis. We focus on how to utilize existing LLMs for mining and understanding relationships in graph data, applying these techniques to recommendation tasks. We propose an innovative framework that combines the strong contextual representation capabilities of LLMs with the relationship extraction and analysis functions of GNNs for mining relationships in graph data. Specifically, we design a new prompt construction framework that integrates relational information of graph data into natural language expressions, aiding LLMs in more intuitively grasping the connectivity information within graph data. Additionally, we introduce graph relationship understanding and analysis functions into LLMs to enhance their focus on connectivity information in graph data. Our evaluation on real-world datasets demonstrates the framework's ability to understand connectivity information in graph data.","sentences":["The extraordinary performance of large language models has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains.","However, the potential of these models in mining relationships from graph data remains under-explored.","Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining.","Yet, current cutting-edge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks.","A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships.","This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis.","We focus on how to utilize existing LLMs for mining and understanding relationships in graph data, applying these techniques to recommendation tasks.","We propose an innovative framework that combines the strong contextual representation capabilities of LLMs with the relationship extraction and analysis functions of GNNs for mining relationships in graph data.","Specifically, we design a new prompt construction framework that integrates relational information of graph data into natural language expressions, aiding LLMs in more intuitively grasping the connectivity information within graph data.","Additionally, we introduce graph relationship understanding and analysis functions into LLMs to enhance their focus on connectivity information in graph data.","Our evaluation on real-world datasets demonstrates the framework's ability to understand connectivity information in graph data."],"url":"http://arxiv.org/abs/2402.09617v1","category":"cs.AI"}
{"created":"2024-02-14 23:09:15","title":"API Pack: A Massive Multilingual Dataset for API Call Generation","abstract":"We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.","sentences":["We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities.","Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding.","Fine-tuning CodeLlama-13B","on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls.","Scaling to 100k examples improves generalization to new APIs not seen during training.","In addition, cross-lingual API call generation is achieved without needing extensive data per language.","The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url."],"url":"http://arxiv.org/abs/2402.09615v1","category":"cs.CL"}
{"created":"2024-02-14 23:05:44","title":"Probabilistic Reasoning in Generative Large Language Models","abstract":"This paper considers the challenges that Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values. This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning. To deal with this problem, we first introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs. We then leverage this new dataset to thoroughly illustrate the specific limitations of LLMs for tasks involving probabilistic reasoning and present several strategies that map the problem to different formal representations, including Python code, probabilistic inference algorithms, and probabilistic logical programming. We conclude by providing an evaluation of our methods on BLInD and on an adaptation of a causal reasoning question-answering dataset, which further shows their practical effectiveness.","sentences":["This paper considers the challenges that Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values.","This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making.","Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning.","To deal with this problem, we first introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs.","We then leverage this new dataset to thoroughly illustrate the specific limitations of LLMs for tasks involving probabilistic reasoning and present several strategies that map the problem to different formal representations, including Python code, probabilistic inference algorithms, and probabilistic logical programming.","We conclude by providing an evaluation of our methods on BLInD and on an adaptation of a causal reasoning question-answering dataset, which further shows their practical effectiveness."],"url":"http://arxiv.org/abs/2402.09614v1","category":"cs.CL"}
{"created":"2024-02-14 22:57:03","title":"Towards Privacy-Aware Sign Language Translation at Scale","abstract":"A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we further discuss the advantages and limitations of self-supervised pretraining and anonymization via facial obfuscation for SLT.","sentences":["A major impediment to the advancement of sign language translation (SLT) is data scarcity.","Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions.","Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for.","In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues.","We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset.","SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4.","Based on controlled experiments, we further discuss the advantages and limitations of self-supervised pretraining and anonymization via facial obfuscation for SLT."],"url":"http://arxiv.org/abs/2402.09611v1","category":"cs.CL"}
{"created":"2024-02-14 22:36:07","title":"LogicPrpBank: A Corpus for Logical Implication and Equivalence","abstract":"Logic reasoning has been critically needed in problem-solving and decision-making. Although Language Models (LMs) have demonstrated capabilities of handling multiple reasoning tasks (e.g., commonsense reasoning), their ability to reason complex mathematical problems, specifically propositional logic, remains largely underexplored. This lack of exploration can be attributed to the limited availability of annotated corpora. Here, we present a well-labeled propositional logic corpus, LogicPrpBank, containing 7093 Propositional Logic Statements (PLSs) across six mathematical subjects, to study a brand-new task of reasoning logical implication and equivalence. We benchmark LogicPrpBank with widely-used LMs to show that our corpus offers a useful resource for this challenging task and there is ample room for model improvement.","sentences":["Logic reasoning has been critically needed in problem-solving and decision-making.","Although Language Models (LMs) have demonstrated capabilities of handling multiple reasoning tasks (e.g., commonsense reasoning), their ability to reason complex mathematical problems, specifically propositional logic, remains largely underexplored.","This lack of exploration can be attributed to the limited availability of annotated corpora.","Here, we present a well-labeled propositional logic corpus, LogicPrpBank, containing 7093 Propositional Logic Statements (PLSs) across six mathematical subjects, to study a brand-new task of reasoning logical implication and equivalence.","We benchmark LogicPrpBank with widely-used LMs to show that our corpus offers a useful resource for this challenging task and there is ample room for model improvement."],"url":"http://arxiv.org/abs/2402.09609v1","category":"cs.CL"}
{"created":"2024-02-14 22:26:07","title":"Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for Single Image Test-Time Adaptation","abstract":"Test-time adaptation (TTA) refers to adapting a trained model to a new domain during testing. Existing TTA techniques rely on having multiple test images from the same domain, yet this may be impractical in real-world applications such as medical imaging, where data acquisition is expensive and imaging conditions vary frequently. Here, we approach such a task, of adapting a medical image segmentation model with only a single unlabeled test image. Most TTA approaches, which directly minimize the entropy of predictions, fail to improve performance significantly in this setting, in which we also observe the choice of batch normalization (BN) layer statistics to be a highly important yet unstable factor due to only having a single test domain example. To overcome this, we propose to instead \\textit{integrate} over predictions made with various estimates of target domain statistics between the training and test statistics, weighted based on their entropy statistics.","sentences":["Test-time adaptation (TTA) refers to adapting a trained model to a new domain during testing.","Existing TTA techniques rely on having multiple test images from the same domain, yet this may be impractical in real-world applications such as medical imaging, where data acquisition is expensive and imaging conditions vary frequently.","Here, we approach such a task, of adapting a medical image segmentation model with only a single unlabeled test image.","Most TTA approaches, which directly minimize the entropy of predictions, fail to improve performance significantly in this setting, in which we also observe the choice of batch normalization (BN) layer statistics to be a highly important yet unstable factor due to only having a single test domain example.","To overcome this, we propose to instead \\textit{integrate} over predictions made with various estimates of target domain statistics between the training and test statistics, weighted based on their entropy statistics."],"url":"http://arxiv.org/abs/2402.09604v1","category":"cs.CV"}
{"created":"2024-02-14 22:23:35","title":"Scalable Graph Self-Supervised Learning","abstract":"In regularization Self-Supervised Learning (SSL) methods for graphs, computational complexity increases with the number of nodes in graphs and embedding dimensions. To mitigate the scalability of non-contrastive graph SSL, we propose a novel approach to reduce the cost of computing the covariance matrix for the pre-training loss function with volume-maximization terms. Our work focuses on reducing the cost associated with the loss computation via graph node or dimension sampling. We provide theoretical insight into why dimension sampling would result in accurate loss computations and support it with mathematical derivation of the novel approach. We develop our experimental setup on the node-level graph prediction tasks, where SSL pre-training has shown to be difficult due to the large size of real world graphs. Our experiments demonstrate that the cost associated with the loss computation can be reduced via node or dimension sampling without lowering the downstream performance. Our results demonstrate that sampling mostly results in improved downstream performance. Ablation studies and experimental analysis are provided to untangle the role of the different factors in the experimental setup.","sentences":["In regularization Self-Supervised Learning (SSL) methods for graphs, computational complexity increases with the number of nodes in graphs and embedding dimensions.","To mitigate the scalability of non-contrastive graph SSL, we propose a novel approach to reduce the cost of computing the covariance matrix for the pre-training loss function with volume-maximization terms.","Our work focuses on reducing the cost associated with the loss computation via graph node or dimension sampling.","We provide theoretical insight into why dimension sampling would result in accurate loss computations and support it with mathematical derivation of the novel approach.","We develop our experimental setup on the node-level graph prediction tasks, where SSL pre-training has shown to be difficult due to the large size of real world graphs.","Our experiments demonstrate that the cost associated with the loss computation can be reduced via node or dimension sampling without lowering the downstream performance.","Our results demonstrate that sampling mostly results in improved downstream performance.","Ablation studies and experimental analysis are provided to untangle the role of the different factors in the experimental setup."],"url":"http://arxiv.org/abs/2402.09603v1","category":"cs.LG"}
{"created":"2024-02-14 21:37:59","title":"A Web-Based Tool for Automatic Data Collection, Curation, and Visualization of Complex Healthcare Survey Studies including Social Network Analysis","abstract":"There is a great concern nowadays regarding alcohol consumption and drug abuse, especially in young people. Analyzing the social environment where these adolescents are immersed, as well as a series of measures determining the alcohol abuse risk or personal situation and perception using a number of questionnaires like AUDIT, FAS, KIDSCREEN, and others, it is possible to gain insight into the current situation of a given individual regarding his/her consumption behavior. But this analysis, in order to be achieved, requires the use of tools that can ease the process of questionnaire creation, data gathering, curation and representation, and later analysis and visualization to the user. This research presents the design and construction of a web-based platform able to facilitate each of the mentioned processes by integrating the different phases into an intuitive system with a graphical user interface that hides the complexity underlying each of the questionnaires and techniques used and presenting the results in a flexible and visual way, avoiding any manual handling of data during the process. Advantages of this approach are shown and compared to the previous situation where some of the tasks were accomplished by time consuming and error prone manipulations of data.","sentences":["There is a great concern nowadays regarding alcohol consumption and drug abuse, especially in young people.","Analyzing the social environment where these adolescents are immersed, as well as a series of measures determining the alcohol abuse risk or personal situation and perception using a number of questionnaires like AUDIT, FAS, KIDSCREEN, and others, it is possible to gain insight into the current situation of a given individual regarding his/her consumption behavior.","But this analysis, in order to be achieved, requires the use of tools that can ease the process of questionnaire creation, data gathering, curation and representation, and later analysis and visualization to the user.","This research presents the design and construction of a web-based platform able to facilitate each of the mentioned processes by integrating the different phases into an intuitive system with a graphical user interface that hides the complexity underlying each of the questionnaires and techniques used and presenting the results in a flexible and visual way, avoiding any manual handling of data during the process.","Advantages of this approach are shown and compared to the previous situation where some of the tasks were accomplished by time consuming and error prone manipulations of data."],"url":"http://arxiv.org/abs/2402.09592v1","category":"cs.AI"}
{"created":"2024-02-14 21:33:13","title":"Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications","abstract":"A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test existing LLMs on this new task. Specifically, we consider nine variations of the T5 LLM and evaluate them on two public datasets obtained from ChEMBL and DrugBank. Our experiments show the early results of using LLMs for this task and provide a perspective on the state-of-the-art. We also emphasize the current limitations and discuss future work that has the potential to improve the performance on this task. The creation of molecules from indications, or vice versa, will allow for more efficient targeting of diseases and significantly reduce the cost of drug discovery, with the potential to revolutionize the field of drug discovery in the era of generative AI.","sentences":["A drug molecule is a substance that changes the organism's mental or physical state.","Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition.","While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process.","The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments.","In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test existing LLMs on this new task.","Specifically, we consider nine variations of the T5 LLM and evaluate them on two public datasets obtained from ChEMBL and DrugBank.","Our experiments show the early results of using LLMs for this task and provide a perspective on the state-of-the-art.","We also emphasize the current limitations and discuss future work that has the potential to improve the performance on this task.","The creation of molecules from indications, or vice versa, will allow for more efficient targeting of diseases and significantly reduce the cost of drug discovery, with the potential to revolutionize the field of drug discovery in the era of generative AI."],"url":"http://arxiv.org/abs/2402.09588v1","category":"cs.AI"}
{"created":"2024-02-14 21:19:33","title":"Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems","abstract":"The potential of Machine Learning Control (MLC) in HVAC systems is hindered by its opaque nature and inference mechanisms, which is challenging for users and modelers to fully comprehend, ultimately leading to a lack of trust in MLC-based decision-making. To address this challenge, this paper investigates and explores Interpretable Machine Learning (IML), a branch of Machine Learning (ML) that enhances transparency and understanding of models and their inferences, to improve the credibility of MLC and its industrial application in HVAC systems. Specifically, we developed an innovative framework that combines the principles of Shapley values and the in-context learning feature of Large Language Models (LLMs). While the Shapley values are instrumental in dissecting the contributions of various features in ML models, LLM provides an in-depth understanding of rule-based parts in MLC; combining them, LLM further packages these insights into a coherent, human-understandable narrative. The paper presents a case study to demonstrate the feasibility of the developed IML framework for model predictive control-based precooling under demand response events in a virtual testbed. The results indicate that the developed framework generates and explains the control signals in accordance with the rule-based rationale.","sentences":["The potential of Machine Learning Control (MLC) in HVAC systems is hindered by its opaque nature and inference mechanisms, which is challenging for users and modelers to fully comprehend, ultimately leading to a lack of trust in MLC-based decision-making.","To address this challenge, this paper investigates and explores Interpretable Machine Learning (IML), a branch of Machine Learning (ML) that enhances transparency and understanding of models and their inferences, to improve the credibility of MLC and its industrial application in HVAC systems.","Specifically, we developed an innovative framework that combines the principles of Shapley values and the in-context learning feature of Large Language Models (LLMs).","While the Shapley values are instrumental in dissecting the contributions of various features in ML models, LLM provides an in-depth understanding of rule-based parts in MLC; combining them, LLM further packages these insights into a coherent, human-understandable narrative.","The paper presents a case study to demonstrate the feasibility of the developed IML framework for model predictive control-based precooling under demand response events in a virtual testbed.","The results indicate that the developed framework generates and explains the control signals in accordance with the rule-based rationale."],"url":"http://arxiv.org/abs/2402.09584v1","category":"cs.AI"}
{"created":"2024-02-14 21:05:55","title":"Combatting deepfakes: Policies to address national security threats and rights violations","abstract":"This paper provides policy recommendations to address threats from deepfakes. First, we provide background information about deepfakes and review the harms they pose. We describe how deepfakes are currently used to proliferate sexual abuse material, commit fraud, manipulate voter behavior, and pose threats to national security. Second, we review previous legislative proposals designed to address deepfakes. Third, we present a comprehensive policy proposal that focuses on addressing multiple parts of the deepfake supply chain. The deepfake supply chain begins with a small number of model developers, model providers, and compute providers, and it expands to include billions of potential deepfake creators. We describe this supply chain in greater detail and describe how entities at each step of the supply chain ought to take reasonable measures to prevent the creation and proliferation of deepfakes. Finally, we address potential counterpoints of our proposal. Overall, deepfakes will present increasingly severe threats to global security and individual liberties. To address these threats, we call on policymakers to enact legislation that addresses multiple parts of the deepfake supply chain.","sentences":["This paper provides policy recommendations to address threats from deepfakes.","First, we provide background information about deepfakes and review the harms they pose.","We describe how deepfakes are currently used to proliferate sexual abuse material, commit fraud, manipulate voter behavior, and pose threats to national security.","Second, we review previous legislative proposals designed to address deepfakes.","Third, we present a comprehensive policy proposal that focuses on addressing multiple parts of the deepfake supply chain.","The deepfake supply chain begins with a small number of model developers, model providers, and compute providers, and it expands to include billions of potential deepfake creators.","We describe this supply chain in greater detail and describe how entities at each step of the supply chain ought to take reasonable measures to prevent the creation and proliferation of deepfakes.","Finally, we address potential counterpoints of our proposal.","Overall, deepfakes will present increasingly severe threats to global security and individual liberties.","To address these threats, we call on policymakers to enact legislation that addresses multiple parts of the deepfake supply chain."],"url":"http://arxiv.org/abs/2402.09581v1","category":"cs.CR"}
{"created":"2024-02-14 21:02:07","title":"Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies","abstract":"The rapid progression in artificial intelligence has facilitated the emergence of large language models like ChatGPT, offering potential applications extending into specialized engineering modeling, especially physics-based building energy modeling. This paper investigates the innovative integration of large language models with building energy modeling software, focusing specifically on the fusion of ChatGPT with EnergyPlus. A literature review is first conducted to reveal a growing trend of incorporating of large language models in engineering modeling, albeit limited research on their application in building energy modeling. We underscore the potential of large language models in addressing building energy modeling challenges and outline potential applications including 1) simulation input generation, 2) simulation output analysis and visualization, 3) conducting error analysis, 4) co-simulation, 5) simulation knowledge extraction and training, and 6) simulation optimization. Three case studies reveal the transformative potential of large language models in automating and optimizing building energy modeling tasks, underscoring the pivotal role of artificial intelligence in advancing sustainable building practices and energy efficiency. The case studies demonstrate that selecting the right large language model techniques is essential to enhance performance and reduce engineering efforts. Besides direct use of large language models, three specific techniques were utilized: 1) prompt engineering, 2) retrieval-augmented generation, and 3) multi-agent large language models. The findings advocate a multidisciplinary approach in future artificial intelligence research, with implications extending beyond building energy modeling to other specialized engineering modeling.","sentences":["The rapid progression in artificial intelligence has facilitated the emergence of large language models like ChatGPT, offering potential applications extending into specialized engineering modeling, especially physics-based building energy modeling.","This paper investigates the innovative integration of large language models with building energy modeling software, focusing specifically on the fusion of ChatGPT with EnergyPlus.","A literature review is first conducted to reveal a growing trend of incorporating of large language models in engineering modeling, albeit limited research on their application in building energy modeling.","We underscore the potential of large language models in addressing building energy modeling challenges and outline potential applications including 1) simulation input generation, 2) simulation output analysis and visualization, 3) conducting error analysis, 4) co-simulation, 5) simulation knowledge extraction and training, and 6) simulation optimization.","Three case studies reveal the transformative potential of large language models in automating and optimizing building energy modeling tasks, underscoring the pivotal role of artificial intelligence in advancing sustainable building practices and energy efficiency.","The case studies demonstrate that selecting the right large language model techniques is essential to enhance performance and reduce engineering efforts.","Besides direct use of large language models, three specific techniques were utilized: 1) prompt engineering, 2) retrieval-augmented generation, and 3) multi-agent large language models.","The findings advocate a multidisciplinary approach in future artificial intelligence research, with implications extending beyond building energy modeling to other specialized engineering modeling."],"url":"http://arxiv.org/abs/2402.09579v1","category":"cs.HC"}
{"created":"2024-02-14 20:53:01","title":"Analyzing the Impact of Computation in Adaptive Dynamic Programming for Stochastic LQR Problem","abstract":"Adaptive dynamic programming (ADP) for stochastic linear quadratic regulation (LQR) demands the precise computation of stochastic integrals during policy iteration (PI). In a fully model-free problem setting, this computation can only be approximated by state samples collected at discrete time points using computational methods such as the canonical Euler-Maruyama method. Our research reveals a critical phenomenon: the sampling period can significantly impact control performance. This impact is due to the fact that computational errors introduced in each step of PI can significantly affect the algorithm's convergence behavior, which in turn influences the resulting control policy. We draw a parallel between PI and Newton's method applied to the Ricatti equation to elucidate how the computation impacts control. In this light, the computational error in each PI step manifests itself as an extra error term in each step of Newton's method, with its upper bound proportional to the computational error. Furthermore, we demonstrate that the convergence rate for ADP in stochastic LQR problems using the Euler-Maruyama method is O(h), with h being the sampling period. A sensorimotor control task finally validates these theoretical findings.","sentences":["Adaptive dynamic programming (ADP) for stochastic linear quadratic regulation (LQR) demands the precise computation of stochastic integrals during policy iteration (PI).","In a fully model-free problem setting, this computation can only be approximated by state samples collected at discrete time points using computational methods such as the canonical Euler-Maruyama method.","Our research reveals a critical phenomenon: the sampling period can significantly impact control performance.","This impact is due to the fact that computational errors introduced in each step of PI can significantly affect the algorithm's convergence behavior, which in turn influences the resulting control policy.","We draw a parallel between PI and Newton's method applied to the Ricatti equation to elucidate how the computation impacts control.","In this light, the computational error in each PI step manifests itself as an extra error term in each step of Newton's method, with its upper bound proportional to the computational error.","Furthermore, we demonstrate that the convergence rate for ADP in stochastic LQR problems using the Euler-Maruyama method is O(h), with h being the sampling period.","A sensorimotor control task finally validates these theoretical findings."],"url":"http://arxiv.org/abs/2402.09575v1","category":"math.OC"}
{"created":"2024-02-14 20:41:37","title":"Automated Plaque Detection and Agatston Score Estimation on Non-Contrast CT Scans: A Multicenter Study","abstract":"Coronary artery calcification (CAC) is a strong and independent predictor of cardiovascular disease (CVD). However, manual assessment of CAC often requires radiological expertise, time, and invasive imaging techniques. The purpose of this multicenter study is to validate an automated cardiac plaque detection model using a 3D multiclass nnU-Net for gated and non-gated non-contrast chest CT volumes. CT scans were performed at three tertiary care hospitals and collected as three datasets, respectively. Heart, aorta, and lung segmentations were determined using TotalSegmentator, while plaques in the coronary arteries and heart valves were manually labeled for 801 volumes. In this work we demonstrate how the nnU-Net semantic segmentation pipeline may be adapted to detect plaques in the coronary arteries and valves. With a linear correction, nnU-Net deep learning methods may also accurately estimate Agatston scores on chest non-contrast CT scans. Compared to manual Agatson scoring, automated Agatston scoring indicated a slope of the linear regression of 0.841 with an intercept of +16 HU (R2 = 0.97). These results are an improvement over previous work assessing automated Agatston score computation in non-gated CT scans.","sentences":["Coronary artery calcification (CAC) is a strong and independent predictor of cardiovascular disease (CVD).","However, manual assessment of CAC often requires radiological expertise, time, and invasive imaging techniques.","The purpose of this multicenter study is to validate an automated cardiac plaque detection model using a 3D multiclass nnU-Net for gated and non-gated non-contrast chest CT volumes.","CT scans were performed at three tertiary care hospitals and collected as three datasets, respectively.","Heart, aorta, and lung segmentations were determined using TotalSegmentator, while plaques in the coronary arteries and heart valves were manually labeled for 801 volumes.","In this work we demonstrate how the nnU-Net semantic segmentation pipeline may be adapted to detect plaques in the coronary arteries and valves.","With a linear correction, nnU-Net deep learning methods may also accurately estimate Agatston scores on chest non-contrast CT scans.","Compared to manual Agatson scoring, automated Agatston scoring indicated a slope of the linear regression of 0.841 with an intercept of +16 HU (R2 = 0.97).","These results are an improvement over previous work assessing automated Agatston score computation in non-gated CT scans."],"url":"http://arxiv.org/abs/2402.09569v1","category":"cs.CV"}
{"created":"2024-02-14 20:33:11","title":"Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph","abstract":"Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification. Through extensive experiments, we reveal two critical roles played by the background nodes in target node classification: enhancing structural connectivity between target nodes, and feature correlation with target nodes. Followingthis, we propose a novel Graph-Skeleton1 model, which properly fetches the background nodes, and further condenses the semantic and topological information of background nodes within similar target-background local structures. Extensive experiments on various web graph datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, for MAG240M dataset with 0.24 billion nodes, our generated skeleton graph achieves highly comparable performance while only containing 1.8% nodes of the original graph.","sentences":["Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot.","Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design.","Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications.","One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes.","In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally.","To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification.","Through extensive experiments, we reveal two critical roles played by the background nodes in target node classification: enhancing structural connectivity between target nodes, and feature correlation with target nodes.","Followingthis, we propose a novel Graph-Skeleton1 model, which properly fetches the background nodes, and further condenses the semantic and topological information of background nodes within similar target-background local structures.","Extensive experiments on various web graph datasets demonstrate the effectiveness and efficiency of the proposed method.","In particular, for MAG240M dataset with 0.24 billion nodes, our generated skeleton graph achieves highly comparable performance while only containing 1.8% nodes of the original graph."],"url":"http://arxiv.org/abs/2402.09565v1","category":"cs.AI"}
{"created":"2024-02-14 20:26:52","title":"ABIDES-Economist: Agent-Based Simulation of Economic Systems with Learning Agents","abstract":"We introduce a multi-agent simulator for economic systems comprised of heterogeneous Households, heterogeneous Firms, Central Bank and Government agents, that could be subjected to exogenous, stochastic shocks. The interaction between agents defines the production and consumption of goods in the economy alongside the flow of money. Each agent can be designed to act according to fixed, rule-based strategies or learn their strategies using interactions with others in the simulator. We ground our simulator by choosing agent heterogeneity parameters based on economic literature, while designing their action spaces in accordance with real data in the United States. Our simulator facilitates the use of reinforcement learning strategies for the agents via an OpenAI Gym style environment definition for the economic system. We demonstrate the utility of our simulator by simulating and analyzing two hypothetical (yet interesting) economic scenarios. The first scenario investigates the impact of heterogeneous household skills on their learned preferences to work at different firms. The second scenario examines the impact of a positive production shock to one of two firms on its pricing strategy in comparison to the second firm. We aspire that our platform sets a stage for subsequent research at the intersection of artificial intelligence and economics.","sentences":["We introduce a multi-agent simulator for economic systems comprised of heterogeneous Households, heterogeneous Firms, Central Bank and Government agents, that could be subjected to exogenous, stochastic shocks.","The interaction between agents defines the production and consumption of goods in the economy alongside the flow of money.","Each agent can be designed to act according to fixed, rule-based strategies or learn their strategies using interactions with others in the simulator.","We ground our simulator by choosing agent heterogeneity parameters based on economic literature, while designing their action spaces in accordance with real data in the United States.","Our simulator facilitates the use of reinforcement learning strategies for the agents via an OpenAI Gym style environment definition for the economic system.","We demonstrate the utility of our simulator by simulating and analyzing two hypothetical (yet interesting) economic scenarios.","The first scenario investigates the impact of heterogeneous household skills on their learned preferences to work at different firms.","The second scenario examines the impact of a positive production shock to one of two firms on its pricing strategy in comparison to the second firm.","We aspire that our platform sets a stage for subsequent research at the intersection of artificial intelligence and economics."],"url":"http://arxiv.org/abs/2402.09563v1","category":"cs.MA"}
{"created":"2024-02-14 20:19:24","title":"Bidirectional Generative Pre-training for Improving Time Series Representation Learning","abstract":"Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-series sequences, even more so after fine-tuning on the task.","sentences":["Learning time-series representations for discriminative tasks has been a long-standing challenge.","Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction.","We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers.","This pre-training task preserves original distribution and data shapes of the time-series.","Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities.","Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs.","By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-series sequences, even more so after fine-tuning on the task."],"url":"http://arxiv.org/abs/2402.09558v1","category":"cs.AI"}
{"created":"2024-02-14 20:10:30","title":"Statistical and Machine Learning Models for Predicting Fire and Other Emergency Events","abstract":"Emergency events in a city cause considerable economic loss to individuals, their families, and the community. Accurate and timely prediction of events can help the emergency fire and rescue services in preparing for and mitigating the consequences of emergency events. In this paper, we present a systematic development of predictive models for various types of emergency events in the City of Edmonton, Canada. We present methods for (i) data collection and dataset development; (ii) descriptive analysis of each event type and its characteristics at different spatiotemporal levels; (iii) feature analysis and selection based on correlation coefficient analysis and feature importance analysis; and (iv) development of prediction models for the likelihood of occurrence of each event type at different temporal and spatial resolutions. We analyze the association of event types with socioeconomic and demographic data at the neighborhood level, identify a set of predictors for each event type, and develop predictive models with negative binomial regression. We conduct evaluations at neighborhood and fire station service area levels. Our results show that the models perform well for most of the event types with acceptable prediction errors for weekly and monthly periods. The evaluation shows that the prediction accuracy is consistent at the level of the fire station, so the predictions can be used in management by fire rescue service departments for planning resource allocation for these time periods. We also examine the impact of the COVID-19 pandemic on the occurrence of events and on the accuracy of event predictor models. Our findings show that COVID-19 had a significant impact on the performance of the event prediction models.","sentences":["Emergency events in a city cause considerable economic loss to individuals, their families, and the community.","Accurate and timely prediction of events can help the emergency fire and rescue services in preparing for and mitigating the consequences of emergency events.","In this paper, we present a systematic development of predictive models for various types of emergency events in the City of Edmonton, Canada.","We present methods for (i) data collection and dataset development; (ii) descriptive analysis of each event type and its characteristics at different spatiotemporal levels; (iii) feature analysis and selection based on correlation coefficient analysis and feature importance analysis; and (iv) development of prediction models for the likelihood of occurrence of each event type at different temporal and spatial resolutions.","We analyze the association of event types with socioeconomic and demographic data at the neighborhood level, identify a set of predictors for each event type, and develop predictive models with negative binomial regression.","We conduct evaluations at neighborhood and fire station service area levels.","Our results show that the models perform well for most of the event types with acceptable prediction errors for weekly and monthly periods.","The evaluation shows that the prediction accuracy is consistent at the level of the fire station, so the predictions can be used in management by fire rescue service departments for planning resource allocation for these time periods.","We also examine the impact of the COVID-19 pandemic on the occurrence of events and on the accuracy of event predictor models.","Our findings show that COVID-19 had a significant impact on the performance of the event prediction models."],"url":"http://arxiv.org/abs/2402.09553v1","category":"cs.AI"}
{"created":"2024-02-14 20:01:41","title":"Dataset Clustering for Improved Offline Policy Learning","abstract":"Offline policy learning aims to discover decision-making policies from previously-collected datasets without additional online interactions with the environment. As the training dataset is fixed, its quality becomes a crucial determining factor in the performance of the learned policy. This paper studies a dataset characteristic that we refer to as multi-behavior, indicating that the dataset is collected using multiple policies that exhibit distinct behaviors. In contrast, a uni-behavior dataset would be collected solely using one policy. We observed that policies learned from a uni-behavior dataset typically outperform those learned from multi-behavior datasets, despite the uni-behavior dataset having fewer examples and less diversity. Therefore, we propose a behavior-aware deep clustering approach that partitions multi-behavior datasets into several uni-behavior subsets, thereby benefiting downstream policy learning. Our approach is flexible and effective; it can adaptively estimate the number of clusters while demonstrating high clustering accuracy, achieving an average Adjusted Rand Index of 0.987 across various continuous control task datasets. Finally, we present improved policy learning examples using dataset clustering and discuss several potential scenarios where our approach might benefit the offline policy learning community.","sentences":["Offline policy learning aims to discover decision-making policies from previously-collected datasets without additional online interactions with the environment.","As the training dataset is fixed, its quality becomes a crucial determining factor in the performance of the learned policy.","This paper studies a dataset characteristic that we refer to as multi-behavior, indicating that the dataset is collected using multiple policies that exhibit distinct behaviors.","In contrast, a uni-behavior dataset would be collected solely using one policy.","We observed that policies learned from a uni-behavior dataset typically outperform those learned from multi-behavior datasets, despite the uni-behavior dataset having fewer examples and less diversity.","Therefore, we propose a behavior-aware deep clustering approach that partitions multi-behavior datasets into several uni-behavior subsets, thereby benefiting downstream policy learning.","Our approach is flexible and effective; it can adaptively estimate the number of clusters while demonstrating high clustering accuracy, achieving an average Adjusted Rand Index of 0.987 across various continuous control task datasets.","Finally, we present improved policy learning examples using dataset clustering and discuss several potential scenarios where our approach might benefit the offline policy learning community."],"url":"http://arxiv.org/abs/2402.09550v1","category":"cs.LG"}
{"created":"2024-02-14 19:45:17","title":"How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?","abstract":"In the field of robotics and automation, navigation systems based on Large Language Models (LLMs) have recently shown impressive performance. However, the security aspects of these systems have received relatively less attention. This paper pioneers the exploration of vulnerabilities in LLM-based navigation models in urban outdoor environments, a critical area given the technology's widespread application in autonomous driving, logistics, and emergency services. Specifically, we introduce a novel Navigational Prompt Suffix (NPS) Attack that manipulates LLM-based navigation models by appending gradient-derived suffixes to the original navigational prompt, leading to incorrect actions. We conducted comprehensive experiments on an LLMs-based navigation model that employs various LLMs for reasoning. Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstrate notable performance declines across three metrics in the face of both white-box and black-box attacks. These results highlight the generalizability and transferability of the NPS Attack, emphasizing the need for enhanced security in LLM-based navigation systems. As an initial countermeasure, we propose the Navigational Prompt Engineering (NPE) Defense strategy, concentrating on navigation-relevant keywords to reduce the impact of adversarial suffixes. While initial findings indicate that this strategy enhances navigational safety, there remains a critical need for the wider research community to develop stronger defense methods to effectively tackle the real-world challenges faced by these systems.","sentences":["In the field of robotics and automation, navigation systems based on Large Language Models (LLMs) have recently shown impressive performance.","However, the security aspects of these systems have received relatively less attention.","This paper pioneers the exploration of vulnerabilities in LLM-based navigation models in urban outdoor environments, a critical area given the technology's widespread application in autonomous driving, logistics, and emergency services.","Specifically, we introduce a novel Navigational Prompt Suffix (NPS) Attack that manipulates LLM-based navigation models by appending gradient-derived suffixes to the original navigational prompt, leading to incorrect actions.","We conducted comprehensive experiments on an LLMs-based navigation model that employs various LLMs for reasoning.","Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstrate notable performance declines across three metrics in the face of both white-box and black-box attacks.","These results highlight the generalizability and transferability of the NPS Attack, emphasizing the need for enhanced security in LLM-based navigation systems.","As an initial countermeasure, we propose the Navigational Prompt Engineering (NPE) Defense strategy, concentrating on navigation-relevant keywords to reduce the impact of adversarial suffixes.","While initial findings indicate that this strategy enhances navigational safety, there remains a critical need for the wider research community to develop stronger defense methods to effectively tackle the real-world challenges faced by these systems."],"url":"http://arxiv.org/abs/2402.09546v1","category":"cs.RO"}
{"created":"2024-02-14 19:31:45","title":"Why Does Differential Privacy with Large Epsilon Defend Against Practical Membership Inference Attacks?","abstract":"For small privacy parameter $\\epsilon$, $\\epsilon$-differential privacy (DP) provides a strong worst-case guarantee that no membership inference attack (MIA) can succeed at determining whether a person's data was used to train a machine learning model. The guarantee of DP is worst-case because: a) it holds even if the attacker already knows the records of all but one person in the data set; and b) it holds uniformly over all data sets. In practical applications, such a worst-case guarantee may be overkill: practical attackers may lack exact knowledge of (nearly all of) the private data, and our data set might be easier to defend, in some sense, than the worst-case data set. Such considerations have motivated the industrial deployment of DP models with large privacy parameter (e.g. $\\epsilon \\geq 7$), and it has been observed empirically that DP with large $\\epsilon$ can successfully defend against state-of-the-art MIAs. Existing DP theory cannot explain these empirical findings: e.g., the theoretical privacy guarantees of $\\epsilon \\geq 7$ are essentially vacuous. In this paper, we aim to close this gap between theory and practice and understand why a large DP parameter can prevent practical MIAs. To tackle this problem, we propose a new privacy notion called practical membership privacy (PMP). PMP models a practical attacker's uncertainty about the contents of the private data. The PMP parameter has a natural interpretation in terms of the success rate of a practical MIA on a given data set. We quantitatively analyze the PMP parameter of two fundamental DP mechanisms: the exponential mechanism and Gaussian mechanism. Our analysis reveals that a large DP parameter often translates into a much smaller PMP parameter, which guarantees strong privacy against practical MIAs. Using our findings, we offer principled guidance for practitioners in choosing the DP parameter.","sentences":["For small privacy parameter $\\epsilon$, $\\epsilon$-differential privacy (DP) provides a strong worst-case guarantee that no membership inference attack (MIA) can succeed at determining whether a person's data was used to train a machine learning model.","The guarantee of DP is worst-case because: a) it holds even if the attacker already knows the records of all but one person in the data set; and b) it holds uniformly over all data sets.","In practical applications, such a worst-case guarantee may be overkill: practical attackers may lack exact knowledge of (nearly all of) the private data, and our data set might be easier to defend, in some sense, than the worst-case data set.","Such considerations have motivated the industrial deployment of DP models with large privacy parameter (e.g. $\\epsilon \\geq 7$), and it has been observed empirically that DP with large $\\epsilon$ can successfully defend against state-of-the-art MIAs.","Existing DP theory cannot explain these empirical findings: e.g., the theoretical privacy guarantees of $\\epsilon \\geq 7$ are essentially vacuous.","In this paper, we aim to close this gap between theory and practice and understand why a large DP parameter can prevent practical MIAs.","To tackle this problem, we propose a new privacy notion called practical membership privacy (PMP).","PMP models a practical attacker's uncertainty about the contents of the private data.","The PMP parameter has a natural interpretation in terms of the success rate of a practical MIA on a given data set.","We quantitatively analyze the PMP parameter of two fundamental DP mechanisms: the exponential mechanism and Gaussian mechanism.","Our analysis reveals that a large DP parameter often translates into a much smaller PMP parameter, which guarantees strong privacy against practical MIAs.","Using our findings, we offer principled guidance for practitioners in choosing the DP parameter."],"url":"http://arxiv.org/abs/2402.09540v1","category":"cs.CR"}
{"created":"2024-02-14 19:29:04","title":"Learning From Lessons Learned: Preliminary Findings From a Study of Learning From Failure","abstract":"Due to various sources of uncertainty, emergent behavior, and ongoing changes, the reliability of many socio-technical systems depends on an iterative and collaborative process in which organizations (1) analyze and learn from system failures, and then (2) co-evolve both the technical and human parts of their systems based on what they learn. Many organizations have defined processes for learning from failure, often involving postmortem analyses conducted after any system failures that are judged to be sufficiently severe. Despite established processes and tool support, our preliminary research, and professional experience, suggest that it is not straightforward to take what was learned from a failure and successfully improve the reliability of the socio-technical system. To better understand this collaborative process and the associated challenges, we are conducting a study of how teams learn from failure. We are gathering incident reports from multiple organizations and conducting interviews with engineers and managers with relevant experience. Our analytic interest is in what is learned by teams as they reflect on failures, the learning processes involved, and how they use what is learned. Our data collection and analysis are not yet complete, but we have so far analyzed 13 incident reports and seven interviews. In this short paper we (1) present our preliminary findings, and (2) outline our broader research plans.","sentences":["Due to various sources of uncertainty, emergent behavior, and ongoing changes, the reliability of many socio-technical systems depends on an iterative and collaborative process in which organizations (1) analyze and learn from system failures, and then (2) co-evolve both the technical and human parts of their systems based on what they learn.","Many organizations have defined processes for learning from failure, often involving postmortem analyses conducted after any system failures that are judged to be sufficiently severe.","Despite established processes and tool support, our preliminary research, and professional experience, suggest that it is not straightforward to take what was learned from a failure and successfully improve the reliability of the socio-technical system.","To better understand this collaborative process and the associated challenges, we are conducting a study of how teams learn from failure.","We are gathering incident reports from multiple organizations and conducting interviews with engineers and managers with relevant experience.","Our analytic interest is in what is learned by teams as they reflect on failures, the learning processes involved, and how they use what is learned.","Our data collection and analysis are not yet complete, but we have so far analyzed 13 incident reports and seven interviews.","In this short paper we (1) present our preliminary findings, and (2) outline our broader research plans."],"url":"http://arxiv.org/abs/2402.09538v1","category":"cs.SE"}
{"created":"2024-02-14 19:00:01","title":"Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls","abstract":"Controllable music generation plays a vital role in human-AI music co-creation. While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks. To bridge this gap, we introduce a novel Parameter-Efficient Fine-Tuning (PEFT) method. This approach enables autoregressive language models to seamlessly address music inpainting tasks. Additionally, our PEFT method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement. We apply this method to fine-tune MusicGen, a leading autoregressive music generation model. Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools. A demo page\\footnote{\\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and source codes\\footnote{\\url{https://github.com/Kikyo-16/airgen}.} are available online.","sentences":["Controllable music generation plays a vital role in human-AI music co-creation.","While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks.","To bridge this gap, we introduce a novel Parameter-Efficient Fine-Tuning (PEFT) method.","This approach enables autoregressive language models to seamlessly address music inpainting tasks.","Additionally, our PEFT method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement.","We apply this method to fine-tune MusicGen, a leading autoregressive music generation model.","Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools.","A demo page\\footnote{\\url{https://kikyo-16.github.io/AIR/}.","} showcasing our work and source codes\\footnote{\\url{https://github.com/Kikyo-16/airgen}.} are available online."],"url":"http://arxiv.org/abs/2402.09508v1","category":"cs.SD"}
{"created":"2024-02-14 18:59:37","title":"On Formally Undecidable Traits of Intelligent Machines","abstract":"Building on work by Alfonseca et al. (2021), we study the conditions necessary for it to be logically possible to prove that an arbitrary artificially intelligent machine will exhibit certain behavior. To do this, we develop a formalism like -- but mathematically distinct from -- the theory of formal languages and their properties. Our formalism affords a precise means for not only talking about the traits we desire of machines (such as them being intelligent, contained, moral, and so forth), but also for detailing the conditions necessary for it to be logically possible to decide whether a given arbitrary machine possesses such a trait or not. Contrary to Alfonseca et al.'s (2021) results, we find that Rice's theorem from computability theory cannot in general be used to determine whether an arbitrary machine possesses a given trait or not. Therefore, it is not necessarily the case that deciding whether an arbitrary machine is intelligent, contained, moral, and so forth is logically impossible.","sentences":["Building on work by Alfonseca et al. (2021), we study the conditions necessary for it to be logically possible to prove that an arbitrary artificially intelligent machine will exhibit certain behavior.","To do this, we develop a formalism like -- but mathematically distinct from -- the theory of formal languages and their properties.","Our formalism affords a precise means for not only talking about the traits we desire of machines (such as them being intelligent, contained, moral, and so forth), but also for detailing the conditions necessary for it to be logically possible to decide whether a given arbitrary machine possesses such a trait or not.","Contrary to Alfonseca et al.'s (2021) results, we find that Rice's theorem from computability theory cannot in general be used to determine whether an arbitrary machine possesses a given trait or not.","Therefore, it is not necessarily the case that deciding whether an arbitrary machine is intelligent, contained, moral, and so forth is logically impossible."],"url":"http://arxiv.org/abs/2402.09500v1","category":"cs.AI"}
{"created":"2024-02-14 17:42:24","title":"Neural Networks Asymptotic Behaviours for the Resolution of Inverse Problems","abstract":"This paper presents a study of the effectiveness of Neural Network (NN) techniques for deconvolution inverse problems relevant for applications in Quantum Field Theory, but also in more general contexts. We consider NN's asymptotic limits, corresponding to Gaussian Processes (GPs), where non-linearities in the parameters of the NN can be neglected. Using these resulting GPs, we address the deconvolution inverse problem in the case of a quantum harmonic oscillator simulated through Monte Carlo techniques on a lattice. In this simple toy model, the results of the inversion can be compared with the known analytical solution. Our findings indicate that solving the inverse problem with a NN yields less performing results than those obtained using the GPs derived from NN's asymptotic limits. Furthermore, we observe the trained NN's accuracy approaching that of GPs with increasing layer width. Notably, one of these GPs defies interpretation as a probabilistic model, offering a novel perspective compared to established methods in the literature. Our results suggest the need for detailed studies of the training dynamics in more realistic set-ups.","sentences":["This paper presents a study of the effectiveness of Neural Network (NN) techniques for deconvolution inverse problems relevant for applications in Quantum Field Theory, but also in more general contexts.","We consider NN's asymptotic limits, corresponding to Gaussian Processes (GPs), where non-linearities in the parameters of the NN can be neglected.","Using these resulting GPs, we address the deconvolution inverse problem in the case of a quantum harmonic oscillator simulated through Monte Carlo techniques on a lattice.","In this simple toy model, the results of the inversion can be compared with the known analytical solution.","Our findings indicate that solving the inverse problem with a NN yields less performing results than those obtained using the GPs derived from NN's asymptotic limits.","Furthermore, we observe the trained NN's accuracy approaching that of GPs with increasing layer width.","Notably, one of these GPs defies interpretation as a probabilistic model, offering a novel perspective compared to established methods in the literature.","Our results suggest the need for detailed studies of the training dynamics in more realistic set-ups."],"url":"http://arxiv.org/abs/2402.09338v2","category":"physics.comp-ph"}
{"created":"2024-02-14 16:45:10","title":"Detection of the most influential variables for preventing postpartum urinary incontinence using machine learning techniques","abstract":"Background: Postpartum urinary incontinence (PUI) is a common issue among postnatal women. Previous studies identified potential related variables, but lacked analysis on certain intrinsic and extrinsic patient variables during pregnancy.   Objective: The study aims to evaluate the most influential variables in PUI using machine learning, focusing on intrinsic, extrinsic, and combined variable groups.   Methods: Data from 93 pregnant women were analyzed using machine learning and oversampling techniques. Four key variables were predicted: occurrence, frequency, intensity of urinary incontinence, and stress urinary incontinence.   Results: Models using extrinsic variables were most accurate, with 70% accuracy for urinary incontinence, 77% for frequency, 71% for intensity, and 93% for stress urinary incontinence.   Conclusions: The study highlights extrinsic variables as significant predictors of PUI issues. This suggests that PUI prevention might be achievable through healthy habits during pregnancy, although further research is needed for confirmation.","sentences":["Background: Postpartum urinary incontinence (PUI) is a common issue among postnatal women.","Previous studies identified potential related variables, but lacked analysis on certain intrinsic and extrinsic patient variables during pregnancy.   ","Objective: The study aims to evaluate the most influential variables in PUI using machine learning, focusing on intrinsic, extrinsic, and combined variable groups.   ","Methods: Data from 93 pregnant women were analyzed using machine learning and oversampling techniques.","Four key variables were predicted: occurrence, frequency, intensity of urinary incontinence, and stress urinary incontinence.   ","Results:","Models using extrinsic variables were most accurate, with 70% accuracy for urinary incontinence, 77% for frequency, 71% for intensity, and 93% for stress urinary incontinence.   ","Conclusions: The study highlights extrinsic variables as significant predictors of PUI issues.","This suggests that PUI prevention might be achievable through healthy habits during pregnancy, although further research is needed for confirmation."],"url":"http://arxiv.org/abs/2402.09498v1","category":"cs.AI"}
{"created":"2024-02-14 15:47:46","title":"Instruction Tuning for Secure Code Generation","abstract":"Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30%), while preserving utility.","sentences":["Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming.","An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences.","However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code.","As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks.","In this work, we introduce SafeCoder to address this gap.","SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline.","We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility.","Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets.","It is able to drastically improve security (by about 30%), while preserving utility."],"url":"http://arxiv.org/abs/2402.09497v1","category":"cs.CR"}
{"created":"2024-02-15 18:55:40","title":"On Gibbs measures for almost additive potentials","abstract":"Given an almost additive sequence of continuous functions with bounded variation $\\mathcal{F}=\\{\\log f_n\\}_{n=1}^{\\infty}$ on a subshift $X$ over finitely many symbols, we study properties of a function $f$ on $X$ such that $\\lim_{n\\to\\infty}\\frac{1}{n}\\int \\log f_n d\\mu=\\int f d\\mu$ for every invariant measure $\\mu$ on $X$. Under some conditions we construct a function $f$ on $X$ and study a relation between the property of $\\mathcal{F}$ and some particular types of $f$. We also study the case when $\\mathcal{F}$ is weakly almost additive. As applications we study images of Gibbs measures for continuous functions under one-block factor maps. In order to study conditions for the image to be a Gibbs measure for a continuous function, we investigate a relation between the almost additivity of the sequences associated to relative pressure functions and the fiber-mixing property of a factor map.","sentences":["Given an almost additive sequence of continuous functions with bounded variation $\\mathcal{F}=\\{\\log f_n\\}_{n=1}^{\\infty}$ on a subshift $X$ over finitely many symbols, we study properties of a function $f$ on $X$ such that $\\lim_{n\\to\\infty}\\frac{1}{n}\\int \\log f_n","d\\mu=\\int f d\\mu$ for every invariant measure $\\mu$ on $X$. Under some conditions we construct a function $f$ on $X$ and study a relation between the property of $\\mathcal{F}$ and some particular types of $f$. We also study the case when $\\mathcal{F}$ is weakly almost additive.","As applications we study images of Gibbs measures for continuous functions under one-block factor maps.","In order to study conditions for the image to be a Gibbs measure for a continuous function, we investigate a relation between the almost additivity of the sequences associated to relative pressure functions and the fiber-mixing property of a factor map."],"url":"http://arxiv.org/abs/2402.10199v1","category":"math.DS"}
{"created":"2024-02-15 18:53:29","title":"Bulk universality for complex eigenvalues of real non-symmetric random matrices with i.i.d. entries","abstract":"We consider an ensemble of non-Hermitian matrices with independent identically distributed real entries that have finite moments. We show that its $k$-point correlation function in the bulk away from the real line converges to a universal limit.","sentences":["We consider an ensemble of non-Hermitian matrices with independent identically distributed real entries that have finite moments.","We show that its $k$-point correlation function in the bulk away from the real line converges to a universal limit."],"url":"http://arxiv.org/abs/2402.10197v1","category":"math.PR"}
{"created":"2024-02-15 18:46:24","title":"Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models","abstract":"In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: \\url{https://github.com/lingchen0331/UQ_ICL}.","sentences":["In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt.","However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed.","Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning.","In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty).","We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties.","The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion.","Extensive experiments are conducted to demonstrate the effectiveness of the decomposition.","The code and data are available at: \\url{https://github.com/lingchen0331/UQ_ICL}."],"url":"http://arxiv.org/abs/2402.10189v1","category":"cs.CL"}
{"created":"2024-02-15 18:45:30","title":"Trainability Barriers in Low-Depth QAOA Landscapes","abstract":"The Quantum Alternating Operator Ansatz (QAOA) is a prominent variational quantum algorithm for solving combinatorial optimization problems. Its effectiveness depends on identifying input parameters that yield high-quality solutions. However, understanding the complexity of training QAOA remains an under-explored area. Previous results have given analytical performance guarantees for a small, fixed number of parameters. At the opposite end of the spectrum, barren plateaus are likely to emerge at $\\Omega(n)$ parameters for $n$ qubits. In this work, we study the difficulty of training in the intermediate regime, which is the focus of most current numerical studies and near-term hardware implementations. Through extensive numerical analysis of the quality and quantity of local minima, we argue that QAOA landscapes can exhibit a superpolynomial growth in the number of low-quality local minima even when the number of parameters scales logarithmically with $n$. This means that the common technique of gradient descent from randomly initialized parameters is doomed to fail beyond small $n$, and emphasizes the need for good initial guesses of the optimal parameters.","sentences":["The Quantum Alternating Operator Ansatz (QAOA) is a prominent variational quantum algorithm for solving combinatorial optimization problems.","Its effectiveness depends on identifying input parameters that yield high-quality solutions.","However, understanding the complexity of training QAOA remains an under-explored area.","Previous results have given analytical performance guarantees for a small, fixed number of parameters.","At the opposite end of the spectrum, barren plateaus are likely to emerge at $\\Omega(n)$ parameters for $n$ qubits.","In this work, we study the difficulty of training in the intermediate regime, which is the focus of most current numerical studies and near-term hardware implementations.","Through extensive numerical analysis of the quality and quantity of local minima, we argue that QAOA landscapes can exhibit a superpolynomial growth in the number of low-quality local minima even when the number of parameters scales logarithmically with $n$. This means that the common technique of gradient descent from randomly initialized parameters is doomed to fail beyond small $n$, and emphasizes the need for good initial guesses of the optimal parameters."],"url":"http://arxiv.org/abs/2402.10188v1","category":"quant-ph"}
{"created":"2024-02-15 18:41:34","title":"A coupled VOF/embedded boundary method to model two-phase flows on arbitrary solid surfaces","abstract":"We present an hybrid VOF/embedded boundary method allowing to model two-phase flows in presence of solids with arbitrary shapes. The method relies on the coupling of existing methods: a geometric Volume of fluid (VOF) method to tackle the two-phase flow and an embedded boundary method to sharply resolve arbitrary solid geometries. Coupling these approaches consistently is not trivial and we present in detail a quad/octree spatial discretization for solving the corresponding partial differential equations.   Modelling contact angle dynamics is a complex physical and numerical problem. We present a Navier-slip boundary condition compatible with the present cut cell method, validated through a Taylor-Couette test case.   To impose the boundary condition when the fluid-fluid interface intersects a solid surface, a geometrical contact angle approach is developed. Our method is validated for several test cases including the spreading of a droplet on a cylinder, and the equilibrium shape of a droplet on a flat or tilted plane in 2D and 3D.   The temporal evolution and convergence of the droplet spreading on a flat plane is also discussed for the moving contact line given the boundary condition (Dirichlet or Navier) used.   The ability of our numerical methodology to resolve contact line statics and dynamics for different solid geometries is thus demonstrated.","sentences":["We present an hybrid VOF/embedded boundary method allowing to model two-phase flows in presence of solids with arbitrary shapes.","The method relies on the coupling of existing methods: a geometric Volume of fluid (VOF) method to tackle the two-phase flow and an embedded boundary method to sharply resolve arbitrary solid geometries.","Coupling these approaches consistently is not trivial and we present in detail a quad/octree spatial discretization for solving the corresponding partial differential equations.   ","Modelling contact angle dynamics is a complex physical and numerical problem.","We present a Navier-slip boundary condition compatible with the present cut cell method, validated through a Taylor-Couette test case.   ","To impose the boundary condition when the fluid-fluid interface intersects a solid surface, a geometrical contact angle approach is developed.","Our method is validated for several test cases including the spreading of a droplet on a cylinder, and the equilibrium shape of a droplet on a flat or tilted plane in 2D and 3D.   ","The temporal evolution and convergence of the droplet spreading on a flat plane is also discussed for the moving contact line given the boundary condition (Dirichlet or Navier) used.   ","The ability of our numerical methodology to resolve contact line statics and dynamics for different solid geometries is thus demonstrated."],"url":"http://arxiv.org/abs/2402.10185v1","category":"physics.flu-dyn"}
{"created":"2024-02-15 18:37:33","title":"The QCD phase diagram and Beam Energy Scan physics: a theory overview","abstract":"We review recent theoretical developments relevant to heavy-ion experiments carried out within the Beam Energy Scan program at the Relativistic Heavy Ion Collider. Our main focus is on the description of the dynamics of systems created in heavy-ion collisions and establishing the necessary connection between the experimental observables and the QCD phase diagram.","sentences":["We review recent theoretical developments relevant to heavy-ion experiments carried out within the Beam Energy Scan program at the Relativistic Heavy Ion Collider.","Our main focus is on the description of the dynamics of systems created in heavy-ion collisions and establishing the necessary connection between the experimental observables and the QCD phase diagram."],"url":"http://arxiv.org/abs/2402.10183v1","category":"nucl-th"}
{"created":"2024-02-15 18:14:33","title":"Curves of growth for transiting exocomets: Application to Fe II lines in the Beta Pictoris system","abstract":"This study introduces the exocomet curve of growth, a new method to analyse the variable absorptions observed in $\\beta$ Pictoris spectrum and link them to the physical properties of the transiting cometary tails. We show that the absorption depth of a comet in a set of lines arising from similar excitation levels of a given chemical species follows a simple curve as a function of the gf-values of the lines. This curve is the analogue of the curve of growth for interstellar absorption lines, where equivalent widths are replaced by absorption depths. To fit this exocomet curve of growth, we introduce a model where the cometary absorption is produced by a homogeneous cloud, covering a limited fraction of the stellar disc. This model is defined by two parameters: $\\alpha$, the covering factor of the cloud, and $\\beta$, related to its typical the optical depth. This model is tested on two comets observed with the Hubble Space Telescope in December 1997 and October 2018, in a set of Fe II lines at 275 nm. The measured absorption depths are found to satisfactory match the two-parameter curve of growth model, indicating that both comets cover roughly 40 % of the stellar disc ($\\alpha=0.4$) and have optical thicknesses close to unity. Then, we show that if we consider a set of lines arising from a wider range of energy levels, the absorbing species seems to be populated at thermodynamical equilibrium, causing the cometary absorption to follow a curve of growth as a function of $gf \\cdot e^{-E_l/k_B T}$ (where T is the temperature of the absorbing medium). For the comet observed on December 6, 1997, we derive a temperature of $10500\\pm500$ K and a total Fe II column density of $(1.11\\pm0.09)\\times10^{15}$ cm$^{-2}$. By probing the population of the highest excited energy levels ($E_l\\sim25000$ cm$^{-1}$), we also estimate an electronic density of $(3\\pm1)\\times10^{7}$ cm$^{-3}$.","sentences":["This study introduces the exocomet curve of growth, a new method to analyse the variable absorptions observed in $\\beta$ Pictoris spectrum and link them to the physical properties of the transiting cometary tails.","We show that the absorption depth of a comet in a set of lines arising from similar excitation levels of a given chemical species follows a simple curve as a function of the gf-values of the lines.","This curve is the analogue of the curve of growth for interstellar absorption lines, where equivalent widths are replaced by absorption depths.","To fit this exocomet curve of growth, we introduce a model where the cometary absorption is produced by a homogeneous cloud, covering a limited fraction of the stellar disc.","This model is defined by two parameters: $\\alpha$, the covering factor of the cloud, and $\\beta$, related to its typical the optical depth.","This model is tested on two comets observed with the Hubble Space Telescope in December 1997 and October 2018, in a set of Fe II lines at 275 nm.","The measured absorption depths are found to satisfactory match the two-parameter curve of growth model, indicating that both comets cover roughly 40 % of the stellar disc ($\\alpha=0.4$) and have optical thicknesses close to unity.","Then, we show that if we consider a set of lines arising from a wider range of energy levels, the absorbing species seems to be populated at thermodynamical equilibrium, causing the cometary absorption to follow a curve of growth as a function of $gf \\cdot e^{-E_l/k_B T}$","(where T is the temperature of the absorbing medium).","For the comet observed on December 6, 1997, we derive a temperature of $10500\\pm500$ K and a total Fe II column density of $(1.11\\pm0.09)\\times10^{15}$ cm$^{-2}$. By probing the population of the highest excited energy levels ($E_l\\sim25000$ cm$^{-1}$), we also estimate an electronic density of $(3\\pm1)\\times10^{7}$ cm$^{-3}$."],"url":"http://arxiv.org/abs/2402.10169v1","category":"astro-ph.EP"}
{"created":"2024-02-15 18:10:52","title":"A geometrical interpretation of critical exponents","abstract":"We develop the hypothesis that the dynamics of a given system may lead to the activity being constricted to a subset of space, characterized by a fractal dimension smaller than the space dimension. We also address how the response function might be sensitive to this change in dimensionality. We discuss how this phenomenon is observable in growth processes and near critical points for systems in equilibrium. In particular, we determine the fractal dimension $d_f$ for the Ising model and validate it via computer simulations for two dimensions.","sentences":["We develop the hypothesis that the dynamics of a given system may lead to the activity being constricted to a subset of space, characterized by a fractal dimension smaller than the space dimension.","We also address how the response function might be sensitive to this change in dimensionality.","We discuss how this phenomenon is observable in growth processes and near critical points for systems in equilibrium.","In particular, we determine the fractal dimension $d_f$ for the Ising model and validate it via computer simulations for two dimensions."],"url":"http://arxiv.org/abs/2402.10167v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-15 18:10:30","title":"Accuracy of discrete- and continuous-time mean-field theories for epidemic processes on complex networks","abstract":"Discrete- and continuous-time approaches are frequently used to model the role of heterogeneity on dynamical interacting agents on the top of complex networks. While, on the one hand, one does not expect drastic differences between these approaches, and the choice is usually based on one's expertise or methodological convenience, on the other hand, a detailed analysis of the differences is necessary to guide the proper choice of one or another approach. We tackle this problem, by comparing both discrete- and continuous-time mean-field theories for the susceptible-infected-susceptible (SIS) epidemic model on random networks with power-law degree distributions. We compare the discrete epidemic link equations (ELE) and continuous pair quenched mean-field (PQMF) theories with the corresponding stochastic simulations, both theories that reckon pairwise interactions explicitly. We show that ELE converges to PQMF theory when the time step goes to zero. The epidemic localization analysis performed reckoning the inverse participation ratio (IPR) indicates that both theories present the same localization depence on the network degree exponent $\\gamma$: for $\\gamma<5/2$ the epidemic is localized on the maximum k-core of network with a vanishing in the infinite-size limit while for $\\gamma>5/2$, the localization happens on hubs what leads to a finite value of IPR. However, the IPR and epidemic threshold of ELE depend on the time-step discretization such that a larger time-step leads to more localized epidemics. A remarkable difference between discrete and continuous time approaches is revealed in the epidemic prevalence near the epidemic threshold, in which the discrete-time stochastic simulations indicate a mean-field critical exponent $\\theta=1$ instead of the value $\\theta=1/(3-\\gamma)$ obtained rigorously and verified numerically for the continuous-time SIS on the same networks.","sentences":["Discrete- and continuous-time approaches are frequently used to model the role of heterogeneity on dynamical interacting agents on the top of complex networks.","While, on the one hand, one does not expect drastic differences between these approaches, and the choice is usually based on one's expertise or methodological convenience, on the other hand, a detailed analysis of the differences is necessary to guide the proper choice of one or another approach.","We tackle this problem, by comparing both discrete- and continuous-time mean-field theories for the susceptible-infected-susceptible (SIS) epidemic model on random networks with power-law degree distributions.","We compare the discrete epidemic link equations (ELE) and continuous pair quenched mean-field (PQMF) theories with the corresponding stochastic simulations, both theories that reckon pairwise interactions explicitly.","We show that ELE converges to PQMF theory when the time step goes to zero.","The epidemic localization analysis performed reckoning the inverse participation ratio (IPR) indicates that both theories present the same localization depence on the network degree exponent $\\gamma$: for $\\gamma<5/2$ the epidemic is localized on the maximum k-core of network with a vanishing in the infinite-size limit while for $\\gamma>5/2$, the localization happens on hubs what leads to a finite value of IPR.","However, the IPR and epidemic threshold of ELE depend on the time-step discretization such that a larger time-step leads to more localized epidemics.","A remarkable difference between discrete and continuous time approaches is revealed in the epidemic prevalence near the epidemic threshold, in which the discrete-time stochastic simulations indicate a mean-field critical exponent $\\theta=1$ instead of the value $\\theta=1/(3-\\gamma)$ obtained rigorously and verified numerically for the continuous-time SIS on the same networks."],"url":"http://arxiv.org/abs/2402.10166v1","category":"physics.soc-ph"}
{"created":"2024-02-15 18:04:04","title":"The double low-mass white dwarf eclipsing binary system J2102-4145 and its possible evolution","abstract":"Approximately 150 low-mass white dwarfs, with masses below 0.4Msun, have been discovered. The majority of these low-mass WDs are observed in binary systems as they cannot be formed through single-star evolution within the Hubble time. In this study, we present a comprehensive analysis of the double low-mass WD eclipsing binary system J2102-4145. Our investigation involved an extensive observational campaign, resulting in the acquisition of approximately 28 hours of high-speed photometric data across multiple nights using NTT/ULTRACAM, SOAR/Goodman, and SMARTS-1m telescopes. These observations have provided critical insights into the orbital characteristics of this system, including parameters such as inclination and orbital period. To disentangle the binary components of J2102-4145, we employed the XT GRID spectral fitting method with GMOS/Gemini-South and X-Shooter data. Additionally, we used the PHOEBE package for light curve analysis on NTT/ULTRACAM high-speed time-series photometry data to constrain the binary star properties. Our analysis reveals remarkable similarities between the two components of this binary system. For the primary star, we determined Teff1 = 13688 +- 65 K, log g1 = 7.36 +- 0.01, R1 = 0.0211 +- 0.0002 Rsun, and M1 = 0.375 +- 0.003 Msun, while the secondary star is characterized by Teff2 = 12952 +- 53 K, log g2 = 7.32 +- 0.01, R2 = 0.0203 +- 0.0002 Rsun, and M2 = 0.31 +- 0.003 Msun. Furthermore, we observe a notable discrepancy between Teff and R of the less massive WD compared to evolutionary sequences for WDs from the literature, which has significant implications for our understanding of WD evolution. We discuss a potential formation scenario for this system that might explain this discrepancy and explore its future evolution. We predict that this system will merge in about 800 Myr, evolving into a helium-rich hot subdwarf star and later into a hybrid He/CO WD.","sentences":["Approximately 150 low-mass white dwarfs, with masses below 0.4Msun, have been discovered.","The majority of these low-mass WDs are observed in binary systems as they cannot be formed through single-star evolution within the Hubble time.","In this study, we present a comprehensive analysis of the double low-mass WD eclipsing binary system J2102-4145.","Our investigation involved an extensive observational campaign, resulting in the acquisition of approximately 28 hours of high-speed photometric data across multiple nights using NTT/ULTRACAM, SOAR/Goodman, and SMARTS-1m telescopes.","These observations have provided critical insights into the orbital characteristics of this system, including parameters such as inclination and orbital period.","To disentangle the binary components of J2102-4145, we employed the XT GRID spectral fitting method with GMOS/Gemini-South and X-Shooter data.","Additionally, we used the PHOEBE package for light curve analysis on NTT/ULTRACAM high-speed time-series photometry data to constrain the binary star properties.","Our analysis reveals remarkable similarities between the two components of this binary system.","For the primary star, we determined Teff1 = 13688 +- 65 K, log g1 = 7.36 +- 0.01, R1 = 0.0211 +- 0.0002 Rsun, and M1 = 0.375 +- 0.003 Msun, while the secondary star is characterized by Teff2 = 12952 +- 53 K, log g2 = 7.32 +- 0.01, R2 = 0.0203 +- 0.0002 Rsun, and M2 = 0.31 +- 0.003 Msun.","Furthermore, we observe a notable discrepancy between Teff and R of the less massive WD compared to evolutionary sequences for WDs from the literature, which has significant implications for our understanding of WD evolution.","We discuss a potential formation scenario for this system that might explain this discrepancy and explore its future evolution.","We predict that this system will merge in about 800 Myr, evolving into a helium-rich hot subdwarf star and later into a hybrid He/CO WD."],"url":"http://arxiv.org/abs/2402.10159v1","category":"astro-ph.SR"}
{"created":"2024-02-15 18:02:32","title":"Revisiting Stochastic Realization Theory using Functional It\u00f4 Calculus","abstract":"This paper considers the problem of constructing finite-dimensional state space realizations for stochastic processes that can be represented as the outputs of a certain type of a causal system driven by a continuous semimartingale input process. The main assumption is that the output process is infinitely differentiable, where the notion of differentiability comes from the functional It\\^o calculus introduced by Dupire as a causal (nonanticipative) counterpart to Malliavin's stochastic calculus of variations. The proposed approach builds on the ideas of Hijab, who had considered the case of processes driven by a Brownian motion, and makes contact with the realization theory of deterministic systems based on formal power series and Chen-Fliess functional expansions.","sentences":["This paper considers the problem of constructing finite-dimensional state space realizations for stochastic processes that can be represented as the outputs of a certain type of a causal system driven by a continuous semimartingale input process.","The main assumption is that the output process is infinitely differentiable, where the notion of differentiability comes from the functional It\\^o calculus introduced by Dupire as a causal (nonanticipative) counterpart to Malliavin's stochastic calculus of variations.","The proposed approach builds on the ideas of Hijab, who had considered the case of processes driven by a Brownian motion, and makes contact with the realization theory of deterministic systems based on formal power series and Chen-Fliess functional expansions."],"url":"http://arxiv.org/abs/2402.10157v1","category":"math.OC"}
{"created":"2024-02-15 18:00:39","title":"Understanding and tuning magnetism in layered Ising-type antiferromagnet FePSe3 for potential 2D magnet","abstract":"Recent development in two-dimensional (2D) magnetic materials have motivated the search for new van der Waals magnetic materials, especially Ising-type magnets with strong magnetic anisotropy. Fe-based MPX3 (M = transition metal, X = chalcogen) compounds such as FePS3 and FePSe3 both exhibit an Ising-type magnetic order, but FePSe3 receives much less attention compared to FePS3. This work focuses on establishing the strategy to engineer magnetic anisotropy and exchange interactions in this less-explored compound. Through chalcogen and metal substitutions, the magnetic anisotropy is found to be immune against S substitution for Se whereas tunable only with heavy Mn substitution for Fe. In particular, Mn substitution leads to a continuous rotation of magnetic moments from the out-of-plane direction towards in-plane. Furthermore, the magnetic ordering temperature displays non-monotonic doping dependence for both chalcogen and metal substitutions but due to different mechanisms. These findings provide deeper insight into the Ising-type magnetism in this important van der Waals material, shedding light on the study of other Ising-type magnetic systems as well as discovering novel 2D magnets for potential applications in spintronics.","sentences":["Recent development in two-dimensional (2D) magnetic materials have motivated the search for new van der Waals magnetic materials, especially Ising-type magnets with strong magnetic anisotropy.","Fe-based MPX3 (M = transition metal, X = chalcogen) compounds such as FePS3 and FePSe3 both exhibit an Ising-type magnetic order, but FePSe3 receives much less attention compared to FePS3.","This work focuses on establishing the strategy to engineer magnetic anisotropy and exchange interactions in this less-explored compound.","Through chalcogen and metal substitutions, the magnetic anisotropy is found to be immune against S substitution for Se whereas tunable only with heavy Mn substitution for Fe.","In particular, Mn substitution leads to a continuous rotation of magnetic moments from the out-of-plane direction towards in-plane.","Furthermore, the magnetic ordering temperature displays non-monotonic doping dependence for both chalcogen and metal substitutions but due to different mechanisms.","These findings provide deeper insight into the Ising-type magnetism in this important van der Waals material, shedding light on the study of other Ising-type magnetic systems as well as discovering novel 2D magnets for potential applications in spintronics."],"url":"http://arxiv.org/abs/2402.10155v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-15 17:49:02","title":"Emergent topological quasiparticle kinetics in constricted nanomagnets","abstract":"The ubiquitous domain wall kinetics under magnetic field or current application describes the dynamic properties in nanostructured magnets. However, when the geometrical size of a nanomagnetic system is constricted to the limiting domain wall length scale, the competing energetics between anisotropy, exchange and dipolar interactions can cause emergent kinetics due to quasiparticle relaxation, similar to bulk magnets of atomic origin. Here, we present a joint experimental and theoretical study to support this argument -- constricted nanomagnets, made of antiferromagnetic and paramagnetic neodymium thin film with honeycomb motif, reveal fast kinetic events at ps time scales due to the relaxation of chiral vortex loop-shaped topological quasiparticles that persist to low temperature in the absence of any external stimuli. Such phenomena are typically found in macroscopic magnetic materials. Our discovery is especially important considering the fact that paramagnets or antiferromagnets have no net magnetization. Yet, the kinetics in neodymium nanostructures is quantitatively similar to that found in ferromagnetic counterparts and only varies with the thickness of the specimen. This suggests that a universal, topological quasiparticle mediated dynamical behavior can be prevalent in nanoscopic magnets, irrespective of the nature of underlying magnetic material.","sentences":["The ubiquitous domain wall kinetics under magnetic field or current application describes the dynamic properties in nanostructured magnets.","However, when the geometrical size of a nanomagnetic system is constricted to the limiting domain wall length scale, the competing energetics between anisotropy, exchange and dipolar interactions can cause emergent kinetics due to quasiparticle relaxation, similar to bulk magnets of atomic origin.","Here, we present a joint experimental and theoretical study to support this argument -- constricted nanomagnets, made of antiferromagnetic and paramagnetic neodymium thin film with honeycomb motif, reveal fast kinetic events at ps time scales due to the relaxation of chiral vortex loop-shaped topological quasiparticles that persist to low temperature in the absence of any external stimuli.","Such phenomena are typically found in macroscopic magnetic materials.","Our discovery is especially important considering the fact that paramagnets or antiferromagnets have no net magnetization.","Yet, the kinetics in neodymium nanostructures is quantitatively similar to that found in ferromagnetic counterparts and only varies with the thickness of the specimen.","This suggests that a universal, topological quasiparticle mediated dynamical behavior can be prevalent in nanoscopic magnets, irrespective of the nature of underlying magnetic material."],"url":"http://arxiv.org/abs/2402.10143v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-15 17:46:40","title":"Linking Through Time: Memory-Enhanced Community Discovery in Temporal Networks","abstract":"Temporal Networks, and more specifically, Markovian Temporal Networks, present a unique challenge regarding the community discovery task. The inherent dynamism of these systems requires an intricate understanding of memory effects and structural heterogeneity, which are often key drivers of network evolution. In this study, we address these aspects by introducing an innovative approach to community detection, centered around a novel modularity function. We focus on demonstrating the improvements our new approach brings to a fundamental aspect of community detection: the detectability threshold problem. We show that by associating memory directly with nodes' memberships and considering it in the expression of the modularity, the detectability threshold can be lowered with respect to cases where memory is not considered, thereby enhancing the quality of the communities discovered. To validate our approach, we carry out extensive numerical simulations, assessing the effectiveness of our method in a controlled setting. Furthermore, we apply our method to real-world data to underscore its practicality and robustness. This application not only demonstrates the method's effectiveness but also reveals its capacity to indirectly tackle additional challenges, such as determining the optimal time window for aggregating data in dynamic graphs. This illustrates the method's versatility in addressing complex aspects of temporal network analysis.","sentences":["Temporal Networks, and more specifically, Markovian Temporal Networks, present a unique challenge regarding the community discovery task.","The inherent dynamism of these systems requires an intricate understanding of memory effects and structural heterogeneity, which are often key drivers of network evolution.","In this study, we address these aspects by introducing an innovative approach to community detection, centered around a novel modularity function.","We focus on demonstrating the improvements our new approach brings to a fundamental aspect of community detection: the detectability threshold problem.","We show that by associating memory directly with nodes' memberships and considering it in the expression of the modularity, the detectability threshold can be lowered with respect to cases where memory is not considered, thereby enhancing the quality of the communities discovered.","To validate our approach, we carry out extensive numerical simulations, assessing the effectiveness of our method in a controlled setting.","Furthermore, we apply our method to real-world data to underscore its practicality and robustness.","This application not only demonstrates the method's effectiveness but also reveals its capacity to indirectly tackle additional challenges, such as determining the optimal time window for aggregating data in dynamic graphs.","This illustrates the method's versatility in addressing complex aspects of temporal network analysis."],"url":"http://arxiv.org/abs/2402.10141v1","category":"physics.soc-ph"}
{"created":"2024-02-15 17:43:22","title":"Fast interpolation and multiplication of unbalanced polynomials","abstract":"We consider the classical problems of interpolating a polynomial given a black box for evaluation, and of multiplying two polynomials, in the setting where the bit-lengths of the coefficients may vary widely, so-called unbalanced polynomials. Writing s for the total bit-length and D for the degree, our new algorithms have expected running time $\\tilde{O}(s \\log D)$, whereas previous methods for (resp.) dense or sparse arithmetic have at least $\\tilde{O}(sD)$ or $\\tilde{O}(s^2)$ bit complexity.","sentences":["We consider the classical problems of interpolating a polynomial given a black box for evaluation, and of multiplying two polynomials, in the setting where the bit-lengths of the coefficients may vary widely, so-called unbalanced polynomials.","Writing s for the total bit-length and D for the degree, our new algorithms have expected running time $\\tilde{O}(s \\log D)$, whereas previous methods for (resp.)","dense or sparse arithmetic have at least $\\tilde{O}(sD)$ or $\\tilde{O}(s^2)$ bit complexity."],"url":"http://arxiv.org/abs/2402.10139v1","category":"cs.SC"}
{"created":"2024-02-15 17:43:13","title":"Transaction Capacity, Security and Latency in Blockchains","abstract":"We analyze how secure a block is after the block becomes k-deep, i.e., security-latency, for Nakamoto consensus under an exponential network delay model. We give parameter regimes for which transactions are safe when sufficiently deep in the chain. We compare our results for Nakamoto consensus under bounded network delay models and obtain analogous bounds for safety violation threshold. Next, modeling the blockchain system as a batch service queue with exponential network delay, we connect the security-latency analysis to sustainable transaction rate of the queue system. As our model assumes exponential network delay, batch service queue models give a meaningful trade-off between transaction capacity, security and latency. As adversary can attack the queue service to hamper the service process, we consider two different attacks for adversary. In an extreme scenario, we modify the selfish-mining attack for this purpose and consider its effect on the sustainable transaction rate of the queue.","sentences":["We analyze how secure a block is after the block becomes k-deep, i.e., security-latency, for Nakamoto consensus under an exponential network delay model.","We give parameter regimes for which transactions are safe when sufficiently deep in the chain.","We compare our results for Nakamoto consensus under bounded network delay models and obtain analogous bounds for safety violation threshold.","Next, modeling the blockchain system as a batch service queue with exponential network delay, we connect the security-latency analysis to sustainable transaction rate of the queue system.","As our model assumes exponential network delay, batch service queue models give a meaningful trade-off between transaction capacity, security and latency.","As adversary can attack the queue service to hamper the service process, we consider two different attacks for adversary.","In an extreme scenario, we modify the selfish-mining attack for this purpose and consider its effect on the sustainable transaction rate of the queue."],"url":"http://arxiv.org/abs/2402.10138v1","category":"cs.CR"}
{"created":"2024-02-15 17:38:58","title":"Birational complexity and dual complexes","abstract":"We introduce the notion of birational complexity of a log Calabi-Yau pair. This invariant measures how far the log Calabi-Yau pair is to being birational to a toric pair. We study fundamental properties of the new invariant, with a particular focus on the geometry of dual complexes.","sentences":["We introduce the notion of birational complexity of a log Calabi-Yau pair.","This invariant measures how far the log Calabi-Yau pair is to being birational to a toric pair.","We study fundamental properties of the new invariant, with a particular focus on the geometry of dual complexes."],"url":"http://arxiv.org/abs/2402.10136v1","category":"math.AG"}
{"created":"2024-02-15 17:21:35","title":"The role of Fock-space correlations in many-body localization","abstract":"Models of many-body localization (MBL) can be represented as tight-binding models in the many-body Hilbert space (Fock space). We explore the role of correlations between matrix elements of the effective Fock-space Hamiltonians in the scaling of MBL critical disorder $W_c(n)$ with the size $n$ of the system. For this purpose, we consider five models, which all have the same distributions of diagonal (energy) and off-diagonal (\"hopping\") Fock-space matrix elements but different Fock-space correlations. These include quantum-dot (QD) and one-dimensional (1D) MBL models, their modifications (uQD and u1D models) with removed correlations of off-diagonal matrix elements, as well a quantum random energy model (QREM) with no correlations at all. Our numerical results are in full consistency with analytical arguments predicting $n^{3/4} (\\ln n)^{-1/4} \\lesssim W_c \\lesssim n \\ln n$ for the scaling of $W_c(n)$ in the QD model (we find $W_c \\sim n$ numerically), $W_c(n) \\sim \\text{const.}$ for the 1D model, $W_c \\sim n \\ln n$ for the uQD and u1D models without off-diagonal correlations, and $W_c \\sim n^{1/2} \\ln n$ for QREM. The key difference between the QD and 1D models is in the structure of correlations of many-body energies. Removing off-diagonal Fock-space correlations makes both these models \"maximally chaotic\". Our findings demonstrate that the scaling of $W_c(n)$ for MBL transitions is governed by a combined effect of Fock-space correlations of diagonal and off-diagonal matrix elements.","sentences":["Models of many-body localization (MBL) can be represented as tight-binding models in the many-body Hilbert space (Fock space).","We explore the role of correlations between matrix elements of the effective Fock-space Hamiltonians in the scaling of MBL critical disorder $W_c(n)$ with the size $n$ of the system.","For this purpose, we consider five models, which all have the same distributions of diagonal (energy) and off-diagonal (\"hopping\")","Fock-space matrix elements but different Fock-space correlations.","These include quantum-dot (QD) and one-dimensional (1D) MBL models, their modifications (uQD and u1D models) with removed correlations of off-diagonal matrix elements, as well a quantum random energy model (QREM) with no correlations at all.","Our numerical results are in full consistency with analytical arguments predicting $n^{3/4} (\\ln n)^{-1/4} \\lesssim W_c \\lesssim n \\ln n$ for the scaling of $W_c(n)$ in the QD model (we find $W_c \\sim n$ numerically), $W_c(n) \\sim \\text{const.}$ for the 1D model, $W_c \\sim n \\ln n$ for the uQD and u1D models without off-diagonal correlations, and $W_c \\sim n^{1/2} \\ln n$ for QREM.","The key difference between the QD and 1D models is in the structure of correlations of many-body energies.","Removing off-diagonal Fock-space correlations makes both these models \"maximally chaotic\".","Our findings demonstrate that the scaling of $W_c(n)$ for MBL transitions is governed by a combined effect of Fock-space correlations of diagonal and off-diagonal matrix elements."],"url":"http://arxiv.org/abs/2402.10123v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-15 17:08:32","title":"Amenable group actions on $L_p$ lattices","abstract":"A result by Ornstein and Weiss states that a free and measure-preserving action of an amenable group on a probability space yields a decomposition of the space in disjoint images, up to a small error, analogous to the one given by the Rokhlin lemma in the case of a single transformation. We generalise this result to non-singular actions, and use it to prove that the theory of an action by automorphisms of an amenable group on a Banach $L_p$ lattice admits a model companion, which is stable and has quantifier elimination.","sentences":["A result by Ornstein and Weiss states that a free and measure-preserving action of an amenable group on a probability space yields a decomposition of the space in disjoint images, up to a small error, analogous to the one given by the Rokhlin lemma in the case of a single transformation.","We generalise this result to non-singular actions, and use it to prove that the theory of an action by automorphisms of an amenable group on a Banach $L_p$ lattice admits a model companion, which is stable and has quantifier elimination."],"url":"http://arxiv.org/abs/2402.10112v1","category":"math.DS"}
{"created":"2024-02-15 16:51:38","title":"Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling","abstract":"Federated Learning (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency. While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks. In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation. We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme. Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while considering both the data and system heterogeneity. Experimental results under practical wireless network settings with real-world prototype demonstrate that the proposed independent sampling scheme substantially outperforms the current best sampling schemes under various training models and datasets.","sentences":["Federated Learning (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency.","While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks.","In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation.","We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme.","Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while considering both the data and system heterogeneity.","Experimental results under practical wireless network settings with real-world prototype demonstrate that the proposed independent sampling scheme substantially outperforms the current best sampling schemes under various training models and datasets."],"url":"http://arxiv.org/abs/2402.10097v1","category":"cs.LG"}
{"created":"2024-02-15 16:50:04","title":"Quantum Linear Magnetoresistance and Fermi Liquid Behavior in Kagome Metal Ni3In2S2","abstract":"Kagome metals gain attention as they manifest a spectrum of quantum phenomena, including superconductivity, charge order, frustrated magnetism, and intertwined correlated states of condensed matter. With regard to electronic band structure, several of the them exhibit non-trivial topological characteristics. Here, we present a thorough investigation on the growth and the physical properties of single crystals of Ni3In2S2 which is established to be a Dirac nodal line Kagome metal. Extensive characterization is attained through temperature and field-dependent resistivity, angle-dependent magnetoresistance and specific heat measurements. In most metals, the Fermi liquid behaviour is mostly restricted to a narrow range of temperature. In Ni3In2S2, this characteristic feature has been observed for an extensive temperature range of 82 K. This is attributed to the strong electron-electron correlation in the material. Specific heat measurements reveal a high Kadowaki-Woods ratio which is in good agreement with strongly correlated systems. Almost linear positive magnetoresistance follows the conventional Kohler scaling which depicts the applicability of semi-classical theories. The angle-dependent magneto-resistance been explained using the Voigt-Thomson formula. Furthermore, de-Haas van Alphen oscillations are observed in magnetization vs. magnetic field measurement which shed light on the topological features in the Shandite Ni3In2S2.","sentences":["Kagome metals gain attention as they manifest a spectrum of quantum phenomena, including superconductivity, charge order, frustrated magnetism, and intertwined correlated states of condensed matter.","With regard to electronic band structure, several of the them exhibit non-trivial topological characteristics.","Here, we present a thorough investigation on the growth and the physical properties of single crystals of Ni3In2S2 which is established to be a Dirac nodal line Kagome metal.","Extensive characterization is attained through temperature and field-dependent resistivity, angle-dependent magnetoresistance and specific heat measurements.","In most metals, the Fermi liquid behaviour is mostly restricted to a narrow range of temperature.","In Ni3In2S2, this characteristic feature has been observed for an extensive temperature range of 82 K. This is attributed to the strong electron-electron correlation in the material.","Specific heat measurements reveal a high Kadowaki-Woods ratio which is in good agreement with strongly correlated systems.","Almost linear positive magnetoresistance follows the conventional Kohler scaling which depicts the applicability of semi-classical theories.","The angle-dependent magneto-resistance been explained using the Voigt-Thomson formula.","Furthermore, de-Haas van Alphen oscillations are observed in magnetization vs. magnetic field measurement which shed light on the topological features in the Shandite Ni3In2S2."],"url":"http://arxiv.org/abs/2402.10096v1","category":"cond-mat.str-el"}
{"created":"2024-02-15 16:34:11","title":"Approximate Message Passing-Enhanced Graph Neural Network for OTFS Data Detection","abstract":"Orthogonal time frequency space (OTFS) modulation has emerged as a promising solution to support high-mobility wireless communications, for which, cost-effective data detectors are critical. Although graph neural network (GNN)-based data detectors can achieve decent detection accuracy at reasonable computation cost, they fail to best harness prior information of transmitted data. To further minimize the data detection error of OTFS systems, this letter develops an AMP-GNN-based detector, leveraging the approximate message passing (AMP) algorithm to iteratively improve the symbol estimates of a GNN. Given the inter-Doppler interference (IDI) symbols incur substantial computational overhead to the constructed GNN, learning-based IDI approximation is implemented to sustain low detection complexity. Simulation results demonstrate a remarkable bit error rate (BER) performance achieved by the proposed AMP-GNN-based detector compared to existing baselines. Meanwhile, the proposed IDI approximation scheme avoids a large amount of computations with negligible BER degradation.","sentences":["Orthogonal time frequency space (OTFS) modulation has emerged as a promising solution to support high-mobility wireless communications, for which, cost-effective data detectors are critical.","Although graph neural network (GNN)-based data detectors can achieve decent detection accuracy at reasonable computation cost, they fail to best harness prior information of transmitted data.","To further minimize the data detection error of OTFS systems, this letter develops an AMP-GNN-based detector, leveraging the approximate message passing (AMP) algorithm to iteratively improve the symbol estimates of a GNN.","Given the inter-Doppler interference (IDI) symbols incur substantial computational overhead to the constructed GNN, learning-based IDI approximation is implemented to sustain low detection complexity.","Simulation results demonstrate a remarkable bit error rate (BER) performance achieved by the proposed AMP-GNN-based detector compared to existing baselines.","Meanwhile, the proposed IDI approximation scheme avoids a large amount of computations with negligible BER degradation."],"url":"http://arxiv.org/abs/2402.10071v1","category":"eess.SP"}
{"created":"2024-02-15 16:33:55","title":"Cohomology groups with compact support for flat line bundles on certain complex Lie groups","abstract":"Let $X$ be a complex surface obtained as the quotient of the complex Euclidean space $\\mathbb{C}^2$ by a discrete subgroup of rank $3$. We investigate the cohomology group $H_0^1(X, E)$ with compact support for a unitary flat line bundle $E$ over $X$. We show the vanishing of $H_0^1(X, E)$ for a certain class of such pairs $(X, E)$, which includes infinitely many examples such that $H^1(X, E)$ is non-Hausdorff and infinite dimensional.","sentences":["Let $X$ be a complex surface obtained as the quotient of the complex Euclidean space $\\mathbb{C}^2$ by a discrete subgroup of rank $3$. We investigate the cohomology group $H_0^1(X, E)$ with compact support for a unitary flat line bundle $E$ over $X$. We show the vanishing of $H_0^1(X, E)$ for a certain class of such pairs $(X, E)$, which includes infinitely many examples such that $H^1(X, E)$ is non-Hausdorff and infinite dimensional."],"url":"http://arxiv.org/abs/2402.10068v1","category":"math.CV"}
{"created":"2024-02-15 16:29:46","title":"X-maps: Direct Depth Lookup for Event-based Structured Light Systems","abstract":"We present a new approach to direct depth estimation for Spatial Augmented Reality (SAR) applications using event cameras. These dynamic vision sensors are a great fit to be paired with laser projectors for depth estimation in a structured light approach. Our key contributions involve a conversion of the projector time map into a rectified X-map, capturing x-axis correspondences for incoming events and enabling direct disparity lookup without any additional search. Compared to previous implementations, this significantly simplifies depth estimation, making it more efficient, while the accuracy is similar to the time map-based process. Moreover, we compensate non-linear temporal behavior of cheap laser projectors by a simple time map calibration, resulting in improved performance and increased depth estimation accuracy. Since depth estimation is executed by two lookups only, it can be executed almost instantly (less than 3 ms per frame with a Python implementation) for incoming events. This allows for real-time interactivity and responsiveness, which makes our approach especially suitable for SAR experiences where low latency, high frame rates and direct feedback are crucial. We present valuable insights gained into data transformed into X-maps and evaluate our depth from disparity estimation against the state of the art time map-based results. Additional results and code are available on our project page: https://fraunhoferhhi.github.io/X-maps/","sentences":["We present a new approach to direct depth estimation for Spatial Augmented Reality (SAR) applications using event cameras.","These dynamic vision sensors are a great fit to be paired with laser projectors for depth estimation in a structured light approach.","Our key contributions involve a conversion of the projector time map into a rectified X-map, capturing x-axis correspondences for incoming events and enabling direct disparity lookup without any additional search.","Compared to previous implementations, this significantly simplifies depth estimation, making it more efficient, while the accuracy is similar to the time map-based process.","Moreover, we compensate non-linear temporal behavior of cheap laser projectors by a simple time map calibration, resulting in improved performance and increased depth estimation accuracy.","Since depth estimation is executed by two lookups only, it can be executed almost instantly (less than 3 ms per frame with a Python implementation) for incoming events.","This allows for real-time interactivity and responsiveness, which makes our approach especially suitable for SAR experiences where low latency, high frame rates and direct feedback are crucial.","We present valuable insights gained into data transformed into X-maps and evaluate our depth from disparity estimation against the state of the art time map-based results.","Additional results and code are available on our project page: https://fraunhoferhhi.github.io/X-maps/"],"url":"http://arxiv.org/abs/2402.10061v1","category":"cs.CV"}
{"created":"2024-02-15 16:24:21","title":"Two optimization problems for the Loewner energy","abstract":"A Jordan curve on the Riemann sphere can be encoded by its conformal welding, a circle homeomorphism. The Loewner energy measures how far a Jordan curve is away from being a circle, or equivalently, how far its welding homeomorphism is away from being a M\\\"obius transformation. We consider two optimizing problems for the Loewner energy, one under the constraint for the curves to pass through $n$ given points on the Riemann sphere, which is the conformal boundary of hyperbolic $3$-space $\\mathbb H^3$; the other under the constraint for $n$ given points on the circle to be welded to another $n$ given points of the circle. The latter problem can be viewed as optimizing space-like curves on the boundary of AdS$^3$ space passing through $n$ prescribed points. We observe that the answers to the two problems exhibit interesting symmetries: optimizing the Jordan curve in $\\partial_\\infty \\mathbb H^3$ gives rise to a welding homeomorphism that is the boundary of a pleated plane in AdS$^3$, whereas optimizing the space-like curve in $\\partial_\\infty\\!\\operatorname{AdS}^3$ gives rise to a Jordan curve that is the boundary of a pleated plane in $\\mathbb H^3$.","sentences":["A Jordan curve on the Riemann sphere can be encoded by its conformal welding, a circle homeomorphism.","The Loewner energy measures how far a Jordan curve is away from being a circle, or equivalently, how far its welding homeomorphism is away from being a M\\\"obius transformation.","We consider two optimizing problems for the Loewner energy, one under the constraint for the curves to pass through $n$ given points on the Riemann sphere, which is the conformal boundary of hyperbolic $3$-space $\\mathbb H^3$; the other under the constraint for $n$ given points on the circle to be welded to another $n$ given points of the circle.","The latter problem can be viewed as optimizing space-like curves on the boundary of AdS$^3$ space passing through $n$ prescribed points.","We observe that the answers to the two problems exhibit interesting symmetries: optimizing the Jordan curve in $\\partial_\\infty \\mathbb H^3$ gives rise to a welding homeomorphism that is the boundary of a pleated plane in AdS$^3$, whereas optimizing the space-like curve in $\\partial_\\infty\\!\\operatorname{AdS}^3$ gives rise to a Jordan curve that is the boundary of a pleated plane in $\\mathbb H^3$."],"url":"http://arxiv.org/abs/2402.10054v1","category":"math.CV"}
{"created":"2024-02-15 16:10:50","title":"On rigidity of complex Hirzebruch genera on $SU$-manifolds","abstract":"We prove that if a complex genus $\\varphi \\colon \\varOmega^U \\to R$ is rigid on $SU$-manifolds with a torus action then $\\varphi$ is the elliptic Krichever genus.","sentences":["We prove that if a complex genus $\\varphi \\colon \\varOmega^U \\to R$ is rigid on $SU$-manifolds with a torus action then $\\varphi$ is the elliptic Krichever genus."],"url":"http://arxiv.org/abs/2402.10049v1","category":"math.AT"}
{"created":"2024-02-15 16:05:17","title":"Dynamics of cold circumstellar gas in debris disks","abstract":"Mounting observational evidence indicates that cold circumstellar gas is present in debris disk systems. This work focuses on various dynamical processes that debris-disk gas may undergo. We review five mechanisms that can transport angular momentum and their applications to debris disks. These include molecular viscosity, hydrodynamic turbulence, magnetohydrodynamic turbulence, magnetized disk winds, and laminar magnetic stress. We find that molecular viscosity can result in $\\alpha$ as high as $\\lesssim 0.1$ for sufficiently low densities, while the Rossby wave instability is a possible source of hydrodynamic turbulence and structure formation. We argue that the vertical shear instability is unlikely due to the long cooling times. The onset of the magnetorotational instability (MRI) is dichotomous: for low density disks the MRI can be excited at the midplane, while for high mass disks it may only be operating at $z>2-3H$, if at all. The MHD wind and laminar magnetic stress mechanisms rely on the configuration and strength of any background large-scale magnetic field, the existence of which is uncertain and possibly unlikely. We conclude that the dominant mechanism and its efficiency in transporting angular momentum varies from one system to the other, depending especially closely on the gas density. More detailed analyses shall be performed in the future focusing on representative, nearby debris disks.","sentences":["Mounting observational evidence indicates that cold circumstellar gas is present in debris disk systems.","This work focuses on various dynamical processes that debris-disk gas may undergo.","We review five mechanisms that can transport angular momentum and their applications to debris disks.","These include molecular viscosity, hydrodynamic turbulence, magnetohydrodynamic turbulence, magnetized disk winds, and laminar magnetic stress.","We find that molecular viscosity can result in $\\alpha$ as high as $\\lesssim 0.1$ for sufficiently low densities, while the Rossby wave instability is a possible source of hydrodynamic turbulence and structure formation.","We argue that the vertical shear instability is unlikely due to the long cooling times.","The onset of the magnetorotational instability (MRI) is dichotomous: for low density disks the MRI can be excited at the midplane, while for high mass disks it may only be operating at $z>2-3H$, if at all.","The MHD wind and laminar magnetic stress mechanisms rely on the configuration and strength of any background large-scale magnetic field, the existence of which is uncertain and possibly unlikely.","We conclude that the dominant mechanism and its efficiency in transporting angular momentum varies from one system to the other, depending especially closely on the gas density.","More detailed analyses shall be performed in the future focusing on representative, nearby debris disks."],"url":"http://arxiv.org/abs/2402.10042v1","category":"astro-ph.EP"}
{"created":"2024-02-15 16:03:51","title":"Motional state analysis of a trapped ion by ultra-narrowband composite pulses","abstract":"In this work, we present a method for measuring the motional state of a two-level system coupled to a harmonic oscillator. Our technique uses ultra-narrowband composite pulses on the blue sideband transition to scan through the populations of the different motional states. Our approach does not assume any previous knowledge of the motional state distribution and is easily implemented. It is applicable both inside and outside of the Lamb-Dicke regime. For higher phonon numbers especially, the composite pulse sequence can be used as a filter for measuring phonon number ranges. We demonstrate this measurement technique using a single trapped ion and show good detection results with the numerically evaluated pulse sequence.","sentences":["In this work, we present a method for measuring the motional state of a two-level system coupled to a harmonic oscillator.","Our technique uses ultra-narrowband composite pulses on the blue sideband transition to scan through the populations of the different motional states.","Our approach does not assume any previous knowledge of the motional state distribution and is easily implemented.","It is applicable both inside and outside of the Lamb-Dicke regime.","For higher phonon numbers especially, the composite pulse sequence can be used as a filter for measuring phonon number ranges.","We demonstrate this measurement technique using a single trapped ion and show good detection results with the numerically evaluated pulse sequence."],"url":"http://arxiv.org/abs/2402.10041v1","category":"quant-ph"}
{"created":"2024-02-15 15:59:59","title":"Predictive Linear Online Tracking for Unknown Targets","abstract":"In this paper, we study the problem of online tracking in linear control systems, where the objective is to follow a moving target. Unlike classical tracking control, the target is unknown, non-stationary, and its state is revealed sequentially, thus, fitting the framework of online non-stochastic control. We consider the case of quadratic costs and propose a new algorithm, called predictive linear online tracking (PLOT). The algorithm uses recursive least squares with exponential forgetting to learn a time-varying dynamic model of the target. The learned model is used in the optimal policy under the framework of receding horizon control. We show the dynamic regret of PLOT scales with $\\mathcal{O}(\\sqrt{TV_T})$, where $V_T$ is the total variation of the target dynamics and $T$ is the time horizon. Unlike prior work, our theoretical results hold for non-stationary targets. We implement PLOT on a real quadrotor and provide open-source software, thus, showcasing one of the first successful applications of online control methods on real hardware.","sentences":["In this paper, we study the problem of online tracking in linear control systems, where the objective is to follow a moving target.","Unlike classical tracking control, the target is unknown, non-stationary, and its state is revealed sequentially, thus, fitting the framework of online non-stochastic control.","We consider the case of quadratic costs and propose a new algorithm, called predictive linear online tracking (PLOT).","The algorithm uses recursive least squares with exponential forgetting to learn a time-varying dynamic model of the target.","The learned model is used in the optimal policy under the framework of receding horizon control.","We show the dynamic regret of PLOT scales with $\\mathcal{O}(\\sqrt{TV_T})$, where $V_T$ is the total variation of the target dynamics and $T$ is the time horizon.","Unlike prior work, our theoretical results hold for non-stationary targets.","We implement PLOT on a real quadrotor and provide open-source software, thus, showcasing one of the first successful applications of online control methods on real hardware."],"url":"http://arxiv.org/abs/2402.10036v1","category":"eess.SY"}
{"created":"2024-02-15 15:58:25","title":"A Causation-Based Computationally Efficient Strategy for Deploying Lagrangian Drifters to Improve Real-Time State Estimation","abstract":"Deploying Lagrangian drifters that facilitate the state estimation of the underlying flow field within a future time interval is practically important. However, the uncertainty in estimating the flow field prevents using standard deterministic approaches for designing strategies and applying trajectory-wise skill scores to evaluate performance. In this paper an information measurement is developed to quantitatively assess the information gain in the estimated flow field by deploying an additional set of drifters. This information measurement is derived by exploiting causal inference. It is characterized by the inferred probability density function of the flow field, which naturally considers the uncertainty. Although the information measurement is an ideal theoretical metric, using it as the direct cost makes the optimization problem computationally expensive. To this end, an effective surrogate cost function is developed. It is highly efficient to compute while capturing the essential features of the information measurement when solving the optimization problem. Based upon these properties, a practical strategy for deploying drifter observations to improve future state estimation is designed. Due to the forecast uncertainty, the approach exploits the expected value of spatial maps of the surrogate cost associated with different forecast realizations to seek the optimal solution. Numerical experiments justify the effectiveness of the surrogate cost. The proposed strategy significantly outperforms the method by randomly deploying the drifters. It is also shown that, under certain conditions, the drifters determined by the expected surrogate cost remain skillful for the state estimation of a single forecast realization of the flow field as in reality.","sentences":["Deploying Lagrangian drifters that facilitate the state estimation of the underlying flow field within a future time interval is practically important.","However, the uncertainty in estimating the flow field prevents using standard deterministic approaches for designing strategies and applying trajectory-wise skill scores to evaluate performance.","In this paper an information measurement is developed to quantitatively assess the information gain in the estimated flow field by deploying an additional set of drifters.","This information measurement is derived by exploiting causal inference.","It is characterized by the inferred probability density function of the flow field, which naturally considers the uncertainty.","Although the information measurement is an ideal theoretical metric, using it as the direct cost makes the optimization problem computationally expensive.","To this end, an effective surrogate cost function is developed.","It is highly efficient to compute while capturing the essential features of the information measurement when solving the optimization problem.","Based upon these properties, a practical strategy for deploying drifter observations to improve future state estimation is designed.","Due to the forecast uncertainty, the approach exploits the expected value of spatial maps of the surrogate cost associated with different forecast realizations to seek the optimal solution.","Numerical experiments justify the effectiveness of the surrogate cost.","The proposed strategy significantly outperforms the method by randomly deploying the drifters.","It is also shown that, under certain conditions, the drifters determined by the expected surrogate cost remain skillful for the state estimation of a single forecast realization of the flow field as in reality."],"url":"http://arxiv.org/abs/2402.10034v1","category":"math.DS"}
{"created":"2024-02-15 15:55:15","title":"Tomography of orbital vortex lines in a topological semimetal","abstract":"Topological defects are inherently stable structures that manifest in a variety of physical settings, from particle physics and cosmology to superfluids and quantum magnets. The geometric structure of Bloch wave functions in a periodic lattice may host topological defects, underpinning the unique properties of topological quantum matter. While pointlike defects, the celebrated Weyl points, have been extensively studied, higher dimensional structures have proven to be harder to pin down. Here, we report the experimental discovery of orbital vortex lines - the first imaging of non-trivial quantum-phase winding at line nodes - in the three-dimensional band structure of a topological semimetal, TaAs. Leveraging dichroic photoemission tomography, we directly image the winding of atomic orbital angular momentum, thereby revealing - and determining the location of - lines of vorticity in full 3D momentum space. We determine the core of the orbital angular momentum vortex to host a so-called almost movable, two-fold spin-degenerate Weyl nodal line, a topological feature predicted to occur in certain non-symmorphic crystals. These results establish the capacity to detect complex topological textures in reciprocal space and may pave the way toward novel orbital transport phenomena in metallic quantum materials.","sentences":["Topological defects are inherently stable structures that manifest in a variety of physical settings, from particle physics and cosmology to superfluids and quantum magnets.","The geometric structure of Bloch wave functions in a periodic lattice may host topological defects, underpinning the unique properties of topological quantum matter.","While pointlike defects, the celebrated Weyl points, have been extensively studied, higher dimensional structures have proven to be harder to pin down.","Here, we report the experimental discovery of orbital vortex lines - the first imaging of non-trivial quantum-phase winding at line nodes - in the three-dimensional band structure of a topological semimetal, TaAs.","Leveraging dichroic photoemission tomography, we directly image the winding of atomic orbital angular momentum, thereby revealing - and determining the location of - lines of vorticity in full 3D momentum space.","We determine the core of the orbital angular momentum vortex to host a so-called almost movable, two-fold spin-degenerate Weyl nodal line, a topological feature predicted to occur in certain non-symmorphic crystals.","These results establish the capacity to detect complex topological textures in reciprocal space and may pave the way toward novel orbital transport phenomena in metallic quantum materials."],"url":"http://arxiv.org/abs/2402.10031v1","category":"cond-mat.str-el"}
{"created":"2024-02-15 15:53:40","title":"The Borel complexity of the class of models of first-order theories","abstract":"We investigate the descriptive complexity of the set of models of first-order theories. Using classical results of Knight and Solovay, we give a sharp condition for complete theories to have a $\\boldsymbol\\Pi_\\omega^0$-complete set of models. We also give sharp conditions for theories to have a $\\boldsymbol\\Pi^0_n$-complete set of models. Finally, we determine the Turing degrees needed to witness the completeness.","sentences":["We investigate the descriptive complexity of the set of models of first-order theories.","Using classical results of Knight and Solovay, we give a sharp condition for complete theories to have a $\\boldsymbol\\Pi_\\omega^0$-complete set of models.","We also give sharp conditions for theories to have a $\\boldsymbol\\Pi^0_n$-complete set of models.","Finally, we determine the Turing degrees needed to witness the completeness."],"url":"http://arxiv.org/abs/2402.10029v1","category":"math.LO"}
{"created":"2024-02-15 15:47:01","title":"ITRUSST Consensus on Standardised Reporting for Transcranial Ultrasound Stimulation","abstract":"As transcranial ultrasound stimulation (TUS) advances as a precise, non-invasive neuromodulatory method, there is a need for consistent reporting standards to enable comparison and reproducibility across studies. To this end, the International Transcranial Ultrasonic Stimulation Safety and Standards Consortium (ITRUSST) formed a subcommittee of experts across several domains to review and suggest standardised reporting parameters for low intensity TUS, resulting in the guide presented here. The scope of the guide is limited to reporting the ultrasound aspects of a study. The guide and supplementary material provide a simple checklist covering the reporting of: (1) the transducer and drive system, (2) the drive system settings, (3) the free field acoustic parameters, (4) the pulse timing parameters, (5) \\emph{in situ} estimates of exposure parameters in the brain, and (6) intensity parameters. Detailed explanations for each of the parameters, including discussions on assumptions, measurements, and calculations, are also provided.","sentences":["As transcranial ultrasound stimulation (TUS) advances as a precise, non-invasive neuromodulatory method, there is a need for consistent reporting standards to enable comparison and reproducibility across studies.","To this end, the International Transcranial Ultrasonic Stimulation Safety and Standards Consortium (ITRUSST) formed a subcommittee of experts across several domains to review and suggest standardised reporting parameters for low intensity TUS, resulting in the guide presented here.","The scope of the guide is limited to reporting the ultrasound aspects of a study.","The guide and supplementary material provide a simple checklist covering the reporting of: (1) the transducer and drive system, (2) the drive system settings, (3) the free field acoustic parameters, (4) the pulse timing parameters, (5) \\emph{in situ} estimates of exposure parameters in the brain, and (6) intensity parameters.","Detailed explanations for each of the parameters, including discussions on assumptions, measurements, and calculations, are also provided."],"url":"http://arxiv.org/abs/2402.10027v1","category":"physics.med-ph"}
{"created":"2024-02-15 15:46:13","title":"Hybrid CNN Bi-LSTM neural network for Hyperspectral image classification","abstract":"Hyper spectral images have drawn the attention of the researchers for its complexity to classify. It has nonlinear relation between the materials and the spectral information provided by the HSI image. Deep learning methods have shown superiority in learning this nonlinearity in comparison to traditional machine learning methods. Use of 3-D CNN along with 2-D CNN have shown great success for learning spatial and spectral features. However, it uses comparatively large number of parameters. Moreover, it is not effective to learn inter layer information. Hence, this paper proposes a neural network combining 3-D CNN, 2-D CNN and Bi-LSTM. The performance of this model has been tested on Indian Pines(IP) University of Pavia(PU) and Salinas Scene(SA) data sets. The results are compared with the state of-the-art deep learning-based models. This model performed better in all three datasets. It could achieve 99.83, 99.98 and 100 percent accuracy using only 30 percent trainable parameters of the state-of-art model in IP, PU and SA datasets respectively.","sentences":["Hyper spectral images have drawn the attention of the researchers for its complexity to classify.","It has nonlinear relation between the materials and the spectral information provided by the HSI image.","Deep learning methods have shown superiority in learning this nonlinearity in comparison to traditional machine learning methods.","Use of 3-D CNN along with 2-D CNN have shown great success for learning spatial and spectral features.","However, it uses comparatively large number of parameters.","Moreover, it is not effective to learn inter layer information.","Hence, this paper proposes a neural network combining 3-D CNN, 2-D CNN and Bi-LSTM.","The performance of this model has been tested on Indian Pines(IP)","University of Pavia(PU) and Salinas Scene(SA) data sets.","The results are compared with the state of-the-art deep learning-based models.","This model performed better in all three datasets.","It could achieve 99.83, 99.98 and 100 percent accuracy using only 30 percent trainable parameters of the state-of-art model in IP, PU and SA datasets respectively."],"url":"http://arxiv.org/abs/2402.10026v1","category":"eess.IV"}
{"created":"2024-02-15 15:41:18","title":"Manifestations of CP Violation in the B Meson System: Theoretical Perspective","abstract":"CP violation in the field of $B$ physics is a crucial topic for exploring the quark sector and search for New Physics, both for theorists and experimentalists. CP violation manifests itself in various ways and in this presentation, we will categorise the decays based on their different dynamics. We aim to present highlights related to the studies of CP violation in each category from a theoretical perspective.","sentences":["CP violation in the field of $B$ physics is a crucial topic for exploring the quark sector and search for New Physics, both for theorists and experimentalists.","CP violation manifests itself in various ways and in this presentation, we will categorise the decays based on their different dynamics.","We aim to present highlights related to the studies of CP violation in each category from a theoretical perspective."],"url":"http://arxiv.org/abs/2402.10023v1","category":"hep-ph"}
{"created":"2024-02-15 15:39:46","title":"SAWEC: Sensing-Assisted Wireless Edge Computing","abstract":"Emerging mobile virtual reality (VR) systems will require to continuously perform complex computer vision tasks on ultra-high-resolution video frames through the execution of deep neural networks (DNNs)-based algorithms. Since state-of-the-art DNNs require computational power that is excessive for mobile devices, techniques based on wireless edge computing (WEC) have been recently proposed. However, existing WEC methods require the transmission and processing of a high amount of video data which may ultimately saturate the wireless link. In this paper, we propose a novel Sensing-Assisted Wireless Edge Computing (SAWEC) paradigm to address this issue. SAWEC leverages knowledge about the physical environment to reduce the end-to-end latency and overall computational burden by transmitting to the edge server only the relevant data for the delivery of the service. Our intuition is that the transmission of the portion of the video frames where there are no changes with respect to previous frames can be avoided. Specifically, we leverage wireless sensing techniques to estimate the location of objects in the environment and obtain insights about the environment dynamics. Hence, only the part of the frames where any environmental change is detected is transmitted and processed. We evaluated SAWEC by using a 10K 360$^{\\circ}$ camera with a Wi-Fi 6 sensing system operating at 160 MHz and performing localization and tracking. We perform experiments in an anechoic chamber and a hall room with two human subjects in six different setups. Experimental results show that SAWEC reduces the channel occupation, and end-to-end latency by 93.81%, and 96.19% respectively while improving the instance segmentation performance by 46.98% with respect to state-of-the-art WEC approaches. For reproducibility purposes, we pledge to share our whole dataset and code repository.","sentences":["Emerging mobile virtual reality (VR) systems will require to continuously perform complex computer vision tasks on ultra-high-resolution video frames through the execution of deep neural networks (DNNs)-based algorithms.","Since state-of-the-art DNNs require computational power that is excessive for mobile devices, techniques based on wireless edge computing (WEC) have been recently proposed.","However, existing WEC methods require the transmission and processing of a high amount of video data which may ultimately saturate the wireless link.","In this paper, we propose a novel Sensing-Assisted Wireless Edge Computing (SAWEC) paradigm to address this issue.","SAWEC leverages knowledge about the physical environment to reduce the end-to-end latency and overall computational burden by transmitting to the edge server only the relevant data for the delivery of the service.","Our intuition is that the transmission of the portion of the video frames where there are no changes with respect to previous frames can be avoided.","Specifically, we leverage wireless sensing techniques to estimate the location of objects in the environment and obtain insights about the environment dynamics.","Hence, only the part of the frames where any environmental change is detected is transmitted and processed.","We evaluated SAWEC by using a 10K 360$^{\\circ}$ camera with a Wi-Fi 6 sensing system operating at 160 MHz and performing localization and tracking.","We perform experiments in an anechoic chamber and a hall room with two human subjects in six different setups.","Experimental results show that SAWEC reduces the channel occupation, and end-to-end latency by 93.81%, and 96.19% respectively while improving the instance segmentation performance by 46.98% with respect to state-of-the-art WEC approaches.","For reproducibility purposes, we pledge to share our whole dataset and code repository."],"url":"http://arxiv.org/abs/2402.10021v1","category":"cs.CV"}
{"created":"2024-02-15 15:32:51","title":"Exploring 2D Materials by High Pressure Synthesis: hBN, Mg-hBN, b-P, b-AsP, and GeAs","abstract":"In materials science, selecting the right synthesis technique for specific compounds is one of the most important steps. High-pressure conditions have a significant effect on the crystal growth processes, leading to the creation of unique structures and properties that usually are not possible under normal conditions. The prime objective of this article is to illustrate the benefits of using high-pressure, high-temperature (HPHT) technique when developing two-dimensional (2D) materials. We could successfully grow bulk single crystals of hexagonal boron nitride (hBN) and magnesium doped hexagonal boron nitride (Mg-hBN) from Mg-B-N solvent. Further exploration of the Mg-B-N system could lead to the crystallization of isotopically 10B and 11B enriched hBN crystals, and other doped variants of it. Black phosphorus (b-P) and black phosphorus doped with arsenic (b-AsP) were obtained by directly converting its elements into melt and subsequently crystallizing them under HPHT. Germanium arsenide (GeAs) bulk single crystals were also obtained from the melt at a pressure of 1 GPa. Upon crystallization, all these compounds exhibit the anticipated layered structures, which makes them easy to exfoliate into 2D flakes, thus providing opportunities to modify their electrical behavior and create new useful devices.","sentences":["In materials science, selecting the right synthesis technique for specific compounds is one of the most important steps.","High-pressure conditions have a significant effect on the crystal growth processes, leading to the creation of unique structures and properties that usually are not possible under normal conditions.","The prime objective of this article is to illustrate the benefits of using high-pressure, high-temperature (HPHT) technique when developing two-dimensional (2D) materials.","We could successfully grow bulk single crystals of hexagonal boron nitride (hBN) and magnesium doped hexagonal boron nitride (Mg-hBN) from Mg-B-N solvent.","Further exploration of the Mg-B-N system could lead to the crystallization of isotopically 10B and 11B enriched hBN crystals, and other doped variants of it.","Black phosphorus (b-P) and black phosphorus doped with arsenic (b-AsP) were obtained by directly converting its elements into melt and subsequently crystallizing them under HPHT.","Germanium arsenide (GeAs) bulk single crystals were also obtained from the melt at a pressure of 1 GPa.","Upon crystallization, all these compounds exhibit the anticipated layered structures, which makes them easy to exfoliate into 2D flakes, thus providing opportunities to modify their electrical behavior and create new useful devices."],"url":"http://arxiv.org/abs/2402.10019v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-15 14:54:46","title":"TIAViz: A Browser-based Visualization Tool for Computational Pathology Models","abstract":"Digital pathology has gained significant traction in modern healthcare systems. This shift from optical microscopes to digital imagery brings with it the potential for improved diagnosis, efficiency, and the integration of AI tools into the pathologists workflow. A critical aspect of this is visualization. Throughout the development of a machine learning (ML) model in digital pathology, it is crucial to have flexible, openly available tools to visualize models, from their outputs and predictions to the underlying annotations and images used to train or test a model. We introduce TIAViz, a Python-based visualization tool built into TIAToolbox which allows flexible, interactive, fully zoomable overlay of a wide variety of information onto whole slide images, including graphs, heatmaps, segmentations, annotations and other WSIs. The UI is browser-based, allowing use either locally, on a remote machine, or on a server to provide publicly available demos. This tool is open source and is made available at: https://github.com/TissueImageAnalytics/tiatoolbox and via pip installation (pip install tiatoolbox) and conda as part of TIAToolbox.","sentences":["Digital pathology has gained significant traction in modern healthcare systems.","This shift from optical microscopes to digital imagery brings with it the potential for improved diagnosis, efficiency, and the integration of AI tools into the pathologists workflow.","A critical aspect of this is visualization.","Throughout the development of a machine learning (ML) model in digital pathology, it is crucial to have flexible, openly available tools to visualize models, from their outputs and predictions to the underlying annotations and images used to train or test a model.","We introduce TIAViz, a Python-based visualization tool built into TIAToolbox which allows flexible, interactive, fully zoomable overlay of a wide variety of information onto whole slide images, including graphs, heatmaps, segmentations, annotations and other WSIs.","The UI is browser-based, allowing use either locally, on a remote machine, or on a server to provide publicly available demos.","This tool is open source and is made available at: https://github.com/TissueImageAnalytics/tiatoolbox and via pip installation (pip install tiatoolbox) and conda as part of TIAToolbox."],"url":"http://arxiv.org/abs/2402.09990v1","category":"eess.IV"}
{"created":"2024-02-15 14:53:58","title":"One-shot omnidirectional pressure integration through matrix inversion","abstract":"In this work, we present a method to perform 2D and 3D omnidirectional pressure integration from velocity measurements with a single-iteration matrix inversion approach. This work builds upon our previous work, where the rotating parallel ray approach was extended to the limit of infinite rays by taking continuous projection integrals of the ray paths and recasting the problem as an iterative matrix inversion problem. This iterative matrix equation is now \"fast-forwarded\" to the \"infinity\" iteration, leading to a different matrix equation that can be solved in a single iteration, thereby presenting the same computational complexity as the Poisson equation. We observe computational speedups of $\\sim10^6$ when compared to brute-force omnidirectional integration methods, enabling the treatment of grids of $\\sim 10^9$ points and potentially even larger in a desktop setup at the time of publication. Further examination of the boundary conditions of our one-shot method shows that omnidirectional pressure integration implements a new type of boundary condition, which treats the boundary points as interior points to the extent that information is available. Finally, we show how the method can be extended from the regular grids typical of particle image velocimetry to the unstructured meshes characteristic of particle tracking velocimetry data.","sentences":["In this work, we present a method to perform 2D and 3D omnidirectional pressure integration from velocity measurements with a single-iteration matrix inversion approach.","This work builds upon our previous work, where the rotating parallel ray approach was extended to the limit of infinite rays by taking continuous projection integrals of the ray paths and recasting the problem as an iterative matrix inversion problem.","This iterative matrix equation is now \"fast-forwarded\" to the \"infinity\" iteration, leading to a different matrix equation that can be solved in a single iteration, thereby presenting the same computational complexity as the Poisson equation.","We observe computational speedups of $\\sim10^6$ when compared to brute-force omnidirectional integration methods, enabling the treatment of grids of $\\sim 10^9$ points and potentially even larger in a desktop setup at the time of publication.","Further examination of the boundary conditions of our one-shot method shows that omnidirectional pressure integration implements a new type of boundary condition, which treats the boundary points as interior points to the extent that information is available.","Finally, we show how the method can be extended from the regular grids typical of particle image velocimetry to the unstructured meshes characteristic of particle tracking velocimetry data."],"url":"http://arxiv.org/abs/2402.09988v1","category":"physics.flu-dyn"}
{"created":"2024-02-15 14:52:13","title":"Testing the Instanton Approach to the Large Amplification Limit of a Diffraction-Amplification Problem","abstract":"The validity of the instanton analysis approach is tested numerically in the case of the diffraction-amplification problem $\\partial_z\\psi -\\frac{i}{2m}\\nabla_{\\perp}^2 \\psi =g\\vert S\\vert^2\\, \\psi$ for $\\ln U\\gg 1$, where $U=\\vert\\psi(0,L)\\vert^2$. Here, $S(x,z)$ is a complex Gaussian random field, $z$ and $x$ respectively are the axial and transverse coordinates, with $0\\le z\\le L$, and both $m\\ne 0$ and $g>0$ are real parameters. To sample the rare and extreme amplification values of interest ($\\ln U\\gg 1$), we devise a specific biased sampling procedure by which $p(U)$, the probability distribution of $U$, is obtained down to values less than $10^{-2270}$ in the far right tail. We find that the agreement of our numerical results with the instanton analysis predictions in Mounaix (2023 {\\it J. Phys. A: Math. Theor.} {\\bf 56} 305001) is remarkable. Both the predicted algebraic tail of $p(U)$ and concentration of the realizations of $S$ onto the leading instanton are clearly confirmed, which validates the instanton analysis numerically in the large $\\ln U$ limit.","sentences":["The validity of the instanton analysis approach is tested numerically in the case of the diffraction-amplification problem $\\partial_z\\psi -\\frac{i}{2m}\\nabla_{\\perp}^2 \\psi =g\\vert S\\vert^2\\, \\psi$ for $\\ln U\\gg 1$, where $U=\\vert\\psi(0,L)\\vert^2$. Here, $S(x,z)$ is a complex Gaussian random field, $z$ and $x$ respectively are the axial and transverse coordinates, with $0\\le z\\le L$, and both $m\\ne 0$ and $g>0$ are real parameters.","To sample the rare and extreme amplification values of interest ($\\ln U\\gg 1$), we devise a specific biased sampling procedure by which $p(U)$, the probability distribution of $U$, is obtained down to values less than $10^{-2270}$ in the far right tail.","We find that the agreement of our numerical results with the instanton analysis predictions in Mounaix (2023 {\\it J. Phys.","A: Math. Theor.} {\\bf 56} 305001) is remarkable.","Both the predicted algebraic tail of $p(U)$ and concentration of the realizations of $S$ onto the leading instanton are clearly confirmed, which validates the instanton analysis numerically in the large $\\ln U$ limit."],"url":"http://arxiv.org/abs/2402.09986v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-15 14:41:55","title":"Deep learning for the design of non-Hermitian topolectrical circuits","abstract":"Non-Hermitian topological phases can produce some remarkable properties, compared with their Hermitian counterpart, such as the breakdown of conventional bulk-boundary correspondence and the non-Hermitian topological edge mode. Here, we introduce several algorithms with multi-layer perceptron (MLP), and convolutional neural network (CNN) in the field of deep learning, to predict the winding of eigenvalues non-Hermitian Hamiltonians. Subsequently, we use the smallest module of the periodic circuit as one unit to construct high-dimensional circuit data features. Further, we use the Dense Convolutional Network (DenseNet), a type of convolutional neural network that utilizes dense connections between layers to design a non-Hermitian topolectrical Chern circuit, as the DenseNet algorithm is more suitable for processing high-dimensional data. Our results demonstrate the effectiveness of the deep learning network in capturing the global topological characteristics of a non-Hermitian system based on training data.","sentences":["Non-Hermitian topological phases can produce some remarkable properties, compared with their Hermitian counterpart, such as the breakdown of conventional bulk-boundary correspondence and the non-Hermitian topological edge mode.","Here, we introduce several algorithms with multi-layer perceptron (MLP), and convolutional neural network (CNN) in the field of deep learning, to predict the winding of eigenvalues non-Hermitian Hamiltonians.","Subsequently, we use the smallest module of the periodic circuit as one unit to construct high-dimensional circuit data features.","Further, we use the Dense Convolutional Network (DenseNet), a type of convolutional neural network that utilizes dense connections between layers to design a non-Hermitian topolectrical Chern circuit, as the DenseNet algorithm is more suitable for processing high-dimensional data.","Our results demonstrate the effectiveness of the deep learning network in capturing the global topological characteristics of a non-Hermitian system based on training data."],"url":"http://arxiv.org/abs/2402.09978v1","category":"physics.app-ph"}
{"created":"2024-02-15 14:36:53","title":"Sensing-assisted Robust SWIPT for Mobile Energy Harvesting Receivers","abstract":"Simultaneous wireless information and power transfer (SWIPT) has been proposed to offer communication services and transfer power to the energy harvesting receiver (EHR) concurrently. However, existing works mainly focused on static EHRs, without considering the location uncertainty caused by the movement of EHRs and location estimation errors. To tackle this issue, this paper considers the sensing-assisted SWIPT design in a networked integrated sensing and communication (ISAC) system in the presence of location uncertainty. A two-phase robust design is proposed to reduce the location uncertainty and improve the power transfer efficiency. In particular, each time frame is divided into two phases, i.e., sensing and WPT phases, via time-splitting. The sensing phase performs collaborative sensing to localize the EHR, whose results are then utilized in the WPT phase for efficient WPT. To minimize the power consumption with given communication and power transfer requirements, a two-layer optimization framework is proposed to jointly optimize the time-splitting ratio, coordinated beamforming policy, and sensing node selection. Simulation results validate the effectiveness of the proposed design and demonstrate the existence of an optimal time-splitting ratio for given location uncertainty.","sentences":["Simultaneous wireless information and power transfer (SWIPT) has been proposed to offer communication services and transfer power to the energy harvesting receiver (EHR) concurrently.","However, existing works mainly focused on static EHRs, without considering the location uncertainty caused by the movement of EHRs and location estimation errors.","To tackle this issue, this paper considers the sensing-assisted SWIPT design in a networked integrated sensing and communication (ISAC) system in the presence of location uncertainty.","A two-phase robust design is proposed to reduce the location uncertainty and improve the power transfer efficiency.","In particular, each time frame is divided into two phases, i.e., sensing and WPT phases, via time-splitting.","The sensing phase performs collaborative sensing to localize the EHR, whose results are then utilized in the WPT phase for efficient WPT.","To minimize the power consumption with given communication and power transfer requirements, a two-layer optimization framework is proposed to jointly optimize the time-splitting ratio, coordinated beamforming policy, and sensing node selection.","Simulation results validate the effectiveness of the proposed design and demonstrate the existence of an optimal time-splitting ratio for given location uncertainty."],"url":"http://arxiv.org/abs/2402.09976v1","category":"eess.SP"}
{"created":"2024-02-15 14:34:23","title":"Interference Mitigation for Network-Level ISAC: An Optimization Perspective","abstract":"Future wireless networks are envisioned to simultaneously provide high data-rate communication and ubiquitous environment-aware services for numerous users. One promising approach to meet this demand is to employ network-level integrated sensing and communications (ISAC) by jointly designing the signal processing and resource allocation over the entire network. However, to unleash the full potential of network-level ISAC, some critical challenges must be tackled. Among them, interference management is one of the most significant ones. In this article, we build up a bridge between interference mitigation techniques and the corresponding optimization methods, which facilitates efficient interference mitigation in network-level ISAC systems. In particular, we first identify several types of interference in network-level ISAC systems, including self-interference, mutual interference, crosstalk, clutter, and multiuser interference. Then, we present several promising techniques that can be utilized to suppress specific types of interference. For each type of interference, we discuss the corresponding problem formulation and identify the associated optimization methods. Moreover, to illustrate the effectiveness of the proposed interference mitigation techniques, two concrete network-level ISAC systems, namely coordinated cellular network-based and distributed antenna-based ISAC systems, are investigated from interference management perspective. Experiment results indicate that it is beneficial to collaboratively employ different interference mitigation techniques and leverage the network structure to achieve the full potential of network-level ISAC. Finally, we highlight several promising future research directions for the design of ISAC systems.","sentences":["Future wireless networks are envisioned to simultaneously provide high data-rate communication and ubiquitous environment-aware services for numerous users.","One promising approach to meet this demand is to employ network-level integrated sensing and communications (ISAC) by jointly designing the signal processing and resource allocation over the entire network.","However, to unleash the full potential of network-level ISAC, some critical challenges must be tackled.","Among them, interference management is one of the most significant ones.","In this article, we build up a bridge between interference mitigation techniques and the corresponding optimization methods, which facilitates efficient interference mitigation in network-level ISAC systems.","In particular, we first identify several types of interference in network-level ISAC systems, including self-interference, mutual interference, crosstalk, clutter, and multiuser interference.","Then, we present several promising techniques that can be utilized to suppress specific types of interference.","For each type of interference, we discuss the corresponding problem formulation and identify the associated optimization methods.","Moreover, to illustrate the effectiveness of the proposed interference mitigation techniques, two concrete network-level ISAC systems, namely coordinated cellular network-based and distributed antenna-based ISAC systems, are investigated from interference management perspective.","Experiment results indicate that it is beneficial to collaboratively employ different interference mitigation techniques and leverage the network structure to achieve the full potential of network-level ISAC.","Finally, we highlight several promising future research directions for the design of ISAC systems."],"url":"http://arxiv.org/abs/2402.09974v1","category":"cs.IT"}
{"created":"2024-02-15 14:28:01","title":"Parameterized Vertex Integrity Revisited","abstract":"Vertex integrity is a graph parameter that measures the connectivity of a graph. Informally, its meaning is that a graph has small vertex integrity if it has a small separator whose removal disconnects the graph into connected components which are themselves also small. Graphs with low vertex integrity are extremely structured; this renders many hard problems tractable and has recently attracted interest in this notion from the parameterized complexity community. In this paper we revisit the NP-complete problem of computing the vertex integrity of a given graph from the point of view of structural parameterizations. We present a number of new results, which also answer some recently posed open questions from the literature. Specifically: We show that unweighted vertex integrity is W[1]-hard parameterized by treedepth; we show that the problem remains W[1]-hard if we parameterize by feedback edge set size (via a reduction from a Bin Packing variant which may be of independent interest); and complementing this we show that the problem is FPT by max-leaf number. Furthermore, for weighted vertex integrity, we show that the problem admits a single-exponential FPT algorithm parameterized by vertex cover or by modular width, the latter result improving upon a previous algorithm which required weights to be polynomially bounded.","sentences":["Vertex integrity is a graph parameter that measures the connectivity of a graph.","Informally, its meaning is that a graph has small vertex integrity if it has a small separator whose removal disconnects the graph into connected components which are themselves also small.","Graphs with low vertex integrity are extremely structured; this renders many hard problems tractable and has recently attracted interest in this notion from the parameterized complexity community.","In this paper we revisit the NP-complete problem of computing the vertex integrity of a given graph from the point of view of structural parameterizations.","We present a number of new results, which also answer some recently posed open questions from the literature.","Specifically: We show that unweighted vertex integrity is W[1]-hard parameterized by treedepth; we show that the problem remains W[1]-hard if we parameterize by feedback edge set size (via a reduction from a Bin Packing variant which may be of independent interest); and complementing this we show that the problem is FPT by max-leaf number.","Furthermore, for weighted vertex integrity, we show that the problem admits a single-exponential FPT algorithm parameterized by vertex cover or by modular width, the latter result improving upon a previous algorithm which required weights to be polynomially bounded."],"url":"http://arxiv.org/abs/2402.09971v1","category":"cs.DS"}
{"created":"2024-02-15 14:18:43","title":"Walsh-domain Neural Network for Power Amplifier Behavioral Modelling and Digital Predistortion","abstract":"This paper investigates the use of Neural Network (NN) nonlinear modelling for Power Amplifier (PA) linearization in the Walsh-Hadamard transceiver architecture. This novel architecture has recently been proposed for ultra-high bandwidth systems to reduce the transceiver power consumption by extensive parallelization of the digital baseband hardware. The parallelization is achieved by replacing two-dimensional quadrature modulation with multi-dimensional Walsh-Hadamard modulation. The open research question for this architecture is whether conventional baseband signal processing algorithms can be similarly parallelized while retaining their performance. A key baseband algorithm, digital predistortion using NN models for PA linearization, will be adapted to the parallel Walsh architecture. A straighforward parallelization of the state-of-the-art NN architecture is extended with a cross-domain Knowledge Distillation pre-training method to achieve linearization performance on par with the quadrature implementation. This result paves the way for the entire baseband processing chain to be adapted into ultra-high bandwidth, low-power Walsh transceivers.","sentences":["This paper investigates the use of Neural Network (NN) nonlinear modelling for Power Amplifier (PA) linearization in the Walsh-Hadamard transceiver architecture.","This novel architecture has recently been proposed for ultra-high bandwidth systems to reduce the transceiver power consumption by extensive parallelization of the digital baseband hardware.","The parallelization is achieved by replacing two-dimensional quadrature modulation with multi-dimensional Walsh-Hadamard modulation.","The open research question for this architecture is whether conventional baseband signal processing algorithms can be similarly parallelized while retaining their performance.","A key baseband algorithm, digital predistortion using NN models for PA linearization, will be adapted to the parallel Walsh architecture.","A straighforward parallelization of the state-of-the-art","NN architecture is extended with a cross-domain Knowledge Distillation pre-training method to achieve linearization performance on par with the quadrature implementation.","This result paves the way for the entire baseband processing chain to be adapted into ultra-high bandwidth, low-power Walsh transceivers."],"url":"http://arxiv.org/abs/2402.09964v1","category":"eess.SP"}
{"created":"2024-02-15 14:15:51","title":"Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through Dynamic Shift Extensions: A Deep Reinforcement Learning Approach","abstract":"Crowdsourced delivery platforms face complex scheduling challenges to match couriers and customer orders. We consider two types of crowdsourced couriers, namely, committed and occasional couriers, each with different compensation schemes. Crowdsourced delivery platforms usually schedule committed courier shifts based on predicted demand. Therefore, platforms may devise an offline schedule for committed couriers before the planning period. However, due to the unpredictability of demand, there are instances where it becomes necessary to make online adjustments to the offline schedule. In this study, we focus on the problem of dynamically adjusting the offline schedule through shift extensions for committed couriers. This problem is modeled as a sequential decision process. The objective is to maximize platform profit by determining the shift extensions of couriers and the assignments of requests to couriers. To solve the model, a Deep Q-Network (DQN) learning approach is developed. Comparing this model with the baseline policy where no extensions are allowed demonstrates the benefits that platforms can gain from allowing shift extensions in terms of reward, reduced lost order costs, and lost requests. Additionally, sensitivity analysis showed that the total extension compensation increases in a nonlinear manner with the arrival rate of requests, and in a linear manner with the arrival rate of occasional couriers. On the compensation sensitivity, the results showed that the normal scenario exhibited the highest average number of shift extensions and, consequently, the fewest average number of lost requests. These findings serve as evidence of the successful learning of such dynamics by the DQN algorithm.","sentences":["Crowdsourced delivery platforms face complex scheduling challenges to match couriers and customer orders.","We consider two types of crowdsourced couriers, namely, committed and occasional couriers, each with different compensation schemes.","Crowdsourced delivery platforms usually schedule committed courier shifts based on predicted demand.","Therefore, platforms may devise an offline schedule for committed couriers before the planning period.","However, due to the unpredictability of demand, there are instances where it becomes necessary to make online adjustments to the offline schedule.","In this study, we focus on the problem of dynamically adjusting the offline schedule through shift extensions for committed couriers.","This problem is modeled as a sequential decision process.","The objective is to maximize platform profit by determining the shift extensions of couriers and the assignments of requests to couriers.","To solve the model, a Deep Q-Network (DQN) learning approach is developed.","Comparing this model with the baseline policy where no extensions are allowed demonstrates the benefits that platforms can gain from allowing shift extensions in terms of reward, reduced lost order costs, and lost requests.","Additionally, sensitivity analysis showed that the total extension compensation increases in a nonlinear manner with the arrival rate of requests, and in a linear manner with the arrival rate of occasional couriers.","On the compensation sensitivity, the results showed that the normal scenario exhibited the highest average number of shift extensions and, consequently, the fewest average number of lost requests.","These findings serve as evidence of the successful learning of such dynamics by the DQN algorithm."],"url":"http://arxiv.org/abs/2402.09961v1","category":"cs.LG"}
{"created":"2024-02-15 14:09:28","title":"LLM-based Federated Recommendation","abstract":"Large Language Models (LLMs), with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods. However, fine-tuning requires users' behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information. The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues. To mitigate these privacy issues, Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-based recommendation presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system's efficiency over time, and second, a high demand on clients' computational and storage resources for local training and inference of LLMs.   To address these challenges, we introduce a Privacy-Preserving LLM-based Recommendation (PPLR) framework. The PPLR framework employs two primary strategies. First, it implements a dynamic balance strategy, which involves the design of dynamic parameter aggregation and adjustment of learning speed for different clients during the training phase, to ensure relatively balanced performance across all clients. Second, PPLR adopts a flexible storage strategy, selectively retaining certain sensitive layers of the language model on the client side while offloading non-sensitive layers to the server. This approach aims to preserve user privacy while efficiently saving computational and storage resources. Experimental results demonstrate that PPLR not only achieves a balanced performance among clients but also enhances overall system performance in a manner that is both computationally and storage-efficient, while effectively protecting user privacy.","sentences":["Large Language Models (LLMs), with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods.","However, fine-tuning requires users' behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information.","The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues.","To mitigate these privacy issues, Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach.","Nevertheless, applying Fed4Rec to LLM-based recommendation presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system's efficiency over time, and second, a high demand on clients' computational and storage resources for local training and inference of LLMs.   ","To address these challenges, we introduce a Privacy-Preserving LLM-based Recommendation (PPLR) framework.","The PPLR framework employs two primary strategies.","First, it implements a dynamic balance strategy, which involves the design of dynamic parameter aggregation and adjustment of learning speed for different clients during the training phase, to ensure relatively balanced performance across all clients.","Second, PPLR adopts a flexible storage strategy, selectively retaining certain sensitive layers of the language model on the client side while offloading non-sensitive layers to the server.","This approach aims to preserve user privacy while efficiently saving computational and storage resources.","Experimental results demonstrate that PPLR not only achieves a balanced performance among clients but also enhances overall system performance in a manner that is both computationally and storage-efficient, while effectively protecting user privacy."],"url":"http://arxiv.org/abs/2402.09959v1","category":"cs.IR"}
{"created":"2024-02-15 14:08:08","title":"On Designing Features for Condition Monitoring of Rotating Machines","abstract":"Various methods for designing input features have been proposed for fault recognition in rotating machines using one-dimensional raw sensor data. The available methods are complex, rely on empirical approaches, and may differ depending on the condition monitoring data used. Therefore, this article proposes a novel algorithm to design input features that unifies the feature extraction process for different time-series sensor data. This new insight for designing/extracting input features is obtained through the lens of histogram theory. The proposed algorithm extracts discriminative input features, which are suitable for a simple classifier to deep neural network-based classifiers. The designed input features are given as input to the classifier with end-to-end training in a single framework for machine conditions recognition. The proposed scheme has been validated through three real-time datasets: a) acoustic dataset, b) CWRU vibration dataset, and c) IMS vibration dataset. The real-time results and comparative study show the effectiveness of the proposed scheme for the prediction of the machine's health states.","sentences":["Various methods for designing input features have been proposed for fault recognition in rotating machines using one-dimensional raw sensor data.","The available methods are complex, rely on empirical approaches, and may differ depending on the condition monitoring data used.","Therefore, this article proposes a novel algorithm to design input features that unifies the feature extraction process for different time-series sensor data.","This new insight for designing/extracting input features is obtained through the lens of histogram theory.","The proposed algorithm extracts discriminative input features, which are suitable for a simple classifier to deep neural network-based classifiers.","The designed input features are given as input to the classifier with end-to-end training in a single framework for machine conditions recognition.","The proposed scheme has been validated through three real-time datasets: a) acoustic dataset, b) CWRU vibration dataset, and c) IMS vibration dataset.","The real-time results and comparative study show the effectiveness of the proposed scheme for the prediction of the machine's health states."],"url":"http://arxiv.org/abs/2402.09957v1","category":"cs.LG"}
{"created":"2024-02-15 13:53:50","title":"A convenient setting for infinite-dimensional analysis","abstract":"In this work, we propose a convenient framework for infinite-dimensional analysis (including real and complex analysis), in which differentiation (in some weak sense) and integration operations can be easily performed, integration by parts can be conveniently established under rather weak conditions, and especially some nice properties and consequences obtained by convolution in Euclidean spaces can be extended to infinite-dimensional spaces in some sense by taking the limit. Compared to the existing tools in infinite-dimensional analysis, our framework enjoys more convenient and clearer links with that of finite dimensions, and hence it is more suitable for computation and studying some analysis problems in infinite-dimensional spaces.","sentences":["In this work, we propose a convenient framework for infinite-dimensional analysis (including real and complex analysis), in which differentiation (in some weak sense) and integration operations can be easily performed, integration by parts can be conveniently established under rather weak conditions, and especially some nice properties and consequences obtained by convolution in Euclidean spaces can be extended to infinite-dimensional spaces in some sense by taking the limit.","Compared to the existing tools in infinite-dimensional analysis, our framework enjoys more convenient and clearer links with that of finite dimensions, and hence it is more suitable for computation and studying some analysis problems in infinite-dimensional spaces."],"url":"http://arxiv.org/abs/2402.09950v1","category":"math.FA"}
{"created":"2024-02-15 13:51:21","title":"Neural 5G Indoor Localization with IMU Supervision","abstract":"Radio signals are well suited for user localization because they are ubiquitous, can operate in the dark and maintain privacy. Many prior works learn mappings between channel state information (CSI) and position fully-supervised. However, that approach relies on position labels which are very expensive to acquire. In this work, this requirement is relaxed by using pseudo-labels during deployment, which are calculated from an inertial measurement unit (IMU). We propose practical algorithms for IMU double integration and training of the localization system. We show decimeter-level accuracy on simulated and challenging real data of 5G measurements. Our IMU-supervised method performs similarly to fully-supervised, but requires much less effort to deploy.","sentences":["Radio signals are well suited for user localization because they are ubiquitous, can operate in the dark and maintain privacy.","Many prior works learn mappings between channel state information (CSI) and position fully-supervised.","However, that approach relies on position labels which are very expensive to acquire.","In this work, this requirement is relaxed by using pseudo-labels during deployment, which are calculated from an inertial measurement unit (IMU).","We propose practical algorithms for IMU double integration and training of the localization system.","We show decimeter-level accuracy on simulated and challenging real data of 5G measurements.","Our IMU-supervised method performs similarly to fully-supervised, but requires much less effort to deploy."],"url":"http://arxiv.org/abs/2402.09948v1","category":"eess.SP"}
{"created":"2024-02-15 13:26:52","title":"Modeling considerations about a microfluidic thermal device","abstract":"Many computational studies on hotspot microfluidic cooling devices found in the literature rely on simplified assumptions and conventions that fail to capture the full complexity of the problem. In this work it is analyzed the modeling of the overall heat transfer in a microfluidic device, focusing on the effect of the different thermal transfer mechanisms (conduction, convection and radiation), temperature-dependent thermophysical properties of the fluid, and the device material. The study is developed as a function of the pressure difference applied on the system based on simulations performed using a Finite Volume Method. Analyzing and comparing the contribution of the different factors to the energy losses, this work provides a critical discussion on the approximations usually considered, in order to perform a reliable modeling of the overall thermal performance of a real PDMS-based microfluidic device.","sentences":["Many computational studies on hotspot microfluidic cooling devices found in the literature rely on simplified assumptions and conventions that fail to capture the full complexity of the problem.","In this work it is analyzed the modeling of the overall heat transfer in a microfluidic device, focusing on the effect of the different thermal transfer mechanisms (conduction, convection and radiation), temperature-dependent thermophysical properties of the fluid, and the device material.","The study is developed as a function of the pressure difference applied on the system based on simulations performed using a Finite Volume Method.","Analyzing and comparing the contribution of the different factors to the energy losses, this work provides a critical discussion on the approximations usually considered, in order to perform a reliable modeling of the overall thermal performance of a real PDMS-based microfluidic device."],"url":"http://arxiv.org/abs/2402.09931v1","category":"physics.flu-dyn"}
{"created":"2024-02-15 13:14:38","title":"Predicting the Energy Demand of a Hardware Video Decoder with Unknown Design Using Software Profiling","abstract":"Energy efficiency for video communications and video-on-demand streaming is essential for mobile devices with a limited battery capacity. Therefore, hardware (HW) decoder implementations are commonly used to significantly reduce the energetic load of video playback. The energy consumption of such a HW implementation largely depends on a previously finalized standardization of a video codec that specifies which coding tools and methods are included in the new video codec. However, during the standardization, the true complexity of a HW implementation is unknown, and the adoption of coding tools relies solely on the expertise of experts in the industry. By using software (SW) decoder profiling, we are able to estimate the SW decoding energy demand with an average error of 1.25%. We propose a method that accurately models the energy demand of existing HW decoders with an average error of 1.79% by exploiting information from software (SW) decoder profiling. Motivated by the low estimation error, we propose a HW decoding energy metric that can predict and estimate the complexity of an unknown HW implementation using information from existing HW decoder implementations and available SW implementations of the future video decoder. By using multiple video codecs for model training, we can predict the complexity of a HW decoder with an error of less than 8% and a minimum error of 4.54% without using the corresponding HW decoder for training.","sentences":["Energy efficiency for video communications and video-on-demand streaming is essential for mobile devices with a limited battery capacity.","Therefore, hardware (HW) decoder implementations are commonly used to significantly reduce the energetic load of video playback.","The energy consumption of such a HW implementation largely depends on a previously finalized standardization of a video codec that specifies which coding tools and methods are included in the new video codec.","However, during the standardization, the true complexity of a HW implementation is unknown, and the adoption of coding tools relies solely on the expertise of experts in the industry.","By using software (SW) decoder profiling, we are able to estimate the SW decoding energy demand with an average error of 1.25%.","We propose a method that accurately models the energy demand of existing HW decoders with an average error of 1.79% by exploiting information from software (SW) decoder profiling.","Motivated by the low estimation error, we propose a HW decoding energy metric that can predict and estimate the complexity of an unknown HW implementation using information from existing HW decoder implementations and available SW implementations of the future video decoder.","By using multiple video codecs for model training, we can predict the complexity of a HW decoder with an error of less than 8% and a minimum error of 4.54% without using the corresponding HW decoder for training."],"url":"http://arxiv.org/abs/2402.09926v1","category":"eess.IV"}
{"created":"2024-02-15 12:55:37","title":"Melting of a vortex matter Wigner crystal","abstract":"The two-dimensional One-Component Plasma (OCP) is a foundational model of the statistical mechanics of interacting particles, describing phenomena common to astrophysics, turbulence, and the Fractional Quantum Hall Effect (FQHE). Despite an extensive literature, the phase diagram of the 2D OCP is still a subject of some controversy. Here we develop a \"vortex matter\" simulator to realize the logarithmic-interaction OCP experimentally by exploiting the topological character of quantized vortices in a thin superfluid layer. Precision optical-tweezer control of the location of quantized vortices enables direct preparation of the OCP ground state with or without defects, and heating from acoustic excitations allows the observation of the melting transition from the solid Wigner crystal through the liquid phase. We present novel theoretical analysis that is in quantitative agreement with experimental observations, and demonstrates how equilibrium states are achieved through the system dynamics. This allows a precise measurement of the superfluid-thermal cloud mutual friction and heating coefficients. This platform provides a route towards solving a number of open problems in systems with long-range interactions. At equilibrium, it could distinguish between the competing scenarios of grain boundary melting and KTHNY theory. Dynamical simulators could test the existence of predicted edge-wave solitons which form a hydrodynamic analogue of topological edge states in the FQHE.","sentences":["The two-dimensional One-Component Plasma (OCP) is a foundational model of the statistical mechanics of interacting particles, describing phenomena common to astrophysics, turbulence, and the Fractional Quantum Hall Effect (FQHE).","Despite an extensive literature, the phase diagram of the 2D OCP is still a subject of some controversy.","Here we develop a \"vortex matter\" simulator to realize the logarithmic-interaction OCP experimentally by exploiting the topological character of quantized vortices in a thin superfluid layer.","Precision optical-tweezer control of the location of quantized vortices enables direct preparation of the OCP ground state with or without defects, and heating from acoustic excitations allows the observation of the melting transition from the solid Wigner crystal through the liquid phase.","We present novel theoretical analysis that is in quantitative agreement with experimental observations, and demonstrates how equilibrium states are achieved through the system dynamics.","This allows a precise measurement of the superfluid-thermal cloud mutual friction and heating coefficients.","This platform provides a route towards solving a number of open problems in systems with long-range interactions.","At equilibrium, it could distinguish between the competing scenarios of grain boundary melting and KTHNY theory.","Dynamical simulators could test the existence of predicted edge-wave solitons which form a hydrodynamic analogue of topological edge states in the FQHE."],"url":"http://arxiv.org/abs/2402.09920v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-15 12:33:50","title":"Computing the EHZ capacity is NP-hard","abstract":"The Ekeland-Hofer-Zehnder capacity (EHZ capacity) is a fundamental symplectic invariant of convex bodies. We show that computing the EHZ capacity of polytopes is NP-hard. For this we reduce the feedback arc set problem in bipartite tournaments to computing the EHZ capacity of simplices.","sentences":["The Ekeland-Hofer-Zehnder capacity (EHZ capacity) is a fundamental symplectic invariant of convex bodies.","We show that computing the EHZ capacity of polytopes is NP-hard.","For this we reduce the feedback arc set problem in bipartite tournaments to computing the EHZ capacity of simplices."],"url":"http://arxiv.org/abs/2402.09914v1","category":"math.SG"}
{"created":"2024-02-15 12:21:23","title":"Efficient implementation of MPC for tracking using ADMM by decoupling its semi-banded structure","abstract":"Model Predictive Control (MPC) for tracking formulation presents numerous advantages compared to standard MPC, such as a larger domain of attraction and recursive feasibility even when abrupt changes in the reference are produced. As a drawback, it includes some extra decision variables in its related optimization problem, leading to a semi-banded structure that differs from the banded structure encountered in standard MPC. This semi-banded structure prevents the direct use of the efficient algorithms available for banded problems. To address this issue, we present an algorithm based on the alternating direction method of multipliers that explicitly takes advantage of the underlying semi-banded structure of the MPC for tracking.","sentences":["Model Predictive Control (MPC) for tracking formulation presents numerous advantages compared to standard MPC, such as a larger domain of attraction and recursive feasibility even when abrupt changes in the reference are produced.","As a drawback, it includes some extra decision variables in its related optimization problem, leading to a semi-banded structure that differs from the banded structure encountered in standard MPC.","This semi-banded structure prevents the direct use of the efficient algorithms available for banded problems.","To address this issue, we present an algorithm based on the alternating direction method of multipliers that explicitly takes advantage of the underlying semi-banded structure of the MPC for tracking."],"url":"http://arxiv.org/abs/2402.09912v1","category":"eess.SY"}
{"created":"2024-02-15 11:58:42","title":"Towards Federated Learning on the Quantum Internet","abstract":"While the majority of focus in quantum computing has so far been on monolithic quantum systems, quantum communication networks and the quantum internet in particular are increasingly receiving attention from researchers and industry alike. The quantum internet may allow a plethora of applications such as distributed or blind quantum computing, though research still is at an early stage, both for its physical implementation as well as algorithms; thus suitable applications are an open research question. We evaluate a potential application for the quantum internet, namely quantum federated learning. We run experiments under different settings in various scenarios (e.g. network constraints) using several datasets from different domains and show that (1) quantum federated learning is a valid alternative for regular training and (2) network topology and nature of training are crucial considerations as they may drastically influence the models performance. The results indicate that more comprehensive research is required to optimally deploy quantum federated learning on a potential quantum internet.","sentences":["While the majority of focus in quantum computing has so far been on monolithic quantum systems, quantum communication networks and the quantum internet in particular are increasingly receiving attention from researchers and industry alike.","The quantum internet may allow a plethora of applications such as distributed or blind quantum computing, though research still is at an early stage, both for its physical implementation as well as algorithms; thus suitable applications are an open research question.","We evaluate a potential application for the quantum internet, namely quantum federated learning.","We run experiments under different settings in various scenarios (e.g. network constraints) using several datasets from different domains and show that (1) quantum federated learning is a valid alternative for regular training and (2) network topology and nature of training are crucial considerations as they may drastically influence the models performance.","The results indicate that more comprehensive research is required to optimally deploy quantum federated learning on a potential quantum internet."],"url":"http://arxiv.org/abs/2402.09902v1","category":"quant-ph"}
{"created":"2024-02-15 11:55:13","title":"Asymptotic construction of locally repairable codes with multiple recovering sets","abstract":"Locally repairable codes have been extensively investigated due to practical applications in distributed and cloud storage systems in recent years. However, not much work on asymptotic behavior of locally repairable codes has been done. In particular, there is few result on constructive lower bound of asymptotic behavior of locally repairable codes with multiple recovering sets. In this paper, we construct some families of asymptotically good locally repairable codes with multiple recovering sets via automorphism groups of function fields of the Garcia-Stichtenoth towers. The main advantage of our construction is to allow more flexibility of localities.","sentences":["Locally repairable codes have been extensively investigated due to practical applications in distributed and cloud storage systems in recent years.","However, not much work on asymptotic behavior of locally repairable codes has been done.","In particular, there is few result on constructive lower bound of asymptotic behavior of locally repairable codes with multiple recovering sets.","In this paper, we construct some families of asymptotically good locally repairable codes with multiple recovering sets via automorphism groups of function fields of the Garcia-Stichtenoth towers.","The main advantage of our construction is to allow more flexibility of localities."],"url":"http://arxiv.org/abs/2402.09898v1","category":"cs.IT"}
{"created":"2024-02-15 11:38:32","title":"A distributive lattice of model structures relating to spectral sequences","abstract":"The $S$-model category structures on filtered chain complexes and bicomplexes were introduced by Cirici, Egas Santander, Livernet and Whitehouse and later generalised by this author. In this paper we show they are left proper, cellular and stable model categories. We use these properties and the Cellularization Principle of Greenlees and Shipley to show that an adjunction with right adjoint the product totalisation functor from bicomplexes to filtered chains is a Quillen equivalence. Combined with other known Quillen equivalences between filtered chains this shows these model categories all present the same homotopy category. We also construct a distributive lattice whose elements are the $S$-model categories of filtered chain complexes.","sentences":["The $S$-model category structures on filtered chain complexes and bicomplexes were introduced by Cirici, Egas Santander, Livernet and Whitehouse and later generalised by this author.","In this paper we show they are left proper, cellular and stable model categories.","We use these properties and the Cellularization Principle of Greenlees and Shipley to show that an adjunction with right adjoint the product totalisation functor from bicomplexes to filtered chains is a Quillen equivalence.","Combined with other known Quillen equivalences between filtered chains this shows these model categories all present the same homotopy category.","We also construct a distributive lattice whose elements are the $S$-model categories of filtered chain complexes."],"url":"http://arxiv.org/abs/2402.09893v1","category":"math.AT"}
{"created":"2024-02-15 11:36:52","title":"Mallows Product Measure","abstract":"Q-exchangeable ergodic distributions on the infinite symmetric group were classified by Gnedin-Olshanski (2012). In this paper, we study a specific linear combination of the ergodic measures and call it the Mallows product measure. From a particle system perspective, the Mallows product measure is a reversible stationary blocking measure of the infinite-species ASEP and it is a natural multi-species extension of the Bernoulli product blocking measures of the one-species ASEP. Moreover, the Mallows product measure can be viewed as the universal product blocking measure of interacting particle systems coming from random walks on Hecke algebras.   For the random infinite permutation distributed according to the Mallows product measure we have computed the joint distribution of its neighboring displacements, as well as several other observables. The key feature of the obtained formulas is their remarkably simple product structure. We project these formulas to ASEP with finitely many species, which in particular recovers a recent result of Adams-Balazs-Jay, and also to ASEP(q,M).   Our main tools are results of Gnedin-Olshanski about ergodic Mallows measures and shift-invariance symmetries of the stochastic colored six vertex model discovered by Borodin-Gorin-Wheeler and Galashin.","sentences":["Q-exchangeable ergodic distributions on the infinite symmetric group were classified by Gnedin-Olshanski (2012).","In this paper, we study a specific linear combination of the ergodic measures and call it the Mallows product measure.","From a particle system perspective, the Mallows product measure is a reversible stationary blocking measure of the infinite-species ASEP and it is a natural multi-species extension of the Bernoulli product blocking measures of the one-species ASEP.","Moreover, the Mallows product measure can be viewed as the universal product blocking measure of interacting particle systems coming from random walks on Hecke algebras.   ","For the random infinite permutation distributed according to the Mallows product measure we have computed the joint distribution of its neighboring displacements, as well as several other observables.","The key feature of the obtained formulas is their remarkably simple product structure.","We project these formulas to ASEP with finitely many species, which in particular recovers a recent result of Adams-Balazs-Jay, and also to ASEP(q,M).   ","Our main tools are results of Gnedin-Olshanski about ergodic Mallows measures and shift-invariance symmetries of the stochastic colored six vertex model discovered by Borodin-Gorin-Wheeler and Galashin."],"url":"http://arxiv.org/abs/2402.09892v1","category":"math.PR"}
{"created":"2024-02-15 11:20:34","title":"Strong coupling in molecular systems: a simple predictor employing routine optical measurements","abstract":"We provide a simple method that enables readily acquired experimental data to be used to predict whether or not a candidate molecular material may exhibit strong coupling. Specifically, we explore the relationship between the hybrid molecular/photonic (polaritonic) states and the bulk optical response of the molecular material. For a given material this approach enables a prediction of the maximum extent of strong coupling (vacuum Rabi splitting), irrespective of the nature of the confined light field. We provide formulae for the upper limit of the splitting in terms of the molar absorption coefficient, the attenuation coefficient, the extinction coefficient (imaginary part of the refractive index) and the absorbance. To illustrate this approach we provide a number of examples, we also discuss some of the limitations of our approach.","sentences":["We provide a simple method that enables readily acquired experimental data to be used to predict whether or not a candidate molecular material may exhibit strong coupling.","Specifically, we explore the relationship between the hybrid molecular/photonic (polaritonic) states and the bulk optical response of the molecular material.","For a given material this approach enables a prediction of the maximum extent of strong coupling (vacuum Rabi splitting), irrespective of the nature of the confined light field.","We provide formulae for the upper limit of the splitting in terms of the molar absorption coefficient, the attenuation coefficient, the extinction coefficient (imaginary part of the refractive index) and the absorbance.","To illustrate this approach we provide a number of examples, we also discuss some of the limitations of our approach."],"url":"http://arxiv.org/abs/2402.09885v1","category":"physics.optics"}
{"created":"2024-02-15 10:59:35","title":"Space-resolved dynamic light scattering within a millimetric drop: from Brownian diffusion to the swelling of hydrogel beads","abstract":"We present a novel dynamic light scattering setup to probe, with time and space resolution, the microscopic dynamics of soft matter systems confined within millimeter-sized spherical drops. By using an ad-hoc optical layout, we tackle the challenges raised by refraction effects due to the unconventional shape of the samples. We first validate the setup by investigating the dynamics of a suspension of Brownian particles. The dynamics measured at different positions in the drop, and hence different scattering angles, are found to be in excellent agreement with those obtained for the same sample in a conventional light scattering setup. We then demonstrate the setup capabilities by investigating a bead made of a polymer hydrogel undergoing swelling. The gel microscopic dynamics exhibit a space dependence that strongly varies with time elapsed since the beginning of swelling. Initially, the dynamics in the periphery of the bead are much faster than in the core, indicative of non-uniform swelling. As the swelling proceeds, the dynamics slow down and become more spatially homogeneous. By comparing the experimental results to numerical and analytical calculations for the dynamics of a homogeneous, purely elastic sphere undergoing swelling, we establish that the mean square displacement of the gel strands deviates from the affine motion inferred from the macroscopic deformation, evolving from fast diffusive-like dynamics at the onset of swelling to slower, yet supradiffusive, rearrangements at later stages.","sentences":["We present a novel dynamic light scattering setup to probe, with time and space resolution, the microscopic dynamics of soft matter systems confined within millimeter-sized spherical drops.","By using an ad-hoc optical layout, we tackle the challenges raised by refraction effects due to the unconventional shape of the samples.","We first validate the setup by investigating the dynamics of a suspension of Brownian particles.","The dynamics measured at different positions in the drop, and hence different scattering angles, are found to be in excellent agreement with those obtained for the same sample in a conventional light scattering setup.","We then demonstrate the setup capabilities by investigating a bead made of a polymer hydrogel undergoing swelling.","The gel microscopic dynamics exhibit a space dependence that strongly varies with time elapsed since the beginning of swelling.","Initially, the dynamics in the periphery of the bead are much faster than in the core, indicative of non-uniform swelling.","As the swelling proceeds, the dynamics slow down and become more spatially homogeneous.","By comparing the experimental results to numerical and analytical calculations for the dynamics of a homogeneous, purely elastic sphere undergoing swelling, we establish that the mean square displacement of the gel strands deviates from the affine motion inferred from the macroscopic deformation, evolving from fast diffusive-like dynamics at the onset of swelling to slower, yet supradiffusive, rearrangements at later stages."],"url":"http://arxiv.org/abs/2402.09875v1","category":"cond-mat.soft"}
{"created":"2024-02-15 10:47:44","title":"Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking","abstract":"Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation. However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the KF. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets.","sentences":["Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation.","However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns.","To address these limitations, we propose two innovative data-driven filtering methods.","Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy.","Moreover, it dispenses with most domain-specific design choices characteristic of the KF.","The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise.","Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods.","Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to.","We also conduct noise robustness analysis of our filters with convincing positive results.","We further propose a new cost function for associating observations with tracks.","Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets."],"url":"http://arxiv.org/abs/2402.09865v1","category":"cs.CV"}
{"created":"2024-02-15 10:28:41","title":"From Exploration to End of Life: Unpacking Sustainability in Physicalization Practices","abstract":"Data physicalizations have gained prominence across domains, but their environmental impact has been largely overlooked. This work addresses this gap by investigating the interplay between sustainability and physicalization practices. We conducted interviews with experts from diverse backgrounds, followed by a survey to gather insights into how they approach physicalization projects and reflect on sustainability. Our thematic analysis revealed sustainability considerations throughout the entire physicalization life cycle -- a framework that encompasses various stages in a physicalization's existence. Notably, we found no single agreed-upon definition for sustainable physicalizations, highlighting the complexity of integrating sustainability into physicalization practices. We outline sustainability challenges and strategies based on participants' experiences and propose the Sustainable Physicalization Practices (SuPPra) Matrix, providing a structured approach for designers to reflect on and enhance the environmental impact of their future physicalizations.","sentences":["Data physicalizations have gained prominence across domains, but their environmental impact has been largely overlooked.","This work addresses this gap by investigating the interplay between sustainability and physicalization practices.","We conducted interviews with experts from diverse backgrounds, followed by a survey to gather insights into how they approach physicalization projects and reflect on sustainability.","Our thematic analysis revealed sustainability considerations throughout the entire physicalization life cycle -- a framework that encompasses various stages in a physicalization's existence.","Notably, we found no single agreed-upon definition for sustainable physicalizations, highlighting the complexity of integrating sustainability into physicalization practices.","We outline sustainability challenges and strategies based on participants' experiences and propose the Sustainable Physicalization Practices (SuPPra) Matrix, providing a structured approach for designers to reflect on and enhance the environmental impact of their future physicalizations."],"url":"http://arxiv.org/abs/2402.09860v1","category":"cs.HC"}
{"created":"2024-02-15 10:22:00","title":"Exploring the magnetic field of Helmholtz and Maxwell coils: a computer-based approach exploiting the superposition principle","abstract":"Teaching magnetism is one of the most challenging topics at undergraduate level in programmes with scientific background. A basic course includes the description of the magnetic interaction along with empirical results such as the Biot-Savart law's. However, evaluating the magnetic field due to certain current carrying system at any point in space is not an easy task, especially for points in space where symmetry arguments cannot be applied. In this paper we study the magnetic field produced by both Helmholtz and Maxwell coils at all points in space by using a hybrid methodology that combines the superposition principle and an analytical result. We implement a computational approach, that is based on iterating $n$ times the magnetic field produced by a finite current-carrying wire, to evaluate the magnetic field at any point in space for coils arrangements without using advanced calculus. This methodology helps teachers and students to explore the field due to systems with different levels of complexity, combining analytical and computational skills to visualize and analyse the magnetic field. After our analysis, we show that this is an useful approach to emphasize fundamental concepts and mitigate some of the issues that arise when evaluating the magnetic field for systems proposed in introductory physics textbooks.","sentences":["Teaching magnetism is one of the most challenging topics at undergraduate level in programmes with scientific background.","A basic course includes the description of the magnetic interaction along with empirical results such as the Biot-Savart law's.","However, evaluating the magnetic field due to certain current carrying system at any point in space is not an easy task, especially for points in space where symmetry arguments cannot be applied.","In this paper we study the magnetic field produced by both Helmholtz and Maxwell coils at all points in space by using a hybrid methodology that combines the superposition principle and an analytical result.","We implement a computational approach, that is based on iterating $n$ times the magnetic field produced by a finite current-carrying wire, to evaluate the magnetic field at any point in space for coils arrangements without using advanced calculus.","This methodology helps teachers and students to explore the field due to systems with different levels of complexity, combining analytical and computational skills to visualize and analyse the magnetic field.","After our analysis, we show that this is an useful approach to emphasize fundamental concepts and mitigate some of the issues that arise when evaluating the magnetic field for systems proposed in introductory physics textbooks."],"url":"http://arxiv.org/abs/2402.09856v1","category":"physics.ed-ph"}
{"created":"2024-02-15 10:03:35","title":"JustSTART: How to Find an RSA Authentication Bypass on Xilinx UltraScale(+) with Fuzzing","abstract":"Fuzzing is a well-established technique in the software domain to uncover bugs and vulnerabilities. Yet, applications of fuzzing for security vulnerabilities in hardware systems are scarce, as principal reasons are requirements for design information access (HDL source code). Moreover, observation of internal hardware state during runtime is typically an ineffective information source, as its documentation is often not publicly available. In addition, such observation during runtime is also inefficient due to bandwidth-limited analysis interfaces (JTAG, and minimal introspection of internal modules). In this work, we investigate fuzzing for 7-Series and UltraScale(+) FPGA configuration engines, the control plane governing the (secure) bitstream configuration within the FPGA. Our goal is to examine the effectiveness of fuzzing to analyze and document the opaque inner workings of FPGA configuration engines, with a primary emphasis on identifying security vulnerabilities. Using only the publicly available chip and dispersed documentation, we first design and implement ConFuzz, an advanced FPGA configuration engine fuzzing and rapid prototyping framework. Based on our detailed understanding of the bitstream file format, we then systematically define 3 novel key fuzzing strategies for Xilinx configuration engines. Moreover, our strategies are executed through mutational structure-aware fuzzers and incorporate various novel custom-tailored, FPGA-specific optimizations. Our evaluation reveals previously undocumented behavior within the configuration engine, including critical findings such as system crashes leading to unresponsive states of the FPGA. In addition, our investigations not only lead to the rediscovery of the starbleed attack but also uncover JustSTART (CVE-2023-20570), capable of circumventing RSA authentication for Xilinx UltraScale(+). Note that we also discuss countermeasures.","sentences":["Fuzzing is a well-established technique in the software domain to uncover bugs and vulnerabilities.","Yet, applications of fuzzing for security vulnerabilities in hardware systems are scarce, as principal reasons are requirements for design information access (HDL source code).","Moreover, observation of internal hardware state during runtime is typically an ineffective information source, as its documentation is often not publicly available.","In addition, such observation during runtime is also inefficient due to bandwidth-limited analysis interfaces (JTAG, and minimal introspection of internal modules).","In this work, we investigate fuzzing for 7-Series and UltraScale(+) FPGA configuration engines, the control plane governing the (secure) bitstream configuration within the FPGA.","Our goal is to examine the effectiveness of fuzzing to analyze and document the opaque inner workings of FPGA configuration engines, with a primary emphasis on identifying security vulnerabilities.","Using only the publicly available chip and dispersed documentation, we first design and implement ConFuzz, an advanced FPGA configuration engine fuzzing and rapid prototyping framework.","Based on our detailed understanding of the bitstream file format, we then systematically define 3 novel key fuzzing strategies for Xilinx configuration engines.","Moreover, our strategies are executed through mutational structure-aware fuzzers and incorporate various novel custom-tailored, FPGA-specific optimizations.","Our evaluation reveals previously undocumented behavior within the configuration engine, including critical findings such as system crashes leading to unresponsive states of the FPGA.","In addition, our investigations not only lead to the rediscovery of the starbleed attack but also uncover JustSTART (CVE-2023-20570), capable of circumventing RSA authentication for Xilinx UltraScale(+).","Note that we also discuss countermeasures."],"url":"http://arxiv.org/abs/2402.09845v1","category":"cs.CR"}
{"created":"2024-02-15 10:01:31","title":"Functions of compact operators under trace class perturbations","abstract":"The paper studies the problem, for which continuous functions $f$ on the real line ${\\Bbb R}$, the difference of the functions $f(B)-f(A)$ of self-adjoint operators $A$ and $B$ with trace class difference must also be of trace class. The main result of the paper shows that this happens if and only if the function $f$ is operator Lipschitz on a neighbourhood of zero.","sentences":["The paper studies the problem, for which continuous functions $f$ on the real line ${\\Bbb R}$, the difference of the functions $f(B)-f(A)$ of self-adjoint operators $A$ and $B$ with trace class difference must also be of trace class.","The main result of the paper shows that this happens if and only if the function $f$ is operator Lipschitz on a neighbourhood of zero."],"url":"http://arxiv.org/abs/2402.09843v1","category":"math.FA"}
{"created":"2024-02-15 10:01:07","title":"Symbolic Solution of Systems of Polynomial Differential Equations Via The Cauchy-Riemann Equations. Applications to Kinetic Differential Equations","abstract":"The differential equations of chemical kinetics are systems of nonlinear (polynomial) differential equations, therefore their solutions cannot usually be found in symbolic form. Here we offer a method to solve classes of kinetic differential equations based on the Cauchy--Riemann equations. It turns out that the method can be used to symbolically solve some polynomial differential equations that are not necessarily kinetic, as well.","sentences":["The differential equations of chemical kinetics are systems of nonlinear (polynomial) differential equations, therefore their solutions cannot usually be found in symbolic form.","Here we offer a method to solve classes of kinetic differential equations based on the Cauchy--Riemann equations.","It turns out that the method can be used to symbolically solve some polynomial differential equations that are not necessarily kinetic, as well."],"url":"http://arxiv.org/abs/2402.09842v1","category":"math-ph"}
{"created":"2024-02-15 09:52:03","title":"R4: rapid reproducible robotics research open hardware control system","abstract":"A key component of any robot is the interface between ROS2 software and physical motors. New robots often use arbitrary, messy mixtures of closed and open motor drivers and error-prone physical mountings, wiring, and connectors to interface them. There is a need for a standardizing OSH component to abstract this complexity, as Arduino did for interfacing to smaller components. We present a OSH printed circuit board to solve this problem once and for all. On the high-level side, it interfaces to Arduino Giga -- acting as an unusually large and robust shield -- and thus to existing open source ROS software stacks. On the lower-level side, it interfaces to existing emerging standard open hardware including OSH motor drivers and relays, which can already be used to drive fully open hardware wheeled and arm robots. This enables the creation of a family of standardized, fully open hardware, fully reproducible, research platforms.","sentences":["A key component of any robot is the interface between ROS2 software and physical motors.","New robots often use arbitrary, messy mixtures of closed and open motor drivers and error-prone physical mountings, wiring, and connectors to interface them.","There is a need for a standardizing OSH component to abstract this complexity, as Arduino did for interfacing to smaller components.","We present a OSH printed circuit board to solve this problem once and for all.","On the high-level side, it interfaces to Arduino Giga -- acting as an unusually large and robust shield -- and thus to existing open source ROS software stacks.","On the lower-level side, it interfaces to existing emerging standard open hardware including OSH motor drivers and relays, which can already be used to drive fully open hardware wheeled and arm robots.","This enables the creation of a family of standardized, fully open hardware, fully reproducible, research platforms."],"url":"http://arxiv.org/abs/2402.09833v1","category":"cs.RO"}
{"created":"2024-02-15 09:41:47","title":"Improved Lower Bounds for Approximating Parameterized Nearest Codeword and Related Problems under ETH","abstract":"In this paper we present a new gap-creating randomized self-reduction for parameterized Maximum Likelihood Decoding problem over $\\mathbb{F}_p$ ($k$-MLD$_p$). The reduction takes a $k$-MLD$_p$ instance with $k\\cdot n$ vectors as input, runs in time $f(k)n^{O(1)}$ for some computable function $f$, outputs a $(3/2-\\varepsilon)$-Gap-$k'$-MLD$_p$ instance for any $\\varepsilon>0$, where $k'=O(k^2\\log k)$. Using this reduction, we show that assuming the randomized Exponential Time Hypothesis (ETH), no algorithms can approximate $k$-MLD$_p$ (and therefore its dual problem $k$-NCP$_p$) within factor $(3/2-\\varepsilon)$ in $f(k)\\cdot n^{o(\\sqrt{k/\\log k})}$ time for any $\\varepsilon>0$.   We then use reduction by Bhattacharyya, Ghoshal, Karthik and Manurangsi (ICALP 2018) to amplify the $(3/2-\\varepsilon)$-gap to any constant. As a result, we show that assuming ETH, no algorithms can approximate $k$-NCP$_p$ and $k$-MDP$_p$ within $\\gamma$-factor in $f(k)n^{o(k^{\\varepsilon_\\gamma})}$ time for some constant $\\varepsilon_\\gamma>0$. Combining with the gap-preserving reduction by Bennett, Cheraghchi, Guruswami and Ribeiro (STOC 2023), we also obtain similar lower bounds for $k$-MDP$_p$, $k$-CVP$_p$ and $k$-SVP$_p$.   These results improve upon the previous $f(k)n^{\\Omega(\\mathsf{poly} \\log k)}$ lower bounds for these problems under ETH using reductions by Bhattacharyya et al. (J.ACM 2021) and Bennett et al. (STOC 2023).","sentences":["In this paper we present a new gap-creating randomized self-reduction for parameterized Maximum Likelihood Decoding problem over $\\mathbb{F}_p$ ($k$-MLD$_p$).","The reduction takes a $k$-MLD$_p$ instance with $k\\cdot n$ vectors as input, runs in time $f(k)n^{O(1)}$ for some computable function $f$, outputs a $(3/2-\\varepsilon)$-Gap-$k'$-MLD$_p$ instance for any $\\varepsilon>0$, where $k'=O(k^2\\log k)$.","Using this reduction, we show that assuming the randomized Exponential Time Hypothesis (ETH), no algorithms can approximate $k$-MLD$_p$ (and therefore its dual problem $k$-NCP$_p$) within factor $(3/2-\\varepsilon)$ in $f(k)\\cdot n^{o(\\sqrt{k/\\log k})}$ time for any $\\varepsilon>0$.   We then use reduction by Bhattacharyya, Ghoshal, Karthik and Manurangsi (ICALP 2018) to amplify the $(3/2-\\varepsilon)$-gap to any constant.","As a result, we show that assuming ETH, no algorithms can approximate $k$-NCP$_p$ and $k$-MDP$_p$ within $\\gamma$-factor in $f(k)n^{o(k^{\\varepsilon_\\gamma})}$ time for some constant $\\varepsilon_\\gamma>0$. Combining with the gap-preserving reduction by Bennett, Cheraghchi, Guruswami and Ribeiro (STOC 2023), we also obtain similar lower bounds for $k$-MDP$_p$, $k$-CVP$_p$ and $k$-SVP$_p$.   These results improve upon the previous $f(k)n^{\\Omega(\\mathsf{poly} \\log k)}$ lower bounds for these problems under ETH using reductions by Bhattacharyya et al. (J.ACM 2021) and Bennett et al.","(STOC 2023)."],"url":"http://arxiv.org/abs/2402.09825v1","category":"cs.CC"}
{"created":"2024-02-15 09:40:34","title":"On the discrete-time origins of replicator dynamics: from convergence to instability and chaos","abstract":"We consider three distinct discrete-time models of learning and evolution in games: a biological model based on intra-species selective pressure, the dynamics induced by pairwise proportional imitation, and the exponential / multiplicative weights algorithm for online learning. Even though these models share the same continuous-time limit - the replicator dynamics - we show that second-order effects play a crucial role and may lead to drastically different behaviors in each model, even in very simple, symmetric two by two games. Specifically, we study the resulting discrete-time dynamics in a class of parametrized congestion games, and we show that (i) in the biological model of intra-species competition, the dynamics remain convergent for any parameter value; (ii) the dynamics of pairwise proportional imitation for different equilibrium configurations exhibit an entire range of behaviors for large step size (stability, instability, and even Li-Yorke chaos); while (iii) for the exponential / multiplicative weights (EW) algorithm increasing step size will (almost) inevitably lead to chaos (again, in the formal, Li-Yorke sense). This divergence of behaviors comes in stark contrast to the globally convergent behavior of the replicator dynamics, and serves to delineate the extent to which the replicator dynamics provide a useful predictor for the long-run behavior of their discrete-time origins.","sentences":["We consider three distinct discrete-time models of learning and evolution in games: a biological model based on intra-species selective pressure, the dynamics induced by pairwise proportional imitation, and the exponential / multiplicative weights algorithm for online learning.","Even though these models share the same continuous-time limit - the replicator dynamics - we show that second-order effects play a crucial role and may lead to drastically different behaviors in each model, even in very simple, symmetric two by two games.","Specifically, we study the resulting discrete-time dynamics in a class of parametrized congestion games, and we show that (i) in the biological model of intra-species competition, the dynamics remain convergent for any parameter value; (ii) the dynamics of pairwise proportional imitation for different equilibrium configurations exhibit an entire range of behaviors for large step size (stability, instability, and even Li-Yorke chaos); while (iii) for the exponential / multiplicative weights (EW) algorithm increasing step size will (almost) inevitably lead to chaos (again, in the formal, Li-Yorke sense).","This divergence of behaviors comes in stark contrast to the globally convergent behavior of the replicator dynamics, and serves to delineate the extent to which the replicator dynamics provide a useful predictor for the long-run behavior of their discrete-time origins."],"url":"http://arxiv.org/abs/2402.09824v1","category":"math.DS"}
{"created":"2024-02-15 09:39:52","title":"On the classification of meromorphic affine connections on complex compact surfaces","abstract":"We classify meromorphic affine connections on compact complex surfaces with algebraic dimension one, extending the work of Inoue,Kobayashi and Ochiai (1981) in the holomorphic case. The motivation is to investigate possible extension of the principle of uniformization of complex compact curves by means of geometric structures, in higher dimensions and allowing singularities to the geometric structures.","sentences":["We classify meromorphic affine connections on compact complex surfaces with algebraic dimension one, extending the work of Inoue,Kobayashi and Ochiai (1981) in the holomorphic case.","The motivation is to investigate possible extension of the principle of uniformization of complex compact curves by means of geometric structures, in higher dimensions and allowing singularities to the geometric structures."],"url":"http://arxiv.org/abs/2402.09823v1","category":"math.AG"}
{"created":"2024-02-15 09:37:14","title":"The 2022 super-Eddington outburst of the source SMC X-2","abstract":"SMC X-2 exhibits X-ray outburst behaviour that makes it one of the most luminous X-ray sources in the Small Magellanic Cloud. In the last decade it has undergone two such massive outbursts - in 2015 and 2022. The first outburst is well reported in the literature, but the 2022 event has yet to be fully described and discussed. That is the goal of this paper. In particular, the post-peak characteristics of the two events are compared. This reveals clear similarities in decay profiles, believed to be related to different accretion mechanisms occurring at different times as the outbursts evolve. The H{\\alpha} emission line indicates that the Be disc undergoes complex structural variability, with evidence of warping as a result of its interaction with the neutron star. The detailed observations reported here will be important for modelling such interactions in this kind of binary systems.","sentences":["SMC X-2 exhibits X-ray outburst behaviour that makes it one of the most luminous X-ray sources in the Small Magellanic Cloud.","In the last decade it has undergone two such massive outbursts - in 2015 and 2022.","The first outburst is well reported in the literature, but the 2022 event has yet to be fully described and discussed.","That is the goal of this paper.","In particular, the post-peak characteristics of the two events are compared.","This reveals clear similarities in decay profiles, believed to be related to different accretion mechanisms occurring at different times as the outbursts evolve.","The H{\\alpha} emission line indicates that the Be disc undergoes complex structural variability, with evidence of warping as a result of its interaction with the neutron star.","The detailed observations reported here will be important for modelling such interactions in this kind of binary systems."],"url":"http://arxiv.org/abs/2402.09822v1","category":"astro-ph.HE"}
{"created":"2024-02-15 09:30:31","title":"On the role of electromagnetic phenomena in some atmospheric processes","abstract":"The crucial part of electromagnetic phenomena in many atmospheric processes is verified by systematized data. The multilayered charged system of clouds represents some dynamically equilibrium structure kept by the ionic and polarization forces. The estimates of acting forces are presented and it is demonstrated the necessity to take into account the plasma-like subsystems, effect on some atmospheric phenomena, including the formation and the maintenance of the structure and characteristics of their movement.","sentences":["The crucial part of electromagnetic phenomena in many atmospheric processes is verified by systematized data.","The multilayered charged system of clouds represents some dynamically equilibrium structure kept by the ionic and polarization forces.","The estimates of acting forces are presented and it is demonstrated the necessity to take into account the plasma-like subsystems, effect on some atmospheric phenomena, including the formation and the maintenance of the structure and characteristics of their movement."],"url":"http://arxiv.org/abs/2402.09815v1","category":"physics.plasm-ph"}
{"created":"2024-02-15 09:23:09","title":"Stability analysis of the vectorial lattice-Boltzmann method","abstract":"We propose a new stability analysis of the Vectorial Lattice-Boltzmann Method (VLBM). The VLBM is a variant of the LBM with extended stability features: it allows to handle compressible flows with shock waves, while the LBM is limited to low-Mach number regime. The stability analysis is based on the Legendre transform theory. We also propose a new tool: the equivalent system analysis, which we conjecture to contains both the stability and the consistency of the VLBM.","sentences":["We propose a new stability analysis of the Vectorial Lattice-Boltzmann Method (VLBM).","The VLBM is a variant of the LBM with extended stability features: it allows to handle compressible flows with shock waves, while the LBM is limited to low-Mach number regime.","The stability analysis is based on the Legendre transform theory.","We also propose a new tool: the equivalent system analysis, which we conjecture to contains both the stability and the consistency of the VLBM."],"url":"http://arxiv.org/abs/2402.09813v1","category":"math.AP"}
{"created":"2024-02-15 09:15:29","title":"3D Cooperative Localization in UAV Systems: CRLB Analysis and Security Solutions","abstract":"This paper presents a robust and secure framework for achieving accurate and reliable cooperative localization in multiple unmanned aerial vehicle (UAV) systems. The Cramer-Rao low bound (CRLB) for the three-dimensional (3D) cooperative localization network is derived, with particular attention given to the non-uniform spatial distribution of anchor nodes. Challenges of mobility and security threats are addressed, corresponding solutions are brought forth and numerically assessed . The proposed solution incorporates two key components: the Mobility Adaptive Gradient Descent (MAGD) and Time-evolving Anomaly Detection (TAD). The MAGD adapts the gradient descent algorithm to handle the configuration changes in cooperative localization systems, ensuring accurate localization in dynamic scenarios. The TAD cooperates with reputation propagation (RP) scheme to detect and mitigate potential attacks by identifying malicious data, enhancing the security and resilience of the cooperative localization.","sentences":["This paper presents a robust and secure framework for achieving accurate and reliable cooperative localization in multiple unmanned aerial vehicle (UAV) systems.","The Cramer-Rao low bound (CRLB) for the three-dimensional (3D) cooperative localization network is derived, with particular attention given to the non-uniform spatial distribution of anchor nodes.","Challenges of mobility and security threats are addressed, corresponding solutions are brought forth and numerically assessed .","The proposed solution incorporates two key components: the Mobility Adaptive Gradient Descent (MAGD) and Time-evolving Anomaly Detection (TAD).","The MAGD adapts the gradient descent algorithm to handle the configuration changes in cooperative localization systems, ensuring accurate localization in dynamic scenarios.","The TAD cooperates with reputation propagation (RP) scheme to detect and mitigate potential attacks by identifying malicious data, enhancing the security and resilience of the cooperative localization."],"url":"http://arxiv.org/abs/2402.09810v1","category":"eess.SP"}
{"created":"2024-02-15 09:13:59","title":"Two trust region type algorithms for solving nonconvex-strongly concave minimax problems","abstract":"In this paper, we propose a Minimax Trust Region (MINIMAX-TR) algorithm and a Minimax Trust Region Algorithm with Contractions and Expansions(MINIMAX-TRACE) algorithm for solving nonconvex-strongly concave minimax problems. Both algorithms can find an $(\\epsilon, \\sqrt{\\epsilon})$-second order stationary point(SSP) within $\\mathcal{O}(\\epsilon^{-1.5})$ iterations, which matches the best well known iteration complexity.","sentences":["In this paper, we propose a Minimax Trust Region (MINIMAX-TR) algorithm and a Minimax Trust Region Algorithm with Contractions and Expansions(MINIMAX-TRACE) algorithm for solving nonconvex-strongly concave minimax problems.","Both algorithms can find an $(\\epsilon, \\sqrt{\\epsilon})$-second order stationary point(SSP) within $\\mathcal{O}(\\epsilon^{-1.5})$ iterations, which matches the best well known iteration complexity."],"url":"http://arxiv.org/abs/2402.09807v1","category":"math.OC"}
{"created":"2024-02-15 08:54:18","title":"Co-Designing a wiki-based community knowledge management system for personal science","abstract":"Personal science is the practice of addressing personally relevant health questions through self-research. Implementing personal science can be challenging, due to the need to develop and adopt research protocols, tools, and methods. While online communities can provide valuable peer support, tools for systematically accessing community knowledge are lacking. The objective of this study is to apply a participatory design process involving a community of personal science practitioners to develop a peer-produced knowledge base that supports the needs of practitioners as consumers and contributors of knowledge. The process led to the development of the Personal Science Wiki, an open repository for documenting and accessing individual self-tracking projects while facilitating the establishment of consensus knowledge. After initial design iterations and a field testing phase, we performed a user study with 21 participants to test and improve the platform, and to explore suitable information architectures. The study deepened our understanding of barriers to scaling the personal science community, established an infrastructure for knowledge management actively used by the community, and provided lessons on challenges, information needs, representations, and architectures to support individuals with their personal health inquiries","sentences":["Personal science is the practice of addressing personally relevant health questions through self-research.","Implementing personal science can be challenging, due to the need to develop and adopt research protocols, tools, and methods.","While online communities can provide valuable peer support, tools for systematically accessing community knowledge are lacking.","The objective of this study is to apply a participatory design process involving a community of personal science practitioners to develop a peer-produced knowledge base that supports the needs of practitioners as consumers and contributors of knowledge.","The process led to the development of the Personal Science Wiki, an open repository for documenting and accessing individual self-tracking projects while facilitating the establishment of consensus knowledge.","After initial design iterations and a field testing phase, we performed a user study with 21 participants to test and improve the platform, and to explore suitable information architectures.","The study deepened our understanding of barriers to scaling the personal science community, established an infrastructure for knowledge management actively used by the community, and provided lessons on challenges, information needs, representations, and architectures to support individuals with their personal health inquiries"],"url":"http://arxiv.org/abs/2402.09799v1","category":"cs.HC"}
{"created":"2024-02-15 08:53:56","title":"Oka-1 manifolds: New examples and properties","abstract":"In this paper we investigate Oka-1 manifolds and Oka-1 maps, a class of complex manifolds and holomorphic maps recently introduced by Alarc\\'on and Forstneric. Oka-1 manifolds are characterised by the property that holomorphic maps from any open Riemann surface to the manifold satisfy the Runge approximation and Weierstrass interpolation conditions, while Oka-1 maps enjoy similar properties for liftings of maps from open Riemann surfaces in the absence of topological obstructions. We also formulate and study the algebraic version of the Oka-1 condition, called aOka-1. We show that it is a birational invariant for projective manifolds and holds for all uniformly rational projective manifolds. This gives a Runge approximation theorem for maps from compact Riemann surfaces to uniformly rational projective manifolds. Finally, we study a class of complex manifolds with an approximation property for holomorphic sprays of discs. This class lies between the smaller class of Oka manifolds and the bigger class of Oka-1 manifolds and has interesting functorial properties.","sentences":["In this paper we investigate Oka-1 manifolds and Oka-1 maps, a class of complex manifolds and holomorphic maps recently introduced by Alarc\\'on and Forstneric.","Oka-1 manifolds are characterised by the property that holomorphic maps from any open Riemann surface to the manifold satisfy the Runge approximation and Weierstrass interpolation conditions, while Oka-1 maps enjoy similar properties for liftings of maps from open Riemann surfaces in the absence of topological obstructions.","We also formulate and study the algebraic version of the Oka-1 condition, called aOka-1.","We show that it is a birational invariant for projective manifolds and holds for all uniformly rational projective manifolds.","This gives a Runge approximation theorem for maps from compact Riemann surfaces to uniformly rational projective manifolds.","Finally, we study a class of complex manifolds with an approximation property for holomorphic sprays of discs.","This class lies between the smaller class of Oka manifolds and the bigger class of Oka-1 manifolds and has interesting functorial properties."],"url":"http://arxiv.org/abs/2402.09798v1","category":"math.CV"}
{"created":"2024-02-15 08:51:49","title":"Closed-form Filtering for Non-linear Systems","abstract":"Sequential Bayesian Filtering aims to estimate the current state distribution of a Hidden Markov Model, given the past observations. The problem is well-known to be intractable for most application domains, except in notable cases such as the tabular setting or for linear dynamical systems with gaussian noise. In this work, we propose a new class of filters based on Gaussian PSD Models, which offer several advantages in terms of density approximation and computational efficiency. We show that filtering can be efficiently performed in closed form when transitions and observations are Gaussian PSD Models. When the transition and observations are approximated by Gaussian PSD Models, we show that our proposed estimator enjoys strong theoretical guarantees, with estimation error that depends on the quality of the approximation and is adaptive to the regularity of the transition probabilities. In particular, we identify regimes in which our proposed filter attains a TV $\\epsilon$-error with memory and computational complexity of $O(\\epsilon^{-1})$ and $O(\\epsilon^{-3/2})$ respectively, including the offline learning step, in contrast to the $O(\\epsilon^{-2})$ complexity of sampling methods such as particle filtering.","sentences":["Sequential Bayesian Filtering aims to estimate the current state distribution of a Hidden Markov Model, given the past observations.","The problem is well-known to be intractable for most application domains, except in notable cases such as the tabular setting or for linear dynamical systems with gaussian noise.","In this work, we propose a new class of filters based on Gaussian PSD Models, which offer several advantages in terms of density approximation and computational efficiency.","We show that filtering can be efficiently performed in closed form when transitions and observations are Gaussian PSD Models.","When the transition and observations are approximated by Gaussian PSD Models, we show that our proposed estimator enjoys strong theoretical guarantees, with estimation error that depends on the quality of the approximation and is adaptive to the regularity of the transition probabilities.","In particular, we identify regimes in which our proposed filter attains a TV $\\epsilon$-error with memory and computational complexity of $O(\\epsilon^{-1})$ and $O(\\epsilon^{-3/2})$ respectively, including the offline learning step, in contrast to the $O(\\epsilon^{-2})$ complexity of sampling methods such as particle filtering."],"url":"http://arxiv.org/abs/2402.09796v1","category":"stat.ML"}
{"created":"2024-02-15 08:50:17","title":"Propulsion of a three-sphere micro-robot in a porous medium","abstract":"Microorganisms and synthetic microswimmers often encounter complex environments consisting of networks of obstacles embedded into viscous fluids. Such settings include biological media, such as mucus with filamentous networks, as well as environmental scenarios, including wet soil and aquifers. A fundamental question in studying their locomotion is how the impermeability of these porous media impact their propulsion performance compared with the case that in a purely viscous fluid. Previous studies showed that the additional resistance due to the embedded obstacles leads to an enhanced propulsion of different types of swimmers, including undulatory swimmers, helical swimmers, and squirmers. In this work we employ a canonical three-sphere swimmer model to probe the impact of propulsion in porous media. The Brinkman equation is utilized to model a sparse network of stationary obstacles embedded into an incompressible Newtonian liquid. We present both a far-field theory and numerical simulations to characterize the propulsion performance of the swimmer in such porous media. In contrast to enhanced propulsion observed in other swimmer models, our results reveal that both the propulsion speed and efficiency of the three-sphere swimmer are largely reduced by the impermeability of the porous medium. We attribute the substantial reduction in propulsion performance to the screened hydrodynamic interactions among the spheres due to the more rapid spatial decays of flows in Brinkman media. These results highlight how enhanced or hindered propulsion in porous media is largely dependent on individual propulsion mechanisms. The specific example and physical insights provided here may guide the design of synthetic microswimmers for effective locomotion in porous media in their potential biological and environmental applications.","sentences":["Microorganisms and synthetic microswimmers often encounter complex environments consisting of networks of obstacles embedded into viscous fluids.","Such settings include biological media, such as mucus with filamentous networks, as well as environmental scenarios, including wet soil and aquifers.","A fundamental question in studying their locomotion is how the impermeability of these porous media impact their propulsion performance compared with the case that in a purely viscous fluid.","Previous studies showed that the additional resistance due to the embedded obstacles leads to an enhanced propulsion of different types of swimmers, including undulatory swimmers, helical swimmers, and squirmers.","In this work we employ a canonical three-sphere swimmer model to probe the impact of propulsion in porous media.","The Brinkman equation is utilized to model a sparse network of stationary obstacles embedded into an incompressible Newtonian liquid.","We present both a far-field theory and numerical simulations to characterize the propulsion performance of the swimmer in such porous media.","In contrast to enhanced propulsion observed in other swimmer models, our results reveal that both the propulsion speed and efficiency of the three-sphere swimmer are largely reduced by the impermeability of the porous medium.","We attribute the substantial reduction in propulsion performance to the screened hydrodynamic interactions among the spheres due to the more rapid spatial decays of flows in Brinkman media.","These results highlight how enhanced or hindered propulsion in porous media is largely dependent on individual propulsion mechanisms.","The specific example and physical insights provided here may guide the design of synthetic microswimmers for effective locomotion in porous media in their potential biological and environmental applications."],"url":"http://arxiv.org/abs/2402.09793v1","category":"physics.flu-dyn"}
{"created":"2024-02-15 08:33:45","title":"Electronic properties of MoSe$_2$ nanowrinkles","abstract":"Mechanical deformations, either spontaneously occurring during sample preparation or purposely induced in their nanoscale manipulation, drastically affect the electronic and optical properties of transition metal dichalcogenide monolayers. In this first-principles work based on density-functional theory, we shed light on the interplay among strain, curvature, and electronic structure of MoSe$_2$ nanowrinkles. We analyze their structural properties highlighting the effects of coexisting local domains of tensile and compressive strain in the same system. By contrasting the band structures of the nanowrinkles against counterparts obtained for flat monolayers subject to the same amount of strain, we clarify that the specific features of the former, such as the moderate variation of the band-gap size and its persisting direct nature, are ruled by curvature rather than strain. The analysis of the wave-function distribution indicates strain-dependent localization of the frontier states in the conduction region while in the valence the sensitivity to strain is much less pronounced. The discussion about transport properties, based on the inspection of the effective masses, reveals excellent perspectives for these systems as active components for (opto)electronic devices.","sentences":["Mechanical deformations, either spontaneously occurring during sample preparation or purposely induced in their nanoscale manipulation, drastically affect the electronic and optical properties of transition metal dichalcogenide monolayers.","In this first-principles work based on density-functional theory, we shed light on the interplay among strain, curvature, and electronic structure of MoSe$_2$ nanowrinkles.","We analyze their structural properties highlighting the effects of coexisting local domains of tensile and compressive strain in the same system.","By contrasting the band structures of the nanowrinkles against counterparts obtained for flat monolayers subject to the same amount of strain, we clarify that the specific features of the former, such as the moderate variation of the band-gap size and its persisting direct nature, are ruled by curvature rather than strain.","The analysis of the wave-function distribution indicates strain-dependent localization of the frontier states in the conduction region while in the valence the sensitivity to strain is much less pronounced.","The discussion about transport properties, based on the inspection of the effective masses, reveals excellent perspectives for these systems as active components for (opto)electronic devices."],"url":"http://arxiv.org/abs/2402.09785v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-15 08:09:17","title":"TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems","abstract":"The Continuous Learning (CL) paradigm consists of continuously evolving the parameters of the Deep Neural Network (DNN) model to progressively learn to perform new tasks without reducing the performance on previous tasks, i.e., avoiding the so-called catastrophic forgetting. However, the DNN parameter update in CL-based autonomous systems is extremely resource-hungry. The existing DNN accelerators cannot be directly employed in CL because they only support the execution of the forward propagation. Only a few prior architectures execute the backpropagation and weight update, but they lack the control and management for CL. Towards this, we design a hardware architecture, TinyCL, to perform CL on resource-constrained autonomous systems. It consists of a processing unit that executes both forward and backward propagation, and a control unit that manages memory-based CL workload. To minimize the memory accesses, the sliding window of the convolutional layer moves in a snake-like fashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at runtime to execute different operations. As per our knowledge, our proposed TinyCL represents the first hardware accelerator that executes CL on autonomous systems. We synthesize the complete TinyCL architecture in a 65 nm CMOS technology node with the conventional ASIC design flow. It executes 1 epoch of training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while 1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s, thus achieving a 58 x speedup, consuming 86 mW in a 4.74 mm2 die.","sentences":["The Continuous Learning (CL) paradigm consists of continuously evolving the parameters of the Deep Neural Network (DNN) model to progressively learn to perform new tasks without reducing the performance on previous tasks, i.e., avoiding the so-called catastrophic forgetting.","However, the DNN parameter update in CL-based autonomous systems is extremely resource-hungry.","The existing DNN accelerators cannot be directly employed in CL because they only support the execution of the forward propagation.","Only a few prior architectures execute the backpropagation and weight update, but they lack the control and management for CL.","Towards this, we design a hardware architecture, TinyCL, to perform CL on resource-constrained autonomous systems.","It consists of a processing unit that executes both forward and backward propagation, and a control unit that manages memory-based CL workload.","To minimize the memory accesses, the sliding window of the convolutional layer moves in a snake-like fashion.","Moreover, the Multiply-and-Accumulate units can be reconfigured at runtime to execute different operations.","As per our knowledge, our proposed TinyCL represents the first hardware accelerator that executes CL on autonomous systems.","We synthesize the complete TinyCL architecture in a 65 nm CMOS technology node with the conventional ASIC design flow.","It executes 1 epoch of training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while 1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s, thus achieving a 58 x speedup, consuming 86 mW in a 4.74 mm2 die."],"url":"http://arxiv.org/abs/2402.09780v1","category":"cs.LG"}
{"created":"2024-02-15 08:06:21","title":"Strategic Vote Timing in Online Elections With Public Tallies","abstract":"We study the effect of public tallies on online elections, in a setting where voting is costly and voters are allowed to strategically time their votes. The strategic importance of choosing \\emph{when} to vote arises when votes are public, such as in online event scheduling polls (e.\\,g., Doodle), or in blockchain governance mechanisms. In particular, there is a tension between voting early to influence future votes and waiting to observe interim results and avoid voting costs if the outcome has already been decided.   Our study draws on empirical findings showing that ``temporal'' bandwagon effects occur when interim results are revealed to the electorate: late voters are more likely to vote for leading candidates. To capture this phenomenon, we analyze a novel model where the electorate consists of informed voters who have a preferred candidate, and uninformed swing voters who can be swayed according to the interim outcome at the time of voting. In our main results, we prove the existence of equilibria where both early and late voting occur with a positive probability, and we characterize conditions that lead to the appearance of ``last minute'' voting behavior, where all informed voters vote late.","sentences":["We study the effect of public tallies on online elections, in a setting where voting is costly and voters are allowed to strategically time their votes.","The strategic importance of choosing \\emph{when} to vote arises when votes are public, such as in online event scheduling polls (e.\\,g., Doodle), or in blockchain governance mechanisms.","In particular, there is a tension between voting early to influence future votes and waiting to observe interim results and avoid voting costs if the outcome has already been decided.   ","Our study draws on empirical findings showing that ``temporal'' bandwagon effects occur when interim results are revealed to the electorate: late voters are more likely to vote for leading candidates.","To capture this phenomenon, we analyze a novel model where the electorate consists of informed voters who have a preferred candidate, and uninformed swing voters who can be swayed according to the interim outcome at the time of voting.","In our main results, we prove the existence of equilibria where both early and late voting occur with a positive probability, and we characterize conditions that lead to the appearance of ``last minute'' voting behavior, where all informed voters vote late."],"url":"http://arxiv.org/abs/2402.09776v1","category":"cs.GT"}
{"created":"2024-02-15 08:03:32","title":"A lattice formulation of Weyl fermions on a single curved surface","abstract":"In the standard lattice domain-wall fermion formulation, one needs two flat domain-walls where both of the left- and right-handed massless modes appear. In this work we investigate a single domain-wall system with a nontrivial curved background. Specifically we consider a massive fermion on a three-dimensional square lattice, whose domain-wall is a two-dimensional sphere. In the free theory, we find that a single Weyl fermion is localized at the wall and it feels gravity through the induced spin connection. With a topologically nontrivial $U(1)$ link gauge field, however, we find a zero mode with the opposite chirality localized at the center where the gauge field is singular. In the latter case, the low-energy effective theory is not chiral but vectorlike. We discuss how to circumvent this obstacle in formulating lattice chiral gauge theory in the single domain-wall fermion system.","sentences":["In the standard lattice domain-wall fermion formulation, one needs two flat domain-walls where both of the left- and right-handed massless modes appear.","In this work we investigate a single domain-wall system with a nontrivial curved background.","Specifically we consider a massive fermion on a three-dimensional square lattice, whose domain-wall is a two-dimensional sphere.","In the free theory, we find that a single Weyl fermion is localized at the wall and it feels gravity through the induced spin connection.","With a topologically nontrivial $U(1)$ link gauge field, however, we find a zero mode with the opposite chirality localized at the center where the gauge field is singular.","In the latter case, the low-energy effective theory is not chiral but vectorlike.","We discuss how to circumvent this obstacle in formulating lattice chiral gauge theory in the single domain-wall fermion system."],"url":"http://arxiv.org/abs/2402.09774v1","category":"hep-lat"}
{"created":"2024-02-15 07:51:45","title":"Doping induced multiferroicity and quantum anomalous Hall effect in $\u03b1$-In$_2$Se$_3$ thin films","abstract":"In flat-band materials, the strong Coulomb interaction between electrons can lead to exotic physical phenomena. Recently, $\\alpha$-In$_2$Se$_3$ thin films were found to possess ferroelectricity and flat bands. In this work, using first-principles calculations, we find that for the monolayer, there is a Weyl point at $\\Gamma$ in the flat band, where the inclusion of the spin-orbit coupling opens a gap. Shifting the Fermi level into the spin-orbit gap gives rise to nontrivial band topology, which is preserved for the bilayer regardless of the interlayer polarization couplings. We further calculate the Chern number and edge states for both the monolayer and bilayer, for which the results suggest that they become quantum anomalous Hall insulators under appropriate dopings. Moreover, we find that the doping-induced magnetism for In$_2$Se$_3$ bilayer is strongly dependent on the interlayer polarization coupling. Therefore, doping the flat bands in In$_2$Se$_3$ bilayer can also yield multiferroicity, where the magnetism is electrically tunable as the system transforms between different polarization states. Our study thus reveals that multiferroicity and nontrivial band topology can be unified into one material for designing multifunctional electronic devices.","sentences":["In flat-band materials, the strong Coulomb interaction between electrons can lead to exotic physical phenomena.","Recently, $\\alpha$-In$_2$Se$_3$ thin films were found to possess ferroelectricity and flat bands.","In this work, using first-principles calculations, we find that for the monolayer, there is a Weyl point at $\\Gamma$ in the flat band, where the inclusion of the spin-orbit coupling opens a gap.","Shifting the Fermi level into the spin-orbit gap gives rise to nontrivial band topology, which is preserved for the bilayer regardless of the interlayer polarization couplings.","We further calculate the Chern number and edge states for both the monolayer and bilayer, for which the results suggest that they become quantum anomalous Hall insulators under appropriate dopings.","Moreover, we find that the doping-induced magnetism for In$_2$Se$_3$ bilayer is strongly dependent on the interlayer polarization coupling.","Therefore, doping the flat bands in In$_2$Se$_3$ bilayer can also yield multiferroicity, where the magnetism is electrically tunable as the system transforms between different polarization states.","Our study thus reveals that multiferroicity and nontrivial band topology can be unified into one material for designing multifunctional electronic devices."],"url":"http://arxiv.org/abs/2402.09770v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-15 07:05:42","title":"Vector spectrometer with Hertz-level resolution and super-recognition capability","abstract":"High-resolution optical spectrometers are crucial in revealing intricate characteristics of signals, determining laser frequencies, measuring physical constants, identifying substances, and advancing biosensing applications. Conventional spectrometers, however, often grapple with inherent trade-offs among spectral resolution, wavelength range, and accuracy. Furthermore, even at high resolution, resolving overlapping spectral lines during spectroscopic analyses remains a huge challenge. Here, we propose a vector spectrometer with ultrahigh resolution, combining broadband optical frequency hopping, ultrafine microwave-photonic scanning, and vector detection. A programmable frequency-hopping laser was developed, facilitating a sub-Hz linewidth and Hz-level frequency stability, an improvement of four and six orders of magnitude, respectively, compared to those of state-of-the-art tunable lasers. We also designed an asymmetric optical transmitter and receiver to eliminate measurement errors arising from modulation nonlinearity and multi-channel crosstalk. The resultant vector spectrometer exhibits an unprecedented frequency resolution of 2 Hz, surpassing the state-of-the-art by four orders of magnitude, over a 33-nm range. Through high-resolution vector analysis, we observed that group delay information enhances the separation capability of overlapping spectral lines by over 47%, significantly streamlining the real-time identification of diverse substances. Our technique fills the gap in optical spectrometers with resolutions below 10 kHz and enables vector measurement to embrace revolution in functionality.","sentences":["High-resolution optical spectrometers are crucial in revealing intricate characteristics of signals, determining laser frequencies, measuring physical constants, identifying substances, and advancing biosensing applications.","Conventional spectrometers, however, often grapple with inherent trade-offs among spectral resolution, wavelength range, and accuracy.","Furthermore, even at high resolution, resolving overlapping spectral lines during spectroscopic analyses remains a huge challenge.","Here, we propose a vector spectrometer with ultrahigh resolution, combining broadband optical frequency hopping, ultrafine microwave-photonic scanning, and vector detection.","A programmable frequency-hopping laser was developed, facilitating a sub-Hz linewidth and Hz-level frequency stability, an improvement of four and six orders of magnitude, respectively, compared to those of state-of-the-art tunable lasers.","We also designed an asymmetric optical transmitter and receiver to eliminate measurement errors arising from modulation nonlinearity and multi-channel crosstalk.","The resultant vector spectrometer exhibits an unprecedented frequency resolution of 2 Hz, surpassing the state-of-the-art by four orders of magnitude, over a 33-nm range.","Through high-resolution vector analysis, we observed that group delay information enhances the separation capability of overlapping spectral lines by over 47%, significantly streamlining the real-time identification of diverse substances.","Our technique fills the gap in optical spectrometers with resolutions below 10 kHz and enables vector measurement to embrace revolution in functionality."],"url":"http://arxiv.org/abs/2402.09752v1","category":"physics.optics"}
{"created":"2024-02-15 07:04:34","title":"Long-time behavior towards viscous-dispersive shock for Navier-Stokes equations of Korteweg type","abstract":"We consider the so-called Naiver-Stokes-Korteweg(NSK) equations for the dynamics of compressible barotropic viscous fluids with internal capillarity. We handle the time-asymptotic stability in 1D of the viscous-dispersive shock wave that is a traveling wave solution to NSK as a viscous-dispersive counterpart of a Riemann shock. More precisely, we prove that when the prescribed far-field states of NSK are connected by a single Hugoniot curve, then solutions of NSK tend to the viscous-dispersive shock wave as time goes to infinity. To obtain the convergence, we extend the theory of $a$-contraction with shifts, used for the Navier-Stokes equations, to the NSK system. The main difficulty in analysis for NSK is due to the third-order derivative terms of the specific volume in the momentum equation. To resolve the problem, we introduce an auxiliary variable that is equivalent to the derivative of the specific volume.","sentences":["We consider the so-called Naiver-Stokes-Korteweg(NSK) equations for the dynamics of compressible barotropic viscous fluids with internal capillarity.","We handle the time-asymptotic stability in 1D of the viscous-dispersive shock wave that is a traveling wave solution to NSK as a viscous-dispersive counterpart of a Riemann shock.","More precisely, we prove that when the prescribed far-field states of NSK are connected by a single Hugoniot curve, then solutions of NSK tend to the viscous-dispersive shock wave as time goes to infinity.","To obtain the convergence, we extend the theory of $a$-contraction with shifts, used for the Navier-Stokes equations, to the NSK system.","The main difficulty in analysis for NSK is due to the third-order derivative terms of the specific volume in the momentum equation.","To resolve the problem, we introduce an auxiliary variable that is equivalent to the derivative of the specific volume."],"url":"http://arxiv.org/abs/2402.09751v1","category":"math.AP"}
{"created":"2024-02-15 06:58:37","title":"The $\\mathcal{PT}$-symmetric quantum Rabi model: Solutions and exceptional points","abstract":"The non-Hermitian one-photon and two-photon quantum Rabi models (QRMs) within imaginary couplings are respectively solved through the Bogoliubov operators approach. Transcendental functions responsible for exact solutions are derived, whose zeros produce the complete spectra. Exceptional points (EPs) can be identified with simultaneously vanishing transcendental function and its derivative with respect to energy. The EP is formed in the two nearest-neighboring excited energy levels, and shifts to the lower coupling strength at higher energy levels. The well-known generalized rotating-wave approximation method in the one-photon QRM is also extended to its non-Hermitian counterpart, and the obtained analytical EPs agree quite well with the exact ones, and the simulated dynamics can describes the basic features of this model. Very interestingly, under the resonant condition in the non-Hermitian two-photon QRM, the lowest two excited states which belong to the same parity and in the same photonic subspace within odd photon numbers can cross, and boh always have real energy levels. Such an EP at this crossing point is totally new, because the energies of the two levels are purely real, in sharp contrast to the conventional EP in the non-Hermitian systems. For both non-Hermitian QRMs, the fidelity susceptibility goes to negative infinity at the EPs, consistent with the recent observations in the non-Hermitian systems.","sentences":["The non-Hermitian one-photon and two-photon quantum Rabi models (QRMs) within imaginary couplings are respectively solved through the Bogoliubov operators approach.","Transcendental functions responsible for exact solutions are derived, whose zeros produce the complete spectra.","Exceptional points (EPs) can be identified with simultaneously vanishing transcendental function and its derivative with respect to energy.","The EP is formed in the two nearest-neighboring excited energy levels, and shifts to the lower coupling strength at higher energy levels.","The well-known generalized rotating-wave approximation method in the one-photon QRM is also extended to its non-Hermitian counterpart, and the obtained analytical EPs agree quite well with the exact ones, and the simulated dynamics can describes the basic features of this model.","Very interestingly, under the resonant condition in the non-Hermitian two-photon QRM, the lowest two excited states which belong to the same parity and in the same photonic subspace within odd photon numbers can cross, and boh always have real energy levels.","Such an EP at this crossing point is totally new, because the energies of the two levels are purely real, in sharp contrast to the conventional EP in the non-Hermitian systems.","For both non-Hermitian QRMs, the fidelity susceptibility goes to negative infinity at the EPs, consistent with the recent observations in the non-Hermitian systems."],"url":"http://arxiv.org/abs/2402.09749v1","category":"quant-ph"}
{"created":"2024-02-15 06:52:42","title":"Alpha-GPT 2.0: Human-in-the-Loop AI for Quantitative Investment","abstract":"Recently, we introduced a new paradigm for alpha mining in the realm of quantitative investment, developing a new interactive alpha mining system framework, Alpha-GPT. This system is centered on iterative Human-AI interaction based on large language models, introducing a Human-in-the-Loop approach to alpha discovery. In this paper, we present the next-generation Alpha-GPT 2.0 \\footnote{Draft. Work in progress}, a quantitative investment framework that further encompasses crucial modeling and analysis phases in quantitative investment. This framework emphasizes the iterative, interactive research between humans and AI, embodying a Human-in-the-Loop strategy throughout the entire quantitative investment pipeline. By assimilating the insights of human researchers into the systematic alpha research process, we effectively leverage the Human-in-the-Loop approach, enhancing the efficiency and precision of quantitative investment research.","sentences":["Recently, we introduced a new paradigm for alpha mining in the realm of quantitative investment, developing a new interactive alpha mining system framework, Alpha-GPT.","This system is centered on iterative Human-AI interaction based on large language models, introducing a Human-in-the-Loop approach to alpha discovery.","In this paper, we present the next-generation Alpha-GPT 2.0 \\footnote{Draft.","Work in progress}, a quantitative investment framework that further encompasses crucial modeling and analysis phases in quantitative investment.","This framework emphasizes the iterative, interactive research between humans and AI, embodying a Human-in-the-Loop strategy throughout the entire quantitative investment pipeline.","By assimilating the insights of human researchers into the systematic alpha research process, we effectively leverage the Human-in-the-Loop approach, enhancing the efficiency and precision of quantitative investment research."],"url":"http://arxiv.org/abs/2402.09746v1","category":"q-fin.CP"}
{"created":"2024-02-15 06:47:47","title":"Quickest Detection of False Data Injection Attack in Distributed Process Tracking","abstract":"This paper addresses the problem of detecting false data injection (FDI) attacks in a distributed network without a fusion center, represented by a connected graph among multiple agent nodes. Each agent node is equipped with a sensor, and uses a Kalman consensus information filter (KCIF) to track a discrete time global process with linear dynamics and additive Gaussian noise. The state estimate of the global process at any sensor is computed from the local observation history and the information received by that agent node from its neighbors. At an unknown time, an attacker starts altering the local observation of one agent node. In the Bayesian setting where there is a known prior distribution of the attack beginning instant, we formulate a Bayesian quickest change detection (QCD) problem for FDI detection in order to minimize the mean detection delay subject to a false alarm probability constraint. While it is well-known that the optimal Bayesian QCD rule involves checking the Shriyaev's statistic against a threshold, we demonstrate how to compute the Shriyaev's statistic at each node in a recursive fashion given our non-i.i.d. observations. Next, we consider non-Bayesian QCD where the attack begins at an arbitrary and unknown time, and the detector seeks to minimize the worst case detection delay subject to a constraint on the mean time to false alarm and probability of misidentification. We use the multiple hypothesis sequential probability ratio test for attack detection and identification at each sensor. For unknown attack strategy, we use the window-limited generalized likelihood ratio (WL-GLR) algorithm to solve the QCD problem. Numerical results demonstrate the performances and trade-offs of the proposed algorithms.","sentences":["This paper addresses the problem of detecting false data injection (FDI) attacks in a distributed network without a fusion center, represented by a connected graph among multiple agent nodes.","Each agent node is equipped with a sensor, and uses a Kalman consensus information filter (KCIF) to track a discrete time global process with linear dynamics and additive Gaussian noise.","The state estimate of the global process at any sensor is computed from the local observation history and the information received by that agent node from its neighbors.","At an unknown time, an attacker starts altering the local observation of one agent node.","In the Bayesian setting where there is a known prior distribution of the attack beginning instant, we formulate a Bayesian quickest change detection (QCD) problem for FDI detection in order to minimize the mean detection delay subject to a false alarm probability constraint.","While it is well-known that the optimal Bayesian QCD rule involves checking the Shriyaev's statistic against a threshold, we demonstrate how to compute the Shriyaev's statistic at each node in a recursive fashion given our non-i.i.d. observations.","Next, we consider non-Bayesian QCD where the attack begins at an arbitrary and unknown time, and the detector seeks to minimize the worst case detection delay subject to a constraint on the mean time to false alarm and probability of misidentification.","We use the multiple hypothesis sequential probability ratio test for attack detection and identification at each sensor.","For unknown attack strategy, we use the window-limited generalized likelihood ratio (WL-GLR) algorithm to solve the QCD problem.","Numerical results demonstrate the performances and trade-offs of the proposed algorithms."],"url":"http://arxiv.org/abs/2402.09743v1","category":"eess.SY"}
{"created":"2024-02-15 06:40:42","title":"Dimension-Dependent Critical Scaling Analysis and Emergent Competing Interaction Scales in a 2D Van der Waals magnet Cr$_{2}$Ge$_{2}$Te$_{6}$","abstract":"We investigate thickness-dependent transformation from a paramagnetic to ferromagnetic phase in Cr$_{2}$Ge$_{2}$Te$_{6}$ (CGT) in bulk and few-layer flake forms. 2D Ising-like critical transition in bulk CGT occurs at $T_{c}$ = 67 K with out-of-plane magnetic anisotropy. Few-layer CGT on hBN/SiO$_{2}$/Si substrate displays the same $T_{c}$ but also exhibits a new critical transition at $T^{\\prime}_c$ = 14.2 K. Here, critical scaling analysis reveals the critical exponents differ significantly from those in bulk and do not align with the known universality classes. Our Density Functional Theory (DFT) and classical calculations indicate competition between magnetocrystalline and dipolar anisotropy emerges with reduced dimensions. The observed behavior is due to minor structural distortions in low dimensional CGT, which modify the balance between spin-orbit coupling, exchange interactions and dipolar anisotropy. This triggers a critical crossover at $T^{\\prime}_c$. Our study shows the emergence of a complex interplay of short- and long-range interactions below $T^{\\prime}_c$ as CGT approaches the 2D limit.","sentences":["We investigate thickness-dependent transformation from a paramagnetic to ferromagnetic phase in Cr$_{2}$Ge$_{2}$Te$_{6}$ (CGT) in bulk and few-layer flake forms.","2D Ising-like critical transition in bulk CGT occurs at $T_{c}$ = 67 K with out-of-plane magnetic anisotropy.","Few-layer CGT on hBN/SiO$_{2}$/Si substrate displays the same $T_{c}$ but also exhibits a new critical transition at $T^{\\prime}_c$ = 14.2 K. Here, critical scaling analysis reveals the critical exponents differ significantly from those in bulk and do not align with the known universality classes.","Our Density Functional Theory (DFT) and classical calculations indicate competition between magnetocrystalline and dipolar anisotropy emerges with reduced dimensions.","The observed behavior is due to minor structural distortions in low dimensional CGT, which modify the balance between spin-orbit coupling, exchange interactions and dipolar anisotropy.","This triggers a critical crossover at $T^{\\prime}_c$. Our study shows the emergence of a complex interplay of short- and long-range interactions below $T^{\\prime}_c$ as CGT approaches the 2D limit."],"url":"http://arxiv.org/abs/2402.09741v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-15 06:34:15","title":"Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection","abstract":"Multimodal hateful content detection is a challenging task that requires complex reasoning across visual and textual modalities. Therefore, creating a meaningful multimodal representation that effectively captures the interplay between visual and textual features through intermediate fusion is critical. Conventional fusion techniques are unable to attend to the modality-specific features effectively. Moreover, most studies exclusively concentrated on English and overlooked other low-resource languages. This paper proposes a context-aware attention framework for multimodal hateful content detection and assesses it for both English and non-English languages. The proposed approach incorporates an attention layer to meaningfully align the visual and textual features. This alignment enables selective focus on modality-specific features before fusing them. We evaluate the proposed approach on two benchmark hateful meme datasets, viz. MUTE (Bengali code-mixed) and MultiOFF (English). Evaluation results demonstrate our proposed approach's effectiveness with F1-scores of $69.7$% and $70.3$% for the MUTE and MultiOFF datasets. The scores show approximately $2.5$% and $3.2$% performance improvement over the state-of-the-art systems on these datasets. Our implementation is available at https://github.com/eftekhar-hossain/Bengali-Hateful-Memes.","sentences":["Multimodal hateful content detection is a challenging task that requires complex reasoning across visual and textual modalities.","Therefore, creating a meaningful multimodal representation that effectively captures the interplay between visual and textual features through intermediate fusion is critical.","Conventional fusion techniques are unable to attend to the modality-specific features effectively.","Moreover, most studies exclusively concentrated on English and overlooked other low-resource languages.","This paper proposes a context-aware attention framework for multimodal hateful content detection and assesses it for both English and non-English languages.","The proposed approach incorporates an attention layer to meaningfully align the visual and textual features.","This alignment enables selective focus on modality-specific features before fusing them.","We evaluate the proposed approach on two benchmark hateful meme datasets, viz.","MUTE (Bengali code-mixed) and MultiOFF (English).","Evaluation results demonstrate our proposed approach's effectiveness with F1-scores of $69.7$% and $70.3$% for the MUTE and MultiOFF datasets.","The scores show approximately $2.5$% and $3.2$% performance improvement over the state-of-the-art systems on these datasets.","Our implementation is available at https://github.com/eftekhar-hossain/Bengali-Hateful-Memes."],"url":"http://arxiv.org/abs/2402.09738v1","category":"cs.CL"}
{"created":"2024-02-15 06:31:09","title":"Dynamics and precise control of fluid V-states using an electron plasma","abstract":"An electron plasma can be confined for a theoretically infinite time in a Penning-Malmberg trap, a linear, azimuthally-symmetric magneto-electrostatic device where upon suitable conditions (high magnetization) the transverse dynamics of the plasma column is isomorphic to the one displayed by a two-dimensional ideal fluid. Fluid dynamics can thus be reproduced in these systems with a very high degree of control on the system's parameters and active excitation of fluid perturbations is made possible by the use of static or time-dependent electric fields (i.e., fluid strains) imparted by electric potentials applied to the azimuthal patches of a sectored electrode of the trap. An example is represented by azimuthal velocity shear phenomena and the insurgence of Kelvin-Helmholtz (KH) instabilities in fluid vortices. We present a study where we exploit multipolar rotating electric fields to generate V-states and observe their dynamics and stability properties. A V-state is the generalization of the 2D Kirchhoff (elliptical) fluid vortex to a generic KH mode, in the nonlinear regime. In particular, we discuss first how we can exploit a combination of techniques (plasma evaporation and tilt-induced transport) to tune the radial vorticity profile, which may have an effect on the dynamics of the growth and decay of the selected KH wave. We also investigate autoresonant (swept-frequency, self-locking) excitation - useful, e.g., for the precise control of the KH mode growth - and discuss the features of autoresonance applied to higher-order KH waves.","sentences":["An electron plasma can be confined for a theoretically infinite time in a Penning-Malmberg trap, a linear, azimuthally-symmetric magneto-electrostatic device where upon suitable conditions (high magnetization) the transverse dynamics of the plasma column is isomorphic to the one displayed by a two-dimensional ideal fluid.","Fluid dynamics can thus be reproduced in these systems with a very high degree of control on the system's parameters and active excitation of fluid perturbations is made possible by the use of static or time-dependent electric fields (i.e., fluid strains) imparted by electric potentials applied to the azimuthal patches of a sectored electrode of the trap.","An example is represented by azimuthal velocity shear phenomena and the insurgence of Kelvin-Helmholtz (KH) instabilities in fluid vortices.","We present a study where we exploit multipolar rotating electric fields to generate V-states and observe their dynamics and stability properties.","A V-state is the generalization of the 2D Kirchhoff (elliptical) fluid vortex to a generic KH mode, in the nonlinear regime.","In particular, we discuss first how we can exploit a combination of techniques (plasma evaporation and tilt-induced transport) to tune the radial vorticity profile, which may have an effect on the dynamics of the growth and decay of the selected KH wave.","We also investigate autoresonant (swept-frequency, self-locking) excitation - useful, e.g., for the precise control of the KH mode growth - and discuss the features of autoresonance applied to higher-order KH waves."],"url":"http://arxiv.org/abs/2402.09737v1","category":"physics.plasm-ph"}
{"created":"2024-02-15 06:26:10","title":"Federated Analytics-Empowered Frequent Pattern Mining for Decentralized Web 3.0 Applications","abstract":"The emerging Web 3.0 paradigm aims to decentralize existing web services, enabling desirable properties such as transparency, incentives, and privacy preservation. However, current Web 3.0 applications supported by blockchain infrastructure still cannot support complex data analytics tasks in a scalable and privacy-preserving way. This paper introduces the emerging federated analytics (FA) paradigm into the realm of Web 3.0 services, enabling data to stay local while still contributing to complex web analytics tasks in a privacy-preserving way. We propose FedWeb, a tailored FA design for important frequent pattern mining tasks in Web 3.0. FedWeb remarkably reduces the number of required participating data owners to support privacy-preserving Web 3.0 data analytics based on a novel distributed differential privacy technique. The correctness of mining results is guaranteed by a theoretically rigid candidate filtering scheme based on Hoeffding's inequality and Chebychev's inequality. Two response budget saving solutions are proposed to further reduce participating data owners. Experiments on three representative Web 3.0 scenarios show that FedWeb can improve data utility by ~25.3% and reduce the participating data owners by ~98.4%.","sentences":["The emerging Web 3.0 paradigm aims to decentralize existing web services, enabling desirable properties such as transparency, incentives, and privacy preservation.","However, current Web 3.0 applications supported by blockchain infrastructure still cannot support complex data analytics tasks in a scalable and privacy-preserving way.","This paper introduces the emerging federated analytics (FA) paradigm into the realm of Web 3.0 services, enabling data to stay local while still contributing to complex web analytics tasks in a privacy-preserving way.","We propose FedWeb, a tailored FA design for important frequent pattern mining tasks in Web 3.0.","FedWeb remarkably reduces the number of required participating data owners to support privacy-preserving Web 3.0 data analytics based on a novel distributed differential privacy technique.","The correctness of mining results is guaranteed by a theoretically rigid candidate filtering scheme based on Hoeffding's inequality and Chebychev's inequality.","Two response budget saving solutions are proposed to further reduce participating data owners.","Experiments on three representative Web 3.0 scenarios show that FedWeb can improve data utility by ~25.3% and reduce the participating data owners by ~98.4%."],"url":"http://arxiv.org/abs/2402.09736v1","category":"cs.DC"}
{"created":"2024-02-15 06:22:50","title":"DFORM: Diffeomorphic vector field alignment for assessing dynamics across learned models","abstract":"Dynamical system models such as Recurrent Neural Networks (RNNs) have become increasingly popular as hypothesis-generating tools in scientific research. Evaluating the dynamics in such networks is key to understanding their learned generative mechanisms. However, comparison of learned dynamics across models is challenging due to their inherent nonlinearity and because a priori there is no enforced equivalence of their coordinate systems. Here, we propose the DFORM (Diffeomorphic vector field alignment for comparing dynamics across learned models) framework. DFORM learns a nonlinear coordinate transformation which provides a continuous, maximally one-to-one mapping between the trajectories of learned models, thus approximating a diffeomorphism between them. The mismatch between DFORM-transformed vector fields defines the orbital similarity between two models, thus providing a generalization of the concepts of smooth orbital and topological equivalence. As an example, we apply DFORM to models trained on a canonical neuroscience task, showing that learned dynamics may be functionally similar, despite overt differences in attractor landscapes.","sentences":["Dynamical system models such as Recurrent Neural Networks (RNNs) have become increasingly popular as hypothesis-generating tools in scientific research.","Evaluating the dynamics in such networks is key to understanding their learned generative mechanisms.","However, comparison of learned dynamics across models is challenging due to their inherent nonlinearity and because a priori there is no enforced equivalence of their coordinate systems.","Here, we propose the DFORM (Diffeomorphic vector field alignment for comparing dynamics across learned models) framework.","DFORM learns a nonlinear coordinate transformation which provides a continuous, maximally one-to-one mapping between the trajectories of learned models, thus approximating a diffeomorphism between them.","The mismatch between DFORM-transformed vector fields defines the orbital similarity between two models, thus providing a generalization of the concepts of smooth orbital and topological equivalence.","As an example, we apply DFORM to models trained on a canonical neuroscience task, showing that learned dynamics may be functionally similar, despite overt differences in attractor landscapes."],"url":"http://arxiv.org/abs/2402.09735v1","category":"cs.LG"}
{"created":"2024-02-15 05:59:21","title":"DOF: Accelerating High-order Differential Operators with Forward Propagation","abstract":"Solving partial differential equations (PDEs) efficiently is essential for analyzing complex physical systems. Recent advancements in leveraging deep learning for solving PDE have shown significant promise. However, machine learning methods, such as Physics-Informed Neural Networks (PINN), face challenges in handling high-order derivatives of neural network-parameterized functions. Inspired by Forward Laplacian, a recent method of accelerating Laplacian computation, we propose an efficient computational framework, Differential Operator with Forward-propagation (DOF), for calculating general second-order differential operators without losing any precision. We provide rigorous proof of the advantages of our method over existing methods, demonstrating two times improvement in efficiency and reduced memory consumption on any architectures. Empirical results illustrate that our method surpasses traditional automatic differentiation (AutoDiff) techniques, achieving 2x improvement on the MLP structure and nearly 20x improvement on the MLP with Jacobian sparsity.","sentences":["Solving partial differential equations (PDEs) efficiently is essential for analyzing complex physical systems.","Recent advancements in leveraging deep learning for solving PDE have shown significant promise.","However, machine learning methods, such as Physics-Informed Neural Networks (PINN), face challenges in handling high-order derivatives of neural network-parameterized functions.","Inspired by Forward Laplacian, a recent method of accelerating Laplacian computation, we propose an efficient computational framework, Differential Operator with Forward-propagation (DOF), for calculating general second-order differential operators without losing any precision.","We provide rigorous proof of the advantages of our method over existing methods, demonstrating two times improvement in efficiency and reduced memory consumption on any architectures.","Empirical results illustrate that our method surpasses traditional automatic differentiation (AutoDiff) techniques, achieving 2x improvement on the MLP structure and nearly 20x improvement on the MLP with Jacobian sparsity."],"url":"http://arxiv.org/abs/2402.09730v1","category":"cs.LG"}
{"created":"2024-02-15 05:26:19","title":"SpaceMeta: Global-Scale Massive Multi-User Virtual Interaction over LEO Satellite Constellations","abstract":"Low latency and high synchronization among users are critical for emerging multi-user virtual interaction applications. However, the existing ground-based cloud solutions are naturally limited by the complex ground topology and fiber speeds, making it difficult to pace with the requirement of multi-user virtual interaction. The growth of low earth orbit (LEO) satellite constellations becomes a promising alternative to ground solutions. To fully exploit the potential of the LEO satellite, in this paper, we study the satellite server selection problem for global-scale multi-user interaction applications over LEO constellations. We propose an effective server selection framework, called SpaceMeta, that jointly selects the ingress satellite servers and relay servers on the communication path to minimize latency and latency discrepancy among users. Extensive experiments using real-world Starlink topology demonstrate that SpaceMeta reduces the latency by 6.72% and the interquartile range (IQR) of user latency by 39.50% compared with state-of-the-art methods.","sentences":["Low latency and high synchronization among users are critical for emerging multi-user virtual interaction applications.","However, the existing ground-based cloud solutions are naturally limited by the complex ground topology and fiber speeds, making it difficult to pace with the requirement of multi-user virtual interaction.","The growth of low earth orbit (LEO) satellite constellations becomes a promising alternative to ground solutions.","To fully exploit the potential of the LEO satellite, in this paper, we study the satellite server selection problem for global-scale multi-user interaction applications over LEO constellations.","We propose an effective server selection framework, called SpaceMeta, that jointly selects the ingress satellite servers and relay servers on the communication path to minimize latency and latency discrepancy among users.","Extensive experiments using real-world Starlink topology demonstrate that SpaceMeta reduces the latency by 6.72% and the interquartile range (IQR) of user latency by 39.50% compared with state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.09720v1","category":"cs.MM"}
{"created":"2024-02-15 05:15:22","title":"An Accelerated Distributed Stochastic Gradient Method with Momentum","abstract":"In this paper, we introduce an accelerated distributed stochastic gradient method with momentum for solving the distributed optimization problem, where a group of $n$ agents collaboratively minimize the average of the local objective functions over a connected network. The method, termed ``Distributed Stochastic Momentum Tracking (DSMT)'', is a single-loop algorithm that utilizes the momentum tracking technique as well as the Loopless Chebyshev Acceleration (LCA) method. We show that DSMT can asymptotically achieve comparable convergence rates as centralized stochastic gradient descent (SGD) method under a general variance condition regarding the stochastic gradients. Moreover, the number of iterations (transient times) required for DSMT to achieve such rates behaves as $\\mathcal{O}(n^{5/3}/(1-\\lambda))$ for minimizing general smooth objective functions, and $\\mathcal{O}(\\sqrt{n/(1-\\lambda)})$ under the Polyak-{\\L}ojasiewicz (PL) condition. Here, the term $1-\\lambda$ denotes the spectral gap of the mixing matrix related to the underlying network topology. Notably, the obtained results do not rely on multiple inter-node communications or stochastic gradient accumulation per iteration, and the transient times are the shortest under the setting to the best of our knowledge.","sentences":["In this paper, we introduce an accelerated distributed stochastic gradient method with momentum for solving the distributed optimization problem, where a group of $n$ agents collaboratively minimize the average of the local objective functions over a connected network.","The method, termed ``Distributed Stochastic Momentum Tracking (DSMT)'', is a single-loop algorithm that utilizes the momentum tracking technique as well as the Loopless Chebyshev Acceleration (LCA) method.","We show that DSMT can asymptotically achieve comparable convergence rates as centralized stochastic gradient descent (SGD) method under a general variance condition regarding the stochastic gradients.","Moreover, the number of iterations (transient times) required for DSMT to achieve such rates behaves as $\\mathcal{O}(n^{5/3}/(1-\\lambda))$ for minimizing general smooth objective functions, and $\\mathcal{O}(\\sqrt{n/(1-\\lambda)})$ under the Polyak-{\\L}ojasiewicz (PL) condition.","Here, the term $1-\\lambda$ denotes the spectral gap of the mixing matrix related to the underlying network topology.","Notably, the obtained results do not rely on multiple inter-node communications or stochastic gradient accumulation per iteration, and the transient times are the shortest under the setting to the best of our knowledge."],"url":"http://arxiv.org/abs/2402.09714v1","category":"math.OC"}
{"created":"2024-02-15 05:07:39","title":"Node Duplication Improves Cold-start Link Prediction","abstract":"Graph Neural Networks (GNNs) are prominent in graph machine learning and have shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless, recent studies show that GNNs struggle to produce good results on low-degree nodes despite their overall strong performance. In practical applications of LP, like recommendation systems, improving performance on low-degree nodes is critical, as it amounts to tackling the cold-start problem of improving the experiences of users with few observed interactions. In this paper, we investigate improving GNNs' LP performance on low-degree nodes while preserving their performance on high-degree nodes and propose a simple yet surprisingly effective augmentation technique called NodeDup. Specifically, NodeDup duplicates low-degree nodes and creates links between nodes and their own duplicates before following the standard supervised LP training scheme. By leveraging a ''multi-view'' perspective for low-degree nodes, NodeDup shows significant LP performance improvements on low-degree nodes without compromising any performance on high-degree nodes. Additionally, as a plug-and-play augmentation module, NodeDup can be easily applied to existing GNNs with very light computational cost. Extensive experiments show that NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated, low-degree, and warm nodes, respectively, on average across all datasets compared to GNNs and state-of-the-art cold-start methods.","sentences":["Graph Neural Networks (GNNs) are prominent in graph machine learning and have shown state-of-the-art performance in Link Prediction (LP) tasks.","Nonetheless, recent studies show that GNNs struggle to produce good results on low-degree nodes despite their overall strong performance.","In practical applications of LP, like recommendation systems, improving performance on low-degree nodes is critical, as it amounts to tackling the cold-start problem of improving the experiences of users with few observed interactions.","In this paper, we investigate improving GNNs' LP performance on low-degree nodes while preserving their performance on high-degree nodes and propose a simple yet surprisingly effective augmentation technique called NodeDup.","Specifically, NodeDup duplicates low-degree nodes and creates links between nodes and their own duplicates before following the standard supervised LP training scheme.","By leveraging a ''multi-view'' perspective for low-degree nodes, NodeDup shows significant LP performance improvements on low-degree nodes without compromising any performance on high-degree nodes.","Additionally, as a plug-and-play augmentation module, NodeDup can be easily applied to existing GNNs with very light computational cost.","Extensive experiments show that NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated, low-degree, and warm nodes, respectively, on average across all datasets compared to GNNs and state-of-the-art cold-start methods."],"url":"http://arxiv.org/abs/2402.09711v1","category":"cs.LG"}
{"created":"2024-02-15 04:41:31","title":"Linear Depth QFT over IBM Heavy-hex Architecture","abstract":"Compiling a given quantum algorithm into a target hardware architecture is a challenging optimization problem. The compiler must take into consideration the coupling graph of physical qubits and the gate operation dependencies. The existing noise in hardware architectures requires the compilation to use as few running cycles as possible. Existing approaches include using SAT solver or heuristics to complete the mapping but these may cause the issue of either long compilation time (e.g., timeout after hours) or suboptimal compilation results in terms of running cycles (e.g., exponentially increasing number of total cycles).   In this paper, we propose an efficient mapping approach for Quantum Fourier Transformation (QFT) circuits over the existing IBM heavy-hex architecture. Such proposal first of all turns the architecture into a structure consisting of a straight line with dangling qubits, and then do the mapping over this generated structure recursively. The calculation shows that there is a linear depth upper bound for the time complexity of these structures and for a special case where there is 1 dangling qubit in every 5 qubits, the time complexity is 5N+O(1). All these results are better than state of the art methods.","sentences":["Compiling a given quantum algorithm into a target hardware architecture is a challenging optimization problem.","The compiler must take into consideration the coupling graph of physical qubits and the gate operation dependencies.","The existing noise in hardware architectures requires the compilation to use as few running cycles as possible.","Existing approaches include using SAT solver or heuristics to complete the mapping but these may cause the issue of either long compilation time (e.g., timeout after hours) or suboptimal compilation results in terms of running cycles (e.g., exponentially increasing number of total cycles).   ","In this paper, we propose an efficient mapping approach for Quantum Fourier Transformation (QFT) circuits over the existing IBM heavy-hex architecture.","Such proposal first of all turns the architecture into a structure consisting of a straight line with dangling qubits, and then do the mapping over this generated structure recursively.","The calculation shows that there is a linear depth upper bound for the time complexity of these structures and for a special case where there is 1 dangling qubit in every 5 qubits, the time complexity is 5N+O(1).","All these results are better than state of the art methods."],"url":"http://arxiv.org/abs/2402.09705v1","category":"quant-ph"}
{"created":"2024-02-15 04:38:26","title":"Canalization reduces the nonlinearity of regulation in biological networks","abstract":"Biological networks such as gene regulatory networks possess desirable properties. They are more robust and controllable than random networks. This motivates the search for structural and dynamical features that evolution has incorporated in biological networks. A recent meta-analysis of published, expert-curated Boolean biological network models has revealed several such features, often referred to as design principles. Among others, the biological networks are enriched for certain recurring network motifs, the dynamic update rules are more redundant, more biased and more canalizing than expected, and the dynamics of biological networks are better approximable by linear and lower-order approximations than those of comparable random networks. Since most of these features are interrelated, it is paramount to disentangle cause and effect, that is, to understand which features evolution actively selects for, and thus truly constitute evolutionary design principles. Here, we show that approximability is strongly dependent on the dynamical robustness of a network, and that increased canalization in biological networks can almost completely explain their recently postulated high approximability.","sentences":["Biological networks such as gene regulatory networks possess desirable properties.","They are more robust and controllable than random networks.","This motivates the search for structural and dynamical features that evolution has incorporated in biological networks.","A recent meta-analysis of published, expert-curated Boolean biological network models has revealed several such features, often referred to as design principles.","Among others, the biological networks are enriched for certain recurring network motifs, the dynamic update rules are more redundant, more biased and more canalizing than expected, and the dynamics of biological networks are better approximable by linear and lower-order approximations than those of comparable random networks.","Since most of these features are interrelated, it is paramount to disentangle cause and effect, that is, to understand which features evolution actively selects for, and thus truly constitute evolutionary design principles.","Here, we show that approximability is strongly dependent on the dynamical robustness of a network, and that increased canalization in biological networks can almost completely explain their recently postulated high approximability."],"url":"http://arxiv.org/abs/2402.09703v1","category":"q-bio.MN"}
{"created":"2024-02-15 04:33:30","title":"HOACS: Homomorphic Obfuscation Assisted Concealing of Secrets to Thwart Trojan Attacks in COTS Processor","abstract":"Commercial-off-the-shelf (COTS) components are often preferred over custom Integrated Circuits (ICs) to achieve reduced system development time and cost, easy adoption of new technologies, and replaceability. Unfortunately, the integration of COTS components introduces serious security concerns. None of the entities in the COTS IC supply chain are trusted from a consumer's perspective, leading to a ''zero trust'' threat model. Any of these entities could introduce hidden malicious circuits or hardware Trojans within the component, allowing an attacker in the field to extract secret information (e.g., cryptographic keys) or cause a functional failure. Existing solutions to counter hardware Trojans are inapplicable in such a zero-trust scenario as they assume either the design house or the foundry to be trusted and consider the design to be available for either analysis or modification. In this work, we have proposed a software-oriented countermeasure to ensure the confidentiality of secret assets against hardware Trojans that can be seamlessly integrated in existing COTS microprocessors. The proposed solution does not require any supply chain entity to be trusted and does not require analysis or modification of the IC design. To protect secret assets in an untrusted microprocessor, the proposed method leverages the concept of residue number coding (RNC) to transform the software functions operating on the asset to be fully homomorphic. We have implemented the proposed solution to protect the secret key within the Advanced Encryption Standard (AES) program and presented a detailed security analysis. We also have developed a plugin for the LLVM compiler toolchain that automatically integrates the solution in AES. Finally, we compare the execution time overhead of the operations in the RNC-based technique with comparable homomorphic solutions and demonstrate significant improvement.","sentences":["Commercial-off-the-shelf (COTS) components are often preferred over custom Integrated Circuits (ICs) to achieve reduced system development time and cost, easy adoption of new technologies, and replaceability.","Unfortunately, the integration of COTS components introduces serious security concerns.","None of the entities in the COTS IC supply chain are trusted from a consumer's perspective, leading to a ''zero trust'' threat model.","Any of these entities could introduce hidden malicious circuits or hardware Trojans within the component, allowing an attacker in the field to extract secret information (e.g., cryptographic keys) or cause a functional failure.","Existing solutions to counter hardware Trojans are inapplicable in such a zero-trust scenario as they assume either the design house or the foundry to be trusted and consider the design to be available for either analysis or modification.","In this work, we have proposed a software-oriented countermeasure to ensure the confidentiality of secret assets against hardware Trojans that can be seamlessly integrated in existing COTS microprocessors.","The proposed solution does not require any supply chain entity to be trusted and does not require analysis or modification of the IC design.","To protect secret assets in an untrusted microprocessor, the proposed method leverages the concept of residue number coding (RNC) to transform the software functions operating on the asset to be fully homomorphic.","We have implemented the proposed solution to protect the secret key within the Advanced Encryption Standard (AES) program and presented a detailed security analysis.","We also have developed a plugin for the LLVM compiler toolchain that automatically integrates the solution in AES.","Finally, we compare the execution time overhead of the operations in the RNC-based technique with comparable homomorphic solutions and demonstrate significant improvement."],"url":"http://arxiv.org/abs/2402.09701v1","category":"cs.CR"}
{"created":"2024-02-15 04:29:29","title":"Observation of topology transition in Floquet non-Hermitian skin effects in silicon photonics","abstract":"Non-Hermitian physics has greatly enriched our understanding of nonequilibrium phenomena and uncovered novel effects such as the non-Hermitian skin effect (NHSE) that has profoundly revolutionized the field. NHSE is typically predicted in systems with nonreciprocal couplings which, however, are difficult to realize in experiments. Without nonreciprocal couplings, the NHSE can also emerge in systems with coexisting gauge fields and loss or gain (e.g., in Floquet non-Hermitian systems). However, such Floquet NHSE remains largely unexplored in experiments. Here, we realize the Floquet NHSEs in periodically modulated optical waveguides integrated on a silicon photonics platform. By engineering the artificial gauge fields induced by the periodical modulation, we observe various Floquet NHSEs and unveil their rich topological transitions. Remarkably, we discover the transitions between the normal unipolar NHSEs and an unconventional bipolar NHSE which is accompanied by the directional reversal of the NHSEs. The underlying physics is revealed by the band winding in complex quasienergy space which undergoes a topology change from isolated loops with the same winding to linked loops with opposite windings. Our work unfolds a new route toward Floquet NHSEs originating from the interplay between gauge fields and dissipation effects and offers fundamentally new ways for steering light and other waves.","sentences":["Non-Hermitian physics has greatly enriched our understanding of nonequilibrium phenomena and uncovered novel effects such as the non-Hermitian skin effect (NHSE) that has profoundly revolutionized the field.","NHSE is typically predicted in systems with nonreciprocal couplings which, however, are difficult to realize in experiments.","Without nonreciprocal couplings, the NHSE can also emerge in systems with coexisting gauge fields and loss or gain (e.g., in Floquet non-Hermitian systems).","However, such Floquet NHSE remains largely unexplored in experiments.","Here, we realize the Floquet NHSEs in periodically modulated optical waveguides integrated on a silicon photonics platform.","By engineering the artificial gauge fields induced by the periodical modulation, we observe various Floquet NHSEs and unveil their rich topological transitions.","Remarkably, we discover the transitions between the normal unipolar NHSEs and an unconventional bipolar NHSE which is accompanied by the directional reversal of the NHSEs.","The underlying physics is revealed by the band winding in complex quasienergy space which undergoes a topology change from isolated loops with the same winding to linked loops with opposite windings.","Our work unfolds a new route toward Floquet NHSEs originating from the interplay between gauge fields and dissipation effects and offers fundamentally new ways for steering light and other waves."],"url":"http://arxiv.org/abs/2402.09700v1","category":"physics.optics"}
{"created":"2024-02-15 03:55:01","title":"Alive but Barely Kicking: News from 3+ years of Swift and XMM-Newton X-ray Monitoring of Quasi-Periodic Eruptions from eRO-QPE1","abstract":"Quasi-periodic Eruptions (QPEs) represent a novel class of extragalactic X-ray transients that are known to repeat at roughly regular intervals of a few hours to days. Their underlying physical mechanism is a topic of heated debate, with most models proposing that they originate either from instabilities within the inner accretion flow or from orbiting objects. At present, our knowledge of how QPEs evolve over an extended timescale of multiple years is limited, except for the unique QPE source GSN 069. In this study, we present results from strategically designed Swift observing programs spanning the past three years, aimed at tracking eruptions from eRO-QPE1. Our main results are: 1) the recurrence time of eruptions can vary between 0.6 and 1.2 days, 2) there is no detectable secular trend in evolution of the recurrence times, 3) consistent with prior studies, their eruption profiles can have complex shapes, and 4) the peak flux of the eruptions has been declining over the past 3 years with the eruptions barely detected in the most recent Swift dataset taken in June of 2023. This trend of weakening eruptions has been reported recently in GSN 069. However, because the background luminosity of eRO-QPE1 is below our detection limit, we cannot verify if the weakening is correlated with the background luminosity (as is claimed to be the case for GSN 069). We discuss these findings within the context of various proposed QPE models.","sentences":["Quasi-periodic Eruptions (QPEs) represent a novel class of extragalactic X-ray transients that are known to repeat at roughly regular intervals of a few hours to days.","Their underlying physical mechanism is a topic of heated debate, with most models proposing that they originate either from instabilities within the inner accretion flow or from orbiting objects.","At present, our knowledge of how QPEs evolve over an extended timescale of multiple years is limited, except for the unique QPE source GSN 069.","In this study, we present results from strategically designed Swift observing programs spanning the past three years, aimed at tracking eruptions from eRO-QPE1.","Our main results are: 1) the recurrence time of eruptions can vary between 0.6 and 1.2 days, 2) there is no detectable secular trend in evolution of the recurrence times, 3) consistent with prior studies, their eruption profiles can have complex shapes, and 4) the peak flux of the eruptions has been declining over the past 3 years with the eruptions barely detected in the most recent Swift dataset taken in June of 2023.","This trend of weakening eruptions has been reported recently in GSN 069.","However, because the background luminosity of eRO-QPE1 is below our detection limit, we cannot verify if the weakening is correlated with the background luminosity (as is claimed to be the case for GSN 069).","We discuss these findings within the context of various proposed QPE models."],"url":"http://arxiv.org/abs/2402.09690v1","category":"astro-ph.HE"}
{"created":"2024-02-15 03:53:33","title":"A System-Level Dynamic Binary Translator using Automatically-Learned Translation Rules","abstract":"System-level emulators have been used extensively for system design, debugging and evaluation. They work by providing a system-level virtual machine to support a guest operating system (OS) running on a platform with the same or different native OS that uses the same or different instruction-set architecture. For such system-level emulation, dynamic binary translation (DBT) is one of the core technologies. A recently proposed learning-based DBT approach has shown a significantly improved performance with a higher quality of translated code using automatically learned translation rules. However, it has only been applied to user-level emulation, and not yet to system-level emulation. In this paper, we explore the feasibility of applying this approach to improve system-level emulation, and use QEMU to build a prototype. ... To achieve better performance, we leverage several optimizations that include coordination overhead reduction to reduce the overhead of each coordination, and coordination elimination and code scheduling to reduce the coordination frequency. Experimental results show that it can achieve an average of 1.36X speedup over QEMU 6.1 with negligible coordination overhead in the system emulation mode using SPEC CINT2006 as application benchmarks and 1.15X on real-world applications.","sentences":["System-level emulators have been used extensively for system design, debugging and evaluation.","They work by providing a system-level virtual machine to support a guest operating system (OS) running on a platform with the same or different native OS that uses the same or different instruction-set architecture.","For such system-level emulation, dynamic binary translation (DBT) is one of the core technologies.","A recently proposed learning-based DBT approach has shown a significantly improved performance with a higher quality of translated code using automatically learned translation rules.","However, it has only been applied to user-level emulation, and not yet to system-level emulation.","In this paper, we explore the feasibility of applying this approach to improve system-level emulation, and use QEMU to build a prototype. ...","To achieve better performance, we leverage several optimizations that include coordination overhead reduction to reduce the overhead of each coordination, and coordination elimination and code scheduling to reduce the coordination frequency.","Experimental results show that it can achieve an average of 1.36X speedup over QEMU 6.1 with negligible coordination overhead in the system emulation mode using SPEC CINT2006 as application benchmarks and 1.15X on real-world applications."],"url":"http://arxiv.org/abs/2402.09688v1","category":"cs.OS"}
{"created":"2024-02-15 03:34:17","title":"Long-Range Backscatter Connectivity via Spaceborne Synthetic Aperture Radar","abstract":"SarComms is a new communication method that enables passive satellite backscatter connectivity using existing spaceborne synthetic aperture radar (SAR) signals. We demonstrate that SAR signals from the European Space Agency's Sentinel-1 satellite, used for imaging the Earth, can also be leveraged to enable ground-to-satellite connectivity. This paper presents the first cooperative, on-the-ground target that modulates SAR backscatter to send information bits and analyzes how to extract it from publicly available Sentinel-1 datasets. To demonstrate the system's feasibility, we evaluate the effectiveness of corner reflectors in the field, develop a deployment algorithm to optimize reflector placement and prototype modulating corner reflectors (both mechanically and electrically controlled) to change the amplitude of backscattered SAR signals.","sentences":["SarComms is a new communication method that enables passive satellite backscatter connectivity using existing spaceborne synthetic aperture radar (SAR) signals.","We demonstrate that SAR signals from the European Space Agency's Sentinel-1 satellite, used for imaging the Earth, can also be leveraged to enable ground-to-satellite connectivity.","This paper presents the first cooperative, on-the-ground target that modulates SAR backscatter to send information bits and analyzes how to extract it from publicly available Sentinel-1 datasets.","To demonstrate the system's feasibility, we evaluate the effectiveness of corner reflectors in the field, develop a deployment algorithm to optimize reflector placement and prototype modulating corner reflectors (both mechanically and electrically controlled) to change the amplitude of backscattered SAR signals."],"url":"http://arxiv.org/abs/2402.09682v1","category":"eess.SP"}
{"created":"2024-02-15 03:33:21","title":"Adaptive multi-spectral mimicking with 2D-material nanoresonator networks","abstract":"Active nanophotonic materials that can emulate and adapt between many different spectral profiles -- with high fidelity and over a broad bandwidth -- could have a far-reaching impact, but are challenging to design due to a high-dimensional and complex design space. Here, we show that a metamaterial network of coupled 2D-material nanoresonators in graphene can adaptively match multiple complex absorption spectra via a set of input voltages. To design such networks, we develop a semi-analytical auto-differentiable dipole-coupled model that allows scalable optimization of high-dimensional networks with many elements and voltage signals. As a demonstration of multi-spectral capability, we design a single network capable of mimicking four spectral targets resembling select gases (nitric oxide, nitrogen dioxide, methane, nitrous oxide) with very high fidelity (${>}\\,90\\%$). Our results are relevant for the design of highly reconfigurable optical materials and platforms for applications in sensing, communication and display technology, and signature and thermal management.","sentences":["Active nanophotonic materials that can emulate and adapt between many different spectral profiles -- with high fidelity and over a broad bandwidth -- could have a far-reaching impact, but are challenging to design due to a high-dimensional and complex design space.","Here, we show that a metamaterial network of coupled 2D-material nanoresonators in graphene can adaptively match multiple complex absorption spectra via a set of input voltages.","To design such networks, we develop a semi-analytical auto-differentiable dipole-coupled model that allows scalable optimization of high-dimensional networks with many elements and voltage signals.","As a demonstration of multi-spectral capability, we design a single network capable of mimicking four spectral targets resembling select gases (nitric oxide, nitrogen dioxide, methane, nitrous oxide) with very high fidelity (${>}\\,90\\%$).","Our results are relevant for the design of highly reconfigurable optical materials and platforms for applications in sensing, communication and display technology, and signature and thermal management."],"url":"http://arxiv.org/abs/2402.09681v1","category":"physics.optics"}
{"created":"2024-02-15 03:31:38","title":"Trade-off relations in open quantum dynamics via Robertson and Maccone-Pati uncertainty relations","abstract":"The Heisenberg uncertainty relation, together with its generalization by Robertson, serves as a fundamental concept in quantum mechanics, encapsulating that non-commutative pairs of observable cannot be measured precisely. In this Letter, we explore the Robertson uncertainty relation to demonstrate its effectiveness in establishing a series of thermodynamic uncertainty relations and quantum speed limits in open quantum dynamics. The derivation utilizes the scaled continuous matrix product state representation that maps the time evolution of quantum continuous measurement to the time evolution of the system and field. Specifically, we consider the Maccone-Pati uncertainty relation, a refinement of the Robertson uncertainty relation, to derive thermodynamic uncertainty relations and quantum speed limits within open quantum dynamics scenarios. These newly derived relations, which use a state orthogonal to the initial state, yield tighter bounds than the previously known bounds. Our findings not only reinforce the significance of the Robertson uncertainty relation, but also expand its applicability to identify uncertainty relations in open quantum dynamics.","sentences":["The Heisenberg uncertainty relation, together with its generalization by Robertson, serves as a fundamental concept in quantum mechanics, encapsulating that non-commutative pairs of observable cannot be measured precisely.","In this Letter, we explore the Robertson uncertainty relation to demonstrate its effectiveness in establishing a series of thermodynamic uncertainty relations and quantum speed limits in open quantum dynamics.","The derivation utilizes the scaled continuous matrix product state representation that maps the time evolution of quantum continuous measurement to the time evolution of the system and field.","Specifically, we consider the Maccone-Pati uncertainty relation, a refinement of the Robertson uncertainty relation, to derive thermodynamic uncertainty relations and quantum speed limits within open quantum dynamics scenarios.","These newly derived relations, which use a state orthogonal to the initial state, yield tighter bounds than the previously known bounds.","Our findings not only reinforce the significance of the Robertson uncertainty relation, but also expand its applicability to identify uncertainty relations in open quantum dynamics."],"url":"http://arxiv.org/abs/2402.09680v1","category":"quant-ph"}
{"created":"2024-02-15 03:23:06","title":"Design and Visual Servoing Control of a Hybrid Dual-Segment Flexible Neurosurgical Robot for Intraventricular Biopsy","abstract":"Traditional rigid endoscopes have challenges in flexibly treating tumors located deep in the brain, and low operability and fixed viewing angles limit its development. This study introduces a novel dual-segment flexible robotic endoscope MicroNeuro, designed to perform biopsies with dexterous surgical manipulation deep in the brain. Taking into account the uncertainty of the control model, an image-based visual servoing with online robot Jacobian estimation has been implemented to enhance motion accuracy. Furthermore, the application of model predictive control with constraints significantly bolsters the flexible robot's ability to adaptively track mobile objects and resist external interference. Experimental results underscore that the proposed control system enhances motion stability and precision. Phantom testing substantiates its considerable potential for deployment in neurosurgery.","sentences":["Traditional rigid endoscopes have challenges in flexibly treating tumors located deep in the brain, and low operability and fixed viewing angles limit its development.","This study introduces a novel dual-segment flexible robotic endoscope MicroNeuro, designed to perform biopsies with dexterous surgical manipulation deep in the brain.","Taking into account the uncertainty of the control model, an image-based visual servoing with online robot Jacobian estimation has been implemented to enhance motion accuracy.","Furthermore, the application of model predictive control with constraints significantly bolsters the flexible robot's ability to adaptively track mobile objects and resist external interference.","Experimental results underscore that the proposed control system enhances motion stability and precision.","Phantom testing substantiates its considerable potential for deployment in neurosurgery."],"url":"http://arxiv.org/abs/2402.09679v1","category":"cs.RO"}
{"created":"2024-02-15 03:09:54","title":"Prompt-based Personalized Federated Learning for Medical Visual Question Answering","abstract":"We present a novel prompt-based personalized federated learning (pFL) method to address data heterogeneity and privacy concerns in traditional medical visual question answering (VQA) methods. Specifically, we regard medical datasets from different organs as clients and use pFL to train personalized transformer-based VQA models for each client. To address the high computational complexity of client-to-client communication in previous pFL methods, we propose a succinct information sharing system by introducing prompts that are small learnable parameters. In addition, the proposed method introduces a reliability parameter to prevent the negative effects of low performance and irrelevant clients. Finally, extensive evaluations on various heterogeneous medical datasets attest to the effectiveness of our proposed method.","sentences":["We present a novel prompt-based personalized federated learning (pFL) method to address data heterogeneity and privacy concerns in traditional medical visual question answering (VQA) methods.","Specifically, we regard medical datasets from different organs as clients and use pFL to train personalized transformer-based VQA models for each client.","To address the high computational complexity of client-to-client communication in previous pFL methods, we propose a succinct information sharing system by introducing prompts that are small learnable parameters.","In addition, the proposed method introduces a reliability parameter to prevent the negative effects of low performance and irrelevant clients.","Finally, extensive evaluations on various heterogeneous medical datasets attest to the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2402.09677v1","category":"cs.CV"}
{"created":"2024-02-15 03:05:45","title":"HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network","abstract":"In data science, hypergraphs are natural models for data exhibiting multi-way relations, whereas graphs only capture pairwise. Nonetheless, many proposed hypergraph neural networks effectively reduce hypergraphs to undirected graphs via symmetrized matrix representations, potentially losing important information. We propose an alternative approach to hypergraph neural networks in which the hypergraph is represented as a non-reversible Markov chain. We use this Markov chain to construct a complex Hermitian Laplacian matrix - the magnetic Laplacian - which serves as the input to our proposed hypergraph neural network. We study HyperMagNet for the task of node classification, and demonstrate its effectiveness over graph-reduction based hypergraph neural networks.","sentences":["In data science, hypergraphs are natural models for data exhibiting multi-way relations, whereas graphs only capture pairwise.","Nonetheless, many proposed hypergraph neural networks effectively reduce hypergraphs to undirected graphs via symmetrized matrix representations, potentially losing important information.","We propose an alternative approach to hypergraph neural networks in which the hypergraph is represented as a non-reversible Markov chain.","We use this Markov chain to construct a complex Hermitian Laplacian matrix - the magnetic Laplacian - which serves as the input to our proposed hypergraph neural network.","We study HyperMagNet for the task of node classification, and demonstrate its effectiveness over graph-reduction based hypergraph neural networks."],"url":"http://arxiv.org/abs/2402.09676v1","category":"cs.LG"}
{"created":"2024-02-15 03:03:49","title":"Repurposing Coal Power Plants into Thermal Energy Storage for Supporting Zero-carbon Data Centers","abstract":"Coal power plants will need to be phased out and face stranded asset risks under the net-zero energy system transition. Repurposing coal power plants could recoup profits and reduce carbon emissions using the existing infrastructure and grid connections. This paper investigates a retrofitting strategy that turns coal power plants into thermal energy storage (TES) and zero-carbon data centers (DCs). The proposed capacity expansion model considers the co-locations of DCs, local renewablewith the system-generation, andlevel coal retir energy storage ement and retrofitting. We optimize the DC system configurations under the hourly-matching carbon policy and flexible operations. Results show that under hourly-matching carbon constraints, the retrofitted TES could complement the operations of lithium-ion batteries (LIBs) to reduce system costs. This could render DCs with optimal co-located renewable generations and energy storage more cost-effective than unconstrained DCs.","sentences":["Coal power plants will need to be phased out and face stranded asset risks under the net-zero energy system transition.","Repurposing coal power plants could recoup profits and reduce carbon emissions using the existing infrastructure and grid connections.","This paper investigates a retrofitting strategy that turns coal power plants into thermal energy storage (TES) and zero-carbon data centers (DCs).","The proposed capacity expansion model considers the co-locations of DCs, local renewablewith the system-generation, andlevel coal retir energy storage ement and retrofitting.","We optimize the DC system configurations under the hourly-matching carbon policy and flexible operations.","Results show that under hourly-matching carbon constraints, the retrofitted TES could complement the operations of lithium-ion batteries (LIBs) to reduce system costs.","This could render DCs with optimal co-located renewable generations and energy storage more cost-effective than unconstrained DCs."],"url":"http://arxiv.org/abs/2402.09675v1","category":"eess.SY"}
{"created":"2024-02-15 02:40:30","title":"A soluble model of a Non-Equilibrium Steady State: the van Kampen objection and other lessons","abstract":"A simple model of charge transport is provided by a classical particle in a smooth random potential and a dissipative coupling to the environment in the form of Markovian noise and friction. The corresponding Non-Equilibrium Steady State (NESS) can be determined analytically when both the disorder and dissipation are weak. We use it to illuminate some foundational issues in non-equilibrium statistical mechanics. We show that Linear Response Theory has a nonempty regime of validity only in the presence of a dissipative coupling to the environment, thereby validating van Kampen's objection. We also show that the Principle of Minimum Entropy Production does not determine the NESS beyond linear order in the electric field, while entropy maximization fails to produce the correct NESS already at linear order.","sentences":["A simple model of charge transport is provided by a classical particle in a smooth random potential and a dissipative coupling to the environment in the form of Markovian noise and friction.","The corresponding Non-Equilibrium Steady State (NESS) can be determined analytically when both the disorder and dissipation are weak.","We use it to illuminate some foundational issues in non-equilibrium statistical mechanics.","We show that Linear Response Theory has a nonempty regime of validity only in the presence of a dissipative coupling to the environment, thereby validating van Kampen's objection.","We also show that the Principle of Minimum Entropy Production does not determine the NESS beyond linear order in the electric field, while entropy maximization fails to produce the correct NESS already at linear order."],"url":"http://arxiv.org/abs/2402.09672v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-15 02:38:23","title":"Exploiting Alpha Transparency In Language And Vision-Based AI Systems","abstract":"This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems. Our method uses this alpha layer as a clandestine channel invisible to human observers but fully actionable by AI image processors. The scope tested for the vulnerability spans representative vision systems from Apple, Microsoft, Google, Salesforce, Nvidia, and Facebook, highlighting the attack's potential breadth. This vulnerability challenges the security protocols of existing and fielded vision systems, from medical imaging to autonomous driving technologies. Our experiments demonstrate that the affected systems, which rely on convolutional neural networks or the latest multimodal language models, cannot quickly mitigate these vulnerabilities through simple patches or updates. Instead, they require retraining and architectural changes, indicating a persistent hole in multimodal technologies without some future adversarial hardening against such vision-language exploits.","sentences":["This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems.","Our method uses this alpha layer as a clandestine channel invisible to human observers but fully actionable by AI image processors.","The scope tested for the vulnerability spans representative vision systems from Apple, Microsoft, Google, Salesforce, Nvidia, and Facebook, highlighting the attack's potential breadth.","This vulnerability challenges the security protocols of existing and fielded vision systems, from medical imaging to autonomous driving technologies.","Our experiments demonstrate that the affected systems, which rely on convolutional neural networks or the latest multimodal language models, cannot quickly mitigate these vulnerabilities through simple patches or updates.","Instead, they require retraining and architectural changes, indicating a persistent hole in multimodal technologies without some future adversarial hardening against such vision-language exploits."],"url":"http://arxiv.org/abs/2402.09671v1","category":"cs.CV"}
{"created":"2024-02-15 02:27:21","title":"Simple realization of a fragile topological lattice with quasi flat-bands in a microcavity array","abstract":"Topological flat bands (TFBs) are increasingly recognized as an important paradigm to study topological effects in the context of strong correlation physics. As a representative example, recently it has been theoretically proposed that the topological non-triviality offers a unique contribution to flat-band superconductivity, which can potentially lead to a higher critical temperature of superconductivity phase transition. Nevertheless, the topological effects within flat bands in bosonic systems, specifically in the context of Bose-Einstein condensation (BEC), are less explored. It has been shown theoretically that non-trivial topological and geometric properties will also have a significant influence in bosonic condensates as well. However, potential experimental realizations have not been extensively studied yet. In this work, we introduce a simple photonic lattice from coupled Kagome and triangular lattices designed based on topological quantum chemistry theory, which supports topologically nontrivial quasi-flat bands. Besides band representation analysis, the non-triviality of these quasi-flat bands is also confirmed by Wilson loop spectra which exhibit winding features. We further discuss the corresponding experimental realization in a microcavity array for future study supporting the potential extension to condensed exciton-polaritons. Notably, we showed that the inevitable in-plane longitudinal-transverse polarization splitting in optical microcavities will not hinder the construction of topological quasi-flat bands. This work acts as an initial step to experimentally explore the physical consequence of non-trivial topology and quantum geometry in quasi-flat bands in bosonic systems, offering potential channels for its direct observation.","sentences":["Topological flat bands (TFBs) are increasingly recognized as an important paradigm to study topological effects in the context of strong correlation physics.","As a representative example, recently it has been theoretically proposed that the topological non-triviality offers a unique contribution to flat-band superconductivity, which can potentially lead to a higher critical temperature of superconductivity phase transition.","Nevertheless, the topological effects within flat bands in bosonic systems, specifically in the context of Bose-Einstein condensation (BEC), are less explored.","It has been shown theoretically that non-trivial topological and geometric properties will also have a significant influence in bosonic condensates as well.","However, potential experimental realizations have not been extensively studied yet.","In this work, we introduce a simple photonic lattice from coupled Kagome and triangular lattices designed based on topological quantum chemistry theory, which supports topologically nontrivial quasi-flat bands.","Besides band representation analysis, the non-triviality of these quasi-flat bands is also confirmed by Wilson loop spectra which exhibit winding features.","We further discuss the corresponding experimental realization in a microcavity array for future study supporting the potential extension to condensed exciton-polaritons.","Notably, we showed that the inevitable in-plane longitudinal-transverse polarization splitting in optical microcavities will not hinder the construction of topological quasi-flat bands.","This work acts as an initial step to experimentally explore the physical consequence of non-trivial topology and quantum geometry in quasi-flat bands in bosonic systems, offering potential channels for its direct observation."],"url":"http://arxiv.org/abs/2402.09665v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-15 02:21:41","title":"Hand Shape and Gesture Recognition using Multiscale Template Matching, Background Subtraction and Binary Image Analysis","abstract":"This paper presents a hand shape classification approach employing multiscale template matching. The integration of background subtraction is utilized to derive a binary image of the hand object, enabling the extraction of key features such as centroid and bounding box. The methodology, while simple, demonstrates effectiveness in basic hand shape classification tasks, laying the foundation for potential applications in straightforward human-computer interaction scenarios. Experimental results highlight the system's capability in controlled environments.","sentences":["This paper presents a hand shape classification approach employing multiscale template matching.","The integration of background subtraction is utilized to derive a binary image of the hand object, enabling the extraction of key features such as centroid and bounding box.","The methodology, while simple, demonstrates effectiveness in basic hand shape classification tasks, laying the foundation for potential applications in straightforward human-computer interaction scenarios.","Experimental results highlight the system's capability in controlled environments."],"url":"http://arxiv.org/abs/2402.09663v1","category":"cs.CV"}
{"created":"2024-02-15 02:15:58","title":"GeoBotsVR: A Robotics Learning Game for Beginners with Hands-on Learning Simulation","abstract":"This article introduces GeoBotsVR, an easily accessible virtual reality game that combines elements of puzzle-solving with robotics learning and aims to cultivate interest and motivation in robotics, programming, and electronics among individuals with limited experience in these domains. The game allows players to build and customize a two-wheeled mobile robot using various robotic components and use their robot to solve various procedurally-generated puzzles in a diverse range of environments. An innovative aspect is the inclusion of a repair feature, requiring players to address randomly generated electronics and programming issues with their robot through hands-on manipulation. GeoBotsVR is designed to be immersive, replayable, and practical application-based, offering an enjoyable and accessible tool for beginners to acquaint themselves with robotics. The game simulates a hands-on learning experience and does not require prior technical knowledge, making it a potentially valuable resource for beginners to get an engaging introduction to the field of robotics.","sentences":["This article introduces GeoBotsVR, an easily accessible virtual reality game that combines elements of puzzle-solving with robotics learning and aims to cultivate interest and motivation in robotics, programming, and electronics among individuals with limited experience in these domains.","The game allows players to build and customize a two-wheeled mobile robot using various robotic components and use their robot to solve various procedurally-generated puzzles in a diverse range of environments.","An innovative aspect is the inclusion of a repair feature, requiring players to address randomly generated electronics and programming issues with their robot through hands-on manipulation.","GeoBotsVR is designed to be immersive, replayable, and practical application-based, offering an enjoyable and accessible tool for beginners to acquaint themselves with robotics.","The game simulates a hands-on learning experience and does not require prior technical knowledge, making it a potentially valuable resource for beginners to get an engaging introduction to the field of robotics."],"url":"http://arxiv.org/abs/2402.09662v1","category":"cs.HC"}
{"created":"2024-02-15 02:15:09","title":"Asymptotic stability for $n$-dimensional isentropic compressible MHD equations without magnetic diffusion","abstract":"Whether the global well-posedness of strong solutions of $n$-dimensional compressible isentropic magnetohydrodynamic (MHD for short) equations without magnetic diffusion holds true or not remains an challenging open problem, even for the small initial data. In recent years, stared from the pioneer work by Wu and Wu [Adv. Math. 310 (2017), 759--888], much more attention has been paid to the system when the magnetic field near an equilibrium state (the background magnetic field for short). In particular, when the background magnetic field satisfies the Diophantine condition (see (1.3) for details), Wu and Zhai [Math. Models Methods Appl. Sci. 33 (2023), no. 13, 2629--2656] established the decay estimates and asymptotic stability for smooth solutions of the 3D compressible isentropic MHD system without magnetic diffusion in $H^{4r+7}(\\mathbb{T}^3)$ with $r>2$ by exploiting a wave structure. In this paper, a new dissipative mechanism is found out and applied so that we can improve the spaces where the decay estimates and asymptotic stability of solutions are taking place by Wu and Zhai. More precisely, we establish the decay estimates of solutions in $H^{r+1}(\\mathbb{T}^n)$ and asymptotic stability result in $H^{\\left(3r+3\\right)^+}(\\mathbb{T}^n)$ for any dimensional periodic domain $\\mathbb{T}^n$ with $n\\geq 2$ and $r>n-1$. Our results provide an approach for establishing the decay estimates and asymptotic stability in the Sobolev spaces with much lower regularity and uniform dimension, which can be used to study many other related models such as the compressible non-isentropic MHD system without magnetic diffusion and so on.","sentences":["Whether the global well-posedness of strong solutions of $n$-dimensional compressible isentropic magnetohydrodynamic (MHD for short) equations without magnetic diffusion holds true or not remains an challenging open problem, even for the small initial data.","In recent years, stared from the pioneer work by Wu and Wu","[Adv.","Math. 310 (2017), 759--888], much more attention has been paid to the system when the magnetic field near an equilibrium state (the background magnetic field for short).","In particular, when the background magnetic field satisfies the Diophantine condition (see (1.3) for details), Wu and Zhai","[Math.","Models Methods Appl.","Sci. 33 (2023), no. 13, 2629--2656] established the decay estimates and asymptotic stability for smooth solutions of the 3D compressible isentropic MHD system without magnetic diffusion in $H^{4r+7}(\\mathbb{T}^3)$ with $r>2$ by exploiting a wave structure.","In this paper, a new dissipative mechanism is found out and applied so that we can improve the spaces where the decay estimates and asymptotic stability of solutions are taking place by Wu and Zhai.","More precisely, we establish the decay estimates of solutions in $H^{r+1}(\\mathbb{T}^n)$ and asymptotic stability result in $H^{\\left(3r+3\\right)^+}(\\mathbb{T}^n)$ for any dimensional periodic domain $\\mathbb{T}^n$ with $n\\geq 2$ and $r>n-1$. Our results provide an approach for establishing the decay estimates and asymptotic stability in the Sobolev spaces with much lower regularity and uniform dimension, which can be used to study many other related models such as the compressible non-isentropic MHD system without magnetic diffusion and so on."],"url":"http://arxiv.org/abs/2402.09661v1","category":"math.AP"}
{"created":"2024-02-15 18:57:07","title":"Direction-dependent conductivity in planar Hall set-ups with tilted Weyl/multi-Weyl semimetals","abstract":"We compute the magnetoelectric conductivity tensors in planar Hall set-ups, which are built with tilted Weyl semimetals (WSMs) and multi-Weyl semimetals (mWSMs), considering distinct relative orientations of the electromagnetic fields ($\\mathbf E $ and $\\mathbf B $) and the direction of the tilt. The non-Drude part of the response arises from a nonzero Berry curvature in the vicinity of the WSM/mWSM node under consideration. Only in the presence of a nonzero tilt do we find linear-in-$ | \\mathbf B| $ terms in set-ups where the tilt-axis is not perpendicular to the plane spanned by $\\mathbf E $ and $ \\mathbf B $. The advantage of the emergence of the linear-in-$ B$ terms is that, unlike the various $| \\mathbf B|^2 $-dependent terms that can contribute to experimental observations, they have purely a topological origin and they dominate the overall response-characteristics in the realistic parameter regimes. The important signatures of these terms are that (1) they change the periodicity of the response from $\\pi $ to $2\\pi$, when we consider their dependence on the angle $\\theta $ between $\\mathbf E $ and $\\mathbf B $; and (2) lead to an overall change in sign of the conductivity, when measured with respect to the $\\mathbf B =0$ case.","sentences":["We compute the magnetoelectric conductivity tensors in planar Hall set-ups, which are built with tilted Weyl semimetals (WSMs) and multi-Weyl semimetals (mWSMs), considering distinct relative orientations of the electromagnetic fields ($\\mathbf E $ and $\\mathbf B $) and the direction of the tilt.","The non-Drude part of the response arises from a nonzero Berry curvature in the vicinity of the WSM/mWSM node under consideration.","Only in the presence of a nonzero tilt do we find linear-in-$ | \\mathbf B| $ terms in set-ups where the tilt-axis is not perpendicular to the plane spanned by $\\mathbf E $ and $ \\mathbf B $.","The advantage of the emergence of the linear-in-$ B$ terms is that, unlike the various $| \\mathbf B|^2 $-dependent terms that can contribute to experimental observations, they have purely a topological origin and they dominate the overall response-characteristics in the realistic parameter regimes.","The important signatures of these terms are that (1) they change the periodicity of the response from $\\pi $ to $2\\pi$, when we consider their dependence on the angle $\\theta $ between $\\mathbf E $ and $\\mathbf B $; and (2) lead to an overall change in sign of the conductivity, when measured with respect to the $\\mathbf B =0$ case."],"url":"http://arxiv.org/abs/2402.10203v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-15 18:15:45","title":"The property $(E_A)$ and local spectral theory","abstract":"In this paper, we introduce and study the spectral property $(E_A)$. This property means that the difference between the approximate point spectrum and the upper semi-Fredholm spectrum coincides with the difference between the approximate point spectrum and the upper semi-Weyl spectrum. Together with local spectral theory, we explore the behavior of this property under certain topological conditions and derive characterizations for the operators that verify it. Furthermore, we establish sufficient conditions that guarantee that a bounded linear operator verifies the property $(E_A)$.","sentences":["In this paper, we introduce and study the spectral property $(E_A)$. This property means that the difference between the approximate point spectrum and the upper semi-Fredholm spectrum coincides with the difference between the approximate point spectrum and the upper semi-Weyl spectrum.","Together with local spectral theory, we explore the behavior of this property under certain topological conditions and derive characterizations for the operators that verify it.","Furthermore, we establish sufficient conditions that guarantee that a bounded linear operator verifies the property $(E_A)$."],"url":"http://arxiv.org/abs/2402.10170v1","category":"math.FA"}
{"created":"2024-02-15 17:34:16","title":"Holographic covering and the fortuity of black holes","abstract":"We propose a classification of BPS states into monotone versus fortuitous in holographic CFTs, based on their behaviors in the large $N$ limit. Intuitively, monotone BPS states form infinite sequences with increasing rank $N$, while fortuitous ones are isolated existences at individual ranks. A precise definition is formulated using supercharge cohomology. We conjecture that under the AdS/CFT correspondence, monotone BPS states are dual to smooth horizonless geometries, while fortuitous ones are responsible for the typical black hole microstate and give the dominant contribution to the entropy. We present supporting evidence for our conjectures in ${\\cal N}=4$ SYM and symmetric product orbifolds.","sentences":["We propose a classification of BPS states into monotone versus fortuitous in holographic CFTs, based on their behaviors in the large $N$ limit.","Intuitively, monotone BPS states form infinite sequences with increasing rank $N$, while fortuitous ones are isolated existences at individual ranks.","A precise definition is formulated using supercharge cohomology.","We conjecture that under the AdS/CFT correspondence, monotone BPS states are dual to smooth horizonless geometries, while fortuitous ones are responsible for the typical black hole microstate and give the dominant contribution to the entropy.","We present supporting evidence for our conjectures in ${\\cal N}=4$ SYM and symmetric product orbifolds."],"url":"http://arxiv.org/abs/2402.10129v1","category":"hep-th"}
{"created":"2024-02-15 16:43:51","title":"Uncovering the Three-Dimensional Structure of Upconverting Core-Shell Nanoparticles with Multislice Electron Ptychography","abstract":"In photon upconverting core-shell nanoparticles, structure strongly dictates performance. Conventional imaging in scanning transmission electron microscopy has sufficient resolution to probe the atomic structure of these nanoparticles, but contrast, dose, and projection limitations make conventional imaging modes insufficient for fully characterizing these structures. Phase retrieval methods provide a promising alternative imaging mode, and in particular, multislice electron ptychography can recover depth-dependent information. Here, we study beam-sensitive photon upconverting core-shell nanoparticles with a multislice ptychography approach using a low electron dose to avoid damage. Large strain fields arise in these heterostructures due to the mismatch in lattice parameter between the core and the shell. We reconstruct both a nanoparticle that appears defect-free and one that has a large break in the side and map the distribution of strain in 3D by computing distortion fields from high-resolution potential images of each slice. In the defect-free nanoparticle, we observe twisting of the shell, while in the broken nanoparticle we measure the 3D position of the crack, the core, and dislocations. These results highlight the advantage of multislice electron ptychography to recover 3D information from a single scan, even under strict electron dose requirements from beam-sensitive samples.","sentences":["In photon upconverting core-shell nanoparticles, structure strongly dictates performance.","Conventional imaging in scanning transmission electron microscopy has sufficient resolution to probe the atomic structure of these nanoparticles, but contrast, dose, and projection limitations make conventional imaging modes insufficient for fully characterizing these structures.","Phase retrieval methods provide a promising alternative imaging mode, and in particular, multislice electron ptychography can recover depth-dependent information.","Here, we study beam-sensitive photon upconverting core-shell nanoparticles with a multislice ptychography approach using a low electron dose to avoid damage.","Large strain fields arise in these heterostructures due to the mismatch in lattice parameter between the core and the shell.","We reconstruct both a nanoparticle that appears defect-free and one that has a large break in the side and map the distribution of strain in 3D by computing distortion fields from high-resolution potential images of each slice.","In the defect-free nanoparticle, we observe twisting of the shell, while in the broken nanoparticle we measure the 3D position of the crack, the core, and dislocations.","These results highlight the advantage of multislice electron ptychography to recover 3D information from a single scan, even under strict electron dose requirements from beam-sensitive samples."],"url":"http://arxiv.org/abs/2402.10084v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-15 16:31:54","title":"NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction","abstract":"Drawing inspiration from the primate brain's intriguing evidence accumulation process, and guided by models from cognitive psychology and neuroscience, the paper introduces the NYCTALE framework, a neuro-inspired and evidence accumulation-based Transformer architecture. The proposed neuro-inspired NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for lung cancer diagnosis. In nature, Nyctales are small owls known for their nocturnal behavior, hunting primarily during the darkness of night. The NYCTALE operates in a similarly vigilant manner, i.e., processing data in an evidence-based fashion and making predictions dynamically/adaptively. Distinct from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the NYCTALE performs predictions only when sufficient amount of evidence is accumulated. In other words, instead of processing all or a pre-defined subset of CT slices, for each person, slices are provided one at a time. The NYCTALE framework then computes an evidence vector associated with contribution of each new CT image. A decision is made once the total accumulated evidence surpasses a specific threshold. Preliminary experimental analyses conducted using a challenging in-house dataset comprising 114 subjects. The results are noteworthy, suggesting that NYCTALE outperforms the benchmark accuracy even with approximately 60% less training data on this demanding and small dataset.","sentences":["Drawing inspiration from the primate brain's intriguing evidence accumulation process, and guided by models from cognitive psychology and neuroscience, the paper introduces the NYCTALE framework, a neuro-inspired and evidence accumulation-based Transformer architecture.","The proposed neuro-inspired NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for lung cancer diagnosis.","In nature, Nyctales are small owls known for their nocturnal behavior, hunting primarily during the darkness of night.","The NYCTALE operates in a similarly vigilant manner, i.e., processing data in an evidence-based fashion and making predictions dynamically/adaptively.","Distinct from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the NYCTALE performs predictions only when sufficient amount of evidence is accumulated.","In other words, instead of processing all or a pre-defined subset of CT slices, for each person, slices are provided one at a time.","The NYCTALE framework then computes an evidence vector associated with contribution of each new CT image.","A decision is made once the total accumulated evidence surpasses a specific threshold.","Preliminary experimental analyses conducted using a challenging in-house dataset comprising 114 subjects.","The results are noteworthy, suggesting that NYCTALE outperforms the benchmark accuracy even with approximately 60% less training data on this demanding and small dataset."],"url":"http://arxiv.org/abs/2402.10066v1","category":"cs.CV"}
{"created":"2024-02-15 16:23:24","title":"Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using Low-rank Updates","abstract":"Timeline algorithms are key parts of online social networks, but during recent years they have been blamed for increasing polarization and disagreement in our society. Opinion-dynamics models have been used to study a variety of phenomena in online social networks, but an open question remains on how these models can be augmented to take into account the fine-grained impact of user-level timeline algorithms. We make progress on this question by providing a way to model the impact of timeline algorithms on opinion dynamics. Specifically, we show how the popular Friedkin--Johnsen opinion-formation model can be augmented based on aggregate information, extracted from timeline data. We use our model to study the problem of minimizing the polarization and disagreement; we assume that we are allowed to make small changes to the users' timeline compositions by strengthening some topics of discussion and penalizing some others. We present a gradient descent-based algorithm for this problem, and show that under realistic parameter settings, our algorithm computes a $(1+\\varepsilon)$-approximate solution in time $\\tilde{O}(m\\sqrt{n} \\lg(1/\\varepsilon))$, where $m$ is the number of edges in the graph and $n$ is the number of vertices. We also present an algorithm that provably computes an $\\varepsilon$-approximation of our model in near-linear time. We evaluate our method on real-world data and show that it effectively reduces the polarization and disagreement in the network. Finally, we release an anonymized graph dataset with ground-truth opinions and more than 27\\,000 nodes (the previously largest publicly available dataset contains less than 550 nodes).","sentences":["Timeline algorithms are key parts of online social networks, but during recent years they have been blamed for increasing polarization and disagreement in our society.","Opinion-dynamics models have been used to study a variety of phenomena in online social networks, but an open question remains on how these models can be augmented to take into account the fine-grained impact of user-level timeline algorithms.","We make progress on this question by providing a way to model the impact of timeline algorithms on opinion dynamics.","Specifically, we show how the popular Friedkin--Johnsen opinion-formation model can be augmented based on aggregate information, extracted from timeline data.","We use our model to study the problem of minimizing the polarization and disagreement; we assume that we are allowed to make small changes to the users' timeline compositions by strengthening some topics of discussion and penalizing some others.","We present a gradient descent-based algorithm for this problem, and show that under realistic parameter settings, our algorithm computes a $(1+\\varepsilon)$-approximate solution in time $\\tilde{O}(m\\sqrt{n} \\lg(1/\\varepsilon))$, where $m$ is the number of edges in the graph and $n$ is the number of vertices.","We also present an algorithm that provably computes an $\\varepsilon$-approximation of our model in near-linear time.","We evaluate our method on real-world data and show that it effectively reduces the polarization and disagreement in the network.","Finally, we release an anonymized graph dataset with ground-truth opinions and more than 27\\,000 nodes (the previously largest publicly available dataset contains less than 550 nodes)."],"url":"http://arxiv.org/abs/2402.10053v1","category":"cs.SI"}
{"created":"2024-02-15 16:10:43","title":"A Field Guide to Ethics in Mathematics","abstract":"Mathematics has become inescapable in modern, digitized societies: there is hardly any area of life left that isn't affected by it, and we as mathematicians play a central role in this. Our actions affect what others, in particular our students, decide to do with mathematics, and how mathematics affects the world, for better or worse. In return, the study of ethics in mathematics (EiM) has become increasingly important, even though it is still unknown to many. This exposition tries to change that, by motivating ethics in mathematics as an interesting, tractable, non-trivial, well-defined and good research area for mathematicians to consider.","sentences":["Mathematics has become inescapable in modern, digitized societies: there is hardly any area of life left that isn't affected by it, and we as mathematicians play a central role in this.","Our actions affect what others, in particular our students, decide to do with mathematics, and how mathematics affects the world, for better or worse.","In return, the study of ethics in mathematics (EiM) has become increasingly important, even though it is still unknown to many.","This exposition tries to change that, by motivating ethics in mathematics as an interesting, tractable, non-trivial, well-defined and good research area for mathematicians to consider."],"url":"http://arxiv.org/abs/2402.10048v1","category":"math.HO"}
{"created":"2024-02-15 16:05:35","title":"How to validate average calibration for machine learning regression tasks ?","abstract":"Average calibration of the uncertainties of machine learning regression tasks can be tested in two ways. One way is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty. The alternative is to compare the mean squared z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different conclusion, as illustrated on an ensemble of datasets from the recent machine learning uncertainty quantification literature. It is shown here that the CE is very sensitive to the distribution of uncertainties, and notably to the presence of outlying uncertainties, and that it cannot be used reliably for calibration testing. By contrast, the ZMS statistic does not present this sensitivity issue and offers the most reliable approach in this context. Implications for the validation of conditional calibration are discussed.","sentences":["Average calibration of the uncertainties of machine learning regression tasks can be tested in two ways.","One way is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty.","The alternative is to compare the mean squared z-scores or scaled errors (ZMS) to 1.","Both approaches might lead to different conclusion, as illustrated on an ensemble of datasets from the recent machine learning uncertainty quantification literature.","It is shown here that the CE is very sensitive to the distribution of uncertainties, and notably to the presence of outlying uncertainties, and that it cannot be used reliably for calibration testing.","By contrast, the ZMS statistic does not present this sensitivity issue and offers the most reliable approach in this context.","Implications for the validation of conditional calibration are discussed."],"url":"http://arxiv.org/abs/2402.10043v1","category":"stat.ML"}
{"created":"2024-02-15 16:01:59","title":"Feature Accentuation: Revealing 'What' Features Respond to in Natural Images","abstract":"Efforts to decode neural network vision models necessitate a comprehensive grasp of both the spatial and semantic facets governing feature responses within images. Most research has primarily centered around attribution methods, which provide explanations in the form of heatmaps, showing where the model directs its attention for a given feature. However, grasping 'where' alone falls short, as numerous studies have highlighted the limitations of those methods and the necessity to understand 'what' the model has recognized at the focal point of its attention. In parallel, 'Feature visualization' offers another avenue for interpreting neural network features. This approach synthesizes an optimal image through gradient ascent, providing clearer insights into 'what' features respond to. However, feature visualizations only provide one global explanation per feature; they do not explain why features activate for particular images. In this work, we introduce a new method to the interpretability tool-kit, 'feature accentuation', which is capable of conveying both where and what in arbitrary input images induces a feature's response. At its core, feature accentuation is image-seeded (rather than noise-seeded) feature visualization. We find a particular combination of parameterization, augmentation, and regularization yields naturalistic visualizations that resemble the seed image and target feature simultaneously. Furthermore, we validate these accentuations are processed along a natural circuit by the model. We make our precise implementation of feature accentuation available to the community as the Faccent library, an extension of Lucent.","sentences":["Efforts to decode neural network vision models necessitate a comprehensive grasp of both the spatial and semantic facets governing feature responses within images.","Most research has primarily centered around attribution methods, which provide explanations in the form of heatmaps, showing where the model directs its attention for a given feature.","However, grasping 'where' alone falls short, as numerous studies have highlighted the limitations of those methods and the necessity to understand 'what' the model has recognized at the focal point of its attention.","In parallel, 'Feature visualization' offers another avenue for interpreting neural network features.","This approach synthesizes an optimal image through gradient ascent, providing clearer insights into 'what' features respond to.","However, feature visualizations only provide one global explanation per feature; they do not explain why features activate for particular images.","In this work, we introduce a new method to the interpretability tool-kit, 'feature accentuation', which is capable of conveying both where and what in arbitrary input images induces a feature's response.","At its core, feature accentuation is image-seeded (rather than noise-seeded) feature visualization.","We find a particular combination of parameterization, augmentation, and regularization yields naturalistic visualizations that resemble the seed image and target feature simultaneously.","Furthermore, we validate these accentuations are processed along a natural circuit by the model.","We make our precise implementation of feature accentuation available to the community as the Faccent library, an extension of Lucent."],"url":"http://arxiv.org/abs/2402.10039v1","category":"cs.CV"}
{"created":"2024-02-15 15:58:42","title":"Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity","abstract":"Purpose: We apply federated learning to train an OCT image classifier simulating a realistic scenario with multiple clients and statistical heterogeneous data distribution where data in the clients lack samples of some categories entirely.   Methods: We investigate the effectiveness of FedAvg and FedProx to train an OCT image classification model in a decentralized fashion, addressing privacy concerns associated with centralizing data. We partitioned a publicly available OCT dataset across multiple clients under IID and Non-IID settings and conducted local training on the subsets for each client. We evaluated two federated learning methods, FedAvg and FedProx for these settings.   Results: Our experiments on the dataset suggest that under IID settings, both methods perform on par with training on a central data pool. However, the performance of both algorithms declines as we increase the statistical heterogeneity across the client data, while FedProx consistently performs better than FedAvg in the increased heterogeneity settings.   Conclusion: Despite the effectiveness of federated learning in the utilization of private data across multiple medical institutions, the large number of clients and heterogeneous distribution of labels deteriorate the performance of both algorithms. Notably, FedProx appears to be more robust to the increased heterogeneity.","sentences":["Purpose: We apply federated learning to train an OCT image classifier simulating a realistic scenario with multiple clients and statistical heterogeneous data distribution where data in the clients lack samples of some categories entirely.   ","Methods: We investigate the effectiveness of FedAvg and FedProx to train an OCT image classification model in a decentralized fashion, addressing privacy concerns associated with centralizing data.","We partitioned a publicly available OCT dataset across multiple clients under IID and Non-IID settings and conducted local training on the subsets for each client.","We evaluated two federated learning methods, FedAvg and FedProx for these settings.   ","Results: Our experiments on the dataset suggest that under IID settings, both methods perform on par with training on a central data pool.","However, the performance of both algorithms declines as we increase the statistical heterogeneity across the client data, while FedProx consistently performs better than FedAvg in the increased heterogeneity settings.   ","Conclusion: Despite the effectiveness of federated learning in the utilization of private data across multiple medical institutions, the large number of clients and heterogeneous distribution of labels deteriorate the performance of both algorithms.","Notably, FedProx appears to be more robust to the increased heterogeneity."],"url":"http://arxiv.org/abs/2402.10035v1","category":"cs.CV"}
{"created":"2024-02-15 15:57:32","title":"Neural Network Approaches for Parameterized Optimal Control","abstract":"We consider numerical approaches for deterministic, finite-dimensional optimal control problems whose dynamics depend on unknown or uncertain parameters. We seek to amortize the solution over a set of relevant parameters in an offline stage to enable rapid decision-making and be able to react to changes in the parameter in the online stage. To tackle the curse of dimensionality arising when the state and/or parameter are high-dimensional, we represent the policy using neural networks. We compare two training paradigms: First, our model-based approach leverages the dynamics and definition of the objective function to learn the value function of the parameterized optimal control problem and obtain the policy using a feedback form. Second, we use actor-critic reinforcement learning to approximate the policy in a data-driven way. Using an example involving a two-dimensional convection-diffusion equation, which features high-dimensional state and parameter spaces, we investigate the accuracy and efficiency of both training paradigms. While both paradigms lead to a reasonable approximation of the policy, the model-based approach is more accurate and considerably reduces the number of PDE solves.","sentences":["We consider numerical approaches for deterministic, finite-dimensional optimal control problems whose dynamics depend on unknown or uncertain parameters.","We seek to amortize the solution over a set of relevant parameters in an offline stage to enable rapid decision-making and be able to react to changes in the parameter in the online stage.","To tackle the curse of dimensionality arising when the state and/or parameter are high-dimensional, we represent the policy using neural networks.","We compare two training paradigms:","First, our model-based approach leverages the dynamics and definition of the objective function to learn the value function of the parameterized optimal control problem and obtain the policy using a feedback form.","Second, we use actor-critic reinforcement learning to approximate the policy in a data-driven way.","Using an example involving a two-dimensional convection-diffusion equation, which features high-dimensional state and parameter spaces, we investigate the accuracy and efficiency of both training paradigms.","While both paradigms lead to a reasonable approximation of the policy, the model-based approach is more accurate and considerably reduces the number of PDE solves."],"url":"http://arxiv.org/abs/2402.10033v1","category":"math.OC"}
{"created":"2024-02-15 15:56:42","title":"Dimension-free Structured Covariance Estimation","abstract":"Given a sample of i.i.d. high-dimensional centered random vectors, we consider a problem of estimation of their covariance matrix $\\Sigma$ with an additional assumption that $\\Sigma$ can be represented as a sum of a few Kronecker products of smaller matrices. Under mild conditions, we derive the first non-asymptotic dimension-free high-probability bound on the Frobenius distance between $\\Sigma$ and a widely used penalized permuted least squares estimate. Because of the hidden structure, the established rate of convergence is faster than in the standard covariance estimation problem.","sentences":["Given a sample of i.i.d. high-dimensional centered random vectors, we consider a problem of estimation of their covariance matrix $\\Sigma$ with an additional assumption that $\\Sigma$ can be represented as a sum of a few Kronecker products of smaller matrices.","Under mild conditions, we derive the first non-asymptotic dimension-free high-probability bound on the Frobenius distance between $\\Sigma$ and a widely used penalized permuted least squares estimate.","Because of the hidden structure, the established rate of convergence is faster than in the standard covariance estimation problem."],"url":"http://arxiv.org/abs/2402.10032v1","category":"math.ST"}
{"created":"2024-02-15 15:31:10","title":"More on graph pebbling number","abstract":"Let $G=(V,E)$ be a simple graph. A function $\\phi:V\\rightarrow \\mathbb{N}\\cup \\{0\\}$ is called a configuration of pebbles on the vertices of $G$ and the quantity $\\sum_{u\\in V}\\phi(u)$ is called the size of $\\phi$ which is just the total number of pebbles assigned to vertices. A pebbling step from a vertex $u$ to one of its neighbors $v$ reduces $\\phi(u)$ by two and increases $\\phi(v)$ by one. Given a specified target vertex $r$ we say that $\\phi$ is $t$-fold $r$-solvable, if some sequence of pebbling steps places at least $t$ pebbles on $r$. Conversely, if no such steps exist, then $\\phi$ is $r$-unsolvable. The minimum positive integer $m$ such that every configuration of size $m$ on the vertices of $G$ is $t$-fold $r$-solvable is denoted by $\\pi_t(G,r)$. The $t$-fold pebbling number of $G$ is defined to be $\\pi_t(G)= max_{r\\in V(G)}\\pi_t(G,r)$. When $t=1$, we simply write $\\pi(G)$, which is the pebbling number of $G$. In this note, we study the pebbling number for some specific graphs. Also we investigate the pebbling number of corona and neighbourhood corona of two graphs.","sentences":["Let $G=(V,E)$ be a simple graph.","A function $\\phi:V\\rightarrow \\mathbb{N}\\cup \\{0\\}$ is called a configuration of pebbles on the vertices of $G$ and the quantity $\\sum_{u\\in V}\\phi(u)$ is called the size of $\\phi$ which is just the total number of pebbles assigned to vertices.","A pebbling step from a vertex $u$ to one of its neighbors $v$ reduces $\\phi(u)$ by two and increases $\\phi(v)$ by one.","Given a specified target vertex $r$ we say that $\\phi$ is $t$-fold $r$-solvable, if some sequence of pebbling steps places at least $t$ pebbles on $r$. Conversely, if no such steps exist, then $\\phi$ is $r$-unsolvable.","The minimum positive integer $m$ such that every configuration of size $m$ on the vertices of $G$ is $t$-fold $r$-solvable is denoted by $\\pi_t(G,r)$. The $t$-fold pebbling number of $G$ is defined to be $\\pi_t(G)= max_{r\\in V(G)}\\pi_t(G,r)$. When $t=1$, we simply write $\\pi(G)$, which is the pebbling number of $G$.","In this note, we study the pebbling number for some specific graphs.","Also we investigate the pebbling number of corona and neighbourhood corona of two graphs."],"url":"http://arxiv.org/abs/2402.10017v1","category":"math.CO"}
{"created":"2024-02-15 15:12:03","title":"Sharp upper bound for anisotropic R\u00e9nyi entropy and Heisenberg uncertainty principle","abstract":"In this paper, we prove the anisotropic Shannon inequality for the Renyi entropy with the best constant on Folland-Stein homogeneous Lie groups. As a consequence, we also prove the optimal Shannon inequality in the same setting. Using a logarithmic Sobolev inequality in the setting of stratified groups, we prove a Heisenberg-type uncertainty principle in the latter setting.","sentences":["In this paper, we prove the anisotropic Shannon inequality for the Renyi entropy with the best constant on Folland-Stein homogeneous Lie groups.","As a consequence, we also prove the optimal Shannon inequality in the same setting.","Using a logarithmic Sobolev inequality in the setting of stratified groups, we prove a Heisenberg-type uncertainty principle in the latter setting."],"url":"http://arxiv.org/abs/2402.10003v1","category":"math.FA"}
{"created":"2024-02-15 15:05:24","title":"Colouring graphs from random lists","abstract":"Given positive integers $k \\leq m$ and a graph $G$, a family of lists $L = \\{L(v) : v \\in V(G)\\}$ is said to be a random $(k,m)$-list-assignment if for every $v \\in V(G)$ the list $L(v)$ is a subset of $\\{1, \\ldots, m\\}$ of size $k$, chosen uniformly at random and independently of the choices of all other vertices. An $n$-vertex graph $G$ is said to be a.a.s. $(k,m)$-colourable if $\\lim_{n \\to \\infty} \\mathbb{P}(G \\textrm{ is } L-colourable) = 1$, where $L$ is a random $(k,m)$-list-assignment. We prove that if $m \\gg n^{1/k^2} \\Delta^{1/k}$ and $m \\geq 3 k^2 \\Delta$, where $\\Delta$ is the maximum degree of $G$ and $k \\geq 3$ is an integer, then $G$ is a.a.s. $(k,m)$-colourable. This is not far from being best possible, forms a continuation of the so-called palette sparsification results, and proves in a strong sense a conjecture of Casselgren. Additionally, we consider this problem under the additional assumption that $G$ is $H$-free for some graph $H$. For various graphs $H$, we estimate the smallest $m$ for which an $H$-free $n$-vertex graph $G$ is a.a.s. $(k,m)$-colourable. This extends and improves several results of Casselgren.","sentences":["Given positive integers $k \\leq m$ and a graph $G$, a family of lists $L = \\{L(v) : v \\in V(G)\\}$ is said to be a random $(k,m)$-list-assignment if for every $v \\in V(G)$ the list $L(v)$ is a subset of $\\{1, \\ldots, m\\}$ of size $k$, chosen uniformly at random and independently of the choices of all other vertices.","An $n$-vertex graph $G$ is said to be a.a.s.","$(k,m)$-colourable if $\\lim_{n \\to \\infty} \\mathbb{P}(G \\textrm{ is } L-colourable) = 1$, where $L$ is a random $(k,m)$-list-assignment.","We prove that if $m \\gg n^{1/k^2} \\Delta^{1/k}$ and $m \\geq 3 k^2 \\Delta$, where $\\Delta$ is the maximum degree of $G$ and $k \\geq 3$ is an integer, then $G$ is a.a.s.","$(k,m)$-colourable.","This is not far from being best possible, forms a continuation of the so-called palette sparsification results, and proves in a strong sense a conjecture of Casselgren.","Additionally, we consider this problem under the additional assumption that $G$ is $H$-free for some graph $H$. For various graphs $H$, we estimate the smallest $m$ for which an $H$-free $n$-vertex graph $G$ is a.a.s.","$(k,m)$-colourable.","This extends and improves several results of Casselgren."],"url":"http://arxiv.org/abs/2402.09998v1","category":"math.CO"}
{"created":"2024-02-15 14:15:05","title":"Rationality of extended unipotent characters","abstract":"We determine the rationality properties of unipotent characters of finite reductive groups arising as fixed points of disconnected reductive groups under a Frobenius map. In the proof we use realisations of characters in $\\ell$-adic cohomology groups of Deligne--Lusztig varieties as well as block theoretic considerations.","sentences":["We determine the rationality properties of unipotent characters of finite reductive groups arising as fixed points of disconnected reductive groups under a Frobenius map.","In the proof we use realisations of characters in $\\ell$-adic cohomology groups of Deligne--Lusztig varieties as well as block theoretic considerations."],"url":"http://arxiv.org/abs/2402.09960v1","category":"math.RT"}
{"created":"2024-02-15 14:00:57","title":"NSI effects on tripartite entanglement in neutrino oscillations","abstract":"In this study, we investigate the impact of new physics on different measures of tripartite entanglement within the context of three-flavor neutrino oscillations. These measures encompass concurrence, entanglement of formation, and negativity. We analyze the influence of new physics on these measures across a range of experimental setups involving both reactors and accelerators. Reactor experiments under consideration include Daya Bay, JUNO, and KamLAND setups, while accelerator experiments encompass T2K, MINOS, and DUNE. Our analysis reveals that accelerator experiments demonstrate greater sensitivity to NSI, with the most pronounced impact observed in the DUNE experiment. Negativity, while a weaker metric compared to EOF and concurrence, exhibits maximal sensitivity to NSI effects, particularly evident when neutrinos possess moderate to high energies. Conversely, reactor experiments demonstrate less sensitivity to NSI, with concurrence and EOF displaying more prominent effects.","sentences":["In this study, we investigate the impact of new physics on different measures of tripartite entanglement within the context of three-flavor neutrino oscillations.","These measures encompass concurrence, entanglement of formation, and negativity.","We analyze the influence of new physics on these measures across a range of experimental setups involving both reactors and accelerators.","Reactor experiments under consideration include Daya Bay, JUNO, and KamLAND setups, while accelerator experiments encompass T2K, MINOS, and DUNE.","Our analysis reveals that accelerator experiments demonstrate greater sensitivity to NSI, with the most pronounced impact observed in the DUNE experiment.","Negativity, while a weaker metric compared to EOF and concurrence, exhibits maximal sensitivity to NSI effects, particularly evident when neutrinos possess moderate to high energies.","Conversely, reactor experiments demonstrate less sensitivity to NSI, with concurrence and EOF displaying more prominent effects."],"url":"http://arxiv.org/abs/2402.09952v1","category":"hep-ph"}
{"created":"2024-02-15 13:40:01","title":"Representation type of higher level cyclotomic quiver Hecke algebras in affine type C","abstract":"We determine representation type of cyclotomic quiver Hecke algebras whose Lie type are affine type C. When they are tame, we give their basic algebras in explicit form under the assumption $\\text{ch}\\ \\mathbb{k}\\ne2$, which we require cellularity to be Morita invariant.","sentences":["We determine representation type of cyclotomic quiver Hecke algebras whose Lie type are affine type C. When they are tame, we give their basic algebras in explicit form under the assumption $\\text{ch}\\ \\mathbb{k}\\ne2$, which we require cellularity to be Morita invariant."],"url":"http://arxiv.org/abs/2402.09940v1","category":"math.RT"}
{"created":"2024-02-15 13:39:44","title":"Optimal Bayesian stepped-wedge cluster randomised trial designs for binary outcome data","abstract":"Under a generalised estimating equation analysis approach, approximate design theory is used to determine Bayesian D-optimal designs. For two examples, considering simple exchangeable and exponential decay correlation structures, we compare the efficiency of identified optimal designs to balanced stepped-wedge designs and corresponding stepped-wedge designs determined by optimising using a normal approximation approach. The dependence of the Bayesian D-optimal designs on the assumed correlation structure is explored; for the considered settings, smaller decay in the correlation between outcomes across time periods, along with larger values of the intra-cluster correlation, leads to designs closer to a balanced design being optimal. Unlike for normal data, it is shown that the optimal design need not be centro-symmetric in the binary outcome case. The efficiency of the Bayesian D-optimal design relative to a balanced design can be large, but situations are demonstrated in which the advantages are small. Similarly, the optimal design from a normal approximation approach is often not much less efficient than the Bayesian D-optimal design. Bayesian D-optimal designs can be readily identified for stepped-wedge cluster randomised trials with binary outcome data. In certain circumstances, principally ones with strong time period effects, they will indicate that a design unlikely to have been identified by previous methods may be substantially more efficient. However, they require a larger number of assumptions than existing optimal designs, and in many situations existing theory under a normal approximation will provide an easier means of identifying an efficient design for binary outcome data.","sentences":["Under a generalised estimating equation analysis approach, approximate design theory is used to determine Bayesian D-optimal designs.","For two examples, considering simple exchangeable and exponential decay correlation structures, we compare the efficiency of identified optimal designs to balanced stepped-wedge designs and corresponding stepped-wedge designs determined by optimising using a normal approximation approach.","The dependence of the Bayesian D-optimal designs on the assumed correlation structure is explored; for the considered settings, smaller decay in the correlation between outcomes across time periods, along with larger values of the intra-cluster correlation, leads to designs closer to a balanced design being optimal.","Unlike for normal data, it is shown that the optimal design need not be centro-symmetric in the binary outcome case.","The efficiency of the Bayesian D-optimal design relative to a balanced design can be large, but situations are demonstrated in which the advantages are small.","Similarly, the optimal design from a normal approximation approach is often not much less efficient than the Bayesian D-optimal design.","Bayesian D-optimal designs can be readily identified for stepped-wedge cluster randomised trials with binary outcome data.","In certain circumstances, principally ones with strong time period effects, they will indicate that a design unlikely to have been identified by previous methods may be substantially more efficient.","However, they require a larger number of assumptions than existing optimal designs, and in many situations existing theory under a normal approximation will provide an easier means of identifying an efficient design for binary outcome data."],"url":"http://arxiv.org/abs/2402.09938v1","category":"stat.ME"}
{"created":"2024-02-15 13:39:34","title":"A Systematic Evaluation of Evolving Highly Nonlinear Boolean Functions in Odd Sizes","abstract":"Boolean functions are mathematical objects used in diverse applications. Different applications also have different requirements, making the research on Boolean functions very active. In the last 30 years, evolutionary algorithms have been shown to be a strong option for evolving Boolean functions in different sizes and with different properties. Still, most of those works consider similar settings and provide results that are mostly interesting from the evolutionary algorithm's perspective. This work considers the problem of evolving highly nonlinear Boolean functions in odd sizes. While the problem formulation sounds simple, the problem is remarkably difficult, and the related work is extremely scarce. We consider three solutions encodings and four Boolean function sizes and run a detailed experimental analysis. Our results show that the problem is challenging, and finding optimal solutions is impossible except for the smallest tested size. However, once we added local search to the evolutionary algorithm, we managed to find a Boolean function in nine inputs with nonlinearity 241, which, to our knowledge, had never been accomplished before with evolutionary algorithms.","sentences":["Boolean functions are mathematical objects used in diverse applications.","Different applications also have different requirements, making the research on Boolean functions very active.","In the last 30 years, evolutionary algorithms have been shown to be a strong option for evolving Boolean functions in different sizes and with different properties.","Still, most of those works consider similar settings and provide results that are mostly interesting from the evolutionary algorithm's perspective.","This work considers the problem of evolving highly nonlinear Boolean functions in odd sizes.","While the problem formulation sounds simple, the problem is remarkably difficult, and the related work is extremely scarce.","We consider three solutions encodings and four Boolean function sizes and run a detailed experimental analysis.","Our results show that the problem is challenging, and finding optimal solutions is impossible except for the smallest tested size.","However, once we added local search to the evolutionary algorithm, we managed to find a Boolean function in nine inputs with nonlinearity 241, which, to our knowledge, had never been accomplished before with evolutionary algorithms."],"url":"http://arxiv.org/abs/2402.09937v1","category":"cs.NE"}
{"created":"2024-02-15 11:08:23","title":"Explaining Kernel Clustering via Decision Trees","abstract":"Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees. However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data. In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means. We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpretable model.","sentences":["Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods.","Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees.","However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data.","In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means.","We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpretable model."],"url":"http://arxiv.org/abs/2402.09881v1","category":"cs.LG"}
{"created":"2024-02-15 10:31:59","title":"Fractional heat equation involving Hardy-Leray Potential","abstract":"In this paper we analyse the existence and non-existence of non-negative solutions to a non-local parabolic equation with a Hardy-Leray type potential. More precisely, we consider the problem $$ \\begin{cases} (w_t-\\Delta w)^s=\\frac{\\lambda}{|x|^{2s}} w+w^p +f, &\\text{ in }\\mathbb{R}^N\\times (0,+\\infty),\\\\ w(x,t)=0, &\\text{ in }\\mathbb{R}^N\\times (-\\infty,0], \\end{cases} $$ where $N> 2s$, $0<s<1$ and $0<\\lambda<\\Lambda_{N,s}$, the optimal constant in the fractional Hardy-Leray inequality. In particular we show the existence of a critical existence exponent $p_{+}(\\lambda, s)$ and of a Fujita-type exponent $F(\\lambda,s)$ such that the following holds:   - Let $p>p_+(\\lambda,s)$. Then there are not any non-negative supersolutions.   - Let $p<p_+(\\lambda,s)$. Then there exist local solutions while concerning global solutions we need to distinguish two cases:   - Let $ 1< p\\le F(\\lambda,s)$. Here we show that a weighted norm of any positive solution blows up in finite time.   - Let $F(\\lambda,s)<p<p_+(\\lambda,s)$. Here we prove the existence of global solutions under suitable hypotheses.","sentences":["In this paper we analyse the existence and non-existence of non-negative solutions to a non-local parabolic equation with a Hardy-Leray type potential.","More precisely, we consider the problem $$ \\begin{cases} (w_t-\\Delta w)^s=\\frac{\\lambda}{|x|^{2s}} w+w^p +f, &\\text{ in }\\mathbb{R}^N\\times (0,+\\infty),\\\\ w(x,t)=0, &\\text{ in }\\mathbb{R}^N\\times (-\\infty,0], \\end{cases} $$ where $N> 2s$, $0<s<1$ and $0<\\lambda<\\Lambda_{N,s}$, the optimal constant in the fractional Hardy-Leray inequality.","In particular we show the existence of a critical existence exponent $p_{+}(\\lambda, s)$ and of a Fujita-type exponent $F(\\lambda,s)$ such that the following holds:   - Let $p>p_+(\\lambda,s)$. Then there are not any non-negative supersolutions.   ","- Let $p<p_+(\\lambda,s)$. Then there exist local solutions while concerning global solutions we need to distinguish two cases:   - Let $ 1< p\\le F(\\lambda,","s)$. Here we show that a weighted norm of any positive solution blows up in finite time.   ","- Let $F(\\lambda,s)<p<p_+(\\lambda,","s)$.","Here we prove the existence of global solutions under suitable hypotheses."],"url":"http://arxiv.org/abs/2402.09862v1","category":"math.AP"}
{"created":"2024-02-15 10:11:28","title":"Recommendations for Baselines and Benchmarking Approximate Gaussian Processes","abstract":"Gaussian processes (GPs) are a mature and widely-used component of the ML toolbox. One of their desirable qualities is automatic hyperparameter selection, which allows for training without user intervention. However, in many realistic settings, approximations are typically needed, which typically do require tuning. We argue that this requirement for tuning complicates evaluation, which has led to a lack of a clear recommendations on which method should be used in which situation. To address this, we make recommendations for comparing GP approximations based on a specification of what a user should expect from a method. In addition, we develop a training procedure for the variational method of Titsias [2009] that leaves no choices to the user, and show that this is a strong baseline that meets our specification. We conclude that benchmarking according to our suggestions gives a clearer view of the current state of the field, and uncovers problems that are still open that future papers should address.","sentences":["Gaussian processes (GPs) are a mature and widely-used component of the ML toolbox.","One of their desirable qualities is automatic hyperparameter selection, which allows for training without user intervention.","However, in many realistic settings, approximations are typically needed, which typically do require tuning.","We argue that this requirement for tuning complicates evaluation, which has led to a lack of a clear recommendations on which method should be used in which situation.","To address this, we make recommendations for comparing GP approximations based on a specification of what a user should expect from a method.","In addition, we develop a training procedure for the variational method of Titsias [2009] that leaves no choices to the user, and show that this is a strong baseline that meets our specification.","We conclude that benchmarking according to our suggestions gives a clearer view of the current state of the field, and uncovers problems that are still open that future papers should address."],"url":"http://arxiv.org/abs/2402.09849v1","category":"cs.LG"}
{"created":"2024-02-15 10:07:50","title":"Modeling low-intensity ultrasound mechanotherapy impact on growing cancer stem cells","abstract":"Targeted therapeutic interventions utilizing low-inten\\-sity ultrasound (LIUS) exhibit substantial potential for hindering the proliferation of cancer stem cells. This investigation introduces a multiscale model and computational framework to comprehensively explore the therapeutic LIUS on poroelastic tumor dynamics, thereby unraveling the intricacies of mechanotransduction mechanisms at play. Our model includes both macroscopic timescales encompassing days and rapid timescales spanning from microseconds to seconds, facilitating an in-depth comprehension of tumor behavior. We unveil the discerning suppression or reorientation of cancer cell proliferation and migration, enhancing a notable redistribution of cellular phases and stresses within the tumor microenvironment. Our findings defy existing paradigms by elucidating the impact of LIUS on cancer stem cell behavior. This endeavor advances our fundamental understanding of mechanotransduction phenomena in the context of LIUS therapy, thus underscoring its promising as a targeted therapeutic modality for cancer treatment. Furthermore, our results make a substantial contribution to the broader scientific community by shedding light on the intricate interplay between mechanical forces, cellular responses, and the spatiotemporal evolution of tumors. These insights hold the promising to promote a new perspective for the future development of pioneering and highly efficacious therapeutic strategies for combating cancer in a personalized manner.","sentences":["Targeted therapeutic interventions utilizing low-inten\\-sity ultrasound (LIUS) exhibit substantial potential for hindering the proliferation of cancer stem cells.","This investigation introduces a multiscale model and computational framework to comprehensively explore the therapeutic LIUS on poroelastic tumor dynamics, thereby unraveling the intricacies of mechanotransduction mechanisms at play.","Our model includes both macroscopic timescales encompassing days and rapid timescales spanning from microseconds to seconds, facilitating an in-depth comprehension of tumor behavior.","We unveil the discerning suppression or reorientation of cancer cell proliferation and migration, enhancing a notable redistribution of cellular phases and stresses within the tumor microenvironment.","Our findings defy existing paradigms by elucidating the impact of LIUS on cancer stem cell behavior.","This endeavor advances our fundamental understanding of mechanotransduction phenomena in the context of LIUS therapy, thus underscoring its promising as a targeted therapeutic modality for cancer treatment.","Furthermore, our results make a substantial contribution to the broader scientific community by shedding light on the intricate interplay between mechanical forces, cellular responses, and the spatiotemporal evolution of tumors.","These insights hold the promising to promote a new perspective for the future development of pioneering and highly efficacious therapeutic strategies for combating cancer in a personalized manner."],"url":"http://arxiv.org/abs/2402.09847v1","category":"math.AP"}
{"created":"2024-02-15 09:33:47","title":"Hydrodynamic models of pulsation period evolution in R Hydrae","abstract":"Pulsation period evolution during the helium-shell flash in the Mira variable R Hya is investigated using consistent stellar evolution and non-linear stellar pulsation computations. The initial and time-dependent inner boundary conditions for the equations of radiation hydrodynamics describing non-linear stellar oscillations were determined using a grid of TP-AGB model sequences with initial masses on the main sequence $1.5M_\\odot\\le M_\\mathrm{ZAMS}\\le 5.0M_\\odot$ and the initial metallicity $Z=0.014$. The setup of initial conditions for hydrodynamic models corresponds to $\\approx 100$ yr prior to the maximum of the helium-shell luminosity and ensures that the stellar envelope of the evolution model is under both hydrostatic and thermal equilibrium. Solution of the equations of hydrodynamics allowed us to determine the temporal variation of the pulsation period $\\Pi(t)$ during $\\approx 500$~yr. Within this time interval R Hya is a fundamental mode pulsator. The period temporal dependencies $\\Pi(t)$ calculated for the AGB star models at the beginning of the third dredge-up phase and with masses $4.4M_\\odot\\le M\\le 4.5M_\\odot$ are in agreement with observational estimates of the period of R Hya obtained during last two centuries. The mean radius of R Hya pulsation models at the end of the XX century ($470 R_\\odot < \\bar{R} < 490 R_\\odot$) agrees with observational estimates obtained using the interferometric angular diameter measurements.","sentences":["Pulsation period evolution during the helium-shell flash in the Mira variable R Hya is investigated using consistent stellar evolution and non-linear stellar pulsation computations.","The initial and time-dependent inner boundary conditions for the equations of radiation hydrodynamics describing non-linear stellar oscillations were determined using a grid of TP-AGB model sequences with initial masses on the main sequence $1.5M_\\odot\\le M_\\mathrm{ZAMS}\\le 5.0M_\\odot$ and the initial metallicity $Z=0.014$. The setup of initial conditions for hydrodynamic models corresponds to $\\approx 100$ yr prior to the maximum of the helium-shell luminosity and ensures that the stellar envelope of the evolution model is under both hydrostatic and thermal equilibrium.","Solution of the equations of hydrodynamics allowed us to determine the temporal variation of the pulsation period $\\Pi(t)$ during $\\approx 500$~yr.","Within this time interval R Hya is a fundamental mode pulsator.","The period temporal dependencies $\\Pi(t)$ calculated for the AGB star models at the beginning of the third dredge-up phase and with masses $4.4M_\\odot\\le M\\le 4.5M_\\odot$ are in agreement with observational estimates of the period of R Hya obtained during last two centuries.","The mean radius of R Hya pulsation models at the end of the XX century ($470 R_\\odot < \\bar{R} < 490 R_\\odot$) agrees with observational estimates obtained using the interferometric angular diameter measurements."],"url":"http://arxiv.org/abs/2402.09819v1","category":"astro-ph.SR"}
{"created":"2024-02-15 09:18:18","title":"TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming","abstract":"Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON","sentences":["Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection.","However, their performance relies heavily on the availability and quality of training data.","There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts.","This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages.","Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text.","Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available.","Manual annotation of such data requires a lot of time, effort, and expertise.","In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework.","One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches.","TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages.","We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data.","Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text.","Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON"],"url":"http://arxiv.org/abs/2402.09811v1","category":"cs.CV"}
{"created":"2024-02-15 08:58:58","title":"Criterion collapse and loss distribution control","abstract":"In this work, we consider the notion of \"criterion collapse,\" in which optimization of one metric implies optimality in another, with a particular focus on conditions for collapse into error probability minimizers under a wide variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM) to non-monotonic criteria underlying recent ascent-descent algorithms explored in the literature (Flooding, SoftAD). We show how collapse in the context of losses with a Bernoulli distribution goes far beyond existing results for CVaR and DRO, then expand our scope to include surrogate losses, showing conditions where monotonic criteria such as tilted ERM cannot avoid collapse, whereas non-monotonic alternatives can.","sentences":["In this work, we consider the notion of \"criterion collapse,\" in which optimization of one metric implies optimality in another, with a particular focus on conditions for collapse into error probability minimizers under a wide variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM) to non-monotonic criteria underlying recent ascent-descent algorithms explored in the literature (Flooding, SoftAD).","We show how collapse in the context of losses with a Bernoulli distribution goes far beyond existing results for CVaR and DRO, then expand our scope to include surrogate losses, showing conditions where monotonic criteria such as tilted ERM cannot avoid collapse, whereas non-monotonic alternatives can."],"url":"http://arxiv.org/abs/2402.09802v1","category":"stat.ML"}
{"created":"2024-02-15 08:43:20","title":"Strong Hamel functions and symmetries","abstract":"A strong Hamel function is a Hamel function that is the geodesic derivative of some 0-homogeneous function. We prove that strong Hamel functions induce dual symmetries and dynamical symmetries and provide the conditions such that these symmetries are induced by strong Hamel functions. We show that projective deformations by strong Hamel functions preserve the $\\chi$-curvature and analyse the relationship with some other functions (Funk and weak Funk functions) preserving curvature tensors under projective deformations. In the flat case, any Hamel function is a strong Hamel function.","sentences":["A strong Hamel function is a Hamel function that is the geodesic derivative of some 0-homogeneous function.","We prove that strong Hamel functions induce dual symmetries and dynamical symmetries and provide the conditions such that these symmetries are induced by strong Hamel functions.","We show that projective deformations by strong Hamel functions preserve the $\\chi$-curvature and analyse the relationship with some other functions (Funk and weak Funk functions) preserving curvature tensors under projective deformations.","In the flat case, any Hamel function is a strong Hamel function."],"url":"http://arxiv.org/abs/2402.09791v1","category":"math.DG"}
{"created":"2024-02-15 08:31:13","title":"Near-infrared spectroscopic characterisation of Gaia ultra-cool dwarf candidates; Spectral types and peculiarities","abstract":"Context: The local census of very low-mass stars and brown dwarfs is crucial to improving our understanding of the stellar-substellar transition and their formation history. These objects, known as ultra-cool dwarfs (UCDs), are essential targets for searches of potentially habitable planets. However, their detection poses a challenge because of their low luminosity. The Gaia survey has identified numerous new UCD candidates thanks to its large survey and precise astrometry.   Aims: We aim to characterise 60 UCD candidates detected by Gaia in the solar neighbourhood with a spectroscopic follow-up to confirm that they are UCDs, as well as to identify peculiarities.   Methods: We acquired the near-infrared (NIR) spectra of 60 objects using the SOFI spectrograph between 0.93 and 2.5 microns (R$\\sim600$). We identified their spectral types using a template-matching method. Their binarity is studied using astrometry and spectral features.   Results: We confirm that 60 objects in the sample have ultra-cool dwarf spectral types close to those expected from astrometry. Their NIR spectra reveal that seven objects could host an unresolved coolest companion and seven UCDs share the same proper motions as other stars. The characterisation of these UCDs is part of a coordinated effort to improve our understanding of the Solar neighbourhood.","sentences":["Context: The local census of very low-mass stars and brown dwarfs is crucial to improving our understanding of the stellar-substellar transition and their formation history.","These objects, known as ultra-cool dwarfs (UCDs), are essential targets for searches of potentially habitable planets.","However, their detection poses a challenge because of their low luminosity.","The Gaia survey has identified numerous new UCD candidates thanks to its large survey and precise astrometry.   ","Aims:","We aim to characterise 60 UCD candidates detected by Gaia in the solar neighbourhood with a spectroscopic follow-up to confirm that they are UCDs, as well as to identify peculiarities.   ","Methods: We acquired the near-infrared (NIR) spectra of 60 objects using the SOFI spectrograph between 0.93 and 2.5 microns (R$\\sim600$).","We identified their spectral types using a template-matching method.","Their binarity is studied using astrometry and spectral features.   ","Results:","We confirm that 60 objects in the sample have ultra-cool dwarf spectral types close to those expected from astrometry.","Their NIR spectra reveal that seven objects could host an unresolved coolest companion and seven UCDs share the same proper motions as other stars.","The characterisation of these UCDs is part of a coordinated effort to improve our understanding of the Solar neighbourhood."],"url":"http://arxiv.org/abs/2402.09783v1","category":"astro-ph.SR"}
{"created":"2024-02-15 08:08:45","title":"Probing the inner Galactic Halo with blue horizontal branch stars: Gaia DR3 based catalogue with atmospheric and stellar parameters","abstract":"Context. Stars that are found on the blue horizontal-branch (BHB) have evolved from low-mass stars that have completed their core hydrogen burning main sequence stage and have undergone the helium flash at the end of their red-giant phase. The fact that their luminosity is virtually constant at all effective temperatures also makes them good standard candles. Aims. We provide a catalogue of BHB stars with stellar parameters that have been calculated from spectral energy distributions (SED), as constructed from multiple large-scale photometric surveys. In addition, we update our previous, Gaia Early Data Release 3 catalogue of BHB stars with parallax errors less than 20% by using the SED results to define the selection criteria. Methods. We selected a large dataset of Gaia Data Release 3 (DR3) objects based only on their position in the colour magnitude diagram, tangential velocity and parallax errors. Spectral energy distributions were then used to evaluate contamination levels in the dataset and derive optimised data quality acceptance constraints. This allowed us to extend the Gaia DR3 colour and absolute magnitude criteria further towards the extreme horizontal-branch. The level of contamination found using SED analysis was confirmed by acquiring spectra using the Ondrejov Echelle spectrograph attached to the Perek 2m telescope at the Astronomical Institute of the Czech Academy of Sciences. Results. We present a catalogue of 9,172 Galactic Halo BHB candidate stars with atmospheric and stellar parameters calculated from synthetic SEDs. We also present an extended Gaia DR3 based catalogue of 22,335 BHB candidate stars with a wider range of effective temperatures and Gaia DR3 parallax errors of less than 20%. This represents an increase of 33% compared to the our 2021 catalogue, with a contamination level of 10%.","sentences":["Context.","Stars that are found on the blue horizontal-branch (BHB) have evolved from low-mass stars that have completed their core hydrogen burning main sequence stage and have undergone the helium flash at the end of their red-giant phase.","The fact that their luminosity is virtually constant at all effective temperatures also makes them good standard candles.","Aims.","We provide a catalogue of BHB stars with stellar parameters that have been calculated from spectral energy distributions (SED), as constructed from multiple large-scale photometric surveys.","In addition, we update our previous, Gaia Early Data Release 3 catalogue of BHB stars with parallax errors less than 20% by using the SED results to define the selection criteria.","Methods.","We selected a large dataset of Gaia Data Release 3 (DR3) objects based only on their position in the colour magnitude diagram, tangential velocity and parallax errors.","Spectral energy distributions were then used to evaluate contamination levels in the dataset and derive optimised data quality acceptance constraints.","This allowed us to extend the Gaia DR3 colour and absolute magnitude criteria further towards the extreme horizontal-branch.","The level of contamination found using SED analysis was confirmed by acquiring spectra using the Ondrejov Echelle spectrograph attached to the Perek 2m telescope at the Astronomical Institute of the Czech Academy of Sciences.","Results.","We present a catalogue of 9,172 Galactic Halo BHB candidate stars with atmospheric and stellar parameters calculated from synthetic SEDs.","We also present an extended Gaia DR3 based catalogue of 22,335 BHB candidate stars with a wider range of effective temperatures and Gaia DR3 parallax errors of less than 20%.","This represents an increase of 33% compared to the our 2021 catalogue, with a contamination level of 10%."],"url":"http://arxiv.org/abs/2402.09779v1","category":"astro-ph.SR"}
{"created":"2024-02-15 07:26:19","title":"Quantifying Systematic Uncertainties in Experimental Physics: An Approximation Method","abstract":"In the domain of physics experiments, data fitting is a pivotal technique for extracting insights from both experimental and simulated datasets. This article presents an approximation method designed to estimate the systematic errors prevalent in data analyses. By applying our method to the Nab experiment, we compare our findings with simulation-derived results, thereby confirming the concordance of our approach with established simulation outcomes. This corroboration highlights the versatility of our method as a good tool for validating simulation results across various experimental contexts.","sentences":["In the domain of physics experiments, data fitting is a pivotal technique for extracting insights from both experimental and simulated datasets.","This article presents an approximation method designed to estimate the systematic errors prevalent in data analyses.","By applying our method to the Nab experiment, we compare our findings with simulation-derived results, thereby confirming the concordance of our approach with established simulation outcomes.","This corroboration highlights the versatility of our method as a good tool for validating simulation results across various experimental contexts."],"url":"http://arxiv.org/abs/2402.09763v1","category":"physics.data-an"}
{"created":"2024-02-15 07:16:50","title":"Extrapolation-Aware Nonparametric Statistical Inference","abstract":"We define extrapolation as any type of statistical inference on a conditional function (e.g., a conditional expectation or conditional quantile) evaluated outside of the support of the conditioning variable. This type of extrapolation occurs in many data analysis applications and can invalidate the resulting conclusions if not taken into account. While extrapolating is straightforward in parametric models, it becomes challenging in nonparametric models. In this work, we extend the nonparametric statistical model to explicitly allow for extrapolation and introduce a class of extrapolation assumptions that can be combined with existing inference techniques to draw extrapolation-aware conclusions. The proposed class of extrapolation assumptions stipulate that the conditional function attains its minimal and maximal directional derivative, in each direction, within the observed support. We illustrate how the framework applies to several statistical applications including prediction and uncertainty quantification. We furthermore propose a consistent estimation procedure that can be used to adjust existing nonparametric estimates to account for extrapolation by providing lower and upper extrapolation bounds. The procedure is empirically evaluated on both simulated and real-world data.","sentences":["We define extrapolation as any type of statistical inference on a conditional function (e.g., a conditional expectation or conditional quantile) evaluated outside of the support of the conditioning variable.","This type of extrapolation occurs in many data analysis applications and can invalidate the resulting conclusions if not taken into account.","While extrapolating is straightforward in parametric models, it becomes challenging in nonparametric models.","In this work, we extend the nonparametric statistical model to explicitly allow for extrapolation and introduce a class of extrapolation assumptions that can be combined with existing inference techniques to draw extrapolation-aware conclusions.","The proposed class of extrapolation assumptions stipulate that the conditional function attains its minimal and maximal directional derivative, in each direction, within the observed support.","We illustrate how the framework applies to several statistical applications including prediction and uncertainty quantification.","We furthermore propose a consistent estimation procedure that can be used to adjust existing nonparametric estimates to account for extrapolation by providing lower and upper extrapolation bounds.","The procedure is empirically evaluated on both simulated and real-world data."],"url":"http://arxiv.org/abs/2402.09758v1","category":"stat.ME"}
{"created":"2024-02-15 06:36:07","title":"QuRating: Selecting High-Quality Data for Training Language Models","abstract":"Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity, as selecting only the highest-rated documents leads to poor results. When we sample using quality ratings as logits over documents, our models achieve lower perplexity and stronger in-context learning performance than baselines. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.","sentences":["Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics.","We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive.","In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value.","We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly.","We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria.","In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data.","We find that it is important to balance quality and diversity, as selecting only the highest-rated documents leads to poor results.","When we sample using quality ratings as logits over documents, our models achieve lower perplexity and stronger in-context learning performance than baselines.","Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset.","We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications."],"url":"http://arxiv.org/abs/2402.09739v1","category":"cs.CL"}
{"created":"2024-02-15 06:14:55","title":"Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States","abstract":"Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates. To do this, we introduce an experimental framework which allows examining LLM's hidden states in different hallucination situations. Building upon this framework, we conduct a series of experiments with language models in the LLaMA family (Touvron et al., 2023). Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one. We then apply various model interpretation techniques to help understand and explain the findings better. Moreover, informed by the empirical observations, we show great potential of using the guidance derived from LLM's hidden representation space to mitigate hallucination. We believe this work provides insights into how LLMs produce hallucinated answers and how to make them occur less often.","sentences":["Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination.","This research aims to see if, how, and to what extent LLMs are aware of hallucination.","More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates.","To do this, we introduce an experimental framework which allows examining LLM's hidden states in different hallucination situations.","Building upon this framework, we conduct a series of experiments with language models in the LLaMA family (Touvron et al., 2023).","Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one.","We then apply various model interpretation techniques to help understand and explain the findings better.","Moreover, informed by the empirical observations, we show great potential of using the guidance derived from LLM's hidden representation space to mitigate hallucination.","We believe this work provides insights into how LLMs produce hallucinated answers and how to make them occur less often."],"url":"http://arxiv.org/abs/2402.09733v1","category":"cs.CL"}
{"created":"2024-02-15 05:33:58","title":"Region Feature Descriptor Adapted to High Affine Transformations","abstract":"To address the issue of feature descriptors being ineffective in representing grayscale feature information when images undergo high affine transformations, leading to a rapid decline in feature matching accuracy, this paper proposes a region feature descriptor based on simulating affine transformations using classification. The proposed method initially categorizes images with different affine degrees to simulate affine transformations and generate a new set of images. Subsequently, it calculates neighborhood information for feature points on this new image set. Finally, the descriptor is generated by combining the grayscale histogram of the maximum stable extremal region to which the feature point belongs and the normalized position relative to the grayscale centroid of the feature point's region. Experimental results, comparing feature matching metrics under affine transformation scenarios, demonstrate that the proposed descriptor exhibits higher precision and robustness compared to existing classical descriptors. Additionally, it shows robustness when integrated with other descriptors.","sentences":["To address the issue of feature descriptors being ineffective in representing grayscale feature information when images undergo high affine transformations, leading to a rapid decline in feature matching accuracy, this paper proposes a region feature descriptor based on simulating affine transformations using classification.","The proposed method initially categorizes images with different affine degrees to simulate affine transformations and generate a new set of images.","Subsequently, it calculates neighborhood information for feature points on this new image set.","Finally, the descriptor is generated by combining the grayscale histogram of the maximum stable extremal region to which the feature point belongs and the normalized position relative to the grayscale centroid of the feature point's region.","Experimental results, comparing feature matching metrics under affine transformation scenarios, demonstrate that the proposed descriptor exhibits higher precision and robustness compared to existing classical descriptors.","Additionally, it shows robustness when integrated with other descriptors."],"url":"http://arxiv.org/abs/2402.09724v1","category":"cs.CV"}
{"created":"2024-02-15 05:05:41","title":"ME-ViT: A Single-Load Memory-Efficient FPGA Accelerator for Vision Transformers","abstract":"Vision Transformers (ViTs) have emerged as a state-of-the-art solution for object classification tasks. However, their computational demands and high parameter count make them unsuitable for real-time inference, prompting the need for efficient hardware implementations. Existing hardware accelerators for ViTs suffer from frequent off-chip memory access, restricting the achievable throughput by memory bandwidth. In devices with a high compute-to-communication ratio (e.g., edge FPGAs with limited bandwidth), off-chip memory access imposes a severe bottleneck on overall throughput. This work proposes ME-ViT, a novel \\underline{M}emory \\underline{E}fficient FPGA accelerator for \\underline{ViT} inference that minimizes memory traffic. We propose a \\textit{single-load policy} in designing ME-ViT: model parameters are only loaded once, intermediate results are stored on-chip, and all operations are implemented in a single processing element. To achieve this goal, we design a memory-efficient processing element (ME-PE), which processes multiple key operations of ViT inference on the same architecture through the reuse of \\textit{multi-purpose buffers}. We also integrate the Softmax and LayerNorm functions into the ME-PE, minimizing stalls between matrix multiplications. We evaluate ME-ViT on systolic array sizes of 32 and 16, achieving up to a 9.22$\\times$ and 17.89$\\times$ overall improvement in memory bandwidth, and a 2.16$\\times$ improvement in throughput per DSP for both designs over state-of-the-art ViT accelerators on FPGA. ME-ViT achieves a power efficiency improvement of up to 4.00$\\times$ (1.03$\\times$) over a GPU (FPGA) baseline. ME-ViT enables up to 5 ME-PE instantiations on a Xilinx Alveo U200, achieving a 5.10$\\times$ improvement in throughput over the state-of-the art FPGA baseline, and a 5.85$\\times$ (1.51$\\times$) improvement in power efficiency over the GPU (FPGA) baseline.","sentences":["Vision Transformers (ViTs) have emerged as a state-of-the-art solution for object classification tasks.","However, their computational demands and high parameter count make them unsuitable for real-time inference, prompting the need for efficient hardware implementations.","Existing hardware accelerators for ViTs suffer from frequent off-chip memory access, restricting the achievable throughput by memory bandwidth.","In devices with a high compute-to-communication ratio (e.g., edge FPGAs with limited bandwidth), off-chip memory access imposes a severe bottleneck on overall throughput.","This work proposes ME-ViT, a novel \\underline{M}emory \\underline{E}fficient FPGA accelerator for \\underline{ViT} inference that minimizes memory traffic.","We propose a \\textit{single-load policy} in designing ME-ViT: model parameters are only loaded once, intermediate results are stored on-chip, and all operations are implemented in a single processing element.","To achieve this goal, we design a memory-efficient processing element (ME-PE), which processes multiple key operations of ViT inference on the same architecture through the reuse of \\textit{multi-purpose buffers}.","We also integrate the Softmax and LayerNorm functions into the ME-PE, minimizing stalls between matrix multiplications.","We evaluate ME-ViT on systolic array sizes of 32 and 16, achieving up to a 9.22$\\times$ and 17.89$\\times$ overall improvement in memory bandwidth, and a 2.16$\\times$ improvement in throughput per DSP for both designs over state-of-the-art ViT accelerators on FPGA.","ME-ViT achieves a power efficiency improvement of up to 4.00$\\times$ (1.03$\\times$) over a GPU (FPGA) baseline.","ME-ViT enables up to 5 ME-PE instantiations on a Xilinx Alveo U200, achieving a 5.10$\\times$ improvement in throughput over the state-of-the art FPGA baseline, and a 5.85$\\times$ (1.51$\\times$) improvement in power efficiency over the GPU (FPGA) baseline."],"url":"http://arxiv.org/abs/2402.09709v1","category":"eess.IV"}
{"created":"2024-02-15 04:58:35","title":"On the adversarial robustness of Locality-Sensitive Hashing in Hamming space","abstract":"Locality-sensitive hashing~[Indyk,Motwani'98] is a classical data structure for approximate nearest neighbor search. It allows, after a close to linear time preprocessing of the input dataset, to find an approximately nearest neighbor of any fixed query in sublinear time in the dataset size. The resulting data structure is randomized and succeeds with high probability for every fixed query.   In many modern applications of nearest neighbor search the queries are chosen adaptively. In this paper, we study the robustness of the locality-sensitive hashing to adaptive queries in Hamming space. We present a simple adversary that can, under mild assumptions on the initial point set, provably find a query to the approximate near neighbor search data structure that the data structure fails on. Crucially, our adaptive algorithm finds the hard query exponentially faster than random sampling.","sentences":["Locality-sensitive hashing~[Indyk,Motwani'98] is a classical data structure for approximate nearest neighbor search.","It allows, after a close to linear time preprocessing of the input dataset, to find an approximately nearest neighbor of any fixed query in sublinear time in the dataset size.","The resulting data structure is randomized and succeeds with high probability for every fixed query.   ","In many modern applications of nearest neighbor search the queries are chosen adaptively.","In this paper, we study the robustness of the locality-sensitive hashing to adaptive queries in Hamming space.","We present a simple adversary that can, under mild assumptions on the initial point set, provably find a query to the approximate near neighbor search data structure that the data structure fails on.","Crucially, our adaptive algorithm finds the hard query exponentially faster than random sampling."],"url":"http://arxiv.org/abs/2402.09707v1","category":"cs.DS"}
{"created":"2024-02-15 04:36:52","title":"Sparse and Faithful Explanations Without Sparse Models","abstract":"Even if a model is not globally sparse, it is possible for decisions made from that model to be accurately and faithfully described by a small number of features. For instance, an application for a large loan might be denied to someone because they have no credit history, which overwhelms any evidence towards their creditworthiness. In this work, we introduce the Sparse Explanation Value (SEV), a new way of measuring sparsity in machine learning models. In the loan denial example above, the SEV is 1 because only one factor is needed to explain why the loan was denied. SEV is a measure of decision sparsity rather than overall model sparsity, and we are able to show that many machine learning models -- even if they are not sparse -- actually have low decision sparsity, as measured by SEV. SEV is defined using movements over a hypercube, allowing SEV to be defined consistently over various model classes, with movement restrictions reflecting real-world constraints. We proposed the algorithms that reduce SEV without sacrificing accuracy, providing sparse and completely faithful explanations, even without globally sparse models.","sentences":["Even if a model is not globally sparse, it is possible for decisions made from that model to be accurately and faithfully described by a small number of features.","For instance, an application for a large loan might be denied to someone because they have no credit history, which overwhelms any evidence towards their creditworthiness.","In this work, we introduce the Sparse Explanation Value (SEV), a new way of measuring sparsity in machine learning models.","In the loan denial example above, the SEV is 1 because only one factor is needed to explain why the loan was denied.","SEV is a measure of decision sparsity rather than overall model sparsity, and we are able to show that many machine learning models -- even if they are not sparse -- actually have low decision sparsity, as measured by SEV.","SEV is defined using movements over a hypercube, allowing SEV to be defined consistently over various model classes, with movement restrictions reflecting real-world constraints.","We proposed the algorithms that reduce SEV without sacrificing accuracy, providing sparse and completely faithful explanations, even without globally sparse models."],"url":"http://arxiv.org/abs/2402.09702v1","category":"cs.LG"}
{"created":"2024-02-15 02:27:23","title":"EntailE: Introducing Textual Entailment in Commonsense Knowledge Graph Completion","abstract":"Commonsense knowledge graph completion is a new challenge for commonsense knowledge graph construction and application. In contrast to factual knowledge graphs such as Freebase and YAGO, commonsense knowledge graphs (CSKGs; e.g., ConceptNet) utilize free-form text to represent named entities, short phrases, and events as their nodes. Such a loose structure results in large and sparse CSKGs, which makes the semantic understanding of these nodes more critical for learning rich commonsense knowledge graph embedding. While current methods leverage semantic similarities to increase the graph density, the semantic plausibility of the nodes and their relations are under-explored. Previous works adopt conceptual abstraction to improve the consistency of modeling (event) plausibility, but they are not scalable enough and still suffer from data sparsity. In this paper, we propose to adopt textual entailment to find implicit entailment relations between CSKG nodes, to effectively densify the subgraph connecting nodes within the same conceptual class, which indicates a similar level of plausibility. Each node in CSKG finds its top entailed nodes using a finetuned transformer over natural language inference (NLI) tasks, which sufficiently capture textual entailment signals. The entailment relation between these nodes are further utilized to: 1) build new connections between source triplets and entailed nodes to densify the sparse CSKGs; 2) enrich the generalization ability of node representations by comparing the node embeddings with a contrastive loss. Experiments on two standard CSKGs demonstrate that our proposed framework EntailE can improve the performance of CSKG completion tasks under both transductive and inductive settings.","sentences":["Commonsense knowledge graph completion is a new challenge for commonsense knowledge graph construction and application.","In contrast to factual knowledge graphs such as Freebase and YAGO, commonsense knowledge graphs (CSKGs; e.g., ConceptNet) utilize free-form text to represent named entities, short phrases, and events as their nodes.","Such a loose structure results in large and sparse CSKGs, which makes the semantic understanding of these nodes more critical for learning rich commonsense knowledge graph embedding.","While current methods leverage semantic similarities to increase the graph density, the semantic plausibility of the nodes and their relations are under-explored.","Previous works adopt conceptual abstraction to improve the consistency of modeling (event) plausibility, but they are not scalable enough and still suffer from data sparsity.","In this paper, we propose to adopt textual entailment to find implicit entailment relations between CSKG nodes, to effectively densify the subgraph connecting nodes within the same conceptual class, which indicates a similar level of plausibility.","Each node in CSKG finds its top entailed nodes using a finetuned transformer over natural language inference (NLI) tasks, which sufficiently capture textual entailment signals.","The entailment relation between these nodes are further utilized to: 1) build new connections between source triplets and entailed nodes to densify the sparse CSKGs; 2) enrich the generalization ability of node representations by comparing the node embeddings with a contrastive loss.","Experiments on two standard CSKGs demonstrate that our proposed framework EntailE can improve the performance of CSKG completion tasks under both transductive and inductive settings."],"url":"http://arxiv.org/abs/2402.09666v1","category":"cs.CL"}
{"created":"2024-02-15 01:50:46","title":"Digital versus Analog Transmissions for Federated Learning over Wireless Networks","abstract":"In this paper, we quantitatively compare these two effective communication schemes, i.e., digital and analog ones, for wireless federated learning (FL) over resource-constrained networks, highlighting their essential differences as well as their respective application scenarios. We first examine both digital and analog transmission methods, together with a unified and fair comparison scheme under practical constraints. A universal convergence analysis under various imperfections is established for FL performance evaluation in wireless networks. These analytical results reveal that the fundamental difference between the two paradigms lies in whether communication and computation are jointly designed or not. The digital schemes decouple the communication design from specific FL tasks, making it difficult to support simultaneous uplink transmission of massive devices with limited bandwidth. In contrast, the analog communication allows over-the-air computation (AirComp), thus achieving efficient spectrum utilization. However, computation-oriented analog transmission reduces power efficiency, and its performance is sensitive to computational errors. Finally, numerical simulations are conducted to verify these theoretical observations.","sentences":["In this paper, we quantitatively compare these two effective communication schemes, i.e., digital and analog ones, for wireless federated learning (FL) over resource-constrained networks, highlighting their essential differences as well as their respective application scenarios.","We first examine both digital and analog transmission methods, together with a unified and fair comparison scheme under practical constraints.","A universal convergence analysis under various imperfections is established for FL performance evaluation in wireless networks.","These analytical results reveal that the fundamental difference between the two paradigms lies in whether communication and computation are jointly designed or not.","The digital schemes decouple the communication design from specific FL tasks, making it difficult to support simultaneous uplink transmission of massive devices with limited bandwidth.","In contrast, the analog communication allows over-the-air computation (AirComp), thus achieving efficient spectrum utilization.","However, computation-oriented analog transmission reduces power efficiency, and its performance is sensitive to computational errors.","Finally, numerical simulations are conducted to verify these theoretical observations."],"url":"http://arxiv.org/abs/2402.09657v1","category":"cs.IT"}
{"created":"2024-02-15 01:28:18","title":"Practitioners' Challenges and Perceptions of CI Build Failure Predictions at Atlassian","abstract":"Continuous Integration (CI) build failures could significantly impact the software development process and teams, such as delaying the release of new features and reducing developers' productivity. In this work, we report on an empirical study that investigates CI build failures throughout product development at Atlassian. Our quantitative analysis found that the repository dimension is the key factor influencing CI build failures. In addition, our qualitative survey revealed that Atlassian developers perceive CI build failures as challenging issues in practice. Furthermore, we found that the CI build prediction can not only provide proactive insight into CI build failures but also facilitate the team's decision-making. Our study sheds light on the challenges and expectations involved in integrating CI build prediction tools into the Bitbucket environment, providing valuable insights for enhancing CI processes.","sentences":["Continuous Integration (CI) build failures could significantly impact the software development process and teams, such as delaying the release of new features and reducing developers' productivity.","In this work, we report on an empirical study that investigates CI build failures throughout product development at Atlassian.","Our quantitative analysis found that the repository dimension is the key factor influencing CI build failures.","In addition, our qualitative survey revealed that Atlassian developers perceive CI build failures as challenging issues in practice.","Furthermore, we found that the CI build prediction can not only provide proactive insight into CI build failures but also facilitate the team's decision-making.","Our study sheds light on the challenges and expectations involved in integrating CI build prediction tools into the Bitbucket environment, providing valuable insights for enhancing CI processes."],"url":"http://arxiv.org/abs/2402.09651v1","category":"cs.SE"}
{"created":"2024-02-15 00:25:29","title":"Stimulated Secondary Emission of Single Photon Avalanche Diodes","abstract":"Large-area next-generation physics experiments rely on using Silicon Photo-Multiplier (SiPM) devices to detect single photons, which trigger charge avalanches. The noise mechanism of external cross-talk occurs when secondary photons produced during a charge avalanche escape from an SiPM and trigger other devices within a detector system. This work presents measured spectra of the secondary photons emitted from the Hamamatsu VUV4 and Fondazione Bruno Kessler VUV-HD3 SiPMs stimulated by laser light, near operational voltages. The work describes the Microscope for the Injection and Emission of Light (MIEL) setup, which is an experimental apparatus constructed for this purpose. Measurements have been performed at a range of over-voltage values and temperatures from 86~K to 293~K. The number of photons produced per avalanche at the source are calculated from the measured spectra and determined to be 40$\\pm$9 and 61$\\pm$11 photons produced per avalanche for the VUV4 and VUV-HD3 respectively at 4 volts over-voltage. No significant temperature dependence is observed within the measurement uncertainties. The overall number of photons emitted per avalanche from each SiPM device are also reported.","sentences":["Large-area next-generation physics experiments rely on using Silicon Photo-Multiplier (SiPM) devices to detect single photons, which trigger charge avalanches.","The noise mechanism of external cross-talk occurs when secondary photons produced during a charge avalanche escape from an SiPM and trigger other devices within a detector system.","This work presents measured spectra of the secondary photons emitted from the Hamamatsu VUV4 and Fondazione Bruno Kessler VUV-HD3 SiPMs stimulated by laser light, near operational voltages.","The work describes the Microscope for the Injection and Emission of Light (MIEL) setup, which is an experimental apparatus constructed for this purpose.","Measurements have been performed at a range of over-voltage values and temperatures from 86~K to 293~K. The number of photons produced per avalanche at the source are calculated from the measured spectra and determined to be 40$\\pm$9 and 61$\\pm$11 photons produced per avalanche for the VUV4 and VUV-HD3 respectively at 4 volts over-voltage.","No significant temperature dependence is observed within the measurement uncertainties.","The overall number of photons emitted per avalanche from each SiPM device are also reported."],"url":"http://arxiv.org/abs/2402.09634v1","category":"physics.ins-det"}
{"created":"2024-02-15 00:21:37","title":"Coronagraphic observations of Si X 1430 nm acquired by DKIST/Cryo-NIRSP with methods for telluric absorption correction","abstract":"We report commissioning observations of the Si X 1430 nm solar coronal line observed coronagraphically with the Cryogenic Near-Infrared Spectropolarimeter (Cryo-NIRSP) at the National Science Foundation's Daniel K. Inouye Solar Telescope (DKIST). These are the first known spatially resolved observations of this spectral line, which has strong potential as a coronal magnetic field diagnostic. The observations target a complex active region located on the solar northeast limb on 4 March 2022. We present a first analysis of this data, which extracts the spectral line properties through a careful treatment of the variable atmospheric transmission that is known to impact this spectral window. Rastered images are created and compared with EUV observations from the SDO/AIA instrument. A method for estimating the electron density from the Si X observations is then demonstrated that makes use of the forbidden line's density-sensitive emissivity and an emission-measure analysis of the SDO/AIA bandpass observations. In addition, we derive an effective temperature and non-thermal line width across the region. This study informs the calibration approaches required for more routine observations of this promising diagnostic line.","sentences":["We report commissioning observations of the Si X 1430 nm solar coronal line observed coronagraphically with the Cryogenic Near-Infrared Spectropolarimeter (Cryo-NIRSP) at the National Science Foundation's Daniel K. Inouye Solar Telescope (DKIST).","These are the first known spatially resolved observations of this spectral line, which has strong potential as a coronal magnetic field diagnostic.","The observations target a complex active region located on the solar northeast limb on 4 March 2022.","We present a first analysis of this data, which extracts the spectral line properties through a careful treatment of the variable atmospheric transmission that is known to impact this spectral window.","Rastered images are created and compared with EUV observations from the SDO/AIA instrument.","A method for estimating the electron density from the Si X observations is then demonstrated that makes use of the forbidden line's density-sensitive emissivity and an emission-measure analysis of the SDO/AIA bandpass observations.","In addition, we derive an effective temperature and non-thermal line width across the region.","This study informs the calibration approaches required for more routine observations of this promising diagnostic line."],"url":"http://arxiv.org/abs/2402.09632v1","category":"astro-ph.SR"}
{"created":"2024-02-15 00:20:30","title":"MiMiC: Minimally Modified Counterfactuals in the Representation Space","abstract":"Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that enables controlled generation. We demonstrate the effectiveness of the proposed approaches in mitigating bias in multiclass classification and in reducing the generation of toxic language, outperforming strong baselines.","sentences":["Language models often exhibit undesirable behaviors, such as gender bias or toxic language.","Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior.","We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   ","We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic'').","This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization.","We further build on this technique and derive a nonlinear intervention that enables controlled generation.","We demonstrate the effectiveness of the proposed approaches in mitigating bias in multiclass classification and in reducing the generation of toxic language, outperforming strong baselines."],"url":"http://arxiv.org/abs/2402.09631v1","category":"cs.LG"}
{"created":"2024-02-15 00:03:36","title":"Gap theorems for complete self-shrinkers of $r$-mean curvature flows","abstract":"In this paper, we prove gap results for complete self-shrinkers of the $r$-mean curvature flow involving a modified second fundamental form. These results extend previous results for self-shrinkers of the mean curvature flow due to Cao-Li and Cheng-Peng. To prove our results we show that, under suitable curvature bounds, proper self-shrinkers are parabolic for a certain second-order differential operator which generalizes the drifted Laplacian and, even if is not proper, this differential operator satisfies an Omori-Yau type maximum principle.","sentences":["In this paper, we prove gap results for complete self-shrinkers of the $r$-mean curvature flow involving a modified second fundamental form.","These results extend previous results for self-shrinkers of the mean curvature flow due to Cao-Li and Cheng-Peng.","To prove our results we show that, under suitable curvature bounds, proper self-shrinkers are parabolic for a certain second-order differential operator which generalizes the drifted Laplacian and, even if is not proper, this differential operator satisfies an Omori-Yau type maximum principle."],"url":"http://arxiv.org/abs/2402.09627v1","category":"math.DG"}
{"created":"2024-02-14 23:57:29","title":"Hitting times in the stochastic block model","abstract":"Given a large connected graph $G=(V,E)$, and two vertices $w,\\neq v$, let $T_{w,v}$ be the first hitting time to $v$ starting from $w$ for the simple random walk on $G$. We prove a general theorem that guarantees, under some assumptions on $G$, to approximate $\\mathbb E[T_{w,v}]$ up to $o(1)$ terms. As a corollary, we derive explicit formulas for the stochastic block model with two communities and connectivity parameters $p$ and $q$, and show that the average hitting times, for fixed $v$ and as $w$ varies, concentrates around four possible values. The proof is purely probabilistic and uses a coupling argument.","sentences":["Given a large connected graph $G=(V,E)$, and two vertices $w,\\neq v$, let $T_{w,v}$ be the first hitting time to $v$ starting from $w$ for the simple random walk on $G$. We prove a general theorem that guarantees, under some assumptions on $G$, to approximate $\\mathbb E[T_{w,v}]$ up to $o(1)$ terms.","As a corollary, we derive explicit formulas for the stochastic block model with two communities and connectivity parameters $p$ and $q$, and show that the average hitting times, for fixed $v$ and as $w$ varies, concentrates around four possible values.","The proof is purely probabilistic and uses a coupling argument."],"url":"http://arxiv.org/abs/2402.09624v1","category":"math.PR"}
{"created":"2024-02-14 23:57:19","title":"Conformalized Adaptive Forecasting of Heterogeneous Trajectories","abstract":"This paper presents a new conformal method for generating simultaneous forecasting bands guaranteed to cover the entire path of a new random trajectory with sufficiently high probability. Prompted by the need for dependable uncertainty estimates in motion planning applications where the behavior of diverse objects may be more or less unpredictable, we blend different techniques from online conformal prediction of single and multiple time series, as well as ideas for addressing heteroscedasticity in regression. This solution is both principled, providing precise finite-sample guarantees, and effective, often leading to more informative predictions than prior methods.","sentences":["This paper presents a new conformal method for generating simultaneous forecasting bands guaranteed to cover the entire path of a new random trajectory with sufficiently high probability.","Prompted by the need for dependable uncertainty estimates in motion planning applications where the behavior of diverse objects may be more or less unpredictable, we blend different techniques from online conformal prediction of single and multiple time series, as well as ideas for addressing heteroscedasticity in regression.","This solution is both principled, providing precise finite-sample guarantees, and effective, often leading to more informative predictions than prior methods."],"url":"http://arxiv.org/abs/2402.09623v1","category":"stat.ML"}
{"created":"2024-02-14 23:56:52","title":"Advanced fuel fusion, phase space engineering, and structure-preserving geometric algorithms","abstract":"Non-thermal advanced fuel fusion trades the requirement of a large amount of recirculating tritium in the system for that of large recirculating power. Phase space engineering technologies utilizing externally injected electromagnetic fields can be applied to meet the challenge of maintaining non-thermal particle distributions at a reasonable cost. The physical processes of the phase space engineering are studied from a theoretical and algorithmic perspective. It is emphasized that the operational space of phase space engineering is limited by the underpinning symplectic dynamics of charged particles. The phase space incompressibility according to the Liouville theorem is just one of many constraints, and Gromov's non-squeezing theorem determines the minimum footprints of the charged particles on every conjugate phase space plane. In this sense and level of sophistication, the mathematical abstraction of phase space engineering is symplectic topology. To simulate the processes of phase space engineering, such as the Maxwell demon and electromagnetic energy extraction, and to accurately calculate the minimum footprints of charged particles, recently developed structure-preserving geometric algorithms can be used. The family of algorithms conserves exactly, on discretized spacetime, symplecticity and thus incompressibility, non-squeezability, and symplectic capacities. The algorithms apply to the dynamics of charged particles under the influence of external electromagnetic fields as well as the charged particle-electromagnetic field system governed by the Vlasov-Maxwell equations.","sentences":["Non-thermal advanced fuel fusion trades the requirement of a large amount of recirculating tritium in the system for that of large recirculating power.","Phase space engineering technologies utilizing externally injected electromagnetic fields can be applied to meet the challenge of maintaining non-thermal particle distributions at a reasonable cost.","The physical processes of the phase space engineering are studied from a theoretical and algorithmic perspective.","It is emphasized that the operational space of phase space engineering is limited by the underpinning symplectic dynamics of charged particles.","The phase space incompressibility according to the Liouville theorem is just one of many constraints, and Gromov's non-squeezing theorem determines the minimum footprints of the charged particles on every conjugate phase space plane.","In this sense and level of sophistication, the mathematical abstraction of phase space engineering is symplectic topology.","To simulate the processes of phase space engineering, such as the Maxwell demon and electromagnetic energy extraction, and to accurately calculate the minimum footprints of charged particles, recently developed structure-preserving geometric algorithms can be used.","The family of algorithms conserves exactly, on discretized spacetime, symplecticity and thus incompressibility, non-squeezability, and symplectic capacities.","The algorithms apply to the dynamics of charged particles under the influence of external electromagnetic fields as well as the charged particle-electromagnetic field system governed by the Vlasov-Maxwell equations."],"url":"http://arxiv.org/abs/2402.09622v1","category":"physics.plasm-ph"}
{"created":"2024-02-14 23:24:01","title":"Dynamic Cooperative MAC Optimization in RSU-Enhanced VANETs: A Distributed Approach","abstract":"This paper presents an optimization approach for cooperative Medium Access Control (MAC) techniques in Vehicular Ad Hoc Networks (VANETs) equipped with Roadside Unit (RSU) to enhance network throughput. Our method employs a distributed cooperative MAC scheme based on Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) protocol, featuring selective RSU probing and adaptive transmission. It utilizes a dual timescale channel access framework, with a ``large-scale'' phase accounting for gradual changes in vehicle locations and a ``small-scale'' phase adapting to rapid channel fluctuations. We propose the RSU Probing and Cooperative Access (RPCA) strategy, a two-stage approach based on dynamic inter-vehicle distances from the RSU. Using optimal sequential planned decision theory, we rigorously prove its optimality in maximizing average system throughput per large-scale phase. For practical implementation in VANETs, we develop a distributed MAC algorithm with periodic location updates. It adjusts thresholds based on inter-vehicle and vehicle-RSU distances during the large-scale phase and accesses channels following the RPCA strategy with updated thresholds during the small-scale phase. Simulation results confirm the effectiveness and efficiency of our algorithm.","sentences":["This paper presents an optimization approach for cooperative Medium Access Control (MAC) techniques in Vehicular Ad Hoc Networks (VANETs) equipped with Roadside Unit (RSU) to enhance network throughput.","Our method employs a distributed cooperative MAC scheme based on Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) protocol, featuring selective RSU probing and adaptive transmission.","It utilizes a dual timescale channel access framework, with a ``large-scale'' phase accounting for gradual changes in vehicle locations and a ``small-scale'' phase adapting to rapid channel fluctuations.","We propose the RSU Probing and Cooperative Access (RPCA) strategy, a two-stage approach based on dynamic inter-vehicle distances from the RSU.","Using optimal sequential planned decision theory, we rigorously prove its optimality in maximizing average system throughput per large-scale phase.","For practical implementation in VANETs, we develop a distributed MAC algorithm with periodic location updates.","It adjusts thresholds based on inter-vehicle and vehicle-RSU distances during the large-scale phase and accesses channels following the RPCA strategy with updated thresholds during the small-scale phase.","Simulation results confirm the effectiveness and efficiency of our algorithm."],"url":"http://arxiv.org/abs/2402.09619v1","category":"eess.SP"}
{"created":"2024-02-14 22:56:19","title":"Spin-orbit torque in single-molecule junctions from ab initio","abstract":"The use of electric fields applied across magnetic heterojunctions that lack spatial inversion symmetry has been previously proposed as a non-magnetic mean of controlling localized magnetic moments through spin-orbit torques (SOT). The implementation of this concept at the single-molecule level has remained a challenge, however. Here, we present first-principle calculations of SOT in a single-molecule junction under bias and beyond linear response. Employing a self-consistency scheme invoking density functional theory and non-equilibrium Green's function theory, we compute the current-induced SOT. Responding to this torque, a localized magnetic moment can tilt. Within the linear regime our quantitative estimates for the SOT in single-molecule junctions yield values similar to those known for magnetic interfaces. Our findings contribute to an improved microscopic understanding of SOT in single molecules.","sentences":["The use of electric fields applied across magnetic heterojunctions that lack spatial inversion symmetry has been previously proposed as a non-magnetic mean of controlling localized magnetic moments through spin-orbit torques (SOT).","The implementation of this concept at the single-molecule level has remained a challenge, however.","Here, we present first-principle calculations of SOT in a single-molecule junction under bias and beyond linear response.","Employing a self-consistency scheme invoking density functional theory and non-equilibrium Green's function theory, we compute the current-induced SOT.","Responding to this torque, a localized magnetic moment can tilt.","Within the linear regime our quantitative estimates for the SOT in single-molecule junctions yield values similar to those known for magnetic interfaces.","Our findings contribute to an improved microscopic understanding of SOT in single molecules."],"url":"http://arxiv.org/abs/2402.09610v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-14 22:27:48","title":"Concatenate codes, save qubits","abstract":"The essential requirement for fault-tolerant quantum computation (FTQC) is the total protocol design to achieve a fair balance of all the critical factors relevant to its practical realization, such as the space overhead, the threshold, and the modularity. A major obstacle in realizing FTQC with conventional protocols, such as those based on the surface code and the concatenated Steane code, has been the space overhead, i.e., the required number of physical qubits per logical qubit. Protocols based on high-rate quantum low-density parity-check (LDPC) codes gather considerable attention as a way to reduce the space overhead, but problematically, the existing fault-tolerant protocols for such quantum LDPC codes sacrifice the other factors. Here we construct a new fault-tolerant protocol to meet these requirements simultaneously based on more recent progress on the techniques for concatenated codes rather than quantum LDPC codes, achieving a constant space overhead, a high threshold, and flexibility in modular architecture designs. In particular, under a physical error rate of $0.1\\%$, our protocol reduces the space overhead to achieve the logical CNOT error rates $10^{-10}$ and $10^{-24}$ by more than $90 \\%$ and $97 \\%$, respectively, compared to the protocol for the surface code. Furthermore, our protocol achieves the threshold of $2.4 \\%$ under a conventional circuit-level error model, substantially outperforming that of the surface code. The use of concatenated codes also naturally introduces abstraction layers essential for the modularity of FTQC architectures. These results indicate that the code-concatenation approach opens a way to significantly save qubits in realizing FTQC while fulfilling the other essential requirements for the practical protocol design.","sentences":["The essential requirement for fault-tolerant quantum computation (FTQC) is the total protocol design to achieve a fair balance of all the critical factors relevant to its practical realization, such as the space overhead, the threshold, and the modularity.","A major obstacle in realizing FTQC with conventional protocols, such as those based on the surface code and the concatenated Steane code, has been the space overhead, i.e., the required number of physical qubits per logical qubit.","Protocols based on high-rate quantum low-density parity-check (LDPC) codes gather considerable attention as a way to reduce the space overhead, but problematically, the existing fault-tolerant protocols for such quantum LDPC codes sacrifice the other factors.","Here we construct a new fault-tolerant protocol to meet these requirements simultaneously based on more recent progress on the techniques for concatenated codes rather than quantum LDPC codes, achieving a constant space overhead, a high threshold, and flexibility in modular architecture designs.","In particular, under a physical error rate of $0.1\\%$, our protocol reduces the space overhead to achieve the logical CNOT error rates $10^{-10}$ and $10^{-24}$ by more than $90 \\%$ and $97 \\%$, respectively, compared to the protocol for the surface code.","Furthermore, our protocol achieves the threshold of $2.4 \\%$ under a conventional circuit-level error model, substantially outperforming that of the surface code.","The use of concatenated codes also naturally introduces abstraction layers essential for the modularity of FTQC architectures.","These results indicate that the code-concatenation approach opens a way to significantly save qubits in realizing FTQC while fulfilling the other essential requirements for the practical protocol design."],"url":"http://arxiv.org/abs/2402.09606v1","category":"quant-ph"}
{"created":"2024-02-14 22:18:50","title":"Understanding the Opportunity-Centric Accessibility for Public Charging Infrastructure","abstract":"In this study, we utilize data from over 28,000 public charging stations (PCSs) and 5.5 million points of interest across twenty U.S. metropolitan areas to underscore the importance of considering the availability of opportunities when assessing accessibility to PCSs, rather than relying solely on spatial proximity. Specifically, we conduct comprehensive comparisons of opportunity-centric accessibility measures with distance-based measures and perform counterfactual analyses under various PCS deployment strategies. Our findings reveal significant inequalities in PCS access across different neighborhoods under distance-based and opportunity-centric measures. However, a greater disparity exists when considering opportunities, with high-income communities having significantly better access to PCSs. Counterfactual analyses suggest that equitable deployment based on distance measures result in the least equitable outcomes when considering opportunities, primarily due to the existing disparity in opportunity distributions in our cities. Our insights highlight the complexity of locating PCSs and can guide nationwide PCS deployment for long-term societal benefits.","sentences":["In this study, we utilize data from over 28,000 public charging stations (PCSs) and 5.5 million points of interest across twenty U.S. metropolitan areas to underscore the importance of considering the availability of opportunities when assessing accessibility to PCSs, rather than relying solely on spatial proximity.","Specifically, we conduct comprehensive comparisons of opportunity-centric accessibility measures with distance-based measures and perform counterfactual analyses under various PCS deployment strategies.","Our findings reveal significant inequalities in PCS access across different neighborhoods under distance-based and opportunity-centric measures.","However, a greater disparity exists when considering opportunities, with high-income communities having significantly better access to PCSs.","Counterfactual analyses suggest that equitable deployment based on distance measures result in the least equitable outcomes when considering opportunities, primarily due to the existing disparity in opportunity distributions in our cities.","Our insights highlight the complexity of locating PCSs and can guide nationwide PCS deployment for long-term societal benefits."],"url":"http://arxiv.org/abs/2402.09602v1","category":"physics.soc-ph"}
{"created":"2024-02-14 22:00:57","title":"Pulmonologists-Level lung cancer detection based on standard blood test results and smoking status using an explainable machine learning approach","abstract":"Lung cancer (LC) remains the primary cause of cancer-related mortality, largely due to late-stage diagnoses. Effective strategies for early detection are therefore of paramount importance. In recent years, machine learning (ML) has demonstrated considerable potential in healthcare by facilitating the detection of various diseases. In this retrospective development and validation study, we developed an ML model based on dynamic ensemble selection (DES) for LC detection. The model leverages standard blood sample analysis and smoking history data from a large population at risk in Denmark. The study includes all patients examined on suspicion of LC in the Region of Southern Denmark from 2009 to 2018. We validated and compared the predictions by the DES model with diagnoses provided by five pulmonologists. Among the 38,944 patients, 9,940 had complete data of which 2,505 (25\\%) had LC. The DES model achieved an area under the roc curve of 0.77$\\pm$0.01, sensitivity of 76.2\\%$\\pm$2.4\\%, specificity of 63.8\\%$\\pm$2.3\\%, positive predictive value of 41.6\\%$\\pm$1.2\\%, and F\\textsubscript{1}-score of 53.8\\%$\\pm$1.1\\%. The DES model outperformed all five pulmonologists, achieving a sensitivity 9\\% higher than their average. The model identified smoking status, age, total calcium levels, neutrophil count, and lactate dehydrogenase as the most important factors for the detection of LC. The results highlight the successful application of the ML approach in detecting LC, surpassing pulmonologists' performance. Incorporating clinical and laboratory data in future risk assessment models can improve decision-making and facilitate timely referrals.","sentences":["Lung cancer (LC) remains the primary cause of cancer-related mortality, largely due to late-stage diagnoses.","Effective strategies for early detection are therefore of paramount importance.","In recent years, machine learning (ML) has demonstrated considerable potential in healthcare by facilitating the detection of various diseases.","In this retrospective development and validation study, we developed an ML model based on dynamic ensemble selection (DES) for LC detection.","The model leverages standard blood sample analysis and smoking history data from a large population at risk in Denmark.","The study includes all patients examined on suspicion of LC in the Region of Southern Denmark from 2009 to 2018.","We validated and compared the predictions by the DES model with diagnoses provided by five pulmonologists.","Among the 38,944 patients, 9,940 had complete data of which 2,505 (25\\%) had LC.","The DES model achieved an area under the roc curve of 0.77$\\pm$0.01, sensitivity of 76.2\\%$\\pm$2.4\\%, specificity of 63.8\\%$\\pm$2.3\\%, positive predictive value of 41.6\\%$\\pm$1.2\\%, and F\\textsubscript{1}-score of 53.8\\%$\\pm$1.1\\%.","The DES model outperformed all five pulmonologists, achieving a sensitivity 9\\% higher than their average.","The model identified smoking status, age, total calcium levels, neutrophil count, and lactate dehydrogenase as the most important factors for the detection of LC.","The results highlight the successful application of the ML approach in detecting LC, surpassing pulmonologists' performance.","Incorporating clinical and laboratory data in future risk assessment models can improve decision-making and facilitate timely referrals."],"url":"http://arxiv.org/abs/2402.09596v1","category":"cs.LG"}
{"created":"2024-02-14 21:34:44","title":"Reconstructing the Geometry of Random Geometric Graphs","abstract":"Random geometric graphs are random graph models defined on metric spaces. Such a model is defined by first sampling points from a metric space and then connecting each pair of sampled points with probability that depends on their distance, independently among pairs. In this work, we show how to efficiently reconstruct the geometry of the underlying space from the sampled graph under the manifold assumption, i.e., assuming that the underlying space is a low dimensional manifold and that the connection probability is a strictly decreasing function of the Euclidean distance between the points in a given embedding of the manifold in $\\mathbb{R}^N$. Our work complements a large body of work on manifold learning, where the goal is to recover a manifold from sampled points sampled in the manifold along with their (approximate) distances.","sentences":["Random geometric graphs are random graph models defined on metric spaces.","Such a model is defined by first sampling points from a metric space and then connecting each pair of sampled points with probability that depends on their distance, independently among pairs.","In this work, we show how to efficiently reconstruct the geometry of the underlying space from the sampled graph under the manifold assumption, i.e., assuming that the underlying space is a low dimensional manifold and that the connection probability is a strictly decreasing function of the Euclidean distance between the points in a given embedding of the manifold in $\\mathbb{R}^N$. Our work complements a large body of work on manifold learning, where the goal is to recover a manifold from sampled points sampled in the manifold along with their (approximate) distances."],"url":"http://arxiv.org/abs/2402.09591v1","category":"cs.LG"}
{"created":"2024-02-14 20:48:58","title":"Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer","abstract":"In Chaos, a minor divergence between two initial conditions exhibits exponential amplification over time, leading to far-away outcomes, known as the butterfly effect. Thus, the distant future is full of uncertainty and hard to forecast. We introduce Group Reservoir Transformer to predict long-term events more accurately and robustly by overcoming two challenges in Chaos: (1) the extensive historical sequences and (2) the sensitivity to initial conditions. A reservoir is attached to a Transformer to efficiently handle arbitrarily long historical lengths, with an extension of a group of reservoirs to reduce the uncertainty due to the initialization variations. Our architecture consistently outperforms state-of-the-art DNN models in multivariate time series, including NLinear, Pyformer, Informer, Autoformer, and the baseline Transformer, with an error reduction of up to -89.43\\% in various fields such as ETTh, ETTm, and air quality, demonstrating that an ensemble of butterfly learning, the prediction can be improved to a more adequate and certain one, despite of the traveling time to the unknown future.","sentences":["In Chaos, a minor divergence between two initial conditions exhibits exponential amplification over time, leading to far-away outcomes, known as the butterfly effect.","Thus, the distant future is full of uncertainty and hard to forecast.","We introduce Group Reservoir Transformer to predict long-term events more accurately and robustly by overcoming two challenges in Chaos: (1) the extensive historical sequences and (2) the sensitivity to initial conditions.","A reservoir is attached to a Transformer to efficiently handle arbitrarily long historical lengths, with an extension of a group of reservoirs to reduce the uncertainty due to the initialization variations.","Our architecture consistently outperforms state-of-the-art DNN models in multivariate time series, including NLinear, Pyformer, Informer, Autoformer, and the baseline Transformer, with an error reduction of up to -89.43\\% in various fields such as ETTh, ETTm, and air quality, demonstrating that an ensemble of butterfly learning, the prediction can be improved to a more adequate and certain one, despite of the traveling time to the unknown future."],"url":"http://arxiv.org/abs/2402.09573v1","category":"cs.LG"}
{"created":"2024-02-14 20:39:07","title":"TAI-GAN: A Temporally and Anatomically Informed Generative Adversarial Network for early-to-late frame conversion in dynamic cardiac PET inter-frame motion correction","abstract":"Inter-frame motion in dynamic cardiac positron emission tomography (PET) using rubidium-82 (82-Rb) myocardial perfusion imaging impacts myocardial blood flow (MBF) quantification and the diagnosis accuracy of coronary artery diseases. However, the high cross-frame distribution variation due to rapid tracer kinetics poses a considerable challenge for inter-frame motion correction, especially for early frames where intensity-based image registration techniques often fail. To address this issue, we propose a novel method called Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) that utilizes an all-to-one mapping to convert early frames into those with tracer distribution similar to the last reference frame. The TAI-GAN consists of a feature-wise linear modulation layer that encodes channel-wise parameters generated from temporal information and rough cardiac segmentation masks with local shifts that serve as anatomical information. Our proposed method was evaluated on a clinical 82-Rb PET dataset, and the results show that our TAI-GAN can produce converted early frames with high image quality, comparable to the real reference frames. After TAI-GAN conversion, the motion estimation accuracy and subsequent myocardial blood flow (MBF) quantification with both conventional and deep learning-based motion correction methods were improved compared to using the original frames.","sentences":["Inter-frame motion in dynamic cardiac positron emission tomography (PET) using rubidium-82 (82-Rb) myocardial perfusion imaging impacts myocardial blood flow (MBF) quantification and the diagnosis accuracy of coronary artery diseases.","However, the high cross-frame distribution variation due to rapid tracer kinetics poses a considerable challenge for inter-frame motion correction, especially for early frames where intensity-based image registration techniques often fail.","To address this issue, we propose a novel method called Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) that utilizes an all-to-one mapping to convert early frames into those with tracer distribution similar to the last reference frame.","The TAI-GAN consists of a feature-wise linear modulation layer that encodes channel-wise parameters generated from temporal information and rough cardiac segmentation masks with local shifts that serve as anatomical information.","Our proposed method was evaluated on a clinical 82-Rb PET dataset, and the results show that our TAI-GAN can produce converted early frames with high image quality, comparable to the real reference frames.","After TAI-GAN conversion, the motion estimation accuracy and subsequent myocardial blood flow (MBF) quantification with both conventional and deep learning-based motion correction methods were improved compared to using the original frames."],"url":"http://arxiv.org/abs/2402.09567v1","category":"eess.IV"}
{"created":"2024-02-14 20:12:51","title":"Use of Agile Practices in Start-ups","abstract":"Context Software start-ups have shown their ability to develop and launch innovative software products and services. Small, motivated teams and uncertain project scope makes start-ups good candidates for adopting Agile practices. Objective We explore how start-ups use Agile practices and what effects can be associated with the use of those practices. Method We use a case survey to analyze 84 start-up cases and 56 Agile practices. We apply statistical methods to test for statistically significant associations between the use of Agile practices, team, and product factors. Results Our results suggest that development of the backlog, use of version control, code refactoring, and development of user stories are the most frequently reported practices. We identify 22 associations between the use of Agile practices, team, and product factors. The use of Agile practices is associated with effects on source code and overall product quality. A team's positive or negative attitude towards best engineering practices is a significant indicator for either adoption or rejection of certain Agile practices. To explore the relationships in our findings, we set forth a number of propositions that can be investigated in future research. Conclusions We conclude that start-ups use Agile practices, however without following any specific methodology. We identify the opportunity for more fine-grained studies into the adoption and effects of individual Agile practices. Start-up practitioners could benefit from Agile practices in terms of better overall quality, tighter control over team performance, and resource utilization.","sentences":["Context Software start-ups have shown their ability to develop and launch innovative software products and services.","Small, motivated teams and uncertain project scope makes start-ups good candidates for adopting Agile practices.","Objective We explore how start-ups use Agile practices and what effects can be associated with the use of those practices.","Method We use a case survey to analyze 84 start-up cases and 56 Agile practices.","We apply statistical methods to test for statistically significant associations between the use of Agile practices, team, and product factors.","Results Our results suggest that development of the backlog, use of version control, code refactoring, and development of user stories are the most frequently reported practices.","We identify 22 associations between the use of Agile practices, team, and product factors.","The use of Agile practices is associated with effects on source code and overall product quality.","A team's positive or negative attitude towards best engineering practices is a significant indicator for either adoption or rejection of certain Agile practices.","To explore the relationships in our findings, we set forth a number of propositions that can be investigated in future research.","Conclusions We conclude that start-ups use Agile practices, however without following any specific methodology.","We identify the opportunity for more fine-grained studies into the adoption and effects of individual Agile practices.","Start-up practitioners could benefit from Agile practices in terms of better overall quality, tighter control over team performance, and resource utilization."],"url":"http://arxiv.org/abs/2402.09555v1","category":"cs.SE"}
{"created":"2024-02-14 20:05:26","title":"Rationality Report Cards: Assessing the Economic Rationality of Large Language Models","abstract":"There is increasing interest in using LLMs as decision-making \"agents.\" Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions -- and more broadly, determining whether an LLM agent is reliable enough to be trusted -- requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained \"elements\" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a \"rationality report card.\" Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models' ability to exhibit rational behavior.","sentences":["There is increasing interest in using LLMs as decision-making \"agents.\"","Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc?","Settling these questions -- and more broadly, determining whether an LLM agent is reliable enough to be trusted -- requires a methodology for assessing such an agent's economic rationality.","In this paper, we provide one.","We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained \"elements\" that an agent should exhibit, along with dependencies between them.","We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a \"rationality report card.\"","Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models' ability to exhibit rational behavior."],"url":"http://arxiv.org/abs/2402.09552v1","category":"cs.CL"}
{"created":"2024-02-14 20:05:21","title":"Zak-OTFS and LDPC Codes","abstract":"Orthogonal Time Frequency Space (OTFS) is a framework for communications and active sensing that processes signals in the delay-Doppler (DD) domain. It is informed by 6G propagation environments, where Doppler spreads measured in kHz make it more and more difficult to estimate channels, and the standard model-dependent approach to wireless communication is starting to break down. We consider Zak-OTFS where inverse Zak transform converts information symbols mounted on DD domain pulses to the time domain for transmission. Zak-OTFS modulation is parameterized by a delay period $\\tau_{p}$ and a Doppler period $\\nu_{p}$, where the product $\\tau_{p}\\nu_{p}=1$. When the channel spread is less than the delay period, and the Doppler spread is less than the Doppler period, the Zak-OTFS input-output relation can be predicted from the response to a single pilot symbol. The highly reliable channel estimates concentrate around the pilot location, and we configure low-density parity-check (LDPC) codes that take advantage of this prior information about reliability. It is advantageous to allocate information symbols to more reliable bins in the DD domain. We report simulation results for a Veh-A channel model where it is not possible to resolve all the paths, showing that LDPC coding extends the range of Doppler spreads for which reliable model-free communication is possible. We show that LDPC coding reduces sensitivity to the choice of transmit filter, making bandwidth expansion less necessary. Finally, we compare BER performance of Zak-OTFS to that of a multicarrier approximation (MC-OTFS), showing LDPC coding amplifies the gains previously reported for uncoded transmission.","sentences":["Orthogonal Time Frequency Space (OTFS) is a framework for communications and active sensing that processes signals in the delay-Doppler (DD) domain.","It is informed by 6G propagation environments, where Doppler spreads measured in kHz make it more and more difficult to estimate channels, and the standard model-dependent approach to wireless communication is starting to break down.","We consider Zak-OTFS where inverse Zak transform converts information symbols mounted on DD domain pulses to the time domain for transmission.","Zak-OTFS modulation is parameterized by a delay period $\\tau_{p}$ and a Doppler period $\\nu_{p}$, where the product $\\tau_{p}\\nu_{p}=1$. When the channel spread is less than the delay period, and the Doppler spread is less than the Doppler period, the Zak-OTFS input-output relation can be predicted from the response to a single pilot symbol.","The highly reliable channel estimates concentrate around the pilot location, and we configure low-density parity-check (LDPC) codes that take advantage of this prior information about reliability.","It is advantageous to allocate information symbols to more reliable bins in the DD domain.","We report simulation results for a Veh-A channel model where it is not possible to resolve all the paths, showing that LDPC coding extends the range of Doppler spreads for which reliable model-free communication is possible.","We show that LDPC coding reduces sensitivity to the choice of transmit filter, making bandwidth expansion less necessary.","Finally, we compare BER performance of Zak-OTFS to that of a multicarrier approximation (MC-OTFS), showing LDPC coding amplifies the gains previously reported for uncoded transmission."],"url":"http://arxiv.org/abs/2402.09551v1","category":"cs.IT"}
{"created":"2024-02-14 19:49:36","title":"On the need for effective tools for debugging quantum programs","abstract":"The ability to incorporate quantum phenomena in computing unlocks a host of new ways to make mistakes. This work surveys existing studies and approaches to debugging quantum programs. It then presents a set of examples that stem from first-hand experience, intended to motivate future research on the subject and the development of novel tools and techniques.","sentences":["The ability to incorporate quantum phenomena in computing unlocks a host of new ways to make mistakes.","This work surveys existing studies and approaches to debugging quantum programs.","It then presents a set of examples that stem from first-hand experience, intended to motivate future research on the subject and the development of novel tools and techniques."],"url":"http://arxiv.org/abs/2402.09547v1","category":"quant-ph"}
{"created":"2024-02-14 19:31:57","title":"Assessing test artifact quality -- A tertiary study","abstract":"Context: Modern software development increasingly relies on software testing for an ever more frequent delivery of high quality software. This puts high demands on the quality of the central artifacts in software testing, test suites and test cases. Objective: We aim to develop a comprehensive model for capturing the dimensions of test case/suite quality, which are relevant for a variety of perspectives. Method: We have carried out a systematic literature review to identify and analyze existing secondary studies on quality aspects of software testing artifacts. Results: We identified 49 relevant secondary studies. Of these 49 studies, less than half did some form of quality appraisal of the included primary studies and only 3 took into account the quality of the primary study when synthesizing the results. We present an aggregation of the context dimensions and factors that can be used to characterize the environment in which the test case/suite quality is investigated. We also provide a comprehensive model of test case/suite quality with definitions for the quality attributes and measurements based on findings in the literature and ISO/IEC 25010:2011. Conclusion: The test artifact quality model presented in the paper can be used to support test artifact quality assessment and improvement initiatives in practice. Furtherm Information and Software Technology 139 (2021): 106620ore, the model can also be used as a framework for documenting context characteristics to make research results more accessible for research and practice.","sentences":["Context: Modern software development increasingly relies on software testing for an ever more frequent delivery of high quality software.","This puts high demands on the quality of the central artifacts in software testing, test suites and test cases.","Objective: We aim to develop a comprehensive model for capturing the dimensions of test case/suite quality, which are relevant for a variety of perspectives.","Method: We have carried out a systematic literature review to identify and analyze existing secondary studies on quality aspects of software testing artifacts.","Results:","We identified 49 relevant secondary studies.","Of these 49 studies, less than half did some form of quality appraisal of the included primary studies and only 3 took into account the quality of the primary study when synthesizing the results.","We present an aggregation of the context dimensions and factors that can be used to characterize the environment in which the test case/suite quality is investigated.","We also provide a comprehensive model of test case/suite quality with definitions for the quality attributes and measurements based on findings in the literature and ISO/IEC 25010:2011.","Conclusion: The test artifact quality model presented in the paper can be used to support test artifact quality assessment and improvement initiatives in practice.","Furtherm Information and Software Technology 139 (2021): 106620ore, the model can also be used as a framework for documenting context characteristics to make research results more accessible for research and practice."],"url":"http://arxiv.org/abs/2402.09541v1","category":"cs.SE"}
{"created":"2024-02-14 19:12:15","title":"The dimension weighted fast multipole method for scattered data approximation","abstract":"The present article is concerned scattered data approximation for higher dimensional data sets which exhibit an anisotropic behavior in the different dimensions. Tailoring sparse polynomial interpolation to this specific situation, we derive very efficient degenerate kernel approximations which we then use in a dimension weighted fast multipole method. This dimension weighted fast multipole method enables to deal with many more dimensions than the standard black-box fast multipole method based on interpolation. A thorough analysis of the method is provided including rigorous error estimates. The accuracy and the cost of the approach are validated by extensive numerical results. As a relevant application, we apply the approach to a shape uncertainty quantification problem.","sentences":["The present article is concerned scattered data approximation for higher dimensional data sets which exhibit an anisotropic behavior in the different dimensions.","Tailoring sparse polynomial interpolation to this specific situation, we derive very efficient degenerate kernel approximations which we then use in a dimension weighted fast multipole method.","This dimension weighted fast multipole method enables to deal with many more dimensions than the standard black-box fast multipole method based on interpolation.","A thorough analysis of the method is provided including rigorous error estimates.","The accuracy and the cost of the approach are validated by extensive numerical results.","As a relevant application, we apply the approach to a shape uncertainty quantification problem."],"url":"http://arxiv.org/abs/2402.09531v1","category":"math.NA"}
{"created":"2024-02-14 19:07:18","title":"C3NN: Cosmological Correlator Convolutional Neural Network -- an interpretable machine learning tool for cosmological analyses","abstract":"Modern cosmological research in large scale structure has witnessed an increasing number of applications of machine learning methods. Among them, Convolutional Neural Networks (CNNs) have received substantial attention due to their outstanding performance in image classification, cosmological parameter inference and various other tasks. However, many models which make use of CNNs are criticized as \"black boxes\" due to the difficulties in relating their outputs intuitively and quantitatively to the cosmological fields under investigation. To overcome this challenge, we present the Cosmological Correlator Convolutional Neural Network (C3NN) -- a fusion of CNN architecture with the framework of cosmological N-point correlation functions (NPCFs). We demonstrate that the output of this model can be expressed explicitly in terms of the analytically tractable NPCFs. Together with other auxiliary algorithms, we are able to open the \"black box\" by quantitatively ranking different orders of the interpretable convolution outputs based on their contribution to classification tasks. As a proof of concept, we demonstrate this by applying our framework to a series of binary classification tasks using Gaussian and Log-normal random fields and relating its outputs to the analytical NPCFs describing the two fields. Furthermore, we exhibit the model's ability to distinguish different dark energy scenarios ($w_0=-0.95$ and $-1.05$) using N-body simulated weak lensing convergence maps and discuss the physical implications coming from their interpretability. With these tests, we show that C3NN combines advanced aspects of machine learning architectures with the framework of cosmological NPCFs, thereby making it an exciting tool with the potential to extract physical insights in a robust and explainable way from observational data.","sentences":["Modern cosmological research in large scale structure has witnessed an increasing number of applications of machine learning methods.","Among them, Convolutional Neural Networks (CNNs) have received substantial attention due to their outstanding performance in image classification, cosmological parameter inference and various other tasks.","However, many models which make use of CNNs are criticized as \"black boxes\" due to the difficulties in relating their outputs intuitively and quantitatively to the cosmological fields under investigation.","To overcome this challenge, we present the Cosmological Correlator Convolutional Neural Network (C3NN) -- a fusion of CNN architecture with the framework of cosmological N-point correlation functions (NPCFs).","We demonstrate that the output of this model can be expressed explicitly in terms of the analytically tractable NPCFs.","Together with other auxiliary algorithms, we are able to open the \"black box\" by quantitatively ranking different orders of the interpretable convolution outputs based on their contribution to classification tasks.","As a proof of concept, we demonstrate this by applying our framework to a series of binary classification tasks using Gaussian and Log-normal random fields and relating its outputs to the analytical NPCFs describing the two fields.","Furthermore, we exhibit the model's ability to distinguish different dark energy scenarios ($w_0=-0.95$ and $-1.05$) using N-body simulated weak lensing convergence maps and discuss the physical implications coming from their interpretability.","With these tests, we show that C3NN combines advanced aspects of machine learning architectures with the framework of cosmological NPCFs, thereby making it an exciting tool with the potential to extract physical insights in a robust and explainable way from observational data."],"url":"http://arxiv.org/abs/2402.09526v1","category":"astro-ph.CO"}
{"created":"2024-02-14 19:00:03","title":"Replica topological order in quantum mixed states and quantum error correction","abstract":"Topological phases of matter offer a promising platform for quantum computation and quantum error correction. Nevertheless, unlike its counterpart in pure states, descriptions of topological order in mixed states remain relatively under-explored. Our work give two definitions for replica topological order in mixed states, which involve $n$ copies of density matrices of the mixed state. Our framework categorizes topological orders in mixed states as either quantum, classical, or trivial, depending on the type of information that can be encoded. For the case of the toric code model in the presence of decoherence, we associate for each phase a quantum channel and describes the structure of the code space. We show that in the quantum-topological phase, there exists a postselection-based error correction protocol that recovers the quantum information, while in the classical-topological phase, the quantum information has decohere and cannot be fully recovered. We accomplish this by describing the mixed state as a projected entangled pairs state (PEPS) and identifying the symmetry-protected topological order of its boundary state to the bulk topology. We discuss the extent that our findings can be extrapolated to $n \\to 1$ limit.","sentences":["Topological phases of matter offer a promising platform for quantum computation and quantum error correction.","Nevertheless, unlike its counterpart in pure states, descriptions of topological order in mixed states remain relatively under-explored.","Our work give two definitions for replica topological order in mixed states, which involve $n$ copies of density matrices of the mixed state.","Our framework categorizes topological orders in mixed states as either quantum, classical, or trivial, depending on the type of information that can be encoded.","For the case of the toric code model in the presence of decoherence, we associate for each phase a quantum channel and describes the structure of the code space.","We show that in the quantum-topological phase, there exists a postselection-based error correction protocol that recovers the quantum information, while in the classical-topological phase, the quantum information has decohere and cannot be fully recovered.","We accomplish this by describing the mixed state as a projected entangled pairs state (PEPS) and identifying the symmetry-protected topological order of its boundary state to the bulk topology.","We discuss the extent that our findings can be extrapolated to $n \\to 1$ limit."],"url":"http://arxiv.org/abs/2402.09516v1","category":"quant-ph"}
{"created":"2024-02-14 19:00:01","title":"Mass Beyond Measure: Eccentric Searches for Black Hole Populations","abstract":"Stellar mass binary black holes of unknown formation mechanism have been observed, motivating new methods for distinguishing distinct black hole populations. This work explores how the orbital eccentricity of stellar mass binary black holes is a viable conduit for making such distinctions. Four different production mechanisms, and their corresponding eccentricity distributions, are studied in the context of an experimental landscape composed of mHz (LISA), dHz (DECIGO), and Hz (LIGO) range gravitational wave detectors. We expand on prior work considering these effects at fixed population eccentricity. We show that a strong signal corresponding to subsets of eccentric populations is effectively hidden from the mHz and dHz range gravitational wave detectors without the incorporation of high eccentricity waveform templates. Even with sufficiently large eccentricity templates, we find dHz range experiments with a LISA-like level of sensitivity are unlikely to aid in distinguishing different populations. We consider the degree to which a mHz range detector like LISA can differentiate among black hole populations independently and in concert with follow-up merger detection for binaries coalescing within a 10 year period. We find that mHz range detectors, with only $e < 0.01$ (nearly circular) sensitivity, can successfully discern eccentric sub-populations except when attempting to distinguish very low eccentricity distributions. In these cases where $e < 0.01$ sensitivity is insufficient, we find that the increase in event counts resulting from $e < 0.1$ sensitivity provides a statistically significant signal for discerning even these low eccentricity sub-populations. While improvements offered by $e<0.1$ sensitivity can be generally increased by $\\mathcal{O}(1)$ factors with $e<0.4$ sensitivity, going beyond this in eccentricity sensitivity provides negligible enhancement.","sentences":["Stellar mass binary black holes of unknown formation mechanism have been observed, motivating new methods for distinguishing distinct black hole populations.","This work explores how the orbital eccentricity of stellar mass binary black holes is a viable conduit for making such distinctions.","Four different production mechanisms, and their corresponding eccentricity distributions, are studied in the context of an experimental landscape composed of mHz (LISA), dHz (DECIGO), and Hz (LIGO) range gravitational wave detectors.","We expand on prior work considering these effects at fixed population eccentricity.","We show that a strong signal corresponding to subsets of eccentric populations is effectively hidden from the mHz and dHz range gravitational wave detectors without the incorporation of high eccentricity waveform templates.","Even with sufficiently large eccentricity templates, we find dHz range experiments with a LISA-like level of sensitivity are unlikely to aid in distinguishing different populations.","We consider the degree to which a mHz range detector like LISA can differentiate among black hole populations independently and in concert with follow-up merger detection for binaries coalescing within a 10 year period.","We find that mHz range detectors, with only $e < 0.01$ (nearly circular) sensitivity, can successfully discern eccentric sub-populations except when attempting to distinguish very low eccentricity distributions.","In these cases where $e < 0.01$ sensitivity is insufficient, we find that the increase in event counts resulting from $e < 0.1$ sensitivity provides a statistically significant signal for discerning even these low eccentricity sub-populations.","While improvements offered by $e<0.1$ sensitivity can be generally increased by $\\mathcal{O}(1)$ factors with $e<0.4$ sensitivity, going beyond this in eccentricity sensitivity provides negligible enhancement."],"url":"http://arxiv.org/abs/2402.09513v1","category":"gr-qc"}
{"created":"2024-02-14 19:00:00","title":"Superconducting Quantum Memory with a Suspended Coaxial Resonator","abstract":"A promising way to store quantum information is by encoding it in the bosonic excitations of microwave resonators. This provides for long coherence times, low dephasing rates, as well as a hardware-efficient approach to quantum error correction. There are two main methods used to make superconducting microwave resonators: traditionally machined out of bulk material, and lithographically fabricated on-chip in thin film. 3D resonators have few loss channels and larger mode volumes, and therefore smaller participations in the lossy parts, but it can be challenging to reach high material qualities. On-chip resonators can use low-loss thin films, but confine the field more tightly, resulting in higher participations and additional loss channels from the dielectric substrate. In this work, we present a design in which a dielectric scaffold supports a thin-film conductor within a 3D package, thus combining the low surface participations of bulk-machined cavities with the high quality and control over materials of thin-film circuits. By incorporating a separate chip containing a transmon qubit, we realize a quantum memory and measure single-photon lifetimes in excess of a millisecond. This hybrid 3D architecture has several advantages for scaling, as it relaxes the importance of the package and permits modular construction with separately-replaceable qubit and resonator devices.","sentences":["A promising way to store quantum information is by encoding it in the bosonic excitations of microwave resonators.","This provides for long coherence times, low dephasing rates, as well as a hardware-efficient approach to quantum error correction.","There are two main methods used to make superconducting microwave resonators: traditionally machined out of bulk material, and lithographically fabricated on-chip in thin film.","3D resonators have few loss channels and larger mode volumes, and therefore smaller participations in the lossy parts, but it can be challenging to reach high material qualities.","On-chip resonators can use low-loss thin films, but confine the field more tightly, resulting in higher participations and additional loss channels from the dielectric substrate.","In this work, we present a design in which a dielectric scaffold supports a thin-film conductor within a 3D package, thus combining the low surface participations of bulk-machined cavities with the high quality and control over materials of thin-film circuits.","By incorporating a separate chip containing a transmon qubit, we realize a quantum memory and measure single-photon lifetimes in excess of a millisecond.","This hybrid 3D architecture has several advantages for scaling, as it relaxes the importance of the package and permits modular construction with separately-replaceable qubit and resonator devices."],"url":"http://arxiv.org/abs/2402.09504v1","category":"quant-ph"}
{"created":"2024-02-14 19:00:00","title":"Deconstructing flavor anomalously","abstract":"Flavor deconstruction refers to ultraviolet completions of the Standard Model where the gauge group is split into multiple factors under which fermions transform non-universally. We propose a mechanism for charging same-family fermions into different factors of a deconstructed gauge theory in a way that gauge anomalies are avoided. The mechanism relies in the inclusion of a strongly-coupled sector, responsible of both anomaly cancellation and the breaking of the non-universal gauge symmetry. As an application, we propose different flavor deconstructions of the Standard Model that, instead of complete families, uniquely identify specific third-family fermions. All these deconstructions allow for a new physics scale that can be as low as few TeV and provide an excellent starting point for the explanation of the Standard Model flavor hierarchies.","sentences":["Flavor deconstruction refers to ultraviolet completions of the Standard Model where the gauge group is split into multiple factors under which fermions transform non-universally.","We propose a mechanism for charging same-family fermions into different factors of a deconstructed gauge theory in a way that gauge anomalies are avoided.","The mechanism relies in the inclusion of a strongly-coupled sector, responsible of both anomaly cancellation and the breaking of the non-universal gauge symmetry.","As an application, we propose different flavor deconstructions of the Standard Model that, instead of complete families, uniquely identify specific third-family fermions.","All these deconstructions allow for a new physics scale that can be as low as few TeV and provide an excellent starting point for the explanation of the Standard Model flavor hierarchies."],"url":"http://arxiv.org/abs/2402.09507v1","category":"hep-ph"}
{"created":"2024-02-15 14:50:41","title":"Semi-parametric financial risk forecasting incorporating multiple realized measures","abstract":"A semi-parametric joint Value-at-Risk (VaR) and Expected Shortfall (ES) forecasting framework employing multiple realized measures is developed. The proposed framework extends the quantile regression using multiple realized measures as exogenous variables to model the VaR. Then, the information from realized measures is used to model the time-varying relationship between VaR and ES. Finally, a measurement equation that models the contemporaneous dependence between the quantile and realized measures is used to complete the model. A quasi-likelihood, built on the asymmetric Laplace distribution, enables the Bayesian inference for the proposed model. An adaptive Markov Chain Monte Carlo method is used for the model estimation. The empirical section evaluates the performance of the proposed framework with six stock markets from January 2000 to June 2022, covering the period of COVID-19. Three realized measures, including 5-minute realized variance, bi-power variation, and realized kernel, are incorporated and evaluated in the proposed framework. One-step ahead VaR and ES forecasting results of the proposed model are compared to a range of parametric and semi-parametric models, lending support to the effectiveness of the proposed framework.","sentences":["A semi-parametric joint Value-at-Risk (VaR) and Expected Shortfall (ES) forecasting framework employing multiple realized measures is developed.","The proposed framework extends the quantile regression using multiple realized measures as exogenous variables to model the VaR. Then, the information from realized measures is used to model the time-varying relationship between VaR and ES.","Finally, a measurement equation that models the contemporaneous dependence between the quantile and realized measures is used to complete the model.","A quasi-likelihood, built on the asymmetric Laplace distribution, enables the Bayesian inference for the proposed model.","An adaptive Markov Chain Monte Carlo method is used for the model estimation.","The empirical section evaluates the performance of the proposed framework with six stock markets from January 2000 to June 2022, covering the period of COVID-19.","Three realized measures, including 5-minute realized variance, bi-power variation, and realized kernel, are incorporated and evaluated in the proposed framework.","One-step ahead VaR and ES forecasting results of the proposed model are compared to a range of parametric and semi-parametric models, lending support to the effectiveness of the proposed framework."],"url":"http://arxiv.org/abs/2402.09985v1","category":"q-fin.RM"}
{"created":"2024-02-15 11:28:44","title":"Dissipation of nonlinear acoustic waves in thermoviscous pores","abstract":"We derive a nonlinear acoustic wave propagation model for analysing the thermoviscous dissipation in narrow pores with wavy walls. As the nonlinear waves propagate in the thermoviscous pores, the wave-steepening effect competes with the bulk dissipation, as well as the thermoviscous heat transfer and shear from the pore walls. Consequently, the length scale of the wave is modified. We use the characteristic nonlinear wave thickness scale to obtain linear and nonlinear wave equations governing the unsteady shock-wall interaction. We also perform two-dimensional shock-resolved DNS of the wave propagation inside the pores and compare the results with model equations. We show that for flat-walls and shock strength parameter $\\epsilon$, the dimensional wall heat-flux and shear scale as $\\epsilon$. For wavy walls, the scaling becomes $\\epsilon^{3/2 - n(k)}$ where $k$ is the wall-waviness wavenumber and the exponent $n$ increases from $0.5$ for $k=0$ to $n(k)\\approx0.65$ for $k=10$, $n(k)\\approx 0.75$ for $k=20$, and $n(k)\\approx0.85$ for $k=40$. Hence, increasing the wall waviness reduces the dependence of the wall heat-flux and shear on nonlinear acoustic wave strength. Furthermore, we show that","sentences":["We derive a nonlinear acoustic wave propagation model for analysing the thermoviscous dissipation in narrow pores with wavy walls.","As the nonlinear waves propagate in the thermoviscous pores, the wave-steepening effect competes with the bulk dissipation, as well as the thermoviscous heat transfer and shear from the pore walls.","Consequently, the length scale of the wave is modified.","We use the characteristic nonlinear wave thickness scale to obtain linear and nonlinear wave equations governing the unsteady shock-wall interaction.","We also perform two-dimensional shock-resolved DNS of the wave propagation inside the pores and compare the results with model equations.","We show that for flat-walls and shock strength parameter $\\epsilon$, the dimensional wall heat-flux and shear scale as $\\epsilon$. For wavy walls, the scaling becomes $\\epsilon^{3/2 - n(k)}$ where $k$ is the wall-waviness wavenumber and the exponent $n$ increases from $0.5$ for $k=0$ to $n(k)\\approx0.65$ for $k=10$, $n(k)\\approx 0.75$ for $k=20$, and $n(k)\\approx0.85$ for $k=40$. Hence, increasing the wall waviness reduces the dependence of the wall heat-flux and shear on nonlinear acoustic wave strength.","Furthermore, we show that"],"url":"http://arxiv.org/abs/2402.09889v1","category":"physics.flu-dyn"}
{"created":"2024-02-15 00:52:34","title":"Multi-Fidelity Methods for Optimization: A Survey","abstract":"Real-world black-box optimization often involves time-consuming or costly experiments and simulations. Multi-fidelity optimization (MFO) stands out as a cost-effective strategy that balances high-fidelity accuracy with computational efficiency through a hierarchical fidelity approach. This survey presents a systematic exploration of MFO, underpinned by a novel text mining framework based on a pre-trained language model. We delve deep into the foundational principles and methodologies of MFO, focusing on three core components -- multi-fidelity surrogate models, fidelity management strategies, and optimization techniques. Additionally, this survey highlights the diverse applications of MFO across several key domains, including machine learning, engineering design optimization, and scientific discovery, showcasing the adaptability and effectiveness of MFO in tackling complex computational challenges. Furthermore, we also envision several emerging challenges and prospects in the MFO landscape, spanning scalability, the composition of lower fidelities, and the integration of human-in-the-loop approaches at the algorithmic level. We also address critical issues related to benchmarking and the advancement of open science within the MFO community. Overall, this survey aims to catalyze further research and foster collaborations in MFO, setting the stage for future innovations and breakthroughs in the field.","sentences":["Real-world black-box optimization often involves time-consuming or costly experiments and simulations.","Multi-fidelity optimization (MFO) stands out as a cost-effective strategy that balances high-fidelity accuracy with computational efficiency through a hierarchical fidelity approach.","This survey presents a systematic exploration of MFO, underpinned by a novel text mining framework based on a pre-trained language model.","We delve deep into the foundational principles and methodologies of MFO, focusing on three core components -- multi-fidelity surrogate models, fidelity management strategies, and optimization techniques.","Additionally, this survey highlights the diverse applications of MFO across several key domains, including machine learning, engineering design optimization, and scientific discovery, showcasing the adaptability and effectiveness of MFO in tackling complex computational challenges.","Furthermore, we also envision several emerging challenges and prospects in the MFO landscape, spanning scalability, the composition of lower fidelities, and the integration of human-in-the-loop approaches at the algorithmic level.","We also address critical issues related to benchmarking and the advancement of open science within the MFO community.","Overall, this survey aims to catalyze further research and foster collaborations in MFO, setting the stage for future innovations and breakthroughs in the field."],"url":"http://arxiv.org/abs/2402.09638v1","category":"cs.LG"}
{"created":"2024-02-14 22:10:42","title":"MCMC-driven learning","abstract":"This paper is intended to appear as a chapter for the Handbook of Markov Chain Monte Carlo. The goal of this chapter is to unify various problems at the intersection of Markov chain Monte Carlo (MCMC) and machine learning$\\unicode{x2014}$which includes black-box variational inference, adaptive MCMC, normalizing flow construction and transport-assisted MCMC, surrogate-likelihood MCMC, coreset construction for MCMC with big data, Markov chain gradient descent, Markovian score climbing, and more$\\unicode{x2014}$within one common framework. By doing so, the theory and methods developed for each may be translated and generalized.","sentences":["This paper is intended to appear as a chapter for the Handbook of Markov Chain Monte Carlo.","The goal of this chapter is to unify various problems at the intersection of Markov chain Monte Carlo (MCMC) and machine learning$\\unicode{x2014}$which includes black-box variational inference, adaptive MCMC, normalizing flow construction and transport-assisted MCMC, surrogate-likelihood MCMC, coreset construction for MCMC with big data, Markov chain gradient descent, Markovian score climbing, and more$\\unicode{x2014}$within one common framework.","By doing so, the theory and methods developed for each may be translated and generalized."],"url":"http://arxiv.org/abs/2402.09598v1","category":"stat.ML"}
{"created":"2024-02-14 21:56:43","title":"Rapid on-demand generation of thermal states in superconducting quantum circuits","abstract":"We experimentally demonstrate the fast generation of thermal states of a transmon using a single-junction quantum-circuit refrigerator (QCR) as an in-situ-tunable environment. Through single-shot readout, we monitor the transmon up to its third-excited state, assessing population distributions controlled by QCR drive pulses. Whereas cooling can be achieved in the weak-drive regime, high-amplitude pulses can generate Boltzmann-distributed populations from a temperature of 110 mK up to 500 mK within 100 ns. As we propose in our work, this fast and efficient temperature control provides an appealing opportunity to demonstrate a quantum heat engine. Our results also pave the way for efficient dissipative state preparation and for reducing the circuit depth in thermally assisted quantum algorithms and quantum annealing.","sentences":["We experimentally demonstrate the fast generation of thermal states of a transmon using a single-junction quantum-circuit refrigerator (QCR) as an in-situ-tunable environment.","Through single-shot readout, we monitor the transmon up to its third-excited state, assessing population distributions controlled by QCR drive pulses.","Whereas cooling can be achieved in the weak-drive regime, high-amplitude pulses can generate Boltzmann-distributed populations from a temperature of 110 mK up to 500 mK within 100 ns.","As we propose in our work, this fast and efficient temperature control provides an appealing opportunity to demonstrate a quantum heat engine.","Our results also pave the way for efficient dissipative state preparation and for reducing the circuit depth in thermally assisted quantum algorithms and quantum annealing."],"url":"http://arxiv.org/abs/2402.09594v1","category":"quant-ph"}
{"created":"2024-02-14 21:25:06","title":"Domain Adaptation for Contrastive Audio-Language Models","abstract":"Audio-Language Models (ALM) aim to be general-purpose audio models by providing zero-shot capabilities at test time. The zero-shot performance of ALM improves by using suitable text prompts for each domain. The text prompts are usually hand-crafted through an ad-hoc process and lead to a drop in ALM generalization and out-of-distribution performance. Existing approaches to improve domain performance, like few-shot learning or fine-tuning, require access to annotated data and iterations of training. Therefore, we propose a test-time domain adaptation method for ALMs that does not require access to annotations. Our method learns a domain vector by enforcing consistency across augmented views of the testing audio. We extensively evaluate our approach on 12 downstream tasks across domains. With just one example, our domain adaptation method leads to 3.2% (max 8.4%) average zero-shot performance improvement. After adaptation, the model still retains the generalization property of ALMs.","sentences":["Audio-Language Models (ALM) aim to be general-purpose audio models by providing zero-shot capabilities at test time.","The zero-shot performance of ALM improves by using suitable text prompts for each domain.","The text prompts are usually hand-crafted through an ad-hoc process and lead to a drop in ALM generalization and out-of-distribution performance.","Existing approaches to improve domain performance, like few-shot learning or fine-tuning, require access to annotated data and iterations of training.","Therefore, we propose a test-time domain adaptation method for ALMs that does not require access to annotations.","Our method learns a domain vector by enforcing consistency across augmented views of the testing audio.","We extensively evaluate our approach on 12 downstream tasks across domains.","With just one example, our domain adaptation method leads to 3.2% (max 8.4%) average zero-shot performance improvement.","After adaptation, the model still retains the generalization property of ALMs."],"url":"http://arxiv.org/abs/2402.09585v1","category":"cs.SD"}
{"created":"2024-02-14 21:18:11","title":"Horseshoe Priors for Sparse Dirichlet-Multinomial Models","abstract":"Bayesian inference for Dirichlet-Multinomial (DM) models has a long and important history. The concentration parameter $\\alpha$ is pivotal in smoothing category probabilities within the multinomial distribution and is crucial for the inference afterward. Due to the lack of a tractable form of its marginal likelihood, $\\alpha$ is often chosen ad-hoc, or estimated using approximation algorithms. A constant $\\alpha$ often leads to inadequate smoothing of probabilities, particularly for sparse compositional count datasets. In this paper, we introduce a novel class of prior distributions facilitating conjugate updating of the concentration parameter, allowing for full Bayesian inference for DM models. Our methodology is based on fast residue computation and admits closed-form posterior moments in specific scenarios. Additionally, our prior provides continuous shrinkage with its heavy tail and substantial mass around zero, ensuring adaptability to the sparsity or quasi-sparsity of the data. We demonstrate the usefulness of our approach on both simulated examples and on a real-world human microbiome dataset. Finally, we conclude with directions for future research.","sentences":["Bayesian inference for Dirichlet-Multinomial (DM) models has a long and important history.","The concentration parameter $\\alpha$ is pivotal in smoothing category probabilities within the multinomial distribution and is crucial for the inference afterward.","Due to the lack of a tractable form of its marginal likelihood, $\\alpha$ is often chosen ad-hoc, or estimated using approximation algorithms.","A constant $\\alpha$ often leads to inadequate smoothing of probabilities, particularly for sparse compositional count datasets.","In this paper, we introduce a novel class of prior distributions facilitating conjugate updating of the concentration parameter, allowing for full Bayesian inference for DM models.","Our methodology is based on fast residue computation and admits closed-form posterior moments in specific scenarios.","Additionally, our prior provides continuous shrinkage with its heavy tail and substantial mass around zero, ensuring adaptability to the sparsity or quasi-sparsity of the data.","We demonstrate the usefulness of our approach on both simulated examples and on a real-world human microbiome dataset.","Finally, we conclude with directions for future research."],"url":"http://arxiv.org/abs/2402.09583v1","category":"stat.ME"}
{"created":"2024-02-14 21:03:08","title":"Complexity Reduction in Machine Learning-Based Wireless Positioning: Minimum Description Features","abstract":"A recent line of research has been investigating deep learning approaches to wireless positioning (WP). Although these WP algorithms have demonstrated high accuracy and robust performance against diverse channel conditions, they also have a major drawback: they require processing high-dimensional features, which can be prohibitive for mobile applications. In this work, we design a positioning neural network (P-NN) that substantially reduces the complexity of deep learning-based WP through carefully crafted minimum description features. Our feature selection is based on maximum power measurements and their temporal locations to convey information needed to conduct WP. We also develop a novel methodology for adaptively selecting the size of feature space, which optimizes over balancing the expected amount of useful information and classification capability, quantified using information-theoretic measures on the signal bin selection. Numerical results show that P-NN achieves a significant advantage in performance-complexity tradeoff over deep learning baselines that leverage the full power delay profile (PDP).","sentences":["A recent line of research has been investigating deep learning approaches to wireless positioning (WP).","Although these WP algorithms have demonstrated high accuracy and robust performance against diverse channel conditions, they also have a major drawback: they require processing high-dimensional features, which can be prohibitive for mobile applications.","In this work, we design a positioning neural network (P-NN) that substantially reduces the complexity of deep learning-based WP through carefully crafted minimum description features.","Our feature selection is based on maximum power measurements and their temporal locations to convey information needed to conduct WP.","We also develop a novel methodology for adaptively selecting the size of feature space, which optimizes over balancing the expected amount of useful information and classification capability, quantified using information-theoretic measures on the signal bin selection.","Numerical results show that P-NN achieves a significant advantage in performance-complexity tradeoff over deep learning baselines that leverage the full power delay profile (PDP)."],"url":"http://arxiv.org/abs/2402.09580v1","category":"cs.LG"}
{"created":"2024-02-14 20:46:46","title":"Hydrodynamization and resummed viscous hydrodynamics","abstract":"In this contributed chapter, I review our current understanding of the applicability of hydrodynamics to modeling the quark-gluon plasma (QGP), focusing on the question of hydrodynamization/thermalization of the QGP and the anisotropic hydrodynamics (aHydro) far-from-equilibrium hydrodynamic framework. I discuss the existence of far-from-equilibrium hydrodynamic attractors and methods for determining attractors within different hydrodynamical frameworks. I also discuss the determination of attractors from exact solutions to the Boltzmann equation in relaxation time approximation and effective kinetic field theory applied to quantum chromodynamics. I then present comparisons of the kinetic attractors with the attractors obtained in standard second-viscous hydrodynamics frameworks and anisotropic hydrodynamics. I demonstrate that, due to the resummation of terms to all orders in the inverse Reynolds number, the anisotropic hydrodynamics framework can describe both the weak- and strong-interaction limits. I then review the phenomenological application of anisotropic hydrodynamics to relativistic heavy-ion collisions using both quasiparticle aHydro and second-order viscous aHydro. The phenomenological results indicate that aHydro provides a controlled extension of dissipative relativistic hydrodynamics to the early-time far-from-equilibrium stage of heavy-ion collisions. This allows one to better describe the data and to extract the temperature dependence of transport coefficients at much higher temperatures than linearized second-order viscous hydrodynamics.","sentences":["In this contributed chapter, I review our current understanding of the applicability of hydrodynamics to modeling the quark-gluon plasma (QGP), focusing on the question of hydrodynamization/thermalization of the QGP and the anisotropic hydrodynamics (aHydro) far-from-equilibrium hydrodynamic framework.","I discuss the existence of far-from-equilibrium hydrodynamic attractors and methods for determining attractors within different hydrodynamical frameworks.","I also discuss the determination of attractors from exact solutions to the Boltzmann equation in relaxation time approximation and effective kinetic field theory applied to quantum chromodynamics.","I then present comparisons of the kinetic attractors with the attractors obtained in standard second-viscous hydrodynamics frameworks and anisotropic hydrodynamics.","I demonstrate that, due to the resummation of terms to all orders in the inverse Reynolds number, the anisotropic hydrodynamics framework can describe both the weak- and strong-interaction limits.","I then review the phenomenological application of anisotropic hydrodynamics to relativistic heavy-ion collisions using both quasiparticle aHydro and second-order viscous aHydro.","The phenomenological results indicate that aHydro provides a controlled extension of dissipative relativistic hydrodynamics to the early-time far-from-equilibrium stage of heavy-ion collisions.","This allows one to better describe the data and to extract the temperature dependence of transport coefficients at much higher temperatures than linearized second-order viscous hydrodynamics."],"url":"http://arxiv.org/abs/2402.09571v1","category":"nucl-th"}
{"created":"2024-02-14 20:24:33","title":"Patch-based adaptive temporal filter and residual evaluation","abstract":"In coherent imaging systems, speckle is a signal-dependent noise that visually strongly degrades images' appearance. A huge amount of SAR data has been acquired from different sensors with different wavelengths, resolutions, incidences and polarizations. We extend the nonlocal filtering strategy to the temporal domain and propose a patch-based adaptive temporal filter (PATF) to take advantage of well-registered multi-temporal SAR images. A patch-based generalised likelihood ratio test is processed to suppress the changed object effects on the multitemporal denoising results. Then, the similarities are transformed into corresponding weights with an exponential function. The denoised value is calculated with a temporal weighted average. Spatial adaptive denoising methods can improve the patch-based weighted temporal average image when the time series is limited. The spatial adaptive denoising step is optional when the time series is large enough. Without reference image, we propose using a patch-based auto-covariance residual evaluation method to examine the ratio image between the noisy and denoised images and look for possible remaining structural contents. It can process automatically and does not rely on a supervised selection of homogeneous regions. It also provides a global score for the whole image. Numerous results demonstrate the effectiveness of the proposed time series denoising method and the usefulness of the residual evaluation method.","sentences":["In coherent imaging systems, speckle is a signal-dependent noise that visually strongly degrades images' appearance.","A huge amount of SAR data has been acquired from different sensors with different wavelengths, resolutions, incidences and polarizations.","We extend the nonlocal filtering strategy to the temporal domain and propose a patch-based adaptive temporal filter (PATF) to take advantage of well-registered multi-temporal SAR images.","A patch-based generalised likelihood ratio test is processed to suppress the changed object effects on the multitemporal denoising results.","Then, the similarities are transformed into corresponding weights with an exponential function.","The denoised value is calculated with a temporal weighted average.","Spatial adaptive denoising methods can improve the patch-based weighted temporal average image when the time series is limited.","The spatial adaptive denoising step is optional when the time series is large enough.","Without reference image, we propose using a patch-based auto-covariance residual evaluation method to examine the ratio image between the noisy and denoised images and look for possible remaining structural contents.","It can process automatically and does not rely on a supervised selection of homogeneous regions.","It also provides a global score for the whole image.","Numerous results demonstrate the effectiveness of the proposed time series denoising method and the usefulness of the residual evaluation method."],"url":"http://arxiv.org/abs/2402.09561v1","category":"cs.CV"}
{"created":"2024-02-14 19:37:53","title":"Rethinking Large Language Model Architectures for Sequential Recommendations","abstract":"Recently, sequential recommendation has been adapted to the LLM paradigm to enjoy the power of LLMs. LLM-based methods usually formulate recommendation information into natural language and the model is trained to predict the next item in an auto-regressive manner. Despite their notable success, the substantial computational overhead of inference poses a significant obstacle to their real-world applicability. In this work, we endeavor to streamline existing LLM-based recommendation models and propose a simple yet highly effective model Lite-LLM4Rec. The primary goal of Lite-LLM4Rec is to achieve efficient inference for the sequential recommendation task. Lite-LLM4Rec circumvents the beam search decoding by using a straight item projection head for ranking scores generation. This design stems from our empirical observation that beam search decoding is ultimately unnecessary for sequential recommendations. Additionally, Lite-LLM4Rec introduces a hierarchical LLM structure tailored to efficiently handle the extensive contextual information associated with items, thereby reducing computational overhead while enjoying the capabilities of LLMs. Experiments on three publicly available datasets corroborate the effectiveness of Lite-LLM4Rec in both performance and inference efficiency (notably 46.8% performance improvement and 97.28% efficiency improvement on ML-1m) over existing LLM-based methods. Our implementations will be open sourced.","sentences":["Recently, sequential recommendation has been adapted to the LLM paradigm to enjoy the power of LLMs.","LLM-based methods usually formulate recommendation information into natural language and the model is trained to predict the next item in an auto-regressive manner.","Despite their notable success, the substantial computational overhead of inference poses a significant obstacle to their real-world applicability.","In this work, we endeavor to streamline existing LLM-based recommendation models and propose a simple yet highly effective model Lite-LLM4Rec.","The primary goal of Lite-LLM4Rec is to achieve efficient inference for the sequential recommendation task.","Lite-LLM4Rec circumvents the beam search decoding by using a straight item projection head for ranking scores generation.","This design stems from our empirical observation that beam search decoding is ultimately unnecessary for sequential recommendations.","Additionally, Lite-LLM4Rec introduces a hierarchical LLM structure tailored to efficiently handle the extensive contextual information associated with items, thereby reducing computational overhead while enjoying the capabilities of LLMs.","Experiments on three publicly available datasets corroborate the effectiveness of Lite-LLM4Rec in both performance and inference efficiency (notably 46.8% performance improvement and 97.28% efficiency improvement on ML-1m) over existing LLM-based methods.","Our implementations will be open sourced."],"url":"http://arxiv.org/abs/2402.09543v1","category":"cs.IR"}
{"created":"2024-02-14 19:09:23","title":"The Manifold Density Function: An Intrinsic Method for the Validation of Manifold Learning","abstract":"We introduce the manifold density function, which is an intrinsic method to validate manifold learning techniques. Our approach adapts and extends Ripley's $K$-function, and categorizes in an unsupervised setting the extent to which an output of a manifold learning algorithm captures the structure of a latent manifold. Our manifold density function generalizes to broad classes of Riemannian manifolds. In particular, we extend the manifold density function to general two-manifolds using the Gauss-Bonnet theorem, and demonstrate that the manifold density function for hypersurfaces is well approximated using the first Laplacian eigenvalue. We prove desirable convergence and robustness properties.","sentences":["We introduce the manifold density function, which is an intrinsic method to validate manifold learning techniques.","Our approach adapts and extends Ripley's $K$-function, and categorizes in an unsupervised setting the extent to which an output of a manifold learning algorithm captures the structure of a latent manifold.","Our manifold density function generalizes to broad classes of Riemannian manifolds.","In particular, we extend the manifold density function to general two-manifolds using the Gauss-Bonnet theorem, and demonstrate that the manifold density function for hypersurfaces is well approximated using the first Laplacian eigenvalue.","We prove desirable convergence and robustness properties."],"url":"http://arxiv.org/abs/2402.09529v1","category":"cs.LG"}
{"created":"2024-02-14 19:00:01","title":"Dissipation driven phase transition in the non-Hermitian Kondo model","abstract":"Non-Hermitian Hamiltonians capture several aspects of open quantum systems, such as dissipation of energy and non-unitary evolution. An example is an optical lattice where the inelastic scattering between the two orbital mobile atoms in their ground state and the atom in a metastable excited state trapped at a particular site and acting as an impurity, results in the two body losses. It was shown in \\cite{nakagawa2018non} that this effect is captured by the non-Hermitian Kondo model. which was shown to exhibit two phases depending on the strength of losses. When the losses are weak, the system exhibits the Kondo phase and when the losses are stronger, the system was shown to exhibit the unscreened phase where the Kondo effect ceases to exist, and the impurity is left unscreened. We re-examined this model using the Bethe Ansatz and found that in addition to the above two phases, the system exhibits a novel $\\widetilde{YSR}$ phase which is present between the Kondo and the unscreened phases. The model is characterized by two renormalization group invariants, a generalized Kondo temperature $T_K$ and a parameter `$\\alpha$' that measures the strength of the loss. The Kondo phase occurs when the losses are weak which corresponds to $0<\\alpha<\\pi/2$. As $\\alpha$ approaches $\\pi/2$, the Kondo cloud shrinks resulting in the formation of a single particle bound state which screens the impurity in the ground state between $\\pi/2<\\alpha<\\pi$. As $\\alpha$ increases, the impurity is unscreened in the ground state but can be screened by the localized bound state for $\\pi<\\alpha<3\\pi/2$. When $\\alpha>3\\pi/2$, one enters the unscreened phase where the impurity cannot be screened. We argue that in addition to the energetics, the system displays different time scales associated with the losses across $\\alpha=\\pi/2$, resulting in a phase transition driven by the dissipation in the system.","sentences":["Non-Hermitian Hamiltonians capture several aspects of open quantum systems, such as dissipation of energy and non-unitary evolution.","An example is an optical lattice where the inelastic scattering between the two orbital mobile atoms in their ground state and the atom in a metastable excited state trapped at a particular site and acting as an impurity, results in the two body losses.","It was shown in \\cite{nakagawa2018non} that this effect is captured by the non-Hermitian Kondo model.","which was shown to exhibit two phases depending on the strength of losses.","When the losses are weak, the system exhibits the Kondo phase and when the losses are stronger, the system was shown to exhibit the unscreened phase where the Kondo effect ceases to exist, and the impurity is left unscreened.","We re-examined this model using the Bethe Ansatz and found that in addition to the above two phases, the system exhibits a novel $\\widetilde{YSR}$ phase which is present between the Kondo and the unscreened phases.","The model is characterized by two renormalization group invariants, a generalized Kondo temperature $T_K$ and a parameter `$\\alpha$' that measures the strength of the loss.","The Kondo phase occurs when the losses are weak which corresponds to $0<\\alpha<\\pi/2$. As $\\alpha$ approaches $\\pi/2$, the Kondo cloud shrinks resulting in the formation of a single particle bound state which screens the impurity in the ground state between $\\pi/2<\\alpha<\\pi$. As $\\alpha$ increases, the impurity is unscreened in the ground state but can be screened by the localized bound state for $\\pi<\\alpha<3\\pi/2$. When $\\alpha>3\\pi/2$, one enters the unscreened phase where the impurity cannot be screened.","We argue that in addition to the energetics, the system displays different time scales associated with the losses across $\\alpha=\\pi/2$, resulting in a phase transition driven by the dissipation in the system."],"url":"http://arxiv.org/abs/2402.09510v1","category":"cond-mat.str-el"}
{"created":"2024-02-14 15:22:59","title":"Multi-Hierarchical Surrogate Learning for Structural Dynamical Crash Simulations Using Graph Convolutional Neural Networks","abstract":"Crash simulations play an essential role in improving vehicle safety, design optimization, and injury risk estimation. Unfortunately, numerical solutions of such problems using state-of-the-art high-fidelity models require significant computational effort. Conventional data-driven surrogate modeling approaches create low-dimensional embeddings for evolving the dynamics in order to circumvent this computational effort. Most approaches directly operate on high-resolution data obtained from numerical discretization, which is both costly and complicated for mapping the flow of information over large spatial distances. Furthermore, working with a fixed resolution prevents the adaptation of surrogate models to environments with variable computing capacities, different visualization resolutions, and different accuracy requirements. We thus propose a multi-hierarchical framework for structurally creating a series of surrogate models for a kart frame, which is a good proxy for industrial-relevant crash simulations, at different levels of resolution. For multiscale phenomena, macroscale features are captured on a coarse surrogate, whereas microscale effects are resolved by finer ones. The learned behavior of the individual surrogates is passed from coarse to finer levels through transfer learning. In detail, we perform a mesh simplification on the kart model to obtain multi-resolution representations of it. We then train a graph-convolutional neural network-based surrogate that learns parameter-dependent low-dimensional latent dynamics on the coarsest representation. Subsequently, another, similarly structured surrogate is trained on the residual of the first surrogate using a finer resolution. This step can be repeated multiple times. By doing so, we construct multiple surrogates for the same system with varying hardware requirements and increasing accuracy.","sentences":["Crash simulations play an essential role in improving vehicle safety, design optimization, and injury risk estimation.","Unfortunately, numerical solutions of such problems using state-of-the-art high-fidelity models require significant computational effort.","Conventional data-driven surrogate modeling approaches create low-dimensional embeddings for evolving the dynamics in order to circumvent this computational effort.","Most approaches directly operate on high-resolution data obtained from numerical discretization, which is both costly and complicated for mapping the flow of information over large spatial distances.","Furthermore, working with a fixed resolution prevents the adaptation of surrogate models to environments with variable computing capacities, different visualization resolutions, and different accuracy requirements.","We thus propose a multi-hierarchical framework for structurally creating a series of surrogate models for a kart frame, which is a good proxy for industrial-relevant crash simulations, at different levels of resolution.","For multiscale phenomena, macroscale features are captured on a coarse surrogate, whereas microscale effects are resolved by finer ones.","The learned behavior of the individual surrogates is passed from coarse to finer levels through transfer learning.","In detail, we perform a mesh simplification on the kart model to obtain multi-resolution representations of it.","We then train a graph-convolutional neural network-based surrogate that learns parameter-dependent low-dimensional latent dynamics on the coarsest representation.","Subsequently, another, similarly structured surrogate is trained on the residual of the first surrogate using a finer resolution.","This step can be repeated multiple times.","By doing so, we construct multiple surrogates for the same system with varying hardware requirements and increasing accuracy."],"url":"http://arxiv.org/abs/2402.09234v2","category":"cs.LG"}
{"created":"2024-02-14 13:00:40","title":"Can AI and humans genuinely communicate?","abstract":"Can AI and humans genuinely communicate? In this article, after giving some background and motivating my proposal (sections 1 to 3), I explore a way to answer this question that I call the \"mental-behavioral methodology\" (sections 4 and 5). This methodology follows the following three steps: First, spell out what mental capacities are sufficient for human communication (as opposed to communication more generally). Second, spell out the experimental paradigms required to test whether a behavior exhibits these capacities. Third, apply or adapt these paradigms to test whether an AI displays the relevant behaviors. If the first two steps are successfully completed, and if the AI passes the tests with human-like results, this constitutes evidence that this AI and humans can genuinely communicate. This mental-behavioral methodology has the advantage that we don't need to understand the workings of black-box algorithms, such as standard deep neural networks. This is comparable to the fact that we don't need to understand how human brains work to know that humans can genuinely communicate. This methodology also has its disadvantages and I will discuss some of them (section 6).","sentences":["Can AI and humans genuinely communicate?","In this article, after giving some background and motivating my proposal (sections 1 to 3), I explore a way to answer this question that I call the \"mental-behavioral methodology\" (sections 4 and 5).","This methodology follows the following three steps: First, spell out what mental capacities are sufficient for human communication (as opposed to communication more generally).","Second, spell out the experimental paradigms required to test whether a behavior exhibits these capacities.","Third, apply or adapt these paradigms to test whether an AI displays the relevant behaviors.","If the first two steps are successfully completed, and if the AI passes the tests with human-like results, this constitutes evidence that this AI and humans can genuinely communicate.","This mental-behavioral methodology has the advantage that we don't need to understand the workings of black-box algorithms, such as standard deep neural networks.","This is comparable to the fact that we don't need to understand how human brains work to know that humans can genuinely communicate.","This methodology also has its disadvantages and I will discuss some of them (section 6)."],"url":"http://arxiv.org/abs/2402.09494v1","category":"cs.HC"}
{"created":"2024-02-15 18:55:41","title":"Chain-of-Thought Reasoning Without Prompting","abstract":"In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \\textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding substantially outperforms the standard greedy decoding.","sentences":["In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting.","These methods, while effective, often involve manually intensive prompt engineering.","Our study takes a novel approach by asking: Can LLMs reason effectively without prompting?","Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \\textit{decoding} process.","Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences.","This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \\textit{intrinsic} reasoning abilities.","Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer.","This confidence metric effectively differentiates between CoT and non-CoT paths.","Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding substantially outperforms the standard greedy decoding."],"url":"http://arxiv.org/abs/2402.10200v1","category":"cs.CL"}
{"created":"2024-02-15 17:58:29","title":"ControlLM: Crafting Diverse Personalities for Language Models","abstract":"As language models continue to scale in size and capability, they display an array of emerging behaviors, both beneficial and concerning. This heightens the need to control model behaviors. We hope to be able to control the personality traits of language models at the inference-time so as to have various character features, on top of which the requirements of different types of tasks can be met. Personality is a higher-level and more abstract behavioral representation for language models. We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference. This approach allows for the precise, real-time adjustment of model behavior. First, we demonstrate ControlLM's capacity to elicit diverse persona behaviors without any training, while precision control allows personality traits to closely match average human values. Subsequently, we showcase improved reasoning and question answering through selective amplification of beneficial attributes like conscientiousness and friendliness. We hope that this work will inspire research on controlling human-like behaviors of language models and provide insights for future research. Our code is publicly available at: https://github.com/wengsyx/ControlLM.","sentences":["As language models continue to scale in size and capability, they display an array of emerging behaviors, both beneficial and concerning.","This heightens the need to control model behaviors.","We hope to be able to control the personality traits of language models at the inference-time so as to have various character features, on top of which the requirements of different types of tasks can be met.","Personality is a higher-level and more abstract behavioral representation for language models.","We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference.","This approach allows for the precise, real-time adjustment of model behavior.","First, we demonstrate ControlLM's capacity to elicit diverse persona behaviors without any training, while precision control allows personality traits to closely match average human values.","Subsequently, we showcase improved reasoning and question answering through selective amplification of beneficial attributes like conscientiousness and friendliness.","We hope that this work will inspire research on controlling human-like behaviors of language models and provide insights for future research.","Our code is publicly available at: https://github.com/wengsyx/ControlLM."],"url":"http://arxiv.org/abs/2402.10151v1","category":"cs.CL"}
{"created":"2024-02-15 17:55:04","title":"Reconstruction of Short-Lived Particles using Graph-Hypergraph Representation Learning","abstract":"In collider experiments, the kinematic reconstruction of heavy, short-lived particles is vital for precision tests of the Standard Model and in searches for physics beyond it. Performing kinematic reconstruction in collider events with many final-state jets, such as the all-hadronic decay of topantitop quark pairs, is challenging. We present HyPER, a graph neural network that uses blended graph-hypergraph representation learning to reconstruct parent particles from sets of final-state objects. HyPER is tested on simulation and shown to perform favorably when compared to existing state-of-the-art reconstruction techniques, while demonstrating superior parameter efficiency. The novel hypergraph approach allows the method to be applied to particle reconstruction in a multitude of different physics processes.","sentences":["In collider experiments, the kinematic reconstruction of heavy, short-lived particles is vital for precision tests of the Standard Model and in searches for physics beyond it.","Performing kinematic reconstruction in collider events with many final-state jets, such as the all-hadronic decay of topantitop quark pairs, is challenging.","We present HyPER, a graph neural network that uses blended graph-hypergraph representation learning to reconstruct parent particles from sets of final-state objects.","HyPER is tested on simulation and shown to perform favorably when compared to existing state-of-the-art reconstruction techniques, while demonstrating superior parameter efficiency.","The novel hypergraph approach allows the method to be applied to particle reconstruction in a multitude of different physics processes."],"url":"http://arxiv.org/abs/2402.10149v1","category":"hep-ph"}
{"created":"2024-02-15 17:49:50","title":"A chaotic maps-based privacy-preserving distributed deep learning for incomplete and Non-IID datasets","abstract":"Federated Learning is a machine learning approach that enables the training of a deep learning model among several participants with sensitive data that wish to share their own knowledge without compromising the privacy of their data. In this research, the authors employ a secured Federated Learning method with an additional layer of privacy and proposes a method for addressing the non-IID challenge. Moreover, differential privacy is compared with chaotic-based encryption as layer of privacy. The experimental approach assesses the performance of the federated deep learning model with differential privacy using both IID and non-IID data. In each experiment, the Federated Learning process improves the average performance metrics of the deep neural network, even in the case of non-IID data.","sentences":["Federated Learning is a machine learning approach that enables the training of a deep learning model among several participants with sensitive data that wish to share their own knowledge without compromising the privacy of their data.","In this research, the authors employ a secured Federated Learning method with an additional layer of privacy and proposes a method for addressing the non-IID challenge.","Moreover, differential privacy is compared with chaotic-based encryption as layer of privacy.","The experimental approach assesses the performance of the federated deep learning model with differential privacy using both IID and non-IID data.","In each experiment, the Federated Learning process improves the average performance metrics of the deep neural network, even in the case of non-IID data."],"url":"http://arxiv.org/abs/2402.10145v1","category":"cs.LG"}
{"created":"2024-02-15 17:07:52","title":"A free boundary Monge-Amp\u00e8re equation and applications to complete Calabi-Yau metrics","abstract":"Let $P$ be a convex body containing the origin in its interior. We study a real Monge-Amp\\`ere equation with singularities along $\\del P$ which is Legendre dual to a certain free boundary Monge-Amp\\`ere equation. This is motivated by the existence problem for complete Calabi-Yau metrics on log Calabi-Yau pairs $(X, D)$ with $D$ an ample, simple normal crossings divisor. We prove the existence of solutions in $C^{\\infty}(P)\\cap C^{1,\\alpha}(\\overline{P})$, and establish the strict convexity of the free boundary. When $P$ is a polytope, we obtain an asymptotic expansion for the solution near the interior of the codimension $1$ faces of $\\del P$.","sentences":["Let $P$ be a convex body containing the origin in its interior.","We study a real Monge-Amp\\`ere equation with singularities along $\\del P$ which is Legendre dual to a certain free boundary Monge-Amp\\`ere equation.","This is motivated by the existence problem for complete Calabi-Yau metrics on log Calabi-Yau pairs $(X, D)$ with $D$ an ample, simple normal crossings divisor.","We prove the existence of solutions in $C^{\\infty}(P)\\cap C^{1,\\alpha}(\\overline{P})$, and establish the strict convexity of the free boundary.","When $P$ is a polytope, we obtain an asymptotic expansion for the solution near the interior of the codimension $1$ faces of $\\del P$."],"url":"http://arxiv.org/abs/2402.10111v1","category":"math.DG"}
{"created":"2024-02-15 17:01:53","title":"Hearing Exotic Smooth Structures: Basic Spectra of Equivariant Manifolds","abstract":"This paper explores the existence and properties of \\emph{basic} eigenvalues and eigenfunctions associated with the Riemannian Laplacian on closed, connected Riemannian manifolds featuring an effective isometric action by a compact Lie group. We introduce the concept of equivariant isospectrality, asserting that two Riemannian manifolds with isometric actions by the same Lie group are equivariantly isospectral if their basic spectra coincide. Our primary focus is investigating the potential existence of homeomorphic yet not diffeomorphic smooth manifolds that can accommodate invariant, equivariantly isospectral metrics. We establish the occurrence of such scenarios for specific homotopy spheres and connected sums. Moreover, the developed theory demonstrates that the ring of invariant admissible scalar curvature functions on certain fixed smooth manifolds fails to distinguish between smooth structures. This implies the existence of homotopy spheres with identical rings of invariant scalar curvature functions, irrespective of the underlying smooth structure.","sentences":["This paper explores the existence and properties of \\emph{basic} eigenvalues and eigenfunctions associated with the Riemannian Laplacian on closed, connected Riemannian manifolds featuring an effective isometric action by a compact Lie group.","We introduce the concept of equivariant isospectrality, asserting that two Riemannian manifolds with isometric actions by the same Lie group are equivariantly isospectral if their basic spectra coincide.","Our primary focus is investigating the potential existence of homeomorphic yet not diffeomorphic smooth manifolds that can accommodate invariant, equivariantly isospectral metrics.","We establish the occurrence of such scenarios for specific homotopy spheres and connected sums.","Moreover, the developed theory demonstrates that the ring of invariant admissible scalar curvature functions on certain fixed smooth manifolds fails to distinguish between smooth structures.","This implies the existence of homotopy spheres with identical rings of invariant scalar curvature functions, irrespective of the underlying smooth structure."],"url":"http://arxiv.org/abs/2402.10106v1","category":"math.DG"}
{"created":"2024-02-15 17:00:05","title":"Pointwise convergence of the Klein-Gordon flow","abstract":"We consider the PDEs version of the Carleson problem in the context of the cubic nonlinear Klein-Gordon equation. This means that we aim to establish the lowest regularity class for which one has almost everywhere pointwise convergence of the solutions to the initial data, as $t \\to 0$. We prove sharp results for initial data in Sobolev spaces and for their randomized counterparts.","sentences":["We consider the PDEs version of the Carleson problem in the context of the cubic nonlinear Klein-Gordon equation.","This means that we aim to establish the lowest regularity class for which one has almost everywhere pointwise convergence of the solutions to the initial data, as $t \\to 0$.","We prove sharp results for initial data in Sobolev spaces and for their randomized counterparts."],"url":"http://arxiv.org/abs/2402.10105v1","category":"math.AP"}
{"created":"2024-02-15 16:37:14","title":"GraphCBAL: Class-Balanced Active Learning for Graph Neural Networks via Reinforcement Learning","abstract":"Graph neural networks (GNNs) have recently demonstrated significant success. Active learning for GNNs aims to query the valuable samples from the unlabeled data for annotation to maximize the GNNs' performance at a low cost. However, most existing methods for reinforced active learning in GNNs may lead to a highly imbalanced class distribution, especially in highly skewed class scenarios. This further adversely affects the classification performance. To tackle this issue, in this paper, we propose a novel reinforced class-balanced active learning framework for GNNs, namely, GraphCBAL. It learns an optimal policy to acquire class-balanced and informative nodes for annotation, maximizing the performance of GNNs trained with selected labeled nodes. GraphCBAL designs class-balance-aware states, as well as a reward function that achieves trade-off between model performance and class balance. We further upgrade GraphCBAL to GraphCBAL++ by introducing a punishment mechanism to obtain a more class-balanced labeled set. Extensive experiments on multiple datasets demonstrate the effectiveness of the proposed approaches, achieving superior performance over state-of-the-art baselines. In particular, our methods can strike the balance between classification results and class balance.","sentences":["Graph neural networks (GNNs) have recently demonstrated significant success.","Active learning for GNNs aims to query the valuable samples from the unlabeled data for annotation to maximize the GNNs' performance at a low cost.","However, most existing methods for reinforced active learning in GNNs may lead to a highly imbalanced class distribution, especially in highly skewed class scenarios.","This further adversely affects the classification performance.","To tackle this issue, in this paper, we propose a novel reinforced class-balanced active learning framework for GNNs, namely, GraphCBAL.","It learns an optimal policy to acquire class-balanced and informative nodes for annotation, maximizing the performance of GNNs trained with selected labeled nodes.","GraphCBAL designs class-balance-aware states, as well as a reward function that achieves trade-off between model performance and class balance.","We further upgrade GraphCBAL to GraphCBAL++ by introducing a punishment mechanism to obtain a more class-balanced labeled set.","Extensive experiments on multiple datasets demonstrate the effectiveness of the proposed approaches, achieving superior performance over state-of-the-art baselines.","In particular, our methods can strike the balance between classification results and class balance."],"url":"http://arxiv.org/abs/2402.10074v1","category":"cs.LG"}
{"created":"2024-02-15 16:02:48","title":"Geometric theory of perturbation dynamics around non-equilibrium fluid flows","abstract":"The present work investigates the evolution of linear perturbations of time-dependent ideal fluid flows with advected quantities, expressed in terms of the second order variations of the action corresponding to a Lagrangian defined on a semidirect product space. This approach is related to Jacobi fields along geodesics and several examples are given explicitly to elucidate our approach. Numerical simulations of the perturbation dynamics are also presented.","sentences":["The present work investigates the evolution of linear perturbations of time-dependent ideal fluid flows with advected quantities, expressed in terms of the second order variations of the action corresponding to a Lagrangian defined on a semidirect product space.","This approach is related to Jacobi fields along geodesics and several examples are given explicitly to elucidate our approach.","Numerical simulations of the perturbation dynamics are also presented."],"url":"http://arxiv.org/abs/2402.10040v1","category":"physics.flu-dyn"}
{"created":"2024-02-15 15:18:06","title":"Enhancing signal detectability in learning-based CT reconstruction with a model observer inspired loss function","abstract":"Deep neural networks used for reconstructing sparse-view CT data are typically trained by minimizing a pixel-wise mean-squared error or similar loss function over a set of training images. However, networks trained with such pixel-wise losses are prone to wipe out small, low-contrast features that are critical for screening and diagnosis. To remedy this issue, we introduce a novel training loss inspired by the model observer framework to enhance the detectability of weak signals in the reconstructions. We evaluate our approach on the reconstruction of synthetic sparse-view breast CT data, and demonstrate an improvement in signal detectability with the proposed loss.","sentences":["Deep neural networks used for reconstructing sparse-view CT data are typically trained by minimizing a pixel-wise mean-squared error or similar loss function over a set of training images.","However, networks trained with such pixel-wise losses are prone to wipe out small, low-contrast features that are critical for screening and diagnosis.","To remedy this issue, we introduce a novel training loss inspired by the model observer framework to enhance the detectability of weak signals in the reconstructions.","We evaluate our approach on the reconstruction of synthetic sparse-view breast CT data, and demonstrate an improvement in signal detectability with the proposed loss."],"url":"http://arxiv.org/abs/2402.10010v1","category":"physics.med-ph"}
{"created":"2024-02-15 15:14:17","title":"Globally solvable time-periodic evolution equations in Gelfand-Shilov classes","abstract":"In this paper we consider a class of evolution operators with coefficients depending on time and space variables $(t,x) \\in \\mathbb{T} \\times \\mathbb{R}^n$, where $\\mathbb{T}$ is the one-dimensional torus and prove necessary and sufficient conditions for their global solvability in (time-periodic) Gelfand-Shilov spaces. The argument of the proof is based on a characterization of these spaces in terms of the eigenfunction expansions given by a fixed self-adjoint, globally elliptic differential operator on $\\mathbb{R}^n$.","sentences":["In this paper we consider a class of evolution operators with coefficients depending on time and space variables $(t,x) \\in \\mathbb{T} \\times \\mathbb{R}^n$, where $\\mathbb{T}$ is the one-dimensional torus and prove necessary and sufficient conditions for their global solvability in (time-periodic) Gelfand-Shilov spaces.","The argument of the proof is based on a characterization of these spaces in terms of the eigenfunction expansions given by a fixed self-adjoint, globally elliptic differential operator on $\\mathbb{R}^n$."],"url":"http://arxiv.org/abs/2402.10006v1","category":"math.AP"}
{"created":"2024-02-15 14:44:05","title":"Do Rogue Wave Exist in the Kadomtesv-Petviashivili Equation ?","abstract":"There is considerable fundamental theoretical and applicative interest in obtaining two-dimensional rogue wave similar to one-dimensional rogue wave of the nonlinear Schr\\\"odinger equation. Here, we first time proposes a self-mapping transformation and analytically predict the existence of a family of novel spatio-temporal rogue wave solutions for the Kadomtesv-Petviashivili equation. We discover that these spatio-temporal rogue waves showing a strong analogy characteristics of the short-lives with rogue waves of the NLS equation. Our fingdings can also provide a solid mathematical basis for theory and application in shallow water, plasma and optics. This technique could be available to construct rogue-like waves of (2+1)-dimensional nonlinear wave models. Also, these studies could be helpful to deepen our understandings and enrich our knowledge about rogue waves.","sentences":["There is considerable fundamental theoretical and applicative interest in obtaining two-dimensional rogue wave similar to one-dimensional rogue wave of the nonlinear Schr\\\"odinger equation.","Here, we first time proposes a self-mapping transformation and analytically predict the existence of a family of novel spatio-temporal rogue wave solutions for the Kadomtesv-Petviashivili equation.","We discover that these spatio-temporal rogue waves showing a strong analogy characteristics of the short-lives with rogue waves of the NLS equation.","Our fingdings can also provide a solid mathematical basis for theory and application in shallow water, plasma and optics.","This technique could be available to construct rogue-like waves of (2+1)-dimensional nonlinear wave models.","Also, these studies could be helpful to deepen our understandings and enrich our knowledge about rogue waves."],"url":"http://arxiv.org/abs/2402.09980v1","category":"nlin.PS"}
{"created":"2024-02-15 14:22:34","title":"Some counting formulas for $\u03bb$-quiddities over the rings $\\mathbb{Z}/2^{m}\\mathbb{Z}$","abstract":"The $\\lambda$-quiddities of size $n$ are $n$-tuples of elements of a fixed set, solutions of a matrix equation appearing in the study of Coxeter's friezes. Their number and their properties are closely linked to the structure and the cardinality of the chosen set. The main objective of this text is to obtain an explicit formula giving the number of $\\lambda$-quiddities of odd size, and a lower and upper bound for the number of $\\lambda$-quiddities of even size, over the rings $\\mathbb{Z}/2^{m}\\mathbb{Z}$ ($m \\geq 2$). We also give explicit formulas concerning the number of $\\lambda$-quiddities of size $n$ over $\\mathbb{Z}/8\\mathbb{Z}$.","sentences":["The $\\lambda$-quiddities of size $n$ are $n$-tuples of elements of a fixed set, solutions of a matrix equation appearing in the study of Coxeter's friezes.","Their number and their properties are closely linked to the structure and the cardinality of the chosen set.","The main objective of this text is to obtain an explicit formula giving the number of $\\lambda$-quiddities of odd size, and a lower and upper bound for the number of $\\lambda$-quiddities of even size, over the rings $\\mathbb{Z}/2^{m}\\mathbb{Z}$ ($m \\geq 2$).","We also give explicit formulas concerning the number of $\\lambda$-quiddities of size $n$ over $\\mathbb{Z}/8\\mathbb{Z}$."],"url":"http://arxiv.org/abs/2402.09968v1","category":"math.CO"}
{"created":"2024-02-15 13:49:14","title":"What can we learn from nonequilibrium response of a strange metal?","abstract":"We critically address the recent experiment [Science 382, 907 (2023)] on nonequilibrium transport and noise in a strange metal YbRh2Si2 patterned into the nanowire shape. In the long device, resistivity, differential resistance and current noise data seem to be consistent allowing us to extract electron-phonon coupling and the temperature dependence of electron-phonon scattering length. The obtained values can be reconciled with the experimental data for the short device only assuming the significant contact resistance. We discuss its possible origin as due to the current redistribution between YbRh2Si2 and its gold covering, and reveal that this redistribution contact resistance should be proportional to the YbRh2Si2 resistivity. We also discuss some subtleties of the noise measurements. Overall, neglecting electron-phonon energy relaxation even in the shortest devices is arguable so that the observed shot noise suppression can hardly be attributed to the failure of quasiparticle concept.","sentences":["We critically address the recent experiment [Science 382, 907 (2023)] on nonequilibrium transport and noise in a strange metal YbRh2Si2 patterned into the nanowire shape.","In the long device, resistivity, differential resistance and current noise data seem to be consistent allowing us to extract electron-phonon coupling and the temperature dependence of electron-phonon scattering length.","The obtained values can be reconciled with the experimental data for the short device only assuming the significant contact resistance.","We discuss its possible origin as due to the current redistribution between YbRh2Si2 and its gold covering, and reveal that this redistribution contact resistance should be proportional to the YbRh2Si2 resistivity.","We also discuss some subtleties of the noise measurements.","Overall, neglecting electron-phonon energy relaxation even in the shortest devices is arguable so that the observed shot noise suppression can hardly be attributed to the failure of quasiparticle concept."],"url":"http://arxiv.org/abs/2402.09946v1","category":"cond-mat.str-el"}
{"created":"2024-02-15 13:44:07","title":"Pinching Dynamics of Thin Films of Binary Mixtures","abstract":"In binary mixtures, the lifetimes of surface bubbles can be five orders of magnitude longer than those in pure liquids because of slightly different compositions of the bulk and the surfaces, leading to a thickness-dependent surface tension of thin films. Taking profit of the resulting simple surface rheology, we derive the equations describing the thickness, flow velocity and surface tension of a single liquid film. Numerical resolution shows that, after a first step of tension equilibration, a parabolic flow with mobile interfaces is associated with film pinching in a further drainage step. Our model paves the way for a better understanding of the rupture dynamics of liquid films.","sentences":["In binary mixtures, the lifetimes of surface bubbles can be five orders of magnitude longer than those in pure liquids because of slightly different compositions of the bulk and the surfaces, leading to a thickness-dependent surface tension of thin films.","Taking profit of the resulting simple surface rheology, we derive the equations describing the thickness, flow velocity and surface tension of a single liquid film.","Numerical resolution shows that, after a first step of tension equilibration, a parabolic flow with mobile interfaces is associated with film pinching in a further drainage step.","Our model paves the way for a better understanding of the rupture dynamics of liquid films."],"url":"http://arxiv.org/abs/2402.09942v1","category":"cond-mat.soft"}
{"created":"2024-02-15 10:59:43","title":"Equational theories of idempotent semifields","abstract":"This paper provides answers to several open problems about equational theories of idempotent semifields. In particular, it is proved that (i) no equational theory of a non-trivial class of idempotent semifields has a finite basis; (ii) there are continuum-many equational theories of classes of idempotent semifields; and (iii) the equational theory of the class of idempotent semifields is co-NP-complete.","sentences":["This paper provides answers to several open problems about equational theories of idempotent semifields.","In particular, it is proved that (i) no equational theory of a non-trivial class of idempotent semifields has a finite basis; (ii) there are continuum-many equational theories of classes of idempotent semifields; and (iii) the equational theory of the class of idempotent semifields is co-NP-complete."],"url":"http://arxiv.org/abs/2402.09876v1","category":"math.LO"}
{"created":"2024-02-15 10:25:55","title":"Neutron Stars constraints on a late G transition","abstract":"It has been suggested recently that the Hubble tension could be eliminated by a sharp, $\\sim 10\\%$ increase of the effective gravitational constant at $z \\sim 0.01$. This would decrease the luminosities of type 1a supernovae in just the needed amount to explain the larger value of the Hubble parameter. In the present paper we call attention to a dramatic effect of such transition on neutron stars. A neutron star that existed at $z=0.01$ would contract, conserving the baryon mass but undergoing a mass reduction. We computed neutron star models, with a realistic equation of state, and obtained that this reduction is typically $ 0.04 M_{\\odot}$. This amounts to an energy of $7 \\times 10^{52}$ erg. The transition will affect {\\it all} neutron stars that formed along the history of each galaxy prior to the transition. Given the large number of neutron stars per galaxy, the liberated energy is huge. An estimate of the expected fluxes of neutrinos and x-rays yields values exceeding observational upper limits, thus rendering the late G transition scenario non-viable.","sentences":["It has been suggested recently that the Hubble tension could be eliminated by a sharp, $\\sim 10\\%$ increase of the effective gravitational constant at $z \\sim 0.01$.","This would decrease the luminosities of type 1a supernovae in just the needed amount to explain the larger value of the Hubble parameter.","In the present paper we call attention to a dramatic effect of such transition on neutron stars.","A neutron star that existed at $z=0.01$ would contract, conserving the baryon mass but undergoing a mass reduction.","We computed neutron star models, with a realistic equation of state, and obtained that this reduction is typically $ 0.04 M_{\\odot}$.","This amounts to an energy of $7 \\times 10^{52}$ erg.","The transition will affect {\\it all} neutron stars that formed along the history of each galaxy prior to the transition.","Given the large number of neutron stars per galaxy, the liberated energy is huge.","An estimate of the expected fluxes of neutrinos and x-rays yields values exceeding observational upper limits, thus rendering the late G transition scenario non-viable."],"url":"http://arxiv.org/abs/2402.09859v1","category":"astro-ph.CO"}
{"created":"2024-02-15 10:23:48","title":"An Approximation Based Theory of Linear Regression","abstract":"The goal of this paper is to provide a theory linear regression based entirely on approximations. It will be argued that the standard linear regression model based theory whether frequentist or Bayesian has failed and that this failure is due to an 'assumed (revealed?) truth' (John Tukey) attitude to the models. This is reflected in the language of statistical inference which involves a concept of truth, for example efficiency, consistency and hypothesis testing. The motivation behind this paper was to remove the word `true' from the theory and practice of linear regression and to replace it by approximation. The approximations considered are the least squares approximations. An approximation is called valid if it contains no irrelevant covariates. This is operationalized using the concept of a Gaussian P-value which is the probability that pure Gaussian noise is better in term of least squares than the covariate. The precise definition given in the paper is intuitive and requires only four simple equations. Given this a valid approximation is one where all the Gaussian P-values are less than a threshold $p0$ specified by the statistician, in this paper with the default value 0.01. This approximations approach is not only much simpler it is overwhelmingly better than the standard model based approach. This will be demonstrated using six real data sets, four from high dimensional regression and two from vector autoregression. Both the simplicity and the superiority of Gaussian P-values derive from their universal exactness and validity. This is in complete contrast to standard F P-values which are valid only for carefully designed simulations.   The paper contains excerpts from an unpublished paper by John Tukey entitled `Issues relevant to an honest account of data-based inference partially in the light of Laurie Davies's paper'.","sentences":["The goal of this paper is to provide a theory linear regression based entirely on approximations.","It will be argued that the standard linear regression model based theory whether frequentist or Bayesian has failed and that this failure is due to an 'assumed (revealed?) truth' (John Tukey) attitude to the models.","This is reflected in the language of statistical inference which involves a concept of truth, for example efficiency, consistency and hypothesis testing.","The motivation behind this paper was to remove the word `true' from the theory and practice of linear regression and to replace it by approximation.","The approximations considered are the least squares approximations.","An approximation is called valid if it contains no irrelevant covariates.","This is operationalized using the concept of a Gaussian P-value which is the probability that pure Gaussian noise is better in term of least squares than the covariate.","The precise definition given in the paper is intuitive and requires only four simple equations.","Given this a valid approximation is one where all the Gaussian P-values are less than a threshold $p0$ specified by the statistician, in this paper with the default value 0.01.","This approximations approach is not only much simpler it is overwhelmingly better than the standard model based approach.","This will be demonstrated using six real data sets, four from high dimensional regression and two from vector autoregression.","Both the simplicity and the superiority of Gaussian P-values derive from their universal exactness and validity.","This is in complete contrast to standard F P-values which are valid only for carefully designed simulations.   ","The paper contains excerpts from an unpublished paper by John Tukey entitled `Issues relevant to an honest account of data-based inference partially in the light of Laurie Davies's paper'."],"url":"http://arxiv.org/abs/2402.09858v1","category":"stat.ME"}
{"created":"2024-02-15 09:45:41","title":"A counterexample to the Pellian equation conjecture of Mordell","abstract":"Let $d\\geq 2$ be a squarefree integer, let $\\omega\\in\\{\\sqrt{d},\\frac{1+\\sqrt{d}}{2}\\}$ be such that $\\mathbb{Z}[\\omega]$ is the ring of algebraic integers of the real quadratic number field $\\mathbb{Q}(\\sqrt{d})$, let $\\varepsilon>1$ be the fundamental unit of $\\mathbb{Z}[\\omega]$ and let $x$ and $y$ be the unique nonnegative integers with $\\varepsilon=x+y\\omega$. In this note, we extend and study the list of known squarefree integers $d\\geq 2$, for which $y$ is divisible by $d$ (cf. OEIS A135735). As a byproduct, we present a counterexample to a conjecture of L. J. Mordell.","sentences":["Let $d\\geq 2$ be a squarefree integer, let $\\omega\\in\\{\\sqrt{d},\\frac{1+\\sqrt{d}}{2}\\}$ be such that $\\mathbb{Z}[\\omega]$ is the ring of algebraic integers of the real quadratic number field $\\mathbb{Q}(\\sqrt{d})$, let $\\varepsilon>1$ be the fundamental unit of $\\mathbb{Z}[\\omega]$ and let $x$ and $y$ be the unique nonnegative integers with $\\varepsilon=x+y\\omega$.","In this note, we extend and study the list of known squarefree integers $d\\geq 2$, for which $y$ is divisible by $d$ (cf. OEIS A135735).","As a byproduct, we present a counterexample to a conjecture of L. J. Mordell."],"url":"http://arxiv.org/abs/2402.09827v1","category":"math.NT"}
{"created":"2024-02-15 08:54:46","title":"Large-scale Benchmarking of Metaphor-based Optimization Heuristics","abstract":"The number of proposed iterative optimization heuristics is growing steadily, and with this growth, there have been many points of discussion within the wider community. One particular criticism that is raised towards many new algorithms is their focus on metaphors used to present the method, rather than emphasizing their potential algorithmic contributions. Several studies into popular metaphor-based algorithms have highlighted these problems, even showcasing algorithms that are functionally equivalent to older existing methods. Unfortunately, this detailed approach is not scalable to the whole set of metaphor-based algorithms. Because of this, we investigate ways in which benchmarking can shed light on these algorithms. To this end, we run a set of 294 algorithm implementations on the BBOB function suite. We investigate how the choice of the budget, the performance measure, or other aspects of experimental design impact the comparison of these algorithms. Our results emphasize why benchmarking is a key step in expanding our understanding of the algorithm space, and what challenges still need to be overcome to fully gauge the potential improvements to the state-of-the-art hiding behind the metaphors.","sentences":["The number of proposed iterative optimization heuristics is growing steadily, and with this growth, there have been many points of discussion within the wider community.","One particular criticism that is raised towards many new algorithms is their focus on metaphors used to present the method, rather than emphasizing their potential algorithmic contributions.","Several studies into popular metaphor-based algorithms have highlighted these problems, even showcasing algorithms that are functionally equivalent to older existing methods.","Unfortunately, this detailed approach is not scalable to the whole set of metaphor-based algorithms.","Because of this, we investigate ways in which benchmarking can shed light on these algorithms.","To this end, we run a set of 294 algorithm implementations on the BBOB function suite.","We investigate how the choice of the budget, the performance measure, or other aspects of experimental design impact the comparison of these algorithms.","Our results emphasize why benchmarking is a key step in expanding our understanding of the algorithm space, and what challenges still need to be overcome to fully gauge the potential improvements to the state-of-the-art hiding behind the metaphors."],"url":"http://arxiv.org/abs/2402.09800v1","category":"cs.NE"}
{"created":"2024-02-15 07:23:34","title":"A Framework For Gait-Based User Demography Estimation Using Inertial Sensors","abstract":"Human gait has been shown to provide crucial motion cues for various applications. Recognizing patterns in human gait has been widely adopted in various application areas such as security, virtual reality gaming, medical rehabilitation, and ailment identification. Furthermore, wearable inertial sensors have been widely used for not only recording gait but also to predict users' demography. Machine Learning techniques such as deep learning, combined with inertial sensor signals, have shown promising results in recognizing patterns in human gait and estimate users' demography. However, the black-box nature of such deep learning models hinders the researchers from uncovering the reasons behind the model's predictions. Therefore, we propose leveraging deep learning and Layer-Wise Relevance Propagation (LRP) to identify the important variables that play a vital role in identifying the users' demography such as age and gender. To assess the efficacy of this approach we train a deep neural network model on a large sensor-based gait dataset consisting of 745 subjects to identify users' age and gender. Using LRP we identify the variables relevant for characterizing the gait patterns. Thus, we enable interpretation of non-linear ML models which are experts in identifying the users' demography based on inertial signals. We believe this approach can not only provide clinicians information about the gait parameters relevant to age and gender but also can be expanded to analyze and diagnose gait disorders.","sentences":["Human gait has been shown to provide crucial motion cues for various applications.","Recognizing patterns in human gait has been widely adopted in various application areas such as security, virtual reality gaming, medical rehabilitation, and ailment identification.","Furthermore, wearable inertial sensors have been widely used for not only recording gait but also to predict users' demography.","Machine Learning techniques such as deep learning, combined with inertial sensor signals, have shown promising results in recognizing patterns in human gait and estimate users' demography.","However, the black-box nature of such deep learning models hinders the researchers from uncovering the reasons behind the model's predictions.","Therefore, we propose leveraging deep learning and Layer-Wise Relevance Propagation (LRP) to identify the important variables that play a vital role in identifying the users' demography such as age and gender.","To assess the efficacy of this approach we train a deep neural network model on a large sensor-based gait dataset consisting of 745 subjects to identify users' age and gender.","Using LRP we identify the variables relevant for characterizing the gait patterns.","Thus, we enable interpretation of non-linear ML models which are experts in identifying the users' demography based on inertial signals.","We believe this approach can not only provide clinicians information about the gait parameters relevant to age and gender but also can be expanded to analyze and diagnose gait disorders."],"url":"http://arxiv.org/abs/2402.09761v1","category":"cs.HC"}
{"created":"2024-02-15 06:08:26","title":"POBEVM: Real-time Video Matting via Progressively Optimize the Target Body and Edge","abstract":"Deep convolutional neural networks (CNNs) based approaches have achieved great performance in video matting. Many of these methods can produce accurate alpha estimation for the target body but typically yield fuzzy or incorrect target edges. This is usually caused by the following reasons: 1) The current methods always treat the target body and edge indiscriminately; 2) Target body dominates the whole target with only a tiny proportion target edge. For the first problem, we propose a CNN-based module that separately optimizes the matting target body and edge (SOBE). And on this basis, we introduce a real-time, trimap-free video matting method via progressively optimizing the matting target body and edge (POBEVM) that is much lighter than previous approaches and achieves significant improvements in the predicted target edge. For the second problem, we propose an Edge-L1-Loss (ELL) function that enforces our network on the matting target edge. Experiments demonstrate our method outperforms prior trimap-free matting methods on both Distinctions-646 (D646) and VideoMatte240K(VM) dataset, especially in edge optimization.","sentences":["Deep convolutional neural networks (CNNs) based approaches have achieved great performance in video matting.","Many of these methods can produce accurate alpha estimation for the target body but typically yield fuzzy or incorrect target edges.","This is usually caused by the following reasons: 1)","The current methods always treat the target body and edge indiscriminately; 2) Target body dominates the whole target with only a tiny proportion target edge.","For the first problem, we propose a CNN-based module that separately optimizes the matting target body and edge (SOBE).","And on this basis, we introduce a real-time, trimap-free video matting method via progressively optimizing the matting target body and edge (POBEVM) that is much lighter than previous approaches and achieves significant improvements in the predicted target edge.","For the second problem, we propose an Edge-L1-Loss (ELL) function that enforces our network on the matting target edge.","Experiments demonstrate our method outperforms prior trimap-free matting methods on both Distinctions-646 (D646) and VideoMatte240K(VM) dataset, especially in edge optimization."],"url":"http://arxiv.org/abs/2402.09731v1","category":"cs.CV"}
{"created":"2024-02-15 05:19:53","title":"DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated Learning as a Service","abstract":"Federated learning (FL) has emerged as a prevalent distributed machine learning scheme that enables collaborative model training without aggregating raw data. Cloud service providers further embrace Federated Learning as a Service (FLaaS), allowing data analysts to execute their FL training pipelines over differentially-protected data. Due to the intrinsic properties of differential privacy, the enforced privacy level on data blocks can be viewed as a privacy budget that requires careful scheduling to cater to diverse training pipelines. Existing privacy budget scheduling studies prioritize either efficiency or fairness individually. In this paper, we propose DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes both efficiency and fairness. We first develop a comprehensive utility function incorporating data analyst-level dominant shares and FL-specific performance metrics. A sequential allocation mechanism is then designed using the Lagrange multiplier method and effective greedy heuristics. We theoretically prove that DPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and Weak Strategy Proofness. We also theoretically prove the existence of a fairness-efficiency tradeoff in privacy budgeting. Extensive experiments demonstrate that DPBalance outperforms state-of-the-art solutions, achieving an average efficiency improvement of $1.44\\times \\sim 3.49 \\times$, and an average fairness improvement of $1.37\\times \\sim 24.32 \\times$.","sentences":["Federated learning (FL) has emerged as a prevalent distributed machine learning scheme that enables collaborative model training without aggregating raw data.","Cloud service providers further embrace Federated Learning as a Service (FLaaS), allowing data analysts to execute their FL training pipelines over differentially-protected data.","Due to the intrinsic properties of differential privacy, the enforced privacy level on data blocks can be viewed as a privacy budget that requires careful scheduling to cater to diverse training pipelines.","Existing privacy budget scheduling studies prioritize either efficiency or fairness individually.","In this paper, we propose DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes both efficiency and fairness.","We first develop a comprehensive utility function incorporating data analyst-level dominant shares and FL-specific performance metrics.","A sequential allocation mechanism is then designed using the Lagrange multiplier method and effective greedy heuristics.","We theoretically prove that DPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and Weak Strategy Proofness.","We also theoretically prove the existence of a fairness-efficiency tradeoff in privacy budgeting.","Extensive experiments demonstrate that DPBalance outperforms state-of-the-art solutions, achieving an average efficiency improvement of $1.44\\times \\sim 3.49 \\times$, and an average fairness improvement of $1.37\\times \\sim 24.32 \\times$."],"url":"http://arxiv.org/abs/2402.09715v1","category":"cs.DC"}
{"created":"2024-02-15 01:34:52","title":"Classical solutions to a BGK-type model relaxing to the isentropic gas dynamics","abstract":"In this paper, we consider a BGK-type kinetic model relaxing to the isentropic gas dynamics in the hydrodynamic limit. We introduce a linearization of the equation around the global equilibrium. Then we prove the global existence of classical solutions with an exponential convergence rate toward the equilibrium state in the periodic domain when the initial data is a small perturbation of the global equilibrium.","sentences":["In this paper, we consider a BGK-type kinetic model relaxing to the isentropic gas dynamics in the hydrodynamic limit.","We introduce a linearization of the equation around the global equilibrium.","Then we prove the global existence of classical solutions with an exponential convergence rate toward the equilibrium state in the periodic domain when the initial data is a small perturbation of the global equilibrium."],"url":"http://arxiv.org/abs/2402.09653v1","category":"math.AP"}
{"created":"2024-02-15 01:25:19","title":"Foul prediction with estimated poses from soccer broadcast video","abstract":"Recent advances in computer vision have made significant progress in tracking and pose estimation of sports players. However, there have been fewer studies on behavior prediction with pose estimation in sports, in particular, the prediction of soccer fouls is challenging because of the smaller image size of each player and of difficulty in the usage of e.g., the ball and pose information. In our research, we introduce an innovative deep learning approach for anticipating soccer fouls. This method integrates video data, bounding box positions, image details, and pose information by curating a novel soccer foul dataset. Our model utilizes a combination of convolutional and recurrent neural networks (CNNs and RNNs) to effectively merge information from these four modalities. The experimental results show that our full model outperformed the ablated models, and all of the RNN modules, bounding box position and image, and estimated pose were useful for the foul prediction. Our findings have important implications for a deeper understanding of foul play in soccer and provide a valuable reference for future research and practice in this area.","sentences":["Recent advances in computer vision have made significant progress in tracking and pose estimation of sports players.","However, there have been fewer studies on behavior prediction with pose estimation in sports, in particular, the prediction of soccer fouls is challenging because of the smaller image size of each player and of difficulty in the usage of e.g., the ball and pose information.","In our research, we introduce an innovative deep learning approach for anticipating soccer fouls.","This method integrates video data, bounding box positions, image details, and pose information by curating a novel soccer foul dataset.","Our model utilizes a combination of convolutional and recurrent neural networks (CNNs and RNNs) to effectively merge information from these four modalities.","The experimental results show that our full model outperformed the ablated models, and all of the RNN modules, bounding box position and image, and estimated pose were useful for the foul prediction.","Our findings have important implications for a deeper understanding of foul play in soccer and provide a valuable reference for future research and practice in this area."],"url":"http://arxiv.org/abs/2402.09650v1","category":"cs.CV"}
{"created":"2024-02-15 01:20:11","title":"Holographic Ricci dark energy in Nonconservative Unimodular Gravity","abstract":"The structure of unimodular gravity (UG) is invariant to a subclass of diffeomorphism, the transverse diffeomorphism, due to the unimodular condition ($\\sqrt{-g}=\\epsilon=cte$). Consequently, there is a freedom to define how the conservation laws of the energy-momentum tensor in unimodular gravity in the cosmological context. One of the main characteristics of the complete system of equations that describe cosmological dynamics in UG is that they form an underdetermined system if the usual conservation law of the energy-momentum tensor is not used in your structure, that is, it is necessary to insert extra information into the system to solve the complete set of equations. In this article, we propose the construction of a background cosmological model based on the description of a holographic dark energy component with a cutoff of the order of Ricci scalar in non-conservative UG. Although this choice is indeed a new addition of information to the cosmological system, the complete set of equations remains underdetermined, however, the new feature of this cosmological model is the appearance of an interaction between matter and dark energy. Indeed, this is a well-known characteristic of cosmological models in which we have holographic dark energy density. Consequently, we propose an ansatz to the interaction term $Q=\\beta H \\rho_{m}$, and obtain the cosmological parameters of our model. We found a viable universe model with similar characteristics to the $\\Lambda \\mathrm{CDM}$ model. We performed statistical analysis of the background model using the \"Cosmic Chronometer\" (CC) data for $H(z)$, and obtain as a result using Akaike Information Criterion (AIC), and the Bayesian Information Criterion (BIC) as model selection criteria that $\\Lambda \\mathrm{CDM}$ prevails as the best model. However, the proposed model is competitive when compared to the cosmological model $\\omega\\mathrm{CDM}$.","sentences":["The structure of unimodular gravity (UG) is invariant to a subclass of diffeomorphism, the transverse diffeomorphism, due to the unimodular condition ($\\sqrt{-g}=\\epsilon=cte$).","Consequently, there is a freedom to define how the conservation laws of the energy-momentum tensor in unimodular gravity in the cosmological context.","One of the main characteristics of the complete system of equations that describe cosmological dynamics in UG is that they form an underdetermined system if the usual conservation law of the energy-momentum tensor is not used in your structure, that is, it is necessary to insert extra information into the system to solve the complete set of equations.","In this article, we propose the construction of a background cosmological model based on the description of a holographic dark energy component with a cutoff of the order of Ricci scalar in non-conservative UG.","Although this choice is indeed a new addition of information to the cosmological system, the complete set of equations remains underdetermined, however, the new feature of this cosmological model is the appearance of an interaction between matter and dark energy.","Indeed, this is a well-known characteristic of cosmological models in which we have holographic dark energy density.","Consequently, we propose an ansatz to the interaction term $Q=\\beta H \\rho_{m}$, and obtain the cosmological parameters of our model.","We found a viable universe model with similar characteristics to the $\\Lambda \\mathrm{CDM}$ model.","We performed statistical analysis of the background model using the \"Cosmic Chronometer\" (CC) data for $H(z)$, and obtain as a result using Akaike Information Criterion (AIC), and the Bayesian Information Criterion (BIC) as model selection criteria that $\\Lambda \\mathrm{CDM}$ prevails as the best model.","However, the proposed model is competitive when compared to the cosmological model $\\omega\\mathrm{CDM}$."],"url":"http://arxiv.org/abs/2402.09645v1","category":"gr-qc"}
{"created":"2024-02-15 01:01:35","title":"Strebel differentials and string field theory","abstract":"A closed string worldsheet of genus $g$ with $n$ punctures can be presented as a contact interaction in which $n$ semi-infinite cylinders are glued together in a specific way via the Strebel differential on it, if $n\\geq1,\\ 2g-2+n>0$. We construct a string field theory of closed strings such that all the Feynman diagrams are represented by such contact interactions. In order to do so, we define off-shell amplitudes in the underlying string theory using the combinatorial Fenchel-Nielsen coordinates to describe the moduli space and derive a recursion relation satisfied by them. Utilizing the Fokker-Planck formalism, we construct a string field theory from which the recursion relation can be deduced through the Schwinger-Dyson equation. The Fokker-Planck Hamiltonian consists of kinetic terms and three string interaction terms.","sentences":["A closed string worldsheet of genus $g$ with $n$ punctures can be presented as a contact interaction in which $n$ semi-infinite cylinders are glued together in a specific way via the Strebel differential on it, if $n\\geq1,\\ 2g-2+n>0$. We construct a string field theory of closed strings such that all the Feynman diagrams are represented by such contact interactions.","In order to do so, we define off-shell amplitudes in the underlying string theory using the combinatorial Fenchel-Nielsen coordinates to describe the moduli space and derive a recursion relation satisfied by them.","Utilizing the Fokker-Planck formalism, we construct a string field theory from which the recursion relation can be deduced through the Schwinger-Dyson equation.","The Fokker-Planck Hamiltonian consists of kinetic terms and three string interaction terms."],"url":"http://arxiv.org/abs/2402.09641v1","category":"hep-th"}
{"created":"2024-02-15 00:29:53","title":"Spatiotemporal Disentanglement of Arteriovenous Malformations in Digital Subtraction Angiography","abstract":"Although Digital Subtraction Angiography (DSA) is the most important imaging for visualizing cerebrovascular anatomy, its interpretation by clinicians remains difficult. This is particularly true when treating arteriovenous malformations (AVMs), where entangled vasculature connecting arteries and veins needs to be carefully identified.The presented method aims to enhance DSA image series by highlighting critical information via automatic classification of vessels using a combination of two learning models: An unsupervised machine learning method based on Independent Component Analysis that decomposes the phases of flow and a convolutional neural network that automatically delineates the vessels in image space. The proposed method was tested on clinical DSA images series and demonstrated efficient differentiation between arteries and veins that provides a viable solution to enhance visualizations for clinical use.","sentences":["Although Digital Subtraction Angiography (DSA) is the most important imaging for visualizing cerebrovascular anatomy, its interpretation by clinicians remains difficult.","This is particularly true when treating arteriovenous malformations (AVMs), where entangled vasculature connecting arteries and veins needs to be carefully identified.","The presented method aims to enhance DSA image series by highlighting critical information via automatic classification of vessels using a combination of two learning models: An unsupervised machine learning method based on Independent Component Analysis that decomposes the phases of flow and a convolutional neural network that automatically delineates the vessels in image space.","The proposed method was tested on clinical DSA images series and demonstrated efficient differentiation between arteries and veins that provides a viable solution to enhance visualizations for clinical use."],"url":"http://arxiv.org/abs/2402.09636v1","category":"eess.IV"}
{"created":"2024-02-15 00:29:32","title":"VisIRNet: Deep Image Alignment for UAV-taken Visible and Infrared Image Pairs","abstract":"This paper proposes a deep learning based solution for multi-modal image alignment regarding UAV-taken images. Many recently proposed state-of-the-art alignment techniques rely on using Lucas-Kanade (LK) based solutions for a successful alignment. However, we show that we can achieve state of the art results without using LK-based methods. Our approach carefully utilizes a two-branch based convolutional neural network (CNN) based on feature embedding blocks. We propose two variants of our approach, where in the first variant (ModelA), we directly predict the new coordinates of only the four corners of the image to be aligned; and in the second one (ModelB), we predict the homography matrix directly. Applying alignment on the image corners forces algorithm to match only those four corners as opposed to computing and matching many (key)points, since the latter may cause many outliers, yielding less accurate alignment. We test our proposed approach on four aerial datasets and obtain state of the art results, when compared to the existing recent deep LK-based architectures.","sentences":["This paper proposes a deep learning based solution for multi-modal image alignment regarding UAV-taken images.","Many recently proposed state-of-the-art alignment techniques rely on using Lucas-Kanade (LK) based solutions for a successful alignment.","However, we show that we can achieve state of the art results without using LK-based methods.","Our approach carefully utilizes a two-branch based convolutional neural network (CNN) based on feature embedding blocks.","We propose two variants of our approach, where in the first variant (ModelA), we directly predict the new coordinates of only the four corners of the image to be aligned; and in the second one (ModelB), we predict the homography matrix directly.","Applying alignment on the image corners forces algorithm to match only those four corners as opposed to computing and matching many (key)points, since the latter may cause many outliers, yielding less accurate alignment.","We test our proposed approach on four aerial datasets and obtain state of the art results, when compared to the existing recent deep LK-based architectures."],"url":"http://arxiv.org/abs/2402.09635v1","category":"cs.CV"}
{"created":"2024-02-15 00:23:25","title":"Graph Neural Network-based Tracking as a Service","abstract":"Recent studies have shown promising results for track finding in dense environments using Graph Neural Network (GNN)-based algorithms. However, GNN-based track finding is computationally slow on CPUs, necessitating the use of coprocessors to accelerate the inference time. Additionally, the large input graph size demands a large device memory for efficient computation, a requirement not met by all computing facilities used for particle physics experiments, particularly those lacking advanced GPUs. Furthermore, deploying the GNN-based track-finding algorithm in a production environment requires the installation of all dependent software packages, exclusively utilized by this algorithm. These computing challenges must be addressed for the successful implementation of GNN-based track-finding algorithm into production settings. In response, we introduce a ``GNN-based tracking as a service'' approach, incorporating a custom backend within the NVIDIA Triton inference server to facilitate GNN-based tracking. This paper presents the performance of this approach using the Perlmutter supercomputer at NERSC.","sentences":["Recent studies have shown promising results for track finding in dense environments using Graph Neural Network (GNN)-based algorithms.","However, GNN-based track finding is computationally slow on CPUs, necessitating the use of coprocessors to accelerate the inference time.","Additionally, the large input graph size demands a large device memory for efficient computation, a requirement not met by all computing facilities used for particle physics experiments, particularly those lacking advanced GPUs.","Furthermore, deploying the GNN-based track-finding algorithm in a production environment requires the installation of all dependent software packages, exclusively utilized by this algorithm.","These computing challenges must be addressed for the successful implementation of GNN-based track-finding algorithm into production settings.","In response, we introduce a ``GNN-based tracking as a service'' approach, incorporating a custom backend within the NVIDIA Triton inference server to facilitate GNN-based tracking.","This paper presents the performance of this approach using the Perlmutter supercomputer at NERSC."],"url":"http://arxiv.org/abs/2402.09633v1","category":"physics.comp-ph"}
{"created":"2024-02-15 00:18:32","title":"Nonlinear stability of shock profiles to Burgers' equation with critical fast diffusion and singularity","abstract":"In this paper we propose the first framework to study Burgers' equation featuring critical fast diffusion in form of $u_t+f(u)_x = (\\ln u)_{xx}$. The solution possesses a strong singularity when $u=0$ hence bringing technical challenges. The main purpose of this paper is to investigate the asymptotic stability of viscous shocks, particularly those with shock profiles vanishing at the far field $x=+\\infty$. To overcome the singularity, we introduce some weight functions and show the nonlinear stability of shock profiles through the weighted energy method. Numerical simulations are also carried out in different cases of fast diffusion with singularity, which illustrate and confirm our theoretical results.","sentences":["In this paper we propose the first framework to study Burgers' equation featuring critical fast diffusion in form of $u_t+f(u)_x = (\\ln u)_{xx}$.","The solution possesses a strong singularity when $u=0$ hence bringing technical challenges.","The main purpose of this paper is to investigate the asymptotic stability of viscous shocks, particularly those with shock profiles vanishing at the far field $x=+\\infty$. To overcome the singularity, we introduce some weight functions and show the nonlinear stability of shock profiles through the weighted energy method.","Numerical simulations are also carried out in different cases of fast diffusion with singularity, which illustrate and confirm our theoretical results."],"url":"http://arxiv.org/abs/2402.09630v1","category":"math.AP"}
{"created":"2024-02-14 23:11:57","title":"On $O(p)\\times O(q)$-invariant constant mean curvature hypersurfaces with singularity","abstract":"We classify the $O(p)\\times O(q)$-invariant constant mean curvature hypersurfaces with singularity at the origin, solving a conjecture of Wu-yi Hsiang.","sentences":["We classify the $O(p)\\times O(q)$-invariant constant mean curvature hypersurfaces with singularity at the origin, solving a conjecture of Wu-yi Hsiang."],"url":"http://arxiv.org/abs/2402.09616v1","category":"math.DG"}
{"created":"2024-02-14 22:32:00","title":"Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families","abstract":"We introduce squared neural Poisson point processes (SNEPPPs) by parameterising the intensity function by the squared norm of a two layer neural network. When the hidden layer is fixed and the second layer has a single neuron, our approach resembles previous uses of squared Gaussian process or kernel methods, but allowing the hidden layer to be learnt allows for additional flexibility. In many cases of interest, the integrated intensity function admits a closed form and can be computed in quadratic time in the number of hidden neurons. We enumerate a far more extensive number of such cases than has previously been discussed. Our approach is more memory and time efficient than naive implementations of squared or exponentiated kernel methods or Gaussian processes. Maximum likelihood and maximum a posteriori estimates in a reparameterisation of the final layer of the intensity function can be obtained by solving a (strongly) convex optimisation problem using projected gradient descent. We demonstrate SNEPPPs on real, and synthetic benchmarks, and provide a software implementation. https://github.com/RussellTsuchida/snefy","sentences":["We introduce squared neural Poisson point processes (SNEPPPs) by parameterising the intensity function by the squared norm of a two layer neural network.","When the hidden layer is fixed and the second layer has a single neuron, our approach resembles previous uses of squared Gaussian process or kernel methods, but allowing the hidden layer to be learnt allows for additional flexibility.","In many cases of interest, the integrated intensity function admits a closed form and can be computed in quadratic time in the number of hidden neurons.","We enumerate a far more extensive number of such cases than has previously been discussed.","Our approach is more memory and time efficient than naive implementations of squared or exponentiated kernel methods or Gaussian processes.","Maximum likelihood and maximum a posteriori estimates in a reparameterisation of the final layer of the intensity function can be obtained by solving a (strongly) convex optimisation problem using projected gradient descent.","We demonstrate SNEPPPs on real, and synthetic benchmarks, and provide a software implementation.","https://github.com/RussellTsuchida/snefy"],"url":"http://arxiv.org/abs/2402.09608v1","category":"cs.LG"}
{"created":"2024-02-14 22:26:41","title":"Scattering for the Quartic Generalized Benjamin-Bona-Mahony Equation","abstract":"The generalized Benjamin-Bona-Mahony equation (gBBM) is a model for nonlinear dispersive waves which, in the long-wave limit, is approximately equivalent to the generalized Korteweg-de Vries equation (gKdV). While the long-time behaviour of small solutions to gKdV is well-understood, the corresponding theory for gBBM has progressed little since the 1990s. Using a space-time resonance approach, I establish linear dispersive decay and scattering for small solutions to the quartic-nonlinear gBBM. To my knowledge, this result provides the first global-in-time pointwise estimates on small solutions to gBBM with a nonlinear power less than or equal to five. Owing to nonzero inflection points in the linearized gBBM dispersion relation, there exist isolated space-time resonances without null structure, but in the course of the proof I show these resonances do not obstruct scattering.","sentences":["The generalized Benjamin-Bona-Mahony equation (gBBM) is a model for nonlinear dispersive waves which, in the long-wave limit, is approximately equivalent to the generalized Korteweg-de Vries equation (gKdV).","While the long-time behaviour of small solutions to gKdV is well-understood, the corresponding theory for gBBM has progressed little since the 1990s.","Using a space-time resonance approach, I establish linear dispersive decay and scattering for small solutions to the quartic-nonlinear gBBM.","To my knowledge, this result provides the first global-in-time pointwise estimates on small solutions to gBBM with a nonlinear power less than or equal to five.","Owing to nonzero inflection points in the linearized gBBM dispersion relation, there exist isolated space-time resonances without null structure, but in the course of the proof I show these resonances do not obstruct scattering."],"url":"http://arxiv.org/abs/2402.09605v1","category":"math.AP"}
{"created":"2024-02-14 22:15:37","title":"Low-Rank Graph Contrastive Learning for Node Classification","abstract":"Graph Neural Networks (GNNs) have been widely used to learn node representations and with outstanding performance on various tasks such as node classification. However, noise, which inevitably exists in real-world graph data, would considerably degrade the performance of GNNs revealed by recent studies. In this work, we propose a novel and robust GNN encoder, Low-Rank Graph Contrastive Learning (LR-GCL). Our method performs transductive node classification in two steps. First, a low-rank GCL encoder named LR-GCL is trained by prototypical contrastive learning with low-rank regularization. Next, using the features produced by LR-GCL, a linear transductive classification algorithm is used to classify the unlabeled nodes in the graph. Our LR-GCL is inspired by the low frequency property of the graph data and its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning. To the best of our knowledge, our theoretical result is among the first to theoretically demonstrate the advantage of low-rank learning in graph contrastive learning supported by strong empirical performance. Extensive experiments on public benchmarks demonstrate the superior performance of LR-GCL and the robustness of the learned node representations. The code of LR-GCL is available at \\url{https://anonymous.4open.science/r/Low-Rank_Graph_Contrastive_Learning-64A6/}.","sentences":["Graph Neural Networks (GNNs) have been widely used to learn node representations and with outstanding performance on various tasks such as node classification.","However, noise, which inevitably exists in real-world graph data, would considerably degrade the performance of GNNs revealed by recent studies.","In this work, we propose a novel and robust GNN encoder, Low-Rank Graph Contrastive Learning (LR-GCL).","Our method performs transductive node classification in two steps.","First, a low-rank GCL encoder named LR-GCL is trained by prototypical contrastive learning with low-rank regularization.","Next, using the features produced by LR-GCL, a linear transductive classification algorithm is used to classify the unlabeled nodes in the graph.","Our LR-GCL is inspired by the low frequency property of the graph data and its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning.","To the best of our knowledge, our theoretical result is among the first to theoretically demonstrate the advantage of low-rank learning in graph contrastive learning supported by strong empirical performance.","Extensive experiments on public benchmarks demonstrate the superior performance of LR-GCL and the robustness of the learned node representations.","The code of LR-GCL is available at \\url{https://anonymous.4open.science/r/Low-Rank_Graph_Contrastive_Learning-64A6/}."],"url":"http://arxiv.org/abs/2402.09600v1","category":"cs.LG"}
{"created":"2024-02-14 21:34:04","title":"Exponential Stability of Higher-Order Fractional Neutral Stochastic Differential Equation via Integral Contractors","abstract":"The existence, uniqueness, and exponential stability results for mild solutions to the fractional neutral stochastic differential system are presented in this article. To demonstrate the results, the concept of bounded integral contractors is combined with the stochastic result and sequencing technique. In contrast to previous publications, we do not need to specify the induced inverse of the controllability operator to prove the stability results, and the relevant nonlinear function does not have to meet the Lipschitz condition. Furthermore, exponential stability results for neutral stochastic differential systems with Poisson jump have been established. Finally, an application to demonstrate the acquired results is discussed. This paper extends the work of Chalishajar et al. \\cite{r4} and Renu Chaudhary et al. \\cite{r3}.","sentences":["The existence, uniqueness, and exponential stability results for mild solutions to the fractional neutral stochastic differential system are presented in this article.","To demonstrate the results, the concept of bounded integral contractors is combined with the stochastic result and sequencing technique.","In contrast to previous publications, we do not need to specify the induced inverse of the controllability operator to prove the stability results, and the relevant nonlinear function does not have to meet the Lipschitz condition.","Furthermore, exponential stability results for neutral stochastic differential systems with Poisson jump have been established.","Finally, an application to demonstrate the acquired results is discussed.","This paper extends the work of Chalishajar et al. \\cite{r4} and Renu Chaudhary et al. \\cite{r3}."],"url":"http://arxiv.org/abs/2402.09590v1","category":"math.DS"}
{"created":"2024-02-14 20:54:51","title":"Symmetries of geodesic flows on covers and rigidity","abstract":"We define and study the foliated centralizer: the group of $C^\\infty$ centralizer elements of the lift of an Anosov system on a non-compact manifold which additionally preserve the stable and unstable foliations. When the Anosov system is the geodesic flow of a closed Riemannian manifold with pinched negative sectional curvatures, we prove some rigidity properties for the foliated centralizer of the lift of the flow to the universal cover: it is a finite-dimensional Lie group, which is moreover discrete (modulo the action of the flow itself) unless the metric is homothetic to some real hyperbolic metric. This result is inspired by the study of isometries of universal covers that appeared originally in the work of Eberlein, and later in Farb and Weinberger, as well as centralizer rigidity results in dynamics.","sentences":["We define and study the foliated centralizer: the group of $C^\\infty$ centralizer elements of the lift of an Anosov system on a non-compact manifold which additionally preserve the stable and unstable foliations.","When the Anosov system is the geodesic flow of a closed Riemannian manifold with pinched negative sectional curvatures, we prove some rigidity properties for the foliated centralizer of the lift of the flow to the universal cover: it is a finite-dimensional Lie group, which is moreover discrete (modulo the action of the flow itself) unless the metric is homothetic to some real hyperbolic metric.","This result is inspired by the study of isometries of universal covers that appeared originally in the work of Eberlein, and later in Farb and Weinberger, as well as centralizer rigidity results in dynamics."],"url":"http://arxiv.org/abs/2402.09576v1","category":"math.DS"}
{"created":"2024-02-14 20:37:06","title":"Restoring the Navier--Stokes dynamics by determining functionals depending on pressure","abstract":"For 2D Navier--Stokes equations in a bounded smooth domain, we construct a system of determining functionals which consists of $N$ linear continuous functionals which depend on pressure $p$ only and of one extra functional which is given by the value of vorticity at a fixed point $x_0\\in\\partial\\Omega$.","sentences":["For 2D Navier--Stokes equations in a bounded smooth domain, we construct a system of determining functionals which consists of $N$ linear continuous functionals which depend on pressure $p$ only and of one extra functional which is given by the value of vorticity at a fixed point $x_0\\in\\partial\\Omega$."],"url":"http://arxiv.org/abs/2402.09566v1","category":"math.AP"}
{"created":"2024-02-14 20:17:04","title":"Enhancing Source Code Representations for Deep Learning with Static Analysis","abstract":"Deep learning techniques applied to program analysis tasks such as code classification, summarization, and bug detection have seen widespread interest. Traditional approaches, however, treat programming source code as natural language text, which may neglect significant structural or semantic details. Additionally, most current methods of representing source code focus solely on the code, without considering beneficial additional context. This paper explores the integration of static analysis and additional context such as bug reports and design patterns into source code representations for deep learning models. We use the Abstract Syntax Tree-based Neural Network (ASTNN) method and augment it with additional context information obtained from bug reports and design patterns, creating an enriched source code representation that significantly enhances the performance of common software engineering tasks such as code classification and code clone detection. Utilizing existing open-source code data, our approach improves the representation and processing of source code, thereby improving task performance.","sentences":["Deep learning techniques applied to program analysis tasks such as code classification, summarization, and bug detection have seen widespread interest.","Traditional approaches, however, treat programming source code as natural language text, which may neglect significant structural or semantic details.","Additionally, most current methods of representing source code focus solely on the code, without considering beneficial additional context.","This paper explores the integration of static analysis and additional context such as bug reports and design patterns into source code representations for deep learning models.","We use the Abstract Syntax Tree-based Neural Network (ASTNN) method and augment it with additional context information obtained from bug reports and design patterns, creating an enriched source code representation that significantly enhances the performance of common software engineering tasks such as code classification and code clone detection.","Utilizing existing open-source code data, our approach improves the representation and processing of source code, thereby improving task performance."],"url":"http://arxiv.org/abs/2402.09557v1","category":"cs.SE"}
{"created":"2024-02-14 20:11:04","title":"Anomalous conductivity due to relativistic Landau quantization","abstract":"We use a recently developed kinetic model derived from the Dirac equation, in order to study electromagnetic wave propagation in superstrong magnetic fields, such as in magnetars, where relativistic Landau quantization is prominent. The leading contribution to the conductivity tensor in such a plasma is calculated. It is found that the electron Hall current has an anomalous contribution, in the quantum relativistic regime, where the effective particle energy has a significant contribution from the diamagnetic and Zeeman energy. As a result, a new quantum resonance frequency appears, and the dispersion relation for the left- and right-hand polarized modes are strongly modified for long and moderate wavelengths. The implications for magnetar physics are discussed.","sentences":["We use a recently developed kinetic model derived from the Dirac equation, in order to study electromagnetic wave propagation in superstrong magnetic fields, such as in magnetars, where relativistic Landau quantization is prominent.","The leading contribution to the conductivity tensor in such a plasma is calculated.","It is found that the electron Hall current has an anomalous contribution, in the quantum relativistic regime, where the effective particle energy has a significant contribution from the diamagnetic and Zeeman energy.","As a result, a new quantum resonance frequency appears, and the dispersion relation for the left- and right-hand polarized modes are strongly modified for long and moderate wavelengths.","The implications for magnetar physics are discussed."],"url":"http://arxiv.org/abs/2402.09554v1","category":"physics.plasm-ph"}
{"created":"2024-02-14 19:34:28","title":"Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning","abstract":"In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning methods across multiple problem settings, regardless of the amount of available replay memory.","sentences":["In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream.","Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data.","Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy.","Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting.","We minimize these instabilities through a simple modification of the optimization geometry.","Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data.","We demonstrate that LPR consistently improves replay-based online continual learning methods across multiple problem settings, regardless of the amount of available replay memory."],"url":"http://arxiv.org/abs/2402.09542v1","category":"cs.LG"}
{"created":"2024-02-14 19:10:40","title":"Reducing Texture Bias of Deep Neural Networks via Edge Enhancing Diffusion","abstract":"Convolutional neural networks (CNNs) for image processing tend to focus on localized texture patterns, commonly referred to as texture bias. While most of the previous works in the literature focus on the task of image classification, we go beyond this and study the texture bias of CNNs in semantic segmentation. In this work, we propose to train CNNs on pre-processed images with less texture to reduce the texture bias. Therein, the challenge is to suppress image texture while preserving shape information. To this end, we utilize edge enhancing diffusion (EED), an anisotropic image diffusion method initially introduced for image compression, to create texture reduced duplicates of existing datasets. Extensive numerical studies are performed with both CNNs and vision transformer models trained on original data and EED-processed data from the Cityscapes dataset and the CARLA driving simulator. We observe strong texture-dependence of CNNs and moderate texture-dependence of transformers. Training CNNs on EED-processed images enables the models to become completely ignorant with respect to texture, demonstrating resilience with respect to texture re-introduction to any degree. Additionally we analyze the performance reduction in depth on a level of connected components in the semantic segmentation and study the influence of EED pre-processing on domain generalization as well as adversarial robustness.","sentences":["Convolutional neural networks (CNNs) for image processing tend to focus on localized texture patterns, commonly referred to as texture bias.","While most of the previous works in the literature focus on the task of image classification, we go beyond this and study the texture bias of CNNs in semantic segmentation.","In this work, we propose to train CNNs on pre-processed images with less texture to reduce the texture bias.","Therein, the challenge is to suppress image texture while preserving shape information.","To this end, we utilize edge enhancing diffusion (EED), an anisotropic image diffusion method initially introduced for image compression, to create texture reduced duplicates of existing datasets.","Extensive numerical studies are performed with both CNNs and vision transformer models trained on original data and EED-processed data from the Cityscapes dataset and the CARLA driving simulator.","We observe strong texture-dependence of CNNs and moderate texture-dependence of transformers.","Training CNNs on EED-processed images enables the models to become completely ignorant with respect to texture, demonstrating resilience with respect to texture re-introduction to any degree.","Additionally we analyze the performance reduction in depth on a level of connected components in the semantic segmentation and study the influence of EED pre-processing on domain generalization as well as adversarial robustness."],"url":"http://arxiv.org/abs/2402.09530v1","category":"cs.CV"}
{"created":"2024-02-14 19:00:14","title":"Non-monotonic specific entropy on the transition line near the QCD critical point","abstract":"We investigate the effect of the QCD critical point on the isentropic trajectories in the QCD phase diagram. We point out that the universality of the critical equation of state and the third law of thermodynamics require the specific entropy (per baryon) along the coexistence (first-order transition) line to be non-monotonic at least on one side of that line. Specifically, a maximum must occur. We show how the location of the maximum relative to the QCD critical point depends on the parameters of the critical equation of state commonly used in the literature. We then examine how the isentropic trajectories followed by adiabatically expanding heavy-ion collision fireballs behave near the critical point. We find that a crucial role is played by the sign of the isochoric temperature derivative of pressure at the critical point; this sign determines on which side of the coexistence curve the specific entropy must be non-monotonic (i.e., has a maximum). We classify different scenarios of the adiabatic expansion that arise depending on the value of the discriminant parameter and the proximity of the trajectory to the critical point.","sentences":["We investigate the effect of the QCD critical point on the isentropic trajectories in the QCD phase diagram.","We point out that the universality of the critical equation of state and the third law of thermodynamics require the specific entropy (per baryon) along the coexistence (first-order transition) line to be non-monotonic at least on one side of that line.","Specifically, a maximum must occur.","We show how the location of the maximum relative to the QCD critical point depends on the parameters of the critical equation of state commonly used in the literature.","We then examine how the isentropic trajectories followed by adiabatically expanding heavy-ion collision fireballs behave near the critical point.","We find that a crucial role is played by the sign of the isochoric temperature derivative of pressure at the critical point; this sign determines on which side of the coexistence curve the specific entropy must be non-monotonic (i.e., has a maximum).","We classify different scenarios of the adiabatic expansion that arise depending on the value of the discriminant parameter and the proximity of the trajectory to the critical point."],"url":"http://arxiv.org/abs/2402.09519v1","category":"nucl-th"}
{"created":"2024-02-14 19:00:03","title":"On solutions of $\\sum_{i=1}^n 1/x_i = 1$ in integers of the form $2^a k^b$, where $k$ is a fixed odd positive integer","abstract":"We give an algorithm that produces all solutions of the equation $\\sum_{i=1}^n 1/x_i = 1$ in integers of the form $2^a k^b$, where $k$ is a fixed odd positive integer at least $3$, $a$ is an element of $\\{0,1,2\\}$ that can vary from term to term, and $b$ is a nonnegative integer that can vary from term to term. We also show that this equation has a nontrivial solution in integers of this form if and only if $k \\leq 4n-11$ or $(k,n) = (3,3)$.","sentences":["We give an algorithm that produces all solutions of the equation $\\sum_{i=1}^n 1/x_i = 1$ in integers of the form $2^a k^b$, where $k$ is a fixed odd positive integer at least $3$, $a$ is an element of $\\{0,1,2\\}$ that can vary from term to term, and $b$ is a nonnegative integer that can vary from term to term.","We also show that this equation has a nontrivial solution in integers of this form if and only if $k \\leq 4n-11$ or $(k,n) = (3,3)$."],"url":"http://arxiv.org/abs/2402.09515v1","category":"math.NT"}
{"created":"2024-02-14 19:00:01","title":"A multiband look at ultraluminous X-ray sources in NGC 7424","abstract":"We studied the multiband properties of two ultraluminous X-ray sources (2CXO J225728.9-410211 = X-1 and 2CXO J225724.7-410343 = X-2) and their surroundings, in the spiral galaxy NGC 7424. Both sources have approached X-ray luminosities L_{X} ~ 10^{40} erg/s at some epochs. Thanks to a more accurate astrometric solution (based on Australia Telescope Compact Array and Gaia data), we identified the point-like optical counterpart of X-1, which looks like an isolated B8 supergiant (M ~ 9 Msun, age ~ 30 Myr). Instead, X-2 is in a star-forming region (size of about 100 pc x 150 pc), near young clusters and ionized gas. Very Large Telescope long-slit spectra show a spatially extended region of HeII 4686 emission around the X-ray position, displaced by about 50 pc from the brightest star cluster, which corresponds to the peak of lower-ionization line emission. We interpret the HeII 4686 emission as a signature of X-ray photo-ionization from the ULX, while the other optical lines are consistent with UV ionization in an ordinary HeII region. The luminosity of this He^{++} nebula puts it in the same class as other classical photo-ionized ULX nebulae such as those around Holmberg II X-1 and NGC 5408 X-1. We locate a strong (5.5-GHz luminosity nu L_{nu} ~ 10^{35} erg/s), steep-spectrum, unresolved radio source at the peak of the low-ionization lines, and discuss alternative physical scenarios for the radio emission. Finally, we use WISE data to obtain an independent estimate of the reddening of the star-forming clump around X-2.","sentences":["We studied the multiband properties of two ultraluminous X-ray sources (2CXO J225728.9-410211 = X-1 and 2CXO J225724.7-410343 = X-2) and their surroundings, in the spiral galaxy NGC 7424.","Both sources have approached X-ray luminosities L_{X} ~ 10^{40} erg/s at some epochs.","Thanks to a more accurate astrometric solution (based on Australia Telescope Compact Array and Gaia data), we identified the point-like optical counterpart of X-1, which looks like an isolated B8 supergiant (M ~ 9 Msun, age ~ 30 Myr).","Instead, X-2 is in a star-forming region (size of about 100 pc x 150 pc), near young clusters and ionized gas.","Very Large Telescope long-slit spectra show a spatially extended region of HeII 4686 emission around the X-ray position, displaced by about 50 pc from the brightest star cluster, which corresponds to the peak of lower-ionization line emission.","We interpret the HeII 4686 emission as a signature of X-ray photo-ionization from the ULX, while the other optical lines are consistent with UV ionization in an ordinary HeII region.","The luminosity of this He^{++} nebula puts it in the same class as other classical photo-ionized ULX nebulae such as those around Holmberg II X-1 and NGC 5408 X-1.","We locate a strong (5.5-GHz luminosity nu L_{nu} ~ 10^{35} erg/s), steep-spectrum, unresolved radio source at the peak of the low-ionization lines, and discuss alternative physical scenarios for the radio emission.","Finally, we use WISE data to obtain an independent estimate of the reddening of the star-forming clump around X-2."],"url":"http://arxiv.org/abs/2402.09512v1","category":"astro-ph.HE"}
{"created":"2024-02-14 18:18:32","title":"Loopy-SLAM: Dense Neural SLAM with Loop Closures","abstract":"Neural RGBD SLAM techniques have shown promise in dense Simultaneous Localization And Mapping (SLAM), yet face challenges such as error accumulation during camera tracking resulting in distorted maps. In response, we introduce Loopy-SLAM that globally optimizes poses and the dense 3D model. We use frame-to-model tracking using a data-driven point-based submap generation method and trigger loop closures online by performing global place recognition. Robust pose graph optimization is used to rigidly align the local submaps. As our representation is point based, map corrections can be performed efficiently without the need to store the entire history of input frames used for mapping as typically required by methods employing a grid based mapping structure. Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNet datasets demonstrate competitive or superior performance in tracking, mapping, and rendering accuracy when compared to existing dense neural RGBD SLAM methods. Project page: notchla.github.io/Loopy-SLAM.","sentences":["Neural RGBD SLAM techniques have shown promise in dense Simultaneous Localization And Mapping (SLAM), yet face challenges such as error accumulation during camera tracking resulting in distorted maps.","In response, we introduce Loopy-SLAM that globally optimizes poses and the dense 3D model.","We use frame-to-model tracking using a data-driven point-based submap generation method and trigger loop closures online by performing global place recognition.","Robust pose graph optimization is used to rigidly align the local submaps.","As our representation is point based, map corrections can be performed efficiently without the need to store the entire history of input frames used for mapping as typically required by methods employing a grid based mapping structure.","Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNet datasets demonstrate competitive or superior performance in tracking, mapping, and rendering accuracy when compared to existing dense neural RGBD SLAM methods.","Project page: notchla.github.io/Loopy-SLAM."],"url":"http://arxiv.org/abs/2402.09944v1","category":"cs.CV"}
{"created":"2024-02-15 18:59:43","title":"Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling","abstract":"Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.","sentences":["Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics.","These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements).","While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors.","These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift.","For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment.","In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction.","HiSS stacks structured state-space models on top of each other to create a temporal hierarchy.","Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE.","Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques.","Code, datasets and videos can be found on https://hiss-csp.github.io."],"url":"http://arxiv.org/abs/2402.10211v1","category":"cs.LG"}
{"created":"2024-02-15 17:16:33","title":"Reusing Softmax Hardware Unit for GELU Computation in Transformers","abstract":"Transformers have improved drastically the performance of natural language processing (NLP) and computer vision applications. The computation of transformers involves matrix multiplications and non-linear activation functions such as softmax and GELU (Gaussion Error Linear Unit) that are accelerated directly in hardware. Currently, function evaluation is done separately for each function and rarely allows for hardware reuse. To mitigate this problem, in this work, we map the computation of GELU to a softmax operator. In this way, the efficient hardware units designed already for softmax can be reused for computing GELU as well. Computation of GELU can enjoy the inherent vectorized nature of softmax and produce in parallel multiple GELU outcomes. Experimental results show that computing GELU via a pre-existing and incrementally modified softmax hardware unit (a) does not reduce the accuracy of representative NLP applications and (b) allows the reduction of the overall hardware area and power by 6.1% and 11.9%, respectively, on average.","sentences":["Transformers have improved drastically the performance of natural language processing (NLP) and computer vision applications.","The computation of transformers involves matrix multiplications and non-linear activation functions such as softmax and GELU (Gaussion Error Linear Unit) that are accelerated directly in hardware.","Currently, function evaluation is done separately for each function and rarely allows for hardware reuse.","To mitigate this problem, in this work, we map the computation of GELU to a softmax operator.","In this way, the efficient hardware units designed already for softmax can be reused for computing GELU as well.","Computation of GELU can enjoy the inherent vectorized nature of softmax and produce in parallel multiple GELU outcomes.","Experimental results show that computing GELU via a pre-existing and incrementally modified softmax hardware unit (a) does not reduce the accuracy of representative NLP applications and (b) allows the reduction of the overall hardware area and power by 6.1% and 11.9%, respectively, on average."],"url":"http://arxiv.org/abs/2402.10118v1","category":"cs.AR"}
{"created":"2024-02-15 16:09:39","title":"OH-Formation Following Vibrationally Induced Reaction Dynamics of H$_2$COO","abstract":"The reaction dynamics of H$_2$COO to form linear HCOOH and dioxirane as first steps for OH-elimination is quantitatively investigated. Using a machine learned potential energy surface at the CASPT2/aug-cc-pVTZ level of theory vibrational excitation along the CH-normal mode $\\nu_{\\rm CH}$ with energies up to 40.0 kcal/mol ($\\sim 5 \\nu_{\\rm CH}$) leads almost exclusively to linear HCOOH which further decomposes into OH+HCO. Although the barrier to form dioxirane is only 21.4 kcal/mol the reaction probability to form dioxirane is two orders of magnitude lower if the CH-stretch mode is excited. Following the dioxirane-formation pathway is facile, however, if in addition the COO-bend vibration is excited with energies equivalent to $\\sim (2 \\nu_{\\rm CH} + 4 \\nu_{\\rm COO})$ or $\\sim (3 \\nu_{\\rm CH} + \\nu_{\\rm COO})$. For OH-formation in the atmosphere the pathway through linear HCOOH is probably most relevant because the alternative pathways (through dioxirane or formic acid) involve several intermediates that can de-excite through collisions, relax {\\it via} Intramolecular vibrational energy redistribution (IVR), or pass through very loose and vulnerable transition states (formic acid). This work demonstrates how, by selectively exciting particular vibrational modes, it is possible to dial into desired reaction channels with a high degree of specificity for a process relevant to atmospheric chemistry.","sentences":["The reaction dynamics of H$_2$COO to form linear HCOOH and dioxirane as first steps for OH-elimination is quantitatively investigated.","Using a machine learned potential energy surface at the CASPT2/aug-cc-pVTZ level of theory vibrational excitation along the CH-normal mode $\\nu_{\\rm CH}$ with energies up to 40.0 kcal/mol ($\\sim 5 \\nu_{\\rm CH}$) leads almost exclusively to linear HCOOH which further decomposes into OH+HCO.","Although the barrier to form dioxirane is only 21.4 kcal/mol the reaction probability to form dioxirane is two orders of magnitude lower if the CH-stretch mode is excited.","Following the dioxirane-formation pathway is facile, however, if in addition the COO-bend vibration is excited with energies equivalent to $\\sim (2 \\nu_{\\rm CH} + 4 \\nu_{\\rm COO})$ or $\\sim (3 \\nu_{\\rm CH} + \\nu_{\\rm","COO})$. For OH-formation in the atmosphere the pathway through linear HCOOH is probably most relevant because the alternative pathways (through dioxirane or formic acid) involve several intermediates that can de-excite through collisions, relax {\\it via} Intramolecular vibrational energy redistribution (IVR), or pass through very loose and vulnerable transition states (formic acid).","This work demonstrates how, by selectively exciting particular vibrational modes, it is possible to dial into desired reaction channels with a high degree of specificity for a process relevant to atmospheric chemistry."],"url":"http://arxiv.org/abs/2402.10047v1","category":"physics.chem-ph"}
{"created":"2024-02-15 15:17:26","title":"Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion","abstract":"Editing signals using large pre-trained models, in a zero-shot manner, has recently seen rapid advancements in the image domain. However, this wave has yet to reach the audio domain. In this paper, we explore two zero-shot editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models. The first, adopted from the image domain, allows text-based editing. The second, is a novel approach for discovering semantically meaningful editing directions without supervision. When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody. Samples can be found on our examples page in https://hilamanor.github.io/AudioEditing/ and code can be found in https://github.com/hilamanor/AudioEditing/ .","sentences":["Editing signals using large pre-trained models, in a zero-shot manner, has recently seen rapid advancements in the image domain.","However, this wave has yet to reach the audio domain.","In this paper, we explore two zero-shot editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models.","The first, adopted from the image domain, allows text-based editing.","The second, is a novel approach for discovering semantically meaningful editing directions without supervision.","When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody.","Samples can be found on our examples page in https://hilamanor.github.io/AudioEditing/ and code can be found in https://github.com/hilamanor/AudioEditing/ ."],"url":"http://arxiv.org/abs/2402.10009v1","category":"cs.SD"}
{"created":"2024-02-15 15:06:33","title":"Privacy Attacks in Decentralized Learning","abstract":"Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph. The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others. In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood. Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD. We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large. We empirically investigate some of the factors that affect the performance of the attack, namely the graph topology, the number of attackers, and their position in the graph.","sentences":["Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph.","The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others.","In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood.","Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD.","We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large.","We empirically investigate some of the factors that affect the performance of the attack, namely the graph topology, the number of attackers, and their position in the graph."],"url":"http://arxiv.org/abs/2402.10001v1","category":"cs.LG"}
{"created":"2024-02-15 15:02:05","title":"iJTyper: An Iterative Type Inference Framework for Java by Integrating Constraint- and Statistically-based Methods","abstract":"Inferring the types of API elements in incomplete code snippets (e.g., those on Q&A forums) is a prepositive step required to work with the code snippets. Existing type inference methods can be mainly categorized as constraint-based or statistically-based. The former imposes higher requirements on code syntax and often suffers from low recall due to the syntactic limitation of code snippets. The latter relies on the statistical regularities learned from a training corpus and does not take full advantage of the type constraints in code snippets, which may lead to low precision. In this paper, we propose an iterative type inference framework for Java, called iJTyper, by integrating the strengths of both constraint- and statistically-based methods. For a code snippet, iJTyper first applies a constraint-based method and augments the code context with the inferred types of API elements. iJTyper then applies a statistically-based method to the augmented code snippet. The predicted candidate types of API elements are further used to improve the constraint-based method by reducing its pre-built knowledge base. iJTyper iteratively executes both methods and performs code context augmentation and knowledge base reduction until a termination condition is satisfied. Finally, the final inference results are obtained by combining the results of both methods. We evaluated iJTyper on two open-source datasets. Results show that 1) iJTyper achieves high average precision/recall of 97.31% and 92.52% on both datasets; 2) iJTyper significantly improves the recall of two state-of-the-art baselines, SnR and MLMTyper, by at least 7.31% and 27.44%, respectively; and 3) iJTyper improves the average precision/recall of the popular language model, ChatGPT, by 3.25% and 0.51% on both datasets.","sentences":["Inferring the types of API elements in incomplete code snippets (e.g., those on Q&A forums) is a prepositive step required to work with the code snippets.","Existing type inference methods can be mainly categorized as constraint-based or statistically-based.","The former imposes higher requirements on code syntax and often suffers from low recall due to the syntactic limitation of code snippets.","The latter relies on the statistical regularities learned from a training corpus and does not take full advantage of the type constraints in code snippets, which may lead to low precision.","In this paper, we propose an iterative type inference framework for Java, called iJTyper, by integrating the strengths of both constraint- and statistically-based methods.","For a code snippet, iJTyper first applies a constraint-based method and augments the code context with the inferred types of API elements.","iJTyper then applies a statistically-based method to the augmented code snippet.","The predicted candidate types of API elements are further used to improve the constraint-based method by reducing its pre-built knowledge base.","iJTyper iteratively executes both methods and performs code context augmentation and knowledge base reduction until a termination condition is satisfied.","Finally, the final inference results are obtained by combining the results of both methods.","We evaluated iJTyper on two open-source datasets.","Results show that 1) iJTyper achieves high average precision/recall of 97.31% and 92.52% on both datasets; 2) iJTyper significantly improves the recall of two state-of-the-art baselines, SnR and MLMTyper, by at least 7.31% and 27.44%, respectively; and 3) iJTyper improves the average precision/recall of the popular language model, ChatGPT, by 3.25% and 0.51% on both datasets."],"url":"http://arxiv.org/abs/2402.09995v1","category":"cs.SE"}
{"created":"2024-02-15 14:49:18","title":"Optimistix: modular optimisation in JAX and Equinox","abstract":"We introduce Optimistix: a nonlinear optimisation library built in JAX and Equinox. Optimistix introduces a novel, modular approach for its minimisers and least-squares solvers. This modularity relies on new practical abstractions for optimisation which we call search and descent, and which generalise classical notions of line search, trust-region, and learning-rate algorithms. It provides high-level APIs and solvers for minimisation, nonlinear least-squares, root-finding, and fixed-point iteration. Optimistix is available at https://github.com/patrick-kidger/optimistix.","sentences":["We introduce Optimistix: a nonlinear optimisation library built in JAX and Equinox.","Optimistix introduces a novel, modular approach for its minimisers and least-squares solvers.","This modularity relies on new practical abstractions for optimisation which we call search and descent, and which generalise classical notions of line search, trust-region, and learning-rate algorithms.","It provides high-level APIs and solvers for minimisation, nonlinear least-squares, root-finding, and fixed-point iteration.","Optimistix is available at https://github.com/patrick-kidger/optimistix."],"url":"http://arxiv.org/abs/2402.09983v1","category":"math.OC"}
{"created":"2024-02-15 13:52:23","title":"Multi-Word Tokenization for Sequence Compression","abstract":"Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length and budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.","sentences":["Large Language Models have proven highly successful at modelling a variety of tasks.","However, this comes at a steep computational cost that hinders wider industrial uptake.","In this pa005 per, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens.","MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length and budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance.","Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation."],"url":"http://arxiv.org/abs/2402.09949v1","category":"cs.CL"}
{"created":"2024-02-15 13:50:00","title":"Explaining Probabilistic Models with Distributional Values","abstract":"A large branch of explainable machine learning is grounded in cooperative game theory. However, research indicates that game-theoretic explanations may mislead or be hard to interpret. We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class). This paper addresses such gap for probabilistic models by generalising cooperative games and value operators. We introduce the distributional values, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs. We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models.","sentences":["A large branch of explainable machine learning is grounded in cooperative game theory.","However, research indicates that game-theoretic explanations may mislead or be hard to interpret.","We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class).","This paper addresses such gap for probabilistic models by generalising cooperative games and value operators.","We introduce the distributional values, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs.","We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models."],"url":"http://arxiv.org/abs/2402.09947v1","category":"cs.LG"}
{"created":"2024-02-15 12:17:15","title":"DE-COP: Detecting Copyrighted Content in Language Models Training Data","abstract":"How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give $\\approx$ 4% accuracy. Our code and datasets are available at https://github.com/avduarte333/DE-COP_Method","sentences":["How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed?","We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text.","We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training.","DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases.","We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases.","Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available.","Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give $\\approx$ 4% accuracy.","Our code and datasets are available at https://github.com/avduarte333/DE-COP_Method"],"url":"http://arxiv.org/abs/2402.09910v1","category":"cs.CL"}
{"created":"2024-02-15 09:31:07","title":"Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment","abstract":"Deep Learning (DL) is undergoing a paradigm shift with the emergence of foundation models, aptly named by their crucial, yet incomplete nature. In this work, we focus on Contrastive Language-Image Pre-training (CLIP), an open-vocabulary foundation model, which achieves high accuracy across many image classification tasks and is often competitive with a fully supervised baseline without being explicitly trained. Nevertheless, there are still domains where zero-shot CLIP performance is far from optimal, such as Remote Sensing (RS) and medical imagery. These domains do not only exhibit fundamentally different distributions compared to natural images, but also commonly rely on complementary modalities, beyond RGB, to derive meaningful insights. To this end, we propose a methodology for the purpose of aligning distinct RS imagery modalities with the visual and textual modalities of CLIP. Our two-stage procedure, comprises of robust fine-tuning CLIP in order to deal with the distribution shift, accompanied by the cross-modal alignment of a RS modality encoder, in an effort to extend the zero-shot capabilities of CLIP. We ultimately demonstrate our method on the tasks of RS imagery classification and cross-modal retrieval. We empirically show that both robust fine-tuning and cross-modal alignment translate to significant performance gains, across several RS benchmark datasets. Notably, these enhancements are achieved without the reliance on textual descriptions, without introducing any task-specific parameters, without training from scratch and without catastrophic forgetting.","sentences":["Deep Learning (DL) is undergoing a paradigm shift with the emergence of foundation models, aptly named by their crucial, yet incomplete nature.","In this work, we focus on Contrastive Language-Image Pre-training (CLIP), an open-vocabulary foundation model, which achieves high accuracy across many image classification tasks and is often competitive with a fully supervised baseline without being explicitly trained.","Nevertheless, there are still domains where zero-shot CLIP performance is far from optimal, such as Remote Sensing (RS) and medical imagery.","These domains do not only exhibit fundamentally different distributions compared to natural images, but also commonly rely on complementary modalities, beyond RGB, to derive meaningful insights.","To this end, we propose a methodology for the purpose of aligning distinct RS imagery modalities with the visual and textual modalities of CLIP.","Our two-stage procedure, comprises of robust fine-tuning CLIP in order to deal with the distribution shift, accompanied by the cross-modal alignment of a RS modality encoder, in an effort to extend the zero-shot capabilities of CLIP.","We ultimately demonstrate our method on the tasks of RS imagery classification and cross-modal retrieval.","We empirically show that both robust fine-tuning and cross-modal alignment translate to significant performance gains, across several RS benchmark datasets.","Notably, these enhancements are achieved without the reliance on textual descriptions, without introducing any task-specific parameters, without training from scratch and without catastrophic forgetting."],"url":"http://arxiv.org/abs/2402.09816v1","category":"cs.CV"}
{"created":"2024-02-15 07:08:11","title":"Robust SVD Made Easy: A fast and reliable algorithm for large-scale data analysis","abstract":"The singular value decomposition (SVD) is a crucial tool in machine learning and statistical data analysis. However, it is highly susceptible to outliers in the data matrix. Existing robust SVD algorithms often sacrifice speed for robustness or fail in the presence of only a few outliers. This study introduces an efficient algorithm, called Spherically Normalized SVD, for robust SVD approximation that is highly insensitive to outliers, computationally scalable, and provides accurate approximations of singular vectors. The proposed algorithm achieves remarkable speed by utilizing only two applications of a standard reduced-rank SVD algorithm to appropriately scaled data, significantly outperforming competing algorithms in computation times. To assess the robustness of the approximated singular vectors and their subspaces against data contamination, we introduce new notions of breakdown points for matrix-valued input, including row-wise, column-wise, and block-wise breakdown points. Theoretical and empirical analyses demonstrate that our algorithm exhibits higher breakdown points compared to standard SVD and its modifications. We empirically validate the effectiveness of our approach in applications such as robust low-rank approximation and robust principal component analysis of high-dimensional microarray datasets. Overall, our study presents a highly efficient and robust solution for SVD approximation that overcomes the limitations of existing algorithms in the presence of outliers.","sentences":["The singular value decomposition (SVD) is a crucial tool in machine learning and statistical data analysis.","However, it is highly susceptible to outliers in the data matrix.","Existing robust SVD algorithms often sacrifice speed for robustness or fail in the presence of only a few outliers.","This study introduces an efficient algorithm, called Spherically Normalized SVD, for robust SVD approximation that is highly insensitive to outliers, computationally scalable, and provides accurate approximations of singular vectors.","The proposed algorithm achieves remarkable speed by utilizing only two applications of a standard reduced-rank SVD algorithm to appropriately scaled data, significantly outperforming competing algorithms in computation times.","To assess the robustness of the approximated singular vectors and their subspaces against data contamination, we introduce new notions of breakdown points for matrix-valued input, including row-wise, column-wise, and block-wise breakdown points.","Theoretical and empirical analyses demonstrate that our algorithm exhibits higher breakdown points compared to standard SVD and its modifications.","We empirically validate the effectiveness of our approach in applications such as robust low-rank approximation and robust principal component analysis of high-dimensional microarray datasets.","Overall, our study presents a highly efficient and robust solution for SVD approximation that overcomes the limitations of existing algorithms in the presence of outliers."],"url":"http://arxiv.org/abs/2402.09754v1","category":"stat.ML"}
{"created":"2024-02-15 04:16:59","title":"Combining Evidence Across Filtrations","abstract":"In anytime-valid sequential inference, it is known that any admissible inference procedure must be based on test martingales and their composite generalization, called e-processes, which are nonnegative processes whose expectation at any arbitrary stopping time is upper-bounded by one. An e-process quantifies the accumulated evidence against a composite null hypothesis over a sequence of outcomes. This paper studies methods for combining e-processes that are computed using different information sets, i.e., filtrations, for a null hypothesis. Even though e-processes constructed on the same filtration can be combined effortlessly (e.g., by averaging), e-processes constructed on different filtrations cannot be combined as easily because their validity in a coarser filtration does not translate to validity in a finer filtration. We discuss three concrete examples of such e-processes in the literature: exchangeability tests, independence tests, and tests for evaluating and comparing forecasts with lags. Our main result establishes that these e-processes can be lifted into any finer filtration using adjusters, which are functions that allow betting on the running maximum of the accumulated wealth (thereby insuring against the loss of evidence). We also develop randomized adjusters that can improve the power of the resulting sequential inference procedure.","sentences":["In anytime-valid sequential inference, it is known that any admissible inference procedure must be based on test martingales and their composite generalization, called e-processes, which are nonnegative processes whose expectation at any arbitrary stopping time is upper-bounded by one.","An e-process quantifies the accumulated evidence against a composite null hypothesis over a sequence of outcomes.","This paper studies methods for combining e-processes that are computed using different information sets, i.e., filtrations, for a null hypothesis.","Even though e-processes constructed on the same filtration can be combined effortlessly (e.g., by averaging), e-processes constructed on different filtrations cannot be combined as easily because their validity in a coarser filtration does not translate to validity in a finer filtration.","We discuss three concrete examples of such e-processes in the literature: exchangeability tests, independence tests, and tests for evaluating and comparing forecasts with lags.","Our main result establishes that these e-processes can be lifted into any finer filtration using adjusters, which are functions that allow betting on the running maximum of the accumulated wealth (thereby insuring against the loss of evidence).","We also develop randomized adjusters that can improve the power of the resulting sequential inference procedure."],"url":"http://arxiv.org/abs/2402.09698v1","category":"stat.ME"}
{"created":"2024-02-15 04:06:18","title":"Seed Optimization with Frozen Generator for Superior Zero-shot Low-light Enhancement","abstract":"In this work, we observe that the generators, which are pre-trained on massive natural images, inherently hold the promising potential for superior low-light image enhancement against varying scenarios.Specifically, we embed a pre-trained generator to Retinex model to produce reflectance maps with enhanced detail and vividness, thereby recovering features degraded by low-light conditions.Taking one step further, we introduce a novel optimization strategy, which backpropagates the gradients to the input seeds rather than the parameters of the low-light enhancement model, thus intactly retaining the generative knowledge learned from natural images and achieving faster convergence speed. Benefiting from the pre-trained knowledge and seed-optimization strategy, the low-light enhancement model can significantly regularize the realness and fidelity of the enhanced result, thus rapidly generating high-quality images without training on any low-light dataset. Extensive experiments on various benchmarks demonstrate the superiority of the proposed method over numerous state-of-the-art methods qualitatively and quantitatively.","sentences":["In this work, we observe that the generators, which are pre-trained on massive natural images, inherently hold the promising potential for superior low-light image enhancement against varying scenarios.","Specifically, we embed a pre-trained generator to Retinex model to produce reflectance maps with enhanced detail and vividness, thereby recovering features degraded by low-light conditions.","Taking one step further, we introduce a novel optimization strategy, which backpropagates the gradients to the input seeds rather than the parameters of the low-light enhancement model, thus intactly retaining the generative knowledge learned from natural images and achieving faster convergence speed.","Benefiting from the pre-trained knowledge and seed-optimization strategy, the low-light enhancement model can significantly regularize the realness and fidelity of the enhanced result, thus rapidly generating high-quality images without training on any low-light dataset.","Extensive experiments on various benchmarks demonstrate the superiority of the proposed method over numerous state-of-the-art methods qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2402.09694v1","category":"cs.CV"}
{"created":"2024-02-15 03:45:44","title":"Robust Learning-Augmented Dictionaries","abstract":"We present the first learning-augmented data structure for implementing dictionaries with optimal consistency and robustness. Our data structure, named RobustSL, is a skip list augmented by predictions of access frequencies of elements in a data sequence. With proper predictions, RobustSL has optimal consistency (achieves static optimality). At the same time, it maintains a logarithmic running time for each operation, ensuring optimal robustness, even if predictions are generated adversarially. Therefore, RobustSL has all the advantages of the recent learning-augmented data structures of Lin, Luo, and Woodruff (ICML 2022) and Cao et al. (arXiv 2023), while providing robustness guarantees that are absent in the previous work. Numerical experiments show that RobustSL outperforms alternative data structures using both synthetic and real datasets.","sentences":["We present the first learning-augmented data structure for implementing dictionaries with optimal consistency and robustness.","Our data structure, named RobustSL, is a skip list augmented by predictions of access frequencies of elements in a data sequence.","With proper predictions, RobustSL has optimal consistency (achieves static optimality).","At the same time, it maintains a logarithmic running time for each operation, ensuring optimal robustness, even if predictions are generated adversarially.","Therefore, RobustSL has all the advantages of the recent learning-augmented data structures of Lin, Luo, and Woodruff (ICML 2022) and Cao et al. (arXiv 2023), while providing robustness guarantees that are absent in the previous work.","Numerical experiments show that RobustSL outperforms alternative data structures using both synthetic and real datasets."],"url":"http://arxiv.org/abs/2402.09687v1","category":"cs.DS"}
{"created":"2024-02-15 01:58:49","title":"Towards Precision Cardiovascular Analysis in Zebrafish: The ZACAF Paradigm","abstract":"Quantifying cardiovascular parameters like ejection fraction in zebrafish as a host of biological investigations has been extensively studied. Since current manual monitoring techniques are time-consuming and fallible, several image processing frameworks have been proposed to automate the process. Most of these works rely on supervised deep-learning architectures. However, supervised methods tend to be overfitted on their training dataset. This means that applying the same framework to new data with different imaging setups and mutant types can severely decrease performance. We have developed a Zebrafish Automatic Cardiovascular Assessment Framework (ZACAF) to quantify the cardiac function in zebrafish. In this work, we further applied data augmentation, Transfer Learning (TL), and Test Time Augmentation (TTA) to ZACAF to improve the performance for the quantification of cardiovascular function quantification in zebrafish. This strategy can be integrated with the available frameworks to aid other researchers. We demonstrate that using TL, even with a constrained dataset, the model can be refined to accommodate a novel microscope setup, encompassing diverse mutant types and accommodating various video recording protocols. Additionally, as users engage in successive rounds of TL, the model is anticipated to undergo substantial enhancements in both generalizability and accuracy. Finally, we applied this approach to assess the cardiovascular function in nrap mutant zebrafish, a model of cardiomyopathy.","sentences":["Quantifying cardiovascular parameters like ejection fraction in zebrafish as a host of biological investigations has been extensively studied.","Since current manual monitoring techniques are time-consuming and fallible, several image processing frameworks have been proposed to automate the process.","Most of these works rely on supervised deep-learning architectures.","However, supervised methods tend to be overfitted on their training dataset.","This means that applying the same framework to new data with different imaging setups and mutant types can severely decrease performance.","We have developed a Zebrafish Automatic Cardiovascular Assessment Framework (ZACAF) to quantify the cardiac function in zebrafish.","In this work, we further applied data augmentation, Transfer Learning (TL), and Test Time Augmentation (TTA) to ZACAF to improve the performance for the quantification of cardiovascular function quantification in zebrafish.","This strategy can be integrated with the available frameworks to aid other researchers.","We demonstrate that using TL, even with a constrained dataset, the model can be refined to accommodate a novel microscope setup, encompassing diverse mutant types and accommodating various video recording protocols.","Additionally, as users engage in successive rounds of TL, the model is anticipated to undergo substantial enhancements in both generalizability and accuracy.","Finally, we applied this approach to assess the cardiovascular function in nrap mutant zebrafish, a model of cardiomyopathy."],"url":"http://arxiv.org/abs/2402.09658v1","category":"eess.IV"}
{"created":"2024-02-15 01:48:41","title":"Evaluating Atypical Gaze Patterns through Vision Models: The Case of Cortical Visual Impairment","abstract":"A wide range of neurological and cognitive disorders exhibit distinct behavioral markers aside from their clinical manifestations. Cortical Visual Impairment (CVI) is a prime example of such conditions, resulting from damage to visual pathways in the brain, and adversely impacting low- and high-level visual function. The characteristics impacted by CVI are primarily described qualitatively, challenging the establishment of an objective, evidence-based measure of CVI severity. To study those characteristics, we propose to create visual saliency maps by adequately prompting deep vision models with attributes of clinical interest. After extracting saliency maps for a curated set of stimuli, we evaluate fixation traces on those from children with CVI through eye tracking technology. Our experiments reveal significant gaze markers that verify clinical knowledge and yield nuanced discriminability when compared to those of age-matched control subjects. Using deep learning to unveil atypical visual saliency is an important step toward establishing an eye-tracking signature for severe neurodevelopmental disorders, like CVI.","sentences":["A wide range of neurological and cognitive disorders exhibit distinct behavioral markers aside from their clinical manifestations.","Cortical Visual Impairment (CVI) is a prime example of such conditions, resulting from damage to visual pathways in the brain, and adversely impacting low- and high-level visual function.","The characteristics impacted by CVI are primarily described qualitatively, challenging the establishment of an objective, evidence-based measure of CVI severity.","To study those characteristics, we propose to create visual saliency maps by adequately prompting deep vision models with attributes of clinical interest.","After extracting saliency maps for a curated set of stimuli, we evaluate fixation traces on those from children with CVI through eye tracking technology.","Our experiments reveal significant gaze markers that verify clinical knowledge and yield nuanced discriminability when compared to those of age-matched control subjects.","Using deep learning to unveil atypical visual saliency is an important step toward establishing an eye-tracking signature for severe neurodevelopmental disorders, like CVI."],"url":"http://arxiv.org/abs/2402.09655v1","category":"eess.SP"}
{"created":"2024-02-15 00:14:41","title":"Smart Information Exchange for Unsupervised Federated Learning via Reinforcement Learning","abstract":"One of the main challenges of decentralized machine learning paradigms such as Federated Learning (FL) is the presence of local non-i.i.d. datasets. Device-to-device transfers (D2D) between distributed devices has been shown to be an effective tool for dealing with this problem and robust to stragglers. In an unsupervised case, however, it is not obvious how data exchanges should take place due to the absence of labels. In this paper, we propose an approach to create an optimal graph for data transfer using Reinforcement Learning. The goal is to form links that will provide the most benefit considering the environment's constraints and improve convergence speed in an unsupervised FL environment. Numerical analysis shows the advantages in terms of convergence speed and straggler resilience of the proposed method to different available FL schemes and benchmark datasets.","sentences":["One of the main challenges of decentralized machine learning paradigms such as Federated Learning (FL) is the presence of local non-i.i.d. datasets.","Device-to-device transfers (D2D) between distributed devices has been shown to be an effective tool for dealing with this problem and robust to stragglers.","In an unsupervised case, however, it is not obvious how data exchanges should take place due to the absence of labels.","In this paper, we propose an approach to create an optimal graph for data transfer using Reinforcement Learning.","The goal is to form links that will provide the most benefit considering the environment's constraints and improve convergence speed in an unsupervised FL environment.","Numerical analysis shows the advantages in terms of convergence speed and straggler resilience of the proposed method to different available FL schemes and benchmark datasets."],"url":"http://arxiv.org/abs/2402.09629v1","category":"cs.LG"}
{"created":"2024-02-14 21:33:18","title":"MLTCP: Congestion Control for DNN Training","abstract":"We present MLTCP, a technique to augment today's congestion control algorithms to accelerate DNN training jobs in shared GPU clusters. MLTCP enables the communication phases of jobs that compete for network bandwidth to interleave with each other, thereby utilizing the network efficiently. At the heart of MLTCP lies a very simple principle based on a key conceptual insight: DNN training flows should scale their congestion window size based on the number of bytes sent at each training iteration. We show that integrating this principle into today's congestion control protocols is straightforward: by adding 30-60 lines of code to Reno, CUBIC, or DCQCN, MLTCP stabilizes flows of different jobs into an interleaved state within a few training iterations, regardless of the number of competing flows or the start time of each flow. Our experiments with popular DNN training jobs demonstrate that enabling MLTCP accelerates the average and 99th percentile training iteration time by up to 2x and 4x, respectively.","sentences":["We present MLTCP, a technique to augment today's congestion control algorithms to accelerate DNN training jobs in shared GPU clusters.","MLTCP enables the communication phases of jobs that compete for network bandwidth to interleave with each other, thereby utilizing the network efficiently.","At the heart of MLTCP lies a very simple principle based on a key conceptual insight: DNN training flows should scale their congestion window size based on the number of bytes sent at each training iteration.","We show that integrating this principle into today's congestion control protocols is straightforward: by adding 30-60 lines of code to Reno, CUBIC, or DCQCN, MLTCP stabilizes flows of different jobs into an interleaved state within a few training iterations, regardless of the number of competing flows or the start time of each flow.","Our experiments with popular DNN training jobs demonstrate that enabling MLTCP accelerates the average and 99th percentile training iteration time by up to 2x and 4x, respectively."],"url":"http://arxiv.org/abs/2402.09589v1","category":"cs.NI"}
{"created":"2024-02-14 21:31:41","title":"DeepATLAS: One-Shot Localization for Biomedical Data","abstract":"This paper introduces the DeepATLAS foundational model for localization tasks in the domain of high-dimensional biomedical data. Upon convergence of the proposed self-supervised objective, a pretrained model maps an input to an anatomically-consistent embedding from which any point or set of points (e.g., boxes or segmentations) may be identified in a one-shot or few-shot approach. As a representative benchmark, a DeepATLAS model pretrained on a comprehensive cohort of 51,000+ unlabeled 3D computed tomography exams yields high one-shot segmentation performance on over 50 anatomic structures across four different external test sets, either matching or exceeding the performance of a standard supervised learning model. Further improvements in accuracy can be achieved by adding a small amount of labeled data using either a semisupervised or more conventional fine-tuning strategy.","sentences":["This paper introduces the DeepATLAS foundational model for localization tasks in the domain of high-dimensional biomedical data.","Upon convergence of the proposed self-supervised objective, a pretrained model maps an input to an anatomically-consistent embedding from which any point or set of points (e.g., boxes or segmentations) may be identified in a one-shot or few-shot approach.","As a representative benchmark, a DeepATLAS model pretrained on a comprehensive cohort of 51,000+ unlabeled 3D computed tomography exams yields high one-shot segmentation performance on over 50 anatomic structures across four different external test sets, either matching or exceeding the performance of a standard supervised learning model.","Further improvements in accuracy can be achieved by adding a small amount of labeled data using either a semisupervised or more conventional fine-tuning strategy."],"url":"http://arxiv.org/abs/2402.09587v1","category":"cs.CV"}
{"created":"2024-02-14 21:29:28","title":"WERank: Towards Rank Degradation Prevention for Self-Supervised Learning Using Weight Regularization","abstract":"A common phenomena confining the representation quality in Self-Supervised Learning (SSL) is dimensional collapse (also known as rank degeneration), where the learned representations are mapped to a low dimensional subspace of the representation space. The State-of-the-Art SSL methods have shown to suffer from dimensional collapse and fall behind maintaining full rank. Recent approaches to prevent this problem have proposed using contrastive losses, regularization techniques, or architectural tricks. We propose WERank, a new regularizer on the weight parameters of the network to prevent rank degeneration at different layers of the network. We provide empirical evidence and mathematical justification to demonstrate the effectiveness of the proposed regularization method in preventing dimensional collapse. We verify the impact of WERank on graph SSL where dimensional collapse is more pronounced due to the lack of proper data augmentation. We empirically demonstrate that WERank is effective in helping BYOL to achieve higher rank during SSL pre-training and consequently downstream accuracy during evaluation probing. Ablation studies and experimental analysis shed lights on the underlying factors behind the performance gains of the proposed approach.","sentences":["A common phenomena confining the representation quality in Self-Supervised Learning (SSL) is dimensional collapse (also known as rank degeneration), where the learned representations are mapped to a low dimensional subspace of the representation space.","The State-of-the-Art SSL methods have shown to suffer from dimensional collapse and fall behind maintaining full rank.","Recent approaches to prevent this problem have proposed using contrastive losses, regularization techniques, or architectural tricks.","We propose WERank, a new regularizer on the weight parameters of the network to prevent rank degeneration at different layers of the network.","We provide empirical evidence and mathematical justification to demonstrate the effectiveness of the proposed regularization method in preventing dimensional collapse.","We verify the impact of WERank on graph SSL where dimensional collapse is more pronounced due to the lack of proper data augmentation.","We empirically demonstrate that WERank is effective in helping BYOL to achieve higher rank during SSL pre-training and consequently downstream accuracy during evaluation probing.","Ablation studies and experimental analysis shed lights on the underlying factors behind the performance gains of the proposed approach."],"url":"http://arxiv.org/abs/2402.09586v1","category":"cs.LG"}
{"created":"2024-02-14 21:06:39","title":"Finnish primary school students' conceptions of machine learning","abstract":"Objective This study investigates what kind of conceptions primary school students have about ML if they are not conceptually \"primed\" with the idea that in ML, humans teach computers. Method Qualitative survey responses from 197 Finnish primary schoolers were analyzed via an abductive method. Findings We identified three partly overlapping ML conception categories, starting from the most accurate one: ML is about teaching machines (34%), ML is about coding (7.6%), and ML is about learning via or about machines (37.1%). Implications The findings suggest that without conceptual clues, children's conceptions of ML are varied and may include misconceptions such as ML is about learning via or about machines. The findings underline the importance of clear and systematic use of key concepts in computer science education. Besides researchers, this study offers insights for teachers, teacher educators, curriculum developers, and policymakers. Method Qualitative survey responses from 197 Finnish primary schoolers were analyzed via an abductive method. Findings We identified three partly overlapping ML conception categories, starting from the most accurate one: ML is about teaching machines (34%), ML is about coding (7.6%), and ML is about learning via or about machines (37.1%). Implications The findings suggest that without conceptual clues, children's conceptions of ML are varied and may include misconceptions such as ML is about learning via or about machines. The findings underline the importance of clear and systematic use of key concepts in computer science education. Besides researchers, this study offers insights for teachers, teacher educators, curriculum developers, and policymakers.","sentences":["Objective This study investigates what kind of conceptions primary school students have about ML if they are not conceptually \"primed\" with the idea that in ML, humans teach computers.","Method Qualitative survey responses from 197 Finnish primary schoolers were analyzed via an abductive method.","Findings We identified three partly overlapping ML conception categories, starting from the most accurate one: ML is about teaching machines (34%), ML is about coding (7.6%), and ML is about learning via or about machines (37.1%).","Implications The findings suggest that without conceptual clues, children's conceptions of ML are varied and may include misconceptions such as ML is about learning via or about machines.","The findings underline the importance of clear and systematic use of key concepts in computer science education.","Besides researchers, this study offers insights for teachers, teacher educators, curriculum developers, and policymakers.","Method Qualitative survey responses from 197 Finnish primary schoolers were analyzed via an abductive method.","Findings We identified three partly overlapping ML conception categories, starting from the most accurate one: ML is about teaching machines (34%), ML is about coding (7.6%), and ML is about learning via or about machines (37.1%).","Implications The findings suggest that without conceptual clues, children's conceptions of ML are varied and may include misconceptions such as ML is about learning via or about machines.","The findings underline the importance of clear and systematic use of key concepts in computer science education.","Besides researchers, this study offers insights for teachers, teacher educators, curriculum developers, and policymakers."],"url":"http://arxiv.org/abs/2402.09582v1","category":"cs.CY"}
{"created":"2024-02-14 20:21:43","title":"Distribution-Free Rates in Neyman-Pearson Classification","abstract":"We consider the problem of Neyman-Pearson classification which models unbalanced classification settings where error w.r.t. a distribution $\\mu_1$ is to be minimized subject to low error w.r.t. a different distribution $\\mu_0$. Given a fixed VC class $\\mathcal{H}$ of classifiers to be minimized over, we provide a full characterization of possible distribution-free rates, i.e., minimax rates over the space of all pairs $(\\mu_0, \\mu_1)$. The rates involve a dichotomy between hard and easy classes $\\mathcal{H}$ as characterized by a simple geometric condition, a three-points-separation condition, loosely related to VC dimension.","sentences":["We consider the problem of Neyman-Pearson classification which models unbalanced classification settings where error w.r.t.","a distribution $\\mu_1$ is to be minimized subject to low error w.r.t.","a different distribution $\\mu_0$. Given a fixed VC class $\\mathcal{H}$ of classifiers to be minimized over, we provide a full characterization of possible distribution-free rates, i.e., minimax rates over the space of all pairs $(\\mu_0, \\mu_1)$. The rates involve a dichotomy between hard and easy classes $\\mathcal{H}$ as characterized by a simple geometric condition, a three-points-separation condition, loosely related to VC dimension."],"url":"http://arxiv.org/abs/2402.09560v1","category":"cs.LG"}
{"created":"2024-02-14 19:53:46","title":"Pareto-Optimal Algorithms for Learning in Games","abstract":"We study the problem of characterizing optimal learning algorithms for playing repeated games against an adversary with unknown payoffs. In this problem, the first player (called the learner) commits to a learning algorithm against a second player (called the optimizer), and the optimizer best-responds by choosing the optimal dynamic strategy for their (unknown but well-defined) payoff. Classic learning algorithms (such as no-regret algorithms) provide some counterfactual guarantees for the learner, but might perform much more poorly than other learning algorithms against particular optimizer payoffs.   In this paper, we introduce the notion of asymptotically Pareto-optimal learning algorithms. Intuitively, if a learning algorithm is Pareto-optimal, then there is no other algorithm which performs asymptotically at least as well against all optimizers and performs strictly better (by at least $\\Omega(T)$) against some optimizer. We show that well-known no-regret algorithms such as Multiplicative Weights and Follow The Regularized Leader are Pareto-dominated. However, while no-regret is not enough to ensure Pareto-optimality, we show that a strictly stronger property, no-swap-regret, is a sufficient condition for Pareto-optimality.   Proving these results requires us to address various technical challenges specific to repeated play, including the fact that there is no simple characterization of how optimizers who are rational in the long-term best-respond against a learning algorithm over multiple rounds of play. To address this, we introduce the idea of the asymptotic menu of a learning algorithm: the convex closure of all correlated distributions over strategy profiles that are asymptotically implementable by an adversary. We show that all no-swap-regret algorithms share the same asymptotic menu, implying that all no-swap-regret algorithms are ``strategically equivalent''.","sentences":["We study the problem of characterizing optimal learning algorithms for playing repeated games against an adversary with unknown payoffs.","In this problem, the first player (called the learner) commits to a learning algorithm against a second player (called the optimizer), and the optimizer best-responds by choosing the optimal dynamic strategy for their (unknown but well-defined) payoff.","Classic learning algorithms (such as no-regret algorithms) provide some counterfactual guarantees for the learner, but might perform much more poorly than other learning algorithms against particular optimizer payoffs.   ","In this paper, we introduce the notion of asymptotically Pareto-optimal learning algorithms.","Intuitively, if a learning algorithm is Pareto-optimal, then there is no other algorithm which performs asymptotically at least as well against all optimizers and performs strictly better (by at least $\\Omega(T)$) against some optimizer.","We show that well-known no-regret algorithms such as Multiplicative Weights and Follow The Regularized Leader are Pareto-dominated.","However, while no-regret is not enough to ensure Pareto-optimality, we show that a strictly stronger property, no-swap-regret, is a sufficient condition for Pareto-optimality.   ","Proving these results requires us to address various technical challenges specific to repeated play, including the fact that there is no simple characterization of how optimizers who are rational in the long-term best-respond against a learning algorithm over multiple rounds of play.","To address this, we introduce the idea of the asymptotic menu of a learning algorithm: the convex closure of all correlated distributions over strategy profiles that are asymptotically implementable by an adversary.","We show that all no-swap-regret algorithms share the same asymptotic menu, implying that all no-swap-regret algorithms are ``strategically equivalent''."],"url":"http://arxiv.org/abs/2402.09549v1","category":"cs.GT"}
{"created":"2024-02-15 17:50:18","title":"Optimizing Temporal Waveform Analysis: A Novel Pipeline for Efficient Characterization of Left Coronary Artery Velocity Profiles","abstract":"Continuously measured arterial blood velocity can provide insight into physiological parameters and potential disease states. The efficient and effective description of the temporal profiles of arterial velocity is crucial for both clinical practice and research. We propose a pipeline to identify the minimum number of points of interest to adequately describe a velocity profile of the left coronary artery. This pipeline employs a novel operation that \"stretches\" a baseline waveform to quantify the utility of a point in fitting other waveforms. Our study introduces a comprehensive pipeline specifically designed to identify the minimal yet crucial number of points needed to accurately represent the velocity profile of the left coronary artery. Additionally, the only location-dependent portion of this pipeline is the first step, choosing all of the possible points of interest. Hence, this work is broadly applicable to other waveforms. This versatility paves the way for a novel non-frequency domain method that can enhance the analysis of physiological waveforms. Such advancements have potential implications in both research and clinical treatment of various diseases, underscoring the broader applicability and impact.","sentences":["Continuously measured arterial blood velocity can provide insight into physiological parameters and potential disease states.","The efficient and effective description of the temporal profiles of arterial velocity is crucial for both clinical practice and research.","We propose a pipeline to identify the minimum number of points of interest to adequately describe a velocity profile of the left coronary artery.","This pipeline employs a novel operation that \"stretches\" a baseline waveform to quantify the utility of a point in fitting other waveforms.","Our study introduces a comprehensive pipeline specifically designed to identify the minimal yet crucial number of points needed to accurately represent the velocity profile of the left coronary artery.","Additionally, the only location-dependent portion of this pipeline is the first step, choosing all of the possible points of interest.","Hence, this work is broadly applicable to other waveforms.","This versatility paves the way for a novel non-frequency domain method that can enhance the analysis of physiological waveforms.","Such advancements have potential implications in both research and clinical treatment of various diseases, underscoring the broader applicability and impact."],"url":"http://arxiv.org/abs/2402.10146v1","category":"q-bio.QM"}
{"created":"2024-02-15 14:54:33","title":"LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition","abstract":"Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two major limitations of existing GMNER methods. 2) The introduction of entity expansion expression and Visual Entailment (VE) Module unifies Visual Grounding (VG) and Entity Grounding (EG). It enables RiVEG to effortlessly inherit the Visual Entailment and Visual Grounding capabilities of any current or prospective multimodal pretraining models. Extensive experiments demonstrate that RiVEG outperforms state-of-the-art methods on the existing GMNER dataset and achieves absolute leads of 10.65%, 6.21%, and 8.83% in all three subtasks.","sentences":["Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions.","GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable.","2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities.","In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge.","This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two major limitations of existing GMNER methods.","2)","The introduction of entity expansion expression and Visual Entailment (VE) Module unifies Visual Grounding (VG) and Entity Grounding (EG).","It enables RiVEG to effortlessly inherit the Visual Entailment and Visual Grounding capabilities of any current or prospective multimodal pretraining models.","Extensive experiments demonstrate that RiVEG outperforms state-of-the-art methods on the existing GMNER dataset and achieves absolute leads of 10.65%, 6.21%, and 8.83% in all three subtasks."],"url":"http://arxiv.org/abs/2402.09989v1","category":"cs.CV"}
{"created":"2024-02-15 12:12:43","title":"On the convergence of Block Majorization-Minimization algorithms on the Grassmann Manifold","abstract":"The Majorization-Minimization (MM) framework is widely used to derive efficient algorithms for specific problems that require the optimization of a cost function (which can be convex or not). It is based on a sequential optimization of a surrogate function over closed convex sets. A natural extension of this framework incorporates ideas of Block Coordinate Descent (BCD) algorithms into the MM framework, also known as block MM. The rationale behind the block extension is to partition the optimization variables into several independent blocks, to obtain a surrogate for each block, and to optimize the surrogate of each block cyclically. The advantage of the block MM is that the construction and successive optimization of the surrogate functions is potentially easier than with the non-block alternative. The purpose of this letter is to exploit the geometrical properties of the Grassmann manifold (a non-convex set) for the purpose of extending classical convergence proofs of the block MM when at least one of the blocks is constrained in this manifold.","sentences":["The Majorization-Minimization (MM) framework is widely used to derive efficient algorithms for specific problems that require the optimization of a cost function (which can be convex or not).","It is based on a sequential optimization of a surrogate function over closed convex sets.","A natural extension of this framework incorporates ideas of Block Coordinate Descent (BCD) algorithms into the MM framework, also known as block MM.","The rationale behind the block extension is to partition the optimization variables into several independent blocks, to obtain a surrogate for each block, and to optimize the surrogate of each block cyclically.","The advantage of the block MM is that the construction and successive optimization of the surrogate functions is potentially easier than with the non-block alternative.","The purpose of this letter is to exploit the geometrical properties of the Grassmann manifold (a non-convex set) for the purpose of extending classical convergence proofs of the block MM when at least one of the blocks is constrained in this manifold."],"url":"http://arxiv.org/abs/2402.09907v1","category":"math.OC"}
{"created":"2024-02-15 07:08:27","title":"Data Smoothing Filling Method based on ScRNA-Seq Data Zero-Value Identification","abstract":"Single-cell RNA sequencing (scRNA-seq) determines RNA expression at single-cell resolution. It provides a powerful tool for studying immunity, regulation, and other life activities of cells. However, due to the limitations of the sequencing technique, the scRNA-seq data are represented with sparsity, whichcontains missing gene values, i.e., zero values, called dropout. Therefore, it is necessary to impute missing values before analyzing scRNA-seq data. However, existing imputation computation methods often only focus on the identification of technical zeros or imputing all zeros based on cell similarity. This study proposes a new method (SFAG) to reconstruct the gene expression relationship matrix by usinggraph regularization technology to preserve the high-dimensional manifold information of the data, andto mine the relationship between genes and cells in the data, and then uses a method of averaging the clustering results to fill in the identified technical zeros. Experimental results show that SFAGcan helpimprove downstream analysis and reconstruct cell trajectory","sentences":["Single-cell RNA sequencing (scRNA-seq) determines RNA expression at single-cell resolution.","It provides a powerful tool for studying immunity, regulation, and other life activities of cells.","However, due to the limitations of the sequencing technique, the scRNA-seq data are represented with sparsity, whichcontains missing gene values, i.e., zero values, called dropout.","Therefore, it is necessary to impute missing values before analyzing scRNA-seq data.","However, existing imputation computation methods often only focus on the identification of technical zeros or imputing all zeros based on cell similarity.","This study proposes a new method (SFAG) to reconstruct the gene expression relationship matrix by usinggraph regularization technology to preserve the high-dimensional manifold information of the data, andto mine the relationship between genes and cells in the data, and then uses a method of averaging the clustering results to fill in the identified technical zeros.","Experimental results show that SFAGcan helpimprove downstream analysis and reconstruct cell trajectory"],"url":"http://arxiv.org/abs/2402.09755v1","category":"q-bio.GN"}
{"created":"2024-02-15 04:01:13","title":"Graphons and the $H$-property","abstract":"A graphon satisfies the $H$-property if graphs sampled from it contain a Hamiltonian decomposition almost surely, which in turn implies that the corresponding network topologies are, e.g., structurally stable and structurally ensemble controllable. In recent papers, we have exhibited a set of conditions that is essentially necessary and sufficient for the $H$-property to hold for the finite-dimensional class of step-graphons. The extension to the infinite-dimensional case of general graphons was hindered by the fact that said conditions relied on objects that do not admit immediate extensions to the infinite-dimensional case. We outline here our approach to bypass this difficulty and state conditions that guarantee that the $H$-property holds for general graphons.","sentences":["A graphon satisfies the $H$-property if graphs sampled from it contain a Hamiltonian decomposition almost surely, which in turn implies that the corresponding network topologies are, e.g., structurally stable and structurally ensemble controllable.","In recent papers, we have exhibited a set of conditions that is essentially necessary and sufficient for the $H$-property to hold for the finite-dimensional class of step-graphons.","The extension to the infinite-dimensional case of general graphons was hindered by the fact that said conditions relied on objects that do not admit immediate extensions to the infinite-dimensional case.","We outline here our approach to bypass this difficulty and state conditions that guarantee that the $H$-property holds for general graphons."],"url":"http://arxiv.org/abs/2402.09692v1","category":"math.OC"}
{"created":"2024-02-15 02:44:44","title":"Subspace Decomposition of Coset Codes","abstract":"A new method is explored for analyzing the performance of coset codes over the binary erasure wiretap channel (BEWC) by decomposing the code over subspaces of the code space. This technique leads to an improved algorithm for calculating equivocation loss. It also provides a continuous-valued function for equivocation loss, permitting proofs of local optimality for certain finite-blocklength code constructions, including a code formed by excluding from the generator matrix all columns which lie within a particular subspace. Subspace decomposition is also used to explore the properties of an alternative secrecy code metric, the chi squared divergence. The chi squared divergence is shown to be far simpler to calculate than equivocation loss. Additionally, the codes which are shown to be locally optimal in terms of equivocation are also proved to be globally optimal in terms of chi squared divergence.","sentences":["A new method is explored for analyzing the performance of coset codes over the binary erasure wiretap channel (BEWC) by decomposing the code over subspaces of the code space.","This technique leads to an improved algorithm for calculating equivocation loss.","It also provides a continuous-valued function for equivocation loss, permitting proofs of local optimality for certain finite-blocklength code constructions, including a code formed by excluding from the generator matrix all columns which lie within a particular subspace.","Subspace decomposition is also used to explore the properties of an alternative secrecy code metric, the chi squared divergence.","The chi squared divergence is shown to be far simpler to calculate than equivocation loss.","Additionally, the codes which are shown to be locally optimal in terms of equivocation are also proved to be globally optimal in terms of chi squared divergence."],"url":"http://arxiv.org/abs/2402.09673v1","category":"cs.IT"}
{"created":"2024-02-15 00:02:41","title":"Degrees of the Wasserstein Distance to Small Toric Models","abstract":"The study of the closest point(s) on a statistical model from a given distribution in the probability simplex with respect to a fixed Wasserstein metric gives rise to a polyhedral norm distance optimization problem. There are two components to the complexity of determining the Wasserstein distance from a data point to a model. One is the combinatorial complexity that is governed by the combinatorics of the Lipschitz polytope of the finite metric to be used. Another is the algebraic complexity, which is governed by the polar degrees of the Zariski closure of the model. We find formulas for the polar degrees of rational normal scrolls and graphical models whose underlying graphs are star trees. Also, the polar degrees of the graphical models with four binary random variables where the graphs are a path on four vertices and the four-cycle, as well as for small, no-three-way interaction models, were computed. We investigate the algebraic degree of computing the Wasserstein distance to a small subset of these models. It was observed that this algebraic degree is typically smaller than the corresponding polar degree.","sentences":["The study of the closest point(s) on a statistical model from a given distribution in the probability simplex with respect to a fixed Wasserstein metric gives rise to a polyhedral norm distance optimization problem.","There are two components to the complexity of determining the Wasserstein distance from a data point to a model.","One is the combinatorial complexity that is governed by the combinatorics of the Lipschitz polytope of the finite metric to be used.","Another is the algebraic complexity, which is governed by the polar degrees of the Zariski closure of the model.","We find formulas for the polar degrees of rational normal scrolls and graphical models whose underlying graphs are star trees.","Also, the polar degrees of the graphical models with four binary random variables where the graphs are a path on four vertices and the four-cycle, as well as for small, no-three-way interaction models, were computed.","We investigate the algebraic degree of computing the Wasserstein distance to a small subset of these models.","It was observed that this algebraic degree is typically smaller than the corresponding polar degree."],"url":"http://arxiv.org/abs/2402.09626v1","category":"math.AG"}
{"created":"2024-02-14 23:40:36","title":"Schnorr Approval-Based Secure and Privacy-Preserving IoV Data Aggregation","abstract":"Secure and privacy-preserving data aggregation in the Internet of Vehicles (IoV) continues to be a focal point of interest in both the industry and academia. Aiming at tackling the challenges and solving the remaining limitations of existing works, this paper introduces a novel Schnorr approval-based IoV data aggregation framework based on a two-layered architecture. In this framework, a server can aggregate the IoV data from clusters without inferring the raw data, real identity and trajectories of vehicles. Notably, we avoid incorporating the widely-accepted techniques such as homomorphic encryption and digital pseudonym to avoid introducing high computation cost to vehicles. We propose a novel concept, data approval, based on the Schnorr signature scheme. With the approval, the fake data injection attack carried out by a cluster head can be defended against. The separation of liability is achieved as well. The evaluation shows that the framework is secure and lightweight for vehicles in terms of the computation and communication costs.","sentences":["Secure and privacy-preserving data aggregation in the Internet of Vehicles (IoV) continues to be a focal point of interest in both the industry and academia.","Aiming at tackling the challenges and solving the remaining limitations of existing works, this paper introduces a novel Schnorr approval-based IoV data aggregation framework based on a two-layered architecture.","In this framework, a server can aggregate the IoV data from clusters without inferring the raw data, real identity and trajectories of vehicles.","Notably, we avoid incorporating the widely-accepted techniques such as homomorphic encryption and digital pseudonym to avoid introducing high computation cost to vehicles.","We propose a novel concept, data approval, based on the Schnorr signature scheme.","With the approval, the fake data injection attack carried out by a cluster head can be defended against.","The separation of liability is achieved as well.","The evaluation shows that the framework is secure and lightweight for vehicles in terms of the computation and communication costs."],"url":"http://arxiv.org/abs/2402.09621v1","category":"cs.CR"}
{"created":"2024-02-14 22:10:21","title":"Consecutive Power Occurrences in Sturmian Words","abstract":"We show that every Sturmian word has the property that the distance between consecutive ending positions of cubes occurring in the word is always bounded by $10$ and this bound is optimal, extending a result of Rampersad, who proved that the bound $9$ holds for the Fibonacci word. We then give a general result showing that for every $e \\in [1,(5+\\sqrt{5})/2)$ there is a natural number $N$, depending only on $e$, such that every Sturmian word has the property that the distance between consecutive ending positions of $e$-powers occurring in the word is uniformly bounded by $N$.","sentences":["We show that every Sturmian word has the property that the distance between consecutive ending positions of cubes occurring in the word is always bounded by $10$ and this bound is optimal, extending a result of Rampersad, who proved that the bound $9$ holds for the Fibonacci word.","We then give a general result showing that for every $e \\in","[1,(5+\\sqrt{5})/2)$ there is a natural number $N$, depending only on $e$, such that every Sturmian word has the property that the distance between consecutive ending positions of $e$-powers occurring in the word is uniformly bounded by $N$."],"url":"http://arxiv.org/abs/2402.09597v1","category":"math.CO"}
{"created":"2024-02-14 20:42:33","title":"Anisotropic Dark Energy from String Compactifications","abstract":"We explore the cosmological dynamics of a minimalistic yet generic string-inspired model for multifield dark energy. Adopting a supergravity four-dimensional viewpoint, we motivate the model's structure arising from superstring compactifications involving a chiral superfield and a pure $U(1)$ gauge sector. The chiral sector gives rise to a pair of scalar fields, such as the axio-dilaton, which are kinetically coupled. However, the scalar potential depends on only one of them, further entwined with the vector field through the gauge kinetic function. The model has two anisotropic attractor solutions that, despite a steep potential and thanks to multifield dynamics, could explain the current accelerated expansion of the Universe while satisfying observational constraints on the late-times cosmological anisotropy. Nevertheless, justifying the parameter space allowing for slow roll dynamics together with the correct cosmological parameters, would be challenging within the landscape of string theory. Intriguingly, we find that the vector field, particularly at one of the studied fixed points, plays a crucial role in enabling geodesic trajectories in the scalar field space while realizing slow-roll dynamics with a steep potential. This observation opens a new avenue for exploring multifield dark energy models within the superstring landscape.","sentences":["We explore the cosmological dynamics of a minimalistic yet generic string-inspired model for multifield dark energy.","Adopting a supergravity four-dimensional viewpoint, we motivate the model's structure arising from superstring compactifications involving a chiral superfield and a pure $U(1)$ gauge sector.","The chiral sector gives rise to a pair of scalar fields, such as the axio-dilaton, which are kinetically coupled.","However, the scalar potential depends on only one of them, further entwined with the vector field through the gauge kinetic function.","The model has two anisotropic attractor solutions that, despite a steep potential and thanks to multifield dynamics, could explain the current accelerated expansion of the Universe while satisfying observational constraints on the late-times cosmological anisotropy.","Nevertheless, justifying the parameter space allowing for slow roll dynamics together with the correct cosmological parameters, would be challenging within the landscape of string theory.","Intriguingly, we find that the vector field, particularly at one of the studied fixed points, plays a crucial role in enabling geodesic trajectories in the scalar field space while realizing slow-roll dynamics with a steep potential.","This observation opens a new avenue for exploring multifield dark energy models within the superstring landscape."],"url":"http://arxiv.org/abs/2402.09570v1","category":"hep-th"}
{"created":"2024-02-14 19:50:27","title":"Does bilevel optimization result in more competitive racing behavior?","abstract":"Two-vehicle racing is natural example of a competitive dynamic game. As with most dynamic games, there are many ways in which the underlying information pattern can be structured, resulting in different equilibrium concepts. For racing in particular, the information pattern assumed plays a large impact in the type of behaviors that can emerge from the two interacting players. For example, blocking behavior is something that cannot emerge from static Nash play, but could presumably emerge from leader-follower play. In this work, we develop a novel model for competitive two-player vehicle racing, complete with simplified aerodynamic drag and drafting effects, as well as position-dependent collision-avoidance responsibility. We use this model to explore the impact that different information patterns have on the resulting competitiveness of the players. A solution approach for solving bilevel optimization problems is developed, which allows us to run a large-scale empirical study comparing how bilevel strategy generation (both as leader and as follower) compares with Nash equilibrium strategy generation as well as a single-player, constant velocity prediction baseline. Each of these choices are evaluated against different combinations of opponent strategy selection method. The somewhat surprising results of this study are discussed throughout.","sentences":["Two-vehicle racing is natural example of a competitive dynamic game.","As with most dynamic games, there are many ways in which the underlying information pattern can be structured, resulting in different equilibrium concepts.","For racing in particular, the information pattern assumed plays a large impact in the type of behaviors that can emerge from the two interacting players.","For example, blocking behavior is something that cannot emerge from static Nash play, but could presumably emerge from leader-follower play.","In this work, we develop a novel model for competitive two-player vehicle racing, complete with simplified aerodynamic drag and drafting effects, as well as position-dependent collision-avoidance responsibility.","We use this model to explore the impact that different information patterns have on the resulting competitiveness of the players.","A solution approach for solving bilevel optimization problems is developed, which allows us to run a large-scale empirical study comparing how bilevel strategy generation (both as leader and as follower) compares with Nash equilibrium strategy generation as well as a single-player, constant velocity prediction baseline.","Each of these choices are evaluated against different combinations of opponent strategy selection method.","The somewhat surprising results of this study are discussed throughout."],"url":"http://arxiv.org/abs/2402.09548v1","category":"cs.GT"}
{"created":"2024-02-14 19:08:30","title":"Jasper: Scalable and Fair Multicast for Financial Exchanges in the Cloud","abstract":"Financial exchanges have recently shown an interest in migrating to the public cloud for scalability, elasticity, and cost savings. However, financial exchanges often have strict network requirements that can be difficult to meet on the cloud. Notably, market participants (MPs) trade based on market data about different activities in the market. Exchanges often use switch multicast to disseminate market data to MPs. However, if one MP receives market data earlier than another, that MP would have an unfair advantage. To prevent this, financial exchanges often equalize exchange-to-MP cable lengths to provide near-simultaneous reception of market data at MPs.   As a cloud tenant, however, building a fair multicast service is challenging because of the lack of switch support for multicast, high latency variance, and the lack of native mechanisms for simultaneous data delivery in the cloud. Jasper introduces a solution that creates an overlay multicast tree within a cloud region that minimizes latency and latency variations through hedging, leverages recent advancements in clock synchronization to achieve simultaneous delivery, and addresses various sources of latency through an optimized DPDK/eBPF implementation -- while scaling to 1000+ emulated receivers. Jasper outperforms a prior system CloudEx and a commercial multicast solution provided by Amazon Web Services.","sentences":["Financial exchanges have recently shown an interest in migrating to the public cloud for scalability, elasticity, and cost savings.","However, financial exchanges often have strict network requirements that can be difficult to meet on the cloud.","Notably, market participants (MPs) trade based on market data about different activities in the market.","Exchanges often use switch multicast to disseminate market data to MPs.","However, if one MP receives market data earlier than another, that MP would have an unfair advantage.","To prevent this, financial exchanges often equalize exchange-to-MP cable lengths to provide near-simultaneous reception of market data at MPs.   ","As a cloud tenant, however, building a fair multicast service is challenging because of the lack of switch support for multicast, high latency variance, and the lack of native mechanisms for simultaneous data delivery in the cloud.","Jasper introduces a solution that creates an overlay multicast tree within a cloud region that minimizes latency and latency variations through hedging, leverages recent advancements in clock synchronization to achieve simultaneous delivery, and addresses various sources of latency through an optimized DPDK/eBPF implementation -- while scaling to 1000+ emulated receivers.","Jasper outperforms a prior system CloudEx and a commercial multicast solution provided by Amazon Web Services."],"url":"http://arxiv.org/abs/2402.09527v1","category":"cs.NI"}
{"created":"2024-02-14 19:00:00","title":"Exploring the Flavor Symmetry Landscape","abstract":"We explore flavor dynamics in the broad scenario of a strongly interacting light Higgs (SILH). Our study focuses on the mechanism of partial fermion compositeness, but is otherwise as systematic as possible. Concretely, we classify the options for the underlying flavor (and CP) symmetries, which are necessary in order to bring this scenario safely within the range of present and future explorations. Our main goal in this context is to provide a practical map between the space of hypotheses (the models) and the experimental ground that will be explored in the medium and long term, in both indirect and direct searches, in practice at HL-LHC and Belle II, in EDM searches and eventually at FCC-hh. Our study encompasses scenarios with the maximal possible flavor symmetry, corresponding to minimal flavor violation (MFV), scenarios with no symmetry, corresponding to the so-called flavor anarchy, and various intermediate cases that complete the picture. One main result is that the scenarios that allow for the lowest new physics scale have intermediate flavor symmetry rather than the maximal symmetry of MFV models. Such optimal models are rather resilient to indirect exploration via flavor and CP violating observables, and can only be satisfactorily explored at a future high-energy collider. On the other hand, the next two decades of indirect exploration will significantly stress the parameter space of a large swat of less optimal but more generic models up to mass scales competing with those of the FCC-hh.","sentences":["We explore flavor dynamics in the broad scenario of a strongly interacting light Higgs (SILH).","Our study focuses on the mechanism of partial fermion compositeness, but is otherwise as systematic as possible.","Concretely, we classify the options for the underlying flavor (and CP) symmetries, which are necessary in order to bring this scenario safely within the range of present and future explorations.","Our main goal in this context is to provide a practical map between the space of hypotheses (the models) and the experimental ground that will be explored in the medium and long term, in both indirect and direct searches, in practice at HL-LHC and Belle II, in EDM searches and eventually at FCC-hh.","Our study encompasses scenarios with the maximal possible flavor symmetry, corresponding to minimal flavor violation (MFV), scenarios with no symmetry, corresponding to the so-called flavor anarchy, and various intermediate cases that complete the picture.","One main result is that the scenarios that allow for the lowest new physics scale have intermediate flavor symmetry rather than the maximal symmetry of MFV models.","Such optimal models are rather resilient to indirect exploration via flavor and CP violating observables, and can only be satisfactorily explored at a future high-energy collider.","On the other hand, the next two decades of indirect exploration will significantly stress the parameter space of a large swat of less optimal but more generic models up to mass scales competing with those of the FCC-hh."],"url":"http://arxiv.org/abs/2402.09503v1","category":"hep-ph"}
{"created":"2024-02-14 11:27:31","title":"PMGDA: A Preference-based Multiple Gradient Descent Algorithm","abstract":"It is desirable in many multi-objective machine learning applications, such as multi-task learning and multi-objective reinforcement learning, to find a Pareto optimal solution that can exactly match a given preference of decision-makers. These problems are often large-scale with available gradient information but cannot be handled very well by the existing algorithms. To tackle this critical issue, this paper proposes a novel predict-and-correct framework for locating the exact Pareto optimal solutions required by a decision maker. In the proposed framework, a constraint function is introduced in the search progress to align the solution with a user-specific preference, which can be optimized simultaneously with multiple objective functions. Experimental results show that our proposed method can efficiently find exact Pareto optimal solutions for standard benchmarks, multi-task, and multi-objective reinforcement learning problems with more than thousands of decision variables.   Code is available at: \\url{https://github.com/xzhang2523/pmgda}.","sentences":["It is desirable in many multi-objective machine learning applications, such as multi-task learning and multi-objective reinforcement learning, to find a Pareto optimal solution that can exactly match a given preference of decision-makers.","These problems are often large-scale with available gradient information but cannot be handled very well by the existing algorithms.","To tackle this critical issue, this paper proposes a novel predict-and-correct framework for locating the exact Pareto optimal solutions required by a decision maker.","In the proposed framework, a constraint function is introduced in the search progress to align the solution with a user-specific preference, which can be optimized simultaneously with multiple objective functions.","Experimental results show that our proposed method can efficiently find exact Pareto optimal solutions for standard benchmarks, multi-task, and multi-objective reinforcement learning problems with more than thousands of decision variables.   ","Code is available at: \\url{https://github.com/xzhang2523/pmgda}."],"url":"http://arxiv.org/abs/2402.09492v1","category":"cs.LG"}
{"created":"2024-02-14 08:09:46","title":"UMOEA/D: A Multiobjective Evolutionary Algorithm for Uniform Pareto Objectives based on Decomposition","abstract":"Multiobjective optimization (MOO) is prevalent in numerous applications, in which a Pareto front (PF) is constructed to display optima under various preferences. Previous methods commonly utilize the set of Pareto objectives (particles on the PF) to represent the entire PF. However, the empirical distribution of the Pareto objectives on the PF is rarely studied, which implicitly impedes the generation of diverse and representative Pareto objectives in previous methods. To bridge the gap, we suggest in this paper constructing \\emph{uniformly distributed} Pareto objectives on the PF, so as to alleviate the limited diversity found in previous MOO approaches. We are the first to formally define the concept of ``uniformity\" for an MOO problem. We optimize the maximal minimal distances on the Pareto front using a neural network, resulting in both asymptotically and non-asymptotically uniform Pareto objectives. Our proposed method is validated through experiments on real-world and synthetic problems, which demonstrates the efficacy in generating high-quality uniform Pareto objectives and the encouraging performance exceeding existing state-of-the-art methods.   The detailed model implementation and the code are scheduled to be open-sourced upon publication.","sentences":["Multiobjective optimization (MOO) is prevalent in numerous applications, in which a Pareto front (PF) is constructed to display optima under various preferences.","Previous methods commonly utilize the set of Pareto objectives (particles on the PF) to represent the entire PF.","However, the empirical distribution of the Pareto objectives on the PF is rarely studied, which implicitly impedes the generation of diverse and representative Pareto objectives in previous methods.","To bridge the gap, we suggest in this paper constructing \\emph{uniformly distributed} Pareto objectives on the PF, so as to alleviate the limited diversity found in previous MOO approaches.","We are the first to formally define the concept of ``uniformity\" for an MOO problem.","We optimize the maximal minimal distances on the Pareto front using a neural network, resulting in both asymptotically and non-asymptotically uniform Pareto objectives.","Our proposed method is validated through experiments on real-world and synthetic problems, which demonstrates the efficacy in generating high-quality uniform Pareto objectives and the encouraging performance exceeding existing state-of-the-art methods.   ","The detailed model implementation and the code are scheduled to be open-sourced upon publication."],"url":"http://arxiv.org/abs/2402.09486v1","category":"cs.LG"}
