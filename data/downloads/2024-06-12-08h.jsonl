{"created":"2024-06-11 17:59:56","title":"An Image is Worth 32 Tokens for Reconstruction and Generation","abstract":"Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images. Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process. Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors. However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities. To overcome this issue, we introduce Transformer-based 1-Dimensional Tokenizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences. TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques. For example, a 256 x 256 x 3 image can be reduced to just 32 discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches. Specifically, using the same generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet 256 x 256 benchmark. The advantages of TiTok become even more significant when it comes to higher resolution. At ImageNet 512 x 512 benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens by 64x, leading to 410x faster generation process. Our best-performing variant can significantly surpasses DiT-XL/2 (gFID 2.13 vs. 3.04) while still generating high-quality samples 74x faster.","sentences":["Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images.","Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process.","Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors.","However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities.","To overcome this issue, we introduce Transformer-based 1-Dimensional Tokenizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences.","TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques.","For example, a 256 x 256 x 3 image can be reduced to just 32 discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods.","Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches.","Specifically, using the same generator framework, TiTok attains 1.97 gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet 256 x 256 benchmark.","The advantages of TiTok become even more significant when it comes to higher resolution.","At ImageNet 512 x 512 benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens by 64x, leading to 410x faster generation process.","Our best-performing variant can significantly surpasses DiT-XL/2 (gFID 2.13 vs. 3.04) while still generating high-quality samples 74x faster."],"url":"http://arxiv.org/abs/2406.07550v1","category":"cs.CV"}
{"created":"2024-06-11 17:59:55","title":"A3VLM: Actionable Articulation-Aware Vision Language Model","abstract":"Vision Language Models (VLMs) have received significant attention in recent years in the robotics community. VLMs are shown to be able to perform complex visual reasoning and scene understanding tasks, which makes them regarded as a potential universal solution for general robotics problems such as manipulation and navigation. However, previous VLMs for robotics such as RT-1, RT-2, and ManipLLM~ have focused on directly learning robot-centric actions. Such approaches require collecting a significant amount of robot interaction data, which is extremely costly in the real world. Thus, we propose A3VLM, an object-centric, actionable, articulation-aware vision language model. A3VLM focuses on the articulation structure and action affordances of objects. Its representation is robot-agnostic and can be translated into robot actions using simple action primitives. Extensive experiments in both simulation benchmarks and real-world settings demonstrate the effectiveness and stability of A3VLM. We release our code and other materials at https://github.com/changhaonan/A3VLM.","sentences":["Vision Language Models (VLMs) have received significant attention in recent years in the robotics community.","VLMs are shown to be able to perform complex visual reasoning and scene understanding tasks, which makes them regarded as a potential universal solution for general robotics problems such as manipulation and navigation.","However, previous VLMs for robotics such as RT-1, RT-2, and ManipLLM~ have focused on directly learning robot-centric actions.","Such approaches require collecting a significant amount of robot interaction data, which is extremely costly in the real world.","Thus, we propose A3VLM, an object-centric, actionable, articulation-aware vision language model.","A3VLM focuses on the articulation structure and action affordances of objects.","Its representation is robot-agnostic and can be translated into robot actions using simple action primitives.","Extensive experiments in both simulation benchmarks and real-world settings demonstrate the effectiveness and stability of A3VLM.","We release our code and other materials at https://github.com/changhaonan/A3VLM."],"url":"http://arxiv.org/abs/2406.07549v1","category":"cs.RO"}
{"created":"2024-06-11 17:59:51","title":"Zero-shot Image Editing with Reference Imitation","abstract":"Image editing serves as a practical yet challenging task considering the diverse demands from users, where one of the hardest parts is to precisely describe how the edited image should look like. In this work, we present a new form of editing, termed imitative editing, to help users exercise their creativity more conveniently. Concretely, to edit an image region of interest, users are free to directly draw inspiration from some in-the-wild references (e.g., some relative pictures come across online), without having to cope with the fit between the reference and the source. Such a design requires the system to automatically figure out what to expect from the reference to perform the editing. For this purpose, we propose a generative training framework, dubbed MimicBrush, which randomly selects two frames from a video clip, masks some regions of one frame, and learns to recover the masked regions using the information from the other frame. That way, our model, developed from a diffusion prior, is able to capture the semantic correspondence between separate images in a self-supervised manner. We experimentally show the effectiveness of our method under various test cases as well as its superiority over existing alternatives. We also construct a benchmark to facilitate further research.","sentences":["Image editing serves as a practical yet challenging task considering the diverse demands from users, where one of the hardest parts is to precisely describe how the edited image should look like.","In this work, we present a new form of editing, termed imitative editing, to help users exercise their creativity more conveniently.","Concretely, to edit an image region of interest, users are free to directly draw inspiration from some in-the-wild references (e.g., some relative pictures come across online), without having to cope with the fit between the reference and the source.","Such a design requires the system to automatically figure out what to expect from the reference to perform the editing.","For this purpose, we propose a generative training framework, dubbed MimicBrush, which randomly selects two frames from a video clip, masks some regions of one frame, and learns to recover the masked regions using the information from the other frame.","That way, our model, developed from a diffusion prior, is able to capture the semantic correspondence between separate images in a self-supervised manner.","We experimentally show the effectiveness of our method under various test cases as well as its superiority over existing alternatives.","We also construct a benchmark to facilitate further research."],"url":"http://arxiv.org/abs/2406.07547v1","category":"cs.CV"}
{"created":"2024-06-11 17:59:48","title":"Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?","abstract":"We present a novel task and benchmark for evaluating the ability of text-to-image(T2I) generation models to produce images that fit commonsense in real life, which we call Commonsense-T2I. Given two adversarial text prompts containing an identical set of action words with minor differences, such as \"a lightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate whether T2I models can conduct visual-commonsense reasoning, e.g. produce images that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\" correspondingly. Commonsense-T2I presents an adversarial challenge, providing pairwise text prompts along with expected outputs. The dataset is carefully hand-curated by experts and annotated with fine-grained labels, such as commonsense type and likelihood of the expected outputs, to assist analyzing model behavior. We benchmark a variety of state-of-the-art (sota) T2I models and surprisingly find that, there is still a large gap between image synthesis and real life photos--even the DALL-E 3 model could only achieve 48.92% on Commonsense-T2I, and the stable diffusion XL model only achieves 24.92% accuracy. Our experiments show that GPT-enriched prompts cannot solve this challenge, and we include a detailed analysis about possible reasons for such deficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation benchmark for T2I commonsense checking, fostering advancements in real life image generation.","sentences":["We present a novel task and benchmark for evaluating the ability of text-to-image(T2I) generation models to produce images that fit commonsense in real life, which we call Commonsense-T2I. Given two adversarial text prompts containing an identical set of action words with minor differences, such as \"a lightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate whether T2I models can conduct visual-commonsense reasoning, e.g. produce images that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\" correspondingly.","Commonsense-T2I presents an adversarial challenge, providing pairwise text prompts along with expected outputs.","The dataset is carefully hand-curated by experts and annotated with fine-grained labels, such as commonsense type and likelihood of the expected outputs, to assist analyzing model behavior.","We benchmark a variety of state-of-the-art (sota) T2I models and surprisingly find that, there is still a large gap between image synthesis and real life photos--even the DALL-E 3 model could only achieve 48.92% on Commonsense-T2I, and the stable diffusion XL model only achieves 24.92% accuracy.","Our experiments show that GPT-enriched prompts cannot solve this challenge, and we include a detailed analysis about possible reasons for such deficiency.","We aim for Commonsense-T2I to serve as a high-quality evaluation benchmark for T2I commonsense checking, fostering advancements in real life image generation."],"url":"http://arxiv.org/abs/2406.07546v1","category":"cs.CV"}
{"created":"2024-06-11 17:59:47","title":"Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena","abstract":"Multiple-choice questions (MCQ) are frequently used to assess large language models (LLMs). Typically, an LLM is given a question and selects the answer deemed most probable after adjustments for factors like length. Unfortunately, LLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to inherent biases of priori unbalanced probabilities, influencing the prediction of answers based on these IDs. Previous research has introduced methods to reduce this ''selection bias'' by simply permutating options on a few test samples and applying to new ones. Another problem of MCQ is the lottery ticket choice by ''random guessing''. The LLM does not learn particular knowledge, but the option is guessed correctly. This situation is especially serious for those small-scale LLMs. To address them, a more thorough approach involves shifting from MCQ to open-style questions, which can fundamentally eliminate selection bias and random guessing issues. However, transitioning causes its own set of challenges in (1) identifying suitable open-style questions and (2) validating the correctness of LLM open-style responses against human-annotated ground-truths. This work aims to tackle these significant difficulties, and establish a new LLM evaluation benchmark through entirely open-style questions. Consequently, we introduce the Open-LLM-Leaderboard to track various LLMs' performance and reflect true capability of them, such as GPT-4o/4/3.5, Claude 3, Gemini, etc. Our code and dataset are available at https://github.com/VILA-Lab/Open-LLM-Leaderboard.","sentences":["Multiple-choice questions (MCQ) are frequently used to assess large language models (LLMs).","Typically, an LLM is given a question and selects the answer deemed most probable after adjustments for factors like length.","Unfortunately, LLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to inherent biases of priori unbalanced probabilities, influencing the prediction of answers based on these IDs.","Previous research has introduced methods to reduce this ''selection bias'' by simply permutating options on a few test samples and applying to new ones.","Another problem of MCQ is the lottery ticket choice by ''random guessing''.","The LLM does not learn particular knowledge, but the option is guessed correctly.","This situation is especially serious for those small-scale LLMs.","To address them, a more thorough approach involves shifting from MCQ to open-style questions, which can fundamentally eliminate selection bias and random guessing issues.","However, transitioning causes its own set of challenges in (1) identifying suitable open-style questions and (2) validating the correctness of LLM open-style responses against human-annotated ground-truths.","This work aims to tackle these significant difficulties, and establish a new LLM evaluation benchmark through entirely open-style questions.","Consequently, we introduce the Open-LLM-Leaderboard to track various LLMs' performance and reflect true capability of them, such as GPT-4o/4/3.5, Claude 3, Gemini, etc.","Our code and dataset are available at https://github.com/VILA-Lab/Open-LLM-Leaderboard."],"url":"http://arxiv.org/abs/2406.07545v1","category":"cs.CL"}
{"created":"2024-06-11 17:59:45","title":"Situational Awareness Matters in 3D Vision Language Reasoning","abstract":"Being able to carry out complicated vision language reasoning tasks in 3D space represents a significant milestone in developing household robots and human-centered embodied AI. In this work, we demonstrate that a critical and distinct challenge in 3D vision language reasoning is situational awareness, which incorporates two key components: (1) The autonomous agent grounds its self-location based on a language prompt. (2) The agent answers open-ended questions from the perspective of its calculated position. To address this challenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D vision language reasoning. We tokenize the 3D scene into sparse voxel representation and propose a language-grounded situation estimator, followed by a situated question answering module. Experiments on the SQA3D and ScanQA datasets show that SIG3D outperforms state-of-the-art models in situation estimation and question answering by a large margin (e.g., an enhancement of over 30% on situation estimation accuracy). Subsequent analysis corroborates our architectural design choices, explores the distinct functions of visual and textual tokens, and highlights the importance of situational awareness in the domain of 3D question answering.","sentences":["Being able to carry out complicated vision language reasoning tasks in 3D space represents a significant milestone in developing household robots and human-centered embodied AI.","In this work, we demonstrate that a critical and distinct challenge in 3D vision language reasoning is situational awareness, which incorporates two key components: (1) The autonomous agent grounds its self-location based on a language prompt.","(2) The agent answers open-ended questions from the perspective of its calculated position.","To address this challenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D vision language reasoning.","We tokenize the 3D scene into sparse voxel representation and propose a language-grounded situation estimator, followed by a situated question answering module.","Experiments on the SQA3D and ScanQA datasets show that SIG3D outperforms state-of-the-art models in situation estimation and question answering by a large margin (e.g., an enhancement of over 30% on situation estimation accuracy).","Subsequent analysis corroborates our architectural design choices, explores the distinct functions of visual and textual tokens, and highlights the importance of situational awareness in the domain of 3D question answering."],"url":"http://arxiv.org/abs/2406.07544v1","category":"cs.CV"}
{"created":"2024-06-11 17:59:35","title":"Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning","abstract":"Recently, vision model pre-training has evolved from relying on manually annotated datasets to leveraging large-scale, web-crawled image-text data. Despite these advances, there is no pre-training method that effectively exploits the interleaved image-text data, which is very prevalent on the Internet. Inspired by the recent success of compression learning in natural language processing, we propose a novel vision model pre-training method called Latent Compression Learning (LCL) for interleaved image-text data. This method performs latent compression learning by maximizing the mutual information between the inputs and outputs of a causal attention model. The training objective can be decomposed into two basic tasks: 1) contrastive learning between visual representation and preceding context, and 2) generating subsequent text based on visual representation. Our experiments demonstrate that our method not only matches the performance of CLIP on paired pre-training datasets (e.g., LAION), but can also leverage interleaved pre-training data (e.g., MMC4) to learn robust visual representation from scratch, showcasing the potential of vision model pre-training with interleaved image-text data. Code is released at https://github.com/OpenGVLab/LCL.","sentences":["Recently, vision model pre-training has evolved from relying on manually annotated datasets to leveraging large-scale, web-crawled image-text data.","Despite these advances, there is no pre-training method that effectively exploits the interleaved image-text data, which is very prevalent on the Internet.","Inspired by the recent success of compression learning in natural language processing, we propose a novel vision model pre-training method called Latent Compression Learning (LCL) for interleaved image-text data.","This method performs latent compression learning by maximizing the mutual information between the inputs and outputs of a causal attention model.","The training objective can be decomposed into two basic tasks: 1) contrastive learning between visual representation and preceding context, and 2) generating subsequent text based on visual representation.","Our experiments demonstrate that our method not only matches the performance of CLIP on paired pre-training datasets (e.g., LAION), but can also leverage interleaved pre-training data (e.g., MMC4) to learn robust visual representation from scratch, showcasing the potential of vision model pre-training with interleaved image-text data.","Code is released at https://github.com/OpenGVLab/LCL."],"url":"http://arxiv.org/abs/2406.07543v1","category":"cs.CV"}
{"created":"2024-06-11 17:59:31","title":"Cognitive Insights Across Languages: Enhancing Multimodal Interview Analysis","abstract":"Cognitive decline is a natural process that occurs as individuals age. Early diagnosis of anomalous decline is crucial for initiating professional treatment that can enhance the quality of life of those affected. To address this issue, we propose a multimodal model capable of predicting Mild Cognitive Impairment and cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation, which comprises audio recordings of clinical interviews. The proposed model demonstrates the ability to transcribe and differentiate between languages used in the interviews. Subsequently, the model extracts audio and text features, combining them into a multimodal architecture to achieve robust and generalized results. Our approach involves in-depth research to implement various features obtained from the proposed modalities.","sentences":["Cognitive decline is a natural process that occurs as individuals age.","Early diagnosis of anomalous decline is crucial for initiating professional treatment that can enhance the quality of life of those affected.","To address this issue, we propose a multimodal model capable of predicting Mild Cognitive Impairment and cognitive scores.","The TAUKADIAL dataset is used to conduct the evaluation, which comprises audio recordings of clinical interviews.","The proposed model demonstrates the ability to transcribe and differentiate between languages used in the interviews.","Subsequently, the model extracts audio and text features, combining them into a multimodal architecture to achieve robust and generalized results.","Our approach involves in-depth research to implement various features obtained from the proposed modalities."],"url":"http://arxiv.org/abs/2406.07542v1","category":"cs.LG"}
{"created":"2024-06-11 17:59:29","title":"CDSA: Conservative Denoising Score-based Algorithm for Offline Reinforcement Learning","abstract":"Distribution shift is a major obstacle in offline reinforcement learning, which necessitates minimizing the discrepancy between the learned policy and the behavior policy to avoid overestimating rare or unseen actions. Previous conservative offline RL algorithms struggle to generalize to unseen actions, despite their success in learning good in-distribution policy. In contrast, we propose to use the gradient fields of the dataset density generated from a pre-trained offline RL algorithm to adjust the original actions. We decouple the conservatism constraints from the policy, thus can benefit wide offline RL algorithms. As a consequence, we propose the Conservative Denoising Score-based Algorithm (CDSA) which utilizes the denoising score-based model to model the gradient of the dataset density, rather than the dataset density itself, and facilitates a more accurate and efficient method to adjust the action generated by the pre-trained policy in a deterministic and continuous MDP environment. In experiments, we show that our approach significantly improves the performance of baseline algorithms in D4RL datasets, and demonstrate the generalizability and plug-and-play capability of our model across different pre-trained offline RL policy in different tasks. We also validate that the agent exhibits greater risk aversion after employing our method while showcasing its ability to generalize effectively across diverse tasks.","sentences":["Distribution shift is a major obstacle in offline reinforcement learning, which necessitates minimizing the discrepancy between the learned policy and the behavior policy to avoid overestimating rare or unseen actions.","Previous conservative offline RL algorithms struggle to generalize to unseen actions, despite their success in learning good in-distribution policy.","In contrast, we propose to use the gradient fields of the dataset density generated from a pre-trained offline RL algorithm to adjust the original actions.","We decouple the conservatism constraints from the policy, thus can benefit wide offline RL algorithms.","As a consequence, we propose the Conservative Denoising Score-based Algorithm (CDSA) which utilizes the denoising score-based model to model the gradient of the dataset density, rather than the dataset density itself, and facilitates a more accurate and efficient method to adjust the action generated by the pre-trained policy in a deterministic and continuous MDP environment.","In experiments, we show that our approach significantly improves the performance of baseline algorithms in D4RL datasets, and demonstrate the generalizability and plug-and-play capability of our model across different pre-trained offline RL policy in different tasks.","We also validate that the agent exhibits greater risk aversion after employing our method while showcasing its ability to generalize effectively across diverse tasks."],"url":"http://arxiv.org/abs/2406.07541v1","category":"cs.LG"}
{"created":"2024-06-11 17:59:01","title":"Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance","abstract":"Recent controllable generation approaches such as FreeControl and Diffusion Self-guidance bring fine-grained spatial and appearance control to text-to-image (T2I) diffusion models without training auxiliary modules. However, these methods optimize the latent embedding for each type of score function with longer diffusion steps, making the generation process time-consuming and limiting their flexibility and use. This work presents Ctrl-X, a simple framework for T2I diffusion controlling structure and appearance without additional training or guidance. Ctrl-X designs feed-forward structure control to enable the structure alignment with a structure image and semantic-aware appearance transfer to facilitate the appearance transfer from a user-input image. Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints. In particular, Ctrl-X supports novel structure and appearance control with arbitrary condition images of any modality, exhibits superior image quality and appearance transfer compared to existing works, and provides instant plug-and-play functionality to any T2I and text-to-video (T2V) diffusion model. See our project page for an overview of the results: https://genforce.github.io/ctrl-x","sentences":["Recent controllable generation approaches such as FreeControl and Diffusion Self-guidance bring fine-grained spatial and appearance control to text-to-image (T2I) diffusion models without training auxiliary modules.","However, these methods optimize the latent embedding for each type of score function with longer diffusion steps, making the generation process time-consuming and limiting their flexibility and use.","This work presents Ctrl-X, a simple framework for T2I diffusion controlling structure and appearance without additional training or guidance.","Ctrl-X designs feed-forward structure control to enable the structure alignment with a structure image and semantic-aware appearance transfer to facilitate the appearance transfer from a user-input image.","Extensive qualitative and quantitative experiments illustrate the superior performance of Ctrl-X on various condition inputs and model checkpoints.","In particular, Ctrl-X supports novel structure and appearance control with arbitrary condition images of any modality, exhibits superior image quality and appearance transfer compared to existing works, and provides instant plug-and-play functionality to any T2I and text-to-video (T2V) diffusion model.","See our project page for an overview of the results: https://genforce.github.io/ctrl-x"],"url":"http://arxiv.org/abs/2406.07540v1","category":"cs.CV"}
{"created":"2024-06-11 17:56:50","title":"On the potential of probing the neutron star composition in accreting X-ray binaries","abstract":"Transiently accreting Low Mass X-Ray Binaries have the potential to probe the core composition of their neutron stars via deep crustal heating caused by nuclear reactions. We statistically assess this deep crustal heating scenario, taking into account the various microphysical and astrophysical uncertainties. We find that despite the sizable uncertainties there is the chance to discriminate different compositional scenarios. Several observed sources statistically challenge a minimal hadronic matter composition, where cooling proceeds exclusively via slow modified Urca reactions. Considering here two exemplary extended uniform compositions, namely ultra-dense hadronic matter with direct Urca emission and ungapped quark matter, we find that they are even within uncertainties distinguishable. We show that although exotic forms of matter are generally only expected in an inner core, which could in principle have any size, sufficiently large astrophysical data sets nonetheless have the potential to statistically discriminate compositional scenarios, in particular when further mass measurements become available.","sentences":["Transiently accreting Low Mass X-Ray Binaries have the potential to probe the core composition of their neutron stars via deep crustal heating caused by nuclear reactions.","We statistically assess this deep crustal heating scenario, taking into account the various microphysical and astrophysical uncertainties.","We find that despite the sizable uncertainties there is the chance to discriminate different compositional scenarios.","Several observed sources statistically challenge a minimal hadronic matter composition, where cooling proceeds exclusively via slow modified Urca reactions.","Considering here two exemplary extended uniform compositions, namely ultra-dense hadronic matter with direct Urca emission and ungapped quark matter, we find that they are even within uncertainties distinguishable.","We show that although exotic forms of matter are generally only expected in an inner core, which could in principle have any size, sufficiently large astrophysical data sets nonetheless have the potential to statistically discriminate compositional scenarios, in particular when further mass measurements become available."],"url":"http://arxiv.org/abs/2406.07534v1","category":"astro-ph.HE"}
{"created":"2024-06-11 17:56:02","title":"Interacting-bath dynamical embedding for capturing non-local electron correlation in solids","abstract":"Quantitative simulation of electronic structure of solids requires the treatment of local and non-local electron correlations on an equal footing. Dynamical mean-field theory, a widely-used quantum embedding algorithm that assumes local self-energy approximation, is challenging to extend to capture long-range electron correlation. In this work, we present a new formulation of Green's function embedding that, instead of embedding the impurity in non-interacting bath through the hybridization function, derives bath representation with general two-particle interactions in a fully ab initio and systematically improvable manner. The resulting interacting-bath dynamical embedding theory (ibDET) simulates local and non-local self-energies using high-level quantum chemistry solvers, such as the coupled-cluster theory. We demonstrate that ibDET combined with the GW theory (GW+ibDET) achieves good agreements with experimental band structure and photoemission spectra while preserving translational symmetry across a range of semiconducting, insulating, and metallic materials. Furthermore, our approach allows quantifying the role of non-local electron correlation in determining spectral properties of materials and addressing the long-standing debate over the bandwidth narrowing of metallic sodium.","sentences":["Quantitative simulation of electronic structure of solids requires the treatment of local and non-local electron correlations on an equal footing.","Dynamical mean-field theory, a widely-used quantum embedding algorithm that assumes local self-energy approximation, is challenging to extend to capture long-range electron correlation.","In this work, we present a new formulation of Green's function embedding that, instead of embedding the impurity in non-interacting bath through the hybridization function, derives bath representation with general two-particle interactions in a fully ab initio and systematically improvable manner.","The resulting interacting-bath dynamical embedding theory (ibDET) simulates local and non-local self-energies using high-level quantum chemistry solvers, such as the coupled-cluster theory.","We demonstrate that ibDET combined with the GW theory (GW+ibDET) achieves good agreements with experimental band structure and photoemission spectra while preserving translational symmetry across a range of semiconducting, insulating, and metallic materials.","Furthermore, our approach allows quantifying the role of non-local electron correlation in determining spectral properties of materials and addressing the long-standing debate over the bandwidth narrowing of metallic sodium."],"url":"http://arxiv.org/abs/2406.07531v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-11 17:55:02","title":"Interpolating between Hausdorff and box dimension","abstract":"Hausdorff and box dimension are two familiar notions of fractal dimension. Box dimension can be larger than Hausdorff dimension, because in the definition of box dimension, all sets in the cover have the same diameter, but for Hausdorff dimension there is no such restriction. This thesis focuses on a family of dimensions parameterised by $\\theta \\in (0,1)$, called the intermediate dimensions, which are defined by requiring that $\\mbox{diam}(U) \\leq (\\mbox{diam}(V))^{\\theta}$ for all sets $U,V$ in the cover.   We begin by generalising the intermediate dimensions to allow for greater refinement in how the relative sizes of the covering sets are restricted. These new dimensions can recover the interpolation between Hausdorff and box dimension for compact sets whose intermediate dimensions do not tend to the Hausdorff dimension as $\\theta \\to 0$. We also use a Moran set construction to prove a necessary and sufficient condition, in terms of Dini derivatives, for a given function to be realised as the intermediate dimensions of a set.   We proceed to prove that the intermediate dimensions of limit sets of infinite conformal iterated function systems are given by the maximum of the Hausdorff dimension of the limit set and the intermediate dimensions of the set of fixed points of the contractions. This applies to sets defined using continued fraction expansions, and has applications to dimensions of projections, fractional Brownian images, and general H\\\"older images.   Finally, we determine a formula for the intermediate dimensions of all self-affine Bedford-McMullen carpets. The functions display features not witnessed in previous examples, such as having countably many phase transitions. We deduce that two carpets have equal intermediate dimensions if and only if the multifractal spectra of the corresponding uniform Bernoulli measures coincide.","sentences":["Hausdorff and box dimension are two familiar notions of fractal dimension.","Box dimension can be larger than Hausdorff dimension, because in the definition of box dimension, all sets in the cover have the same diameter, but for Hausdorff dimension there is no such restriction.","This thesis focuses on a family of dimensions parameterised by $\\theta \\in (0,1)$, called the intermediate dimensions, which are defined by requiring that $\\mbox{diam}(U) \\leq (\\mbox{diam}(V))^{\\theta}$ for all sets $U,V$ in the cover.   ","We begin by generalising the intermediate dimensions to allow for greater refinement in how the relative sizes of the covering sets are restricted.","These new dimensions can recover the interpolation between Hausdorff and box dimension for compact sets whose intermediate dimensions do not tend to the Hausdorff dimension as $\\theta \\to 0$.","We also use a Moran set construction to prove a necessary and sufficient condition, in terms of Dini derivatives, for a given function to be realised as the intermediate dimensions of a set.   ","We proceed to prove that the intermediate dimensions of limit sets of infinite conformal iterated function systems are given by the maximum of the Hausdorff dimension of the limit set and the intermediate dimensions of the set of fixed points of the contractions.","This applies to sets defined using continued fraction expansions, and has applications to dimensions of projections, fractional Brownian images, and general H\\\"older images.   ","Finally, we determine a formula for the intermediate dimensions of all self-affine Bedford-McMullen carpets.","The functions display features not witnessed in previous examples, such as having countably many phase transitions.","We deduce that two carpets have equal intermediate dimensions if and only if the multifractal spectra of the corresponding uniform Bernoulli measures coincide."],"url":"http://arxiv.org/abs/2406.07527v1","category":"math.MG"}
{"created":"2024-06-11 17:53:58","title":"Cosmological constraints on $\u039b_{\\rm s}$CDM scenario in a type II minimally modified gravity","abstract":"The idea of a rapid sign-switching cosmological constant (mirror AdS-dS transition) in the late universe at $z\\sim1.7$, known as the $\\Lambda_{\\rm s}$CDM model, has significantly improved the fit to observational data and provides a promising scenario for alleviating major cosmological tensions, such as the $H_0$ and $S_8$ tensions. However, in the absence of a fully predictive model, implementing this fit required conjecturing that the dynamics of the linear perturbations are governed by general relativity. Recent work embedding the $\\Lambda_{\\rm s}$CDM model with the Lagrangian of a type-II minimally modified gravity known as VCDM has propelled $\\Lambda_{\\rm s}$CDM to a fully predictive model, removing the uncertainty related to the aforementioned assumption; we call this new model $\\Lambda_{\\rm s}$VCDM. In this work, we demonstrate that not only does $\\Lambda_{\\rm s}$CDM fit the data better than the standard $\\Lambda$CDM model, but the new model, $\\Lambda_{\\rm s}$VCDM, performs even better in alleviating cosmological tensions while also providing a better fit to the data, including CMB, BAO, SNe Ia, and Cosmic Shear measurements. Our findings highlight the $\\Lambda_{\\rm s}$CDM framework, particularly the $\\Lambda_{\\rm s}$VCDM model, as a compelling alternative to the standard $\\Lambda$CDM model, especially by successfully alleviating the $H_0$ tension. Additionally, these models predict higher values for $\\sigma_8$, indicating enhanced structuring, albeit with lower present-day matter density parameter values and consequently reduced $S_8$ values, alleviating the $S_8$ tension as well. This demonstrates that the data are well fit by a combination of background and linear perturbations, both having dynamics differing from those of $\\Lambda$CDM. This paves the way for further exploration of new ways for embedding the sign-switching cosmological constant into other models.","sentences":["The idea of a rapid sign-switching cosmological constant (mirror AdS-dS transition) in the late universe at $z\\sim1.7$, known as the $\\Lambda_{\\rm s}$CDM model, has significantly improved the fit to observational data and provides a promising scenario for alleviating major cosmological tensions, such as the $H_0$ and $S_8$ tensions.","However, in the absence of a fully predictive model, implementing this fit required conjecturing that the dynamics of the linear perturbations are governed by general relativity.","Recent work embedding the $\\Lambda_{\\rm s}$CDM model with the Lagrangian of a type-II minimally modified gravity known as VCDM has propelled $\\Lambda_{\\rm s}$CDM to a fully predictive model, removing the uncertainty related to the aforementioned assumption; we call this new model $\\Lambda_{\\rm s}$VCDM.","In this work, we demonstrate that not only does $\\Lambda_{\\rm s}$CDM fit the data better than the standard $\\Lambda$CDM model, but the new model, $\\Lambda_{\\rm s}$VCDM, performs even better in alleviating cosmological tensions while also providing a better fit to the data, including CMB, BAO, SNe Ia, and Cosmic Shear measurements.","Our findings highlight the $\\Lambda_{\\rm s}$CDM framework, particularly the $\\Lambda_{\\rm s}$VCDM model, as a compelling alternative to the standard $\\Lambda$CDM model, especially by successfully alleviating the $H_0$ tension.","Additionally, these models predict higher values for $\\sigma_8$, indicating enhanced structuring, albeit with lower present-day matter density parameter values and consequently reduced $S_8$ values, alleviating the $S_8$ tension as well.","This demonstrates that the data are well fit by a combination of background and linear perturbations, both having dynamics differing from those of $\\Lambda$CDM.","This paves the way for further exploration of new ways for embedding the sign-switching cosmological constant into other models."],"url":"http://arxiv.org/abs/2406.07526v1","category":"astro-ph.CO"}
{"created":"2024-06-11 17:53:25","title":"Will Southeast Asia be the next global manufacturing hub? A multiway cointegration, causality, and dynamic connectedness analyses on factors influencing offshore decisions","abstract":"The COVID-19 pandemic has compelled multinational corporations to diversify their global supply chain risk and to relocate their factories to Southeast Asian countries beyond China. Such recent phenomena provide a good opportunity to understand the factors that influenced offshore decisions in the last two decades. We propose a new conceptual framework based on econometric approaches to examine the relationships between these factors. Firstly, the Vector Auto Regression (VAR) for multi-way cointegration analysis by a Johansen test as well as the embedding Granger causality analysis to examine offshore decisions--innovation, technology readiness, infrastructure, foreign direct investment (FDI), and intermediate imports. Secondly, a Quantile Vector Autoregressive (QVAR) model is used to assess the dynamic connectedness among Southeast Asian countries based on the offshore factors. This study explores a system-wide experiment to evaluate the spillover effects of offshore decisions. It reports a comprehensive analysis using time-series data collected from the World Bank. The results of the cointegration, causality, and dynamic connectedness analyses show that a subset of Southeast Asian countries have spillover effects on each other. These countries present a multi-way cointegration and dynamic connectedness relationship. The study contributes to policymaking by providing a data-driven innovative approach through a new conceptual framework.","sentences":["The COVID-19 pandemic has compelled multinational corporations to diversify their global supply chain risk and to relocate their factories to Southeast Asian countries beyond China.","Such recent phenomena provide a good opportunity to understand the factors that influenced offshore decisions in the last two decades.","We propose a new conceptual framework based on econometric approaches to examine the relationships between these factors.","Firstly, the Vector Auto Regression (VAR) for multi-way cointegration analysis by a Johansen test as well as the embedding Granger causality analysis to examine offshore decisions--innovation, technology readiness, infrastructure, foreign direct investment (FDI), and intermediate imports.","Secondly, a Quantile Vector Autoregressive (QVAR) model is used to assess the dynamic connectedness among Southeast Asian countries based on the offshore factors.","This study explores a system-wide experiment to evaluate the spillover effects of offshore decisions.","It reports a comprehensive analysis using time-series data collected from the World Bank.","The results of the cointegration, causality, and dynamic connectedness analyses show that a subset of Southeast Asian countries have spillover effects on each other.","These countries present a multi-way cointegration and dynamic connectedness relationship.","The study contributes to policymaking by providing a data-driven innovative approach through a new conceptual framework."],"url":"http://arxiv.org/abs/2406.07525v1","category":"econ.GN"}
{"created":"2024-06-11 17:51:40","title":"Simple and Effective Masked Diffusion Language Models","abstract":"While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling. In this work, we show that simple masked discrete diffusion is more performant than previously thought. We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. Our objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. We release our code at: https://github.com/kuleshov-group/mdlm","sentences":["While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling.","In this work, we show that simple masked discrete diffusion is more performant than previously thought.","We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements.","Our objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model.","On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity.","We release our code at: https://github.com/kuleshov-group/mdlm"],"url":"http://arxiv.org/abs/2406.07524v1","category":"cs.CL"}
{"created":"2024-06-11 17:50:51","title":"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling","abstract":"Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.","sentences":["Efficiently modeling sequences with infinite context length has been a long-standing problem.","Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization.","In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA).","Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism.","We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks.","When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length.","As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming.","A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba."],"url":"http://arxiv.org/abs/2406.07522v1","category":"cs.CL"}
{"created":"2024-06-11 17:50:15","title":"Neural Gaffer: Relighting Any Object via Diffusion","abstract":"Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting. Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight. Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive. In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition. Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model. We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy. Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion. Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field.","sentences":["Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting.","Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight.","Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive.","In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition.","Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model.","We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy.","Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion.","Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field."],"url":"http://arxiv.org/abs/2406.07520v1","category":"cs.CV"}
{"created":"2024-06-11 17:50:04","title":"Physics-guided weak-form discovery of reduced-order models for trapped ultracold hydrodynamics","abstract":"We study the relaxation of a highly collisional, ultracold but nondegenerate gas of polar molecules. Confined within a harmonic trap, the gas is subject to fluid-gaseous coupled dynamics that lead to a breakdown of first-order hydrodynamics. An attempt to treat these higher-order hydrodynamic effects was previously made with a Gaussian ansatz and coarse-graining model parameter [R. R. W. Wang & J. L. Bohn, Phys. Rev. A 108, 013322 (2023)], leading to an approximate set of equations for a few collective observables accessible to experiments. Here we present substantially improved reduced-order models for these same observables, admissible beyond previous parameter regimes, discovered directly from particle simulations using the WSINDy algorithm (Weak-form Sparse Identification of Nonlinear Dynamics). The interpretable nature of the learning algorithm enables estimation of previously unknown physical quantities and discovery of model terms with candidate physical mechanisms, revealing new physics in mixed collisional regimes. Our approach constitutes a general framework for data-driven model identification leveraging known physics.","sentences":["We study the relaxation of a highly collisional, ultracold but nondegenerate gas of polar molecules.","Confined within a harmonic trap, the gas is subject to fluid-gaseous coupled dynamics that lead to a breakdown of first-order hydrodynamics.","An attempt to treat these higher-order hydrodynamic effects was previously made with a Gaussian ansatz and coarse-graining model parameter [R. R. W. Wang & J. L. Bohn, Phys.","Rev. A 108, 013322 (2023)], leading to an approximate set of equations for a few collective observables accessible to experiments.","Here we present substantially improved reduced-order models for these same observables, admissible beyond previous parameter regimes, discovered directly from particle simulations using the WSINDy algorithm (Weak-form Sparse Identification of Nonlinear Dynamics).","The interpretable nature of the learning algorithm enables estimation of previously unknown physical quantities and discovery of model terms with candidate physical mechanisms, revealing new physics in mixed collisional regimes.","Our approach constitutes a general framework for data-driven model identification leveraging known physics."],"url":"http://arxiv.org/abs/2406.07519v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-11 17:49:35","title":"On constant mean curvature 1-immersions of surfaces into hyperbolic 3-manifolds","abstract":"Motivated by the work of Bryant on constant mean curvature (CMC) $1$-immersions of surfaces into the hyperbolic space H^3 and after the results of Tarantello (2023), we pursue a possible parametrization for the moduli space of (CMC) 1-immersions of a surface S (closed, orientable and of genus >1) into hyperbolic 3-manifolds. Those immersions enter as \"critical\" object in our analysis. In fact, they can be attained only as limits of the (CMC) c-immersions (as c tends to 1), obtained in Huang-Lucia-Tarantello (2022), for |c|<1. However, such passage to the limit can be prevented by possible blow-up phenomena, so that the pullback metrics of the (CMC) c-immersions may yield (at the limit) to a singular metric with conical singularities at finitely many points (the blow-up points). In case of genus g=2, blow up can occur only at a single point, and in Tarantello (2023) it was shown how it could be prevented and the passage to the limit ensured in terms of the Kodaira map. In this note we sharpen this result and for genus g=2, we obtaina condition (we believe sharp) which involves only the Kodaira map on the six Weierstrass points. In addition we tackle the case of higher genus, where multiple blow-up points occur. In this case, we need to identify a suitable replacement of the Kodaira map, now defined on the space of non-zero effective divisors. More importantly, we need to improve in a substantial way the asymptotic analysis of Tarantello (2023) limited to the case of \"blow-up\" with minimal mass. In this direction we give a contribution which best applies to the case of genus g=3, but also provides a relevant step and a convincing indication on what should happen in the general case.","sentences":["Motivated by the work of Bryant on constant mean curvature (CMC) $1$-immersions of surfaces into the hyperbolic space H^3 and after the results of Tarantello (2023), we pursue a possible parametrization for the moduli space of (CMC) 1-immersions of a surface S (closed, orientable and of genus >1) into hyperbolic 3-manifolds.","Those immersions enter as \"critical\" object in our analysis.","In fact, they can be attained only as limits of the (CMC) c-immersions (as c tends to 1), obtained in Huang-Lucia-Tarantello (2022), for |c|<1.","However, such passage to the limit can be prevented by possible blow-up phenomena, so that the pullback metrics of the (CMC) c-immersions may yield (at the limit) to a singular metric with conical singularities at finitely many points (the blow-up points).","In case of genus g=2, blow up can occur only at a single point, and in Tarantello (2023) it was shown how it could be prevented and the passage to the limit ensured in terms of the Kodaira map.","In this note we sharpen this result and for genus g=2, we obtaina condition (we believe sharp) which involves only the Kodaira map on the six Weierstrass points.","In addition we tackle the case of higher genus, where multiple blow-up points occur.","In this case, we need to identify a suitable replacement of the Kodaira map, now defined on the space of non-zero effective divisors.","More importantly, we need to improve in a substantial way the asymptotic analysis of Tarantello (2023) limited to the case of \"blow-up\" with minimal mass.","In this direction we give a contribution which best applies to the case of genus g=3, but also provides a relevant step and a convincing indication on what should happen in the general case."],"url":"http://arxiv.org/abs/2406.07518v1","category":"math.DG"}
{"created":"2024-06-11 17:48:50","title":"The canonical trace of Cohen-Macaulay algebras of codimension 2","abstract":"In the present paper, we investigate a conjecture of J\\\"urgen Herzog. Let $S$ be a local regular ring with residue field $K$ or a positively graded $K$-algebra, $I\\subset S$ be a perfect ideal of grade two, and let $R=S/I$ with canonical module $\\omega_R$. Herzog conjectured that the canonical trace $\\text{tr}(\\omega_R)$ is obtained by specialization from the generic case of maximal minors. We prove this conjecture in several cases, and present a criterion that guarantees that the canonical trace specializes under some additional assumptions. As the final conclusion of all of our results, we classify the nearly Gorenstein monomial ideals of height two.","sentences":["In the present paper, we investigate a conjecture of J\\\"urgen Herzog.","Let $S$ be a local regular ring with residue field $K$ or a positively graded $K$-algebra, $I\\subset S$ be a perfect ideal of grade two, and let $R=S/I$ with canonical module $\\omega_R$. Herzog conjectured that the canonical trace $\\text{tr}(\\omega_R)$ is obtained by specialization from the generic case of maximal minors.","We prove this conjecture in several cases, and present a criterion that guarantees that the canonical trace specializes under some additional assumptions.","As the final conclusion of all of our results, we classify the nearly Gorenstein monomial ideals of height two."],"url":"http://arxiv.org/abs/2406.07517v1","category":"math.AC"}
{"created":"2024-06-11 17:47:27","title":"Instant 3D Human Avatar Generation using Image Diffusion Models","abstract":"We present AvatarPopUp, a method for fast, high quality 3D human avatar generation from different input modalities, such as images and text prompts and with control over the generated pose and shape. The common theme is the use of diffusion-based image generation networks that are specialized for each particular task, followed by a 3D lifting network. We purposefully decouple the generation from the 3D modeling which allow us to leverage powerful image synthesis priors, trained on billions of text-image pairs. We fine-tune latent diffusion networks with additional image conditioning to solve tasks such as image generation and back-view prediction, and to support qualitatively different multiple 3D hypotheses. Our partial fine-tuning approach allows to adapt the networks for each task without inducing catastrophic forgetting. In our experiments, we demonstrate that our method produces accurate, high-quality 3D avatars with diverse appearance that respect the multimodal text, image, and body control signals. Our approach can produce a 3D model in as few as 2 seconds, a four orders of magnitude speedup w.r.t. the vast majority of existing methods, most of which solve only a subset of our tasks, and with fewer controls, thus enabling applications that require the controlled 3D generation of human avatars at scale. The project website can be found at https://www.nikoskolot.com/avatarpopup/.","sentences":["We present AvatarPopUp, a method for fast, high quality 3D human avatar generation from different input modalities, such as images and text prompts and with control over the generated pose and shape.","The common theme is the use of diffusion-based image generation networks that are specialized for each particular task, followed by a 3D lifting network.","We purposefully decouple the generation from the 3D modeling which allow us to leverage powerful image synthesis priors, trained on billions of text-image pairs.","We fine-tune latent diffusion networks with additional image conditioning to solve tasks such as image generation and back-view prediction, and to support qualitatively different multiple 3D hypotheses.","Our partial fine-tuning approach allows to adapt the networks for each task without inducing catastrophic forgetting.","In our experiments, we demonstrate that our method produces accurate, high-quality 3D avatars with diverse appearance that respect the multimodal text, image, and body control signals.","Our approach can produce a 3D model in as few as 2 seconds, a four orders of magnitude speedup w.r.t.","the vast majority of existing methods, most of which solve only a subset of our tasks, and with fewer controls, thus enabling applications that require the controlled 3D generation of human avatars at scale.","The project website can be found at https://www.nikoskolot.com/avatarpopup/."],"url":"http://arxiv.org/abs/2406.07516v1","category":"cs.CV"}
{"created":"2024-06-11 17:46:16","title":"Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement","abstract":"Synthesized data from generative models is increasingly considered as an alternative to human-annotated data for fine-tuning Large Language Models. This raises concerns about model collapse: a drop in performance of models fine-tuned on generated data. Considering that it is easier for both humans and machines to tell between good and bad examples than to generate high-quality samples, we investigate the use of feedback on synthesized data to prevent model collapse. We derive theoretical conditions under which a Gaussian mixture classification model can achieve asymptotically optimal performance when trained on feedback-augmented synthesized data, and provide supporting simulations for finite regimes. We illustrate our theoretical predictions on two practical problems: computing matrix eigenvalues with transformers and news summarization with large language models, which both undergo model collapse when trained on model-generated data. We show that training from feedback-augmented synthesized data, either by pruning incorrect predictions or by selecting the best of several guesses, can prevent model collapse, validating popular approaches like RLHF.","sentences":["Synthesized data from generative models is increasingly considered as an alternative to human-annotated data for fine-tuning Large Language Models.","This raises concerns about model collapse: a drop in performance of models fine-tuned on generated data.","Considering that it is easier for both humans and machines to tell between good and bad examples than to generate high-quality samples, we investigate the use of feedback on synthesized data to prevent model collapse.","We derive theoretical conditions under which a Gaussian mixture classification model can achieve asymptotically optimal performance when trained on feedback-augmented synthesized data, and provide supporting simulations for finite regimes.","We illustrate our theoretical predictions on two practical problems: computing matrix eigenvalues with transformers and news summarization with large language models, which both undergo model collapse when trained on model-generated data.","We show that training from feedback-augmented synthesized data, either by pruning incorrect predictions or by selecting the best of several guesses, can prevent model collapse, validating popular approaches like RLHF."],"url":"http://arxiv.org/abs/2406.07515v1","category":"cs.LG"}
{"created":"2024-06-11 17:41:26","title":"Flow Map Matching","abstract":"Generative models based on dynamical transport of measure, such as diffusion models, flow matching models, and stochastic interpolants, learn an ordinary or stochastic differential equation whose trajectories push initial conditions from a known base distribution onto the target. While training is cheap, samples are generated via simulation, which is more expensive than one-step models like GANs. To close this gap, we introduce flow map matching -- an algorithm that learns the two-time flow map of an underlying ordinary differential equation. The approach leads to an efficient few-step generative model whose step count can be chosen a-posteriori to smoothly trade off accuracy for computational expense. Leveraging the stochastic interpolant framework, we introduce losses for both direct training of flow maps and distillation from pre-trained (or otherwise known) velocity fields. Theoretically, we show that our approach unifies many existing few-step generative models, including consistency models, consistency trajectory models, progressive distillation, and neural operator approaches, which can be obtained as particular cases of our formalism. With experiments on CIFAR-10 and ImageNet 32x32, we show that flow map matching leads to high-quality samples with significantly reduced sampling cost compared to diffusion or stochastic interpolant methods.","sentences":["Generative models based on dynamical transport of measure, such as diffusion models, flow matching models, and stochastic interpolants, learn an ordinary or stochastic differential equation whose trajectories push initial conditions from a known base distribution onto the target.","While training is cheap, samples are generated via simulation, which is more expensive than one-step models like GANs.","To close this gap, we introduce flow map matching -- an algorithm that learns the two-time flow map of an underlying ordinary differential equation.","The approach leads to an efficient few-step generative model whose step count can be chosen a-posteriori to smoothly trade off accuracy for computational expense.","Leveraging the stochastic interpolant framework, we introduce losses for both direct training of flow maps and distillation from pre-trained (or otherwise known) velocity fields.","Theoretically, we show that our approach unifies many existing few-step generative models, including consistency models, consistency trajectory models, progressive distillation, and neural operator approaches, which can be obtained as particular cases of our formalism.","With experiments on CIFAR-10 and ImageNet 32x32, we show that flow map matching leads to high-quality samples with significantly reduced sampling cost compared to diffusion or stochastic interpolant methods."],"url":"http://arxiv.org/abs/2406.07507v1","category":"cs.LG"}
{"created":"2024-06-11 17:40:31","title":"Understanding Visual Concepts Across Models","abstract":"Large multimodal models such as Stable Diffusion can generate, detect, and classify new visual concepts after fine-tuning just a single word embedding. Do models learn similar words for the same concepts (i.e. <orange-cat> = orange + cat)? We conduct a large-scale analysis on three state-of-the-art models in text-to-image generation, open-set object detection, and zero-shot classification, and find that new word embeddings are model-specific and non-transferable. Across 4,800 new embeddings trained for 40 diverse visual concepts on four standard datasets, we find perturbations within an $\\epsilon$-ball to any prior embedding that generate, detect, and classify an arbitrary concept. When these new embeddings are spliced into new models, fine-tuning that targets the original model is lost. We show popular soft prompt-tuning approaches find these perturbative solutions when applied to visual concept learning tasks, and embeddings for visual concepts are not transferable. Code for reproducing our work is available at: https://visual-words.github.io.","sentences":["Large multimodal models such as Stable Diffusion can generate, detect, and classify new visual concepts after fine-tuning just a single word embedding.","Do models learn similar words for the same concepts (i.e. <orange-cat> = orange + cat)?","We conduct a large-scale analysis on three state-of-the-art models in text-to-image generation, open-set object detection, and zero-shot classification, and find that new word embeddings are model-specific and non-transferable.","Across 4,800 new embeddings trained for 40 diverse visual concepts on four standard datasets, we find perturbations within an $\\epsilon$-ball to any prior embedding that generate, detect, and classify an arbitrary concept.","When these new embeddings are spliced into new models, fine-tuning that targets the original model is lost.","We show popular soft prompt-tuning approaches find these perturbative solutions when applied to visual concept learning tasks, and embeddings for visual concepts are not transferable.","Code for reproducing our work is available at: https://visual-words.github.io."],"url":"http://arxiv.org/abs/2406.07506v1","category":"cs.CV"}
{"created":"2024-06-11 17:37:45","title":"Image Textualization: An Automatic Framework for Creating Accurate and Detailed Image Descriptions","abstract":"Image description datasets play a crucial role in the advancement of various applications such as image understanding, text-to-image generation, and text-image retrieval. Currently, image description datasets primarily originate from two sources. One source is the scraping of image-text pairs from the web. Despite their abundance, these descriptions are often of low quality and noisy. Another is through human labeling. Datasets such as COCO are generally very short and lack details. Although detailed image descriptions can be annotated by humans, the high annotation cost limits the feasibility. These limitations underscore the need for more efficient and scalable methods to generate accurate and detailed image descriptions. In this paper, we propose an innovative framework termed Image Textualization (IT), which automatically produces high-quality image descriptions by leveraging existing multi-modal large language models (MLLMs) and multiple vision expert models in a collaborative manner, which maximally convert the visual information into text. To address the current lack of benchmarks for detailed descriptions, we propose several benchmarks for comprehensive evaluation, which verifies the quality of image descriptions created by our framework. Furthermore, we show that LLaVA-7B, benefiting from training on IT-curated descriptions, acquire improved capability to generate richer image descriptions, substantially increasing the length and detail of their output with less hallucination.","sentences":["Image description datasets play a crucial role in the advancement of various applications such as image understanding, text-to-image generation, and text-image retrieval.","Currently, image description datasets primarily originate from two sources.","One source is the scraping of image-text pairs from the web.","Despite their abundance, these descriptions are often of low quality and noisy.","Another is through human labeling.","Datasets such as COCO are generally very short and lack details.","Although detailed image descriptions can be annotated by humans, the high annotation cost limits the feasibility.","These limitations underscore the need for more efficient and scalable methods to generate accurate and detailed image descriptions.","In this paper, we propose an innovative framework termed Image Textualization (IT), which automatically produces high-quality image descriptions by leveraging existing multi-modal large language models (MLLMs) and multiple vision expert models in a collaborative manner, which maximally convert the visual information into text.","To address the current lack of benchmarks for detailed descriptions, we propose several benchmarks for comprehensive evaluation, which verifies the quality of image descriptions created by our framework.","Furthermore, we show that LLaVA-7B, benefiting from training on IT-curated descriptions, acquire improved capability to generate richer image descriptions, substantially increasing the length and detail of their output with less hallucination."],"url":"http://arxiv.org/abs/2406.07502v1","category":"cs.CV"}
{"created":"2024-06-11 17:37:44","title":"How big is a tiling's return module?","abstract":"The rank of a tiling's return module depends on the geometry of its tiles and is not a topological invariant. However, the rank of the first \\v Cech cohomology $\\check H^1(\\Omega)$ gives upper and lower bounds for the size of the return module. For all sufficiently large patches, the rank of the return module is at most the same as the rank of the cohomology. For a generic choice of tile shapes and an arbitrary reference patch, the rank of the return module is at least the rank of $\\check H^1(\\Omega)$. Therefore, for generic tile shapes and sufficiently large patches, the rank of the return module is equal to the rank of $\\check H^1(\\Omega)$.","sentences":["The rank of a tiling's return module depends on the geometry of its tiles and is not a topological invariant.","However, the rank of the first \\v","Cech cohomology $\\check H^1(\\Omega)$ gives upper and lower bounds for the size of the return module.","For all sufficiently large patches, the rank of the return module is at most the same as the rank of the cohomology.","For a generic choice of tile shapes and an arbitrary reference patch, the rank of the return module is at least the rank of $\\check H^1(\\Omega)$. Therefore, for generic tile shapes and sufficiently large patches, the rank of the return module is equal to the rank of $\\check H^1(\\Omega)$."],"url":"http://arxiv.org/abs/2406.07501v1","category":"math.DS"}
{"created":"2024-06-11 17:35:39","title":"SPIN: Spacecraft Imagery for Navigation","abstract":"Data acquired in space operational conditions is scarce due to the costs and complexity of space operations. This poses a challenge to learning-based visual-based navigation algorithms employed in autonomous spacecraft navigation. Existing datasets, which largely depend on computer-simulated data, have partially filled this gap. However, the image generation tools they use are proprietary, which limits the evaluation of methods to unseen scenarios. Furthermore, these datasets provide limited ground-truth data, primarily focusing on the spacecraft's translation and rotation relative to the camera. To address these limitations, we present SPIN (SPacecraft Imagery for Navigation), an open-source realistic spacecraft image generation tool for relative navigation between two spacecrafts. SPIN provides a wide variety of ground-truth data and allows researchers to employ custom 3D models of satellites, define specific camera-relative poses, and adjust various settings such as camera parameters and environmental illumination conditions. For the task of spacecraft pose estimation, we compare the results of training with a SPIN-generated dataset against existing synthetic datasets. We show a %50 average error reduction in common testbed data (that simulates realistic space conditions). Both the SPIN tool (and source code) and our enhanced version of the synthetic datasets will be publicly released upon paper acceptance on GitHub https://github.com/vpulab/SPIN.","sentences":["Data acquired in space operational conditions is scarce due to the costs and complexity of space operations.","This poses a challenge to learning-based visual-based navigation algorithms employed in autonomous spacecraft navigation.","Existing datasets, which largely depend on computer-simulated data, have partially filled this gap.","However, the image generation tools they use are proprietary, which limits the evaluation of methods to unseen scenarios.","Furthermore, these datasets provide limited ground-truth data, primarily focusing on the spacecraft's translation and rotation relative to the camera.","To address these limitations, we present SPIN (SPacecraft Imagery for Navigation), an open-source realistic spacecraft image generation tool for relative navigation between two spacecrafts.","SPIN provides a wide variety of ground-truth data and allows researchers to employ custom 3D models of satellites, define specific camera-relative poses, and adjust various settings such as camera parameters and environmental illumination conditions.","For the task of spacecraft pose estimation, we compare the results of training with a SPIN-generated dataset against existing synthetic datasets.","We show a %50 average error reduction in common testbed data (that simulates realistic space conditions).","Both the SPIN tool (and source code) and our enhanced version of the synthetic datasets will be publicly released upon paper acceptance on GitHub https://github.com/vpulab/SPIN."],"url":"http://arxiv.org/abs/2406.07500v1","category":"cs.CV"}
{"created":"2024-06-11 17:32:28","title":"A pilot protocol and cohort for the investigation of non-pathological variability in speech","abstract":"Background Speech-based biomarkers have potential as a means for regular, objective assessment of symptom severity, remotely and in-clinic in combination with advanced analytical models. However, the complex nature of speech and the often subtle changes associated with health mean that findings are highly dependent on methodological and cohort choices. These are often not reported adequately in studies investigating speech-based health assessment Objective To develop and apply an exemplar protocol to generate a pilot dataset of healthy speech with detailed metadata for the assessment of factors in the speech recording-analysis pipeline, including device choice, speech elicitation task and non-pathological variability. Methods We developed our collection protocol and choice of exemplar speech features based on a thematic literature review. Our protocol includes the elicitation of three different speech types. With a focus towards remote applications, we also choose to collect speech with three different microphone types. We developed a pipeline to extract a set of 14 exemplar speech features. Results We collected speech from 28 individuals three times in one day, repeated at the same times 8-11 weeks later, and from 25 healthy individuals three times in one week. Participant characteristics collected included sex, age, native language status and voice use habits of the participant. A preliminary set of 14 speech features covering timing, prosody, voice quality, articulation and spectral moment characteristics were extracted that provide a resource of normative values. Conclusions There are multiple methodological factors involved in the collection, processing and analysis of speech recordings. Consistent reporting and greater harmonisation of study protocols are urgently required to aid the translation of speech processing into clinical research and practice.","sentences":["Background Speech-based biomarkers have potential as a means for regular, objective assessment of symptom severity, remotely and in-clinic in combination with advanced analytical models.","However, the complex nature of speech and the often subtle changes associated with health mean that findings are highly dependent on methodological and cohort choices.","These are often not reported adequately in studies investigating speech-based health assessment Objective To develop and apply an exemplar protocol to generate a pilot dataset of healthy speech with detailed metadata for the assessment of factors in the speech recording-analysis pipeline, including device choice, speech elicitation task and non-pathological variability.","Methods We developed our collection protocol and choice of exemplar speech features based on a thematic literature review.","Our protocol includes the elicitation of three different speech types.","With a focus towards remote applications, we also choose to collect speech with three different microphone types.","We developed a pipeline to extract a set of 14 exemplar speech features.","Results We collected speech from 28 individuals three times in one day, repeated at the same times 8-11 weeks later, and from 25 healthy individuals three times in one week.","Participant characteristics collected included sex, age, native language status and voice use habits of the participant.","A preliminary set of 14 speech features covering timing, prosody, voice quality, articulation and spectral moment characteristics were extracted that provide a resource of normative values.","Conclusions There are multiple methodological factors involved in the collection, processing and analysis of speech recordings.","Consistent reporting and greater harmonisation of study protocols are urgently required to aid the translation of speech processing into clinical research and practice."],"url":"http://arxiv.org/abs/2406.07497v1","category":"cs.SD"}
{"created":"2024-06-11 17:32:21","title":"TextGrad: Automatic \"Differentiation\" via Text","abstract":"AI is undergoing a paradigm shift, with breakthroughs achieved by systems orchestrating multiple large language models (LLMs) and other complex components. As a result, developing principled and automated optimization methods for compound AI systems is one of the most important new challenges. Neural networks faced a similar challenge in its early days until backpropagation and automatic differentiation transformed the field by making optimization turn-key. Inspired by this, we introduce TextGrad, a powerful framework performing automatic ``differentiation'' via text. TextGrad backpropagates textual feedback provided by LLMs to improve individual components of a compound AI system. In our framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures. TextGrad follows PyTorch's syntax and abstraction and is flexible and easy-to-use. It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework. We showcase TextGrad's effectiveness and generality across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning. Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\\%$ to $55\\%$, yields $20\\%$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity. TextGrad lays a foundation to accelerate the development of the next-generation of AI systems.","sentences":["AI is undergoing a paradigm shift, with breakthroughs achieved by systems orchestrating multiple large language models (LLMs) and other complex components.","As a result, developing principled and automated optimization methods for compound AI systems is one of the most important new challenges.","Neural networks faced a similar challenge in its early days until backpropagation and automatic differentiation transformed the field by making optimization turn-key.","Inspired by this, we introduce TextGrad, a powerful framework performing automatic ``differentiation'' via text.","TextGrad backpropagates textual feedback provided by LLMs to improve individual components of a compound AI system.","In our framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures.","TextGrad follows PyTorch's syntax and abstraction and is flexible and easy-to-use.","It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework.","We showcase TextGrad's effectiveness and generality across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning.","Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\\%$ to $55\\%$, yields $20\\%$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity.","TextGrad lays a foundation to accelerate the development of the next-generation of AI systems."],"url":"http://arxiv.org/abs/2406.07496v1","category":"cs.CL"}
{"created":"2024-06-11 17:30:22","title":"CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization","abstract":"Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant.","sentences":["Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries.","Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges.","This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases.","We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models.","We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities.","We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement.","We observe that only a few datasets span across all subdomains.","The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines.","Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant."],"url":"http://arxiv.org/abs/2406.07494v1","category":"cs.CL"}
{"created":"2024-06-11 17:29:55","title":"Spread Complexity of High Energy Neutrino Propagation over Astrophysical Distances","abstract":"Spread complexity measures the minimized spread of quantum states over all choices of basis. It generalizes Krylov operator complexity to quantum states under continuous Hamiltonian evolution. In this paper, we study spread complexity in the context of high-energy astrophysical neutrinos and propose a new flavor ratio based on complexity. Our findings indicate that our proposal might favor an initial ratio of fluxes as $\\phi_{\\nu_e}^0: \\phi_{\\nu_\\mu}^0: \\phi_{\\nu_\\tau}^0 = 1:0:0$ over a more generally expected ratio of $1:2:0$, when the IceCube neutrino observatory achieves its projected sensitivity to discriminate between flavors. Additionally, complexity-based definitions of flavor ratios exhibit a slight but nonzero sensitivity to the neutrino mass ordering, which traditional flavor ratios cannot capture.","sentences":["Spread complexity measures the minimized spread of quantum states over all choices of basis.","It generalizes Krylov operator complexity to quantum states under continuous Hamiltonian evolution.","In this paper, we study spread complexity in the context of high-energy astrophysical neutrinos and propose a new flavor ratio based on complexity.","Our findings indicate that our proposal might favor an initial ratio of fluxes as $\\phi_{\\nu_e}^0: \\phi_{\\nu_\\mu}^0:","\\phi_{\\nu_\\tau}^0 = 1:0:0$ over a more generally expected ratio of $1:2:0$, when the IceCube neutrino observatory achieves its projected sensitivity to discriminate between flavors.","Additionally, complexity-based definitions of flavor ratios exhibit a slight but nonzero sensitivity to the neutrino mass ordering, which traditional flavor ratios cannot capture."],"url":"http://arxiv.org/abs/2406.07491v1","category":"hep-ph"}
{"created":"2024-06-11 17:29:29","title":"Prospects for the detection of Dark Matter with Long-lived Mediators in the Sun using the Southern Wide-field Gamma-ray Observatory","abstract":"The operation of the next generation of gamma-ray observatories will lead to a great advance in dark matter searches. In this paper, we use the hidden sectors hypothesis within the so-called secluded models to calculate the capabilities of the Southern Wide-field Gamma-ray Observatory (SWGO) to detect gamma-ray signatures produced by dark matter particles concentrated in the Sun. We assume the dark matter particle annihilates into metastable mediators which decay into $\\gamma\\gamma$, $e^+e^-$, $\\tau^+\\tau^-$, and $\\bar{b}b$ outside the Sun. We found that the SWGO will be able to probe a spin-dependent cross-section of about $10^{-46}$ cm$^2$ for dark matter masses smaller than 5 TeV. This result shows an unprecedented sensitivity surpassing the current instruments by more than one order of magnitude.","sentences":["The operation of the next generation of gamma-ray observatories will lead to a great advance in dark matter searches.","In this paper, we use the hidden sectors hypothesis within the so-called secluded models to calculate the capabilities of the Southern Wide-field Gamma-ray Observatory (SWGO) to detect gamma-ray signatures produced by dark matter particles concentrated in the Sun.","We assume the dark matter particle annihilates into metastable mediators which decay into $\\gamma\\gamma$, $e^+e^-$, $\\tau^+\\tau^-$, and $\\bar{b}b$ outside the Sun.","We found that the SWGO will be able to probe a spin-dependent cross-section of about $10^{-46}$ cm$^2$ for dark matter masses smaller than 5 TeV. This result shows an unprecedented sensitivity surpassing the current instruments by more than one order of magnitude."],"url":"http://arxiv.org/abs/2406.07489v1","category":"astro-ph.HE"}
{"created":"2024-06-11 17:26:58","title":"PITCH: Productivity and Mental Well-being Coaching through Daily Conversational Interaction","abstract":"Efficient task planning is essential for productivity and mental well-being, yet individuals often struggle to create realistic plans and reflect upon their productivity. Leveraging the advancement in artificial intelligence (AI), conversational agents have emerged as a promising tool for enhancing productivity. Our work focuses on externalizing plans through conversation, aiming to solidify intentions and foster focused action, thereby positively impacting their productivity and mental well-being. We share our plan of designing a conversational agent to offer insightful questions and reflective prompts for increasing plan adherence by leveraging the social interactivity of natural conversations. Previous studies have shown the effectiveness of such agents, but many interventions remain static, leading to decreased user engagement over time. To address this limitation, we propose a novel rotation and context-aware prompting strategy, providing users with varied interventions daily. Our system, PITCH, utilizes large language models (LLMs) to facilitate externalization and reflection on daily plans. Through this study, we investigate the impact of externalizing tasks with conversational agents on productivity and mental well-being, and the effectiveness of a rotation strategy in maintaining user engagement.","sentences":["Efficient task planning is essential for productivity and mental well-being, yet individuals often struggle to create realistic plans and reflect upon their productivity.","Leveraging the advancement in artificial intelligence (AI), conversational agents have emerged as a promising tool for enhancing productivity.","Our work focuses on externalizing plans through conversation, aiming to solidify intentions and foster focused action, thereby positively impacting their productivity and mental well-being.","We share our plan of designing a conversational agent to offer insightful questions and reflective prompts for increasing plan adherence by leveraging the social interactivity of natural conversations.","Previous studies have shown the effectiveness of such agents, but many interventions remain static, leading to decreased user engagement over time.","To address this limitation, we propose a novel rotation and context-aware prompting strategy, providing users with varied interventions daily.","Our system, PITCH, utilizes large language models (LLMs) to facilitate externalization and reflection on daily plans.","Through this study, we investigate the impact of externalizing tasks with conversational agents on productivity and mental well-being, and the effectiveness of a rotation strategy in maintaining user engagement."],"url":"http://arxiv.org/abs/2406.07485v1","category":"cs.HC"}
{"created":"2024-06-11 17:26:14","title":"Towards Generalized Hydrological Forecasting using Transformer Models for 120-Hour Streamflow Prediction","abstract":"This study explores the efficacy of a Transformer model for 120-hour streamflow prediction across 125 diverse locations in Iowa, US. Utilizing data from the preceding 72 hours, including precipitation, evapotranspiration, and discharge values, we developed a generalized model to predict future streamflow. Our approach contrasts with traditional methods that typically rely on location-specific models. We benchmarked the Transformer model's performance against three deep learning models (LSTM, GRU, and Seq2Seq) and the Persistence approach, employing Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency (KGE), Pearson's r, and Normalized Root Mean Square Error (NRMSE) as metrics. The study reveals the Transformer model's superior performance, maintaining higher median NSE and KGE scores and exhibiting the lowest NRMSE values. This indicates its capability to accurately simulate and predict streamflow, adapting effectively to varying hydrological conditions and geographical variances. Our findings underscore the Transformer model's potential as an advanced tool in hydrological modeling, offering significant improvements over traditional and contemporary approaches.","sentences":["This study explores the efficacy of a Transformer model for 120-hour streamflow prediction across 125 diverse locations in Iowa, US.","Utilizing data from the preceding 72 hours, including precipitation, evapotranspiration, and discharge values, we developed a generalized model to predict future streamflow.","Our approach contrasts with traditional methods that typically rely on location-specific models.","We benchmarked the Transformer model's performance against three deep learning models (LSTM, GRU, and Seq2Seq) and the Persistence approach, employing Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency (KGE), Pearson's r, and Normalized Root Mean Square Error (NRMSE) as metrics.","The study reveals the Transformer model's superior performance, maintaining higher median NSE and KGE scores and exhibiting the lowest NRMSE values.","This indicates its capability to accurately simulate and predict streamflow, adapting effectively to varying hydrological conditions and geographical variances.","Our findings underscore the Transformer model's potential as an advanced tool in hydrological modeling, offering significant improvements over traditional and contemporary approaches."],"url":"http://arxiv.org/abs/2406.07484v1","category":"cs.LG"}
{"created":"2024-06-11 17:24:30","title":"The end of multiple choice tests: using AI to enhance assessment","abstract":"Effective teaching relies on knowing what students know-or think they know. Revealing student thinking is challenging. Often used because of their ease of grading, even the best multiple choice (MC) tests, those using research based distractors (wrong answers) are intrinsically limited in the insights they provide due to two factors. When distractors do not reflect student beliefs they can be ignored, increasing the likelihood that the correct answer will be chosen by chance. Moreover, making the correct choice does not guarantee that the student understands why it is correct. To address these limitations, we recommend asking students to explain why they chose their answer, and why \"wrong\" choices are wrong. Using a discipline-trained artificial intelligence-based bot it is possible to analyze their explanations, identifying the concepts and scientific principles that maybe missing or misapplied. The bot also makes suggestions for how instructors can use these data to better guide student thinking. In a small \"proof of concept\" study, we tested this approach using questions from the Biology Concepts Instrument (BCI). The result was rapid, informative, and provided actionable feedback on student thinking. It appears that the use of AI addresses the weaknesses of conventional MC test. It seems likely that incorporating AI-analyzed formative assessments will lead to improved overall learning outcomes.","sentences":["Effective teaching relies on knowing what students know-or think they know.","Revealing student thinking is challenging.","Often used because of their ease of grading, even the best multiple choice (MC) tests, those using research based distractors (wrong answers) are intrinsically limited in the insights they provide due to two factors.","When distractors do not reflect student beliefs they can be ignored, increasing the likelihood that the correct answer will be chosen by chance.","Moreover, making the correct choice does not guarantee that the student understands why it is correct.","To address these limitations, we recommend asking students to explain why they chose their answer, and why \"wrong\" choices are wrong.","Using a discipline-trained artificial intelligence-based bot it is possible to analyze their explanations, identifying the concepts and scientific principles that maybe missing or misapplied.","The bot also makes suggestions for how instructors can use these data to better guide student thinking.","In a small \"proof of concept\" study, we tested this approach using questions from the Biology Concepts Instrument (BCI).","The result was rapid, informative, and provided actionable feedback on student thinking.","It appears that the use of AI addresses the weaknesses of conventional MC test.","It seems likely that incorporating AI-analyzed formative assessments will lead to improved overall learning outcomes."],"url":"http://arxiv.org/abs/2406.07481v1","category":"cs.CY"}
{"created":"2024-06-11 17:23:48","title":"Lower bounds for sphere packing in arbitrary norms","abstract":"We show that in any $d$-dimensional real normed space, unit balls can be packed with density at least \\[\\frac{(1-o(1))d\\log d}{2^{d+1}},\\] improving a result of Schmidt from 1958 by a logarithmic factor and generalizing the recent result of Campos, Jenssen, Michelen, and Sahasrabudhe in the $\\ell_2$ norm. Our main tools are the graph-theoretic result used in the $\\ell_2$ construction and recent progress on the Bourgain slicing problem.","sentences":["We show that in any $d$-dimensional real normed space, unit balls can be packed with density at least \\[\\frac{(1-o(1))d\\log d}{2^{d+1}},\\] improving a result of Schmidt from 1958 by a logarithmic factor and generalizing the recent result of Campos, Jenssen, Michelen, and Sahasrabudhe in the $\\ell_2$ norm.","Our main tools are the graph-theoretic result used in the $\\ell_2$ construction and recent progress on the Bourgain slicing problem."],"url":"http://arxiv.org/abs/2406.07479v1","category":"math.MG"}
{"created":"2024-06-11 17:22:23","title":"VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs","abstract":"In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks. Building upon its predecessor, VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC) connector, which effectively captures the intricate spatial and temporal dynamics of video data. Additionally, we integrate an Audio Branch into the model through joint training, thereby enriching the multimodal understanding capabilities of the model by seamlessly incorporating audio cues. Comprehensive evaluations on multiple-choice video question answering (MC-VQA), open-ended video question answering (OE-VQA), and video captioning (VC) tasks demonstrate that VideoLLaMA 2 consistently achieves competitive results among open-source models and even gets close to some proprietary models on several benchmarks. Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and audio-video question-answering (AQA & OE-AVQA) benchmarks over existing models. These advancements underline VideoLLaMA 2's superior performance in multimodal comprehension, setting a new standard for intelligent video analysis systems. All models are public to facilitate further research.","sentences":["In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks.","Building upon its predecessor, VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC) connector, which effectively captures the intricate spatial and temporal dynamics of video data.","Additionally, we integrate an Audio Branch into the model through joint training, thereby enriching the multimodal understanding capabilities of the model by seamlessly incorporating audio cues.","Comprehensive evaluations on multiple-choice video question answering (MC-VQA), open-ended video question answering (OE-VQA), and video captioning (VC) tasks demonstrate that VideoLLaMA 2 consistently achieves competitive results among open-source models and even gets close to some proprietary models on several benchmarks.","Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and audio-video question-answering (AQA & OE-AVQA) benchmarks over existing models.","These advancements underline VideoLLaMA 2's superior performance in multimodal comprehension, setting a new standard for intelligent video analysis systems.","All models are public to facilitate further research."],"url":"http://arxiv.org/abs/2406.07476v1","category":"cs.CV"}
{"created":"2024-06-11 17:20:28","title":"Quantifying Local Model Validity using Active Learning","abstract":"Real-world applications of machine learning models are often subject to legal or policy-based regulations. Some of these regulations require ensuring the validity of the model, i.e., the approximation error being smaller than a threshold. A global metric is generally too insensitive to determine the validity of a specific prediction, whereas evaluating local validity is costly since it requires gathering additional data.We propose learning the model error to acquire a local validity estimate while reducing the amount of required data through active learning. Using model validation benchmarks, we provide empirical evidence that the proposed method can lead to an error model with sufficient discriminative properties using a relatively small amount of data. Furthermore, an increased sensitivity to local changes of the validity bounds compared to alternative approaches is demonstrated.","sentences":["Real-world applications of machine learning models are often subject to legal or policy-based regulations.","Some of these regulations require ensuring the validity of the model, i.e., the approximation error being smaller than a threshold.","A global metric is generally too insensitive to determine the validity of a specific prediction, whereas evaluating local validity is costly since it requires gathering additional data.","We propose learning the model error to acquire a local validity estimate while reducing the amount of required data through active learning.","Using model validation benchmarks, we provide empirical evidence that the proposed method can lead to an error model with sufficient discriminative properties using a relatively small amount of data.","Furthermore, an increased sensitivity to local changes of the validity bounds compared to alternative approaches is demonstrated."],"url":"http://arxiv.org/abs/2406.07474v1","category":"stat.ML"}
{"created":"2024-06-11 17:19:26","title":"4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models","abstract":"Existing dynamic scene generation methods mostly rely on distilling knowledge from pre-trained 3D generative models, which are typically fine-tuned on synthetic object datasets. As a result, the generated scenes are often object-centric and lack photorealism. To address these limitations, we introduce a novel pipeline designed for photorealistic text-to-4D scene generation, discarding the dependency on multi-view generative models and instead fully utilizing video generative models trained on diverse real-world datasets. Our method begins by generating a reference video using the video generation model. We then learn the canonical 3D representation of the video using a freeze-time video, delicately generated from the reference video. To handle inconsistencies in the freeze-time video, we jointly learn a per-frame deformation to model these imperfections. We then learn the temporal deformation based on the canonical representation to capture dynamic interactions in the reference video. The pipeline facilitates the generation of dynamic scenes with enhanced photorealism and structural integrity, viewable from multiple perspectives, thereby setting a new standard in 4D scene generation.","sentences":["Existing dynamic scene generation methods mostly rely on distilling knowledge from pre-trained 3D generative models, which are typically fine-tuned on synthetic object datasets.","As a result, the generated scenes are often object-centric and lack photorealism.","To address these limitations, we introduce a novel pipeline designed for photorealistic text-to-4D scene generation, discarding the dependency on multi-view generative models and instead fully utilizing video generative models trained on diverse real-world datasets.","Our method begins by generating a reference video using the video generation model.","We then learn the canonical 3D representation of the video using a freeze-time video, delicately generated from the reference video.","To handle inconsistencies in the freeze-time video, we jointly learn a per-frame deformation to model these imperfections.","We then learn the temporal deformation based on the canonical representation to capture dynamic interactions in the reference video.","The pipeline facilitates the generation of dynamic scenes with enhanced photorealism and structural integrity, viewable from multiple perspectives, thereby setting a new standard in 4D scene generation."],"url":"http://arxiv.org/abs/2406.07472v1","category":"cs.CV"}
{"created":"2024-06-11 17:18:11","title":"OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding","abstract":"Surgical scene perception via videos are critical for advancing robotic surgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology. However, the scarcity of diverse and richly annotated video datasets has hindered the development of intelligent systems for surgical workflow analysis. Existing datasets for surgical workflow analysis, which typically face challenges such as small scale, a lack of diversity in surgery and phase categories, and the absence of time-localized annotations, limit the requirements for action understanding and model generalization validation in complex and diverse real-world surgical scenarios. To address this gap, we introduce OphNet, a large-scale, expert-annotated video benchmark for ophthalmic surgical workflow understanding. OphNet features: 1) A diverse collection of 2,278 surgical videos spanning 66 types of cataract, glaucoma, and corneal surgeries, with detailed annotations for 102 unique surgical phases and 150 granular operations; 2) It offers sequential and hierarchical annotations for each surgery, phase, and operation, enabling comprehensive understanding and improved interpretability; 3) Moreover, OphNet provides time-localized annotations, facilitating temporal localization and prediction tasks within surgical workflows. With approximately 205 hours of surgical videos, OphNet is about 20 times larger than the largest existing surgical workflow analysis benchmark. Our dataset and code have been made available at: \\url{https://github.com/minghu0830/OphNet-benchmark}.","sentences":["Surgical scene perception via videos are critical for advancing robotic surgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology.","However, the scarcity of diverse and richly annotated video datasets has hindered the development of intelligent systems for surgical workflow analysis.","Existing datasets for surgical workflow analysis, which typically face challenges such as small scale, a lack of diversity in surgery and phase categories, and the absence of time-localized annotations, limit the requirements for action understanding and model generalization validation in complex and diverse real-world surgical scenarios.","To address this gap, we introduce OphNet, a large-scale, expert-annotated video benchmark for ophthalmic surgical workflow understanding.","OphNet features: 1) A diverse collection of 2,278 surgical videos spanning 66 types of cataract, glaucoma, and corneal surgeries, with detailed annotations for 102 unique surgical phases and 150 granular operations; 2) It offers sequential and hierarchical annotations for each surgery, phase, and operation, enabling comprehensive understanding and improved interpretability; 3) Moreover, OphNet provides time-localized annotations, facilitating temporal localization and prediction tasks within surgical workflows.","With approximately 205 hours of surgical videos, OphNet is about 20 times larger than the largest existing surgical workflow analysis benchmark.","Our dataset and code have been made available at: \\url{https://github.com/minghu0830/OphNet-benchmark}."],"url":"http://arxiv.org/abs/2406.07471v1","category":"cs.CV"}
{"created":"2024-06-11 17:11:08","title":"Microbiomes Through The Looking Glass","abstract":"Bacterial communities are pivotal to maintaining ecological function and preserving the rich tapestry of biological diversity. The rapid development of environmental sequencing technologies, such as metagenomics, has revolutionized our capacity to probe such diversity. However, despite these advances, a theoretical understanding connecting empirical data with ecosystem modelling, in particular in the framework of disordered systems akin to spin glasses, is still in its infancy. Here, we present a comprehensive framework using theories of disordered systems to decode microbiome data, which offers insight into the ecological forces that shape macroecological states. By employing the quenched disordered generalized Lotka-Volterra model, we analyze species abundance data in healthy and diseased human gut microbiomes. Results reveal the emergence of two distinct patterns of species-interaction networks, elucidating the pathways through which dysbiosis may drive microbiome instability. Interaction patterns thus provide a window into the systemic shifts accompanying the transition from health to disease, offering a new perspective on the dynamics of the microbial community. Our findings suggest the potential of disordered systems theory to characterize microbiomes by capturing the essence of ecological interactions and their consequences on stability and functioning, leveraging our understanding of the linkages of dysbiosis and microbial dynamics.","sentences":["Bacterial communities are pivotal to maintaining ecological function and preserving the rich tapestry of biological diversity.","The rapid development of environmental sequencing technologies, such as metagenomics, has revolutionized our capacity to probe such diversity.","However, despite these advances, a theoretical understanding connecting empirical data with ecosystem modelling, in particular in the framework of disordered systems akin to spin glasses, is still in its infancy.","Here, we present a comprehensive framework using theories of disordered systems to decode microbiome data, which offers insight into the ecological forces that shape macroecological states.","By employing the quenched disordered generalized Lotka-Volterra model, we analyze species abundance data in healthy and diseased human gut microbiomes.","Results reveal the emergence of two distinct patterns of species-interaction networks, elucidating the pathways through which dysbiosis may drive microbiome instability.","Interaction patterns thus provide a window into the systemic shifts accompanying the transition from health to disease, offering a new perspective on the dynamics of the microbial community.","Our findings suggest the potential of disordered systems theory to characterize microbiomes by capturing the essence of ecological interactions and their consequences on stability and functioning, leveraging our understanding of the linkages of dysbiosis and microbial dynamics."],"url":"http://arxiv.org/abs/2406.07465v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-11 17:09:31","title":"Reconfigurable Intelligent Surfaces in Dynamic Rich Scattering Environments: BiLSTM-Based Optimization for Accurate User Localization","abstract":"The integration of reconfigurable intelligent surfaces (RIS) in wireless environments offers channel programmability and dynamic control over propagation channels, which is expected to play a crucial role in sixth generation (6G) networks. The majority of RIS-related research has focused on simpler, quasi-free-space conditions, where wireless channels are typically modeled analytically. However, many practical localization scenarios unfold in environments characterized by rich scattering that also change over time. These dynamic and complex conditions pose significant challenges in determining the optimal RIS configuration to maximize localization accuracy. In this paper, we present our approach to overcoming this challenge. This paper introduces a novel approach that leverages a bidirectional long-short term memory (biLSTM) network, trained with a simulator that accurately reflects wave physics, to capture the relationship between wireless channels and the RIS configuration under dynamic, rich-scattering conditions. We use this approach to optimize RIS configurations for enhanced user equipment (UE) localization, measured by mean squared error (MSE). Through extensive simulations, we demonstrate that our approach adapts RIS configurations to significantly improve localization accuracy in such dynamically changing rich scattering environments.","sentences":["The integration of reconfigurable intelligent surfaces (RIS) in wireless environments offers channel programmability and dynamic control over propagation channels, which is expected to play a crucial role in sixth generation (6G) networks.","The majority of RIS-related research has focused on simpler, quasi-free-space conditions, where wireless channels are typically modeled analytically.","However, many practical localization scenarios unfold in environments characterized by rich scattering that also change over time.","These dynamic and complex conditions pose significant challenges in determining the optimal RIS configuration to maximize localization accuracy.","In this paper, we present our approach to overcoming this challenge.","This paper introduces a novel approach that leverages a bidirectional long-short term memory (biLSTM) network, trained with a simulator that accurately reflects wave physics, to capture the relationship between wireless channels and the RIS configuration under dynamic, rich-scattering conditions.","We use this approach to optimize RIS configurations for enhanced user equipment (UE) localization, measured by mean squared error (MSE).","Through extensive simulations, we demonstrate that our approach adapts RIS configurations to significantly improve localization accuracy in such dynamically changing rich scattering environments."],"url":"http://arxiv.org/abs/2406.07463v1","category":"eess.SP"}
{"created":"2024-06-11 17:08:53","title":"Rayleigh surface waves of extremal elastic materials","abstract":"Extremal elastic materials here refer to a specific class of elastic materials whose elastic matrices exhibit one or more zero eigenvalues, resulting in soft deformation modes that, in principle, cost no energy. They can be approximated through artificially designed solid microstructures. Extremal elastic materials have exotic bulk wave properties unavailable with conventional solids due to the soft modes, offering unprecedented opportunities for manipulating bulk waves, e.g., acting as phonon polarizers for elastic waves or invisibility cloaks for underwater acoustic waves. Despite their potential, Rayleigh surface waves, crucially linked to bulk wave behaviors of such extremal elastic materials, have largely remained unexplored so far. In this paper, we theoretically investigate the propagation of Rayleigh waves in extremal elastic materials based on continuum theory and verify our findings with designed microstructure metamaterials based on pantographic structures. Dispersion relations and polarizations of Rayleigh waves in extremal elastic materials are derived, and the impact of higher order gradient effects is also investigated by using strain gradient theory. This study provides a continuum model for exploring surface waves in extremal elastic materials and may stimulate applications of extremal elastic materials for controlling surface waves.","sentences":["Extremal elastic materials here refer to a specific class of elastic materials whose elastic matrices exhibit one or more zero eigenvalues, resulting in soft deformation modes that, in principle, cost no energy.","They can be approximated through artificially designed solid microstructures.","Extremal elastic materials have exotic bulk wave properties unavailable with conventional solids due to the soft modes, offering unprecedented opportunities for manipulating bulk waves, e.g., acting as phonon polarizers for elastic waves or invisibility cloaks for underwater acoustic waves.","Despite their potential, Rayleigh surface waves, crucially linked to bulk wave behaviors of such extremal elastic materials, have largely remained unexplored so far.","In this paper, we theoretically investigate the propagation of Rayleigh waves in extremal elastic materials based on continuum theory and verify our findings with designed microstructure metamaterials based on pantographic structures.","Dispersion relations and polarizations of Rayleigh waves in extremal elastic materials are derived, and the impact of higher order gradient effects is also investigated by using strain gradient theory.","This study provides a continuum model for exploring surface waves in extremal elastic materials and may stimulate applications of extremal elastic materials for controlling surface waves."],"url":"http://arxiv.org/abs/2406.07462v1","category":"physics.class-ph"}
{"created":"2024-06-11 17:08:21","title":"Noise-robust Speech Separation with Fast Generative Correction","abstract":"Speech separation, the task of isolating multiple speech sources from a mixed audio signal, remains challenging in noisy environments. In this paper, we propose a generative correction method to enhance the output of a discriminative separator. By leveraging a generative corrector based on a diffusion model, we refine the separation process for single-channel mixture speech by removing noises and perceptually unnatural distortions. Furthermore, we optimize the generative model using a predictive loss to streamline the diffusion model's reverse process into a single step and rectify any associated errors by the reverse process. Our method achieves state-of-the-art performance on the in-domain Libri2Mix noisy dataset, and out-of-domain WSJ with a variety of noises, improving SI-SNR by 22-35% relative to SepFormer, demonstrating robustness and strong generalization capabilities.","sentences":["Speech separation, the task of isolating multiple speech sources from a mixed audio signal, remains challenging in noisy environments.","In this paper, we propose a generative correction method to enhance the output of a discriminative separator.","By leveraging a generative corrector based on a diffusion model, we refine the separation process for single-channel mixture speech by removing noises and perceptually unnatural distortions.","Furthermore, we optimize the generative model using a predictive loss to streamline the diffusion model's reverse process into a single step and rectify any associated errors by the reverse process.","Our method achieves state-of-the-art performance on the in-domain Libri2Mix noisy dataset, and out-of-domain WSJ with a variety of noises, improving SI-SNR by 22-35% relative to SepFormer, demonstrating robustness and strong generalization capabilities."],"url":"http://arxiv.org/abs/2406.07461v1","category":"eess.AS"}
{"created":"2024-06-11 17:01:52","title":"Estimating the Hallucination Rate of Generative AI","abstract":"This work is about estimating the hallucination rate for in-context learning (ICL) with Generative AI. In ICL, a conditional generative model (CGM) is prompted with a dataset and asked to make a prediction based on that dataset. The Bayesian interpretation of ICL assumes that the CGM is calculating a posterior predictive distribution over an unknown Bayesian model of a latent parameter and data. With this perspective, we define a \\textit{hallucination} as a generated prediction that has low-probability under the true latent parameter. We develop a new method that takes an ICL problem -- that is, a CGM, a dataset, and a prediction question -- and estimates the probability that a CGM will generate a hallucination. Our method only requires generating queries and responses from the model and evaluating its response log probability. We empirically evaluate our method on synthetic regression and natural language ICL tasks using large language models.","sentences":["This work is about estimating the hallucination rate for in-context learning (ICL) with Generative AI.","In ICL, a conditional generative model (CGM) is prompted with a dataset and asked to make a prediction based on that dataset.","The Bayesian interpretation of ICL assumes that the CGM is calculating a posterior predictive distribution over an unknown Bayesian model of a latent parameter and data.","With this perspective, we define a \\textit{hallucination} as a generated prediction that has low-probability under the true latent parameter.","We develop a new method that takes an ICL problem -- that is, a CGM, a dataset, and a prediction question -- and estimates the probability that a CGM will generate a hallucination.","Our method only requires generating queries and responses from the model and evaluating its response log probability.","We empirically evaluate our method on synthetic regression and natural language ICL tasks using large language models."],"url":"http://arxiv.org/abs/2406.07457v1","category":"cs.LG"}
{"created":"2024-06-11 17:01:41","title":"Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis","abstract":"In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model. We developed a model-free RLHF best policy identification algorithm, called $\\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM). The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which resembles the result in classic RL, where $c_{\\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size. Moreover, $\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach. Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift.","sentences":["In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model.","We developed a model-free RLHF best policy identification algorithm, called $\\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM).","The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one.","$\\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which resembles the result in classic RL, where $c_{\\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size.","Moreover, $\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach.","Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift."],"url":"http://arxiv.org/abs/2406.07455v1","category":"cs.LG"}
{"created":"2024-06-11 16:57:58","title":"Function spaces on Corson-like compacta","abstract":"For an index set $\\Gamma$ and a cardinal number $\\kappa$ the $\\Sigma_{\\kappa}$-product of real lines $\\Sigma_{\\kappa}(\\mathbb{R}^{\\Gamma})$ consist of all elements of $\\mathbb{R}^{\\Gamma}$ with $<\\kappa$ nonzero coordinates. A compact space is $\\kappa$-Corson if it can be embedded into $\\Sigma_{\\kappa}(\\mathbb{R}^{\\Gamma})$ for some $\\Gamma$. We also consider a class of compact spaces wider than the class of $\\omega$-Corson compact spaces, investigated by Nakhmanson and Yakovlev as well as Marciszewski, Plebanek and Zakrzewski called $NY$ compact spaces. For a Tychonoff space $X$, let $C_{p}(X)$ be the space of real continuous functions on the space $X$, endowed with the pointwise convergence topology. We present here a characterisation of $\\kappa$-Corson compact spaces $K$ for regular, uncountable cardinal numbers $\\kappa$ in terms of function spaces $C_{p}(K)$, extending a theorem of Bell and Marciszewski and a theorem of Pol. We also prove that classes of $NY$ compact spaces and $\\omega$-Corson compact spaces $K$ are preserved by linear homeomorphisms of function spaces $C_{p}(K)$.","sentences":["For an index set $\\Gamma$ and a cardinal number $\\kappa$ the $\\Sigma_{\\kappa}$-product of real lines $\\Sigma_{\\kappa}(\\mathbb{R}^{\\Gamma})$ consist of all elements of $\\mathbb{R}^{\\Gamma}$ with $<\\kappa$ nonzero coordinates.","A compact space is $\\kappa$-Corson if it can be embedded into $\\Sigma_{\\kappa}(\\mathbb{R}^{\\Gamma})$ for some $\\Gamma$. We also consider a class of compact spaces wider than the class of $\\omega$-Corson compact spaces, investigated by Nakhmanson and Yakovlev as well as Marciszewski, Plebanek and Zakrzewski called $NY$ compact spaces.","For a Tychonoff space $X$, let $C_{p}(X)$ be the space of real continuous functions on the space $X$, endowed with the pointwise convergence topology.","We present here a characterisation of $\\kappa$-Corson compact spaces $K$ for regular, uncountable cardinal numbers $\\kappa$ in terms of function spaces $C_{p}(K)$, extending a theorem of Bell and Marciszewski and a theorem of Pol.","We also prove that classes of $NY$ compact spaces and $\\omega$-Corson compact spaces $K$ are preserved by linear homeomorphisms of function spaces $C_{p}(K)$."],"url":"http://arxiv.org/abs/2406.07452v1","category":"math.GN"}
{"created":"2024-06-11 16:57:48","title":"An Optimism-based Approach to Online Evaluation of Generative Models","abstract":"Existing frameworks for evaluating and comparing generative models typically target an offline setting, where the evaluator has access to full batches of data produced by the models. However, in many practical scenarios, the goal is to identify the best model using the fewest generated samples to minimize the costs of querying data from the models. Such an online comparison is challenging with current offline assessment methods. In this work, we propose an online evaluation framework to find the generative model that maximizes a standard assessment score among a group of available models. Our method uses an optimism-based multi-armed bandit framework to identify the model producing data with the highest evaluation score, quantifying the quality and diversity of generated data. Specifically, we study the online assessment of generative models based on the Fr\\'echet Inception Distance (FID) and Inception Score (IS) metrics and propose the FID-UCB and IS-UCB algorithms leveraging the upper confidence bound approach in online learning. We prove sub-linear regret bounds for these algorithms and present numerical results on standard image datasets, demonstrating their effectiveness in identifying the score-maximizing generative model.","sentences":["Existing frameworks for evaluating and comparing generative models typically target an offline setting, where the evaluator has access to full batches of data produced by the models.","However, in many practical scenarios, the goal is to identify the best model using the fewest generated samples to minimize the costs of querying data from the models.","Such an online comparison is challenging with current offline assessment methods.","In this work, we propose an online evaluation framework to find the generative model that maximizes a standard assessment score among a group of available models.","Our method uses an optimism-based multi-armed bandit framework to identify the model producing data with the highest evaluation score, quantifying the quality and diversity of generated data.","Specifically, we study the online assessment of generative models based on the Fr\\'echet Inception Distance (FID) and Inception Score (IS) metrics and propose the FID-UCB and IS-UCB algorithms leveraging the upper confidence bound approach in online learning.","We prove sub-linear regret bounds for these algorithms and present numerical results on standard image datasets, demonstrating their effectiveness in identifying the score-maximizing generative model."],"url":"http://arxiv.org/abs/2406.07451v1","category":"cs.LG"}
{"created":"2024-06-11 16:55:38","title":"Benchmarking Vision-Language Contrastive Methods for Medical Representation Learning","abstract":"We perform a comprehensive benchmarking of contrastive frameworks for learning multimodal representations in the medical domain. Through this study, we aim to answer the following research questions: (i) How transferable are general-domain representations to the medical domain? (ii) Is multimodal contrastive training sufficient, or does it benefit from unimodal training as well? (iii) What is the impact of feature granularity on the effectiveness of multimodal medical representation learning? To answer these questions, we investigate eight contrastive learning approaches under identical training setups, and train them on 2.8 million image-text pairs from four datasets, and evaluate them on 25 downstream tasks, including classification (zero-shot and linear probing), image-to-text and text-to-image retrieval, and visual question-answering. Our findings suggest a positive answer to the first question, a negative answer to the second question, and the benefit of learning fine-grained features. Finally, we make our code publicly available.","sentences":["We perform a comprehensive benchmarking of contrastive frameworks for learning multimodal representations in the medical domain.","Through this study, we aim to answer the following research questions: (i) How transferable are general-domain representations to the medical domain?","(ii) Is multimodal contrastive training sufficient, or does it benefit from unimodal training as well?","(iii) What is the impact of feature granularity on the effectiveness of multimodal medical representation learning?","To answer these questions, we investigate eight contrastive learning approaches under identical training setups, and train them on 2.8 million image-text pairs from four datasets, and evaluate them on 25 downstream tasks, including classification (zero-shot and linear probing), image-to-text and text-to-image retrieval, and visual question-answering.","Our findings suggest a positive answer to the first question, a negative answer to the second question, and the benefit of learning fine-grained features.","Finally, we make our code publicly available."],"url":"http://arxiv.org/abs/2406.07450v1","category":"cs.CV"}
{"created":"2024-06-11 16:53:14","title":"Holographic and Gravity-Thermodynamic Approaches in Entropic Cosmology: Bayesian Assessment using late-time Data","abstract":"We investigate the cosmological implications of entropy-based approaches in the context of Holographic Dark Energy (HDE) and Gravity-Thermodynamics (GT) formalisms. We utilise the extended Barrow entropy form, with the index parameter $\\Delta$, representing the fractal dimension of the horizon. We also test implementing different parameter ranges for $\\Delta$, which can be extended to Tsallis' interpretation within the same formal cosmology. We perform a Bayesian analysis to constrain the cosmological parameters using the Pantheon+, more recent DESy5, DESI, and, as a supplement, Quasar datasets. We find that the HDE model within almost all data combinations performs extremely well in comparison to the GT approach, which is usually strongly disfavored. Using the combination of DESy5+DESI alone, we find that the GT approaches are disfavored at $|\\log \\mathcal{B}| \\sim 5.8$ and $|\\log \\mathcal{B}| \\sim 6.2$ for the Barrow and Tsallis limits on $\\Delta$, respectively, wrt $\\Lambda$CDM model. While the HDE approach is statistically equivalent to $\\Lambda$CDM when comparing the Bayesian evidence. We also investigate the evolution of the dark energy equation of state and place limits on the same, consistent with quintessence-like behaviour in the HDE approaches.","sentences":["We investigate the cosmological implications of entropy-based approaches in the context of Holographic Dark Energy (HDE) and Gravity-Thermodynamics (GT) formalisms.","We utilise the extended Barrow entropy form, with the index parameter $\\Delta$, representing the fractal dimension of the horizon.","We also test implementing different parameter ranges for $\\Delta$, which can be extended to Tsallis' interpretation within the same formal cosmology.","We perform a Bayesian analysis to constrain the cosmological parameters using the Pantheon+, more recent DESy5, DESI, and, as a supplement, Quasar datasets.","We find that the HDE model within almost all data combinations performs extremely well in comparison to the GT approach, which is usually strongly disfavored.","Using the combination of DESy5+DESI alone, we find that the GT approaches are disfavored at $|\\log \\mathcal{B}|","\\sim 5.8$ and $|\\log \\mathcal{B}|","\\sim 6.2$ for the Barrow and Tsallis limits on $\\Delta$, respectively, wrt","$\\Lambda$CDM model.","While the HDE approach is statistically equivalent to $\\Lambda$CDM when comparing the Bayesian evidence.","We also investigate the evolution of the dark energy equation of state and place limits on the same, consistent with quintessence-like behaviour in the HDE approaches."],"url":"http://arxiv.org/abs/2406.07446v1","category":"astro-ph.CO"}
{"created":"2024-06-11 16:51:14","title":"On the Robustness of Document-Level Relation Extraction Models to Entity Name Variations","abstract":"Driven by the demand for cross-sentence and large-scale relation extraction, document-level relation extraction (DocRE) has attracted increasing research interest. Despite the continuous improvement in performance, we find that existing DocRE models which initially perform well may make more mistakes when merely changing the entity names in the document, hindering the generalization to novel entity names. To this end, we systematically investigate the robustness of DocRE models to entity name variations in this work. We first propose a principled pipeline to generate entity-renamed documents by replacing the original entity names with names from Wikidata. By applying the pipeline to DocRED and Re-DocRED datasets, we construct two novel benchmarks named Env-DocRED and Env-Re-DocRED for robustness evaluation. Experimental results show that both three representative DocRE models and two in-context learned large language models consistently lack sufficient robustness to entity name variations, particularly on cross-sentence relation instances and documents with more entities. Finally, we propose an entity variation robust training method which not only improves the robustness of DocRE models but also enhances their understanding and reasoning capabilities. We further verify that the basic idea of this method can be effectively transferred to in-context learning for DocRE as well.","sentences":["Driven by the demand for cross-sentence and large-scale relation extraction, document-level relation extraction (DocRE) has attracted increasing research interest.","Despite the continuous improvement in performance, we find that existing DocRE models which initially perform well may make more mistakes when merely changing the entity names in the document, hindering the generalization to novel entity names.","To this end, we systematically investigate the robustness of DocRE models to entity name variations in this work.","We first propose a principled pipeline to generate entity-renamed documents by replacing the original entity names with names from Wikidata.","By applying the pipeline to DocRED and Re-DocRED datasets, we construct two novel benchmarks named Env-DocRED and Env-Re-DocRED for robustness evaluation.","Experimental results show that both three representative DocRE models and two in-context learned large language models consistently lack sufficient robustness to entity name variations, particularly on cross-sentence relation instances and documents with more entities.","Finally, we propose an entity variation robust training method which not only improves the robustness of DocRE models but also enhances their understanding and reasoning capabilities.","We further verify that the basic idea of this method can be effectively transferred to in-context learning for DocRE as well."],"url":"http://arxiv.org/abs/2406.07444v1","category":"cs.CL"}
{"created":"2024-06-11 16:48:17","title":"Textual Similarity as a Key Metric in Machine Translation Quality Estimation","abstract":"Machine Translation (MT) Quality Estimation (QE) assesses translation reliability without reference texts. This study introduces \"textual similarity\" as a new metric for QE, using sentence transformers and cosine similarity to measure semantic closeness. Analyzing data from the MLQE-PE dataset, we found that textual similarity exhibits stronger correlations with human scores than traditional metrics (hter, model evaluation etc.). Employing GAMMs as a statistical tool, we demonstrated that textual similarity consistently outperforms other metrics across multiple language pairs in predicting human scores. We also found that \"hter\" actually failed to predict human scores in QE. Our findings highlight the effectiveness of textual similarity as a robust QE metric, recommending its integration with other metrics into QE frameworks and MT system training for improved accuracy and usability.","sentences":["Machine Translation (MT) Quality Estimation (QE) assesses translation reliability without reference texts.","This study introduces \"textual similarity\" as a new metric for QE, using sentence transformers and cosine similarity to measure semantic closeness.","Analyzing data from the MLQE-PE dataset, we found that textual similarity exhibits stronger correlations with human scores than traditional metrics (hter, model evaluation etc.).","Employing GAMMs as a statistical tool, we demonstrated that textual similarity consistently outperforms other metrics across multiple language pairs in predicting human scores.","We also found that \"hter\" actually failed to predict human scores in QE.","Our findings highlight the effectiveness of textual similarity as a robust QE metric, recommending its integration with other metrics into QE frameworks and MT system training for improved accuracy and usability."],"url":"http://arxiv.org/abs/2406.07440v1","category":"cs.CL"}
{"created":"2024-06-11 16:46:47","title":"Search for photons above 10$^{18}$ eV by simultaneously measuring the atmospheric depth and the muon content of air showers at the Pierre Auger Observatory","abstract":"The Pierre Auger Observatory is the most sensitive instrument to detect photons with energies above $10^{17}$ eV. It measures extensive air showers generated by ultra high energy cosmic rays using a hybrid technique that exploits the combination of a fluorescence detector with a ground array of particle detectors. The signatures of a photon-induced air shower are a larger atmospheric depth of the shower maximum ($X_{max}$) and a steeper lateral distribution function, along with a lower number of muons with respect to the bulk of hadron-induced cascades. In this work, a new analysis technique in the energy interval between 1 and 30 EeV (1 EeV = $10^{18}$ eV) has been developed by combining the fluorescence detector-based measurement of $X_{max}$ with the specific features of the surface detector signal through a parameter related to the air shower muon content, derived from the universality of the air shower development. No evidence of a statistically significant signal due to photon primaries was found using data collected in about 12 years of operation. Thus, upper bounds to the integral photon flux have been set using a detailed calculation of the detector exposure, in combination with a data-driven background estimation. The derived 95% confidence level upper limits are 0.0403, 0.01113, 0.0035, 0.0023, and 0.0021 km$^{-2}$ sr$^{-1}$ yr$^{-1}$ above 1, 2, 3, 5, and 10 EeV, respectively, leading to the most stringent upper limits on the photon flux in the EeV range. Compared with past results, the upper limits were improved by about 40% for the lowest energy threshold and by a factor 3 above 3 EeV, where no candidates were found and the expected background is negligible. The presented limits can be used to probe the assumptions on chemical composition of ultra-high energy cosmic rays and allow for the constraint of the mass and lifetime phase space of super-heavy dark matter particles.","sentences":["The Pierre Auger Observatory is the most sensitive instrument to detect photons with energies above $10^{17}$ eV. It measures extensive air showers generated by ultra high energy cosmic rays using a hybrid technique that exploits the combination of a fluorescence detector with a ground array of particle detectors.","The signatures of a photon-induced air shower are a larger atmospheric depth of the shower maximum ($X_{max}$) and a steeper lateral distribution function, along with a lower number of muons with respect to the bulk of hadron-induced cascades.","In this work, a new analysis technique in the energy interval between 1 and 30 EeV (1 EeV = $10^{18}$ eV) has been developed by combining the fluorescence detector-based measurement of $X_{max}$ with the specific features of the surface detector signal through a parameter related to the air shower muon content, derived from the universality of the air shower development.","No evidence of a statistically significant signal due to photon primaries was found using data collected in about 12 years of operation.","Thus, upper bounds to the integral photon flux have been set using a detailed calculation of the detector exposure, in combination with a data-driven background estimation.","The derived 95% confidence level upper limits are 0.0403, 0.01113, 0.0035, 0.0023, and 0.0021 km$^{-2}$ sr$^{-1}$ yr$^{-1}$","above 1, 2, 3, 5, and 10 EeV, respectively, leading to the most stringent upper limits on the photon flux in the EeV range.","Compared with past results, the upper limits were improved by about 40% for the lowest energy threshold and by a factor 3 above 3 EeV, where no candidates were found and the expected background is negligible.","The presented limits can be used to probe the assumptions on chemical composition of ultra-high energy cosmic rays and allow for the constraint of the mass and lifetime phase space of super-heavy dark matter particles."],"url":"http://arxiv.org/abs/2406.07439v1","category":"astro-ph.HE"}
{"created":"2024-06-11 16:45:34","title":"Graph-based multi-Feature fusion method for speech emotion recognition","abstract":"Exploring proper way to conduct multi-speech feature fusion for cross-corpus speech emotion recognition is crucial as different speech features could provide complementary cues reflecting human emotion status. While most previous approaches only extract a single speech feature for emotion recognition, existing fusion methods such as concatenation, parallel connection, and splicing ignore heterogeneous patterns in the interaction between features and features, resulting in performance of existing systems. In this paper, we propose a novel graph-based fusion method to explicitly model the relationships between every pair of speech features. Specifically, we propose a multi-dimensional edge features learning strategy called Graph-based multi-Feature fusion method for speech emotion recognition. It represents each speech feature as a node and learns multi-dimensional edge features to explicitly describe the relationship between each feature-feature pair in the context of emotion recognition. This way, the learned multi-dimensional edge features encode speech feature-level information from both the vertex and edge dimensions. Our Approach consists of three modules: an Audio Feature Generation(AFG)module, an Audio-Feature Multi-dimensional Edge Feature(AMEF) module and a Speech Emotion Recognition (SER) module. The proposed methodology yielded satisfactory outcomes on the SEWA dataset. Furthermore, the method demonstrated enhanced performance compared to the baseline in the AVEC 2019 Workshop and Challenge. We used data from two cultures as our training and validation sets: two cultures containing German and Hungarian on the SEWA dataset, the CCC scores for German are improved by 17.28% for arousal and 7.93% for liking. The outcomes of our methodology demonstrate a 13% improvement over alternative fusion techniques, including those employing one dimensional edge-based feature fusion approach.","sentences":["Exploring proper way to conduct multi-speech feature fusion for cross-corpus speech emotion recognition is crucial as different speech features could provide complementary cues reflecting human emotion status.","While most previous approaches only extract a single speech feature for emotion recognition, existing fusion methods such as concatenation, parallel connection, and splicing ignore heterogeneous patterns in the interaction between features and features, resulting in performance of existing systems.","In this paper, we propose a novel graph-based fusion method to explicitly model the relationships between every pair of speech features.","Specifically, we propose a multi-dimensional edge features learning strategy called Graph-based multi-Feature fusion method for speech emotion recognition.","It represents each speech feature as a node and learns multi-dimensional edge features to explicitly describe the relationship between each feature-feature pair in the context of emotion recognition.","This way, the learned multi-dimensional edge features encode speech feature-level information from both the vertex and edge dimensions.","Our Approach consists of three modules: an Audio Feature Generation(AFG)module, an Audio-Feature Multi-dimensional Edge Feature(AMEF) module and a Speech Emotion Recognition (SER) module.","The proposed methodology yielded satisfactory outcomes on the SEWA dataset.","Furthermore, the method demonstrated enhanced performance compared to the baseline in the AVEC 2019 Workshop and Challenge.","We used data from two cultures as our training and validation sets: two cultures containing German and Hungarian on the SEWA dataset, the CCC scores for German are improved by 17.28% for arousal and 7.93% for liking.","The outcomes of our methodology demonstrate a 13% improvement over alternative fusion techniques, including those employing one dimensional edge-based feature fusion approach."],"url":"http://arxiv.org/abs/2406.07437v1","category":"cs.SD"}
{"created":"2024-06-11 16:45:17","title":"McEval: Massively Multilingual Code Evaluation","abstract":"Code large language models (LLMs) have shown remarkable advances in code understanding, completion, and generation tasks. Programming benchmarks, comprised of a selection of code challenges and corresponding test cases, serve as a standard to evaluate the capability of different LLMs in such tasks. However, most existing benchmarks primarily focus on Python and are still restricted to a limited number of languages, where other languages are translated from the Python samples (e.g. MultiPL-E) degrading the data diversity. To further facilitate the research of code LLMs, we propose a massively multilingual code benchmark covering 40 programming languages (McEval) with 16K test samples, which substantially pushes the limits of code LLMs in multilingual scenarios. The benchmark contains challenging code completion, understanding, and generation evaluation tasks with finely curated massively multilingual instruction corpora McEval-Instruct. In addition, we introduce an effective multilingual coder mCoder trained on McEval-Instruct to support multilingual programming language generation. Extensive experimental results on McEval show that there is still a difficult journey between open-source models and closed-source LLMs (e.g. GPT-series models) in numerous languages. The instruction corpora, evaluation benchmark, and leaderboard are available at \\url{https://mceval.github.io/}.","sentences":["Code large language models (LLMs) have shown remarkable advances in code understanding, completion, and generation tasks.","Programming benchmarks, comprised of a selection of code challenges and corresponding test cases, serve as a standard to evaluate the capability of different LLMs in such tasks.","However, most existing benchmarks primarily focus on Python and are still restricted to a limited number of languages, where other languages are translated from the Python samples (e.g. MultiPL-E) degrading the data diversity.","To further facilitate the research of code LLMs, we propose a massively multilingual code benchmark covering 40 programming languages (McEval) with 16K test samples, which substantially pushes the limits of code LLMs in multilingual scenarios.","The benchmark contains challenging code completion, understanding, and generation evaluation tasks with finely curated massively multilingual instruction corpora McEval-Instruct.","In addition, we introduce an effective multilingual coder mCoder trained on McEval-Instruct to support multilingual programming language generation.","Extensive experimental results on McEval show that there is still a difficult journey between open-source models and closed-source LLMs (e.g. GPT-series models) in numerous languages.","The instruction corpora, evaluation benchmark, and leaderboard are available at \\url{https://mceval.github.io/}."],"url":"http://arxiv.org/abs/2406.07436v1","category":"cs.PL"}
{"created":"2024-06-11 16:42:17","title":"Beware of Aliases -- Signal Preservation is Crucial for Robust Image Restoration","abstract":"Image restoration networks are usually comprised of an encoder and a decoder, responsible for aggregating image content from noisy, distorted data and to restore clean, undistorted images, respectively. Data aggregation as well as high-resolution image generation both usually come at the risk of involving aliases, i.e.~standard architectures put their ability to reconstruct the model input in jeopardy to reach high PSNR values on validation data. The price to be paid is low model robustness. In this work, we show that simply providing alias-free paths in state-of-the-art reconstruction transformers supports improved model robustness at low costs on the restoration performance. We do so by proposing BOA-Restormer, a transformer-based image restoration model that executes downsampling and upsampling operations partly in the frequency domain to ensure alias-free paths along the entire model while potentially preserving all relevant high-frequency information.","sentences":["Image restoration networks are usually comprised of an encoder and a decoder, responsible for aggregating image content from noisy, distorted data and to restore clean, undistorted images, respectively.","Data aggregation as well as high-resolution image generation both usually come at the risk of involving aliases, i.e.~standard architectures put their ability to reconstruct the model input in jeopardy to reach high PSNR values on validation data.","The price to be paid is low model robustness.","In this work, we show that simply providing alias-free paths in state-of-the-art reconstruction transformers supports improved model robustness at low costs on the restoration performance.","We do so by proposing BOA-Restormer, a transformer-based image restoration model that executes downsampling and upsampling operations partly in the frequency domain to ensure alias-free paths along the entire model while potentially preserving all relevant high-frequency information."],"url":"http://arxiv.org/abs/2406.07435v1","category":"cs.CV"}
{"created":"2024-06-11 16:36:27","title":"RE-algebras, quasi-determinants and the full Toda system","abstract":"In 1991, Gelfand and Retakh embodied the idea of a noncommutative Dieudonne determinant in the case of RTT algebra, namely, they found a representation of the quantum determinant of RTT algebra in the form of a product of principal quasi-determinants. In this note we construct an analogue of the above statement for the RE-algebra corresponding to the Drinfeld R-matrix for the order $n=2,3$. Namely, we have found a family of quasi-determinants that are principal with respect to the antidiagonal, commuting among themselves, whose product turns out to be the quantum determinant of this algebra. This family generalizes the construction of integrals of the full Toda system due to Deift et al. for the quantum case of RE-algebras. In our opinion, this result also clarifies the role of RE-algebras as a quantum homogeneous spaces and can be used to construct effective quantum field theories with a boundary.","sentences":["In 1991, Gelfand and Retakh embodied the idea of a noncommutative Dieudonne determinant in the case of RTT algebra, namely, they found a representation of the quantum determinant of RTT algebra in the form of a product of principal quasi-determinants.","In this note we construct an analogue of the above statement for the RE-algebra corresponding to the Drinfeld R-matrix for the order $n=2,3$. Namely, we have found a family of quasi-determinants that are principal with respect to the antidiagonal, commuting among themselves, whose product turns out to be the quantum determinant of this algebra.","This family generalizes the construction of integrals of the full Toda system due to Deift et al.","for the quantum case of RE-algebras.","In our opinion, this result also clarifies the role of RE-algebras as a quantum homogeneous spaces and can be used to construct effective quantum field theories with a boundary."],"url":"http://arxiv.org/abs/2406.07434v1","category":"hep-th"}
{"created":"2024-06-11 16:34:36","title":"Shortcuts to adiabaticity designed via time-rescaling follow the same transitionless route","abstract":"Time-rescaling (TR) has been recently proposed as a method to engineer fast processes, also known as shortcuts to adiabaticity (STA), which enables the coherent control of quantum systems beyond the adiabatic regime [B. L. Bernardo, Phys. Rev. Res. 2, 013133 (2020)]. The method provides the Hamiltonians that generate the fast processes without requiring information about the instantaneous eigenstates of a reference protocol, whereas experimental implementations dismiss additional coupling fields when compared with adiabatic protocols. Here, we revisit the technique and show that the obtained fast dynamics are transitionless, similar to the ones designed via the famous counterdiabatic (CD) approach. We also show that the time evolution of the STA found via TR relates to that of the reference adiabatic protocol by a simple reparametrization of time. To illustrate our findings, we studied the problem of speeding up the stimulated Raman adiabatic passage (STIRAP) and found out that TR revealed quantum dynamics more robust to parameter variations than those provided by the CD technique. Our results shed new light on the application of TR in the control of larger quantum systems.","sentences":["Time-rescaling (TR) has been recently proposed as a method to engineer fast processes, also known as shortcuts to adiabaticity (STA), which enables the coherent control of quantum systems beyond the adiabatic regime","[B. L. Bernardo, Phys.","Rev. Res. 2, 013133 (2020)].","The method provides the Hamiltonians that generate the fast processes without requiring information about the instantaneous eigenstates of a reference protocol, whereas experimental implementations dismiss additional coupling fields when compared with adiabatic protocols.","Here, we revisit the technique and show that the obtained fast dynamics are transitionless, similar to the ones designed via the famous counterdiabatic (CD) approach.","We also show that the time evolution of the STA found via TR relates to that of the reference adiabatic protocol by a simple reparametrization of time.","To illustrate our findings, we studied the problem of speeding up the stimulated Raman adiabatic passage (STIRAP) and found out that TR revealed quantum dynamics more robust to parameter variations than those provided by the CD technique.","Our results shed new light on the application of TR in the control of larger quantum systems."],"url":"http://arxiv.org/abs/2406.07433v1","category":"quant-ph"}
{"created":"2024-06-11 16:30:30","title":"GemNet: Menu-Based, Strategy-Proof Multi-Bidder Auctions Through Deep Learning","abstract":"Differentiable economics uses deep learning for automated mechanism design. Despite strong progress, it has remained an open problem to learn multi-bidder, general, and fully strategy-proof (SP) auctions. We introduce GEneral Menu-based NETwork (GemNet), which significantly extends the menu-based approach of RochetNet [D\\\"utting et al., 2023] to the multi-bidder setting. The challenge in achieving SP is to learn bidder-independent menus that are feasible, so that the optimal menu choices for each bidder do not over-allocate items when taken together (we call this menu compatibility). GemNet penalizes the failure of menu compatibility during training, and transforms learned menus after training through price changes, by considering a set of discretized bidder values and reasoning about Lipschitz smoothness to guarantee menu compatibility on the entire value space. This approach is general, leaving undisturbed trained menus that already satisfy menu compatibility and reducing to RochetNet for a single bidder. Mixed-integer linear programs are used for menu transforms and through a number of optimizations, including adaptive grids and methods to skip menu elements, we scale to large auction design problems. GemNet learns auctions with better revenue than affine maximization methods, achieves exact SP whereas previous general multi-bidder methods are approximately SP, and offers greatly enhanced interpretability.","sentences":["Differentiable economics uses deep learning for automated mechanism design.","Despite strong progress, it has remained an open problem to learn multi-bidder, general, and fully strategy-proof (SP) auctions.","We introduce GEneral Menu-based NETwork (GemNet), which significantly extends the menu-based approach of RochetNet [D\\\"utting et al., 2023] to the multi-bidder setting.","The challenge in achieving SP is to learn bidder-independent menus that are feasible, so that the optimal menu choices for each bidder do not over-allocate items when taken together (we call this menu compatibility).","GemNet penalizes the failure of menu compatibility during training, and transforms learned menus after training through price changes, by considering a set of discretized bidder values and reasoning about Lipschitz smoothness to guarantee menu compatibility on the entire value space.","This approach is general, leaving undisturbed trained menus that already satisfy menu compatibility and reducing to RochetNet for a single bidder.","Mixed-integer linear programs are used for menu transforms and through a number of optimizations, including adaptive grids and methods to skip menu elements, we scale to large auction design problems.","GemNet learns auctions with better revenue than affine maximization methods, achieves exact SP whereas previous general multi-bidder methods are approximately SP, and offers greatly enhanced interpretability."],"url":"http://arxiv.org/abs/2406.07428v1","category":"cs.GT"}
{"created":"2024-06-11 16:27:32","title":"DERM12345: A Large, Multisource Dermatoscopic Skin Lesion Dataset with 38 Subclasses","abstract":"Skin lesion datasets provide essential information for understanding various skin conditions and developing effective diagnostic tools. They aid the artificial intelligence-based early detection of skin cancer, facilitate treatment planning, and contribute to medical education and research. Published large datasets have partially coverage the subclassifications of the skin lesions. This limitation highlights the need for more expansive and varied datasets to reduce false predictions and help improve the failure analysis for skin lesions. This study presents a diverse dataset comprising 12,345 dermatoscopic images with 38 subclasses of skin lesions collected in Turkiye which comprises different skin types in the transition zone between Europe and Asia. Each subgroup contains high-resolution photos and expert annotations, providing a strong and reliable basis for future research. The detailed analysis of each subgroup provided in this study facilitates targeted research endeavors and enhances the depth of understanding regarding the skin lesions. This dataset distinguishes itself through a diverse structure with 5 super classes, 15 main classes, 38 subclasses and its 12,345 high-resolution dermatoscopic images.","sentences":["Skin lesion datasets provide essential information for understanding various skin conditions and developing effective diagnostic tools.","They aid the artificial intelligence-based early detection of skin cancer, facilitate treatment planning, and contribute to medical education and research.","Published large datasets have partially coverage the subclassifications of the skin lesions.","This limitation highlights the need for more expansive and varied datasets to reduce false predictions and help improve the failure analysis for skin lesions.","This study presents a diverse dataset comprising 12,345 dermatoscopic images with 38 subclasses of skin lesions collected in Turkiye which comprises different skin types in the transition zone between Europe and Asia.","Each subgroup contains high-resolution photos and expert annotations, providing a strong and reliable basis for future research.","The detailed analysis of each subgroup provided in this study facilitates targeted research endeavors and enhances the depth of understanding regarding the skin lesions.","This dataset distinguishes itself through a diverse structure with 5 super classes, 15 main classes, 38 subclasses and its 12,345 high-resolution dermatoscopic images."],"url":"http://arxiv.org/abs/2406.07426v1","category":"eess.IV"}
{"created":"2024-06-11 16:27:05","title":"On 2nd-order fully-nonlinear equations with links to 3rd-order fully-nonlinear equations","abstract":"We derive the general conditions for fully-nonlinear symmetry-integrable second-order evolution equations and their first-order recursion operators. We then apply the established Propositions to find links between a class of fully-nonlinear third-order symmetry-integrable evolution equations and fully-nonlinear second-order symmetry-integrable evolution equations.","sentences":["We derive the general conditions for fully-nonlinear symmetry-integrable second-order evolution equations and their first-order recursion operators.","We then apply the established Propositions to find links between a class of fully-nonlinear third-order symmetry-integrable evolution equations and fully-nonlinear second-order symmetry-integrable evolution equations."],"url":"http://arxiv.org/abs/2406.07425v1","category":"nlin.SI"}
{"created":"2024-06-11 16:23:33","title":"Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for Sampling","abstract":"Monte Carlo methods, Variational Inference, and their combinations play a pivotal role in sampling from intractable probability distributions. However, current studies lack a unified evaluation framework, relying on disparate performance measures and limited method comparisons across diverse tasks, complicating the assessment of progress and hindering the decision-making of practitioners. In response to these challenges, our work introduces a benchmark that evaluates sampling methods using a standardized task suite and a broad range of performance criteria. Moreover, we study existing metrics for quantifying mode collapse and introduce novel metrics for this purpose. Our findings provide insights into strengths and weaknesses of existing sampling methods, serving as a valuable reference for future developments. The code is publicly available here.","sentences":["Monte Carlo methods, Variational Inference, and their combinations play a pivotal role in sampling from intractable probability distributions.","However, current studies lack a unified evaluation framework, relying on disparate performance measures and limited method comparisons across diverse tasks, complicating the assessment of progress and hindering the decision-making of practitioners.","In response to these challenges, our work introduces a benchmark that evaluates sampling methods using a standardized task suite and a broad range of performance criteria.","Moreover, we study existing metrics for quantifying mode collapse and introduce novel metrics for this purpose.","Our findings provide insights into strengths and weaknesses of existing sampling methods, serving as a valuable reference for future developments.","The code is publicly available here."],"url":"http://arxiv.org/abs/2406.07423v1","category":"cs.LG"}
{"created":"2024-06-11 16:22:57","title":"Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation","abstract":"The multi-codebook speech codec enables the application of large language models (LLM) in TTS but bottlenecks efficiency and robustness due to multi-sequence prediction. To avoid this obstacle, we propose Single-Codec, a single-codebook single-sequence codec, which employs a disentangled VQ-VAE to decouple speech into a time-invariant embedding and a phonetically-rich discrete sequence. Furthermore, the encoder is enhanced with 1) contextual modeling with a BLSTM module to exploit the temporal information, 2) a hybrid sampling module to alleviate distortion from upsampling and downsampling, and 3) a resampling module to encourage discrete units to carry more phonetic information. Compared with multi-codebook codecs, e.g., EnCodec and TiCodec, Single-Codec demonstrates higher reconstruction quality with a lower bandwidth of only 304bps. The effectiveness of Single-Code is further validated by LLM-TTS experiments, showing improved naturalness and intelligibility.","sentences":["The multi-codebook speech codec enables the application of large language models (LLM) in TTS but bottlenecks efficiency and robustness due to multi-sequence prediction.","To avoid this obstacle, we propose Single-Codec, a single-codebook single-sequence codec, which employs a disentangled VQ-VAE to decouple speech into a time-invariant embedding and a phonetically-rich discrete sequence.","Furthermore, the encoder is enhanced with 1) contextual modeling with a BLSTM module to exploit the temporal information, 2) a hybrid sampling module to alleviate distortion from upsampling and downsampling, and 3) a resampling module to encourage discrete units to carry more phonetic information.","Compared with multi-codebook codecs, e.g., EnCodec and TiCodec, Single-Codec demonstrates higher reconstruction quality with a lower bandwidth of only 304bps.","The effectiveness of Single-Code is further validated by LLM-TTS experiments, showing improved naturalness and intelligibility."],"url":"http://arxiv.org/abs/2406.07422v1","category":"eess.AS"}
{"created":"2024-06-11 16:22:34","title":"A Comprehensive Investigation on Speaker Augmentation for Speaker Recognition","abstract":"Data augmentation (DA) has played a pivotal role in the success of deep speaker recognition. Current DA techniques primarily focus on speaker-preserving augmentation, which does not change the speaker trait of the speech and does not create new speakers. Recent research has shed light on the potential of speaker augmentation, which generates new speakers to enrich the training dataset. In this study, we delve into two speaker augmentation approaches: speed perturbation (SP) and vocal tract length perturbation (VTLP). Despite the empirical utilization of both methods, a comprehensive investigation into their efficacy is lacking. Our study, conducted using two public datasets, VoxCeleb and CN-Celeb, revealed that both SP and VTLP are proficient at generating new speakers, leading to significant performance improvements in speaker recognition. Furthermore, they exhibit distinct properties in sensitivity to perturbation factors and data complexity, hinting at the potential benefits of their fusion. Our research underscores the substantial potential of speaker augmentation, highlighting the importance of in-depth exploration and analysis.","sentences":["Data augmentation (DA) has played a pivotal role in the success of deep speaker recognition.","Current DA techniques primarily focus on speaker-preserving augmentation, which does not change the speaker trait of the speech and does not create new speakers.","Recent research has shed light on the potential of speaker augmentation, which generates new speakers to enrich the training dataset.","In this study, we delve into two speaker augmentation approaches: speed perturbation (SP) and vocal tract length perturbation (VTLP).","Despite the empirical utilization of both methods, a comprehensive investigation into their efficacy is lacking.","Our study, conducted using two public datasets, VoxCeleb and CN-Celeb, revealed that both SP and VTLP are proficient at generating new speakers, leading to significant performance improvements in speaker recognition.","Furthermore, they exhibit distinct properties in sensitivity to perturbation factors and data complexity, hinting at the potential benefits of their fusion.","Our research underscores the substantial potential of speaker augmentation, highlighting the importance of in-depth exploration and analysis."],"url":"http://arxiv.org/abs/2406.07421v1","category":"cs.SD"}
{"created":"2024-06-11 16:21:33","title":"Enhanced Gene Selection in Single-Cell Genomics: Pre-Filtering Synergy and Reinforced Optimization","abstract":"Recent advancements in single-cell genomics necessitate precision in gene panel selection to interpret complex biological data effectively. Those methods aim to streamline the analysis of scRNA-seq data by focusing on the most informative genes that contribute significantly to the specific analysis task. Traditional selection methods, which often rely on expert domain knowledge, embedded machine learning models, or heuristic-based iterative optimization, are prone to biases and inefficiencies that may obscure critical genomic signals. Recognizing the limitations of traditional methods, we aim to transcend these constraints with a refined strategy. In this study, we introduce an iterative gene panel selection strategy that is applicable to clustering tasks in single-cell genomics. Our method uniquely integrates results from other gene selection algorithms, providing valuable preliminary boundaries or prior knowledge as initial guides in the search space to enhance the efficiency of our framework. Furthermore, we incorporate the stochastic nature of the exploration process in reinforcement learning (RL) and its capability for continuous optimization through reward-based feedback. This combination mitigates the biases inherent in the initial boundaries and harnesses RL's adaptability to refine and target gene panel selection dynamically. To illustrate the effectiveness of our method, we conducted detailed comparative experiments, case studies, and visualization analysis.","sentences":["Recent advancements in single-cell genomics necessitate precision in gene panel selection to interpret complex biological data effectively.","Those methods aim to streamline the analysis of scRNA-seq data by focusing on the most informative genes that contribute significantly to the specific analysis task.","Traditional selection methods, which often rely on expert domain knowledge, embedded machine learning models, or heuristic-based iterative optimization, are prone to biases and inefficiencies that may obscure critical genomic signals.","Recognizing the limitations of traditional methods, we aim to transcend these constraints with a refined strategy.","In this study, we introduce an iterative gene panel selection strategy that is applicable to clustering tasks in single-cell genomics.","Our method uniquely integrates results from other gene selection algorithms, providing valuable preliminary boundaries or prior knowledge as initial guides in the search space to enhance the efficiency of our framework.","Furthermore, we incorporate the stochastic nature of the exploration process in reinforcement learning (RL) and its capability for continuous optimization through reward-based feedback.","This combination mitigates the biases inherent in the initial boundaries and harnesses RL's adaptability to refine and target gene panel selection dynamically.","To illustrate the effectiveness of our method, we conducted detailed comparative experiments, case studies, and visualization analysis."],"url":"http://arxiv.org/abs/2406.07418v1","category":"cs.AI"}
{"created":"2024-06-11 16:20:30","title":"The geometry of polynomial representations in positive characteristic","abstract":"A $\\mathbf{GL}$-variety is a (typically infinite dimensional) variety modeled on the polynomial representation theory of the general linear group. In previous work, we studied these varieties in characteristic 0. In this paper, we obtain results in positive characteristic: for example, we prove a version of Chevalley's theorem on constructible sets. We give an application of our theory to strength of polynomials.","sentences":["A $\\mathbf{GL}$-variety is a (typically infinite dimensional) variety modeled on the polynomial representation theory of the general linear group.","In previous work, we studied these varieties in characteristic 0.","In this paper, we obtain results in positive characteristic: for example, we prove a version of Chevalley's theorem on constructible sets.","We give an application of our theory to strength of polynomials."],"url":"http://arxiv.org/abs/2406.07415v1","category":"math.AG"}
{"created":"2024-06-11 16:18:15","title":"Holistic Memory Diversification for Incremental Learning in Growing Graphs","abstract":"This paper addresses the challenge of incremental learning in growing graphs with increasingly complex tasks. The goal is to continually train a graph model to handle new tasks while retaining its inference ability on previous tasks. Existing methods usually neglect the importance of memory diversity, limiting in effectively selecting high-quality memory from previous tasks and remembering broad previous knowledge within the scarce memory on graphs. To address that, we introduce a novel holistic Diversified Memory Selection and Generation (DMSG) framework for incremental learning in graphs, which first introduces a buffer selection strategy that considers both intra-class and inter-class diversities, employing an efficient greedy algorithm for sampling representative training nodes from graphs into memory buffers after learning each new task. Then, to adequately rememorize the knowledge preserved in the memory buffer when learning new tasks, we propose a diversified memory generation replay method. This method first utilizes a variational layer to generate the distribution of buffer node embeddings and sample synthesized ones for replaying. Furthermore, an adversarial variational embedding learning method and a reconstruction-based decoder are proposed to maintain the integrity and consolidate the generalization of the synthesized node embeddings, respectively. Finally, we evaluate our model on node classification tasks involving increasing class numbers. Extensive experimental results on publicly accessible datasets demonstrate the superiority of DMSG over state-of-the-art methods.","sentences":["This paper addresses the challenge of incremental learning in growing graphs with increasingly complex tasks.","The goal is to continually train a graph model to handle new tasks while retaining its inference ability on previous tasks.","Existing methods usually neglect the importance of memory diversity, limiting in effectively selecting high-quality memory from previous tasks and remembering broad previous knowledge within the scarce memory on graphs.","To address that, we introduce a novel holistic Diversified Memory Selection and Generation (DMSG) framework for incremental learning in graphs, which first introduces a buffer selection strategy that considers both intra-class and inter-class diversities, employing an efficient greedy algorithm for sampling representative training nodes from graphs into memory buffers after learning each new task.","Then, to adequately rememorize the knowledge preserved in the memory buffer when learning new tasks, we propose a diversified memory generation replay method.","This method first utilizes a variational layer to generate the distribution of buffer node embeddings and sample synthesized ones for replaying.","Furthermore, an adversarial variational embedding learning method and a reconstruction-based decoder are proposed to maintain the integrity and consolidate the generalization of the synthesized node embeddings, respectively.","Finally, we evaluate our model on node classification tasks involving increasing class numbers.","Extensive experimental results on publicly accessible datasets demonstrate the superiority of DMSG over state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.07413v1","category":"cs.LG"}
{"created":"2024-06-11 16:15:06","title":"VersiCode: Towards Version-controllable Code Generation","abstract":"Significant research has focused on improving the performance of large language model on code-related tasks due to their practical importance. Although performance is typically evaluated using public benchmark datasets, the existing datasets do not account for the concept of \\emph{version}, which is crucial in professional software development. In this paper, we introduce VersiCode, the first comprehensive dataset designed to assess the ability of large language models to generate verifiable code for specific library versions. VersiCode encompasses 300 libraries across more than 2,000 versions spanning 9 years. We design two dedicated evaluation tasks: version-specific code completion (VSCC) and version-aware code editing (VACE). Comprehensive experiments are conducted to benchmark the performance of LLMs, revealing the challenging nature of these tasks and VersiCode, that even state-of-the-art LLMs struggle to generate version-correct code. This dataset, together with the proposed tasks, sheds light on LLMs' capabilities and limitations in handling version-specific code generation, and opens up an important new area of research for further investigation. The resources can be found at https://github.com/wutong8023/VersiCode.","sentences":["Significant research has focused on improving the performance of large language model on code-related tasks due to their practical importance.","Although performance is typically evaluated using public benchmark datasets, the existing datasets do not account for the concept of \\emph{version}, which is crucial in professional software development.","In this paper, we introduce VersiCode, the first comprehensive dataset designed to assess the ability of large language models to generate verifiable code for specific library versions.","VersiCode encompasses 300 libraries across more than 2,000 versions spanning 9 years.","We design two dedicated evaluation tasks: version-specific code completion (VSCC) and version-aware code editing (VACE).","Comprehensive experiments are conducted to benchmark the performance of LLMs, revealing the challenging nature of these tasks and VersiCode, that even state-of-the-art LLMs struggle to generate version-correct code.","This dataset, together with the proposed tasks, sheds light on LLMs' capabilities and limitations in handling version-specific code generation, and opens up an important new area of research for further investigation.","The resources can be found at https://github.com/wutong8023/VersiCode."],"url":"http://arxiv.org/abs/2406.07411v1","category":"cs.SE"}
{"created":"2024-06-11 16:13:11","title":"ADDOPT: An Additive Manufacturing Optimal Control Framework Demonstrated in Minimizing Layer-Level Thermal Variance in Electron Beam Powder Bed Fusion","abstract":"Additive manufacturing (AM) techniques hold promise but face significant challenges in process planning and optimization. The large temporal and spatial variations in temperature that can occur in layer-wise AM lead to thermal excursions, resulting in property variations and defects. These variations cannot always be fully mitigated by simple static parameter search. To address this challenge, we propose a general approach based on modeling AM processes on the part-scale in state-space and framing AM process planning as a numerical optimal control problem. We demonstrate this approach on the problem of minimizing thermal variation in a given layer in the electron beam powder bed fusion (EB-PBF) AM process, and are able to compute globally optimal dynamic process plans. These optimized process plans are then evaluated in simulation, achieving an 87% and 86% reduction in cumulative variance compared to random spot melting and a uniform power field respectively, and are further validated in experiment. This one-shot feedforward planning approach expands the capabilities of AM technology by minimizing the need for experimentation and iteration to achieve process optimization. Further, this work opens the possibility for the application of optimal control theory to part-scale optimization and control in AM.","sentences":["Additive manufacturing (AM) techniques hold promise but face significant challenges in process planning and optimization.","The large temporal and spatial variations in temperature that can occur in layer-wise AM lead to thermal excursions, resulting in property variations and defects.","These variations cannot always be fully mitigated by simple static parameter search.","To address this challenge, we propose a general approach based on modeling AM processes on the part-scale in state-space and framing AM process planning as a numerical optimal control problem.","We demonstrate this approach on the problem of minimizing thermal variation in a given layer in the electron beam powder bed fusion (EB-PBF) AM process, and are able to compute globally optimal dynamic process plans.","These optimized process plans are then evaluated in simulation, achieving an 87% and 86% reduction in cumulative variance compared to random spot melting and a uniform power field respectively, and are further validated in experiment.","This one-shot feedforward planning approach expands the capabilities of AM technology by minimizing the need for experimentation and iteration to achieve process optimization.","Further, this work opens the possibility for the application of optimal control theory to part-scale optimization and control in AM."],"url":"http://arxiv.org/abs/2406.07408v1","category":"eess.SY"}
{"created":"2024-06-11 16:10:37","title":"Enhancing Tabular Data Optimization with a Flexible Graph-based Reinforced Exploration Strategy","abstract":"Tabular data optimization methods aim to automatically find an optimal feature transformation process that generates high-value features and improves the performance of downstream machine learning tasks. Current frameworks for automated feature transformation rely on iterative sequence generation tasks, optimizing decision strategies through performance feedback from downstream tasks. However, these approaches fail to effectively utilize historical decision-making experiences and overlook potential relationships among generated features, thus limiting the depth of knowledge extraction. Moreover, the granularity of the decision-making process lacks dynamic backtracking capabilities for individual features, leading to insufficient adaptability when encountering inefficient pathways, adversely affecting overall robustness and exploration efficiency. To address the limitations observed in current automatic feature engineering frameworks, we introduce a novel method that utilizes a feature-state transformation graph to effectively preserve the entire feature transformation journey, where each node represents a specific transformation state. During exploration, three cascading agents iteratively select nodes and idea mathematical operations to generate new transformation states. This strategy leverages the inherent properties of the graph structure, allowing for the preservation and reuse of valuable transformations. It also enables backtracking capabilities through graph pruning techniques, which can rectify inefficient transformation paths. To validate the efficacy and flexibility of our approach, we conducted comprehensive experiments and detailed case studies, demonstrating superior performance in diverse scenarios.","sentences":["Tabular data optimization methods aim to automatically find an optimal feature transformation process that generates high-value features and improves the performance of downstream machine learning tasks.","Current frameworks for automated feature transformation rely on iterative sequence generation tasks, optimizing decision strategies through performance feedback from downstream tasks.","However, these approaches fail to effectively utilize historical decision-making experiences and overlook potential relationships among generated features, thus limiting the depth of knowledge extraction.","Moreover, the granularity of the decision-making process lacks dynamic backtracking capabilities for individual features, leading to insufficient adaptability when encountering inefficient pathways, adversely affecting overall robustness and exploration efficiency.","To address the limitations observed in current automatic feature engineering frameworks, we introduce a novel method that utilizes a feature-state transformation graph to effectively preserve the entire feature transformation journey, where each node represents a specific transformation state.","During exploration, three cascading agents iteratively select nodes and idea mathematical operations to generate new transformation states.","This strategy leverages the inherent properties of the graph structure, allowing for the preservation and reuse of valuable transformations.","It also enables backtracking capabilities through graph pruning techniques, which can rectify inefficient transformation paths.","To validate the efficacy and flexibility of our approach, we conducted comprehensive experiments and detailed case studies, demonstrating superior performance in diverse scenarios."],"url":"http://arxiv.org/abs/2406.07404v1","category":"cs.LG"}
{"created":"2024-06-11 16:07:24","title":"Guiding LLM Temporal Logic Generation with Explicit Separation of Data and Control","abstract":"Temporal logics are powerful tools that are widely used for the synthesis and verification of reactive systems. The recent progress on Large Language Models (LLMs) has the potential to make the process of writing such specifications more accessible. However, writing specifications in temporal logics remains challenging for all but the most expert users. A key question in using LLMs for temporal logic specification engineering is to understand what kind of guidance is most helpful to the LLM and the users to easily produce specifications. Looking specifically at the problem of reactive program synthesis, we explore the impact of providing an LLM with guidance on the separation of control and data--making explicit for the LLM what functionality is relevant for the specification, and treating the remaining functionality as an implementation detail for a series of pre-defined functions and predicates. We present a benchmark set and find that this separation of concerns improves specification generation. Our benchmark provides a test set against which to verify future work in LLM generation of temporal logic specifications.","sentences":["Temporal logics are powerful tools that are widely used for the synthesis and verification of reactive systems.","The recent progress on Large Language Models (LLMs) has the potential to make the process of writing such specifications more accessible.","However, writing specifications in temporal logics remains challenging for all but the most expert users.","A key question in using LLMs for temporal logic specification engineering is to understand what kind of guidance is most helpful to the LLM and the users to easily produce specifications.","Looking specifically at the problem of reactive program synthesis, we explore the impact of providing an LLM with guidance on the separation of control and data--making explicit for the LLM","what functionality is relevant for the specification, and treating the remaining functionality as an implementation detail for a series of pre-defined functions and predicates.","We present a benchmark set and find that this separation of concerns improves specification generation.","Our benchmark provides a test set against which to verify future work in LLM generation of temporal logic specifications."],"url":"http://arxiv.org/abs/2406.07400v1","category":"cs.LG"}
{"created":"2024-06-11 16:05:15","title":"Visual Representation Learning with Stochastic Frame Prediction","abstract":"Self-supervised learning of image representations by predicting future frames is a promising direction but still remains a challenge. This is because of the under-determined nature of frame prediction; multiple potential futures can arise from a single current frame. To tackle this challenge, in this paper, we revisit the idea of stochastic video generation that learns to capture uncertainty in frame prediction and explore its effectiveness for representation learning. Specifically, we design a framework that trains a stochastic frame prediction model to learn temporal information between frames. Moreover, to learn dense information within each frame, we introduce an auxiliary masked image modeling objective along with a shared decoder architecture. We find this architecture allows for combining both objectives in a synergistic and compute-efficient manner. We demonstrate the effectiveness of our framework on a variety of tasks from video label propagation and vision-based robot learning domains, such as video segmentation, pose tracking, vision-based robotic locomotion, and manipulation tasks. Code is available on the project webpage: https://sites.google.com/view/2024rsp.","sentences":["Self-supervised learning of image representations by predicting future frames is a promising direction but still remains a challenge.","This is because of the under-determined nature of frame prediction; multiple potential futures can arise from a single current frame.","To tackle this challenge, in this paper, we revisit the idea of stochastic video generation that learns to capture uncertainty in frame prediction and explore its effectiveness for representation learning.","Specifically, we design a framework that trains a stochastic frame prediction model to learn temporal information between frames.","Moreover, to learn dense information within each frame, we introduce an auxiliary masked image modeling objective along with a shared decoder architecture.","We find this architecture allows for combining both objectives in a synergistic and compute-efficient manner.","We demonstrate the effectiveness of our framework on a variety of tasks from video label propagation and vision-based robot learning domains, such as video segmentation, pose tracking, vision-based robotic locomotion, and manipulation tasks.","Code is available on the project webpage: https://sites.google.com/view/2024rsp."],"url":"http://arxiv.org/abs/2406.07398v1","category":"cs.CV"}
{"created":"2024-06-11 16:03:17","title":"Holographic reconstruction of black hole spacetime: machine learning and entanglement entropy","abstract":"We investigate the bulk reconstruction of AdS black hole spacetime emergent from quantum entanglement within a machine learning framework. Utilizing neural ordinary differential equations alongside Monte-Carlo integration, we develop a method tailored for continuous training functions to extract the general isotropic bulk metric from entanglement entropy data. To validate our approach, we first apply our machine learning algorithm to holographic entanglement entropy data derived from the Gubser-Rocha and superconductor models, which serve as representative models of strongly coupled matters in holography. Our algorithm successfully extracts the corresponding bulk metrics from these data. Additionally, we extend our methodology to many-body systems by employing entanglement entropy data from a fermionic tight-binding chain at half filling, exemplifying critical one-dimensional systems, and derive the associated bulk metric. We find that the metrics for a tight-binding chain and the Gubser-Rocha model are similar. We speculate this similarity is due to the metallic property of these models.","sentences":["We investigate the bulk reconstruction of AdS black hole spacetime emergent from quantum entanglement within a machine learning framework.","Utilizing neural ordinary differential equations alongside Monte-Carlo integration, we develop a method tailored for continuous training functions to extract the general isotropic bulk metric from entanglement entropy data.","To validate our approach, we first apply our machine learning algorithm to holographic entanglement entropy data derived from the Gubser-Rocha and superconductor models, which serve as representative models of strongly coupled matters in holography.","Our algorithm successfully extracts the corresponding bulk metrics from these data.","Additionally, we extend our methodology to many-body systems by employing entanglement entropy data from a fermionic tight-binding chain at half filling, exemplifying critical one-dimensional systems, and derive the associated bulk metric.","We find that the metrics for a tight-binding chain and the Gubser-Rocha model are similar.","We speculate this similarity is due to the metallic property of these models."],"url":"http://arxiv.org/abs/2406.07395v1","category":"hep-th"}
{"created":"2024-06-11 16:01:07","title":"Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B","abstract":"This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.","sentences":["This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks.","Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs.","The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance.","Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench.","The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications."],"url":"http://arxiv.org/abs/2406.07394v1","category":"cs.AI"}
{"created":"2024-06-11 15:57:01","title":"DiffCom: Channel Received Signal is a Natural Condition to Guide Diffusion Posterior Sampling","abstract":"End-to-end visual communication systems typically optimize a trade-off between channel bandwidth costs and signal-level distortion metrics. However, under challenging physical conditions, this traditional discriminative communication paradigm often results in unrealistic reconstructions with perceptible blurring and aliasing artifacts, despite the inclusion of perceptual or adversarial losses for optimizing. This issue primarily stems from the receiver's limited knowledge about the underlying data manifold and the use of deterministic decoding mechanisms. To address these limitations, this paper introduces DiffCom, a novel end-to-end generative communication paradigm that utilizes off-the-shelf generative priors and probabilistic diffusion models for decoding, thereby improving perceptual quality without heavily relying on bandwidth costs and received signal quality. Unlike traditional systems that rely on deterministic decoders optimized solely for distortion metrics, our DiffCom leverages raw channel-received signal as a fine-grained condition to guide stochastic posterior sampling. Our approach ensures that reconstructions remain on the manifold of real data with a novel confirming constraint, enhancing the robustness and reliability of the generated outcomes. Furthermore, DiffCom incorporates a blind posterior sampling technique to address scenarios with unknown forward transmission characteristics. Extensive experimental validations demonstrate that DiffCom not only produces realistic reconstructions with details faithful to the original data but also achieves superior robustness against diverse wireless transmission degradations. Collectively, these advancements establish DiffCom as a new benchmark in designing generative communication systems that offer enhanced robustness and generalization superiorities.","sentences":["End-to-end visual communication systems typically optimize a trade-off between channel bandwidth costs and signal-level distortion metrics.","However, under challenging physical conditions, this traditional discriminative communication paradigm often results in unrealistic reconstructions with perceptible blurring and aliasing artifacts, despite the inclusion of perceptual or adversarial losses for optimizing.","This issue primarily stems from the receiver's limited knowledge about the underlying data manifold and the use of deterministic decoding mechanisms.","To address these limitations, this paper introduces DiffCom, a novel end-to-end generative communication paradigm that utilizes off-the-shelf generative priors and probabilistic diffusion models for decoding, thereby improving perceptual quality without heavily relying on bandwidth costs and received signal quality.","Unlike traditional systems that rely on deterministic decoders optimized solely for distortion metrics, our DiffCom leverages raw channel-received signal as a fine-grained condition to guide stochastic posterior sampling.","Our approach ensures that reconstructions remain on the manifold of real data with a novel confirming constraint, enhancing the robustness and reliability of the generated outcomes.","Furthermore, DiffCom incorporates a blind posterior sampling technique to address scenarios with unknown forward transmission characteristics.","Extensive experimental validations demonstrate that DiffCom not only produces realistic reconstructions with details faithful to the original data but also achieves superior robustness against diverse wireless transmission degradations.","Collectively, these advancements establish DiffCom as a new benchmark in designing generative communication systems that offer enhanced robustness and generalization superiorities."],"url":"http://arxiv.org/abs/2406.07390v1","category":"eess.SP"}
{"created":"2024-06-11 15:54:45","title":"Probabilistic models and statistics for electronic financial markets in the digital age","abstract":"The scope of this manuscript is to review some recent developments in statistics for discretely observed semimartingales which are motivated by applications for financial markets. Our journey through this area stops to take closer looks at a few selected topics discussing recent literature. We moreover highlight and explain the important role played by some classical concepts of probability and statistics. We focus on three main aspects: Testing for jumps; rough fractional stochastic volatility; and limit order microstructure noise. We review jump tests based on extreme value theory and complement the literature proposing new statistical methods. They are based on asymptotic theory of order statistics and the R\\'{e}nyi representation. The second stage of our journey visits a recent strand of research showing that volatility is rough. We further investigate this and establish a minimax lower bound exploring frontiers to what extent the regularity of latent volatility can be recovered in a more general framework. Finally, we discuss a stochastic boundary model with one-sided microstructure noise for high-frequency limit order prices and its probabilistic and statistical foundation.","sentences":["The scope of this manuscript is to review some recent developments in statistics for discretely observed semimartingales which are motivated by applications for financial markets.","Our journey through this area stops to take closer looks at a few selected topics discussing recent literature.","We moreover highlight and explain the important role played by some classical concepts of probability and statistics.","We focus on three main aspects: Testing for jumps; rough fractional stochastic volatility; and limit order microstructure noise.","We review jump tests based on extreme value theory and complement the literature proposing new statistical methods.","They are based on asymptotic theory of order statistics and the R\\'{e}nyi representation.","The second stage of our journey visits a recent strand of research showing that volatility is rough.","We further investigate this and establish a minimax lower bound exploring frontiers to what extent the regularity of latent volatility can be recovered in a more general framework.","Finally, we discuss a stochastic boundary model with one-sided microstructure noise for high-frequency limit order prices and its probabilistic and statistical foundation."],"url":"http://arxiv.org/abs/2406.07388v1","category":"q-fin.ST"}
{"created":"2024-06-11 15:53:09","title":"Spin-orbit coupling induced orbital entanglement in a three-band Hubbard model","abstract":"The effect of the spin-orbit coupling on the ground state properties of the square-lattice three-band Hubbard model with a single electron per site is studied by a generalized Hartree-Fock approximation. We calculate the full phase diagram and show that there appear additional orbital-entangled phases brought about by competition of various exchange channels or by the spin-orbit coupling in addition to conventional states stabilized by the Kugel-Khomskii mechanism. One of these phases previously proposed to explain magnetic properties of Sr$_2$VO$_4$ is characterized by vanishing dipolar magnetic moments and antiferro-octupolar ordering. We calculated microscopic parameters for this material and demonstrate that it is located near a phase boundary of two orbital-entangled and two conventional antiferromagnetic ferro-orbital states.","sentences":["The effect of the spin-orbit coupling on the ground state properties of the square-lattice three-band Hubbard model with a single electron per site is studied by a generalized Hartree-Fock approximation.","We calculate the full phase diagram and show that there appear additional orbital-entangled phases brought about by competition of various exchange channels or by the spin-orbit coupling in addition to conventional states stabilized by the Kugel-Khomskii mechanism.","One of these phases previously proposed to explain magnetic properties of Sr$_2$VO$_4$ is characterized by vanishing dipolar magnetic moments and antiferro-octupolar ordering.","We calculated microscopic parameters for this material and demonstrate that it is located near a phase boundary of two orbital-entangled and two conventional antiferromagnetic ferro-orbital states."],"url":"http://arxiv.org/abs/2406.07386v1","category":"cond-mat.str-el"}
{"created":"2024-06-11 15:51:42","title":"Disrupting Bipartite Trading Networks: Matching for Revenue Maximization","abstract":"We model the role of an online platform disrupting a market with unit-demand buyers and unit-supply sellers. Each seller can transact with a subset of the buyers whom she already knows, as well as with any additional buyers to whom she is introduced by the platform. Given these constraints on trade, prices and transactions are induced by a competitive equilibrium. The platform's revenue is proportional to the total price of all trades between platform-introduced buyers and sellers.   In general, we show that the platform's revenue-maximization problem is computationally intractable. We provide structural results for revenue-optimal matchings and isolate special cases in which the platform can efficiently compute them. Furthermore, in a market where the maximum increase in social welfare that the platform can create is $\\Delta W$, we prove that the platform can attain revenue $\\Omega(\\Delta W/\\log(\\min\\{n,m\\}))$, where $n$ and $m$ are the numbers of buyers and sellers, respectively. When $\\Delta W$ is large compared to welfare without the platform, this gives a polynomial-time algorithm that guarantees a logarithmic approximation of the optimal welfare as revenue. We also show that even when the platform optimizes for revenue, the social welfare is at least an $O(\\log(\\min\\{n,m\\}))$-approximation to the optimal welfare. Finally, we prove significantly stronger bounds for revenue and social welfare in homogeneous-goods markets.","sentences":["We model the role of an online platform disrupting a market with unit-demand buyers and unit-supply sellers.","Each seller can transact with a subset of the buyers whom she already knows, as well as with any additional buyers to whom she is introduced by the platform.","Given these constraints on trade, prices and transactions are induced by a competitive equilibrium.","The platform's revenue is proportional to the total price of all trades between platform-introduced buyers and sellers.   ","In general, we show that the platform's revenue-maximization problem is computationally intractable.","We provide structural results for revenue-optimal matchings and isolate special cases in which the platform can efficiently compute them.","Furthermore, in a market where the maximum increase in social welfare that the platform can create is $\\Delta W$, we prove that the platform can attain revenue $\\Omega(\\Delta W/\\log(\\min\\{n,m\\}))$, where $n$ and $m$ are the numbers of buyers and sellers, respectively.","When $\\Delta W$ is large compared to welfare without the platform, this gives a polynomial-time algorithm that guarantees a logarithmic approximation of the optimal welfare as revenue.","We also show that even when the platform optimizes for revenue, the social welfare is at least an $O(\\log(\\min\\{n,m\\}))$-approximation to the optimal welfare.","Finally, we prove significantly stronger bounds for revenue and social welfare in homogeneous-goods markets."],"url":"http://arxiv.org/abs/2406.07385v1","category":"cs.GT"}
{"created":"2024-06-11 15:50:43","title":"Strong Repulsive Lifshitz-van der Waals Forces on Suspended Graphene","abstract":"Understanding surface forces of two-dimensional (2D) materials is of fundamental importance as they govern molecular dynamics and atomic deposition in nanoscale proximity. Despite recent observations in wetting transparency and remote epitaxy on substrate-supported graphene, very little is known about the many-body effects on their van der Waals (vdW) interactions, such as the role of surrounding vacuum in wettability of suspended 2D monolayers. Here we report on a stark repulsive Lifshitz-van der Waals (vdW) force generated at surfaces of suspended 2D materials, arising from quantum fluctuation coupled with the atomic thickness and birefringence of 2D monolayer. In combination with our theoretical framework taking into account the many-body Lifshitz formulism, we present direct measurement of Lifshitz-vdW repulsion on suspended graphene using atomic force microscopy. We report a repulsive force of up to 1.4 kN/m$^2$ at a separation of 8.8 nm between a gold-coated AFM tip and a sheet of suspended graphene, more than two orders of magnitude greater than the Casimir-Lifshitz repulsion demonstrated in fluids. Our findings suggest that suspended 2D materials are intrinsically repulsive surfaces with substantially lowered wettability. The amplified Lifshitz-vdW repulsion could offer technological opportunities such as molecular actuation and controlled atomic assembly.","sentences":["Understanding surface forces of two-dimensional (2D) materials is of fundamental importance as they govern molecular dynamics and atomic deposition in nanoscale proximity.","Despite recent observations in wetting transparency and remote epitaxy on substrate-supported graphene, very little is known about the many-body effects on their van der Waals (vdW) interactions, such as the role of surrounding vacuum in wettability of suspended 2D monolayers.","Here we report on a stark repulsive Lifshitz-van der Waals (vdW) force generated at surfaces of suspended 2D materials, arising from quantum fluctuation coupled with the atomic thickness and birefringence of 2D monolayer.","In combination with our theoretical framework taking into account the many-body Lifshitz formulism, we present direct measurement of Lifshitz-vdW repulsion on suspended graphene using atomic force microscopy.","We report a repulsive force of up to 1.4 kN/m$^2$ at a separation of 8.8 nm between a gold-coated AFM tip and a sheet of suspended graphene, more than two orders of magnitude greater than the Casimir-Lifshitz repulsion demonstrated in fluids.","Our findings suggest that suspended 2D materials are intrinsically repulsive surfaces with substantially lowered wettability.","The amplified Lifshitz-vdW repulsion could offer technological opportunities such as molecular actuation and controlled atomic assembly."],"url":"http://arxiv.org/abs/2406.07384v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-11 15:50:42","title":"Federated Multi-Agent DRL for Radio Resource Management in Industrial 6G in-X subnetworks","abstract":"Recently, 6G in-X subnetworks have been proposed as low-power short-range radio cells to support localized extreme wireless connectivity inside entities such as industrial robots, vehicles, and the human body. Deployment of in-X subnetworks within these entities may result in rapid changes in interference levels and thus, varying link quality. This paper investigates distributed dynamic channel allocation to mitigate inter-subnetwork interference in dense in-factory deployments of 6G in-X subnetworks. This paper introduces two new techniques, Federated Multi-Agent Double Deep Q-Network (F-MADDQN) and Federated Multi-Agent Deep Proximal Policy Optimization (F-MADPPO), for channel allocation in 6G in-X subnetworks. These techniques are based on a client-to-server horizontal federated reinforcement learning framework. The methods require sharing only local model weights with a centralized gNB for federated aggregation thereby preserving local data privacy and security. Simulations were conducted using a practical indoor factory environment proposed by 5G-ACIA and 3GPP models for in-factory environments. The results showed that the proposed methods achieved slightly better performance than baseline schemes with significantly reduced signaling overhead compared to the baseline solutions. The schemes also showed better robustness and generalization ability to changes in deployment densities and propagation parameters.","sentences":["Recently, 6G in-X subnetworks have been proposed as low-power short-range radio cells to support localized extreme wireless connectivity inside entities such as industrial robots, vehicles, and the human body.","Deployment of in-X subnetworks within these entities may result in rapid changes in interference levels and thus, varying link quality.","This paper investigates distributed dynamic channel allocation to mitigate inter-subnetwork interference in dense in-factory deployments of 6G in-X subnetworks.","This paper introduces two new techniques, Federated Multi-Agent Double Deep Q-Network (F-MADDQN) and Federated Multi-Agent Deep Proximal Policy Optimization (F-MADPPO), for channel allocation in 6G in-X subnetworks.","These techniques are based on a client-to-server horizontal federated reinforcement learning framework.","The methods require sharing only local model weights with a centralized gNB for federated aggregation thereby preserving local data privacy and security.","Simulations were conducted using a practical indoor factory environment proposed by 5G-ACIA and 3GPP models for in-factory environments.","The results showed that the proposed methods achieved slightly better performance than baseline schemes with significantly reduced signaling overhead compared to the baseline solutions.","The schemes also showed better robustness and generalization ability to changes in deployment densities and propagation parameters."],"url":"http://arxiv.org/abs/2406.07383v1","category":"eess.SP"}
{"created":"2024-06-11 15:49:08","title":"World Models with Hints of Large Language Models for Goal Achieving","abstract":"Reinforcement learning struggles in the face of long-horizon tasks and sparse goals due to the difficulty in manual reward specification. While existing methods address this by adding intrinsic rewards, they may fail to provide meaningful guidance in long-horizon decision-making tasks with large state and action spaces, lacking purposeful exploration. Inspired by human cognition, we propose a new multi-modal model-based RL approach named Dreaming with Large Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks. By assigning higher intrinsic rewards to samples that align with the hints outlined by the language model during model rollouts, DLLM guides the agent toward meaningful and efficient exploration. Extensive experiments demonstrate that the DLLM outperforms recent methods in various challenging, sparse-reward environments such as HomeGrid, Crafter, and Minecraft by 27.7\\%, 21.1\\%, and 9.9\\%, respectively.","sentences":["Reinforcement learning struggles in the face of long-horizon tasks and sparse goals due to the difficulty in manual reward specification.","While existing methods address this by adding intrinsic rewards, they may fail to provide meaningful guidance in long-horizon decision-making tasks with large state and action spaces, lacking purposeful exploration.","Inspired by human cognition, we propose a new multi-modal model-based RL approach named Dreaming with Large Language Models (DLLM).","DLLM integrates the proposed hinting subgoals from the LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks.","By assigning higher intrinsic rewards to samples that align with the hints outlined by the language model during model rollouts, DLLM guides the agent toward meaningful and efficient exploration.","Extensive experiments demonstrate that the DLLM outperforms recent methods in various challenging, sparse-reward environments such as HomeGrid, Crafter, and Minecraft by 27.7\\%, 21.1\\%, and 9.9\\%, respectively."],"url":"http://arxiv.org/abs/2406.07381v1","category":"cs.AI"}
{"created":"2024-06-11 15:45:24","title":"Large Language Models for Constrained-Based Causal Discovery","abstract":"Causality is essential for understanding complex systems, such as the economy, the brain, and the climate. Constructing causal graphs often relies on either data-driven or expert-driven approaches, both fraught with challenges. The former methods, like the celebrated PC algorithm, face issues with data requirements and assumptions of causal sufficiency, while the latter demand substantial time and domain knowledge. This work explores the capabilities of Large Language Models (LLMs) as an alternative to domain experts for causal graph generation. We frame conditional independence queries as prompts to LLMs and employ the PC algorithm with the answers. The performance of the LLM-based conditional independence oracle on systems with known causal graphs shows a high degree of variability. We improve the performance through a proposed statistical-inspired voting schema that allows some control over false-positive and false-negative rates. Inspecting the chain-of-thought argumentation, we find causal reasoning to justify its answer to a probabilistic query. We show evidence that knowledge-based CIT could eventually become a complementary tool for data-driven causal discovery.","sentences":["Causality is essential for understanding complex systems, such as the economy, the brain, and the climate.","Constructing causal graphs often relies on either data-driven or expert-driven approaches, both fraught with challenges.","The former methods, like the celebrated PC algorithm, face issues with data requirements and assumptions of causal sufficiency, while the latter demand substantial time and domain knowledge.","This work explores the capabilities of Large Language Models (LLMs) as an alternative to domain experts for causal graph generation.","We frame conditional independence queries as prompts to LLMs and employ the PC algorithm with the answers.","The performance of the LLM-based conditional independence oracle on systems with known causal graphs shows a high degree of variability.","We improve the performance through a proposed statistical-inspired voting schema that allows some control over false-positive and false-negative rates.","Inspecting the chain-of-thought argumentation, we find causal reasoning to justify its answer to a probabilistic query.","We show evidence that knowledge-based CIT could eventually become a complementary tool for data-driven causal discovery."],"url":"http://arxiv.org/abs/2406.07378v1","category":"cs.AI"}
{"created":"2024-06-11 15:45:00","title":"COLoRIS: Localization-agnostic Smart Surfaces Enabling Opportunistic ISAC in 6G Networks","abstract":"The integration of Smart Surfaces in 6G communication networks, also dubbed as Reconfigurable Intelligent Surfaces (RISs), is a promising paradigm change gaining significant attention given its disruptive features. RISs are a key enabler in the realm of 6G Integrated Sensing and Communication (ISAC) systems where novel services can be offered together with the future mobile networks communication capabilities. This paper addresses the critical challenge of precisely localizing users within a communication network by leveraging the controlled-reflective properties of RIS elements without relying on more power-hungry traditional methods, e.g., GPS, adverting the need of deploying additional infrastructure and even avoiding interfering with communication efforts. Moreover, we go one step beyond: we build COLoRIS, an Opportunistic ISAC approach that leverages localization agnostic RIS configurations to accurately position mobile users via trained learning models. Extensive experimental validation and simulations in large-scale synthetic scenarios show 5% positioning errors (with respect to field size) under different conditions. Further, we show that a low-complexity version running in a limited off-the-shelf (embedded, low-power) system achieves positioning errors in the 11% range at a negligible +2% energy expense with respect to the classical RIS.","sentences":["The integration of Smart Surfaces in 6G communication networks, also dubbed as Reconfigurable Intelligent Surfaces (RISs), is a promising paradigm change gaining significant attention given its disruptive features.","RISs are a key enabler in the realm of 6G Integrated Sensing and Communication (ISAC) systems where novel services can be offered together with the future mobile networks communication capabilities.","This paper addresses the critical challenge of precisely localizing users within a communication network by leveraging the controlled-reflective properties of RIS elements without relying on more power-hungry traditional methods, e.g., GPS, adverting the need of deploying additional infrastructure and even avoiding interfering with communication efforts.","Moreover, we go one step beyond: we build COLoRIS, an Opportunistic ISAC approach that leverages localization agnostic RIS configurations to accurately position mobile users via trained learning models.","Extensive experimental validation and simulations in large-scale synthetic scenarios show 5% positioning errors (with respect to field size) under different conditions.","Further, we show that a low-complexity version running in a limited off-the-shelf (embedded, low-power) system achieves positioning errors in the 11% range at a negligible +2% energy expense with respect to the classical RIS."],"url":"http://arxiv.org/abs/2406.07377v1","category":"cs.NI"}
{"created":"2024-06-11 15:34:43","title":"When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models","abstract":"Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.","sentences":["Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation.","While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain.","We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding.","We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.","Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs.","Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods.","Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM."],"url":"http://arxiv.org/abs/2406.07368v1","category":"cs.CL"}
{"created":"2024-06-11 15:33:51","title":"A Positivstellensatz on the Matrix Algebra of Finitely Generated Free Group","abstract":"Positivstellens{\\\"a}tze are a group of theorems on the positivity of involution algebras over $\\mathbb{R}$ or $\\mathbb{C}$. One of the most well-known Positivstellensatz is the solution to Hilbert's 17th problem given by E. Artin, which asserts that a real polynomial in $n$ commutative variables is nonnegative on real affine space if and only if it is a sum of fractional squares. Let $m$ and $n$ be two positive integers. For the free group $F_n$ generated by $n$ letters, and a symmetric polynomial $b$ with variables in $F_n$ and with $n$-by-$n$ complex matrices coefficients, we use real algebraic geometry to give a new proof showing that $b$ is a sum of Hermitian squares if and only if $b$ is mapped to a positive semidefinite matrix under any finitely dimensional unitary representation of $F_n$.","sentences":["Positivstellens{\\\"a}tze are a group of theorems on the positivity of involution algebras over $\\mathbb{R}$ or $\\mathbb{C}$. One of the most well-known Positivstellensatz is the solution to Hilbert's 17th problem given by E. Artin, which asserts that a real polynomial in $n$ commutative variables is nonnegative on real affine space if and only if it is a sum of fractional squares.","Let $m$ and $n$ be two positive integers.","For the free group $F_n$ generated by $n$ letters, and a symmetric polynomial $b$ with variables in $F_n$ and with $n$-by-$n$ complex matrices coefficients, we use real algebraic geometry to give a new proof showing that $b$ is a sum of Hermitian squares if and only if $b$ is mapped to a positive semidefinite matrix under any finitely dimensional unitary representation of $F_n$."],"url":"http://arxiv.org/abs/2406.07367v1","category":"math.RT"}
{"created":"2024-06-11 15:32:39","title":"Fast and accurate evaluation of Biot-Savart integrals over spatial curves","abstract":"The Biot-Savart law is relevant in physical contexts including electromagnetism and fluid dynamics. In the latter case, when the rotation of a fluid is confined to a set of very thin vortex filaments, this law describes the velocity field induced by the spatial arrangement of these objects. The Biot-Savart law is at the core of vortex methods used in the simulation of classical and quantum fluid flows. Naive methods are inefficient when dealing with large numbers of vortex elements, which makes them inadequate for simulating turbulent vortex flows. Here we exploit a direct analogy between the Biot-Savart law and electrostatics to adapt Ewald summation methods, routinely used in molecular dynamics simulations, to vortex filament simulations in three-dimensional periodic domains. In this context, the basic idea is to split the induced velocity onto (i) a coarse-grained velocity generated by a Gaussian-filtered vorticity field, and (ii) a short-range correction accounting for near-singular behaviour near the vortices. The former component can be accurately and efficiently evaluated using the nonuniform fast Fourier transform algorithm. Analytical accuracy estimates are provided as a function of the parameters entering the method. We also discuss how to properly account for the finite vortex core size in kinetic energy estimations. Using numerical experiments, we verify the accuracy and the conservation properties of the proposed approach. Moreover, we demonstrate the $O(N \\log N)$ complexity of the method over a wide range of problem sizes $N$, considerably better than the $O(N^2)$ cost of a naive approach.","sentences":["The Biot-Savart law is relevant in physical contexts including electromagnetism and fluid dynamics.","In the latter case, when the rotation of a fluid is confined to a set of very thin vortex filaments, this law describes the velocity field induced by the spatial arrangement of these objects.","The Biot-Savart law is at the core of vortex methods used in the simulation of classical and quantum fluid flows.","Naive methods are inefficient when dealing with large numbers of vortex elements, which makes them inadequate for simulating turbulent vortex flows.","Here we exploit a direct analogy between the Biot-Savart law and electrostatics to adapt Ewald summation methods, routinely used in molecular dynamics simulations, to vortex filament simulations in three-dimensional periodic domains.","In this context, the basic idea is to split the induced velocity onto (i) a coarse-grained velocity generated by a Gaussian-filtered vorticity field, and (ii) a short-range correction accounting for near-singular behaviour near the vortices.","The former component can be accurately and efficiently evaluated using the nonuniform fast Fourier transform algorithm.","Analytical accuracy estimates are provided as a function of the parameters entering the method.","We also discuss how to properly account for the finite vortex core size in kinetic energy estimations.","Using numerical experiments, we verify the accuracy and the conservation properties of the proposed approach.","Moreover, we demonstrate the $O(N \\log N)$ complexity of the method over a wide range of problem sizes $N$, considerably better than the $O(N^2)$ cost of a naive approach."],"url":"http://arxiv.org/abs/2406.07366v1","category":"physics.comp-ph"}
{"created":"2024-06-11 15:32:32","title":"BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction","abstract":"Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity. In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model. Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications. Therefore, we first construct a few-shot ASQP dataset (FSQP) that contains richer categories and is more balanced for the few-shot study. Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence. However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates. To tackle this issue, we further propose a Broadview Soft Prompting (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates. Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen-Shannon divergence. BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates. Then, we aggregate the results of multi-templates by voting mechanism. Empirical results demonstrate that BvSP significantly outperforms the stateof-the-art methods under four few-shot settings and other public datasets. Our code and dataset are available at https://github.com/byinhao/BvSP.","sentences":["Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity.","In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model.","Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications.","Therefore, we first construct a few-shot ASQP dataset (FSQP) that contains richer categories and is more balanced for the few-shot study.","Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence.","However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates.","To tackle this issue, we further propose a Broadview Soft Prompting (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates.","Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen-Shannon divergence.","BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates.","Then, we aggregate the results of multi-templates by voting mechanism.","Empirical results demonstrate that BvSP significantly outperforms the stateof-the-art methods under four few-shot settings and other public datasets.","Our code and dataset are available at https://github.com/byinhao/BvSP."],"url":"http://arxiv.org/abs/2406.07365v1","category":"cs.CL"}
{"created":"2024-06-11 15:28:58","title":"AI.vs.Clinician: Unveiling Intricate Interactions Between AI and Clinicians through an Open-Access Database","abstract":"Artificial Intelligence (AI) plays a crucial role in medical field and has the potential to revolutionize healthcare practices. However, the success of AI models and their impacts hinge on the synergy between AI and medical specialists, with clinicians assuming a dominant role. Unfortunately, the intricate dynamics and interactions between AI and clinicians remain undiscovered and thus hinder AI from being translated into medical practice. To address this gap, we have curated a groundbreaking database called AI.vs.Clinician. This database is the first of its kind for studying the interactions between AI and clinicians. It derives from 7,500 collaborative diagnosis records on a life-threatening medical emergency -- Sepsis -- from 14 medical centers across China. For the patient cohorts well-chosen from MIMIC databases, the AI-related information comprises the model property, feature input, diagnosis decision, and inferred probabilities of sepsis onset presently and within next three hours. The clinician-related information includes the viewed examination data and sequence, viewed time, preliminary and final diagnosis decisions with or without AI assistance, and recommended treatment.","sentences":["Artificial Intelligence (AI) plays a crucial role in medical field and has the potential to revolutionize healthcare practices.","However, the success of AI models and their impacts hinge on the synergy between AI and medical specialists, with clinicians assuming a dominant role.","Unfortunately, the intricate dynamics and interactions between AI and clinicians remain undiscovered and thus hinder AI from being translated into medical practice.","To address this gap, we have curated a groundbreaking database called AI.vs.Clinician.","This database is the first of its kind for studying the interactions between AI and clinicians.","It derives from 7,500 collaborative diagnosis records on a life-threatening medical emergency -- Sepsis -- from 14 medical centers across China.","For the patient cohorts well-chosen from MIMIC databases, the AI-related information comprises the model property, feature input, diagnosis decision, and inferred probabilities of sepsis onset presently and within next three hours.","The clinician-related information includes the viewed examination data and sequence, viewed time, preliminary and final diagnosis decisions with or without AI assistance, and recommended treatment."],"url":"http://arxiv.org/abs/2406.07362v1","category":"cs.HC"}
{"created":"2024-06-11 15:27:01","title":"GLIMPSE: Pragmatically Informative Multi-Document Summarization for Scholarly Reviews","abstract":"Scientific peer review is essential for the quality of academic publications. However, the increasing number of paper submissions to conferences has strained the reviewing process. This surge poses a burden on area chairs who have to carefully read an ever-growing volume of reviews and discern each reviewer's main arguments as part of their decision process. In this paper, we introduce \\sys, a summarization method designed to offer a concise yet comprehensive overview of scholarly reviews. Unlike traditional consensus-based methods, \\sys extracts both common and unique opinions from the reviews. We introduce novel uniqueness scores based on the Rational Speech Act framework to identify relevant sentences in the reviews. Our method aims to provide a pragmatic glimpse into all reviews, offering a balanced perspective on their opinions. Our experimental results with both automatic metrics and human evaluation show that \\sys generates more discriminative summaries than baseline methods in terms of human evaluation while achieving comparable performance with these methods in terms of automatic metrics.","sentences":["Scientific peer review is essential for the quality of academic publications.","However, the increasing number of paper submissions to conferences has strained the reviewing process.","This surge poses a burden on area chairs who have to carefully read an ever-growing volume of reviews and discern each reviewer's main arguments as part of their decision process.","In this paper, we introduce \\sys, a summarization method designed to offer a concise yet comprehensive overview of scholarly reviews.","Unlike traditional consensus-based methods, \\sys extracts both common and unique opinions from the reviews.","We introduce novel uniqueness scores based on the Rational Speech Act framework to identify relevant sentences in the reviews.","Our method aims to provide a pragmatic glimpse into all reviews, offering a balanced perspective on their opinions.","Our experimental results with both automatic metrics and human evaluation show that \\sys generates more discriminative summaries than baseline methods in terms of human evaluation while achieving comparable performance with these methods in terms of automatic metrics."],"url":"http://arxiv.org/abs/2406.07359v1","category":"cs.CL"}
{"created":"2024-06-11 15:26:57","title":"AI Sandbagging: Language Models can Strategically Underperform on Evaluations","abstract":"Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of sandbagging $\\unicode{x2013}$ which we define as \"strategic underperformance on an evaluation\". In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted, or password-locked, to target specific scores on a capability evaluation. Even more, we found that a capable password-locked model (Llama 3 70b) is reasonably able to emulate a less capable model (Llama 2 7b). Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.","sentences":["Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation.","However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability.","These conflicting interests lead to the problem of sandbagging $\\unicode{x2013}$ which we define as \"strategic underperformance on an evaluation\".","In this paper we assess sandbagging capabilities in contemporary language models (LMs).","We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations.","Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password.","This behaviour generalizes to high-quality, held-out benchmarks such as WMDP.","In addition, we show that both frontier and smaller models can be prompted, or password-locked, to target specific scores on a capability evaluation.","Even more, we found that a capable password-locked model (Llama 3 70b) is reasonably able to emulate a less capable model (Llama 2 7b).","Overall, our results suggest that capability evaluations are vulnerable to sandbagging.","This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems."],"url":"http://arxiv.org/abs/2406.07358v1","category":"cs.AI"}
{"created":"2024-06-11 15:25:03","title":"The Theory of Intrinsic Time: A Primer","abstract":"The concept of time mostly plays a subordinate role in finance and economics. The assumption is that time flows continuously and that time series data should be analyzed at regular, equidistant intervals. Nonetheless, already nearly 60 years ago, the concept of an event-based measure of time was first introduced. This paper expands on this theme by discussing the paradigm of intrinsic time, its origins, history, and modern applications. Departing from traditional, continuous measures of time, intrinsic time proposes an event-based, algorithmic framework that captures the dynamic and fluctuating nature of real-world phenomena more accurately. Unsuspected implications arise in general for complex systems and specifically for financial markets. For instance, novel structures and regularities are revealed, otherwise obscured by any analysis utilizing equidistant time intervals. Of particular interest is the emergence of a multiplicity of scaling laws, a hallmark signature of an underlying organizational principle in complex systems. Moreover, a central insight from this novel paradigm is the realization that universal time does not exist; instead, time is observer-dependent, shaped by the intrinsic activity unfolding within complex systems. This research opens up new avenues for economic modeling and forecasting, paving the way for a deeper understanding of the invisible forces that guide the evolution and emergence of market dynamics and financial systems. An exciting and rich landscape of possibilities emerges within the paradigm of intrinsic time.","sentences":["The concept of time mostly plays a subordinate role in finance and economics.","The assumption is that time flows continuously and that time series data should be analyzed at regular, equidistant intervals.","Nonetheless, already nearly 60 years ago, the concept of an event-based measure of time was first introduced.","This paper expands on this theme by discussing the paradigm of intrinsic time, its origins, history, and modern applications.","Departing from traditional, continuous measures of time, intrinsic time proposes an event-based, algorithmic framework that captures the dynamic and fluctuating nature of real-world phenomena more accurately.","Unsuspected implications arise in general for complex systems and specifically for financial markets.","For instance, novel structures and regularities are revealed, otherwise obscured by any analysis utilizing equidistant time intervals.","Of particular interest is the emergence of a multiplicity of scaling laws, a hallmark signature of an underlying organizational principle in complex systems.","Moreover, a central insight from this novel paradigm is the realization that universal time does not exist; instead, time is observer-dependent, shaped by the intrinsic activity unfolding within complex systems.","This research opens up new avenues for economic modeling and forecasting, paving the way for a deeper understanding of the invisible forces that guide the evolution and emergence of market dynamics and financial systems.","An exciting and rich landscape of possibilities emerges within the paradigm of intrinsic time."],"url":"http://arxiv.org/abs/2406.07354v1","category":"q-fin.ST"}
{"created":"2024-06-11 15:22:48","title":"Toxic Memes: A Survey of Computational Perspectives on the Detection and Explanation of Meme Toxicities","abstract":"Internet memes, channels for humor, social commentary, and cultural expression, are increasingly used to spread toxic messages. Studies on the computational analyses of toxic memes have significantly grown over the past five years, and the only three surveys on computational toxic meme analysis cover only work published until 2022, leading to inconsistent terminology and unexplored trends. Our work fills this gap by surveying content-based computational perspectives on toxic memes, and reviewing key developments until early 2024. Employing the PRISMA methodology, we systematically extend the previously considered papers, achieving a threefold result. First, we survey 119 new papers, analyzing 158 computational works focused on content-based toxic meme analysis. We identify over 30 datasets used in toxic meme analysis and examine their labeling systems. Second, after observing the existence of unclear definitions of meme toxicity in computational works, we introduce a new taxonomy for categorizing meme toxicity types. We also note an expansion in computational tasks beyond the simple binary classification of memes as toxic or non-toxic, indicating a shift towards achieving a nuanced comprehension of toxicity. Third, we identify three content-based dimensions of meme toxicity under automatic study: target, intent, and conveyance tactics. We develop a framework illustrating the relationships between these dimensions and meme toxicities. The survey analyzes key challenges and recent trends, such as enhanced cross-modal reasoning, integrating expert and cultural knowledge, the demand for automatic toxicity explanations, and handling meme toxicity in low-resource languages. Also, it notes the rising use of Large Language Models (LLMs) and generative AI for detecting and generating toxic memes. Finally, it proposes pathways for advancing toxic meme detection and interpretation.","sentences":["Internet memes, channels for humor, social commentary, and cultural expression, are increasingly used to spread toxic messages.","Studies on the computational analyses of toxic memes have significantly grown over the past five years, and the only three surveys on computational toxic meme analysis cover only work published until 2022, leading to inconsistent terminology and unexplored trends.","Our work fills this gap by surveying content-based computational perspectives on toxic memes, and reviewing key developments until early 2024.","Employing the PRISMA methodology, we systematically extend the previously considered papers, achieving a threefold result.","First, we survey 119 new papers, analyzing 158 computational works focused on content-based toxic meme analysis.","We identify over 30 datasets used in toxic meme analysis and examine their labeling systems.","Second, after observing the existence of unclear definitions of meme toxicity in computational works, we introduce a new taxonomy for categorizing meme toxicity types.","We also note an expansion in computational tasks beyond the simple binary classification of memes as toxic or non-toxic, indicating a shift towards achieving a nuanced comprehension of toxicity.","Third, we identify three content-based dimensions of meme toxicity under automatic study: target, intent, and conveyance tactics.","We develop a framework illustrating the relationships between these dimensions and meme toxicities.","The survey analyzes key challenges and recent trends, such as enhanced cross-modal reasoning, integrating expert and cultural knowledge, the demand for automatic toxicity explanations, and handling meme toxicity in low-resource languages.","Also, it notes the rising use of Large Language Models (LLMs) and generative AI for detecting and generating toxic memes.","Finally, it proposes pathways for advancing toxic meme detection and interpretation."],"url":"http://arxiv.org/abs/2406.07353v1","category":"cs.CL"}
{"created":"2024-06-11 15:21:01","title":"Stochastic Analysis of Homogeneous Wireless Networks Assisted by Intelligent Reflecting Surfaces","abstract":"In this paper, we study the impact of the existence of multiple IRSs in a homogeneous wireless network, in which all BSs, users (U), and IRSs are spatially distributed by an independent homogeneous PPP, with density $\\lambda_{{\\rm BS}}\\rm{[BS/m^2]}$, $\\lambda_{{\\rm U}}\\rm{[U/m^2]}$, and $\\lambda_{{\\rm IRS}}\\rm{[IRS/m^2]}$, respectively. We utilize a uniformly random serving strategy for BS and IRS to create stochastic symmetry in the network. We analyze the performance of the network and study the effect of the existence of the IRS on the network performance. To this end, for a typical user in the system, we derive analytical upper and lower bounds on the expectation of the power (second statistical moment) of the desired signal and the interference caused by BSs and other users. After that, we obtain analytical upper bounds on the decay of the probability of the power of the desired signal and the interference for the typical user (which results in a lower bound for the cumulative distribution function (CDF)). Moreover, we derive upper bounds on the decay of the probability of the capacity of one typical user, which results in a lower bound for the outage probability. In the numerical results, we observe that the numerical calculation of the power of the desired signal and the interference is near the derived lower bounds and we show that the increment of the parameter ${(\\lambda_{\\rm IRS})}$ causes increment in powers of both the desired and interference signals. We also observe that the increment of the parameter ${\\lambda_{\\rm IRS}}$ causes the decrement of outage probability.","sentences":["In this paper, we study the impact of the existence of multiple IRSs in a homogeneous wireless network, in which all BSs, users (U), and IRSs are spatially distributed by an independent homogeneous PPP, with density $\\lambda_{{\\rm BS}}\\rm{[BS/m^2]}$, $\\lambda_{{\\rm U}}\\rm{[U/m^2]}$, and $\\lambda_{{\\rm IRS}}\\rm{[IRS/m^2]}$, respectively.","We utilize a uniformly random serving strategy for BS and IRS to create stochastic symmetry in the network.","We analyze the performance of the network and study the effect of the existence of the IRS on the network performance.","To this end, for a typical user in the system, we derive analytical upper and lower bounds on the expectation of the power (second statistical moment) of the desired signal and the interference caused by BSs and other users.","After that, we obtain analytical upper bounds on the decay of the probability of the power of the desired signal and the interference for the typical user (which results in a lower bound for the cumulative distribution function (CDF)).","Moreover, we derive upper bounds on the decay of the probability of the capacity of one typical user, which results in a lower bound for the outage probability.","In the numerical results, we observe that the numerical calculation of the power of the desired signal and the interference is near the derived lower bounds and we show that the increment of the parameter ${(\\lambda_{\\rm IRS})}$ causes increment in powers of both the desired and interference signals.","We also observe that the increment of the parameter ${\\lambda_{\\rm IRS}}$ causes the decrement of outage probability."],"url":"http://arxiv.org/abs/2406.07352v1","category":"cs.IT"}
{"created":"2024-06-11 15:17:52","title":"Finite $W$-algebra invariants via Lax type operators","abstract":"We use variations on Lax type operators to find explicit formulas for certain elements of finite $W$-algebras. These give a complete set of generators for all finite $W$-algebras of types B,C,D for which the Dynkin grading is even.","sentences":["We use variations on Lax type operators to find explicit formulas for certain elements of finite $W$-algebras.","These give a complete set of generators for all finite $W$-algebras of types B,C,D for which the Dynkin grading is even."],"url":"http://arxiv.org/abs/2406.07350v1","category":"math.RT"}
{"created":"2024-06-11 15:16:05","title":"Erasing Radio Frequency Fingerprinting via Active Adversarial Perturbation","abstract":"Radio Frequency (RF) fingerprinting is to identify a wireless device from its uniqueness of the analog circuitry or hardware imperfections. However, unlike the MAC address which can be modified, such hardware feature is inevitable for the signal emitted to air, which can possibly reveal device whereabouts, e.g., a sniffer can use a pre-trained model to identify a nearby device when receiving its signal. Such fingerprint may expose critical private information, e.g., the associated upper-layer applications or the end-user. In this paper, we propose to erase such RF feature for wireless devices, which can prevent fingerprinting by actively perturbation from the signal perspective. Specifically, we consider a common RF fingerprinting scenario, where machine learning models are trained from pilot signal data for identification. A novel adversarial attack solution is designed to generate proper perturbations, whereby the perturbed pilot signal can hide the hardware feature and misclassify the model. We theoretically show that the perturbation would not affect the communication function within a tolerable perturbation threshold. We also implement the pilot signal fingerprinting and the proposed perturbation process in a practical LTE system. Extensive experiment results demonstrate that the RF fingerprints can be effectively erased to protect the user privacy.","sentences":["Radio Frequency (RF) fingerprinting is to identify a wireless device from its uniqueness of the analog circuitry or hardware imperfections.","However, unlike the MAC address which can be modified, such hardware feature is inevitable for the signal emitted to air, which can possibly reveal device whereabouts, e.g., a sniffer can use a pre-trained model to identify a nearby device when receiving its signal.","Such fingerprint may expose critical private information, e.g., the associated upper-layer applications or the end-user.","In this paper, we propose to erase such RF feature for wireless devices, which can prevent fingerprinting by actively perturbation from the signal perspective.","Specifically, we consider a common RF fingerprinting scenario, where machine learning models are trained from pilot signal data for identification.","A novel adversarial attack solution is designed to generate proper perturbations, whereby the perturbed pilot signal can hide the hardware feature and misclassify the model.","We theoretically show that the perturbation would not affect the communication function within a tolerable perturbation threshold.","We also implement the pilot signal fingerprinting and the proposed perturbation process in a practical LTE system.","Extensive experiment results demonstrate that the RF fingerprints can be effectively erased to protect the user privacy."],"url":"http://arxiv.org/abs/2406.07349v1","category":"cs.CR"}
{"created":"2024-06-11 15:15:33","title":"DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering","abstract":"Retrieval-Augmented Generation (RAG) has significantly demonstrated the performance of Large Language Models (LLMs) in the knowledge-intensive tasks, such as Question-Answering (QA). RAG expands the query context by incorporating external knowledge bases to enhance the response accuracy. However, it would be inefficient to access LLMs multiple times for each query and unreliable to retrieve all the relevant documents by a single query. We find that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query. To mine the relevance, a two-stage retrieval framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is proposed to improve document retrieval recall and the accuracy of answers while maintaining efficiency. Also, a small classifier is applied to two different selection strategies to determine the contribution of the retrieved documents to answering the query and retrieve the relatively relevant documents. Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment. The experimental results on multi-hop QA datasets show that DR-RAG can significantly improve the accuracy of the answers and achieve new progress in QA systems.","sentences":["Retrieval-Augmented Generation (RAG) has significantly demonstrated the performance of Large Language Models (LLMs) in the knowledge-intensive tasks, such as Question-Answering (QA).","RAG expands the query context by incorporating external knowledge bases to enhance the response accuracy.","However, it would be inefficient to access LLMs multiple times for each query and unreliable to retrieve all the relevant documents by a single query.","We find that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query.","To mine the relevance, a two-stage retrieval framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is proposed to improve document retrieval recall and the accuracy of answers while maintaining efficiency.","Also, a small classifier is applied to two different selection strategies to determine the contribution of the retrieved documents to answering the query and retrieve the relatively relevant documents.","Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment.","The experimental results on multi-hop QA datasets show that DR-RAG can significantly improve the accuracy of the answers and achieve new progress in QA systems."],"url":"http://arxiv.org/abs/2406.07348v1","category":"cs.LG"}
{"created":"2024-06-11 15:14:21","title":"Few-Body Quantum Chaos, Localization, and Multi-Photon Entanglement in Optical Synthetic Frequency Dimension","abstract":"Generation and control of entanglement are fundamental tasks in quantum information processing. In this paper, we propose a novel approach to generate controllable frequency-entangled photons by using the concept of synthetic frequency dimension in an optical system. Such a system consists of a ring resonator made by a tailored third-order nonlinear media to induce photon-photon interactions and a periodic modulator to manipulate coupling between different frequency modes. We show this system provides a unique platform for the exploration of distinct few- or many-body quantum phases including chaos, localization, and integrability in a highly integrable photonics platform. In particular, we develop the potential experimental method to calculate the spectral form factor, which characterizes the degree of chaos in the system and differentiates between these phases based on observable measurements. Interestingly, the transition signatures of each phase can lead to an efficient generation of frequency-entangled multi photons. This work is the first to explore rich and controllable quantum phases beyond single particle in a synthetic dimension.","sentences":["Generation and control of entanglement are fundamental tasks in quantum information processing.","In this paper, we propose a novel approach to generate controllable frequency-entangled photons by using the concept of synthetic frequency dimension in an optical system.","Such a system consists of a ring resonator made by a tailored third-order nonlinear media to induce photon-photon interactions and a periodic modulator to manipulate coupling between different frequency modes.","We show this system provides a unique platform for the exploration of distinct few- or many-body quantum phases including chaos, localization, and integrability in a highly integrable photonics platform.","In particular, we develop the potential experimental method to calculate the spectral form factor, which characterizes the degree of chaos in the system and differentiates between these phases based on observable measurements.","Interestingly, the transition signatures of each phase can lead to an efficient generation of frequency-entangled multi photons.","This work is the first to explore rich and controllable quantum phases beyond single particle in a synthetic dimension."],"url":"http://arxiv.org/abs/2406.07346v1","category":"quant-ph"}
{"created":"2024-06-11 15:09:21","title":"Targeting spectroscopic accuracy for dispersion bound systems from ab initio techniques: translational eigenstates of Ne@C$_{70}$","abstract":"We investigate the endofullerene system Ne@C$_{70}$, by constructing a three-dimensional Potential Energy Surface (PES) describing the translational motion of the Ne atom. We compare a plethora of electronic structure methods including: MP2, SCS-MP2, SOS-MP2, RPA@PBE, C(HF)-RPA, which were previously used for He@C$_{60}$ in J. Chem. Phys. 160, 104303 (2024), alongside B86bPBE-25X-XDM and B86bPBE-50X-XDM. The reduction in symmetry moving from C$_{60}$ to C$_{70}$ introduces a double well potential along the anisotropic direction, which forms a test of the sensitivity and effectiveness of the methods. Due to the large cost of these calculations, the PES is interpolated using Gaussian Process Regression due to its effectiveness with sparse training data. The nuclear Hamiltonian is diagonalised using a symmetrised double minimum basis set outlined in J. Chem. Phys. 159, 164308 (2023), with translational energies having error bars $\\pm 1$ and $\\pm 2$ cm$^{-1}$. We quantify the shape of the ground state wavefunction by considering its prolateness and kurtosis, and compare the eigenfunctions between electronic structure methods from their Hellinger distance. We find no consistency between electronic structure methods as they find a range of barrier heights and minima positions of the double well, and different translational eigenspectra which also differ from the Lennard-Jones (LJ) PES given in J. Chem. Phys. 101, 2126,2140 (1994). We find that generating effective LJ parameters for each electronic structure method cannot reproduce the full PES, nor recreate the eigenstates and this suggests that the LJ form of the PES, while simple, may not be best suited to describe these systems. Even though MP2 and RPA@PBE performed best for He@C$_{60}$, due to the lack of concordance between all electronic structure methods we require more experimental data in order to properly validate the choice.","sentences":["We investigate the endofullerene system Ne@C$_{70}$, by constructing a three-dimensional Potential Energy Surface (PES) describing the translational motion of the Ne atom.","We compare a plethora of electronic structure methods including: MP2, SCS-MP2, SOS-MP2, RPA@PBE, C(HF)-RPA, which were previously used for He@C$_{60}$ in J. Chem.","Phys. 160, 104303 (2024), alongside B86bPBE-25X-XDM and B86bPBE-50X-XDM.","The reduction in symmetry moving from C$_{60}$ to C$_{70}$ introduces a double well potential along the anisotropic direction, which forms a test of the sensitivity and effectiveness of the methods.","Due to the large cost of these calculations, the PES is interpolated using Gaussian Process Regression due to its effectiveness with sparse training data.","The nuclear Hamiltonian is diagonalised using a symmetrised double minimum basis set outlined in J. Chem.","Phys. 159, 164308 (2023), with translational energies having error bars $\\pm 1$ and $\\pm 2$ cm$^{-1}$. We quantify the shape of the ground state wavefunction by considering its prolateness and kurtosis, and compare the eigenfunctions between electronic structure methods from their Hellinger distance.","We find no consistency between electronic structure methods as they find a range of barrier heights and minima positions of the double well, and different translational eigenspectra which also differ from the Lennard-Jones (LJ) PES given in J. Chem.","Phys. 101, 2126,2140 (1994).","We find that generating effective LJ parameters for each electronic structure method cannot reproduce the full PES, nor recreate the eigenstates and this suggests that the LJ form of the PES, while simple, may not be best suited to describe these systems.","Even though MP2 and RPA@PBE performed best for He@C$_{60}$, due to the lack of concordance between all electronic structure methods we require more experimental data in order to properly validate the choice."],"url":"http://arxiv.org/abs/2406.07343v1","category":"physics.chem-ph"}
{"created":"2024-06-11 15:08:14","title":"EdgeTimer: Adaptive Multi-Timescale Scheduling in Mobile Edge Computing with Deep Reinforcement Learning","abstract":"In mobile edge computing (MEC), resource scheduling is crucial to task requests' performance and service providers' cost, involving multi-layer heterogeneous scheduling decisions. Existing schedulers typically adopt static timescales to regularly update scheduling decisions of each layer, without adaptive adjustment of timescales for different layers, resulting in potentially poor performance in practice.   We notice that the adaptive timescales would significantly improve the trade-off between the operation cost and delay performance. Based on this insight, we propose EdgeTimer, the first work to automatically generate adaptive timescales to update multi-layer scheduling decisions using deep reinforcement learning (DRL). First, EdgeTimer uses a three-layer hierarchical DRL framework to decouple the multi-layer decision-making task into a hierarchy of independent sub-tasks for improving learning efficiency. Second, to cope with each sub-task, EdgeTimer adopts a safe multi-agent DRL algorithm for decentralized scheduling while ensuring system reliability. We apply EdgeTimer to a wide range of Kubernetes scheduling rules, and evaluate it using production traces with different workload patterns. Extensive trace-driven experiments demonstrate that EdgeTimer can learn adaptive timescales, irrespective of workload patterns and built-in scheduling rules. It obtains up to 9.1x more profit than existing approaches without sacrificing the delay performance.","sentences":["In mobile edge computing (MEC), resource scheduling is crucial to task requests' performance and service providers' cost, involving multi-layer heterogeneous scheduling decisions.","Existing schedulers typically adopt static timescales to regularly update scheduling decisions of each layer, without adaptive adjustment of timescales for different layers, resulting in potentially poor performance in practice.   ","We notice that the adaptive timescales would significantly improve the trade-off between the operation cost and delay performance.","Based on this insight, we propose EdgeTimer, the first work to automatically generate adaptive timescales to update multi-layer scheduling decisions using deep reinforcement learning (DRL).","First, EdgeTimer uses a three-layer hierarchical DRL framework to decouple the multi-layer decision-making task into a hierarchy of independent sub-tasks for improving learning efficiency.","Second, to cope with each sub-task, EdgeTimer adopts a safe multi-agent DRL algorithm for decentralized scheduling while ensuring system reliability.","We apply EdgeTimer to a wide range of Kubernetes scheduling rules, and evaluate it using production traces with different workload patterns.","Extensive trace-driven experiments demonstrate that EdgeTimer can learn adaptive timescales, irrespective of workload patterns and built-in scheduling rules.","It obtains up to 9.1x more profit than existing approaches without sacrificing the delay performance."],"url":"http://arxiv.org/abs/2406.07342v1","category":"cs.NI"}
{"created":"2024-06-11 15:07:08","title":"Formally Verified Approximate Policy Iteration","abstract":"We formally verify an algorithm for approximate policy iteration on Factored Markov Decision Processes using the interactive theorem prover Isabelle/HOL. Next, we show how the formalized algorithm can be refined to an executable, verified implementation. The implementation is evaluated on benchmark problems to show its practicability. As part of the refinement, we develop verified software to certify Linear Programming solutions. The algorithm builds on a diverse library of formalized mathematics and pushes existing methodologies for interactive theorem provers to the limits. We discuss the process of the verification project and the modifications to the algorithm needed for formal verification.","sentences":["We formally verify an algorithm for approximate policy iteration on Factored Markov Decision Processes using the interactive theorem prover Isabelle/HOL.","Next, we show how the formalized algorithm can be refined to an executable, verified implementation.","The implementation is evaluated on benchmark problems to show its practicability.","As part of the refinement, we develop verified software to certify Linear Programming solutions.","The algorithm builds on a diverse library of formalized mathematics and pushes existing methodologies for interactive theorem provers to the limits.","We discuss the process of the verification project and the modifications to the algorithm needed for formal verification."],"url":"http://arxiv.org/abs/2406.07340v1","category":"cs.AI"}
{"created":"2024-06-11 15:06:34","title":"Capacity Credit Evaluation of Generalized Energy Storage Considering Endogenous Uncertainty","abstract":"Generalized energy storage (GES), encompassing both physical and virtual energy storage, can provide remarkable but uncertain adequacy flexibility. When assessing GES's contribution to resource adequacy, the literature typically considers exogenous uncertainties (e.g., failures and stochastic response) but overlooks endogenous uncertainties, such as self-scheduling in liberal markets and decision-dependent uncertainty (DDU). In this regard, this paper proposes a novel capacity credit evaluation framework to accurately quantify GES's contribution to resource adequacy, where a sequential coordinated dispatch method is proposed to capture realistic GES operations by coordinating self-scheduling in the day-ahead energy market and real-time adequacy-oriented dispatch in the capacity market. To incorporate DDU of GES (i.e., responsiveness affected by dispatch decisions and prices in capacity market), we present a chance-constrained optimization approach and tractable solution methodologies for real-time dispatch. We propose a practical adequacy assessment method to quantify the impact of DDU on capacity credit by evaluating the consequence of ignoring DDU. Additionally, a novel capacity credit index called equivalent storage capacity substitution is introduced to quantify the equivalent deterministic storage capacity of the uncertain virtual energy storage. Simulations show that the proposed method yields reliable and accurate capacity credit values by accounting for self-scheduling of GES and managing the risk from DDU. Finally, key impact factors of GES's capacity credit are thoroughly discussed, offering valuable insights for the decision-making of capacity market operators.","sentences":["Generalized energy storage (GES), encompassing both physical and virtual energy storage, can provide remarkable but uncertain adequacy flexibility.","When assessing GES's contribution to resource adequacy, the literature typically considers exogenous uncertainties (e.g., failures and stochastic response) but overlooks endogenous uncertainties, such as self-scheduling in liberal markets and decision-dependent uncertainty (DDU).","In this regard, this paper proposes a novel capacity credit evaluation framework to accurately quantify GES's contribution to resource adequacy, where a sequential coordinated dispatch method is proposed to capture realistic GES operations by coordinating self-scheduling in the day-ahead energy market and real-time adequacy-oriented dispatch in the capacity market.","To incorporate DDU of GES (i.e., responsiveness affected by dispatch decisions and prices in capacity market), we present a chance-constrained optimization approach and tractable solution methodologies for real-time dispatch.","We propose a practical adequacy assessment method to quantify the impact of DDU on capacity credit by evaluating the consequence of ignoring DDU.","Additionally, a novel capacity credit index called equivalent storage capacity substitution is introduced to quantify the equivalent deterministic storage capacity of the uncertain virtual energy storage.","Simulations show that the proposed method yields reliable and accurate capacity credit values by accounting for self-scheduling of GES and managing the risk from DDU.","Finally, key impact factors of GES's capacity credit are thoroughly discussed, offering valuable insights for the decision-making of capacity market operators."],"url":"http://arxiv.org/abs/2406.07338v1","category":"eess.SY"}
{"created":"2024-06-11 15:04:25","title":"Searching for gravitational waves from stellar-mass binary black holes early inspiral","abstract":"The early inspiral from stellar-mass binary black holes can emit milli-Hertz gravitational wave signals, making them detectable sources for space-borne gravitational wave missions like TianQin. However, the traditional matched filtering technique poses a significant challenge for analyzing this kind of signals, as it requires an impractically high number of templates ranging from $10^{31}$ to $10^{40}$. We propose a search strategy that involves two main parts: initially, we reduce the dimensionality of the simulated signals using incremental principal component analysis (IPCA). Subsequently we train the convolutional neural networks (CNNs) based on the compressed TianQin data obtained from IPCA, aiming to develop both a detection model and a point parameter estimation model. The compression efficiency for the trained IPCA model achieves a cumulative variance ratio of 95.6% when applied to $10^6$ simulated signals. To evaluate the performance of CNN we generate the receiver operating characteristic curve for the detection model which is applied to the test data with varying signal-to-noise ratios. At a false alarm probability of 5% the corresponding true alarm probability for signals with a signal-to-noise ratio of 50 is 86.5%. Subsequently, we introduce the point estimation model to evaluate the value of the chirp mass of corresponding sBBH signals with an error. For signals with a signal-to-noise ratio of 50, the trained point estimation CNN model can estimate the chirp mass of most test events, with a standard deviation error of 2.48 $M_{\\odot}$ and a relative error precision of 0.12.","sentences":["The early inspiral from stellar-mass binary black holes can emit milli-Hertz gravitational wave signals, making them detectable sources for space-borne gravitational wave missions like TianQin.","However, the traditional matched filtering technique poses a significant challenge for analyzing this kind of signals, as it requires an impractically high number of templates ranging from $10^{31}$ to $10^{40}$. We propose a search strategy that involves two main parts: initially, we reduce the dimensionality of the simulated signals using incremental principal component analysis (IPCA).","Subsequently we train the convolutional neural networks (CNNs) based on the compressed TianQin data obtained from IPCA, aiming to develop both a detection model and a point parameter estimation model.","The compression efficiency for the trained IPCA model achieves a cumulative variance ratio of 95.6% when applied to $10^6$ simulated signals.","To evaluate the performance of CNN we generate the receiver operating characteristic curve for the detection model which is applied to the test data with varying signal-to-noise ratios.","At a false alarm probability of 5% the corresponding true alarm probability for signals with a signal-to-noise ratio of 50 is 86.5%.","Subsequently, we introduce the point estimation model to evaluate the value of the chirp mass of corresponding sBBH signals with an error.","For signals with a signal-to-noise ratio of 50, the trained point estimation CNN model can estimate the chirp mass of most test events, with a standard deviation error of 2.48 $M_{\\odot}$ and a relative error precision of 0.12."],"url":"http://arxiv.org/abs/2406.07336v1","category":"astro-ph.SR"}
{"created":"2024-06-11 15:01:20","title":"Minimizing Energy Costs in Deep Learning Model Training: The Gaussian Sampling Approach","abstract":"Computing the loss gradient via backpropagation consumes considerable energy during deep learning (DL) model training. In this paper, we propose a novel approach to efficiently compute DL models' gradients to mitigate the substantial energy overhead associated with backpropagation. Exploiting the over-parameterized nature of DL models and the smoothness of their loss landscapes, we propose a method called {\\em GradSamp} for sampling gradient updates from a Gaussian distribution. Specifically, we update model parameters at a given epoch (chosen periodically or randomly) by perturbing the parameters (element-wise) from the previous epoch with Gaussian ``noise''. The parameters of the Gaussian distribution are estimated using the error between the model parameter values from the two previous epochs. {\\em GradSamp} not only streamlines gradient computation but also enables skipping entire epochs, thereby enhancing overall efficiency. We rigorously validate our hypothesis across a diverse set of standard and non-standard CNN and transformer-based models, spanning various computer vision tasks such as image classification, object detection, and image segmentation. Additionally, we explore its efficacy in out-of-distribution scenarios such as Domain Adaptation (DA), Domain Generalization (DG), and decentralized settings like Federated Learning (FL). Our experimental results affirm the effectiveness of {\\em GradSamp} in achieving notable energy savings without compromising performance, underscoring its versatility and potential impact in practical DL applications.","sentences":["Computing the loss gradient via backpropagation consumes considerable energy during deep learning (DL) model training.","In this paper, we propose a novel approach to efficiently compute DL models' gradients to mitigate the substantial energy overhead associated with backpropagation.","Exploiting the over-parameterized nature of DL models and the smoothness of their loss landscapes, we propose a method called {\\em GradSamp} for sampling gradient updates from a Gaussian distribution.","Specifically, we update model parameters at a given epoch (chosen periodically or randomly) by perturbing the parameters (element-wise) from the previous epoch with Gaussian ``noise''.","The parameters of the Gaussian distribution are estimated using the error between the model parameter values from the two previous epochs.","{\\em GradSamp} not only streamlines gradient computation but also enables skipping entire epochs, thereby enhancing overall efficiency.","We rigorously validate our hypothesis across a diverse set of standard and non-standard CNN and transformer-based models, spanning various computer vision tasks such as image classification, object detection, and image segmentation.","Additionally, we explore its efficacy in out-of-distribution scenarios such as Domain Adaptation (DA), Domain Generalization (DG), and decentralized settings like Federated Learning (FL).","Our experimental results affirm the effectiveness of {\\em GradSamp} in achieving notable energy savings without compromising performance, underscoring its versatility and potential impact in practical DL applications."],"url":"http://arxiv.org/abs/2406.07332v1","category":"cs.CV"}
{"created":"2024-06-11 15:00:33","title":"CTC-based Non-autoregressive Textless Speech-to-Speech Translation","abstract":"Direct speech-to-speech translation (S2ST) has achieved impressive translation quality, but it often faces the challenge of slow decoding due to the considerable length of speech sequences. Recently, some research has turned to non-autoregressive (NAR) models to expedite decoding, yet the translation quality typically lags behind autoregressive (AR) models significantly. In this paper, we investigate the performance of CTC-based NAR models in S2ST, as these models have shown impressive results in machine translation. Experimental results demonstrate that by combining pretraining, knowledge distillation, and advanced NAR training techniques such as glancing training and non-monotonic latent alignments, CTC-based NAR models achieve translation quality comparable to the AR model, while preserving up to 26.81$\\times$ decoding speedup.","sentences":["Direct speech-to-speech translation (S2ST) has achieved impressive translation quality, but it often faces the challenge of slow decoding due to the considerable length of speech sequences.","Recently, some research has turned to non-autoregressive (NAR) models to expedite decoding, yet the translation quality typically lags behind autoregressive (AR) models significantly.","In this paper, we investigate the performance of CTC-based NAR models in S2ST, as these models have shown impressive results in machine translation.","Experimental results demonstrate that by combining pretraining, knowledge distillation, and advanced NAR training techniques such as glancing training and non-monotonic latent alignments, CTC-based NAR models achieve translation quality comparable to the AR model, while preserving up to 26.81$\\times$ decoding speedup."],"url":"http://arxiv.org/abs/2406.07330v1","category":"cs.CL"}
{"created":"2024-06-11 14:59:29","title":"Realistic Data Generation for 6D Pose Estimation of Surgical Instruments","abstract":"Automation in surgical robotics has the potential to improve patient safety and surgical efficiency, but it is difficult to achieve due to the need for robust perception algorithms. In particular, 6D pose estimation of surgical instruments is critical to enable the automatic execution of surgical maneuvers based on visual feedback. In recent years, supervised deep learning algorithms have shown increasingly better performance at 6D pose estimation tasks; yet, their success depends on the availability of large amounts of annotated data. In household and industrial settings, synthetic data, generated with 3D computer graphics software, has been shown as an alternative to minimize annotation costs of 6D pose datasets. However, this strategy does not translate well to surgical domains as commercial graphics software have limited tools to generate images depicting realistic instrument-tissue interactions. To address these limitations, we propose an improved simulation environment for surgical robotics that enables the automatic generation of large and diverse datasets for 6D pose estimation of surgical instruments. Among the improvements, we developed an automated data generation pipeline and an improved surgical scene. To show the applicability of our system, we generated a dataset of 7.5k images with pose annotations of a surgical needle that was used to evaluate a state-of-the-art pose estimation network. The trained model obtained a mean translational error of 2.59mm on a challenging dataset that presented varying levels of occlusion. These results highlight our pipeline's success in training and evaluating novel vision algorithms for surgical robotics applications.","sentences":["Automation in surgical robotics has the potential to improve patient safety and surgical efficiency, but it is difficult to achieve due to the need for robust perception algorithms.","In particular, 6D pose estimation of surgical instruments is critical to enable the automatic execution of surgical maneuvers based on visual feedback.","In recent years, supervised deep learning algorithms have shown increasingly better performance at 6D pose estimation tasks; yet, their success depends on the availability of large amounts of annotated data.","In household and industrial settings, synthetic data, generated with 3D computer graphics software, has been shown as an alternative to minimize annotation costs of 6D pose datasets.","However, this strategy does not translate well to surgical domains as commercial graphics software have limited tools to generate images depicting realistic instrument-tissue interactions.","To address these limitations, we propose an improved simulation environment for surgical robotics that enables the automatic generation of large and diverse datasets for 6D pose estimation of surgical instruments.","Among the improvements, we developed an automated data generation pipeline and an improved surgical scene.","To show the applicability of our system, we generated a dataset of 7.5k images with pose annotations of a surgical needle that was used to evaluate a state-of-the-art pose estimation network.","The trained model obtained a mean translational error of 2.59mm on a challenging dataset that presented varying levels of occlusion.","These results highlight our pipeline's success in training and evaluating novel vision algorithms for surgical robotics applications."],"url":"http://arxiv.org/abs/2406.07328v1","category":"cs.RO"}
{"created":"2024-06-11 14:59:24","title":"3D-Properties: Identifying Challenges in DPO and Charting a Path Forward","abstract":"Aligning large language models (LLMs) with human preference has recently gained tremendous attention, with the canonical yet costly RLHF-PPO and the simple and straightforward Direct Preference Optimization (DPO) as two examples. Despite the efficiency, DPO has rarely be used in the state-of-the-art production-level LLMs, implying its potential pathologies. In this work, we revisit DPO with a comprehensive examination of its empirical efficacy and a systematic comparison with RLHF-PPO. We identify the \\textbf{3D}-properties of DPO's learning outcomes: the \\textbf{D}rastic drop in the likelihood of rejected responses, the \\textbf{D}egradation into LLM unlearning, and the \\textbf{D}ispersion effect on unseen responses through experiments with both a carefully designed toy model and practical LLMs on tasks including mathematical problem-solving and instruction following. These findings inherently connect to some observations made by related works and we additionally contribute a plausible theoretical explanation for them. Accordingly, we propose easy regularization methods to mitigate the issues caused by \\textbf{3D}-properties, improving the training stability and final performance of DPO. Our contributions also include an investigation into how the distribution of the paired preference data impacts the effectiveness of DPO. We hope this work could offer research directions to narrow the gap between reward-free preference learning methods and reward-based ones.","sentences":["Aligning large language models (LLMs) with human preference has recently gained tremendous attention, with the canonical yet costly RLHF-PPO and the simple and straightforward Direct Preference Optimization (DPO) as two examples.","Despite the efficiency, DPO has rarely be used in the state-of-the-art production-level LLMs, implying its potential pathologies.","In this work, we revisit DPO with a comprehensive examination of its empirical efficacy and a systematic comparison with RLHF-PPO.","We identify the \\textbf{3D}-properties of DPO's learning outcomes: the \\textbf{D}rastic drop in the likelihood of rejected responses, the \\textbf{D}egradation into LLM unlearning, and the \\textbf{D}ispersion effect on unseen responses through experiments with both a carefully designed toy model and practical LLMs on tasks including mathematical problem-solving and instruction following.","These findings inherently connect to some observations made by related works and we additionally contribute a plausible theoretical explanation for them.","Accordingly, we propose easy regularization methods to mitigate the issues caused by \\textbf{3D}-properties, improving the training stability and final performance of DPO.","Our contributions also include an investigation into how the distribution of the paired preference data impacts the effectiveness of DPO.","We hope this work could offer research directions to narrow the gap between reward-free preference learning methods and reward-based ones."],"url":"http://arxiv.org/abs/2406.07327v1","category":"cs.AI"}
{"created":"2024-06-11 14:59:18","title":"Beyond Training: Optimizing Reinforcement Learning Based Job Shop Scheduling Through Adaptive Action Sampling","abstract":"Learned construction heuristics for scheduling problems have become increasingly competitive with established solvers and heuristics in recent years. In particular, significant improvements have been observed in solution approaches using deep reinforcement learning (DRL). While much attention has been paid to the design of network architectures and training algorithms to achieve state-of-the-art results, little research has investigated the optimal use of trained DRL agents during inference. Our work is based on the hypothesis that, similar to search algorithms, the utilization of trained DRL agents should be dependent on the acceptable computational budget. We propose a simple yet effective parameterization, called $\\delta$-sampling that manipulates the trained action vector to bias agent behavior towards exploration or exploitation during solution construction. By following this approach, we can achieve a more comprehensive coverage of the search space while still generating an acceptable number of solutions. In addition, we propose an algorithm for obtaining the optimal parameterization for such a given number of solutions and any given trained agent. Experiments extending existing training protocols for job shop scheduling problems with our inference method validate our hypothesis and result in the expected improvements of the generated solutions.","sentences":["Learned construction heuristics for scheduling problems have become increasingly competitive with established solvers and heuristics in recent years.","In particular, significant improvements have been observed in solution approaches using deep reinforcement learning (DRL).","While much attention has been paid to the design of network architectures and training algorithms to achieve state-of-the-art results, little research has investigated the optimal use of trained DRL agents during inference.","Our work is based on the hypothesis that, similar to search algorithms, the utilization of trained DRL agents should be dependent on the acceptable computational budget.","We propose a simple yet effective parameterization, called $\\delta$-sampling that manipulates the trained action vector to bias agent behavior towards exploration or exploitation during solution construction.","By following this approach, we can achieve a more comprehensive coverage of the search space while still generating an acceptable number of solutions.","In addition, we propose an algorithm for obtaining the optimal parameterization for such a given number of solutions and any given trained agent.","Experiments extending existing training protocols for job shop scheduling problems with our inference method validate our hypothesis and result in the expected improvements of the generated solutions."],"url":"http://arxiv.org/abs/2406.07325v1","category":"cs.AI"}
{"created":"2024-06-11 14:53:07","title":"Should XAI Nudge Human Decisions with Explanation Biasing?","abstract":"This paper reviews our previous trials of Nudge-XAI, an approach that introduces automatic biases into explanations from explainable AIs (XAIs) with the aim of leading users to better decisions, and it discusses the benefits and challenges. Nudge-XAI uses a user model that predicts the influence of providing an explanation or emphasizing it and attempts to guide users toward AI-suggested decisions without coercion. The nudge design is expected to enhance the autonomy of users, reduce the risk associated with an AI making decisions without users' full agreement, and enable users to avoid AI failures. To discuss the potential of Nudge-XAI, this paper reports a post-hoc investigation of previous experimental results using cluster analysis. The results demonstrate the diversity of user behavior in response to Nudge-XAI, which supports our aim of enhancing user autonomy. However, it also highlights the challenge of users who distrust AI and falsely make decisions contrary to AI suggestions, suggesting the need for personalized adjustment of the strength of nudges to make this approach work more generally.","sentences":["This paper reviews our previous trials of Nudge-XAI, an approach that introduces automatic biases into explanations from explainable AIs (XAIs) with the aim of leading users to better decisions, and it discusses the benefits and challenges.","Nudge-XAI uses a user model that predicts the influence of providing an explanation or emphasizing it and attempts to guide users toward AI-suggested decisions without coercion.","The nudge design is expected to enhance the autonomy of users, reduce the risk associated with an AI making decisions without users' full agreement, and enable users to avoid AI failures.","To discuss the potential of Nudge-XAI, this paper reports a post-hoc investigation of previous experimental results using cluster analysis.","The results demonstrate the diversity of user behavior in response to Nudge-XAI, which supports our aim of enhancing user autonomy.","However, it also highlights the challenge of users who distrust AI and falsely make decisions contrary to AI suggestions, suggesting the need for personalized adjustment of the strength of nudges to make this approach work more generally."],"url":"http://arxiv.org/abs/2406.07323v1","category":"cs.HC"}
{"created":"2024-06-11 14:52:40","title":"A derivation of Dickson-Brewer polynomials using the Cayley-Hamilton theorem","abstract":"In this note, the first-order Dickson polynomials are introduced through a particular case of the expression of the trace of the $n^{th}$ power of a matrix in terms of powers of the trace and determinant of the matrix itself. The technique relies on the Cayley-Hamilton theorem and its application to the derivation of formulas due to Carlitz and to second-order Dickson polynomials is straightforward. Finally, generalization of Dickson polynomials over finite fields and multivariate Dickson polynomials are evoked as potential avenues of investigation in the same framework.","sentences":["In this note, the first-order Dickson polynomials are introduced through a particular case of the expression of the trace of the $n^{th}$ power of a matrix in terms of powers of the trace and determinant of the matrix itself.","The technique relies on the Cayley-Hamilton theorem and its application to the derivation of formulas due to Carlitz and to second-order Dickson polynomials is straightforward.","Finally, generalization of Dickson polynomials over finite fields and multivariate Dickson polynomials are evoked as potential avenues of investigation in the same framework."],"url":"http://arxiv.org/abs/2406.07322v1","category":"math.NT"}
{"created":"2024-06-11 14:49:04","title":"A Framework for Efficient Model Evaluation through Stratification, Sampling, and Estimation","abstract":"Model performance evaluation is a critical and expensive task in machine learning and computer vision. Without clear guidelines, practitioners often estimate model accuracy using a one-time random selection of the data. However, by employing tailored sampling and estimation strategies, one can obtain more precise estimates and reduce annotation costs. In this paper, we propose a statistical framework for model evaluation that includes stratification, sampling, and estimation components. We examine the statistical properties of each component and evaluate their efficiency (precision). One key result of our work is that stratification via k-means clustering based on accurate predictions of model performance yields efficient estimators. Our experiments on computer vision datasets show that this method consistently provides more precise accuracy estimates than the traditional simple random sampling, even with substantial efficiency gains of 10x. We also find that model-assisted estimators, which leverage predictions of model accuracy on the unlabeled portion of the dataset, are generally more efficient than the traditional estimates based solely on the labeled data.","sentences":["Model performance evaluation is a critical and expensive task in machine learning and computer vision.","Without clear guidelines, practitioners often estimate model accuracy using a one-time random selection of the data.","However, by employing tailored sampling and estimation strategies, one can obtain more precise estimates and reduce annotation costs.","In this paper, we propose a statistical framework for model evaluation that includes stratification, sampling, and estimation components.","We examine the statistical properties of each component and evaluate their efficiency (precision).","One key result of our work is that stratification via k-means clustering based on accurate predictions of model performance yields efficient estimators.","Our experiments on computer vision datasets show that this method consistently provides more precise accuracy estimates than the traditional simple random sampling, even with substantial efficiency gains of 10x.","We also find that model-assisted estimators, which leverage predictions of model accuracy on the unlabeled portion of the dataset, are generally more efficient than the traditional estimates based solely on the labeled data."],"url":"http://arxiv.org/abs/2406.07320v1","category":"cs.CV"}
{"created":"2024-06-11 14:48:52","title":"The representation and computational efficiency of the Tolman-Oppenheimer-Volkoff equations in isotropic coordinates","abstract":"This study aims to provide an analytical scheme for computing equilibrium configurations of relativistic stars by solving the Tolman-Oppenheimer-Volkoff equations directly in isotropic polar coordinates, as opposed to the commonly applied methods of rescaling the radial profile of corresponding solutions obtained in curvature coordinates. This study also provides evidence that the differential equation for gravitational mass may be replaced by an algebraic expression relating the metric potential to the energy density in the form of the quartic equation. Nevertheless, the greater computational expense of evaluating the algebraic equation renders its application less efficient. A further objective of this study was to evaluate the performance of the present computational scheme in the computational time and numerical accuracy. Our results indicate that the computational time increases with the stiffness of the constituent matter inside the star. Conversely, the absolute difference between the gravitational mass obtained by the proposed method and that computed via the use of LORENE packages initially increases rapidly with the central energy density, but the rate of growth subsequently declines as the maximum mass configuration is approached.","sentences":["This study aims to provide an analytical scheme for computing equilibrium configurations of relativistic stars by solving the Tolman-Oppenheimer-Volkoff equations directly in isotropic polar coordinates, as opposed to the commonly applied methods of rescaling the radial profile of corresponding solutions obtained in curvature coordinates.","This study also provides evidence that the differential equation for gravitational mass may be replaced by an algebraic expression relating the metric potential to the energy density in the form of the quartic equation.","Nevertheless, the greater computational expense of evaluating the algebraic equation renders its application less efficient.","A further objective of this study was to evaluate the performance of the present computational scheme in the computational time and numerical accuracy.","Our results indicate that the computational time increases with the stiffness of the constituent matter inside the star.","Conversely, the absolute difference between the gravitational mass obtained by the proposed method and that computed via the use of LORENE packages initially increases rapidly with the central energy density, but the rate of growth subsequently declines as the maximum mass configuration is approached."],"url":"http://arxiv.org/abs/2406.07319v1","category":"gr-qc"}
{"created":"2024-06-11 14:44:37","title":"Rethinking the impact of noisy labels in graph classification: A utility and privacy perspective","abstract":"Graph neural networks based on message-passing mechanisms have achieved advanced results in graph classification tasks. However, their generalization performance degrades when noisy labels are present in the training data. Most existing noisy labeling approaches focus on the visual domain or graph node classification tasks and analyze the impact of noisy labels only from a utility perspective. Unlike existing work, in this paper, we measure the effects of noise labels on graph classification from data privacy and model utility perspectives. We find that noise labels degrade the model's generalization performance and enhance the ability of membership inference attacks on graph data privacy. To this end, we propose the robust graph neural network approach with noisy labeled graph classification. Specifically, we first accurately filter the noisy samples by high-confidence samples and the first feature principal component vector of each class. Then, the robust principal component vectors and the model output under data augmentation are utilized to achieve noise label correction guided by dual spatial information. Finally, supervised graph contrastive learning is introduced to enhance the embedding quality of the model and protect the privacy of the training graph data. The utility and privacy of the proposed method are validated by comparing twelve different methods on eight real graph classification datasets. Compared with the state-of-the-art methods, the RGLC method achieves at most and at least 7.8% and 0.8% performance gain at 30% noisy labeling rate, respectively, and reduces the accuracy of privacy attacks to below 60%.","sentences":["Graph neural networks based on message-passing mechanisms have achieved advanced results in graph classification tasks.","However, their generalization performance degrades when noisy labels are present in the training data.","Most existing noisy labeling approaches focus on the visual domain or graph node classification tasks and analyze the impact of noisy labels only from a utility perspective.","Unlike existing work, in this paper, we measure the effects of noise labels on graph classification from data privacy and model utility perspectives.","We find that noise labels degrade the model's generalization performance and enhance the ability of membership inference attacks on graph data privacy.","To this end, we propose the robust graph neural network approach with noisy labeled graph classification.","Specifically, we first accurately filter the noisy samples by high-confidence samples and the first feature principal component vector of each class.","Then, the robust principal component vectors and the model output under data augmentation are utilized to achieve noise label correction guided by dual spatial information.","Finally, supervised graph contrastive learning is introduced to enhance the embedding quality of the model and protect the privacy of the training graph data.","The utility and privacy of the proposed method are validated by comparing twelve different methods on eight real graph classification datasets.","Compared with the state-of-the-art methods, the RGLC method achieves at most and at least 7.8% and 0.8% performance gain at 30% noisy labeling rate, respectively, and reduces the accuracy of privacy attacks to below 60%."],"url":"http://arxiv.org/abs/2406.07314v1","category":"cs.LG"}
{"created":"2024-06-11 14:42:16","title":"Quantum MEP hydrodynamical model for charge transport","abstract":"A well known procedure to get quantum hydrodynamical models for charge transport is to resort to the Wigner equations and deduce the hierarchy of the moment equations as in the semiclassical approach. If one truncates the moment hierarchy to a finite order, the resulting set of balance equations requires some closure assumption because the number of unknowns exceed the number of equations. In the classical and semiclassical kinetic theory a sound approach to get the desired closure relations is that based on the Maximum Entropy Principle (MEP) [13] (see[20] for charge transport in semiconductors). In [9] a quantum MEP hydrodynamical model has been devised for charge transport in the parabolic band approximation by introducing quantum correction based on the equilibrium Wigner function [30]. An extension to electron moving in pristine graphene has been obtained in [29]. Here we present a quantum hydrodynamical model which is valid for a general energy band considering a closure of the moment system deduced by the Wigner equation resorting to a quantum version of MEP. Explicit formulas for quantum correction at order \\hbar^2 are obtained with the aid of the Moyal calculus for silicon and graphene removing the limitation that the quantum corrections are based on the equilibrium Wigner function as in [9, 29]. As an application, quantum correction to the mobilities are deduced.","sentences":["A well known procedure to get quantum hydrodynamical models for charge transport is to resort to the Wigner equations and deduce the hierarchy of the moment equations as in the semiclassical approach.","If one truncates the moment hierarchy to a finite order, the resulting set of balance equations requires some closure assumption because the number of unknowns exceed the number of equations.","In the classical and semiclassical kinetic theory a sound approach to get the desired closure relations is that based on the Maximum Entropy Principle (MEP)","[13] (see[20] for charge transport in semiconductors).","In [9] a quantum MEP hydrodynamical model has been devised for charge transport in the parabolic band approximation by introducing quantum correction based on the equilibrium Wigner function","[30].","An extension to electron moving in pristine graphene has been obtained in [29].","Here we present a quantum hydrodynamical model which is valid for a general energy band considering a closure of the moment system deduced by the Wigner equation resorting to a quantum version of MEP.","Explicit formulas for quantum correction at order \\hbar^2 are obtained with the aid of the Moyal calculus for silicon and graphene removing the limitation that the quantum corrections are based on the equilibrium Wigner function as in [9, 29].","As an application, quantum correction to the mobilities are deduced."],"url":"http://arxiv.org/abs/2406.07312v1","category":"math-ph"}
{"created":"2024-06-11 14:36:50","title":"Revealing Predictive Maintenance Strategies from Comprehensive Data Analysis of ASTRI-Horn Historical Monitoring Data","abstract":"Modern telescope facilities generate data from various sources, including sensors, weather stations, LiDARs, and FRAMs. Sophisticated software architectures using the Internet of Things (IoT) and big data technologies are required to manage this data. This study explores the potential of sensor data for innovative maintenance techniques, such as predictive maintenance (PdM), to prevent downtime that can affect research. We analyzed historical data from the ASTRI-Horn Cherenkov telescope, spanning seven years, examining data patterns and variable correlations. The findings offer insights for triggering predictive maintenance model development in telescope facilities.","sentences":["Modern telescope facilities generate data from various sources, including sensors, weather stations, LiDARs, and FRAMs.","Sophisticated software architectures using the Internet of Things (IoT) and big data technologies are required to manage this data.","This study explores the potential of sensor data for innovative maintenance techniques, such as predictive maintenance (PdM), to prevent downtime that can affect research.","We analyzed historical data from the ASTRI-Horn Cherenkov telescope, spanning seven years, examining data patterns and variable correlations.","The findings offer insights for triggering predictive maintenance model development in telescope facilities."],"url":"http://arxiv.org/abs/2406.07308v1","category":"astro-ph.IM"}
{"created":"2024-06-11 14:32:57","title":"Alexandrov-Fenchel inequalities for capillary hypersurfaces in hyperbolic space","abstract":"In this article, we first introduce the quermassintegrals for compact hypersurfaces with capillary boundaries in hyperbolic space from a variational viewpoint, and then we solve an isoperimetric type problem in hyperbolic space. By constructing a new locally constrained inverse curvature flow, we obtain the Alexandrov-Fenchel inequalities for convex capillary hypersurfaces in hyperbolic space. This generalizes a theorem of Brendle-Guan-Li \\cite{BGL} for convex closed hypersurfaces in hyperbolic space.","sentences":["In this article, we first introduce the quermassintegrals for compact hypersurfaces with capillary boundaries in hyperbolic space from a variational viewpoint, and then we solve an isoperimetric type problem in hyperbolic space.","By constructing a new locally constrained inverse curvature flow, we obtain the Alexandrov-Fenchel inequalities for convex capillary hypersurfaces in hyperbolic space.","This generalizes a theorem of Brendle-Guan-Li \\cite{BGL} for convex closed hypersurfaces in hyperbolic space."],"url":"http://arxiv.org/abs/2406.07304v1","category":"math.DG"}
{"created":"2024-06-11 14:30:34","title":"BertaQA: How Much Do Language Models Know About Local Culture?","abstract":"Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.","sentences":["Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects.","This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent.","To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque.","The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest.","We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics.","However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English.","To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language.","Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics.","Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA."],"url":"http://arxiv.org/abs/2406.07302v1","category":"cs.CL"}
{"created":"2024-06-11 14:28:53","title":"Shadows, rings and optical appearance of a magnetically charged regular black hole illuminated by various accretion disks","abstract":"The Event Horizon Telescope (EHT) imaging of the supermassive black holes at the centers of Messier 87 galaxy and the Milky Way galaxy marks a significant step in observing the photon rings and central brightness depression that define the optical appearance of black holes with an accretion disk scenario. Inspired by this, we take into account a static and spherically symmetric magnetically charged regular black hole (MCRBH) metric characterized by its mass and an additional parameter q, which arises from the coupling of Einstein gravity and nonlinear electrodynamics (NLED) in the weak field approximation. This parameterized model offers a robust foundation for testing the coupling of Einstein gravity and NLED in the weak-field approximation, using the EHT observational results. In this study, we investigate the geodesic motion of particles around the solution, followed by a discussion of its fundamental geometrical characteristics such as scalar invariants. Using null geodesics, we examine how the model parameter influences the behavior of the photon sphere radius and the associated shadow silhouette. We seek constraints on q by applying the EHT results for supermassive black holes M87* and Sgr A*. Furthermore, it is observed that the geodesics of time-like particles are susceptible to variations in q, which can have an impact on the traits of the innermost stable circular orbit and the marginally bounded orbit. Our primary objective is to probe how the free parameter q affects various aspects of the accretion disk surrounding the MCRBH using the thin-disk approximation. Next, we discuss the physical characteristics of the thin accretion disk as well as the observed shadows and rings of the MCRBH, along with its luminosity, across various accretion models. Ultimately, variations in accretion models and the parameter q yield distinct shadow images and optical appearances of the MCRBH.","sentences":["The Event Horizon Telescope (EHT) imaging of the supermassive black holes at the centers of Messier 87 galaxy and the Milky Way galaxy marks a significant step in observing the photon rings and central brightness depression that define the optical appearance of black holes with an accretion disk scenario.","Inspired by this, we take into account a static and spherically symmetric magnetically charged regular black hole (MCRBH) metric characterized by its mass and an additional parameter q, which arises from the coupling of Einstein gravity and nonlinear electrodynamics (NLED) in the weak field approximation.","This parameterized model offers a robust foundation for testing the coupling of Einstein gravity and NLED in the weak-field approximation, using the EHT observational results.","In this study, we investigate the geodesic motion of particles around the solution, followed by a discussion of its fundamental geometrical characteristics such as scalar invariants.","Using null geodesics, we examine how the model parameter influences the behavior of the photon sphere radius and the associated shadow silhouette.","We seek constraints on q by applying the EHT results for supermassive black holes M87* and Sgr A*.","Furthermore, it is observed that the geodesics of time-like particles are susceptible to variations in q, which can have an impact on the traits of the innermost stable circular orbit and the marginally bounded orbit.","Our primary objective is to probe how the free parameter q affects various aspects of the accretion disk surrounding the MCRBH using the thin-disk approximation.","Next, we discuss the physical characteristics of the thin accretion disk as well as the observed shadows and rings of the MCRBH, along with its luminosity, across various accretion models.","Ultimately, variations in accretion models and the parameter q yield distinct shadow images and optical appearances of the MCRBH."],"url":"http://arxiv.org/abs/2406.07300v1","category":"astro-ph.HE"}
{"created":"2024-06-11 14:22:37","title":"Joint Learning of Context and Feedback Embeddings in Spoken Dialogue","abstract":"Short feedback responses, such as backchannels, play an important role in spoken dialogue. So far, most of the modeling of feedback responses has focused on their timing, often neglecting how their lexical and prosodic form influence their contextual appropriateness and conversational function. In this paper, we investigate the possibility of embedding short dialogue contexts and feedback responses in the same representation space using a contrastive learning objective. In our evaluation, we primarily focus on how such embeddings can be used as a context-feedback appropriateness metric and thus for feedback response ranking in U.S. English dialogues. Our results show that the model outperforms humans given the same ranking task and that the learned embeddings carry information about the conversational function of feedback responses.","sentences":["Short feedback responses, such as backchannels, play an important role in spoken dialogue.","So far, most of the modeling of feedback responses has focused on their timing, often neglecting how their lexical and prosodic form influence their contextual appropriateness and conversational function.","In this paper, we investigate the possibility of embedding short dialogue contexts and feedback responses in the same representation space using a contrastive learning objective.","In our evaluation, we primarily focus on how such embeddings can be used as a context-feedback appropriateness metric and thus for feedback response ranking in U.S. English dialogues.","Our results show that the model outperforms humans given the same ranking task and that the learned embeddings carry information about the conversational function of feedback responses."],"url":"http://arxiv.org/abs/2406.07291v1","category":"cs.CL"}
{"created":"2024-06-11 14:17:12","title":"Can We Achieve High-quality Direct Speech-to-Speech Translation without Parallel Speech Data?","abstract":"Recently proposed two-pass direct speech-to-speech translation (S2ST) models decompose the task into speech-to-text translation (S2TT) and text-to-speech (TTS) within an end-to-end model, yielding promising results. However, the training of these models still relies on parallel speech data, which is extremely challenging to collect. In contrast, S2TT and TTS have accumulated a large amount of data and pretrained models, which have not been fully utilized in the development of S2ST models. Inspired by this, in this paper, we first introduce a composite S2ST model named ComSpeech, which can seamlessly integrate any pretrained S2TT and TTS models into a direct S2ST model. Furthermore, to eliminate the reliance on parallel speech data, we propose a novel training method ComSpeech-ZS that solely utilizes S2TT and TTS data. It aligns representations in the latent space through contrastive learning, enabling the speech synthesis capability learned from the TTS data to generalize to S2ST in a zero-shot manner. Experimental results on the CVSS dataset show that when the parallel speech data is available, ComSpeech surpasses previous two-pass models like UnitY and Translatotron 2 in both translation quality and decoding speed. When there is no parallel speech data, ComSpeech-ZS lags behind \\name by only 0.7 ASR-BLEU and outperforms the cascaded models.","sentences":["Recently proposed two-pass direct speech-to-speech translation (S2ST) models decompose the task into speech-to-text translation (S2TT) and text-to-speech (TTS) within an end-to-end model, yielding promising results.","However, the training of these models still relies on parallel speech data, which is extremely challenging to collect.","In contrast, S2TT and TTS have accumulated a large amount of data and pretrained models, which have not been fully utilized in the development of S2ST models.","Inspired by this, in this paper, we first introduce a composite S2ST model named ComSpeech, which can seamlessly integrate any pretrained S2TT and TTS models into a direct S2ST model.","Furthermore, to eliminate the reliance on parallel speech data, we propose a novel training method ComSpeech-ZS that solely utilizes S2TT and TTS data.","It aligns representations in the latent space through contrastive learning, enabling the speech synthesis capability learned from the TTS data to generalize to S2ST in a zero-shot manner.","Experimental results on the CVSS dataset show that when the parallel speech data is available, ComSpeech surpasses previous two-pass models like UnitY and Translatotron 2 in both translation quality and decoding speed.","When there is no parallel speech data, ComSpeech-ZS lags behind \\name by only 0.7 ASR-BLEU and outperforms the cascaded models."],"url":"http://arxiv.org/abs/2406.07289v1","category":"cs.CL"}
{"created":"2024-06-11 14:16:14","title":"Fine-tuning with HED-IT: The impact of human post-editing for dialogical language models","abstract":"Automatic methods for generating and gathering linguistic data have proven effective for fine-tuning Language Models (LMs) in languages less resourced than English. Still, while there has been emphasis on data quantity, less attention has been given to its quality. In this work, we investigate the impact of human intervention on machine-generated data when fine-tuning dialogical models. In particular, we study (1) whether post-edited dialogues exhibit higher perceived quality compared to the originals that were automatically generated; (2) whether fine-tuning with post-edited dialogues results in noticeable differences in the generated outputs; and (3) whether post-edited dialogues influence the outcomes when considering the parameter size of the LMs. To this end we created HED-IT, a large-scale dataset where machine-generated dialogues are paired with the version post-edited by humans. Using both the edited and unedited portions of HED-IT, we fine-tuned three different sizes of an LM. Results from both human and automatic evaluation show that the different quality of training data is clearly perceived and it has an impact also on the models trained on such data. Additionally, our findings indicate that larger models are less sensitive to data quality, whereas this has a crucial impact on smaller models. These results enhance our comprehension of the impact of human intervention on training data in the development of high-quality LMs.","sentences":["Automatic methods for generating and gathering linguistic data have proven effective for fine-tuning Language Models (LMs) in languages less resourced than English.","Still, while there has been emphasis on data quantity, less attention has been given to its quality.","In this work, we investigate the impact of human intervention on machine-generated data when fine-tuning dialogical models.","In particular, we study (1) whether post-edited dialogues exhibit higher perceived quality compared to the originals that were automatically generated; (2) whether fine-tuning with post-edited dialogues results in noticeable differences in the generated outputs; and (3) whether post-edited dialogues influence the outcomes when considering the parameter size of the LMs.","To this end we created HED-IT, a large-scale dataset where machine-generated dialogues are paired with the version post-edited by humans.","Using both the edited and unedited portions of HED-IT, we fine-tuned three different sizes of an LM.","Results from both human and automatic evaluation show that the different quality of training data is clearly perceived and it has an impact also on the models trained on such data.","Additionally, our findings indicate that larger models are less sensitive to data quality, whereas this has a crucial impact on smaller models.","These results enhance our comprehension of the impact of human intervention on training data in the development of high-quality LMs."],"url":"http://arxiv.org/abs/2406.07288v1","category":"cs.CL"}
{"created":"2024-06-11 14:13:25","title":"Exploring Waveform Variations among Neutron Star Ray-tracing Codes for Complex Emission Geometries","abstract":"Pulse Profile Modeling (PPM), the technique used to infer mass, radius and geometric parameters for rotation-powered millisecond pulsars using data from the Neutron Star Interior Composition Explorer (NICER), relies on relativistic ray-tracing of thermal X-ray photons from hot spots on the neutron star surface to the observer. To verify our ray-tracing codes we have in the past conducted cross-tests for simple hot spot geometries, focusing primarily on the implementation of the space-time model. In this paper, we present verification for test problems that explore the more complex hot spot geometries that are now being employed in the NICER PPM analyses. We conclude that the accuracy of our computed waveforms is in general sufficiently high for analyses of current NICER data sets. We have however identified some extreme configurations where extra care may be needed.","sentences":["Pulse Profile Modeling (PPM), the technique used to infer mass, radius and geometric parameters for rotation-powered millisecond pulsars using data from the Neutron Star Interior Composition Explorer (NICER), relies on relativistic ray-tracing of thermal X-ray photons from hot spots on the neutron star surface to the observer.","To verify our ray-tracing codes we have in the past conducted cross-tests for simple hot spot geometries, focusing primarily on the implementation of the space-time model.","In this paper, we present verification for test problems that explore the more complex hot spot geometries that are now being employed in the NICER PPM analyses.","We conclude that the accuracy of our computed waveforms is in general sufficiently high for analyses of current NICER data sets.","We have however identified some extreme configurations where extra care may be needed."],"url":"http://arxiv.org/abs/2406.07285v1","category":"astro-ph.HE"}
{"created":"2024-06-11 14:12:31","title":"Unsupervised Object Detection with Theoretical Guarantees","abstract":"Unsupervised object detection using deep neural networks is typically a difficult problem with few to no guarantees about the learned representation. In this work we present the first unsupervised object detection method that is theoretically guaranteed to recover the true object positions up to quantifiable small shifts. We develop an unsupervised object detection architecture and prove that the learned variables correspond to the true object positions up to small shifts related to the encoder and decoder receptive field sizes, the object sizes, and the widths of the Gaussians used in the rendering process. We perform detailed analysis of how the error depends on each of these variables and perform synthetic experiments validating our theoretical predictions up to a precision of individual pixels. We also perform experiments on CLEVR-based data and show that, unlike current SOTA object detection methods (SAM, CutLER), our method's prediction errors always lie within our theoretical bounds. We hope that this work helps open up an avenue of research into object detection methods with theoretical guarantees.","sentences":["Unsupervised object detection using deep neural networks is typically a difficult problem with few to no guarantees about the learned representation.","In this work we present the first unsupervised object detection method that is theoretically guaranteed to recover the true object positions up to quantifiable small shifts.","We develop an unsupervised object detection architecture and prove that the learned variables correspond to the true object positions up to small shifts related to the encoder and decoder receptive field sizes, the object sizes, and the widths of the Gaussians used in the rendering process.","We perform detailed analysis of how the error depends on each of these variables and perform synthetic experiments validating our theoretical predictions up to a precision of individual pixels.","We also perform experiments on CLEVR-based data and show that, unlike current SOTA object detection methods (SAM, CutLER), our method's prediction errors always lie within our theoretical bounds.","We hope that this work helps open up an avenue of research into object detection methods with theoretical guarantees."],"url":"http://arxiv.org/abs/2406.07284v1","category":"cs.CV"}
{"created":"2024-06-11 14:06:10","title":"$\\mathscr{D}$-modules on the basic affine space and large $\\mathfrak{g}$-modules","abstract":"In this paper, we treat $\\mathscr{D}$-modules on the basic affine space $G/U$ and their global sections for a semisimple complex algebraic group $G$. Our aim is to prepare basic results about large non-irreducible modules for the branching problem and harmonic analysis of reductive Lie groups. A main tool is a formula given by Bezrukavnikov--Braverman--Positselskii. The formula is about a product of functions and their Fourier transforms on $G/U$ like Capelli's identity. Using the formula, we give a generalization of the Beilinson--Bernstein correspondence.   We show that the global sections of holonomic $\\mathscr{D}$-modules are also holonomic using the formula. As a consequence, we give a large algebra action on the $\\mathfrak{u}$-cohomologies $H^i(\\mathfrak{u}; V)$ of a $\\mathfrak{g}$-module $V$ when $V$ is realized as a holonomic $\\mathscr{D}$-module. We consider affinity of the supports of the $\\mathfrak{t}$-modules $H^i(\\mathfrak{u}; V)$.","sentences":["In this paper, we treat $\\mathscr{D}$-modules on the basic affine space $G/U$ and their global sections for a semisimple complex algebraic group $G$. Our aim is to prepare basic results about large non-irreducible modules for the branching problem and harmonic analysis of reductive Lie groups.","A main tool is a formula given by Bezrukavnikov--Braverman--Positselskii.","The formula is about a product of functions and their Fourier transforms on $G/U$ like Capelli's identity.","Using the formula, we give a generalization of the Beilinson--Bernstein correspondence.   ","We show that the global sections of holonomic $\\mathscr{D}$-modules are also holonomic using the formula.","As a consequence, we give a large algebra action on the $\\mathfrak{u}$-cohomologies $H^i(\\mathfrak{u}; V)$ of a $\\mathfrak{g}$-module $V$ when $V$ is realized as a holonomic $\\mathscr{D}$-module.","We consider affinity of the supports of the $\\mathfrak{t}$-modules $H^i(\\mathfrak{u}; V)$."],"url":"http://arxiv.org/abs/2406.07279v1","category":"math.RT"}
{"created":"2024-06-11 14:04:25","title":"Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication","abstract":"Effective communication requires the ability to refer to specific parts of an observation in relation to others. While emergent communication literature shows success in developing various language properties, no research has shown the emergence of such positional references. This paper demonstrates how agents can communicate about spatial relationships within their observations. The results indicate that agents can develop a language capable of expressing the relationships between parts of their observation, achieving over 90% accuracy when trained in a referential game which requires such communication. Using a collocation measure, we demonstrate how the agents create such references. This analysis suggests that agents use a mixture of non-compositional and compositional messages to convey spatial relationships. We also show that the emergent language is interpretable by humans. The translation accuracy is tested by communicating with the receiver agent, where the receiver achieves over 78% accuracy using parts of this lexicon, confirming that the interpretation of the emergent language was successful.","sentences":["Effective communication requires the ability to refer to specific parts of an observation in relation to others.","While emergent communication literature shows success in developing various language properties, no research has shown the emergence of such positional references.","This paper demonstrates how agents can communicate about spatial relationships within their observations.","The results indicate that agents can develop a language capable of expressing the relationships between parts of their observation, achieving over 90% accuracy when trained in a referential game which requires such communication.","Using a collocation measure, we demonstrate how the agents create such references.","This analysis suggests that agents use a mixture of non-compositional and compositional messages to convey spatial relationships.","We also show that the emergent language is interpretable by humans.","The translation accuracy is tested by communicating with the receiver agent, where the receiver achieves over 78% accuracy using parts of this lexicon, confirming that the interpretation of the emergent language was successful."],"url":"http://arxiv.org/abs/2406.07277v1","category":"cs.CL"}
{"created":"2024-06-11 14:02:23","title":"DCA-Bench: A Benchmark for Dataset Curation Agents","abstract":"The quality of datasets plays an increasingly crucial role in the research and development of modern artificial intelligence (AI). Despite the proliferation of open dataset platforms nowadays, data quality issues, such as insufficient documentation, inaccurate annotations, and ethical concerns, remain common in datasets widely used in AI. Furthermore, these issues are often subtle and difficult to be detected by rule-based scripts, requiring expensive manual identification and verification by dataset users or maintainers. With the increasing capability of large language models (LLMs), it is promising to streamline the curation of datasets with LLM agents. In this work, as the initial step towards this goal, we propose a dataset curation agent benchmark, DCA-Bench, to measure LLM agents' capability of detecting hidden dataset quality issues. Specifically, we collect diverse real-world dataset quality issues from eight open dataset platforms as a testbed. Additionally, to establish an automatic pipeline for evaluating the success of LLM agents, which requires a nuanced understanding of the agent outputs, we implement a dedicated Evaluator using another LLM agent. We demonstrate that the LLM-based Evaluator empirically aligns well with human evaluation, allowing reliable automatic evaluation on the proposed benchmark. We further conduct experiments on several baseline LLM agents on the proposed benchmark and demonstrate the complexity of the task, indicating that applying LLMs to real-world dataset curation still requires further in-depth exploration and innovation. Finally, the proposed benchmark can also serve as a testbed for measuring the capability of LLMs in problem discovery rather than just problem-solving. The benchmark suite is available at \\url{https://github.com/TRAIS-Lab/dca-bench}.","sentences":["The quality of datasets plays an increasingly crucial role in the research and development of modern artificial intelligence (AI).","Despite the proliferation of open dataset platforms nowadays, data quality issues, such as insufficient documentation, inaccurate annotations, and ethical concerns, remain common in datasets widely used in AI.","Furthermore, these issues are often subtle and difficult to be detected by rule-based scripts, requiring expensive manual identification and verification by dataset users or maintainers.","With the increasing capability of large language models (LLMs), it is promising to streamline the curation of datasets with LLM agents.","In this work, as the initial step towards this goal, we propose a dataset curation agent benchmark, DCA-Bench, to measure LLM agents' capability of detecting hidden dataset quality issues.","Specifically, we collect diverse real-world dataset quality issues from eight open dataset platforms as a testbed.","Additionally, to establish an automatic pipeline for evaluating the success of LLM agents, which requires a nuanced understanding of the agent outputs, we implement a dedicated Evaluator using another LLM agent.","We demonstrate that the LLM-based Evaluator empirically aligns well with human evaluation, allowing reliable automatic evaluation on the proposed benchmark.","We further conduct experiments on several baseline LLM agents on the proposed benchmark and demonstrate the complexity of the task, indicating that applying LLMs to real-world dataset curation still requires further in-depth exploration and innovation.","Finally, the proposed benchmark can also serve as a testbed for measuring the capability of LLMs in problem discovery rather than just problem-solving.","The benchmark suite is available at \\url{https://github.com/TRAIS-Lab/dca-bench}."],"url":"http://arxiv.org/abs/2406.07275v1","category":"cs.AI"}
{"created":"2024-06-11 14:01:22","title":"Improved criteria of detecting multipartite entanglement structure","abstract":"Multipartite entanglement is one of the crucial resources in quantum information processing tasks such as quantum metrology, quantum computing and quantum communications. It is essential to verify not only the multipartite entanglement, but also the entanglement structure in both fundamental theories and the applications of quantum information technologies. However, it is proved to be challenging to detect the entanglement structures, including entanglement depth, entanglement intactness and entanglement stretchability, especially for general states and large-scale quantum systems. By using the partitions of the tensor product space we propose a systematic method to construct powerful entanglement witnesses which identify better the multipartite entanglement structures. Besides, an efficient algorithm using semi-definite programming and a gradient descent algorithm are designed to detect entanglement structure from the inner polytope of the convex set containing all the states with the same entanglement structure. We demonstrate by detailed examples that our criteria perform better than other known ones. Our results may be applied to many quantum information processing tasks.","sentences":["Multipartite entanglement is one of the crucial resources in quantum information processing tasks such as quantum metrology, quantum computing and quantum communications.","It is essential to verify not only the multipartite entanglement, but also the entanglement structure in both fundamental theories and the applications of quantum information technologies.","However, it is proved to be challenging to detect the entanglement structures, including entanglement depth, entanglement intactness and entanglement stretchability, especially for general states and large-scale quantum systems.","By using the partitions of the tensor product space we propose a systematic method to construct powerful entanglement witnesses which identify better the multipartite entanglement structures.","Besides, an efficient algorithm using semi-definite programming and a gradient descent algorithm are designed to detect entanglement structure from the inner polytope of the convex set containing all the states with the same entanglement structure.","We demonstrate by detailed examples that our criteria perform better than other known ones.","Our results may be applied to many quantum information processing tasks."],"url":"http://arxiv.org/abs/2406.07274v1","category":"quant-ph"}
{"created":"2024-06-11 13:57:27","title":"Integrated Near Field Sensing and Communications Using Unitary Approximate Message Passing Based Matrix Factorization","abstract":"Due to the utilization of large antenna arrays at base stations (BSs) and the operations of wireless communications in high frequency bands, mobile terminals often find themselves in the near-field of the array aperture. In this work, we address the signal processing challenges of integrated near-field localization and communication in uplink transmission of an integrated sensing and communication (ISAC) system, where the BS performs joint near-field localization and signal detection (JNFLSD). We show that JNFLSD can be formulated as a matrix factorization (MF) problem with proper structures imposed on the factor matrices. Then, leveraging the variational inference (VI) and unitary approximate message passing (UAMP), we develop a low complexity Bayesian approach to MF, called UAMP-MF, to handle a generic MF problem. We then apply the UAMP-MF algorithm to solve the JNFLSD problem, where the factor matrix structures are fully exploited. Extensive simulation results are provided to demonstrate the superior performance of the proposed method.","sentences":["Due to the utilization of large antenna arrays at base stations (BSs) and the operations of wireless communications in high frequency bands, mobile terminals often find themselves in the near-field of the array aperture.","In this work, we address the signal processing challenges of integrated near-field localization and communication in uplink transmission of an integrated sensing and communication (ISAC) system, where the BS performs joint near-field localization and signal detection (JNFLSD).","We show that JNFLSD can be formulated as a matrix factorization (MF) problem with proper structures imposed on the factor matrices.","Then, leveraging the variational inference (VI) and unitary approximate message passing (UAMP), we develop a low complexity Bayesian approach to MF, called UAMP-MF, to handle a generic MF problem.","We then apply the UAMP-MF algorithm to solve the JNFLSD problem, where the factor matrix structures are fully exploited.","Extensive simulation results are provided to demonstrate the superior performance of the proposed method."],"url":"http://arxiv.org/abs/2406.07272v1","category":"cs.IT"}
{"created":"2024-06-11 13:55:37","title":"3D Voxel Maps to 2D Occupancy Maps for Efficient Path Planning for Aerial and Ground Robots","abstract":"This article introduces a novel method for converting 3D voxel maps, commonly utilized by robots for localization and navigation, into 2D occupancy maps that can be used for more computationally efficient large-scale navigation, both in the sense of computation time and memory usage. The main aim is to effectively integrate the distinct mapping advantages of 2D and 3D maps to enable efficient path planning for both unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). The proposed method uses the free space representation in the UFOMap mapping solution to generate 2D occupancy maps with height and slope information. In the process of 3D to 2D map conversion, the proposed method conducts safety checks and eliminates free spaces in the map with dimensions (in the height axis) lower than the robot's safety margins. This allows an aerial or ground robot to navigate safely, relying primarily on the 2D map generated by the method. Additionally, the method extracts height and slope data from the 3D voxel map. The slope data identifies areas too steep for a ground robot to traverse, marking them as occupied, thus enabling a more accurate representation of the terrain for ground robots. The height data is utilized to convert paths generated using the 2D map into paths in 3D space for both UAVs and UGVs. The effectiveness of the proposed method is evaluated in two different environments.","sentences":["This article introduces a novel method for converting 3D voxel maps, commonly utilized by robots for localization and navigation, into 2D occupancy maps that can be used for more computationally efficient large-scale navigation, both in the sense of computation time and memory usage.","The main aim is to effectively integrate the distinct mapping advantages of 2D and 3D maps to enable efficient path planning for both unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs).","The proposed method uses the free space representation in the UFOMap mapping solution to generate 2D occupancy maps with height and slope information.","In the process of 3D to 2D map conversion, the proposed method conducts safety checks and eliminates free spaces in the map with dimensions (in the height axis) lower than the robot's safety margins.","This allows an aerial or ground robot to navigate safely, relying primarily on the 2D map generated by the method.","Additionally, the method extracts height and slope data from the 3D voxel map.","The slope data identifies areas too steep for a ground robot to traverse, marking them as occupied, thus enabling a more accurate representation of the terrain for ground robots.","The height data is utilized to convert paths generated using the 2D map into paths in 3D space for both UAVs and UGVs.","The effectiveness of the proposed method is evaluated in two different environments."],"url":"http://arxiv.org/abs/2406.07270v1","category":"cs.RO"}
{"created":"2024-06-11 13:53:27","title":"The geometry of efficient codes: how rate-distortion trade-offs distort the latent representations of generative models","abstract":"Living organisms rely on internal models of the world to act adaptively. These models cannot encode every detail and hence need to compress information. From a cognitive standpoint, information compression can manifest as a distortion of latent representations, resulting in the emergence of representations that may not accurately reflect the external world or its geometry. Rate-distortion theory formalizes the optimal way to compress information, by considering factors such as capacity limitations, the frequency and the utility of stimuli. However, while this theory explains why the above factors distort latent representations, it does not specify which specific distortions they produce. To address this question, here we systematically explore the geometry of the latent representations that emerge in generative models that operate under the principles of rate-distortion theory ($\\beta$-VAEs). Our results highlight that three main classes of distortions of internal representations -- prototypization, specialization, orthogonalization -- emerge as signatures of information compression, under constraints on capacity, data distributions and tasks. These distortions can coexist, giving rise to a rich landscape of latent spaces, whose geometry could differ significantly across generative models subject to different constraints. Our findings contribute to explain how the normative constraints of rate-distortion theory distort the geometry of latent representations of generative models of artificial systems and living organisms.","sentences":["Living organisms rely on internal models of the world to act adaptively.","These models cannot encode every detail and hence need to compress information.","From a cognitive standpoint, information compression can manifest as a distortion of latent representations, resulting in the emergence of representations that may not accurately reflect the external world or its geometry.","Rate-distortion theory formalizes the optimal way to compress information, by considering factors such as capacity limitations, the frequency and the utility of stimuli.","However, while this theory explains why the above factors distort latent representations, it does not specify which specific distortions they produce.","To address this question, here we systematically explore the geometry of the latent representations that emerge in generative models that operate under the principles of rate-distortion theory ($\\beta$-VAEs).","Our results highlight that three main classes of distortions of internal representations -- prototypization, specialization, orthogonalization -- emerge as signatures of information compression, under constraints on capacity, data distributions and tasks.","These distortions can coexist, giving rise to a rich landscape of latent spaces, whose geometry could differ significantly across generative models subject to different constraints.","Our findings contribute to explain how the normative constraints of rate-distortion theory distort the geometry of latent representations of generative models of artificial systems and living organisms."],"url":"http://arxiv.org/abs/2406.07269v1","category":"q-bio.NC"}
{"created":"2024-06-11 13:52:29","title":"Advancing Grounded Multimodal Named Entity Recognition via LLM-Based Reformulation and Box-Based Segmentation","abstract":"Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging attributes: 1) The tenuous correlation between images and text on social media contributes to a notable proportion of named entities being ungroundable. 2) There exists a distinction between coarse-grained noun phrases used in similar tasks (e.g., phrase localization) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as connecting bridges. This reformulation brings two benefits: 1) It enables us to optimize the MNER module for optimal MNER performance and eliminates the need to pre-extract region features using object detection methods, thus naturally addressing the two major limitations of existing GMNER methods. 2) The introduction of Entity Expansion Expression module and Visual Entailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG). This endows the proposed framework with unlimited data and model scalability. Furthermore, to address the potential ambiguity stemming from the coarse-grained bounding box output in GMNER, we further construct the new Segmented Multimodal Named Entity Recognition (SMNER) task and corresponding Twitter-SMNER dataset aimed at generating fine-grained segmentation masks, and experimentally demonstrate the feasibility and effectiveness of using box prompt-based Segment Anything Model (SAM) to empower any GMNER model with the ability to accomplish the SMNER task. Extensive experiments demonstrate that RiVEG significantly outperforms SoTA methods on four datasets across the MNER, GMNER, and SMNER tasks.","sentences":["Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify named entities, entity types and their corresponding visual regions.","GMNER task exhibits two challenging attributes: 1) The tenuous correlation between images and text on social media contributes to a notable proportion of named entities being ungroundable.","2) There exists a distinction between coarse-grained noun phrases used in similar tasks (e.g., phrase localization) and fine-grained named entities.","In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as connecting bridges.","This reformulation brings two benefits: 1) It enables us to optimize the MNER module for optimal MNER performance and eliminates the need to pre-extract region features using object detection methods, thus naturally addressing the two major limitations of existing GMNER methods.","2)","The introduction of Entity Expansion Expression module and Visual Entailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG).","This endows the proposed framework with unlimited data and model scalability.","Furthermore, to address the potential ambiguity stemming from the coarse-grained bounding box output in GMNER, we further construct the new Segmented Multimodal Named Entity Recognition (SMNER) task and corresponding Twitter-SMNER dataset aimed at generating fine-grained segmentation masks, and experimentally demonstrate the feasibility and effectiveness of using box prompt-based Segment Anything Model (SAM) to empower any GMNER model with the ability to accomplish the SMNER task.","Extensive experiments demonstrate that RiVEG significantly outperforms SoTA methods on four datasets across the MNER, GMNER, and SMNER tasks."],"url":"http://arxiv.org/abs/2406.07268v1","category":"cs.MM"}
{"created":"2024-06-11 13:51:51","title":"Efficient 3D Molecular Generation with Flow Matching and Scale Optimal Transport","abstract":"Generative models for 3D drug design have gained prominence recently for their potential to design ligands directly within protein pockets. Current approaches, however, often suffer from very slow sampling times or generate molecules with poor chemical validity. Addressing these limitations, we propose Semla, a scalable E(3)-equivariant message passing architecture. We further introduce a molecular generation model, MolFlow, which is trained using flow matching along with scale optimal transport, a novel extension of equivariant optimal transport. Our model produces state-of-the-art results on benchmark datasets with just 100 sampling steps. Crucially, MolFlow samples high quality molecules with as few as 20 steps, corresponding to a two order-of-magnitude speed-up compared to state-of-the-art, without sacrificing performance. Furthermore, we highlight limitations of current evaluation methods for 3D generation and propose new benchmark metrics for unconditional molecular generators. Finally, using these new metrics, we compare our model's ability to generate high quality samples against current approaches and further demonstrate MolFlow's strong performance.","sentences":["Generative models for 3D drug design have gained prominence recently for their potential to design ligands directly within protein pockets.","Current approaches, however, often suffer from very slow sampling times or generate molecules with poor chemical validity.","Addressing these limitations, we propose Semla, a scalable E(3)-equivariant message passing architecture.","We further introduce a molecular generation model, MolFlow, which is trained using flow matching along with scale optimal transport, a novel extension of equivariant optimal transport.","Our model produces state-of-the-art results on benchmark datasets with just 100 sampling steps.","Crucially, MolFlow samples high quality molecules with as few as 20 steps, corresponding to a two order-of-magnitude speed-up compared to state-of-the-art, without sacrificing performance.","Furthermore, we highlight limitations of current evaluation methods for 3D generation and propose new benchmark metrics for unconditional molecular generators.","Finally, using these new metrics, we compare our model's ability to generate high quality samples against current approaches and further demonstrate MolFlow's strong performance."],"url":"http://arxiv.org/abs/2406.07266v1","category":"cs.LG"}
{"created":"2024-06-11 13:48:36","title":"Remarks on $q$-difference opers arising from quantum toroidal algebras","abstract":"We propose a conjectural correspondence between the spectra of the Bethe algebra for the quantum toroidal $\\mathfrak{gl}_2$ algebra on relaxed Verma modules, and $q$-hypergeometric opers with apparent singularities. We introduce alongside the notion of apparent singularities for linear $q$-difference operators and discuss some of their properties. We also touch on a generalization to $\\mathfrak{gl}_n$.","sentences":["We propose a conjectural correspondence between the spectra of the Bethe algebra for the quantum toroidal $\\mathfrak{gl}_2$ algebra on relaxed Verma modules, and $q$-hypergeometric opers with apparent singularities.","We introduce alongside the notion of apparent singularities for linear $q$-difference operators and discuss some of their properties.","We also touch on a generalization to $\\mathfrak{gl}_n$."],"url":"http://arxiv.org/abs/2406.07265v1","category":"math.QA"}
{"created":"2024-06-11 13:39:35","title":"Evidence of surface $p$-wave superconductivity and higher-order topology in MoTe$_2$","abstract":"Exploration of nontrivial superconductivity and electronic band topology is at the core of condensed matter physics and applications to quantum information. The transition-metal dichalcogenide (TMDC) MoTe$_2$ has been proposed as an ideal candidate to explore the interplay between topology and superconductivity, but their studies remain limited because of the high-pressure environments required to control the topological phase transition. In this work, we demonstrate the tunable superconductivity and the resultant higher-order topology of MoTe$_2$ under extreme pressure. In the pressured T$_d$ phase, Andreev reflection spectroscopy reveals two-gap features, indicating that the Weyl fermions lead to a topological $s^{\\pm}$-wave multigap superconductivity. On the other hand, the high-pressure 1T$'$ phase presents $p$-wave surface superconductivity emergent from the second-order topological bands via the bulk-to-surface proximity effect. Our analysis suggests that the topological hinge states generated from second-order topological bands evolve into zero-energy Majorana hinge states in the second-order topological superconductor. These results demonstrate the potential realization of topological superconductivity in MoTe$_2$, thus opening a pathway for studying various topological natures of TMDC materials.","sentences":["Exploration of nontrivial superconductivity and electronic band topology is at the core of condensed matter physics and applications to quantum information.","The transition-metal dichalcogenide (TMDC) MoTe$_2$ has been proposed as an ideal candidate to explore the interplay between topology and superconductivity, but their studies remain limited because of the high-pressure environments required to control the topological phase transition.","In this work, we demonstrate the tunable superconductivity and the resultant higher-order topology of MoTe$_2$ under extreme pressure.","In the pressured T$_d$ phase, Andreev reflection spectroscopy reveals two-gap features, indicating that the Weyl fermions lead to a topological $s^{\\pm}$-wave multigap superconductivity.","On the other hand, the high-pressure 1T$'$ phase presents $p$-wave surface superconductivity emergent from the second-order topological bands via the bulk-to-surface proximity effect.","Our analysis suggests that the topological hinge states generated from second-order topological bands evolve into zero-energy Majorana hinge states in the second-order topological superconductor.","These results demonstrate the potential realization of topological superconductivity in MoTe$_2$, thus opening a pathway for studying various topological natures of TMDC materials."],"url":"http://arxiv.org/abs/2406.07260v1","category":"cond-mat.supr-con"}
{"created":"2024-06-11 13:39:07","title":"Scientific Computing with Large Language Models","abstract":"We provide an overview of the emergence of large language models for scientific computing applications. We highlight use cases that involve natural language processing of scientific documents and specialized languages designed to describe physical systems. For the former, chatbot style applications appear in medicine, mathematics and physics and can be used iteratively with domain experts for problem solving. We also review specialized languages within molecular biology, the languages of molecules, proteins, and DNA where language models are being used to predict properties and even create novel physical systems at much faster rates than traditional computing methods.","sentences":["We provide an overview of the emergence of large language models for scientific computing applications.","We highlight use cases that involve natural language processing of scientific documents and specialized languages designed to describe physical systems.","For the former, chatbot style applications appear in medicine, mathematics and physics and can be used iteratively with domain experts for problem solving.","We also review specialized languages within molecular biology, the languages of molecules, proteins, and DNA where language models are being used to predict properties and even create novel physical systems at much faster rates than traditional computing methods."],"url":"http://arxiv.org/abs/2406.07259v1","category":"cs.CL"}
{"created":"2024-06-11 13:36:19","title":"Scholarly Question Answering using Large Language Models in the NFDI4DataScience Gateway","abstract":"This paper introduces a scholarly Question Answering (QA) system on top of the NFDI4DataScience Gateway, employing a Retrieval Augmented Generation-based (RAG) approach. The NFDI4DS Gateway, as a foundational framework, offers a unified and intuitive interface for querying various scientific databases using federated search. The RAG-based scholarly QA, powered by a Large Language Model (LLM), facilitates dynamic interaction with search results, enhancing filtering capabilities and fostering a conversational engagement with the Gateway search. The effectiveness of both the Gateway and the scholarly QA system is demonstrated through experimental analysis.","sentences":["This paper introduces a scholarly Question Answering (QA) system on top of the NFDI4DataScience Gateway, employing a Retrieval Augmented Generation-based (RAG) approach.","The NFDI4DS Gateway, as a foundational framework, offers a unified and intuitive interface for querying various scientific databases using federated search.","The RAG-based scholarly QA, powered by a Large Language Model (LLM), facilitates dynamic interaction with search results, enhancing filtering capabilities and fostering a conversational engagement with the Gateway search.","The effectiveness of both the Gateway and the scholarly QA system is demonstrated through experimental analysis."],"url":"http://arxiv.org/abs/2406.07257v1","category":"cs.CL"}
{"created":"2024-06-11 13:35:50","title":"AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection","abstract":"The rapid advancements in speech technologies over the past two decades have led to human-level performance in tasks like automatic speech recognition (ASR) for fluent speech. However, the efficacy of these models diminishes when applied to atypical speech, such as stuttering. This paper introduces AS-70, the first publicly available Mandarin stuttered speech dataset, which stands out as the largest dataset in its category. Encompassing conversational and voice command reading speech, AS-70 includes verbatim manual transcription, rendering it suitable for various speech-related tasks. Furthermore, baseline systems are established, and experimental results are presented for ASR and stuttering event detection (SED) tasks. By incorporating this dataset into the model fine-tuning, significant improvements in the state-of-the-art ASR models, e.g., Whisper and Hubert, are observed, enhancing their inclusivity in addressing stuttered speech.","sentences":["The rapid advancements in speech technologies over the past two decades have led to human-level performance in tasks like automatic speech recognition (ASR) for fluent speech.","However, the efficacy of these models diminishes when applied to atypical speech, such as stuttering.","This paper introduces AS-70, the first publicly available Mandarin stuttered speech dataset, which stands out as the largest dataset in its category.","Encompassing conversational and voice command reading speech, AS-70 includes verbatim manual transcription, rendering it suitable for various speech-related tasks.","Furthermore, baseline systems are established, and experimental results are presented for ASR and stuttering event detection (SED) tasks.","By incorporating this dataset into the model fine-tuning, significant improvements in the state-of-the-art ASR models, e.g., Whisper and Hubert, are observed, enhancing their inclusivity in addressing stuttered speech."],"url":"http://arxiv.org/abs/2406.07256v1","category":"cs.SD"}
{"created":"2024-06-11 13:34:57","title":"Towards Realistic Data Generation for Real-World Super-Resolution","abstract":"Existing image super-resolution (SR) techniques often fail to generalize effectively in complex real-world settings due to the significant divergence between training data and practical scenarios. To address this challenge, previous efforts have either manually simulated intricate physical-based degradations or utilized learning-based techniques, yet these approaches remain inadequate for producing large-scale, realistic, and diverse data simultaneously. In this paper, we introduce a novel Realistic Decoupled Data Generator (RealDGen), an unsupervised learning data generation framework designed for real-world super-resolution. We meticulously develop content and degradation extraction strategies, which are integrated into a novel content-degradation decoupled diffusion model to create realistic low-resolution images from unpaired real LR and HR images. Extensive experiments demonstrate that RealDGen excels in generating large-scale, high-quality paired data that mirrors real-world degradations, significantly advancing the performance of popular SR models on various real-world benchmarks.","sentences":["Existing image super-resolution (SR) techniques often fail to generalize effectively in complex real-world settings due to the significant divergence between training data and practical scenarios.","To address this challenge, previous efforts have either manually simulated intricate physical-based degradations or utilized learning-based techniques, yet these approaches remain inadequate for producing large-scale, realistic, and diverse data simultaneously.","In this paper, we introduce a novel Realistic Decoupled Data Generator (RealDGen), an unsupervised learning data generation framework designed for real-world super-resolution.","We meticulously develop content and degradation extraction strategies, which are integrated into a novel content-degradation decoupled diffusion model to create realistic low-resolution images from unpaired real LR and HR images.","Extensive experiments demonstrate that RealDGen excels in generating large-scale, high-quality paired data that mirrors real-world degradations, significantly advancing the performance of popular SR models on various real-world benchmarks."],"url":"http://arxiv.org/abs/2406.07255v1","category":"cs.CV"}
{"created":"2024-06-11 13:34:05","title":"Hybrid Reinforcement Learning from Offline Observation Alone","abstract":"We consider the hybrid reinforcement learning setting where the agent has access to both offline data and online interactive access. While Reinforcement Learning (RL) research typically assumes offline data contains complete action, reward and transition information, datasets with only state information (also known as observation-only datasets) are more general, abundant and practical. This motivates our study of the hybrid RL with observation-only offline dataset framework. While the task of competing with the best policy \"covered\" by the offline data can be solved if a reset model of the environment is provided (i.e., one that can be reset to any state), we show evidence of hardness when only given the weaker trace model (i.e., one can only reset to the initial states and must produce full traces through the environment), without further assumption of admissibility of the offline data. Under the admissibility assumptions -- that the offline data could actually be produced by the policy class we consider -- we propose the first algorithm in the trace model setting that provably matches the performance of algorithms that leverage a reset model. We also perform proof-of-concept experiments that suggest the effectiveness of our algorithm in practice.","sentences":["We consider the hybrid reinforcement learning setting where the agent has access to both offline data and online interactive access.","While Reinforcement Learning (RL) research typically assumes offline data contains complete action, reward and transition information, datasets with only state information (also known as observation-only datasets) are more general, abundant and practical.","This motivates our study of the hybrid RL with observation-only offline dataset framework.","While the task of competing with the best policy \"covered\" by the offline data can be solved if a reset model of the environment is provided (i.e., one that can be reset to any state), we show evidence of hardness when only given the weaker trace model (i.e., one can only reset to the initial states and must produce full traces through the environment), without further assumption of admissibility of the offline data.","Under the admissibility assumptions -- that the offline data could actually be produced by the policy class we consider -- we propose the first algorithm in the trace model setting that provably matches the performance of algorithms that leverage a reset model.","We also perform proof-of-concept experiments that suggest the effectiveness of our algorithm in practice."],"url":"http://arxiv.org/abs/2406.07253v1","category":"cs.LG"}
{"created":"2024-06-11 13:33:33","title":"Is One GPU Enough? Pushing Image Generation at Higher-Resolutions with Foundation Models","abstract":"In this work, we introduce Pixelsmith, a zero-shot text-to-image generative framework to sample images at higher resolutions with a single GPU. We are the first to show that it is possible to scale the output of a pre-trained diffusion model by a factor of 1000, opening the road for gigapixel image generation at no additional cost. Our cascading method uses the image generated at the lowest resolution as a baseline to sample at higher resolutions. For the guidance, we introduce the Slider, a tunable mechanism that fuses the overall structure contained in the first-generated image with enhanced fine details. At each inference step, we denoise patches rather than the entire latent space, minimizing memory demands such that a single GPU can handle the process, regardless of the image's resolution. Our experimental results show that Pixelsmith not only achieves higher quality and diversity compared to existing techniques, but also reduces sampling time and artifacts. The code for our work is available at https://github.com/Thanos-DB/Pixelsmith.","sentences":["In this work, we introduce Pixelsmith, a zero-shot text-to-image generative framework to sample images at higher resolutions with a single GPU.","We are the first to show that it is possible to scale the output of a pre-trained diffusion model by a factor of 1000, opening the road for gigapixel image generation at no additional cost.","Our cascading method uses the image generated at the lowest resolution as a baseline to sample at higher resolutions.","For the guidance, we introduce the Slider, a tunable mechanism that fuses the overall structure contained in the first-generated image with enhanced fine details.","At each inference step, we denoise patches rather than the entire latent space, minimizing memory demands such that a single GPU can handle the process, regardless of the image's resolution.","Our experimental results show that Pixelsmith not only achieves higher quality and diversity compared to existing techniques, but also reduces sampling time and artifacts.","The code for our work is available at https://github.com/Thanos-DB/Pixelsmith."],"url":"http://arxiv.org/abs/2406.07251v1","category":"cs.CV"}
{"created":"2024-06-11 13:32:40","title":"Description and Discussion on DCASE 2024 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring","abstract":"We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge Task 2: First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring. Continuing from last year's DCASE 2023 Challenge Task 2, we organize the task as a first-shot problem under domain generalization required settings. The main goal of the first-shot problem is to enable rapid deployment of ASD systems for new kinds of machines without the need for machine-specific hyperparameter tunings. This problem setting was realized by (1) giving only one section for each machine type and (2) having completely different machine types for the development and evaluation datasets. For the DCASE 2024 Challenge Task 2, data of completely new machine types were newly collected and provided as the evaluation dataset. In addition, attribute information such as the machine operation conditions were concealed for several machine types to mimic situations where such information are unavailable. We will add challenge results and analysis of the submissions after the challenge submission deadline.","sentences":["We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 Challenge Task 2: First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring.","Continuing from last year's DCASE 2023 Challenge Task 2, we organize the task as a first-shot problem under domain generalization required settings.","The main goal of the first-shot problem is to enable rapid deployment of ASD systems for new kinds of machines without the need for machine-specific hyperparameter tunings.","This problem setting was realized by (1) giving only one section for each machine type and (2) having completely different machine types for the development and evaluation datasets.","For the DCASE 2024 Challenge Task 2, data of completely new machine types were newly collected and provided as the evaluation dataset.","In addition, attribute information such as the machine operation conditions were concealed for several machine types to mimic situations where such information are unavailable.","We will add challenge results and analysis of the submissions after the challenge submission deadline."],"url":"http://arxiv.org/abs/2406.07250v1","category":"eess.AS"}
{"created":"2024-06-11 13:32:11","title":"Are Protein Language Models Compute Optimal?","abstract":"While protein language models (pLMs) have transformed biological research, the scaling laws governing their improvement remain underexplored. By adapting methodologies from NLP scaling laws, we investigated the optimal ratio between model parameters and training tokens within a fixed compute budget. Our study reveals that pLM sizes scale sublinearly with compute budget, showing diminishing returns in performance as model size increases, and we identify a performance plateau in training loss comparable to the one found in relevant works in the field. Our findings suggest that widely-used pLMs might not be compute-optimal, indicating that larger models could achieve convergence more efficiently. Training a 35M model on a reduced token set, we attained perplexity results comparable to larger models like ESM-2 (15B) and xTrimoPGLM (100B) with a single dataset pass. This work paves the way towards more compute-efficient pLMs, democratizing their training and practical application in computational biology.","sentences":["While protein language models (pLMs) have transformed biological research, the scaling laws governing their improvement remain underexplored.","By adapting methodologies from NLP scaling laws, we investigated the optimal ratio between model parameters and training tokens within a fixed compute budget.","Our study reveals that pLM sizes scale sublinearly with compute budget, showing diminishing returns in performance as model size increases, and we identify a performance plateau in training loss comparable to the one found in relevant works in the field.","Our findings suggest that widely-used pLMs might not be compute-optimal, indicating that larger models could achieve convergence more efficiently.","Training a 35M model on a reduced token set, we attained perplexity results comparable to larger models like ESM-2 (15B) and xTrimoPGLM (100B) with a single dataset pass.","This work paves the way towards more compute-efficient pLMs, democratizing their training and practical application in computational biology."],"url":"http://arxiv.org/abs/2406.07249v1","category":"q-bio.BM"}
{"created":"2024-06-11 13:29:34","title":"Dynamical Mean-Field Theory of Self-Attention Neural Networks","abstract":"Transformer-based models have demonstrated exceptional performance across diverse domains, becoming the state-of-the-art solution for addressing sequential machine learning problems. Even though we have a general understanding of the fundamental components in the transformer architecture, little is known about how they operate or what are their expected dynamics. Recently, there has been an increasing interest in exploring the relationship between attention mechanisms and Hopfield networks, promising to shed light on the statistical physics of transformer networks. However, to date, the dynamical regimes of transformer-like models have not been studied in depth. In this paper, we address this gap by using methods for the study of asymmetric Hopfield networks in nonequilibrium regimes --namely path integral methods over generating functionals, yielding dynamics governed by concurrent mean-field variables. Assuming 1-bit tokens and weights, we derive analytical approximations for the behavior of large self-attention neural networks coupled to a softmax output, which become exact in the large limit size. Our findings reveal nontrivial dynamical phenomena, including nonequilibrium phase transitions associated with chaotic bifurcations, even for very simple configurations with a few encoded features and a very short context window. Finally, we discuss the potential of our analytic approach to improve our understanding of the inner workings of transformer models, potentially reducing computational training costs and enhancing model interpretability.","sentences":["Transformer-based models have demonstrated exceptional performance across diverse domains, becoming the state-of-the-art solution for addressing sequential machine learning problems.","Even though we have a general understanding of the fundamental components in the transformer architecture, little is known about how they operate or what are their expected dynamics.","Recently, there has been an increasing interest in exploring the relationship between attention mechanisms and Hopfield networks, promising to shed light on the statistical physics of transformer networks.","However, to date, the dynamical regimes of transformer-like models have not been studied in depth.","In this paper, we address this gap by using methods for the study of asymmetric Hopfield networks in nonequilibrium regimes --namely path integral methods over generating functionals, yielding dynamics governed by concurrent mean-field variables.","Assuming 1-bit tokens and weights, we derive analytical approximations for the behavior of large self-attention neural networks coupled to a softmax output, which become exact in the large limit size.","Our findings reveal nontrivial dynamical phenomena, including nonequilibrium phase transitions associated with chaotic bifurcations, even for very simple configurations with a few encoded features and a very short context window.","Finally, we discuss the potential of our analytic approach to improve our understanding of the inner workings of transformer models, potentially reducing computational training costs and enhancing model interpretability."],"url":"http://arxiv.org/abs/2406.07247v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-11 13:27:36","title":"Collisionless shock in a relativistically hot unmagnetized electron-positron plasma","abstract":"In this work, we investigate collisionless shocks propagating in a relativistically hot unmagnetized electron-positron plasmas. We estimate the dissipation fraction at shocks in the relativistically hot plasma, showing that it is sufficiently large to explain the observation of gamma-ray bursts even when the shock is not highly relativistic. It is shown by two-dimensional particle in cell simulations that magnetic fields are generated around the shock front by the Weibel instability, as in the cold upstream plasma. However, in contrast to the cold upstream plasma, no particles are accelerated at the shock in the simulation time of $t = 3600 \\omega_p^{-1}$. The decay of the magnetic field in the downstream region is slower for slower shock velocities in the hot plasma cases. Applying the slow decay of the downstream magnetic field, we propose a model that generate magnetic fields in large downstream region, which is required from the standard model of the gamma-ray burst afterglow.","sentences":["In this work, we investigate collisionless shocks propagating in a relativistically hot unmagnetized electron-positron plasmas.","We estimate the dissipation fraction at shocks in the relativistically hot plasma, showing that it is sufficiently large to explain the observation of gamma-ray bursts even when the shock is not highly relativistic.","It is shown by two-dimensional particle in cell simulations that magnetic fields are generated around the shock front by the Weibel instability, as in the cold upstream plasma.","However, in contrast to the cold upstream plasma, no particles are accelerated at the shock in the simulation time of $t = 3600 \\omega_p^{-1}$. The decay of the magnetic field in the downstream region is slower for slower shock velocities in the hot plasma cases.","Applying the slow decay of the downstream magnetic field, we propose a model that generate magnetic fields in large downstream region, which is required from the standard model of the gamma-ray burst afterglow."],"url":"http://arxiv.org/abs/2406.07244v1","category":"astro-ph.HE"}
{"created":"2024-06-11 13:23:14","title":"MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs","abstract":"Generative large language models (LLMs) have been shown to exhibit harmful biases and stereotypes. While safety fine-tuning typically takes place in English, if at all, these models are being used by speakers of many different languages. There is existing evidence that the performance of these models is inconsistent across languages and that they discriminate based on demographic factors of the user. Motivated by this, we investigate whether the social stereotypes exhibited by LLMs differ as a function of the language used to prompt them, while controlling for cultural differences and task accuracy. To this end, we present MBBQ (Multilingual Bias Benchmark for Question-answering), a carefully curated version of the English BBQ dataset extended to Dutch, Spanish, and Turkish, which measures stereotypes commonly held across these languages. We further complement MBBQ with a parallel control dataset to measure task performance on the question-answering task independently of bias. Our results based on several open-source and proprietary LLMs confirm that some non-English languages suffer from bias more than English, even when controlling for cultural shifts. Moreover, we observe significant cross-lingual differences in bias behaviour for all except the most accurate models. With the release of MBBQ, we hope to encourage further research on bias in multilingual settings. The dataset and code are available at https://github.com/Veranep/MBBQ.","sentences":["Generative large language models (LLMs) have been shown to exhibit harmful biases and stereotypes.","While safety fine-tuning typically takes place in English, if at all, these models are being used by speakers of many different languages.","There is existing evidence that the performance of these models is inconsistent across languages and that they discriminate based on demographic factors of the user.","Motivated by this, we investigate whether the social stereotypes exhibited by LLMs differ as a function of the language used to prompt them, while controlling for cultural differences and task accuracy.","To this end, we present MBBQ (Multilingual Bias Benchmark for Question-answering), a carefully curated version of the English BBQ dataset extended to Dutch, Spanish, and Turkish, which measures stereotypes commonly held across these languages.","We further complement MBBQ with a parallel control dataset to measure task performance on the question-answering task independently of bias.","Our results based on several open-source and proprietary LLMs confirm that some non-English languages suffer from bias more than English, even when controlling for cultural shifts.","Moreover, we observe significant cross-lingual differences in bias behaviour for all except the most accurate models.","With the release of MBBQ, we hope to encourage further research on bias in multilingual settings.","The dataset and code are available at https://github.com/Veranep/MBBQ."],"url":"http://arxiv.org/abs/2406.07243v1","category":"cs.CL"}
{"created":"2024-06-11 13:16:09","title":"CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems","abstract":"Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic anyone's voice with just a 3-second sample from that specific unseen speaker. Unfortunately, malicious attackers may exploit these technologies, causing misuse and security issues. Anti-spoofing models have been developed to detect fake speech. However, the open question of whether current SOTA anti-spoofing models can effectively counter deepfake audios from codec-based speech synthesis systems remains unanswered. In this paper, we curate an extensive collection of contemporary SOTA codec models, employing them to re-create synthesized speech. This endeavor leads to the creation of CodecFake, the first codec-based deepfake audio dataset. Additionally, we verify that anti-spoofing models trained on commonly used datasets cannot detect synthesized speech from current codec-based speech generation systems. The proposed CodecFake dataset empowers these models to counter this challenge effectively.","sentences":["Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic anyone's voice with just a 3-second sample from that specific unseen speaker.","Unfortunately, malicious attackers may exploit these technologies, causing misuse and security issues.","Anti-spoofing models have been developed to detect fake speech.","However, the open question of whether current SOTA anti-spoofing models can effectively counter deepfake audios from codec-based speech synthesis systems remains unanswered.","In this paper, we curate an extensive collection of contemporary SOTA codec models, employing them to re-create synthesized speech.","This endeavor leads to the creation of CodecFake, the first codec-based deepfake audio dataset.","Additionally, we verify that anti-spoofing models trained on commonly used datasets cannot detect synthesized speech from current codec-based speech generation systems.","The proposed CodecFake dataset empowers these models to counter this challenge effectively."],"url":"http://arxiv.org/abs/2406.07237v1","category":"eess.AS"}
{"created":"2024-06-11 13:13:56","title":"Non-equilibrium fluctuations of the direct cascade in Surface Quasi Geostrophic turbulence","abstract":"We study the temporal fluctuations of the flux of surface potential energy in Surface Quasi-Geostrophic (SQG) turbulence. By means of high-resolution, direct numerical simulations of the SQG model in the regime of forced and dissipated cascade of temperature variance, we show that the instantaneous imbalance in the energy budget originates a subleading correction to the spectrum of the turbulent cascade. Using a multiple-scale approach combined with a dimensional closure we derive a theoretical prediction for the power-law behavior of the corrections, which holds for a class of turbulent transport equations known as {\\alpha}-turbulence. Further, we develop and apply a method to disentangle the equilibrium and non-equilibrium contribution in the instantaneous spectra, which can be generalized to other turbulent systems.","sentences":["We study the temporal fluctuations of the flux of surface potential energy in Surface Quasi-Geostrophic (SQG) turbulence.","By means of high-resolution, direct numerical simulations of the SQG model in the regime of forced and dissipated cascade of temperature variance, we show that the instantaneous imbalance in the energy budget originates a subleading correction to the spectrum of the turbulent cascade.","Using a multiple-scale approach combined with a dimensional closure we derive a theoretical prediction for the power-law behavior of the corrections, which holds for a class of turbulent transport equations known as {\\alpha}-turbulence.","Further, we develop and apply a method to disentangle the equilibrium and non-equilibrium contribution in the instantaneous spectra, which can be generalized to other turbulent systems."],"url":"http://arxiv.org/abs/2406.07235v1","category":"physics.flu-dyn"}
{"created":"2024-06-11 13:12:39","title":"OPFData: Large-scale datasets for AC optimal power flow with topological perturbations","abstract":"Solving the AC optimal power flow problem (AC-OPF) is critical to the efficient and safe planning and operation of power grids. Small efficiency improvements in this domain have the potential to lead to billions of dollars of cost savings, and significant reductions in emissions from fossil fuel generators. Recent work on data-driven solution methods for AC-OPF shows the potential for large speed improvements compared to traditional solvers; however, no large-scale open datasets for this problem exist. We present the largest readily-available collection of solved AC-OPF problems to date. This collection is orders of magnitude larger than existing readily-available datasets, allowing training of high-capacity data-driven models. Uniquely, it includes topological perturbations - a critical requirement for usage in realistic power grid operations. We hope this resource will spur the community to scale research to larger grid sizes with variable topology.","sentences":["Solving the AC optimal power flow problem (AC-OPF) is critical to the efficient and safe planning and operation of power grids.","Small efficiency improvements in this domain have the potential to lead to billions of dollars of cost savings, and significant reductions in emissions from fossil fuel generators.","Recent work on data-driven solution methods for AC-OPF shows the potential for large speed improvements compared to traditional solvers; however, no large-scale open datasets for this problem exist.","We present the largest readily-available collection of solved AC-OPF problems to date.","This collection is orders of magnitude larger than existing readily-available datasets, allowing training of high-capacity data-driven models.","Uniquely, it includes topological perturbations - a critical requirement for usage in realistic power grid operations.","We hope this resource will spur the community to scale research to larger grid sizes with variable topology."],"url":"http://arxiv.org/abs/2406.07234v1","category":"cs.LG"}
{"created":"2024-06-11 13:10:39","title":"DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms","abstract":"Recently, large language models (LLMs) enhanced by self-reflection have achieved promising performance on machine translation. The key idea is guiding LLMs to generate translation with human-like feedback. However, existing self-reflection methods lack effective feedback information, limiting the translation performance. To address this, we introduce a DUAL-REFLECT framework, leveraging the dual learning of translation tasks to provide effective feedback, thereby enhancing the models' self-reflective abilities and improving translation performance. The application of this method across various translation tasks has proven its effectiveness in improving translation accuracy and eliminating ambiguities, especially in translation tasks with low-resource language pairs.","sentences":["Recently, large language models (LLMs) enhanced by self-reflection have achieved promising performance on machine translation.","The key idea is guiding LLMs to generate translation with human-like feedback.","However, existing self-reflection methods lack effective feedback information, limiting the translation performance.","To address this, we introduce a DUAL-REFLECT framework, leveraging the dual learning of translation tasks to provide effective feedback, thereby enhancing the models' self-reflective abilities and improving translation performance.","The application of this method across various translation tasks has proven its effectiveness in improving translation accuracy and eliminating ambiguities, especially in translation tasks with low-resource language pairs."],"url":"http://arxiv.org/abs/2406.07232v1","category":"cs.CL"}
{"created":"2024-06-11 13:09:16","title":"Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms","abstract":"Understanding commonsense knowledge is crucial in the field of Natural Language Processing (NLP). However, the presence of demographic terms in commonsense knowledge poses a potential risk of compromising the performance of NLP models. This study aims to investigate and propose methods for enhancing the performance and effectiveness of a commonsense polarization classifier by mitigating the influence of demographic terms. Three methods are introduced in this paper: (1) hierarchical generalization of demographic terms (2) threshold-based augmentation and (3) integration of hierarchical generalization and threshold-based augmentation methods (IHTA). The first method involves replacing demographic terms with more general ones based on a term hierarchy ontology, aiming to mitigate the influence of specific terms. To address the limited bias-related information, the second method measures the polarization of demographic terms by comparing the changes in the model's predictions when these terms are masked versus unmasked. This method augments commonsense sentences containing terms with high polarization values by replacing their predicates with synonyms generated by ChatGPT. The third method combines the two approaches, starting with threshold-based augmentation followed by hierarchical generalization. The experiments show that the first method increases the accuracy over the baseline by 2.33%, and the second one by 0.96% over standard augmentation methods. The IHTA techniques yielded an 8.82% and 9.96% higher accuracy than threshold-based and standard augmentation methods, respectively.","sentences":["Understanding commonsense knowledge is crucial in the field of Natural Language Processing (NLP).","However, the presence of demographic terms in commonsense knowledge poses a potential risk of compromising the performance of NLP models.","This study aims to investigate and propose methods for enhancing the performance and effectiveness of a commonsense polarization classifier by mitigating the influence of demographic terms.","Three methods are introduced in this paper: (1) hierarchical generalization of demographic terms (2) threshold-based augmentation and (3) integration of hierarchical generalization and threshold-based augmentation methods (IHTA).","The first method involves replacing demographic terms with more general ones based on a term hierarchy ontology, aiming to mitigate the influence of specific terms.","To address the limited bias-related information, the second method measures the polarization of demographic terms by comparing the changes in the model's predictions when these terms are masked versus unmasked.","This method augments commonsense sentences containing terms with high polarization values by replacing their predicates with synonyms generated by ChatGPT.","The third method combines the two approaches, starting with threshold-based augmentation followed by hierarchical generalization.","The experiments show that the first method increases the accuracy over the baseline by 2.33%, and the second one by 0.96% over standard augmentation methods.","The IHTA techniques yielded an 8.82% and 9.96% higher accuracy than threshold-based and standard augmentation methods, respectively."],"url":"http://arxiv.org/abs/2406.07229v1","category":"cs.CL"}
{"created":"2024-06-11 13:09:16","title":"Needle In A Multimodal Haystack","abstract":"With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.","sentences":["With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive.","However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored.","In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents.","Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning.","In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document.","Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation.","We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs.","Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH."],"url":"http://arxiv.org/abs/2406.07230v1","category":"cs.CV"}
{"created":"2024-06-11 13:06:28","title":"Haptic Repurposing with GenAI","abstract":"Mixed Reality aims to merge the digital and physical worlds to create immersive human-computer interactions. Despite notable advancements, the absence of realistic haptic feedback often breaks the immersive experience by creating a disconnect between visual and tactile perceptions. This paper introduces Haptic Repurposing with GenAI, an innovative approach to enhance MR interactions by transforming any physical objects into adaptive haptic interfaces for AI-generated virtual assets. Utilizing state-of-the-art generative AI models, this system captures both 2D and 3D features of physical objects and, through user-directed prompts, generates corresponding virtual objects that maintain the physical form of the original objects. Through model-based object tracking, the system dynamically anchors virtual assets to physical props in real time, allowing objects to visually morph into any user-specified virtual object. This paper details the system's development, presents findings from usability studies that validate its effectiveness, and explores its potential to significantly enhance interactive MR environments. The hope is this work can lay a foundation for further research into AI-driven spatial transformation in immersive and haptic technologies.","sentences":["Mixed Reality aims to merge the digital and physical worlds to create immersive human-computer interactions.","Despite notable advancements, the absence of realistic haptic feedback often breaks the immersive experience by creating a disconnect between visual and tactile perceptions.","This paper introduces Haptic Repurposing with GenAI, an innovative approach to enhance MR interactions by transforming any physical objects into adaptive haptic interfaces for AI-generated virtual assets.","Utilizing state-of-the-art generative AI models, this system captures both 2D and 3D features of physical objects and, through user-directed prompts, generates corresponding virtual objects that maintain the physical form of the original objects.","Through model-based object tracking, the system dynamically anchors virtual assets to physical props in real time, allowing objects to visually morph into any user-specified virtual object.","This paper details the system's development, presents findings from usability studies that validate its effectiveness, and explores its potential to significantly enhance interactive MR environments.","The hope is this work can lay a foundation for further research into AI-driven spatial transformation in immersive and haptic technologies."],"url":"http://arxiv.org/abs/2406.07228v1","category":"cs.HC"}
{"created":"2024-06-11 13:05:36","title":"Detecting Markovianity of Quantum Processes via Recurrent Neural Networks","abstract":"We present a novel methodology utilizing Recurrent Neural Networks (RNNs) to classify Markovian and non-Markovian quantum processes, leveraging time series data derived from Choi states. The model exhibits exceptional accuracy, surpassing 95%, across diverse scenarios, encompassing dephasing and Pauli channels in an arbitrary basis, and generalized amplitude damping dynamics. Additionally, the developed model shows efficient forecasting capabilities for the analyzed time series data. These results suggest the potential of RNNs in discerning and predicting the Markovian nature of quantum processes.","sentences":["We present a novel methodology utilizing Recurrent Neural Networks (RNNs) to classify Markovian and non-Markovian quantum processes, leveraging time series data derived from Choi states.","The model exhibits exceptional accuracy, surpassing 95%, across diverse scenarios, encompassing dephasing and Pauli channels in an arbitrary basis, and generalized amplitude damping dynamics.","Additionally, the developed model shows efficient forecasting capabilities for the analyzed time series data.","These results suggest the potential of RNNs in discerning and predicting the Markovian nature of quantum processes."],"url":"http://arxiv.org/abs/2406.07226v1","category":"quant-ph"}
{"created":"2024-06-11 13:04:30","title":"A generic and robust quantum agent inspired by deep meta-reinforcement learning","abstract":"Deep reinforcement learning (deep RL) has enabled human- or superhuman- performances in various applications. Recently, deep RL has also been adopted to improve the performance of quantum control. However, a large volume of data is typically required to train the neural network in deep RL, making it inefficient compared with the traditional optimal quantum control method. Here, we thus develop a new training algorithm inspired by the deep meta-reinforcement learning (deep meta-RL), which requires significantly less training data. The trained neural network is adaptive and robust. In addition, the algorithm proposed by us has been applied to design the Hadamard gate and show that for a wide range of parameters the infidelity of the obtained gate can be made of the order 0.0001. Our algorithm can also automatically adjust the number of pulses required to generate the target gate, which is different from the traditional optimal quantum control method which typically fixes the number of pulses a-priory. The results of this paper can pave the way towards constructing a universally robust quantum agent catering to the different demands in quantum technologies.","sentences":["Deep reinforcement learning (deep RL) has enabled human- or superhuman- performances in various applications.","Recently, deep RL has also been adopted to improve the performance of quantum control.","However, a large volume of data is typically required to train the neural network in deep RL, making it inefficient compared with the traditional optimal quantum control method.","Here, we thus develop a new training algorithm inspired by the deep meta-reinforcement learning (deep meta-RL), which requires significantly less training data.","The trained neural network is adaptive and robust.","In addition, the algorithm proposed by us has been applied to design the Hadamard gate and show that for a wide range of parameters the infidelity of the obtained gate can be made of the order 0.0001.","Our algorithm can also automatically adjust the number of pulses required to generate the target gate, which is different from the traditional optimal quantum control method which typically fixes the number of pulses a-priory.","The results of this paper can pave the way towards constructing a universally robust quantum agent catering to the different demands in quantum technologies."],"url":"http://arxiv.org/abs/2406.07225v1","category":"quant-ph"}
{"created":"2024-06-11 13:03:43","title":"Differentiability and Optimization of Multiparameter Persistent Homology","abstract":"Real-valued functions on geometric data -- such as node attributes on a graph -- can be optimized using descriptors from persistent homology, allowing the user to incorporate topological terms in the loss function. When optimizing a single real-valued function (the one-parameter setting), there is a canonical choice of descriptor for persistent homology: the barcode. The operation mapping a real-valued function to its barcode is differentiable almost everywhere, and the convergence of gradient descent for losses using barcodes is relatively well understood. When optimizing a vector-valued function (the multiparameter setting), there is no unique choice of descriptor for multiparameter persistent homology, and many distinct descriptors have been proposed. This calls for the development of a general framework for differentiability and optimization that applies to a wide range of multiparameter homological descriptors. In this article, we develop such a framework and show that it encompasses well-known descriptors of different flavors, such as signed barcodes and the multiparameter persistence landscape. We complement the theory with numerical experiments supporting the idea that optimizing multiparameter homological descriptors can lead to improved performances compared to optimizing one-parameter descriptors, even when using the simplest and most efficiently computable multiparameter descriptors.","sentences":["Real-valued functions on geometric data -- such as node attributes on a graph -- can be optimized using descriptors from persistent homology, allowing the user to incorporate topological terms in the loss function.","When optimizing a single real-valued function (the one-parameter setting), there is a canonical choice of descriptor for persistent homology: the barcode.","The operation mapping a real-valued function to its barcode is differentiable almost everywhere, and the convergence of gradient descent for losses using barcodes is relatively well understood.","When optimizing a vector-valued function (the multiparameter setting), there is no unique choice of descriptor for multiparameter persistent homology, and many distinct descriptors have been proposed.","This calls for the development of a general framework for differentiability and optimization that applies to a wide range of multiparameter homological descriptors.","In this article, we develop such a framework and show that it encompasses well-known descriptors of different flavors, such as signed barcodes and the multiparameter persistence landscape.","We complement the theory with numerical experiments supporting the idea that optimizing multiparameter homological descriptors can lead to improved performances compared to optimizing one-parameter descriptors, even when using the simplest and most efficiently computable multiparameter descriptors."],"url":"http://arxiv.org/abs/2406.07224v1","category":"cs.CG"}
{"created":"2024-06-11 13:01:50","title":"Improving Autoformalization using Type Checking","abstract":"Large language models show promise for autoformalization, the task of automatically translating natural language into formal languages. However, current autoformalization methods remain limited. The last reported state-of-the-art performance on the ProofNet formalization benchmark for the Lean proof assistant, achieved using Codex for Lean 3, only showed successful formalization of 16.1% of informal statements. Similarly, our evaluation of GPT-4o for Lean 4 only produces successful translations 34.9% of the time. Our analysis shows that the performance of these models is largely limited by their inability to generate formal statements that successfully type-check (i.e., are syntactically correct and consistent with types) - with a whopping 86.6% of GPT-4o errors starting from a type-check failure. In this work, we propose a method to fix this issue through decoding with type-check filtering, where we initially sample a diverse set of candidate formalizations for an informal statement, then use the Lean proof assistant to filter out candidates that do not type-check. Using GPT-4o as a base model, and combining our method with self-consistency, we obtain a +18.3% absolute increase in formalization accuracy, and achieve a new state-of-the-art of 53.2% on ProofNet with Lean 4.","sentences":["Large language models show promise for autoformalization, the task of automatically translating natural language into formal languages.","However, current autoformalization methods remain limited.","The last reported state-of-the-art performance on the ProofNet formalization benchmark for the Lean proof assistant, achieved using Codex for Lean 3, only showed successful formalization of 16.1% of informal statements.","Similarly, our evaluation of GPT-4o for Lean 4 only produces successful translations 34.9% of the time.","Our analysis shows that the performance of these models is largely limited by their inability to generate formal statements that successfully type-check (i.e., are syntactically correct and consistent with types) - with a whopping 86.6% of GPT-4o errors starting from a type-check failure.","In this work, we propose a method to fix this issue through decoding with type-check filtering, where we initially sample a diverse set of candidate formalizations for an informal statement, then use the Lean proof assistant to filter out candidates that do not type-check.","Using GPT-4o as a base model, and combining our method with self-consistency, we obtain a +18.3% absolute increase in formalization accuracy, and achieve a new state-of-the-art of 53.2% on ProofNet with Lean 4."],"url":"http://arxiv.org/abs/2406.07222v1","category":"cs.CL"}
{"created":"2024-06-11 13:01:45","title":"Open-World Human-Object Interaction Detection via Multi-modal Prompts","abstract":"In this paper, we develop \\textbf{MP-HOI}, a powerful Multi-modal Prompt-based HOI detector designed to leverage both textual descriptions for open-set generalization and visual exemplars for handling high ambiguity in descriptions, realizing HOI detection in the open world. Specifically, it integrates visual prompts into existing language-guided-only HOI detectors to handle situations where textual descriptions face difficulties in generalization and to address complex scenarios with high interaction ambiguity. To facilitate MP-HOI training, we build a large-scale HOI dataset named Magic-HOI, which gathers six existing datasets into a unified label space, forming over 186K images with 2.4K objects, 1.2K actions, and 20K HOI interactions. Furthermore, to tackle the long-tail issue within the Magic-HOI dataset, we introduce an automated pipeline for generating realistically annotated HOI images and present SynHOI, a high-quality synthetic HOI dataset containing 100K images. Leveraging these two datasets, MP-HOI optimizes the HOI task as a similarity learning process between multi-modal prompts and objects/interactions via a unified contrastive loss, to learn generalizable and transferable objects/interactions representations from large-scale data. MP-HOI could serve as a generalist HOI detector, surpassing the HOI vocabulary of existing expert models by more than 30 times. Concurrently, our results demonstrate that MP-HOI exhibits remarkable zero-shot capability in real-world scenarios and consistently achieves a new state-of-the-art performance across various benchmarks.","sentences":["In this paper, we develop \\textbf{MP-HOI}, a powerful Multi-modal Prompt-based HOI detector designed to leverage both textual descriptions for open-set generalization and visual exemplars for handling high ambiguity in descriptions, realizing HOI detection in the open world.","Specifically, it integrates visual prompts into existing language-guided-only HOI detectors to handle situations where textual descriptions face difficulties in generalization and to address complex scenarios with high interaction ambiguity.","To facilitate MP-HOI training, we build a large-scale HOI dataset named Magic-HOI, which gathers six existing datasets into a unified label space, forming over 186K images with 2.4K objects, 1.2K actions, and 20K HOI interactions.","Furthermore, to tackle the long-tail issue within the Magic-HOI dataset, we introduce an automated pipeline for generating realistically annotated HOI images and present SynHOI, a high-quality synthetic HOI dataset containing 100K images.","Leveraging these two datasets, MP-HOI optimizes the HOI task as a similarity learning process between multi-modal prompts and objects/interactions via a unified contrastive loss, to learn generalizable and transferable objects/interactions representations from large-scale data.","MP-HOI could serve as a generalist HOI detector, surpassing the HOI vocabulary of existing expert models by more than 30 times.","Concurrently, our results demonstrate that MP-HOI exhibits remarkable zero-shot capability in real-world scenarios and consistently achieves a new state-of-the-art performance across various benchmarks."],"url":"http://arxiv.org/abs/2406.07221v1","category":"cs.CV"}
{"created":"2024-06-11 12:50:53","title":"A Synthetic Dataset for Personal Attribute Inference","abstract":"Recently, powerful Large Language Models (LLMs) have become easily accessible to hundreds of millions of users worldwide. However, their strong capabilities and vast world knowledge do not come without associated privacy risks. In this work, we focus on the emerging privacy threat LLMs pose - the ability to accurately infer personal information from online texts. Despite the growing importance of LLM-based author profiling, research in this area has been hampered by a lack of suitable public datasets, largely due to ethical and privacy concerns associated with real personal data. In this work, we take two steps to address this problem: (i) we construct a simulation framework for the popular social media platform Reddit using LLM agents seeded with synthetic personal profiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic dataset of over 7800 comments manually labeled for personal attributes. We validate our dataset with a human study showing that humans barely outperform random guessing on the task of distinguishing our synthetic comments from real ones. Further, we verify that our dataset enables meaningful personal attribute inference research by showing across 18 state-of-the-art LLMs that our synthetic comments allow us to draw the same conclusions as real-world data. Together, this indicates that our dataset and pipeline provide a strong and privacy-preserving basis for future research toward understanding and mitigating the inference-based privacy threats LLMs pose.","sentences":["Recently, powerful Large Language Models (LLMs) have become easily accessible to hundreds of millions of users worldwide.","However, their strong capabilities and vast world knowledge do not come without associated privacy risks.","In this work, we focus on the emerging privacy threat LLMs pose - the ability to accurately infer personal information from online texts.","Despite the growing importance of LLM-based author profiling, research in this area has been hampered by a lack of suitable public datasets, largely due to ethical and privacy concerns associated with real personal data.","In this work, we take two steps to address this problem: (i) we construct a simulation framework for the popular social media platform Reddit using LLM agents seeded with synthetic personal profiles; (ii) using this framework, we generate SynthPAI, a diverse synthetic dataset of over 7800 comments manually labeled for personal attributes.","We validate our dataset with a human study showing that humans barely outperform random guessing on the task of distinguishing our synthetic comments from real ones.","Further, we verify that our dataset enables meaningful personal attribute inference research by showing across 18 state-of-the-art LLMs that our synthetic comments allow us to draw the same conclusions as real-world data.","Together, this indicates that our dataset and pipeline provide a strong and privacy-preserving basis for future research toward understanding and mitigating the inference-based privacy threats LLMs pose."],"url":"http://arxiv.org/abs/2406.07217v1","category":"cs.LG"}
{"created":"2024-06-11 12:41:54","title":"Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems with Large Language Models","abstract":"Large language models (LLMs) present a valuable technology for various applications in healthcare, but their tendency to hallucinate introduces unacceptable uncertainty in critical decision-making situations. Human-AI collaboration (HAIC) can mitigate this uncertainty by combining human and AI strengths for better outcomes. This paper presents a novel guided deferral system that provides intelligent guidance when AI defers cases to human decision-makers. We leverage LLMs' verbalisation capabilities and internal states to create this system, demonstrating that fine-tuning smaller LLMs with data from larger models enhances performance while maintaining computational efficiency. A pilot study showcases the effectiveness of our deferral system.","sentences":["Large language models (LLMs) present a valuable technology for various applications in healthcare, but their tendency to hallucinate introduces unacceptable uncertainty in critical decision-making situations.","Human-AI collaboration (HAIC) can mitigate this uncertainty by combining human and AI strengths for better outcomes.","This paper presents a novel guided deferral system that provides intelligent guidance when AI defers cases to human decision-makers.","We leverage LLMs' verbalisation capabilities and internal states to create this system, demonstrating that fine-tuning smaller LLMs with data from larger models enhances performance while maintaining computational efficiency.","A pilot study showcases the effectiveness of our deferral system."],"url":"http://arxiv.org/abs/2406.07212v1","category":"cs.CL"}
{"created":"2024-06-11 12:34:19","title":"The green hydrogen ambition and implementation gap","abstract":"Green hydrogen is critical for decarbonising hard-to-electrify sectors, but faces high costs and investment risks. Here we define and quantify the green hydrogen ambition and implementation gap, showing that meeting hydrogen expectations will remain challenging despite surging announcements of projects and subsidies. Tracking 137 projects over three years, we identify a wide 2022 implementation gap with only 2% of global capacity announcements finished on schedule. In contrast, the 2030 ambition gap towards 1.5{\\deg}C scenarios is gradually closing as the announced project pipeline has nearly tripled to 441 GW within three years. However, we estimate that, without carbon pricing, realising all these projects would require global subsidies of \\$1.6 trillion (\\$1.2 - 2.6 trillion range), far exceeding announced subsidies. Given past and future implementation gaps, policymakers must prepare for prolonged green hydrogen scarcity. Policy support needs to secure hydrogen investments, but should focus on applications where hydrogen is indispensable.","sentences":["Green hydrogen is critical for decarbonising hard-to-electrify sectors, but faces high costs and investment risks.","Here we define and quantify the green hydrogen ambition and implementation gap, showing that meeting hydrogen expectations will remain challenging despite surging announcements of projects and subsidies.","Tracking 137 projects over three years, we identify a wide 2022 implementation gap with only 2% of global capacity announcements finished on schedule.","In contrast, the 2030 ambition gap towards 1.5{\\deg}C scenarios is gradually closing as the announced project pipeline has nearly tripled to 441 GW within three years.","However, we estimate that, without carbon pricing, realising all these projects would require global subsidies of \\$1.6 trillion (\\$1.2 - 2.6 trillion range), far exceeding announced subsidies.","Given past and future implementation gaps, policymakers must prepare for prolonged green hydrogen scarcity.","Policy support needs to secure hydrogen investments, but should focus on applications where hydrogen is indispensable."],"url":"http://arxiv.org/abs/2406.07210v1","category":"econ.GN"}
{"created":"2024-06-11 12:32:53","title":"MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance","abstract":"Recent advancements in text-to-image generation models have dramatically enhanced the generation of photorealistic images from textual prompts, leading to an increased interest in personalized text-to-image applications, particularly in multi-subject scenarios. However, these advances are hindered by two main challenges: firstly, the need to accurately maintain the details of each referenced subject in accordance with the textual descriptions; and secondly, the difficulty in achieving a cohesive representation of multiple subjects in a single image without introducing inconsistencies. To address these concerns, our research introduces the MS-Diffusion framework for layout-guided zero-shot image personalization with multi-subjects. This innovative approach integrates grounding tokens with the feature resampler to maintain detail fidelity among subjects. With the layout guidance, MS-Diffusion further improves the cross-attention to adapt to the multi-subject inputs, ensuring that each subject condition acts on specific areas. The proposed multi-subject cross-attention orchestrates harmonious inter-subject compositions while preserving the control of texts. Comprehensive quantitative and qualitative experiments affirm that this method surpasses existing models in both image and text fidelity, promoting the development of personalized text-to-image generation.","sentences":["Recent advancements in text-to-image generation models have dramatically enhanced the generation of photorealistic images from textual prompts, leading to an increased interest in personalized text-to-image applications, particularly in multi-subject scenarios.","However, these advances are hindered by two main challenges: firstly, the need to accurately maintain the details of each referenced subject in accordance with the textual descriptions; and secondly, the difficulty in achieving a cohesive representation of multiple subjects in a single image without introducing inconsistencies.","To address these concerns, our research introduces the MS-Diffusion framework for layout-guided zero-shot image personalization with multi-subjects.","This innovative approach integrates grounding tokens with the feature resampler to maintain detail fidelity among subjects.","With the layout guidance, MS-Diffusion further improves the cross-attention to adapt to the multi-subject inputs, ensuring that each subject condition acts on specific areas.","The proposed multi-subject cross-attention orchestrates harmonious inter-subject compositions while preserving the control of texts.","Comprehensive quantitative and qualitative experiments affirm that this method surpasses existing models in both image and text fidelity, promoting the development of personalized text-to-image generation."],"url":"http://arxiv.org/abs/2406.07209v1","category":"cs.CV"}
{"created":"2024-06-11 12:26:02","title":"Model order reduction for seismic applications","abstract":"We propose a model order reduction approach to speed up the computation of seismograms, i.e. the solution of the seismic wave equation evaluated at a receiver location, for different model parameters. Our approach achieves a reduction of the unknowns by a factor of approximately 1000 for various numerical experiments for a 2D subsurface model of Groningen, the Netherlands, even if the wave speeds of the subsurface are relatively varied. Moreover, using parallel computing, the reduced model can approximate the (time domain) seismogram in a lower wall clock time than an implicit Newmark-beta method. To realize this reduction, we exploit the fact that seismograms are low-pass filtered for the observed seismic events by considering the Laplace-transformed problem in frequency domain. Therefore, we can avoid the high frequencies that would require many reduced basis functions to reach the desired accuracy and generally make the reduced order approximation of wave problems challenging. Instead, we can prove for our ansatz that for a fixed subsurface model the reduced order approximation converges exponentially fast in the frequency range of interest in the Laplace domain. We build the reduced model from solutions of the Laplace-transformed problem via a (Proper Orthogonal Decomposition-)Greedy algorithm targeting the construction of the reduced model to the time domain seismograms; the latter is achieved by using an a posteriori error estimator that does not require computing any time domain counterparts. Finally, we show that we obtain a stable reduced model thus overcoming the challenge that standard model reduction approaches do not necessarily yield a stable reduced model for wave problems.","sentences":["We propose a model order reduction approach to speed up the computation of seismograms, i.e. the solution of the seismic wave equation evaluated at a receiver location, for different model parameters.","Our approach achieves a reduction of the unknowns by a factor of approximately 1000 for various numerical experiments for a 2D subsurface model of Groningen, the Netherlands, even if the wave speeds of the subsurface are relatively varied.","Moreover, using parallel computing, the reduced model can approximate the (time domain) seismogram in a lower wall clock time than an implicit Newmark-beta method.","To realize this reduction, we exploit the fact that seismograms are low-pass filtered for the observed seismic events by considering the Laplace-transformed problem in frequency domain.","Therefore, we can avoid the high frequencies that would require many reduced basis functions to reach the desired accuracy and generally make the reduced order approximation of wave problems challenging.","Instead, we can prove for our ansatz that for a fixed subsurface model the reduced order approximation converges exponentially fast in the frequency range of interest in the Laplace domain.","We build the reduced model from solutions of the Laplace-transformed problem via a (Proper Orthogonal Decomposition-)Greedy algorithm targeting the construction of the reduced model to the time domain seismograms; the latter is achieved by using an a posteriori error estimator that does not require computing any time domain counterparts.","Finally, we show that we obtain a stable reduced model thus overcoming the challenge that standard model reduction approaches do not necessarily yield a stable reduced model for wave problems."],"url":"http://arxiv.org/abs/2406.07207v1","category":"math.NA"}
{"created":"2024-06-11 12:24:31","title":"Sensing food quality by silicene nanosheets : a Density Functional Theory study","abstract":"Volatile organic compounds (VOCs) emitted by food products are considered markers for assessing quality of food. In this work, first-principles Density Functional Theory (DFT) and Non-equilibrium Green's function (NEGF) methods have been employed to model chemo-resistive gas sensor based on two-dimensional silicene based nanosheets that can sense the six different VOCs emitted by standard food products. Our calculations with unpassivated and flourine passivated silicene(F-silicene) sheets as sensor materials show that flourine passivated silicene has significantly better sensitivity towards all six VOC molecules (Acetone, Dimethylsulfide, Ethanol, Methanol, Methylacetate and Toluene). Moreover, flourinated silicene sensor is found to be capable of separately recognising four VOCs, a much better performance than r-GO used in a recent experiment. We analyse the microscopic picture influencing sensing capabilities of un-passivated and fluorinated silicene from the perspectives of adsorption energy, charge transfer and changes in the electronic structure. We find that better sensing ability of fluorinated silicene nanosheet can be correlated with the changes in the electronic structures near the Fermi level upon adsorption of different VOCs. The results imply that passivated silicene can work better as a sensor than r-GO in case of generic food VOCs. The results are important since modelling of various two-dimensional nano-sensors can be done in the similar way for detection of more complex VOCs emitted by specific food products.","sentences":["Volatile organic compounds (VOCs) emitted by food products are considered markers for assessing quality of food.","In this work, first-principles Density Functional Theory (DFT) and Non-equilibrium Green's function (NEGF) methods have been employed to model chemo-resistive gas sensor based on two-dimensional silicene based nanosheets that can sense the six different VOCs emitted by standard food products.","Our calculations with unpassivated and flourine passivated silicene(F-silicene) sheets as sensor materials show that flourine passivated silicene has significantly better sensitivity towards all six VOC molecules (Acetone, Dimethylsulfide, Ethanol, Methanol, Methylacetate and Toluene).","Moreover, flourinated silicene sensor is found to be capable of separately recognising four VOCs, a much better performance than r-GO used in a recent experiment.","We analyse the microscopic picture influencing sensing capabilities of un-passivated and fluorinated silicene from the perspectives of adsorption energy, charge transfer and changes in the electronic structure.","We find that better sensing ability of fluorinated silicene nanosheet can be correlated with the changes in the electronic structures near the Fermi level upon adsorption of different VOCs.","The results imply that passivated silicene can work better as a sensor than r-GO in case of generic food VOCs.","The results are important since modelling of various two-dimensional nano-sensors can be done in the similar way for detection of more complex VOCs emitted by specific food products."],"url":"http://arxiv.org/abs/2406.07205v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-11 12:23:01","title":"ParaCLAP -- Towards a general language-audio model for computational paralinguistic tasks","abstract":"Contrastive language-audio pretraining (CLAP) has recently emerged as a method for making audio analysis more generalisable. Specifically, CLAP-style models are able to `answer' a diverse set of language queries, extending the capabilities of audio models beyond a closed set of labels. However, CLAP relies on a large set of (audio, query) pairs for pretraining. While such sets are available for general audio tasks, like captioning or sound event detection, there are no datasets with matched audio and text queries for computational paralinguistic (CP) tasks. As a result, the community relies on generic CLAP models trained for general audio with limited success. In the present study, we explore training considerations for ParaCLAP, a CLAP-style model suited to CP, including a novel process for creating audio-language queries. We demonstrate its effectiveness on a set of computational paralinguistic tasks, where it is shown to surpass the performance of open-source state-of-the-art models.","sentences":["Contrastive language-audio pretraining (CLAP) has recently emerged as a method for making audio analysis more generalisable.","Specifically, CLAP-style models are able to `answer' a diverse set of language queries, extending the capabilities of audio models beyond a closed set of labels.","However, CLAP relies on a large set of (audio, query) pairs for pretraining.","While such sets are available for general audio tasks, like captioning or sound event detection, there are no datasets with matched audio and text queries for computational paralinguistic (CP) tasks.","As a result, the community relies on generic CLAP models trained for general audio with limited success.","In the present study, we explore training considerations for ParaCLAP, a CLAP-style model suited to CP, including a novel process for creating audio-language queries.","We demonstrate its effectiveness on a set of computational paralinguistic tasks, where it is shown to surpass the performance of open-source state-of-the-art models."],"url":"http://arxiv.org/abs/2406.07203v1","category":"cs.SD"}
{"created":"2024-06-11 12:02:09","title":"Superscaling analysis of inclusive electron and (anti)neutrino scattering within the coherent density fluctuation model","abstract":"The experimental data from quasielastic electron and (anti)neutrino scattering on $^{12}$C are reanalyzed in terms of a new scaling variable $\\psi^*$ suggested by the interacting relativistic Fermi gas with scalar and vector interactions, which is known to generate a relativistic effective mass for the interacting nucleons. We construct a new scaling function $f^\\text{QE}(\\psi^*)$ for the inclusive lepton scattering from nuclei within the coherent density fluctuation model (CDFM). The latter is a natural extension of the relativistic Fermi gas model to finite nuclei. In this work, on the basis of the scaling function obtained within CDFM with a relativistic effective mass $m_N^* =0.8 m_N$, we calculate and compare the theoretical predictions with a large set of experimental data for inclusive ($e,e'$) and (anti)neutrino cross sections. The model also includes the contribution of weak two-body currents in the two-particle two-hole sector, evaluated within a fully relativistic Fermi gas. Good agreement with experimental data is found over the whole range of electron and (anti)neutrino energies.","sentences":["The experimental data from quasielastic electron and (anti)neutrino scattering on $^{12}$C are reanalyzed in terms of a new scaling variable $\\psi^*$ suggested by the interacting relativistic Fermi gas with scalar and vector interactions, which is known to generate a relativistic effective mass for the interacting nucleons.","We construct a new scaling function $f^\\text{QE}(\\psi^*)$ for the inclusive lepton scattering from nuclei within the coherent density fluctuation model (CDFM).","The latter is a natural extension of the relativistic Fermi gas model to finite nuclei.","In this work, on the basis of the scaling function obtained within CDFM with a relativistic effective mass $m_N^* =0.8 m_N$, we calculate and compare the theoretical predictions with a large set of experimental data for inclusive ($e,e'$) and (anti)neutrino cross sections.","The model also includes the contribution of weak two-body currents in the two-particle two-hole sector, evaluated within a fully relativistic Fermi gas.","Good agreement with experimental data is found over the whole range of electron and (anti)neutrino energies."],"url":"http://arxiv.org/abs/2406.07190v1","category":"nucl-th"}
{"created":"2024-06-11 12:01:09","title":"Merging Improves Self-Critique Against Jailbreak Attacks","abstract":"The robustness of large language models (LLMs) against adversarial manipulations, such as jailbreak attacks, remains a significant challenge. In this work, we propose an approach that enhances the self-critique capability of the LLM and further fine-tunes it over sanitized synthetic data. This is done with the addition of an external critic model that can be merged with the original, thus bolstering self-critique capabilities and improving the robustness of the LLMs response to adversarial prompts. Our results demonstrate that the combination of merging and self-critique can reduce the attack success rate of adversaries significantly, thus offering a promising defense mechanism against jailbreak attacks. Code, data and models released at https://github.com/vicgalle/merging-self-critique-jailbreaks .","sentences":["The robustness of large language models (LLMs) against adversarial manipulations, such as jailbreak attacks, remains a significant challenge.","In this work, we propose an approach that enhances the self-critique capability of the LLM and further fine-tunes it over sanitized synthetic data.","This is done with the addition of an external critic model that can be merged with the original, thus bolstering self-critique capabilities and improving the robustness of the LLMs response to adversarial prompts.","Our results demonstrate that the combination of merging and self-critique can reduce the attack success rate of adversaries significantly, thus offering a promising defense mechanism against jailbreak attacks.","Code, data and models released at https://github.com/vicgalle/merging-self-critique-jailbreaks ."],"url":"http://arxiv.org/abs/2406.07188v1","category":"cs.CL"}
{"created":"2024-06-11 11:49:19","title":"Well-posedness and stability for the two-phase periodic quasistationary Stokes flow","abstract":"The two-phase horizontally periodic quasistationary Stokes flow in $\\mathbb{R}^2$, describing the motion of two immiscible fluids with equal viscosities that are separated by a sharp interface, which is parameterized as the graph of a function $f=f(t)$, is considered in the general case when both gravity and surface tension effects are included. Using potential theory, the moving boundary problem is formulated as a fully nonlinear and nonlocal parabolic problem for the function $f$. Based on abstract parabolic theory, it is proven that the problem is well-posed in all subcritical spaces $\\mathrm{H}^r(\\mathbb{S})$, $r\\in(3/2,2)$. Moreover, the stability properties of the flat equilibria are analyzed in dependence on the physical properties of the fluids.","sentences":["The two-phase horizontally periodic quasistationary Stokes flow in $\\mathbb{R}^2$, describing the motion of two immiscible fluids with equal viscosities that are separated by a sharp interface, which is parameterized as the graph of a function $f=f(t)$, is considered in the general case when both gravity and surface tension effects are included.","Using potential theory, the moving boundary problem is formulated as a fully nonlinear and nonlocal parabolic problem for the function $f$. Based on abstract parabolic theory, it is proven that the problem is well-posed in all subcritical spaces $\\mathrm{H}^r(\\mathbb{S})$, $r\\in(3/2,2)$. Moreover, the stability properties of the flat equilibria are analyzed in dependence on the physical properties of the fluids."],"url":"http://arxiv.org/abs/2406.07181v1","category":"math.AP"}
{"created":"2024-06-11 11:44:36","title":"Detailed chemical abundances of the globular cluster Terzan 6 in the inner bulge","abstract":"We used near-infrared spectroscopy at medium-high resolution (R=8,000$-$25,000) to perform the first comprehensive chemical study of the intermediate luminosity bulge globular cluster Terzan~6. We derived detailed abundances and abundance patterns of 27 giant stars, likely members of Terzan~6, based on their accurate Hubble Space Telescope proper motions and line-of-sight radial velocities. From the spectral analysis of these stars, we determined an average heliocentric radial velocity of 143.3$\\pm$1.0 km s$^{-1}$ with a velocity dispersion of 5.1$\\pm$0.7 km s$^{-1}$ and an average [Fe/H]=$-0.65\\pm0.01$ and a low 1$\\sigma$ dispersion of 0.03 dex. We also measured some depletion of [Mn/Fe] with respect to the solar-scaled values and enhancement of for [Ca/Fe], [Si/Fe], [Mg/Fe], [Ti/Fe], [O/Fe], [Al/Fe], [Na/Fe], and, to a lower extent, for [K/Fe], consistent with previous measurements of other bulge globular clusters and favoring the scenario of a rapid bulge formation and chemical enrichment. Some spread in the light element abundances suggest the presence of first- and second-generation stars, typical of genuine globulars. Finally, we measured some depletion of carbon and low $\\rm ^{12}C/^{13}C$ isotopic ratios, as in previous studies of field and cluster bulge giants, indicating that extra-mixing mechanisms should be at work during the post main sequence evolution in the high metallicity regime as well.","sentences":["We used near-infrared spectroscopy at medium-high resolution (R=8,000$-$25,000) to perform the first comprehensive chemical study of the intermediate luminosity bulge globular cluster Terzan~6.","We derived detailed abundances and abundance patterns of 27 giant stars, likely members of Terzan~6, based on their accurate Hubble Space Telescope proper motions and line-of-sight radial velocities.","From the spectral analysis of these stars, we determined an average heliocentric radial velocity of 143.3$\\pm$1.0 km s$^{-1}$ with a velocity dispersion of 5.1$\\pm$0.7 km s$^{-1}$ and an average [Fe/H]=$-0.65\\pm0.01$ and a low 1$\\sigma$ dispersion of 0.03 dex.","We also measured some depletion of [Mn/Fe] with respect to the solar-scaled values and enhancement of for [Ca/Fe], [Si/Fe], [Mg/Fe], [Ti/Fe], [O/Fe], [Al/Fe], [Na/Fe], and, to a lower extent, for [K/Fe], consistent with previous measurements of other bulge globular clusters and favoring the scenario of a rapid bulge formation and chemical enrichment.","Some spread in the light element abundances suggest the presence of first- and second-generation stars, typical of genuine globulars.","Finally, we measured some depletion of carbon and low $\\rm ^{12}C/^{13}C$ isotopic ratios, as in previous studies of field and cluster bulge giants, indicating that extra-mixing mechanisms should be at work during the post main sequence evolution in the high metallicity regime as well."],"url":"http://arxiv.org/abs/2406.07180v1","category":"astro-ph.GA"}
{"created":"2024-06-11 11:41:41","title":"Comparative Analysis of $k$-essence and Quintessence Scalar Field Models: A Data Analysis Approach","abstract":"We perform a comparative analysis of quintessence and $k$-essence scalar field models in the data analysis perspective. We study the quintessence field with an exponential potential and the $k$-essence field with an inverse square potential in the present work. Before delving into data analysis, we provide a brief perspective on dynamical evolution on both of the models and obtain the stability constraints on the model parameters. We adopt Bayesian inference procedure to estimate the model parameters that best-fit the data. A comprehensive analysis utilizing Observational Hubble data (OHD) and Pantheon+ compilation of Type Ia supernovae (SNIa) shows that $k$-essence model fits the data slightly better than the quintessence model while the evidence of these models in comparison with the $\\Lambda$CDM model is weak. The value of the Hubble constant predicted by both the models is in close agreement with the value obtained by the Planck2018 collaboration assuming the $\\Lambda$CDM model.","sentences":["We perform a comparative analysis of quintessence and $k$-essence scalar field models in the data analysis perspective.","We study the quintessence field with an exponential potential and the $k$-essence field with an inverse square potential in the present work.","Before delving into data analysis, we provide a brief perspective on dynamical evolution on both of the models and obtain the stability constraints on the model parameters.","We adopt Bayesian inference procedure to estimate the model parameters that best-fit the data.","A comprehensive analysis utilizing Observational Hubble data (OHD) and Pantheon+ compilation of Type Ia supernovae (SNIa) shows that $k$-essence model fits the data slightly better than the quintessence model while the evidence of these models in comparison with the $\\Lambda$CDM model is weak.","The value of the Hubble constant predicted by both the models is in close agreement with the value obtained by the Planck2018 collaboration assuming the $\\Lambda$CDM model."],"url":"http://arxiv.org/abs/2406.07179v1","category":"astro-ph.CO"}
{"created":"2024-06-11 11:40:12","title":"TernaryLLM: Ternarized Large Language Model","abstract":"Large language models (LLMs) have achieved remarkable performance on Natural Language Processing (NLP) tasks, but they are hindered by high computational costs and memory requirements. Ternarization, an extreme form of quantization, offers a solution by reducing memory usage and enabling energy-efficient floating-point additions. However, applying ternarization to LLMs faces challenges stemming from outliers in both weights and activations. In this work, observing asymmetric outliers and non-zero means in weights, we introduce Dual Learnable Ternarization (DLT), which enables both scales and shifts to be learnable. We also propose Outlier-Friendly Feature Knowledge Distillation (OFF) to recover the information lost in extremely low-bit quantization. The proposed OFF can incorporate semantic information and is insensitive to outliers. At the core of OFF is maximizing the mutual information between features in ternarized and floating-point models using cosine similarity. Extensive experiments demonstrate that our TernaryLLM surpasses previous low-bit quantization methods on the standard text generation and zero-shot benchmarks for different LLM families. Specifically, for one of the most powerful open-source models, LLaMA-3, our approach (W1.58A16) outperforms the previous state-of-the-art method (W2A16) by 5.8 in terms of perplexity on C4 and by 8.2% in terms of average accuracy on zero-shot tasks.","sentences":["Large language models (LLMs) have achieved remarkable performance on Natural Language Processing (NLP) tasks, but they are hindered by high computational costs and memory requirements.","Ternarization, an extreme form of quantization, offers a solution by reducing memory usage and enabling energy-efficient floating-point additions.","However, applying ternarization to LLMs faces challenges stemming from outliers in both weights and activations.","In this work, observing asymmetric outliers and non-zero means in weights, we introduce Dual Learnable Ternarization (DLT), which enables both scales and shifts to be learnable.","We also propose Outlier-Friendly Feature Knowledge Distillation (OFF) to recover the information lost in extremely low-bit quantization.","The proposed OFF can incorporate semantic information and is insensitive to outliers.","At the core of OFF is maximizing the mutual information between features in ternarized and floating-point models using cosine similarity.","Extensive experiments demonstrate that our TernaryLLM surpasses previous low-bit quantization methods on the standard text generation and zero-shot benchmarks for different LLM families.","Specifically, for one of the most powerful open-source models, LLaMA-3, our approach (W1.58A16) outperforms the previous state-of-the-art method (W2A16) by 5.8 in terms of perplexity on C4 and by 8.2% in terms of average accuracy on zero-shot tasks."],"url":"http://arxiv.org/abs/2406.07177v1","category":"cs.LG"}
{"created":"2024-06-11 11:39:44","title":"RAD: A Comprehensive Dataset for Benchmarking the Robustness of Image Anomaly Detection","abstract":"Robustness against noisy imaging is crucial for practical image anomaly detection systems. This study introduces a Robust Anomaly Detection (RAD) dataset with free views, uneven illuminations, and blurry collections to systematically evaluate the robustness of current anomaly detection methods. Specifically, RAD aims to identify foreign objects on working platforms as anomalies. The collection process incorporates various sources of imaging noise, such as viewpoint changes, uneven illuminations, and blurry collections, to replicate real-world inspection scenarios. Subsequently, we assess and analyze 11 state-of-the-art unsupervised and zero-shot methods on RAD. Our findings indicate that: 1) Variations in viewpoint, illumination, and blurring affect anomaly detection methods to varying degrees; 2) Methods relying on memory banks and assisted by synthetic anomalies demonstrate stronger robustness; 3) Effectively leveraging the general knowledge of foundational models is a promising avenue for enhancing the robustness of anomaly detection methods.","sentences":["Robustness against noisy imaging is crucial for practical image anomaly detection systems.","This study introduces a Robust Anomaly Detection (RAD) dataset with free views, uneven illuminations, and blurry collections to systematically evaluate the robustness of current anomaly detection methods.","Specifically, RAD aims to identify foreign objects on working platforms as anomalies.","The collection process incorporates various sources of imaging noise, such as viewpoint changes, uneven illuminations, and blurry collections, to replicate real-world inspection scenarios.","Subsequently, we assess and analyze 11 state-of-the-art unsupervised and zero-shot methods on RAD.","Our findings indicate that: 1) Variations in viewpoint, illumination, and blurring affect anomaly detection methods to varying degrees; 2) Methods relying on memory banks and assisted by synthetic anomalies demonstrate stronger robustness; 3) Effectively leveraging the general knowledge of foundational models is a promising avenue for enhancing the robustness of anomaly detection methods."],"url":"http://arxiv.org/abs/2406.07176v1","category":"cs.CV"}
{"created":"2024-06-11 11:31:43","title":"Large deviation principle for generalized multiple intersection local times of multidimensional Brownian motion","abstract":"In this paper we consider examples of positive generalized Wiener functions and we establish a large deviation principle for the generalized multiple intersection local time of the multidimensional Brownian motion.","sentences":["In this paper we consider examples of positive generalized Wiener functions and we establish a large deviation principle for the generalized multiple intersection local time of the multidimensional Brownian motion."],"url":"http://arxiv.org/abs/2406.07173v1","category":"math.PR"}
{"created":"2024-06-11 11:27:38","title":"Enhancing Membrane-Based Scanning Force Microscopy Through an Optical Cavity","abstract":"The new generation of strained silicon nitride resonators harbors great promise for scanning force microscopy, especially when combined with the extensive toolbox of cavity optomechanics. However, accessing a mechanical resonator inside an optical cavity with a scanning tip is challenging. Here, we experimentally demonstrate a cavity-based scanning force microscope based on a silicon nitride membrane sensor. We overcome geometric constraints by making use of the extended nature of the mechanical resonator normal modes, which allows us to spatially separate the scanning and readout sites of the membrane. Our microscope is geared towards low-temperature applications in the zeptonewton regime, such as nanoscale nuclear spin detection and imaging.","sentences":["The new generation of strained silicon nitride resonators harbors great promise for scanning force microscopy, especially when combined with the extensive toolbox of cavity optomechanics.","However, accessing a mechanical resonator inside an optical cavity with a scanning tip is challenging.","Here, we experimentally demonstrate a cavity-based scanning force microscope based on a silicon nitride membrane sensor.","We overcome geometric constraints by making use of the extended nature of the mechanical resonator normal modes, which allows us to spatially separate the scanning and readout sites of the membrane.","Our microscope is geared towards low-temperature applications in the zeptonewton regime, such as nanoscale nuclear spin detection and imaging."],"url":"http://arxiv.org/abs/2406.07171v1","category":"physics.optics"}
{"created":"2024-06-11 11:25:37","title":"RecMoDiffuse: Recurrent Flow Diffusion for Human Motion Generation","abstract":"Human motion generation has paramount importance in computer animation. It is a challenging generative temporal modelling task due to the vast possibilities of human motion, high human sensitivity to motion coherence and the difficulty of accurately generating fine-grained motions. Recently, diffusion methods have been proposed for human motion generation due to their high sample quality and expressiveness. However, generated sequences still suffer from motion incoherence, and are limited to short duration, and simpler motion and take considerable time during inference. To address these limitations, we propose \\textit{RecMoDiffuse: Recurrent Flow Diffusion}, a new recurrent diffusion formulation for temporal modelling. Unlike previous work, which applies diffusion to the whole sequence without any temporal dependency, an approach that inherently makes temporal consistency hard to achieve. Our method explicitly enforces temporal constraints with the means of normalizing flow models in the diffusion process and thereby extends diffusion to the temporal dimension. We demonstrate the effectiveness of RecMoDiffuse in the temporal modelling of human motion. Our experiments show that RecMoDiffuse achieves comparable results with state-of-the-art methods while generating coherent motion sequences and reducing the computational overhead in the inference stage.","sentences":["Human motion generation has paramount importance in computer animation.","It is a challenging generative temporal modelling task due to the vast possibilities of human motion, high human sensitivity to motion coherence and the difficulty of accurately generating fine-grained motions.","Recently, diffusion methods have been proposed for human motion generation due to their high sample quality and expressiveness.","However, generated sequences still suffer from motion incoherence, and are limited to short duration, and simpler motion and take considerable time during inference.","To address these limitations, we propose \\textit{RecMoDiffuse: Recurrent Flow Diffusion}, a new recurrent diffusion formulation for temporal modelling.","Unlike previous work, which applies diffusion to the whole sequence without any temporal dependency, an approach that inherently makes temporal consistency hard to achieve.","Our method explicitly enforces temporal constraints with the means of normalizing flow models in the diffusion process and thereby extends diffusion to the temporal dimension.","We demonstrate the effectiveness of RecMoDiffuse in the temporal modelling of human motion.","Our experiments show that RecMoDiffuse achieves comparable results with state-of-the-art methods while generating coherent motion sequences and reducing the computational overhead in the inference stage."],"url":"http://arxiv.org/abs/2406.07169v1","category":"cs.CV"}
{"created":"2024-06-11 11:20:05","title":"Teaching Language Models to Self-Improve by Learning from Language Feedback","abstract":"Aligning Large Language Models (LLMs) with human intentions and values is crucial yet challenging. Current methods primarily rely on human preferences, which are costly and insufficient in capturing nuanced feedback expressed in natural language. In this paper, we present Self-Refinement Tuning (SRT), a method that leverages model feedback for alignment, thereby reducing reliance on human annotations. SRT uses a base language model (e.g., Tulu2) to generate initial responses, which are critiqued and refined by a more advanced model (e.g., GPT-4-Turbo). This process enables the base model to self-evaluate and improve its outputs, facilitating continuous learning. SRT further optimizes the model by learning from its self-generated feedback and refinements, creating a feedback loop that promotes model improvement. Our empirical evaluations demonstrate that SRT significantly outperforms strong baselines across diverse tasks and model sizes. When applied to a 70B parameter model, SRT increases the win rate from 9.6\\% to 25.8\\% on the AlpacaEval 2.0 benchmark, surpassing well-established systems such as GPT-4-0314, Claude 2, and Gemini. Our analysis highlights the crucial role of language feedback in the success of SRT, suggesting potential for further exploration in this direction.","sentences":["Aligning Large Language Models (LLMs) with human intentions and values is crucial yet challenging.","Current methods primarily rely on human preferences, which are costly and insufficient in capturing nuanced feedback expressed in natural language.","In this paper, we present Self-Refinement Tuning (SRT), a method that leverages model feedback for alignment, thereby reducing reliance on human annotations.","SRT uses a base language model (e.g., Tulu2) to generate initial responses, which are critiqued and refined by a more advanced model (e.g., GPT-4-Turbo).","This process enables the base model to self-evaluate and improve its outputs, facilitating continuous learning.","SRT further optimizes the model by learning from its self-generated feedback and refinements, creating a feedback loop that promotes model improvement.","Our empirical evaluations demonstrate that SRT significantly outperforms strong baselines across diverse tasks and model sizes.","When applied to a 70B parameter model, SRT increases the win rate from 9.6\\% to 25.8\\% on the AlpacaEval 2.0 benchmark, surpassing well-established systems such as GPT-4-0314, Claude 2, and Gemini.","Our analysis highlights the crucial role of language feedback in the success of SRT, suggesting potential for further exploration in this direction."],"url":"http://arxiv.org/abs/2406.07168v1","category":"cs.CL"}
{"created":"2024-06-11 11:17:08","title":"Ultrametric-preserving functions as monoid endomorphisms","abstract":"Let $\\mathbb{R}^{+}=[0, \\infty)$ and let $\\mathbf{End}_{\\mathbb{R}^+}$ be the set of all endomorphisms of the monoid $(\\mathbb{R}^+, \\vee)$. The set $\\mathbf{End}_{\\mathbb{R}^+}$ is a monoid with respect to the operation of the function composition $g \\circ f$. It is shown that $g : \\mathbb{R}^+ \\to \\mathbb{R}^+$ is pseudometric-preserving iff $g \\in \\mathbf{End}_{\\mathbb{R}^+}$. In particular, a function $f : \\mathbb{R}^+ \\to \\mathbb{R}^+$ is ultrametric-preserving iff it is an endomorphism of $(\\mathbb{R}^+,\\vee)$ with kelnel consisting only the zero point. We prove that a given $\\mathbf{A} \\subseteq \\mathbf{End}_{\\mathbb{R}^+}$ is a submonoid of $(\\mathbf{End}, \\circ)$ iff there is a class $\\mathbf{X}$ of pseudoultrametric spaces such that $\\mathbf{A}$ coincides with the set of all functions which preserve the spaces from $\\mathbf{X}$. An explicit construction of such $\\mathbf{X}$ is given.","sentences":["Let $\\mathbb{R}^{+}=[0, \\infty)$ and let $\\mathbf{End}_{\\mathbb{R}^+}$ be the set of all endomorphisms of the monoid $(\\mathbb{R}^+, \\vee)$. The set $\\mathbf{End}_{\\mathbb{R}^+}$ is a monoid with respect to the operation of the function composition $g \\circ f$.","It is shown that $g : \\mathbb{R}^+ \\to \\mathbb{R}^+$ is pseudometric-preserving iff $g \\in \\mathbf{End}_{\\mathbb{R}^+}$. In particular, a function $f : \\mathbb{R}^+ \\to \\mathbb{R}^+$ is ultrametric-preserving iff it is an endomorphism of $(\\mathbb{R}^+,\\vee)$ with kelnel consisting only the zero point.","We prove that a given $\\mathbf{A} \\subseteq \\mathbf{End}_{\\mathbb{R}^+}$ is a submonoid of $(\\mathbf{End}, \\circ)$ iff there is a class $\\mathbf{X}$ of pseudoultrametric spaces such that $\\mathbf{A}$ coincides with the set of all functions which preserve the spaces from $\\mathbf{X}$. An explicit construction of such $\\mathbf{X}$ is given."],"url":"http://arxiv.org/abs/2406.07166v1","category":"math.GN"}
{"created":"2024-06-11 11:16:35","title":"Realizing RF Wavefront Copying with RIS for Future Extended Reality Applications","abstract":"Lately a new approach to Extended Reality (XR), denoted as XR-RF, has been proposed which is realized by combining Radio Frequency (RF) Imaging and programmable wireless environments (PWEs). RF Imaging is a technique that aims to detect geometric and material features of an object through RF waves. On the other hand, the PWE focuses on the the conversion of the wireless RF propagation in a controllable, by software, entity through the utilization of Reconfigurable Intelligent Surfaces (RISs), which can have a controllable interaction with impinging RF waves. In that sense, this dynamic synergy leverages the potential of RF Imaging to detect the structure of an object through RF wavefronts and the PWE's ability to selectively replicate those RF wavefronts from one spatial location to wherever an XR-RF mobile user is presently located. Then the captured wavefront, through appropriate hardware, is mapped to the visual representation of the object through machine learning models. As a key aspect of the XR-RF's system workflow is the wavefront copying mechanism, this work introduces a new PWE configuration algorithm for XR-RF. Moreover, it is shown that the waveform replication process inevitably yields imprecision in the replication process. After statistical analysis, based on simulation results, it is shown that this imprecision can be effectively modeled by the gamma distribution.","sentences":["Lately a new approach to Extended Reality (XR), denoted as XR-RF, has been proposed which is realized by combining Radio Frequency (RF) Imaging and programmable wireless environments (PWEs).","RF Imaging is a technique that aims to detect geometric and material features of an object through RF waves.","On the other hand, the PWE focuses on the the conversion of the wireless RF propagation in a controllable, by software, entity through the utilization of Reconfigurable Intelligent Surfaces (RISs), which can have a controllable interaction with impinging RF waves.","In that sense, this dynamic synergy leverages the potential of RF Imaging to detect the structure of an object through RF wavefronts and the PWE's ability to selectively replicate those RF wavefronts from one spatial location to wherever an XR-RF mobile user is presently located.","Then the captured wavefront, through appropriate hardware, is mapped to the visual representation of the object through machine learning models.","As a key aspect of the XR-RF's system workflow is the wavefront copying mechanism, this work introduces a new PWE configuration algorithm for XR-RF.","Moreover, it is shown that the waveform replication process inevitably yields imprecision in the replication process.","After statistical analysis, based on simulation results, it is shown that this imprecision can be effectively modeled by the gamma distribution."],"url":"http://arxiv.org/abs/2406.07165v1","category":"cs.ET"}
{"created":"2024-06-11 11:13:29","title":"FaceGPT: Self-supervised Learning to Chat about 3D Human Faces","abstract":"We introduce FaceGPT, a self-supervised learning framework for Large Vision-Language Models (VLMs) to reason about 3D human faces from images and text. Typical 3D face reconstruction methods are specialized algorithms that lack semantic reasoning capabilities. FaceGPT overcomes this limitation by embedding the parameters of a 3D morphable face model (3DMM) into the token space of a VLM, enabling the generation of 3D faces from both textual and visual inputs. FaceGPT is trained in a self-supervised manner as a model-based autoencoder from in-the-wild images. In particular, the hidden state of LLM is projected into 3DMM parameters and subsequently rendered as 2D face image to guide the self-supervised learning process via image-based reconstruction. Without relying on expensive 3D annotations of human faces, FaceGPT obtains a detailed understanding about 3D human faces, while preserving the capacity to understand general user instructions. Our experiments demonstrate that FaceGPT not only achieves high-quality 3D face reconstructions but also retains the ability for general-purpose visual instruction following. Furthermore, FaceGPT learns fully self-supervised to generate 3D faces based on complex textual inputs, which opens a new direction in human face analysis.","sentences":["We introduce FaceGPT, a self-supervised learning framework for Large Vision-Language Models (VLMs) to reason about 3D human faces from images and text.","Typical 3D face reconstruction methods are specialized algorithms that lack semantic reasoning capabilities.","FaceGPT overcomes this limitation by embedding the parameters of a 3D morphable face model (3DMM) into the token space of a VLM, enabling the generation of 3D faces from both textual and visual inputs.","FaceGPT is trained in a self-supervised manner as a model-based autoencoder from in-the-wild images.","In particular, the hidden state of LLM is projected into 3DMM parameters and subsequently rendered as 2D face image to guide the self-supervised learning process via image-based reconstruction.","Without relying on expensive 3D annotations of human faces, FaceGPT obtains a detailed understanding about 3D human faces, while preserving the capacity to understand general user instructions.","Our experiments demonstrate that FaceGPT not only achieves high-quality 3D face reconstructions but also retains the ability for general-purpose visual instruction following.","Furthermore, FaceGPT learns fully self-supervised to generate 3D faces based on complex textual inputs, which opens a new direction in human face analysis."],"url":"http://arxiv.org/abs/2406.07163v1","category":"cs.CV"}
{"created":"2024-06-11 11:12:51","title":"EmoBox: Multilingual Multi-corpus Speech Emotion Recognition Toolkit and Benchmark","abstract":"Speech emotion recognition (SER) is an important part of human-computer interaction, receiving extensive attention from both industry and academia. However, the current research field of SER has long suffered from the following problems: 1) There are few reasonable and universal splits of the datasets, making comparing different models and methods difficult. 2) No commonly used benchmark covers numerous corpus and languages for researchers to refer to, making reproduction a burden. In this paper, we propose EmoBox, an out-of-the-box multilingual multi-corpus speech emotion recognition toolkit, along with a benchmark for both intra-corpus and cross-corpus settings. For intra-corpus settings, we carefully designed the data partitioning for different datasets. For cross-corpus settings, we employ a foundation SER model, emotion2vec, to mitigate annotation errors and obtain a test set that is fully balanced in speakers and emotions distributions. Based on EmoBox, we present the intra-corpus SER results of 10 pre-trained speech models on 32 emotion datasets with 14 languages, and the cross-corpus SER results on 4 datasets with the fully balanced test sets. To the best of our knowledge, this is the largest SER benchmark, across language scopes and quantity scales. We hope that our toolkit and benchmark can facilitate the research of SER in the community.","sentences":["Speech emotion recognition (SER) is an important part of human-computer interaction, receiving extensive attention from both industry and academia.","However, the current research field of SER has long suffered from the following problems: 1) There are few reasonable and universal splits of the datasets, making comparing different models and methods difficult.","2) No commonly used benchmark covers numerous corpus and languages for researchers to refer to, making reproduction a burden.","In this paper, we propose EmoBox, an out-of-the-box multilingual multi-corpus speech emotion recognition toolkit, along with a benchmark for both intra-corpus and cross-corpus settings.","For intra-corpus settings, we carefully designed the data partitioning for different datasets.","For cross-corpus settings, we employ a foundation SER model, emotion2vec, to mitigate annotation errors and obtain a test set that is fully balanced in speakers and emotions distributions.","Based on EmoBox, we present the intra-corpus SER results of 10 pre-trained speech models on 32 emotion datasets with 14 languages, and the cross-corpus SER results on 4 datasets with the fully balanced test sets.","To the best of our knowledge, this is the largest SER benchmark, across language scopes and quantity scales.","We hope that our toolkit and benchmark can facilitate the research of SER in the community."],"url":"http://arxiv.org/abs/2406.07162v1","category":"cs.SD"}
{"created":"2024-06-11 11:09:00","title":"ACCO: Automated Causal CNN Scheduling Optimizer for Real-Time Edge Accelerators","abstract":"Spatio-Temporal Convolutional Neural Networks (ST-CNN) allow extending CNN capabilities from image processing to consecutive temporal-pattern recognition. Generally, state-of-the-art (SotA) ST-CNNs inflate the feature maps and weights from well-known CNN backbones to represent the additional time dimension. However, edge computing applications would suffer tremendously from such large computation or memory overhead. Fortunately, the overlapping nature of ST-CNN enables various optimizations, such as the dilated causal convolution structure and Depth-First (DF) layer fusion to reuse the computation between time steps and CNN sliding windows, respectively. Yet, no hardware-aware approach has been proposed that jointly explores the optimal strategy from a scheduling as well as a hardware point of view. To this end, we present ACCO, an automated optimizer that explores efficient Causal CNN transformation and DF scheduling for ST-CNNs on edge hardware accelerators. By cost-modeling the computation and data movement on the accelerator architecture, ACCO automatically selects the best scheduling strategy for the given hardware-algorithm target. Compared to the fixed dilated causal structure, ST-CNNs with ACCO reach an ~8.4x better Energy-Delay-Product. Meanwhile, ACCO improves ~20% in layer-fusion optimals compared to the SotA DF exploration toolchain. When jointly optimizing ST-CNN on the temporal and spatial dimension, ACCO's scheduling outcomes are on average 19x faster and 37x more energy-efficient than spatial DF schemes.","sentences":["Spatio-Temporal Convolutional Neural Networks (ST-CNN) allow extending CNN capabilities from image processing to consecutive temporal-pattern recognition.","Generally, state-of-the-art (SotA) ST-CNNs inflate the feature maps and weights from well-known CNN backbones to represent the additional time dimension.","However, edge computing applications would suffer tremendously from such large computation or memory overhead.","Fortunately, the overlapping nature of ST-CNN enables various optimizations, such as the dilated causal convolution structure and Depth-First (DF) layer fusion to reuse the computation between time steps and CNN sliding windows, respectively.","Yet, no hardware-aware approach has been proposed that jointly explores the optimal strategy from a scheduling as well as a hardware point of view.","To this end, we present ACCO, an automated optimizer that explores efficient Causal CNN transformation and DF scheduling for ST-CNNs on edge hardware accelerators.","By cost-modeling the computation and data movement on the accelerator architecture, ACCO automatically selects the best scheduling strategy for the given hardware-algorithm target.","Compared to the fixed dilated causal structure, ST-CNNs with ACCO reach an ~8.4x better Energy-Delay-Product.","Meanwhile, ACCO improves ~20% in layer-fusion optimals compared to the SotA DF exploration toolchain.","When jointly optimizing ST-CNN on the temporal and spatial dimension, ACCO's scheduling outcomes are on average 19x faster and 37x more energy-efficient than spatial DF schemes."],"url":"http://arxiv.org/abs/2406.07161v1","category":"eess.SP"}
{"created":"2024-06-11 11:06:01","title":"Eigenstate plateau transition and equilibration in 1D quantum lattice models","abstract":"We report on a remarkable spectral phenomenon in a generic type of quantum lattice gas model. As the interaction strength increases, eigenstates spontaneously reorganize and lead to plateaus of the interaction energy, with gaps opening akin to continuous phase transitions. Perturbation theory identifies a hidden structure underlying eigenstates within each plateau, resulting in a statistical shift in the wavefunction amplitudes described by extreme value theory. The structured eigenstates manifest themselves naturally in far-from-equilibrium dynamics proceeding through multiple universal stages. Our findings reveal a profound connection between emergent properties in high-energy states and out-of-equilibrium dynamics, providing insights into the impact of interactions across the entire energy spectrum. The results are directly relevant to experiments probing equilibration in quantum spin and lattice gases.","sentences":["We report on a remarkable spectral phenomenon in a generic type of quantum lattice gas model.","As the interaction strength increases, eigenstates spontaneously reorganize and lead to plateaus of the interaction energy, with gaps opening akin to continuous phase transitions.","Perturbation theory identifies a hidden structure underlying eigenstates within each plateau, resulting in a statistical shift in the wavefunction amplitudes described by extreme value theory.","The structured eigenstates manifest themselves naturally in far-from-equilibrium dynamics proceeding through multiple universal stages.","Our findings reveal a profound connection between emergent properties in high-energy states and out-of-equilibrium dynamics, providing insights into the impact of interactions across the entire energy spectrum.","The results are directly relevant to experiments probing equilibration in quantum spin and lattice gases."],"url":"http://arxiv.org/abs/2406.07159v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-11 11:04:49","title":"Quantum repeaters based on stationary Gottesman-Kitaev-Preskill qubits","abstract":"Quantum repeaters that incorporate quantum error correction codes have been shown to be a promising alternative compared with the original quantum repeaters that rely upon probabilistic quantum error detection depending on classical communication over remote repeater stations. A particularly efficient way of encoding qubits into an error correction code is through bosonic codes where even a single oscillator mode serves as a sufficiently large, physical system. Here we consider the bosonic Gottesman-Kitaev-Preskill (GKP) code as a natural choice for a loss-correction-based quantum repeater. However, unlike existing treatments, we focus on the excitation loss that occurs in the local, stationary memory qubits as represented by, for instance, collective atomic spin modes. We analyze and assess the performance of such a GKP-based quantum repeater where, apart from the initial state generations and distributions, all operations can be performed via deterministic linear mode transformations, as opposed to other existing memory-based quantum repeater schemes.","sentences":["Quantum repeaters that incorporate quantum error correction codes have been shown to be a promising alternative compared with the original quantum repeaters that rely upon probabilistic quantum error detection depending on classical communication over remote repeater stations.","A particularly efficient way of encoding qubits into an error correction code is through bosonic codes where even a single oscillator mode serves as a sufficiently large, physical system.","Here we consider the bosonic Gottesman-Kitaev-Preskill (GKP) code as a natural choice for a loss-correction-based quantum repeater.","However, unlike existing treatments, we focus on the excitation loss that occurs in the local, stationary memory qubits as represented by, for instance, collective atomic spin modes.","We analyze and assess the performance of such a GKP-based quantum repeater where, apart from the initial state generations and distributions, all operations can be performed via deterministic linear mode transformations, as opposed to other existing memory-based quantum repeater schemes."],"url":"http://arxiv.org/abs/2406.07158v1","category":"quant-ph"}
{"created":"2024-06-11 11:02:56","title":"Four-qubit photonic system for publicly verifiable quantum random numbers and generation of public and private key","abstract":"We theoretically propose and experimentally demonstrate the use of a configurable four-qubit photonic system to generate a publicly verifiable quantum random numbers, to perform entanglement verification, and to generate a secure public and private key. Quantum circuits, to generate the desired four-qubit states and its experimental realization in the photonic architecture is carried out using photon pairs entangled in polarization and path degree of freedom. By performing measurements on the four-qubit system and accessing partial information of the four-qubit state for public verification, we generate publicly verified and purely secured random bits at the rate of 370 kbps. When the system is used for generating public and private keys, an equal number of public and private keys are generated simultaneously. We also record about 97.9\\% of sampled bits from four-qubit states passing entanglement verification. The theoretical model of noise on the four-qubit state and its effect on the generation rate of verified and secured bits are in perfect agreement with the experimental results. This demonstrates the practical use of the small-scale multi-qubit photonic system for quantum-safe applications by providing the option for real-time verification of the security feature of the quantum system.","sentences":["We theoretically propose and experimentally demonstrate the use of a configurable four-qubit photonic system to generate a publicly verifiable quantum random numbers, to perform entanglement verification, and to generate a secure public and private key.","Quantum circuits, to generate the desired four-qubit states and its experimental realization in the photonic architecture is carried out using photon pairs entangled in polarization and path degree of freedom.","By performing measurements on the four-qubit system and accessing partial information of the four-qubit state for public verification, we generate publicly verified and purely secured random bits at the rate of 370 kbps.","When the system is used for generating public and private keys, an equal number of public and private keys are generated simultaneously.","We also record about 97.9\\% of sampled bits from four-qubit states passing entanglement verification.","The theoretical model of noise on the four-qubit state and its effect on the generation rate of verified and secured bits are in perfect agreement with the experimental results.","This demonstrates the practical use of the small-scale multi-qubit photonic system for quantum-safe applications by providing the option for real-time verification of the security feature of the quantum system."],"url":"http://arxiv.org/abs/2406.07156v1","category":"quant-ph"}
{"created":"2024-06-11 11:02:04","title":"Scaling Large-Language-Model-based Multi-Agent Collaboration","abstract":"Pioneering advancements in large language model-powered agents have underscored the design pattern of multi-agent collaboration, demonstrating that collective intelligence can surpass the capabilities of each individual. Inspired by the neural scaling law, which posits that increasing neurons leads to emergent abilities, this study investigates whether a similar principle applies to increasing agents in multi-agent collaboration. Technically, we propose multi-agent collaboration networks (MacNet), which utilize directed acyclic graphs to organize agents and streamline their interactive reasoning via topological ordering, with solutions derived from their dialogues. Extensive experiments show that MacNet consistently outperforms baseline models, enabling effective agent collaboration across various network topologies and supporting cooperation among more than a thousand agents. Notably, we observed a small-world collaboration phenomenon, where topologies resembling small-world properties achieved superior performance. Additionally, we identified a collaborative scaling law, indicating that normalized solution quality follows a logistic growth pattern as scaling agents, with collaborative emergence occurring much earlier than previously observed instances of neural emergence. The code and data will be available at https://github.com/OpenBMB/ChatDev.","sentences":["Pioneering advancements in large language model-powered agents have underscored the design pattern of multi-agent collaboration, demonstrating that collective intelligence can surpass the capabilities of each individual.","Inspired by the neural scaling law, which posits that increasing neurons leads to emergent abilities, this study investigates whether a similar principle applies to increasing agents in multi-agent collaboration.","Technically, we propose multi-agent collaboration networks (MacNet), which utilize directed acyclic graphs to organize agents and streamline their interactive reasoning via topological ordering, with solutions derived from their dialogues.","Extensive experiments show that MacNet consistently outperforms baseline models, enabling effective agent collaboration across various network topologies and supporting cooperation among more than a thousand agents.","Notably, we observed a small-world collaboration phenomenon, where topologies resembling small-world properties achieved superior performance.","Additionally, we identified a collaborative scaling law, indicating that normalized solution quality follows a logistic growth pattern as scaling agents, with collaborative emergence occurring much earlier than previously observed instances of neural emergence.","The code and data will be available at https://github.com/OpenBMB/ChatDev."],"url":"http://arxiv.org/abs/2406.07155v1","category":"cs.AI"}
{"created":"2024-06-11 10:57:28","title":"EEG classification for visual brain decoding with spatio-temporal and transformer based paradigms","abstract":"In this work, we delve into the EEG classification task in the domain of visual brain decoding via two frameworks, involving two different learning paradigms. Considering the spatio-temporal nature of EEG data, one of our frameworks is based on a CNN-BiLSTM model. The other involves a CNN-Transformer architecture which inherently involves the more versatile attention based learning paradigm. In both cases, a special 1D-CNN feature extraction module is used to generate the initial embeddings with 1D convolutions in the time and the EEG channel domains. Considering the EEG signals are noisy, non stationary and the discriminative features are even less clear (than in semantically structured data such as text or image), we also follow a window-based classification followed by majority voting during inference, to yield labels at a signal level. To illustrate how brain patterns correlate with different image classes, we visualize t-SNE plots of the BiLSTM embeddings alongside brain activation maps for the top 10 classes. These visualizations provide insightful revelations into the distinct neural signatures associated with each visual category, showcasing the BiLSTM's capability to capture and represent the discriminative brain activity linked to visual stimuli. We demonstrate the performance of our approach on the updated EEG-Imagenet dataset with positive comparisons with state-of-the-art methods.","sentences":["In this work, we delve into the EEG classification task in the domain of visual brain decoding via two frameworks, involving two different learning paradigms.","Considering the spatio-temporal nature of EEG data, one of our frameworks is based on a CNN-BiLSTM model.","The other involves a CNN-Transformer architecture which inherently involves the more versatile attention based learning paradigm.","In both cases, a special 1D-CNN feature extraction module is used to generate the initial embeddings with 1D convolutions in the time and the EEG channel domains.","Considering the EEG signals are noisy, non stationary and the discriminative features are even less clear (than in semantically structured data such as text or image), we also follow a window-based classification followed by majority voting during inference, to yield labels at a signal level.","To illustrate how brain patterns correlate with different image classes, we visualize t-SNE plots of the BiLSTM embeddings alongside brain activation maps for the top 10 classes.","These visualizations provide insightful revelations into the distinct neural signatures associated with each visual category, showcasing the BiLSTM's capability to capture and represent the discriminative brain activity linked to visual stimuli.","We demonstrate the performance of our approach on the updated EEG-Imagenet dataset with positive comparisons with state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.07153v1","category":"cs.HC"}
{"created":"2024-06-11 10:52:17","title":"EEG-ImageNet: An Electroencephalogram Dataset and Benchmarks with Image Visual Stimuli of Multi-Granularity Labels","abstract":"Identifying and reconstructing what we see from brain activity gives us a special insight into investigating how the biological visual system represents the world. While recent efforts have achieved high-performance image classification and high-quality image reconstruction from brain signals collected by Functional Magnetic Resonance Imaging (fMRI) or magnetoencephalogram (MEG), the expensiveness and bulkiness of these devices make relevant applications difficult to generalize to practical applications. On the other hand, Electroencephalography (EEG), despite its advantages of ease of use, cost-efficiency, high temporal resolution, and non-invasive nature, has not been fully explored in relevant studies due to the lack of comprehensive datasets. To address this gap, we introduce EEG-ImageNet, a novel EEG dataset comprising recordings from 16 subjects exposed to 4000 images selected from the ImageNet dataset. EEG-ImageNet consists of 5 times EEG-image pairs larger than existing similar EEG benchmarks. EEG-ImageNet is collected with image stimuli of multi-granularity labels, i.e., 40 images with coarse-grained labels and 40 with fine-grained labels. Based on it, we establish benchmarks for object classification and image reconstruction. Experiments with several commonly used models show that the best models can achieve object classification with accuracy around 60% and image reconstruction with two-way identification around 64%. These results demonstrate the dataset's potential to advance EEG-based visual brain-computer interfaces, understand the visual perception of biological systems, and provide potential applications in improving machine visual models.","sentences":["Identifying and reconstructing what we see from brain activity gives us a special insight into investigating how the biological visual system represents the world.","While recent efforts have achieved high-performance image classification and high-quality image reconstruction from brain signals collected by Functional Magnetic Resonance Imaging (fMRI) or magnetoencephalogram (MEG), the expensiveness and bulkiness of these devices make relevant applications difficult to generalize to practical applications.","On the other hand, Electroencephalography (EEG), despite its advantages of ease of use, cost-efficiency, high temporal resolution, and non-invasive nature, has not been fully explored in relevant studies due to the lack of comprehensive datasets.","To address this gap, we introduce EEG-ImageNet, a novel EEG dataset comprising recordings from 16 subjects exposed to 4000 images selected from the ImageNet dataset.","EEG-ImageNet consists of 5 times EEG-image pairs larger than existing similar EEG benchmarks.","EEG-ImageNet is collected with image stimuli of multi-granularity labels, i.e., 40 images with coarse-grained labels and 40 with fine-grained labels.","Based on it, we establish benchmarks for object classification and image reconstruction.","Experiments with several commonly used models show that the best models can achieve object classification with accuracy around 60% and image reconstruction with two-way identification around 64%.","These results demonstrate the dataset's potential to advance EEG-based visual brain-computer interfaces, understand the visual perception of biological systems, and provide potential applications in improving machine visual models."],"url":"http://arxiv.org/abs/2406.07151v1","category":"cs.MM"}
{"created":"2024-06-11 10:50:45","title":"From Policy to Practice: The Cost of Europe's Green Hydrogen Ambitions","abstract":"The European Commission's new definition of green hydrogen provides clear guidelines and legal certainty for producers and consumers. However, the strict criteria for electrolysis production, requiring additionality, temporal correlation, and geographical correlation, could increase hydrogen costs, affecting its competitiveness as an energy carrier. This study examines the impact of these European regulations using a stochastic capacity expansion model for the European energy market up to 2048. We analyze how these requirements influence costs and investment decisions. Our results show that green hydrogen production requirements will raise system costs by 82 Euro billion from 2024 to 2048, driven mainly by a rapid transition from fossil fuels to renewable energy. The additionality requirement, which mandates the use of new renewable energy installations for electrolysis, emerges as the most expensive to comply with but also the most effective in accelerating the transition to renewable power, particularly before 2030.","sentences":["The European Commission's new definition of green hydrogen provides clear guidelines and legal certainty for producers and consumers.","However, the strict criteria for electrolysis production, requiring additionality, temporal correlation, and geographical correlation, could increase hydrogen costs, affecting its competitiveness as an energy carrier.","This study examines the impact of these European regulations using a stochastic capacity expansion model for the European energy market up to 2048.","We analyze how these requirements influence costs and investment decisions.","Our results show that green hydrogen production requirements will raise system costs by 82 Euro billion from 2024 to 2048, driven mainly by a rapid transition from fossil fuels to renewable energy.","The additionality requirement, which mandates the use of new renewable energy installations for electrolysis, emerges as the most expensive to comply with but also the most effective in accelerating the transition to renewable power, particularly before 2030."],"url":"http://arxiv.org/abs/2406.07149v1","category":"econ.GN"}
{"created":"2024-06-11 10:48:26","title":"Wearable Device-Based Physiological Signal Monitoring: An Assessment Study of Cognitive Load Across Tasks","abstract":"This study employs cutting-edge wearable monitoring technology to conduct high-precision, high-temporal-resolution cognitive load assessment on EEG data from the FP1 channel and heart rate variability (HRV) data of secondary vocational students(SVS). By jointly analyzing these two critical physiological indicators, the research delves into their application value in assessing cognitive load among SVS students and their utility across various tasks. The study designed two experiments to validate the efficacy of the proposed approach: Initially, a random forest classification model, developed using the N-BACK task, enabled the precise decoding of physiological signal characteristics in SVS students under different levels of cognitive load, achieving a classification accuracy of 97%. Subsequently, this classification model was applied in a cross-task experiment involving the National Computer Rank Examination, demonstrating the method's significant applicability and cross-task transferability in diverse learning contexts. Conducted with high portability, this research holds substantial theoretical and practical significance for optimizing teaching resource allocation in secondary vocational education, as well as for cognitive load assessment methods and monitoring. Currently, the research findings are undergoing trial implementation in the school.","sentences":["This study employs cutting-edge wearable monitoring technology to conduct high-precision, high-temporal-resolution cognitive load assessment on EEG data from the FP1 channel and heart rate variability (HRV) data of secondary vocational students(SVS).","By jointly analyzing these two critical physiological indicators, the research delves into their application value in assessing cognitive load among SVS students and their utility across various tasks.","The study designed two experiments to validate the efficacy of the proposed approach: Initially, a random forest classification model, developed using the N-BACK task, enabled the precise decoding of physiological signal characteristics in SVS students under different levels of cognitive load, achieving a classification accuracy of 97%.","Subsequently, this classification model was applied in a cross-task experiment involving the National Computer Rank Examination, demonstrating the method's significant applicability and cross-task transferability in diverse learning contexts.","Conducted with high portability, this research holds substantial theoretical and practical significance for optimizing teaching resource allocation in secondary vocational education, as well as for cognitive load assessment methods and monitoring.","Currently, the research findings are undergoing trial implementation in the school."],"url":"http://arxiv.org/abs/2406.07147v1","category":"cs.HC"}
{"created":"2024-06-11 10:45:59","title":"Benchmarking and Boosting Radiology Report Generation for 3D High-Resolution Medical Images","abstract":"Automatic radiology report generation can significantly benefit the labor-intensive process of report writing by radiologists, especially for 3D radiographs like CT scans, which are crucial for broad clinical diagnostics yet underexplored compared to 2D radiographs. Existing methods often handle 3D volumes either slice-wise or with aggressive downsampling due to current GPU memory limitations, which results in a loss of the inherent 3D nature and critical details. To overcome these issues, we introduce a novel framework that efficiently and effectively generates radiology reports for high-resolution (HR) 3D volumes, based on large language models (LLMs). Specifically, our framework utilizes low-resolution (LR) visual tokens as queries to mine information from HR tokens, preserving detailed HR information while reducing computational costs by only processing HR informed LR visual queries. Further benefiting the field, we curate and release BIMCV-RG, a new dataset with 5,328 HR 3D volumes and paired reports, establishing the first benchmarks for report generation from 3D HR medical images. Our method consistently surpasses existing methods on this benchmark across three different settings: normal-resolution, high-resolution inputs, and zero-shot domain transfer, all at an acceptable computational cost, trainable on a single A100-80G.","sentences":["Automatic radiology report generation can significantly benefit the labor-intensive process of report writing by radiologists, especially for 3D radiographs like CT scans, which are crucial for broad clinical diagnostics yet underexplored compared to 2D radiographs.","Existing methods often handle 3D volumes either slice-wise or with aggressive downsampling due to current GPU memory limitations, which results in a loss of the inherent 3D nature and critical details.","To overcome these issues, we introduce a novel framework that efficiently and effectively generates radiology reports for high-resolution (HR) 3D volumes, based on large language models (LLMs).","Specifically, our framework utilizes low-resolution (LR) visual tokens as queries to mine information from HR tokens, preserving detailed HR information while reducing computational costs by only processing HR informed LR visual queries.","Further benefiting the field, we curate and release BIMCV-RG, a new dataset with 5,328 HR 3D volumes and paired reports, establishing the first benchmarks for report generation from 3D HR medical images.","Our method consistently surpasses existing methods on this benchmark across three different settings: normal-resolution, high-resolution inputs, and zero-shot domain transfer, all at an acceptable computational cost, trainable on a single A100-80G."],"url":"http://arxiv.org/abs/2406.07146v1","category":"cs.CV"}
{"created":"2024-06-11 10:45:41","title":"Failures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models","abstract":"In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others. Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug and legislative bodies to audit models. Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model's failure. In this paper, we introduce a post-hoc method that utilizes \\emph{deep reinforcement learning} to explore and construct the landscape of failure modes in pre-trained discriminative and generative models. With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes. We empirically show the effectiveness of the proposed method across common Computer Vision, Natural Language Processing, and Vision-Language tasks.","sentences":["In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others.","Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug and legislative bodies to audit models.","Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model's failure.","In this paper, we introduce a post-hoc method that utilizes \\emph{deep reinforcement learning} to explore and construct the landscape of failure modes in pre-trained discriminative and generative models.","With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes.","We empirically show the effectiveness of the proposed method across common Computer Vision, Natural Language Processing, and Vision-Language tasks."],"url":"http://arxiv.org/abs/2406.07145v1","category":"cs.LG"}
{"created":"2024-06-11 10:40:54","title":"Identifiable Object-Centric Representation Learning via Probabilistic Slot Attention","abstract":"Learning modular object-centric representations is crucial for systematic generalization. Existing methods show promising object-binding capabilities empirically, but theoretical identifiability guarantees remain relatively underdeveloped. Understanding when object-centric representations can theoretically be identified is crucial for scaling slot-based methods to high-dimensional images with correctness guarantees. To that end, we propose a probabilistic slot-attention algorithm that imposes an aggregate mixture prior over object-centric slot representations, thereby providing slot identifiability guarantees without supervision, up to an equivalence relation. We provide empirical verification of our theoretical identifiability result using both simple 2-dimensional data and high-resolution imaging datasets.","sentences":["Learning modular object-centric representations is crucial for systematic generalization.","Existing methods show promising object-binding capabilities empirically, but theoretical identifiability guarantees remain relatively underdeveloped.","Understanding when object-centric representations can theoretically be identified is crucial for scaling slot-based methods to high-dimensional images with correctness guarantees.","To that end, we propose a probabilistic slot-attention algorithm that imposes an aggregate mixture prior over object-centric slot representations, thereby providing slot identifiability guarantees without supervision, up to an equivalence relation.","We provide empirical verification of our theoretical identifiability result using both simple 2-dimensional data and high-resolution imaging datasets."],"url":"http://arxiv.org/abs/2406.07141v1","category":"cs.LG"}
{"created":"2024-06-11 10:35:11","title":"Path-based packing of icosahedral shells into multi-component aggregates","abstract":"Multi-component aggregates are being intensively researched in various fields because of their highly tunable properties and wide applications. Due to the complex configurational space of these systems, research would greatly benefit from a general theoretical framework for the prediction of stable structures, which, however, is largely incomplete at present. Here we propose a general theory for the construction of multi-component icosahedral structures by assembling concentric shells of different chiral and achiral types, consisting of particles of different sizes. By mapping shell sequences into paths in the hexagonal lattice, we establish simple and general rules for building a wide variety of magic icosahedral structures, and we evaluate the optimal size-mismatch between particles in the different shells. Our predictions are confirmed by numerical simulations for different systems.","sentences":["Multi-component aggregates are being intensively researched in various fields because of their highly tunable properties and wide applications.","Due to the complex configurational space of these systems, research would greatly benefit from a general theoretical framework for the prediction of stable structures, which, however, is largely incomplete at present.","Here we propose a general theory for the construction of multi-component icosahedral structures by assembling concentric shells of different chiral and achiral types, consisting of particles of different sizes.","By mapping shell sequences into paths in the hexagonal lattice, we establish simple and general rules for building a wide variety of magic icosahedral structures, and we evaluate the optimal size-mismatch between particles in the different shells.","Our predictions are confirmed by numerical simulations for different systems."],"url":"http://arxiv.org/abs/2406.07137v1","category":"physics.atm-clus"}
{"created":"2024-06-11 10:30:19","title":"Progressive Query Expansion for Retrieval Over Cost-constrained Data Sources","abstract":"Query expansion has been employed for a long time to improve the accuracy of query retrievers. Earlier works relied on pseudo-relevance feedback (PRF) techniques, which augment a query with terms extracted from documents retrieved in a first stage. However, the documents may be noisy hindering the effectiveness of the ranking. To avoid this, recent studies have instead used Large Language Models (LLMs) to generate additional content to expand a query. These techniques are prone to hallucination and also focus on the LLM usage cost. However, the cost may be dominated by the retrieval in several important practical scenarios, where the corpus is only available via APIs which charge a fee per retrieved document. We propose combining classic PRF techniques with LLMs and create a progressive query expansion algorithm ProQE that iteratively expands the query as it retrieves more documents. ProQE is compatible with both sparse and dense retrieval systems. Our experimental results on four retrieval datasets show that ProQE outperforms state-of-the-art baselines by 37% and is the most cost-effective.","sentences":["Query expansion has been employed for a long time to improve the accuracy of query retrievers.","Earlier works relied on pseudo-relevance feedback (PRF) techniques, which augment a query with terms extracted from documents retrieved in a first stage.","However, the documents may be noisy hindering the effectiveness of the ranking.","To avoid this, recent studies have instead used Large Language Models (LLMs) to generate additional content to expand a query.","These techniques are prone to hallucination and also focus on the LLM usage cost.","However, the cost may be dominated by the retrieval in several important practical scenarios, where the corpus is only available via APIs which charge a fee per retrieved document.","We propose combining classic PRF techniques with LLMs and create a progressive query expansion algorithm ProQE that iteratively expands the query as it retrieves more documents.","ProQE is compatible with both sparse and dense retrieval systems.","Our experimental results on four retrieval datasets show that ProQE outperforms state-of-the-art baselines by 37% and is the most cost-effective."],"url":"http://arxiv.org/abs/2406.07136v1","category":"cs.IR"}
{"created":"2024-06-11 10:29:32","title":"Smart Wireless Environment Enhanced Telecommunications: A Network Stabilisation Paradigm for Mobile Operators","abstract":"Due to the uncontrolled and complex real-life radio propagation environments, Claude Shannon's information theory of communications describes fundamental limits to state-of-the-art 5G radio access network (RAN) capacity, with respect to fixed radio resource usage. Fortunately, recent research has found that a holographic metasurface-based new physical layer architecture may hold the key to overcome these fundamental limits of current mobile networks under a new paradigm, smart wireless environment (SWE), where the long-standing challenge of mobile communications, fading channel hostility, may be solved, leading to a step-change boost in network performance and user experience. Despite recent research activities in SWE, the best way to implement it as a network operator remains an open challenge. In this industrial review, we adopt a novel yet realistic mobile channel stabilisation perspective for network operators to understand this paradigm shift. More specifically, we provide a technical analysis of the synergy between key next-gen mobile network enablers, e.g., holographic metasurface, wireless sensing, and machine intelligence, as well as of how this synergy leads to a robust future RAN architecture. Against the as yet unclear theoretical boundaries and low technology readiness level (TRL) of SWE enhanced telecommunications, we conclude by identifying critical challenges in future commercial deployments.","sentences":["Due to the uncontrolled and complex real-life radio propagation environments, Claude Shannon's information theory of communications describes fundamental limits to state-of-the-art 5G radio access network (RAN) capacity, with respect to fixed radio resource usage.","Fortunately, recent research has found that a holographic metasurface-based new physical layer architecture may hold the key to overcome these fundamental limits of current mobile networks under a new paradigm, smart wireless environment (SWE), where the long-standing challenge of mobile communications, fading channel hostility, may be solved, leading to a step-change boost in network performance and user experience.","Despite recent research activities in SWE, the best way to implement it as a network operator remains an open challenge.","In this industrial review, we adopt a novel yet realistic mobile channel stabilisation perspective for network operators to understand this paradigm shift.","More specifically, we provide a technical analysis of the synergy between key next-gen mobile network enablers, e.g., holographic metasurface, wireless sensing, and machine intelligence, as well as of how this synergy leads to a robust future RAN architecture.","Against the as yet unclear theoretical boundaries and low technology readiness level (TRL) of SWE enhanced telecommunications, we conclude by identifying critical challenges in future commercial deployments."],"url":"http://arxiv.org/abs/2406.07135v1","category":"cs.IT"}
{"created":"2024-06-11 10:29:24","title":"Translating speech with just images","abstract":"Visually grounded speech models link speech to images. We extend this connection by linking images to text via an existing image captioning system, and as a result gain the ability to map speech audio directly to text. This approach can be used for speech translation with just images by having the audio in a different language from the generated captions. We investigate such a system on a real low-resource language, Yor\\`ub\\'a, and propose a Yor\\`ub\\'a-to-English speech translation model that leverages pretrained components in order to be able to learn in the low-resource regime. To limit overfitting, we find that it is essential to use a decoding scheme that produces diverse image captions for training. Results show that the predicted translations capture the main semantics of the spoken audio, albeit in a simpler and shorter form.","sentences":["Visually grounded speech models link speech to images.","We extend this connection by linking images to text via an existing image captioning system, and as a result gain the ability to map speech audio directly to text.","This approach can be used for speech translation with just images by having the audio in a different language from the generated captions.","We investigate such a system on a real low-resource language, Yor\\`ub\\'a, and propose a Yor\\`ub\\'a-to-English speech translation model that leverages pretrained components in order to be able to learn in the low-resource regime.","To limit overfitting, we find that it is essential to use a decoding scheme that produces diverse image captions for training.","Results show that the predicted translations capture the main semantics of the spoken audio, albeit in a simpler and shorter form."],"url":"http://arxiv.org/abs/2406.07133v1","category":"eess.AS"}
{"created":"2024-06-11 10:28:02","title":"ICGAN: An implicit conditioning method for interpretable feature control of neural audio synthesis","abstract":"Neural audio synthesis methods can achieve high-fidelity and realistic sound generation by utilizing deep generative models. Such models typically rely on external labels which are often discrete as conditioning information to achieve guided sound generation. However, it remains difficult to control the subtle changes in sounds without appropriate and descriptive labels, especially given a limited dataset. This paper proposes an implicit conditioning method for neural audio synthesis using generative adversarial networks that allows for interpretable control of the acoustic features of synthesized sounds. Our technique creates a continuous conditioning space that enables timbre manipulation without relying on explicit labels. We further introduce an evaluation metric to explore controllability and demonstrate that our approach is effective in enabling a degree of controlled variation of different synthesized sound effects for in-domain and cross-domain sounds.","sentences":["Neural audio synthesis methods can achieve high-fidelity and realistic sound generation by utilizing deep generative models.","Such models typically rely on external labels which are often discrete as conditioning information to achieve guided sound generation.","However, it remains difficult to control the subtle changes in sounds without appropriate and descriptive labels, especially given a limited dataset.","This paper proposes an implicit conditioning method for neural audio synthesis using generative adversarial networks that allows for interpretable control of the acoustic features of synthesized sounds.","Our technique creates a continuous conditioning space that enables timbre manipulation without relying on explicit labels.","We further introduce an evaluation metric to explore controllability and demonstrate that our approach is effective in enabling a degree of controlled variation of different synthesized sound effects for in-domain and cross-domain sounds."],"url":"http://arxiv.org/abs/2406.07131v1","category":"cs.SD"}
{"created":"2024-06-11 10:25:01","title":"Assessing the Impact of Alpha Particles on Thermal Confinement in JET D-T Plasmas through Global GENE-Tango Simulations","abstract":"The capability of the global, electromagnetic gyrokinetic GENE code interfaced with the transport Tango solver is exploited to address the impact of fusion alpha particles (in their dual role of fast particles and heating source) on plasma profiles and performance at JET in the discharges with the highest quasi-stationary peak fusion power during the DTE2 experimental campaigns. Employing radially global nonlinear electromagnetic GENE-Tango simulations, we compare results with/without alpha particles and alpha heating. Our findings reveal that alpha particles have a negligible impact on turbulent transport, with GENE-Tango converging to similar plasma profiles regardless of their inclusion as a kinetic species in GENE. On the other hand, alpha heating is found to contribute to the peaking of the electron temperature profiles, leading to a 1keV drop on the on-axis electron temperature when alpha heating is neglected in Tango. The minimal impact of alpha particles on turbulent transport in this JET discharge - despite this being the shot with the highest fusion output - is attributed to the low content of fusion alpha in this discharge. To assess the potential impact of alpha particles on turbulent transport in regimes with higher alpha particle density, as expected in ITER and fusion reactors, we artificially increased the alpha particle concentration to levels expected for ITER. By performing global nonlinear GENE standalone simulations, we found that increasing the alpha particle density beyond five times the nominal value lead to significant overall turbulence destabilization.","sentences":["The capability of the global, electromagnetic gyrokinetic GENE code interfaced with the transport Tango solver is exploited to address the impact of fusion alpha particles (in their dual role of fast particles and heating source) on plasma profiles and performance at JET in the discharges with the highest quasi-stationary peak fusion power during the DTE2 experimental campaigns.","Employing radially global nonlinear electromagnetic GENE-Tango simulations, we compare results with/without alpha particles and alpha heating.","Our findings reveal that alpha particles have a negligible impact on turbulent transport, with GENE-Tango converging to similar plasma profiles regardless of their inclusion as a kinetic species in GENE.","On the other hand, alpha heating is found to contribute to the peaking of the electron temperature profiles, leading to a 1keV drop on the on-axis electron temperature when alpha heating is neglected in Tango.","The minimal impact of alpha particles on turbulent transport in this JET discharge - despite this being the shot with the highest fusion output - is attributed to the low content of fusion alpha in this discharge.","To assess the potential impact of alpha particles on turbulent transport in regimes with higher alpha particle density, as expected in ITER and fusion reactors, we artificially increased the alpha particle concentration to levels expected for ITER.","By performing global nonlinear GENE standalone simulations, we found that increasing the alpha particle density beyond five times the nominal value lead to significant overall turbulence destabilization."],"url":"http://arxiv.org/abs/2406.07130v1","category":"physics.plasm-ph"}
{"created":"2024-06-11 10:24:02","title":"Mining Frequent Structures in Conceptual Models","abstract":"The problem of using structured methods to represent knowledge is well-known in conceptual modeling and has been studied for many years. It has been proven that adopting modeling patterns represents an effective structural method. Patterns are, indeed, generalizable recurrent structures that can be exploited as solutions to design problems. They aid in understanding and improving the process of creating models. The undeniable value of using patterns in conceptual modeling was demonstrated in several experimental studies. However, discovering patterns in conceptual models is widely recognized as a highly complex task and a systematic solution to pattern identification is currently lacking. In this paper, we propose a general approach to the problem of discovering frequent structures, as they occur in conceptual modeling languages. As proof of concept for our scientific contribution, we provide an implementation of the approach, by focusing on UML class diagrams, in particular OntoUML models. This implementation comprises an exploratory tool, which, through the combination of a frequent subgraph mining algorithm and graph manipulation techniques, can process multiple conceptual models and discover recurrent structures according to multiple criteria. The primary objective is to offer a support facility for language engineers. This can be employed to leverage both good and bad modeling practices, to evolve and maintain the conceptual modeling language, and to promote the reuse of encoded experience in designing better models with the given language.","sentences":["The problem of using structured methods to represent knowledge is well-known in conceptual modeling and has been studied for many years.","It has been proven that adopting modeling patterns represents an effective structural method.","Patterns are, indeed, generalizable recurrent structures that can be exploited as solutions to design problems.","They aid in understanding and improving the process of creating models.","The undeniable value of using patterns in conceptual modeling was demonstrated in several experimental studies.","However, discovering patterns in conceptual models is widely recognized as a highly complex task and a systematic solution to pattern identification is currently lacking.","In this paper, we propose a general approach to the problem of discovering frequent structures, as they occur in conceptual modeling languages.","As proof of concept for our scientific contribution, we provide an implementation of the approach, by focusing on UML class diagrams, in particular OntoUML models.","This implementation comprises an exploratory tool, which, through the combination of a frequent subgraph mining algorithm and graph manipulation techniques, can process multiple conceptual models and discover recurrent structures according to multiple criteria.","The primary objective is to offer a support facility for language engineers.","This can be employed to leverage both good and bad modeling practices, to evolve and maintain the conceptual modeling language, and to promote the reuse of encoded experience in designing better models with the given language."],"url":"http://arxiv.org/abs/2406.07129v1","category":"cs.AI"}
{"created":"2024-06-11 10:23:11","title":"Rigidity of the extremal Kerr-Newman horizon","abstract":"We prove that the intrinsic geometry of compact cross-sections of an extremal horizon in four-dimensional Einstein-Maxwell theory must admit a Killing vector field or is static. This implies that any such horizon must be an extremal Kerr-Newman horizon and completes the classification of the associated near-horizon geometries. The same results hold with a cosmological constant.","sentences":["We prove that the intrinsic geometry of compact cross-sections of an extremal horizon in four-dimensional Einstein-Maxwell theory must admit a Killing vector field or is static.","This implies that any such horizon must be an extremal Kerr-Newman horizon and completes the classification of the associated near-horizon geometries.","The same results hold with a cosmological constant."],"url":"http://arxiv.org/abs/2406.07128v1","category":"gr-qc"}
{"created":"2024-06-11 10:18:58","title":"Logical Distillation of Graph Neural Networks","abstract":"We present a logic based interpretable model for learning on graphs and an algorithm to distill this model from a Graph Neural Network (GNN). Recent results have shown connections between the expressivity of GNNs and the two-variable fragment of first-order logic with counting quantifiers (C2). We introduce a decision-tree based model which leverages an extension of C2 to distill interpretable logical classifiers from GNNs. We test our approach on multiple GNN architectures. The distilled models are interpretable, succinct, and attain similar accuracy to the underlying GNN. Furthermore, when the ground truth is expressible in C2, our approach outperforms the GNN.","sentences":["We present a logic based interpretable model for learning on graphs and an algorithm to distill this model from a Graph Neural Network (GNN).","Recent results have shown connections between the expressivity of GNNs and the two-variable fragment of first-order logic with counting quantifiers (C2).","We introduce a decision-tree based model which leverages an extension of C2 to distill interpretable logical classifiers from GNNs.","We test our approach on multiple GNN architectures.","The distilled models are interpretable, succinct, and attain similar accuracy to the underlying GNN.","Furthermore, when the ground truth is expressible in C2, our approach outperforms the GNN."],"url":"http://arxiv.org/abs/2406.07126v1","category":"cs.LG"}
{"created":"2024-06-11 17:56:14","title":"Hearing Anything Anywhere","abstract":"Recent years have seen immense progress in 3D computer vision and computer graphics, with emerging tools that can virtualize real-world 3D environments for numerous Mixed Reality (XR) applications. However, alongside immersive visual experiences, immersive auditory experiences are equally vital to our holistic perception of an environment. In this paper, we aim to reconstruct the spatial acoustic characteristics of an arbitrary environment given only a sparse set of (roughly 12) room impulse response (RIR) recordings and a planar reconstruction of the scene, a setup that is easily achievable by ordinary users. To this end, we introduce DiffRIR, a differentiable RIR rendering framework with interpretable parametric models of salient acoustic features of the scene, including sound source directivity and surface reflectivity. This allows us to synthesize novel auditory experiences through the space with any source audio. To evaluate our method, we collect a dataset of RIR recordings and music in four diverse, real environments. We show that our model outperforms state-ofthe-art baselines on rendering monaural and binaural RIRs and music at unseen locations, and learns physically interpretable parameters characterizing acoustic properties of the sound source and surfaces in the scene.","sentences":["Recent years have seen immense progress in 3D computer vision and computer graphics, with emerging tools that can virtualize real-world 3D environments for numerous Mixed Reality (XR) applications.","However, alongside immersive visual experiences, immersive auditory experiences are equally vital to our holistic perception of an environment.","In this paper, we aim to reconstruct the spatial acoustic characteristics of an arbitrary environment given only a sparse set of (roughly 12) room impulse response (RIR) recordings and a planar reconstruction of the scene, a setup that is easily achievable by ordinary users.","To this end, we introduce DiffRIR, a differentiable RIR rendering framework with interpretable parametric models of salient acoustic features of the scene, including sound source directivity and surface reflectivity.","This allows us to synthesize novel auditory experiences through the space with any source audio.","To evaluate our method, we collect a dataset of RIR recordings and music in four diverse, real environments.","We show that our model outperforms state-ofthe-art baselines on rendering monaural and binaural RIRs and music at unseen locations, and learns physically interpretable parameters characterizing acoustic properties of the sound source and surfaces in the scene."],"url":"http://arxiv.org/abs/2406.07532v1","category":"cs.SD"}
{"created":"2024-06-11 17:39:46","title":"Just Because We Camp, Doesn't Mean We Should: The Ethics of Modelling Queer Voices","abstract":"Modern voice cloning models claim to be able to capture a diverse range of voices. We test the ability of a typical pipeline to capture the style known colloquially as \"gay voice\" and notice a homogenisation effect: synthesised speech is rated as sounding significantly \"less gay\" (by LGBTQ+ participants) than its corresponding ground-truth for speakers with \"gay voice\", but ratings actually increase for control speakers. Loss of \"gay voice\" has implications for accessibility. We also find that for speakers with \"gay voice\", loss of \"gay voice\" corresponds to lower similarity ratings.   However, we caution that improving the ability of such models to synthesise ``gay voice'' comes with a great number of risks. We use this pipeline as a starting point for a discussion on the ethics of modelling queer voices more broadly. Collecting \"clean\" queer data has safety and fairness ramifications, and the resulting technology may cause harms from mockery to death.","sentences":["Modern voice cloning models claim to be able to capture a diverse range of voices.","We test the ability of a typical pipeline to capture the style known colloquially as \"gay voice\" and notice a homogenisation effect: synthesised speech is rated as sounding significantly \"less gay\" (by LGBTQ+ participants) than its corresponding ground-truth for speakers with \"gay voice\", but ratings actually increase for control speakers.","Loss of \"gay voice\" has implications for accessibility.","We also find that for speakers with \"gay voice\", loss of \"gay voice\" corresponds to lower similarity ratings.   ","However, we caution that improving the ability of such models to synthesise ``gay voice'' comes with a great number of risks.","We use this pipeline as a starting point for a discussion on the ethics of modelling queer voices more broadly.","Collecting \"clean\" queer data has safety and fairness ramifications, and the resulting technology may cause harms from mockery to death."],"url":"http://arxiv.org/abs/2406.07504v1","category":"cs.CL"}
{"created":"2024-06-11 17:23:05","title":"Neutrino magnetic dipole portal with low energy neutrino nucleus scattering data","abstract":"Sterile neutrinos that couple to the Standard Model via the neutrino magnetic dipole portals have been extensively studied at various experiments. In this work, we scrutinize these interactions for sterile neutrinos in the mass range of $\\unit[0.1]{}-\\unit[50]{MeV}$ through the nuclear and electron recoils at various neutrino scattering experiments. For the $e$-flavor specific dipole portal, we demonstrate that Dresden-II can provide leading constraints for $m_N \\lesssim \\unit[0.5]{MeV}$, setting aside currently unresolved theoretical uncertainties. For the $\\mu$-flavor case, we show that the COHERENT experiment can probe a unique parameter region for $m_N$ in the range of $\\unit[10]{}-\\unit[40]{MeV}$ with the full dataset collected by the CsI[Na] scintillation detector, including both the energy and timing structure of the neutrino beam. We also present limits on the parameter regions of the $\\tau$-flavor dipole portal using measurements of the solar neutrino flux from dark matter direct detection experiments.","sentences":["Sterile neutrinos that couple to the Standard Model via the neutrino magnetic dipole portals have been extensively studied at various experiments.","In this work, we scrutinize these interactions for sterile neutrinos in the mass range of $\\unit[0.1]{}-\\unit[50]{MeV}$ through the nuclear and electron recoils at various neutrino scattering experiments.","For the $e$-flavor specific dipole portal, we demonstrate that Dresden-II can provide leading constraints for $m_N \\lesssim \\unit[0.5]{MeV}$, setting aside currently unresolved theoretical uncertainties.","For the $\\mu$-flavor case, we show that the COHERENT experiment can probe a unique parameter region for $m_N$ in the range of $\\unit[10]{}-\\unit[40]{MeV}$ with the full dataset collected by the CsI[Na] scintillation detector, including both the energy and timing structure of the neutrino beam.","We also present limits on the parameter regions of the $\\tau$-flavor dipole portal using measurements of the solar neutrino flux from dark matter direct detection experiments."],"url":"http://arxiv.org/abs/2406.07477v1","category":"hep-ph"}
{"created":"2024-06-11 16:51:47","title":"Metastability in networks of nonlinear stochastic integrate-and-fire neurons","abstract":"Neurons in the brain continuously process the barrage of sensory inputs they receive from the environment. A wide array of experimental work has shown that the collective activity of neural populations encodes and processes this constant bombardment of information. How these collective patterns of activity depend on single neuron properties is often unclear. Single-neuron recordings have shown that individual neural responses to inputs are nonlinear, which prevents a straightforward extrapolation from single neuron features to emergent collective states. In this work, we use a field theoretic formulation of a stochastic leaky integrate-and-fire model to study the impact of nonlinear intensity functions on macroscopic network activity. We show that the interplay between nonlinear spike emission and membrane potential resets can i) give rise to metastable transitions between active firing rate states, and ii) can enhance or suppress mean firing rates and membrane potentials in opposite directions.","sentences":["Neurons in the brain continuously process the barrage of sensory inputs they receive from the environment.","A wide array of experimental work has shown that the collective activity of neural populations encodes and processes this constant bombardment of information.","How these collective patterns of activity depend on single neuron properties is often unclear.","Single-neuron recordings have shown that individual neural responses to inputs are nonlinear, which prevents a straightforward extrapolation from single neuron features to emergent collective states.","In this work, we use a field theoretic formulation of a stochastic leaky integrate-and-fire model to study the impact of nonlinear intensity functions on macroscopic network activity.","We show that the interplay between nonlinear spike emission and membrane potential resets can i) give rise to metastable transitions between active firing rate states, and ii) can enhance or suppress mean firing rates and membrane potentials in opposite directions."],"url":"http://arxiv.org/abs/2406.07445v1","category":"q-bio.NC"}
{"created":"2024-06-11 16:05:11","title":"Stable SAP-like collective charging of ultracold atoms quantum batteries","abstract":"We propose a novel quantum battery realized with a few interacting particles in a three-well system with different on-site energies, which could be realized with ultracold atom platforms. We prepare the initial state in the lowest energy well and charge the battery using a Spatial Adiabatic Passage (SAP)-based protocol, enabling the population of a higher energy well. We examine the charging under varying interaction strengths and reveal that the consideration of collective charging results in an intriguing oscillatory behavior of the final charge for finite interactions, through diabatic evolution. Our findings open a new avenue for building stable and controllable quantum batteries.","sentences":["We propose a novel quantum battery realized with a few interacting particles in a three-well system with different on-site energies, which could be realized with ultracold atom platforms.","We prepare the initial state in the lowest energy well and charge the battery using a Spatial Adiabatic Passage (SAP)-based protocol, enabling the population of a higher energy well.","We examine the charging under varying interaction strengths and reveal that the consideration of collective charging results in an intriguing oscillatory behavior of the final charge for finite interactions, through diabatic evolution.","Our findings open a new avenue for building stable and controllable quantum batteries."],"url":"http://arxiv.org/abs/2406.07397v1","category":"quant-ph"}
{"created":"2024-06-11 15:41:56","title":"Improving the realism of robotic surgery simulation through injection of learning-based estimated errors","abstract":"The development of algorithms for automation of subtasks during robotic surgery can be accelerated by the availability of realistic simulation environments. In this work, we focus on one aspect of the realism of a surgical simulator, which is the positional accuracy of the robot. In current simulators, robots have perfect or near-perfect accuracy, which is not representative of their physical counterparts. We therefore propose a pair of neural networks, trained by data collected from a physical robot, to estimate both the controller error and the kinematic and non-kinematic error. These error estimates are then injected within the simulator to produce a simulated robot that has the characteristic performance of the physical robot. In this scenario, we believe it is sufficient for the estimated error used in the simulation to have a statistically similar distribution to the actual error of the physical robot. This is less stringent, and therefore more tenable, than the requirement for error compensation of a physical robot, where the estimated error should equal the actual error. Our results demonstrate that error injection reduces the mean position and orientation differences between the simulated and physical robots from 5.0 mm / 3.6 deg to 1.3 mm / 1.7 deg, respectively, which represents reductions by factors of 3.8 and 2.1.","sentences":["The development of algorithms for automation of subtasks during robotic surgery can be accelerated by the availability of realistic simulation environments.","In this work, we focus on one aspect of the realism of a surgical simulator, which is the positional accuracy of the robot.","In current simulators, robots have perfect or near-perfect accuracy, which is not representative of their physical counterparts.","We therefore propose a pair of neural networks, trained by data collected from a physical robot, to estimate both the controller error and the kinematic and non-kinematic error.","These error estimates are then injected within the simulator to produce a simulated robot that has the characteristic performance of the physical robot.","In this scenario, we believe it is sufficient for the estimated error used in the simulation to have a statistically similar distribution to the actual error of the physical robot.","This is less stringent, and therefore more tenable, than the requirement for error compensation of a physical robot, where the estimated error should equal the actual error.","Our results demonstrate that error injection reduces the mean position and orientation differences between the simulated and physical robots from 5.0 mm / 3.6 deg to 1.3 mm / 1.7 deg, respectively, which represents reductions by factors of 3.8 and 2.1."],"url":"http://arxiv.org/abs/2406.07375v1","category":"cs.RO"}
{"created":"2024-06-11 14:43:43","title":"Experimental Modeling of Chiral Active Robots and a Minimal Model of Non-Gaussian Displacements","abstract":"We design 3D-printed motor-driven active particles and find that their dynamics can be characterized using the model of overdamped chiral active Brownian particles (ABPs), as demonstrated by measured angular statistics and translational mean squared displacements (MSDs). Furthermore, we propose a minimal model that reproduces the double-peak velocity distributions and further predicts a transition from the single-peak to the double-peak displacement distributions in short-time regimes. The model provides a clear physics picture of these phenomena, originating from the competition between the active motion and the translational diffusion. Our experiments confirm such picture. The minimal model enhances our understanding of activity-driven non-Gaussian phenomena. The designed particles could be further applied in the study of collective chiral motions.","sentences":["We design 3D-printed motor-driven active particles and find that their dynamics can be characterized using the model of overdamped chiral active Brownian particles (ABPs), as demonstrated by measured angular statistics and translational mean squared displacements (MSDs).","Furthermore, we propose a minimal model that reproduces the double-peak velocity distributions and further predicts a transition from the single-peak to the double-peak displacement distributions in short-time regimes.","The model provides a clear physics picture of these phenomena, originating from the competition between the active motion and the translational diffusion.","Our experiments confirm such picture.","The minimal model enhances our understanding of activity-driven non-Gaussian phenomena.","The designed particles could be further applied in the study of collective chiral motions."],"url":"http://arxiv.org/abs/2406.07313v1","category":"cond-mat.soft"}
{"created":"2024-06-11 14:28:24","title":"Exploring Large Language Models for Relevance Judgments in Tetun","abstract":"The Cranfield paradigm has served as a foundational approach for developing test collections, with relevance judgments typically conducted by human assessors. However, the emergence of large language models (LLMs) has introduced new possibilities for automating these tasks. This paper explores the feasibility of using LLMs to automate relevance assessments, particularly within the context of low-resource languages. In our study, LLMs are employed to automate relevance judgment tasks, by providing a series of query-document pairs in Tetun as the input text. The models are tasked with assigning relevance scores to each pair, where these scores are then compared to those from human annotators to evaluate the inter-annotator agreement levels. Our investigation reveals results that align closely with those reported in studies of high-resource languages.","sentences":["The Cranfield paradigm has served as a foundational approach for developing test collections, with relevance judgments typically conducted by human assessors.","However, the emergence of large language models (LLMs) has introduced new possibilities for automating these tasks.","This paper explores the feasibility of using LLMs to automate relevance assessments, particularly within the context of low-resource languages.","In our study, LLMs are employed to automate relevance judgment tasks, by providing a series of query-document pairs in Tetun as the input text.","The models are tasked with assigning relevance scores to each pair, where these scores are then compared to those from human annotators to evaluate the inter-annotator agreement levels.","Our investigation reveals results that align closely with those reported in studies of high-resource languages."],"url":"http://arxiv.org/abs/2406.07299v1","category":"cs.IR"}
{"created":"2024-06-11 11:50:29","title":"Bilevel optimization with sustainability perspective: a survey on applications","abstract":"Bilevel optimization, a well-established field for modeling hierarchical decision-making problems, has recently intersected with sustainability studies and practices, resulting in a series of works focusing on bilevel optimization problems involving multiple decision makers with diverse economic, environmental, and social objectives. This survey offers a comprehensive overview of sustainable bilevel optimization applications. First, we introduce the main concepts related to the nature of bilevel optimization problems and present some typical mathematical formulations for bilevel pricing problems that cover the majority of the collected applications. Then, we review the most relevant works published in sustainable bilevel optimization, giving a classification based on the application domains and their association with well-known operations research problems, while briefly discussing the proposed solution methodologies. We survey applications on transportation and logistics, production planning and manufacturing, water, waste, and agriculture management, supply chains, and disaster prevention and response. Finally, we outline a list of open questions and opportunities for future research in this domain.","sentences":["Bilevel optimization, a well-established field for modeling hierarchical decision-making problems, has recently intersected with sustainability studies and practices, resulting in a series of works focusing on bilevel optimization problems involving multiple decision makers with diverse economic, environmental, and social objectives.","This survey offers a comprehensive overview of sustainable bilevel optimization applications.","First, we introduce the main concepts related to the nature of bilevel optimization problems and present some typical mathematical formulations for bilevel pricing problems that cover the majority of the collected applications.","Then, we review the most relevant works published in sustainable bilevel optimization, giving a classification based on the application domains and their association with well-known operations research problems, while briefly discussing the proposed solution methodologies.","We survey applications on transportation and logistics, production planning and manufacturing, water, waste, and agriculture management, supply chains, and disaster prevention and response.","Finally, we outline a list of open questions and opportunities for future research in this domain."],"url":"http://arxiv.org/abs/2406.07184v1","category":"math.OC"}
{"created":"2024-06-11 11:32:01","title":"ULog: Unsupervised Log Parsing with Large Language Models through Log Contrastive Units","abstract":"Log parsing serves as an essential prerequisite for various log analysis tasks. Recent advancements in this field have improved parsing accuracy by leveraging the semantics in logs through fine-tuning large language models (LLMs) or learning from in-context demonstrations. However, these methods heavily depend on labeled examples to achieve optimal performance. In practice, collecting sufficient labeled data is challenging due to the large scale and continuous evolution of logs, leading to performance degradation of existing log parsers after deployment. To address this issue, we propose ULog, an unsupervised LLM-based method for efficient and off-the-shelf log parsing. Our key insight is that while LLMs may struggle with direct log parsing, their performance can be significantly enhanced through comparative analysis across multiple logs that differ only in their parameter parts. We refer to such groups of logs as Log Contrastive Units (LCUs). Given the vast volume of logs, obtaining LCUs is difficult. Therefore, ULog introduces a hybrid ranking scheme to effectively search for LCUs by jointly considering the commonality and variability among logs. Additionally, ULog crafts a novel parsing prompt for LLMs to identify contrastive patterns and extract meaningful log structures from LCUs. Experiments on large-scale public datasets demonstrate that ULog significantly outperforms state-of-the-art log parsers in terms of accuracy and efficiency, providing an effective and scalable solution for real-world deployment.","sentences":["Log parsing serves as an essential prerequisite for various log analysis tasks.","Recent advancements in this field have improved parsing accuracy by leveraging the semantics in logs through fine-tuning large language models (LLMs) or learning from in-context demonstrations.","However, these methods heavily depend on labeled examples to achieve optimal performance.","In practice, collecting sufficient labeled data is challenging due to the large scale and continuous evolution of logs, leading to performance degradation of existing log parsers after deployment.","To address this issue, we propose ULog, an unsupervised LLM-based method for efficient and off-the-shelf log parsing.","Our key insight is that while LLMs may struggle with direct log parsing, their performance can be significantly enhanced through comparative analysis across multiple logs that differ only in their parameter parts.","We refer to such groups of logs as Log Contrastive Units (LCUs).","Given the vast volume of logs, obtaining LCUs is difficult.","Therefore, ULog introduces a hybrid ranking scheme to effectively search for LCUs by jointly considering the commonality and variability among logs.","Additionally, ULog crafts a novel parsing prompt for LLMs to identify contrastive patterns and extract meaningful log structures from LCUs.","Experiments on large-scale public datasets demonstrate that ULog significantly outperforms state-of-the-art log parsers in terms of accuracy and efficiency, providing an effective and scalable solution for real-world deployment."],"url":"http://arxiv.org/abs/2406.07174v1","category":"cs.SE"}
{"created":"2024-06-11 11:15:49","title":"Collective dynamics of active dumbbells near a circular obstacle","abstract":"In this article, we present the collective dynamics of active dumbbells in the presence of a static circular obstacle using Brownian dynamics simulation. The active dumbbells aggregate on the surface of a circular obstacle beyond a critical radius. The aggregation is non-uniform along the circumference, and the aggregate size increases with the activity and the curvature radius. The dense aggregate of active dumbbells displays persistent rotational motion with a certain angular speed, which linearly increases with the activity. Further, we show the strong polar ordering of the active dumbbells within the aggregate. The polar ordering exhibits a long-range correlation, with the correlation length corresponding to the aggregate size. Additionally, we show that the residence time of an active dumbbell on the obstacle surface grows rapidly with area fraction due to many-body interactions that lead to a slowdown of the rotational diffusion. The article further considers the dynamical behavior of a tracer particle in the solution of active dumbbells. Interestingly, the speed of the passive tracer particle displays a crossover from monotonically decreasing to increasing with the tracer particle's size upon increasing the dumbbells' speed. Furthermore, the effective diffusion of the tracer particle displays the non-monotonic behavior with area fraction; the initial increase of the diffusivity is followed by a decrease for larger area fraction.","sentences":["In this article, we present the collective dynamics of active dumbbells in the presence of a static circular obstacle using Brownian dynamics simulation.","The active dumbbells aggregate on the surface of a circular obstacle beyond a critical radius.","The aggregation is non-uniform along the circumference, and the aggregate size increases with the activity and the curvature radius.","The dense aggregate of active dumbbells displays persistent rotational motion with a certain angular speed, which linearly increases with the activity.","Further, we show the strong polar ordering of the active dumbbells within the aggregate.","The polar ordering exhibits a long-range correlation, with the correlation length corresponding to the aggregate size.","Additionally, we show that the residence time of an active dumbbell on the obstacle surface grows rapidly with area fraction due to many-body interactions that lead to a slowdown of the rotational diffusion.","The article further considers the dynamical behavior of a tracer particle in the solution of active dumbbells.","Interestingly, the speed of the passive tracer particle displays a crossover from monotonically decreasing to increasing with the tracer particle's size upon increasing the dumbbells' speed.","Furthermore, the effective diffusion of the tracer particle displays the non-monotonic behavior with area fraction; the initial increase of the diffusivity is followed by a decrease for larger area fraction."],"url":"http://arxiv.org/abs/2406.07164v1","category":"cond-mat.soft"}
{"created":"2024-06-11 10:29:28","title":"Categories of Line Defects and Cohomological Hall Algebras","abstract":"Any four-dimensional Supersymmetric Quantum Field Theory with eight supercharges can be associated to a monoidal category of BPS line defects. Any Coulomb vacuum of such a theory can be conjecturally associated to an \"algebra of BPS particles'', exemplified by certain Cohomological Hall Algebras. We conjecture the existence of a monoidal functor from the category of line defects to a certain category of bimodules for the BPS Algebra in any Coulomb vacuum. We describe images of simple objects under the conjectural functor and study their monoidal structure in examples. As we vary the choice of vacuum, we expect the collection of functors associated to any given theory to capture the full information of the original monoidal category of lines.","sentences":["Any four-dimensional Supersymmetric Quantum Field Theory with eight supercharges can be associated to a monoidal category of BPS line defects.","Any Coulomb vacuum of such a theory can be conjecturally associated to an \"algebra of BPS particles'', exemplified by certain Cohomological Hall Algebras.","We conjecture the existence of a monoidal functor from the category of line defects to a certain category of bimodules for the BPS Algebra in any Coulomb vacuum.","We describe images of simple objects under the conjectural functor and study their monoidal structure in examples.","As we vary the choice of vacuum, we expect the collection of functors associated to any given theory to capture the full information of the original monoidal category of lines."],"url":"http://arxiv.org/abs/2406.07134v1","category":"hep-th"}
{"created":"2024-06-11 10:16:55","title":"CARACAS: vehiCular ArchitectuRe for detAiled Can Attacks Simulation","abstract":"Modern vehicles are increasingly vulnerable to attacks that exploit network infrastructures, particularly the Controller Area Network (CAN) networks. To effectively counter such threats using contemporary tools like Intrusion Detection Systems (IDSs) based on data analysis and classification, large datasets of CAN messages become imperative. This paper delves into the feasibility of generating synthetic datasets by harnessing the modeling capabilities of simulation frameworks such as Simulink coupled with a robust representation of attack models to present CARACAS, a vehicular model, including component control via CAN messages and attack injection capabilities. CARACAS showcases the efficacy of this methodology, including a Battery Electric Vehicle (BEV) model, and focuses on attacks targeting torque control in two distinct scenarios.","sentences":["Modern vehicles are increasingly vulnerable to attacks that exploit network infrastructures, particularly the Controller Area Network (CAN) networks.","To effectively counter such threats using contemporary tools like Intrusion Detection Systems (IDSs) based on data analysis and classification, large datasets of CAN messages become imperative.","This paper delves into the feasibility of generating synthetic datasets by harnessing the modeling capabilities of simulation frameworks such as Simulink coupled with a robust representation of attack models to present CARACAS, a vehicular model, including component control via CAN messages and attack injection capabilities.","CARACAS showcases the efficacy of this methodology, including a Battery Electric Vehicle (BEV) model, and focuses on attacks targeting torque control in two distinct scenarios."],"url":"http://arxiv.org/abs/2406.07125v1","category":"cs.CR"}
{"created":"2024-06-11 10:12:10","title":"CHARME: A chain-based reinforcement learning approach for the minor embedding problem","abstract":"Quantum Annealing (QA) holds great potential for solving combinatorial optimization problems efficiently. However, the effectiveness of QA algorithms heavily relies on the embedding of problem instances, represented as logical graphs, into the quantum unit processing (QPU) whose topology is in form of a limited connectivity graph, known as the minor embedding Problem. Existing methods for the minor embedding problem suffer from scalability issues when confronted with larger problem sizes. In this paper, we propose a novel approach utilizing Reinforcement Learning (RL) techniques to address the minor embedding problem, named CHARME. CHARME includes three key components: a Graph Neural Network (GNN) architecture for policy modeling, a state transition algorithm ensuring solution validity, and an order exploration strategy for effective training. Through comprehensive experiments on synthetic and real-world instances, we demonstrate that the efficiency of our proposed order exploration strategy as well as our proposed RL framework, CHARME. In details, CHARME yields superior solutions compared to fast embedding methods such as Minorminer and ATOM. Moreover, our method surpasses the OCT-based approach, known for its slower runtime but high-quality solutions, in several cases. In addition, our proposed exploration enhances the efficiency of the training of the CHARME framework by providing better solutions compared to the greedy strategy.","sentences":["Quantum Annealing (QA) holds great potential for solving combinatorial optimization problems efficiently.","However, the effectiveness of QA algorithms heavily relies on the embedding of problem instances, represented as logical graphs, into the quantum unit processing (QPU) whose topology is in form of a limited connectivity graph, known as the minor embedding Problem.","Existing methods for the minor embedding problem suffer from scalability issues when confronted with larger problem sizes.","In this paper, we propose a novel approach utilizing Reinforcement Learning (RL) techniques to address the minor embedding problem, named CHARME.","CHARME includes three key components: a Graph Neural Network (GNN) architecture for policy modeling, a state transition algorithm ensuring solution validity, and an order exploration strategy for effective training.","Through comprehensive experiments on synthetic and real-world instances, we demonstrate that the efficiency of our proposed order exploration strategy as well as our proposed RL framework, CHARME.","In details, CHARME yields superior solutions compared to fast embedding methods such as Minorminer and ATOM.","Moreover, our method surpasses the OCT-based approach, known for its slower runtime but high-quality solutions, in several cases.","In addition, our proposed exploration enhances the efficiency of the training of the CHARME framework by providing better solutions compared to the greedy strategy."],"url":"http://arxiv.org/abs/2406.07124v1","category":"cs.AI"}
{"created":"2024-06-11 10:06:53","title":"T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text","abstract":"In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.","sentences":["In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook.","However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions.","To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding.","Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text.","Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method.","To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.","Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data.","Our project homepage is https://t2sgpt-demo.yinaoxiong.cn."],"url":"http://arxiv.org/abs/2406.07119v1","category":"cs.CV"}
{"created":"2024-06-11 10:02:07","title":"Augmenting Offline RL with Unlabeled Data","abstract":"Recent advancements in offline Reinforcement Learning (Offline RL) have led to an increased focus on methods based on conservative policy updates to address the Out-of-Distribution (OOD) issue. These methods typically involve adding behavior regularization or modifying the critic learning objective, focusing primarily on states or actions with substantial dataset support. However, we challenge this prevailing notion by asserting that the absence of an action or state from a dataset does not necessarily imply its suboptimality. In this paper, we propose a novel approach to tackle the OOD problem. We introduce an offline RL teacher-student framework, complemented by a policy similarity measure. This framework enables the student policy to gain insights not only from the offline RL dataset but also from the knowledge transferred by a teacher policy. The teacher policy is trained using another dataset consisting of state-action pairs, which can be viewed as practical domain knowledge acquired without direct interaction with the environment. We believe this additional knowledge is key to effectively solving the OOD issue. This research represents a significant advancement in integrating a teacher-student network into the actor-critic framework, opening new avenues for studies on knowledge transfer in offline RL and effectively addressing the OOD challenge.","sentences":["Recent advancements in offline Reinforcement Learning (Offline RL) have led to an increased focus on methods based on conservative policy updates to address the Out-of-Distribution (OOD) issue.","These methods typically involve adding behavior regularization or modifying the critic learning objective, focusing primarily on states or actions with substantial dataset support.","However, we challenge this prevailing notion by asserting that the absence of an action or state from a dataset does not necessarily imply its suboptimality.","In this paper, we propose a novel approach to tackle the OOD problem.","We introduce an offline RL teacher-student framework, complemented by a policy similarity measure.","This framework enables the student policy to gain insights not only from the offline RL dataset but also from the knowledge transferred by a teacher policy.","The teacher policy is trained using another dataset consisting of state-action pairs, which can be viewed as practical domain knowledge acquired without direct interaction with the environment.","We believe this additional knowledge is key to effectively solving the OOD issue.","This research represents a significant advancement in integrating a teacher-student network into the actor-critic framework, opening new avenues for studies on knowledge transfer in offline RL and effectively addressing the OOD challenge."],"url":"http://arxiv.org/abs/2406.07117v1","category":"cs.AI"}
{"created":"2024-06-11 10:00:18","title":"Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees","abstract":"Tool-augmented large language models (LLMs) leverage tools, often in the form of APIs, to enhance their reasoning capabilities on complex tasks, thus taking on the role of intelligent agents interacting with the real world. The recently introduced ToolLLaMA model by Qin et al. [2024] utilizes the depth-first search-based decision tree (DFSDT) method for reasoning with $16000+$ real-world APIs, which effectively improves the planning and inferencing performance of tool-augmented LLMs compared to traditional chain reasoning approaches. However, their approach only employs successful paths from decision trees (also called inference trees) for supervised fine-tuning (SFT) during training, which does not fully exploit the advantages of the tree of thought. In this study, we propose an inference trajectory optimization framework based on the preference data extracted from decision trees to address this limitation. We first introduce a novel method for constructing preference data from the tree of thought, capitalizing on the failed explorations previously overlooked in the trees. Specifically, we generate an effective step-wise preference dataset, named ToolPreference, for tool use based on the ToolBench dataset. In the subsequent training phase, we first fine-tune the LLM with tool-usage expert trajectories and then use these step-wise preference pairs for direct preference optimization (DPO) to update the policy of the LLM, resulting in our ToolPrefer-LLaMA (TP-LLaMA) model. Our experiments demonstrate that by obtaining insights from errors in inference trees, TP-LLaMA significantly outperforms the baselines across almost all test scenarios by a large margin and exhibits better generalization capabilities with unseen APIs. At the same time, TP-LLaMA has also demonstrated superior reasoning efficiency compared to the baselines, making it more suitable for complex tool-usage reasoning tasks.","sentences":["Tool-augmented large language models (LLMs) leverage tools, often in the form of APIs, to enhance their reasoning capabilities on complex tasks, thus taking on the role of intelligent agents interacting with the real world.","The recently introduced ToolLLaMA model by Qin et al.","[2024] utilizes the depth-first search-based decision tree (DFSDT) method for reasoning with $16000+$ real-world APIs, which effectively improves the planning and inferencing performance of tool-augmented LLMs compared to traditional chain reasoning approaches.","However, their approach only employs successful paths from decision trees (also called inference trees) for supervised fine-tuning (SFT) during training, which does not fully exploit the advantages of the tree of thought.","In this study, we propose an inference trajectory optimization framework based on the preference data extracted from decision trees to address this limitation.","We first introduce a novel method for constructing preference data from the tree of thought, capitalizing on the failed explorations previously overlooked in the trees.","Specifically, we generate an effective step-wise preference dataset, named ToolPreference, for tool use based on the ToolBench dataset.","In the subsequent training phase, we first fine-tune the LLM with tool-usage expert trajectories and then use these step-wise preference pairs for direct preference optimization (DPO) to update the policy of the LLM, resulting in our ToolPrefer-LLaMA (TP-LLaMA) model.","Our experiments demonstrate that by obtaining insights from errors in inference trees, TP-LLaMA significantly outperforms the baselines across almost all test scenarios by a large margin and exhibits better generalization capabilities with unseen APIs.","At the same time, TP-LLaMA has also demonstrated superior reasoning efficiency compared to the baselines, making it more suitable for complex tool-usage reasoning tasks."],"url":"http://arxiv.org/abs/2406.07115v1","category":"cs.CL"}
{"created":"2024-06-11 09:58:27","title":"Unlocking the Potential of the Metaverse for Innovative and Immersive Digital Care","abstract":"The Metaverse, a persistent, immersive virtual environment, has the immense potential to revolutionize healthcare by transforming patient care, medical education, and research. This paper explores the applications, benefits, and challenges associated with this transformative technology, highlighting its ability to improve patient engagement, communication, access to information, and health outcomes. The paper also examines how the analysis of Metaverse data using machine learning techniques can unlock insights to further enhance healthcare applications. The discussion summarizes key findings, analyzes the significance and practical implications of Metaverse integration, and identifies areas for future research. It underscores the role of major tech companies in developing Metaverse-based solutions and the importance of addressing emerging opportunities and challenges to unlock the transformative potential of this technology in healthcare. The paper concludes by emphasizing the need for collaboration between stakeholders to ensure the ethical and effective implementation of these technologies, ultimately leading to a more accessible, personalized, and efficient healthcare system.","sentences":["The Metaverse, a persistent, immersive virtual environment, has the immense potential to revolutionize healthcare by transforming patient care, medical education, and research.","This paper explores the applications, benefits, and challenges associated with this transformative technology, highlighting its ability to improve patient engagement, communication, access to information, and health outcomes.","The paper also examines how the analysis of Metaverse data using machine learning techniques can unlock insights to further enhance healthcare applications.","The discussion summarizes key findings, analyzes the significance and practical implications of Metaverse integration, and identifies areas for future research.","It underscores the role of major tech companies in developing Metaverse-based solutions and the importance of addressing emerging opportunities and challenges to unlock the transformative potential of this technology in healthcare.","The paper concludes by emphasizing the need for collaboration between stakeholders to ensure the ethical and effective implementation of these technologies, ultimately leading to a more accessible, personalized, and efficient healthcare system."],"url":"http://arxiv.org/abs/2406.07114v1","category":"cs.CY"}
{"created":"2024-06-11 09:57:04","title":"Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene Graph","abstract":"Locating objects referred to in natural language poses a significant challenge for autonomous agents. Existing CLIP-based open-vocabulary methods successfully perform 3D object retrieval with simple (bare) queries but cannot cope with ambiguous descriptions that demand an understanding of object relations. To tackle this problem, we propose a modular approach called BBQ (Beyond Bare Queries), which constructs 3D scene spatial graph representation with metric edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm. BBQ employs robust DINO-powered associations to form 3D objects, an advanced raycasting algorithm to project them to 2D, and a vision-language model to describe them as graph nodes. On Replica and ScanNet datasets, we show that the designed method accurately constructs 3D object-centric maps. We have demonstrated that their quality takes a leading place for open-vocabulary 3D semantic segmentation against other zero-shot methods. Also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class. On Sr3D and Nr3D benchmarks, our deductive approach demonstrates a significant improvement, enabling retrieving objects by complex queries compared to other state-of-the-art methods. Considering our design solutions, we achieved a processing speed approximately x3 times faster than the closest analog. This promising performance enables our approach for usage in applied intelligent robotics projects. We make the code publicly available at linukc.github.io/bbq/.","sentences":["Locating objects referred to in natural language poses a significant challenge for autonomous agents.","Existing CLIP-based open-vocabulary methods successfully perform 3D object retrieval with simple (bare) queries but cannot cope with ambiguous descriptions that demand an understanding of object relations.","To tackle this problem, we propose a modular approach called BBQ (Beyond Bare Queries), which constructs 3D scene spatial graph representation with metric edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm.","BBQ employs robust DINO-powered associations to form 3D objects, an advanced raycasting algorithm to project them to 2D, and a vision-language model to describe them as graph nodes.","On Replica and ScanNet datasets, we show that the designed method accurately constructs 3D object-centric maps.","We have demonstrated that their quality takes a leading place for open-vocabulary 3D semantic segmentation against other zero-shot methods.","Also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class.","On Sr3D and Nr3D benchmarks, our deductive approach demonstrates a significant improvement, enabling retrieving objects by complex queries compared to other state-of-the-art methods.","Considering our design solutions, we achieved a processing speed approximately x3 times faster than the closest analog.","This promising performance enables our approach for usage in applied intelligent robotics projects.","We make the code publicly available at linukc.github.io/bbq/."],"url":"http://arxiv.org/abs/2406.07113v1","category":"cs.CV"}
{"created":"2024-06-11 09:42:47","title":"MR-RawNet: Speaker verification system with multiple temporal resolutions for variable duration utterances using raw waveforms","abstract":"In speaker verification systems, the utilization of short utterances presents a persistent challenge, leading to performance degradation primarily due to insufficient phonetic information to characterize the speakers. To overcome this obstacle, we propose a novel structure, MR-RawNet, designed to enhance the robustness of speaker verification systems against variable duration utterances using raw waveforms. The MR-RawNet extracts time-frequency representations from raw waveforms via a multi-resolution feature extractor that optimally adjusts both temporal and spectral resolutions simultaneously. Furthermore, we apply a multi-resolution attention block that focuses on diverse and extensive temporal contexts, ensuring robustness against changes in utterance length. The experimental results, conducted on VoxCeleb1 dataset, demonstrate that the MR-RawNet exhibits superior performance in handling utterances of variable duration compared to other raw waveform-based systems.","sentences":["In speaker verification systems, the utilization of short utterances presents a persistent challenge, leading to performance degradation primarily due to insufficient phonetic information to characterize the speakers.","To overcome this obstacle, we propose a novel structure, MR-RawNet, designed to enhance the robustness of speaker verification systems against variable duration utterances using raw waveforms.","The MR-RawNet extracts time-frequency representations from raw waveforms via a multi-resolution feature extractor that optimally adjusts both temporal and spectral resolutions simultaneously.","Furthermore, we apply a multi-resolution attention block that focuses on diverse and extensive temporal contexts, ensuring robustness against changes in utterance length.","The experimental results, conducted on VoxCeleb1 dataset, demonstrate that the MR-RawNet exhibits superior performance in handling utterances of variable duration compared to other raw waveform-based systems."],"url":"http://arxiv.org/abs/2406.07103v1","category":"eess.AS"}
{"created":"2024-06-11 09:42:03","title":"D-GRIL: End-to-End Topological Learning with 2-parameter Persistence","abstract":"End-to-end topological learning using 1-parameter persistence is well-known. We show that the framework can be enhanced using 2-parameter persistence by adopting a recently introduced 2-parameter persistence based vectorization technique called GRIL. We establish a theoretical foundation of differentiating GRIL producing D-GRIL. We show that D-GRIL can be used to learn a bifiltration function on standard benchmark graph datasets. Further, we exhibit that this framework can be applied in the context of bio-activity prediction in drug discovery.","sentences":["End-to-end topological learning using 1-parameter persistence is well-known.","We show that the framework can be enhanced using 2-parameter persistence by adopting a recently introduced 2-parameter persistence based vectorization technique called GRIL.","We establish a theoretical foundation of differentiating GRIL producing D-GRIL.","We show that D-GRIL can be used to learn a bifiltration function on standard benchmark graph datasets.","Further, we exhibit that this framework can be applied in the context of bio-activity prediction in drug discovery."],"url":"http://arxiv.org/abs/2406.07100v1","category":"cs.LG"}
{"created":"2024-06-11 09:38:46","title":"Guiding Catalogue Enrichment with User Queries","abstract":"Techniques for knowledge graph (KGs) enrichment have been increasingly crucial for commercial applications that rely on evolving product catalogues. However, because of the huge search space of potential enrichment, predictions from KG completion (KGC) methods suffer from low precision, making them unreliable for real-world catalogues. Moreover, candidate facts for enrichment have varied relevance to users. While making correct predictions for incomplete triplets in KGs has been the main focus of KGC method, the relevance of when to apply such predictions has been neglected. Motivated by the product search use case, we address the angle of generating relevant completion for a catalogue using user search behaviour and the users property association with a product. In this paper, we present our intuition for identifying enrichable data points and use general-purpose KGs to show-case the performance benefits. In particular, we extract entity-predicate pairs from user queries, which are more likely to be correct and relevant, and use these pairs to guide the prediction of KGC methods. We assess our method on two popular encyclopedia KGs, DBPedia and YAGO 4. Our results from both automatic and human evaluations show that query guidance can significantly improve the correctness and relevance of prediction.","sentences":["Techniques for knowledge graph (KGs) enrichment have been increasingly crucial for commercial applications that rely on evolving product catalogues.","However, because of the huge search space of potential enrichment, predictions from KG completion (KGC) methods suffer from low precision, making them unreliable for real-world catalogues.","Moreover, candidate facts for enrichment have varied relevance to users.","While making correct predictions for incomplete triplets in KGs has been the main focus of KGC method, the relevance of when to apply such predictions has been neglected.","Motivated by the product search use case, we address the angle of generating relevant completion for a catalogue using user search behaviour and the users property association with a product.","In this paper, we present our intuition for identifying enrichable data points and use general-purpose KGs to show-case the performance benefits.","In particular, we extract entity-predicate pairs from user queries, which are more likely to be correct and relevant, and use these pairs to guide the prediction of KGC methods.","We assess our method on two popular encyclopedia KGs, DBPedia and YAGO 4.","Our results from both automatic and human evaluations show that query guidance can significantly improve the correctness and relevance of prediction."],"url":"http://arxiv.org/abs/2406.07098v1","category":"cs.IR"}
{"created":"2024-06-11 09:37:52","title":"Fast Context-Biasing for CTC and Transducer ASR models with CTC-based Word Spotter","abstract":"Accurate recognition of rare and new words remains a pressing problem for contextualized Automatic Speech Recognition (ASR) systems. Most context-biasing methods involve modification of the ASR model or the beam-search decoding algorithm, complicating model reuse and slowing down inference. This work presents a new approach to fast context-biasing with CTC-based Word Spotter (CTC-WS) for CTC and Transducer (RNN-T) ASR models. The proposed method matches CTC log-probabilities against a compact context graph to detect potential context-biasing candidates. The valid candidates then replace their greedy recognition counterparts in corresponding frame intervals. A Hybrid Transducer-CTC model enables the CTC-WS application for the Transducer model. The results demonstrate a significant acceleration of the context-biasing recognition with a simultaneous improvement in F-score and WER compared to baseline methods. The proposed method is publicly available in the NVIDIA NeMo toolkit.","sentences":["Accurate recognition of rare and new words remains a pressing problem for contextualized Automatic Speech Recognition (ASR) systems.","Most context-biasing methods involve modification of the ASR model or the beam-search decoding algorithm, complicating model reuse and slowing down inference.","This work presents a new approach to fast context-biasing with CTC-based Word Spotter (CTC-WS) for CTC and Transducer (RNN-T) ASR models.","The proposed method matches CTC log-probabilities against a compact context graph to detect potential context-biasing candidates.","The valid candidates then replace their greedy recognition counterparts in corresponding frame intervals.","A Hybrid Transducer-CTC model enables the CTC-WS application for the Transducer model.","The results demonstrate a significant acceleration of the context-biasing recognition with a simultaneous improvement in F-score and WER compared to baseline methods.","The proposed method is publicly available in the NVIDIA NeMo toolkit."],"url":"http://arxiv.org/abs/2406.07096v1","category":"eess.AS"}
{"created":"2024-06-11 09:37:51","title":"Data Complexity in Expressive Description Logics With Path Expressions","abstract":"We investigate the data complexity of the satisfiability problem for the very expressive description logic ZOIQ (a.k.a. ALCHb Self reg OIQ) over quasi-forests and establish its NP-completeness. This completes the data complexity landscape for decidable fragments of ZOIQ, and reproves known results on decidable fragments of OWL2 (SR family). Using the same technique, we establish coNEXPTIME-completeness (w.r.t. the combined complexity) of the entailment problem of rooted queries in ZIQ.","sentences":["We investigate the data complexity of the satisfiability problem for the very expressive description logic ZOIQ (a.k.a. ALCHb Self reg OIQ) over quasi-forests and establish its NP-completeness.","This completes the data complexity landscape for decidable fragments of ZOIQ, and reproves known results on decidable fragments of OWL2 (SR family).","Using the same technique, we establish coNEXPTIME-completeness (w.r.t.","the combined complexity) of the entailment problem of rooted queries in ZIQ."],"url":"http://arxiv.org/abs/2406.07095v1","category":"cs.LO"}
{"created":"2024-06-11 09:30:02","title":"RS-Agent: Automating Remote Sensing Tasks through Intelligent Agents","abstract":"An increasing number of models have achieved great performance in remote sensing tasks with the recent development of Large Language Models (LLMs) and Visual Language Models (VLMs). However, these models are constrained to basic vision and language instruction-tuning tasks, facing challenges in complex remote sensing applications. Additionally, these models lack specialized expertise in professional domains. To address these limitations, we propose a LLM-driven remote sensing intelligent agent named RS-Agent. Firstly, RS-Agent is powered by a large language model (LLM) that acts as its \"Central Controller,\" enabling it to understand and respond to various problems intelligently. Secondly, our RS-Agent integrates many high-performance remote sensing image processing tools, facilitating multi-tool and multi-turn conversations. Thirdly, our RS-Agent can answer professional questions by leveraging robust knowledge documents. We conducted experiments using several datasets, e.g., RSSDIVCS, RSVQA, and DOTAv1. The experimental results demonstrate that our RS-Agent delivers outstanding performance in many tasks, i.e., scene classification, visual question answering, and object counting tasks.","sentences":["An increasing number of models have achieved great performance in remote sensing tasks with the recent development of Large Language Models (LLMs) and Visual Language Models (VLMs).","However, these models are constrained to basic vision and language instruction-tuning tasks, facing challenges in complex remote sensing applications.","Additionally, these models lack specialized expertise in professional domains.","To address these limitations, we propose a LLM-driven remote sensing intelligent agent named RS-Agent.","Firstly, RS-Agent is powered by a large language model (LLM) that acts as its \"Central Controller,\" enabling it to understand and respond to various problems intelligently.","Secondly, our RS-Agent integrates many high-performance remote sensing image processing tools, facilitating multi-tool and multi-turn conversations.","Thirdly, our RS-Agent can answer professional questions by leveraging robust knowledge documents.","We conducted experiments using several datasets, e.g., RSSDIVCS, RSVQA, and DOTAv1.","The experimental results demonstrate that our RS-Agent delivers outstanding performance in many tasks, i.e., scene classification, visual question answering, and object counting tasks."],"url":"http://arxiv.org/abs/2406.07089v1","category":"cs.CV"}
{"created":"2024-06-11 09:16:43","title":"Efficient Mixture Learning in Black-Box Variational Inference","abstract":"Mixture variational distributions in black box variational inference (BBVI) have demonstrated impressive results in challenging density estimation tasks. However, currently scaling the number of mixture components can lead to a linear increase in the number of learnable parameters and a quadratic increase in inference time due to the evaluation of the evidence lower bound (ELBO). Our two key contributions address these limitations. First, we introduce the novel Multiple Importance Sampling Variational Autoencoder (MISVAE), which amortizes the mapping from input to mixture-parameter space using one-hot encodings. Fortunately, with MISVAE, each additional mixture component incurs a negligible increase in network parameters. Second, we construct two new estimators of the ELBO for mixtures in BBVI, enabling a tremendous reduction in inference time with marginal or even improved impact on performance. Collectively, our contributions enable scalability to hundreds of mixture components and provide superior estimation performance in shorter time, with fewer network parameters compared to previous Mixture VAEs. Experimenting with MISVAE, we achieve astonishing, SOTA results on MNIST. Furthermore, we empirically validate our estimators in other BBVI settings, including Bayesian phylogenetic inference, where we improve inference times for the SOTA mixture model on eight data sets.","sentences":["Mixture variational distributions in black box variational inference (BBVI) have demonstrated impressive results in challenging density estimation tasks.","However, currently scaling the number of mixture components can lead to a linear increase in the number of learnable parameters and a quadratic increase in inference time due to the evaluation of the evidence lower bound (ELBO).","Our two key contributions address these limitations.","First, we introduce the novel Multiple Importance Sampling Variational Autoencoder (MISVAE), which amortizes the mapping from input to mixture-parameter space using one-hot encodings.","Fortunately, with MISVAE, each additional mixture component incurs a negligible increase in network parameters.","Second, we construct two new estimators of the ELBO for mixtures in BBVI, enabling a tremendous reduction in inference time with marginal or even improved impact on performance.","Collectively, our contributions enable scalability to hundreds of mixture components and provide superior estimation performance in shorter time, with fewer network parameters compared to previous Mixture VAEs.","Experimenting with MISVAE, we achieve astonishing, SOTA results on MNIST.","Furthermore, we empirically validate our estimators in other BBVI settings, including Bayesian phylogenetic inference, where we improve inference times for the SOTA mixture model on eight data sets."],"url":"http://arxiv.org/abs/2406.07083v1","category":"cs.LG"}
{"created":"2024-06-11 09:11:17","title":"Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning","abstract":"Large language models (LLMs) exhibit outstanding performance in machine translation via in-context learning. In contrast to sentence-level translation, document-level translation (DOCMT) by LLMs based on in-context learning faces two major challenges: firstly, document translations generated by LLMs are often incoherent; secondly, the length of demonstration for in-context learning is usually limited. To address these issues, we propose a Context-Aware Prompting method (CAP), which enables LLMs to generate more accurate, cohesive, and coherent translations via in-context learning. CAP takes into account multi-level attention, selects the most relevant sentences to the current one as context, and then generates a summary from these collected sentences. Subsequently, sentences most similar to the summary are retrieved from the datastore as demonstrations, which effectively guide LLMs in generating cohesive and coherent translations. We conduct extensive experiments across various DOCMT tasks, and the results demonstrate the effectiveness of our approach, particularly in zero pronoun translation (ZPT) and literary translation tasks.","sentences":["Large language models (LLMs) exhibit outstanding performance in machine translation via in-context learning.","In contrast to sentence-level translation, document-level translation (DOCMT) by LLMs based on in-context learning faces two major challenges: firstly, document translations generated by LLMs are often incoherent; secondly, the length of demonstration for in-context learning is usually limited.","To address these issues, we propose a Context-Aware Prompting method (CAP), which enables LLMs to generate more accurate, cohesive, and coherent translations via in-context learning.","CAP takes into account multi-level attention, selects the most relevant sentences to the current one as context, and then generates a summary from these collected sentences.","Subsequently, sentences most similar to the summary are retrieved from the datastore as demonstrations, which effectively guide LLMs in generating cohesive and coherent translations.","We conduct extensive experiments across various DOCMT tasks, and the results demonstrate the effectiveness of our approach, particularly in zero pronoun translation (ZPT) and literary translation tasks."],"url":"http://arxiv.org/abs/2406.07081v1","category":"cs.CL"}
{"created":"2024-06-11 09:06:41","title":"Unified Modeling Enhanced Multimodal Learning for Precision Neuro-Oncology","abstract":"Multimodal learning, integrating histology images and genomics, promises to enhance precision oncology with comprehensive views at microscopic and molecular levels. However, existing methods may not sufficiently model the shared or complementary information for more effective integration. In this study, we introduce a Unified Modeling Enhanced Multimodal Learning (UMEML) framework that employs a hierarchical attention structure to effectively leverage shared and complementary features of both modalities of histology and genomics. Specifically, to mitigate unimodal bias from modality imbalance, we utilize a query-based cross-attention mechanism for prototype clustering in the pathology encoder. Our prototype assignment and modularity strategy are designed to align shared features and minimizes modality gaps. An additional registration mechanism with learnable tokens is introduced to enhance cross-modal feature integration and robustness in multimodal unified modeling. Our experiments demonstrate that our method surpasses previous state-of-the-art approaches in glioma diagnosis and prognosis tasks, underscoring its superiority in precision neuro-Oncology.","sentences":["Multimodal learning, integrating histology images and genomics, promises to enhance precision oncology with comprehensive views at microscopic and molecular levels.","However, existing methods may not sufficiently model the shared or complementary information for more effective integration.","In this study, we introduce a Unified Modeling Enhanced Multimodal Learning (UMEML) framework that employs a hierarchical attention structure to effectively leverage shared and complementary features of both modalities of histology and genomics.","Specifically, to mitigate unimodal bias from modality imbalance, we utilize a query-based cross-attention mechanism for prototype clustering in the pathology encoder.","Our prototype assignment and modularity strategy are designed to align shared features and minimizes modality gaps.","An additional registration mechanism with learnable tokens is introduced to enhance cross-modal feature integration and robustness in multimodal unified modeling.","Our experiments demonstrate that our method surpasses previous state-of-the-art approaches in glioma diagnosis and prognosis tasks, underscoring its superiority in precision neuro-Oncology."],"url":"http://arxiv.org/abs/2406.07078v1","category":"cs.CV"}
{"created":"2024-06-11 08:53:15","title":"TIM: Temporal Interaction Model in Notification System","abstract":"Modern mobile applications heavily rely on the notification system to acquire daily active users and enhance user engagement. Being able to proactively reach users, the system has to decide when to send notifications to users. Although many researchers have studied optimizing the timing of sending notifications, they only utilized users' contextual features, without modeling users' behavior patterns. Additionally, these efforts only focus on individual notifications, and there is a lack of studies on optimizing the holistic timing of multiple notifications within a period. To bridge these gaps, we propose the Temporal Interaction Model (TIM), which models users' behavior patterns by estimating CTR in every time slot over a day in our short video application Kuaishou. TIM leverages long-term user historical interaction sequence features such as notification receipts, clicks, watch time and effective views, and employs a temporal attention unit (TAU) to extract user behavior patterns. Moreover, we provide an elegant strategy of holistic notifications send time control to improve user engagement while minimizing disruption. We evaluate the effectiveness of TIM through offline experiments and online A/B tests. The results indicate that TIM is a reliable tool for forecasting user behavior, leading to a remarkable enhancement in user engagement without causing undue disturbance.","sentences":["Modern mobile applications heavily rely on the notification system to acquire daily active users and enhance user engagement.","Being able to proactively reach users, the system has to decide when to send notifications to users.","Although many researchers have studied optimizing the timing of sending notifications, they only utilized users' contextual features, without modeling users' behavior patterns.","Additionally, these efforts only focus on individual notifications, and there is a lack of studies on optimizing the holistic timing of multiple notifications within a period.","To bridge these gaps, we propose the Temporal Interaction Model (TIM), which models users' behavior patterns by estimating CTR in every time slot over a day in our short video application Kuaishou.","TIM leverages long-term user historical interaction sequence features such as notification receipts, clicks, watch time and effective views, and employs a temporal attention unit (TAU) to extract user behavior patterns.","Moreover, we provide an elegant strategy of holistic notifications send time control to improve user engagement while minimizing disruption.","We evaluate the effectiveness of TIM through offline experiments and online A/B tests.","The results indicate that TIM is a reliable tool for forecasting user behavior, leading to a remarkable enhancement in user engagement without causing undue disturbance."],"url":"http://arxiv.org/abs/2406.07067v1","category":"cs.IR"}
{"created":"2024-06-11 08:45:41","title":"Reconstructing the Tropical Pacific Upper Ocean using Online Data Assimilation with a Deep Learning model","abstract":"A deep learning (DL) model, based on a transformer architecture, is trained on a climate-model dataset and compared with a standard linear inverse model (LIM) in the tropical Pacific. We show that the DL model produces more accurate forecasts compared to the LIM when tested on a reanalysis dataset. We then assess the ability of an ensemble Kalman filter to reconstruct the monthly-averaged upper ocean from a noisy set of 24 sea-surface temperature observations designed to mimic existing coral proxy measurements, and compare results for the DL model and LIM. Due to signal damping in the DL model, we implement a novel inflation technique by adding noise from hindcast experiments. Results show that assimilating observations with the DL model yields better reconstructions than the LIM for observation averaging times ranging from one month to one year. The improved reconstruction is due to the enhanced predictive capabilities of the DL model, which map the memory of past observations to future assimilation times.","sentences":["A deep learning (DL) model, based on a transformer architecture, is trained on a climate-model dataset and compared with a standard linear inverse model (LIM) in the tropical Pacific.","We show that the DL model produces more accurate forecasts compared to the LIM when tested on a reanalysis dataset.","We then assess the ability of an ensemble Kalman filter to reconstruct the monthly-averaged upper ocean from a noisy set of 24 sea-surface temperature observations designed to mimic existing coral proxy measurements, and compare results for the DL model and LIM.","Due to signal damping in the DL model, we implement a novel inflation technique by adding noise from hindcast experiments.","Results show that assimilating observations with the DL model yields better reconstructions than the LIM for observation averaging times ranging from one month to one year.","The improved reconstruction is due to the enhanced predictive capabilities of the DL model, which map the memory of past observations to future assimilation times."],"url":"http://arxiv.org/abs/2406.07063v1","category":"physics.ao-ph"}
{"created":"2024-06-11 08:41:21","title":"Reading Miscue Detection in Primary School through Automatic Speech Recognition","abstract":"Automatic reading diagnosis systems can benefit both teachers for more efficient scoring of reading exercises and students for accessing reading exercises with feedback more easily. However, there are limited studies on Automatic Speech Recognition (ASR) for child speech in languages other than English, and limited research on ASR-based reading diagnosis systems. This study investigates how efficiently state-of-the-art (SOTA) pretrained ASR models recognize Dutch native children speech and manage to detect reading miscues. We found that Hubert Large finetuned on Dutch speech achieves SOTA phoneme-level child speech recognition (PER at 23.1\\%), while Whisper (Faster Whisper Large-v2) achieves SOTA word-level performance (WER at 9.8\\%). Our findings suggest that Wav2Vec2 Large and Whisper are the two best ASR models for reading miscue detection. Specifically, Wav2Vec2 Large shows the highest recall at 0.83, whereas Whisper exhibits the highest precision at 0.52 and an F1 score of 0.52.","sentences":["Automatic reading diagnosis systems can benefit both teachers for more efficient scoring of reading exercises and students for accessing reading exercises with feedback more easily.","However, there are limited studies on Automatic Speech Recognition (ASR) for child speech in languages other than English, and limited research on ASR-based reading diagnosis systems.","This study investigates how efficiently state-of-the-art (SOTA) pretrained ASR models recognize Dutch native children speech and manage to detect reading miscues.","We found that Hubert Large finetuned on Dutch speech achieves SOTA phoneme-level child speech recognition (PER at 23.1\\%), while Whisper (Faster Whisper Large-v2) achieves SOTA word-level performance (WER at 9.8\\%).","Our findings suggest that Wav2Vec2 Large and Whisper are the two best ASR models for reading miscue detection.","Specifically, Wav2Vec2 Large shows the highest recall at 0.83, whereas Whisper exhibits the highest precision at 0.52 and an F1 score of 0.52."],"url":"http://arxiv.org/abs/2406.07060v1","category":"cs.CL"}
{"created":"2024-06-11 08:38:13","title":"Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study","abstract":"Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges. Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements. In this work, we establish MultiTrust, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects: truthfulness, safety, robustness, fairness, and privacy. Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets. Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability. For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs. Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field. Code and resources are publicly available at: https://multi-trust.github.io/.","sentences":["Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges.","Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements.","In this work, we establish MultiTrust, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects: truthfulness, safety, robustness, fairness, and privacy.","Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets.","Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability.","For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs.","Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field.","Code and resources are publicly available at: https://multi-trust.github.io/."],"url":"http://arxiv.org/abs/2406.07057v1","category":"cs.CL"}
{"created":"2024-06-11 08:35:37","title":"CoEvol: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation","abstract":"In recent years, instruction fine-tuning (IFT) on large language models (LLMs) has garnered considerable attention to enhance model performance on unseen tasks. Attempts have been made on automatic construction and effective selection for IFT data. However, we posit that previous methods have not fully harnessed the potential of LLMs for enhancing data quality. The responses within IFT data could be further enhanced by leveraging the capabilities of LLMs themselves. In this paper, we propose CoEvol, an LLM-based multi-agent cooperation framework for the improvement of responses to instructions. To effectively refine the responses, we develop an iterative framework following a debate-advise-edit-judge paradigm. A two-stage multi-agent debate strategy is further devised to ensure the diversity and reliability of editing suggestions within the framework. Empirically, models equipped with CoEvol outperform competitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its effectiveness in enhancing instruction-following capabilities for LLMs.","sentences":["In recent years, instruction fine-tuning (IFT) on large language models (LLMs) has garnered considerable attention to enhance model performance on unseen tasks.","Attempts have been made on automatic construction and effective selection for IFT data.","However, we posit that previous methods have not fully harnessed the potential of LLMs for enhancing data quality.","The responses within IFT data could be further enhanced by leveraging the capabilities of LLMs themselves.","In this paper, we propose CoEvol, an LLM-based multi-agent cooperation framework for the improvement of responses to instructions.","To effectively refine the responses, we develop an iterative framework following a debate-advise-edit-judge paradigm.","A two-stage multi-agent debate strategy is further devised to ensure the diversity and reliability of editing suggestions within the framework.","Empirically, models equipped with CoEvol outperform competitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its effectiveness in enhancing instruction-following capabilities for LLMs."],"url":"http://arxiv.org/abs/2406.07054v1","category":"cs.CL"}
{"created":"2024-06-11 08:29:33","title":"Bayesian inference of nuclear incompressibility from proton elliptic flow in central Au+Au collisions at 400 MeV/nucleon","abstract":"The incompressibility $K$ of symmetric nuclear matter (SNM) is inferred in a Bayesian analysis of proton elliptic flow in mid-central Au + Au collisions at $E = 400$ MeV/nucleon using a Gaussian process (GP) emulator of the isospin-dependent quantum molecular dynamics (IQMD) model for heavy-ion collisions, with or without considering the momentum dependence of single-nucleon potentials. Consistent but with smaller quantified uncertainties than previous results from forward modeling of the collective flow in heavy-ion collisions using IQMD, considering the momentum dependence of nucleon potentials, $K=191.3^{+3.7}_{-6.3}$ MeV at 68\\% confidence level, indicating a very soft SNM equation of state, is inferred from the combined data of the rapidity and transverse momentum dependence of the proton elliptic flow in the Au+Au collisions considered. Ignoring the momentum dependence of single-nucleon potentials, the extracted value for $K$ is $234.7^{+14.6}_{-11.4}$ MeV, in agreement with its fiducial value derived from giant resonance studies.","sentences":["The incompressibility $K$ of symmetric nuclear matter (SNM) is inferred in a Bayesian analysis of proton elliptic flow in mid-central Au + Au collisions at $E = 400$ MeV/nucleon using a Gaussian process (GP) emulator of the isospin-dependent quantum molecular dynamics (IQMD) model for heavy-ion collisions, with or without considering the momentum dependence of single-nucleon potentials.","Consistent but with smaller quantified uncertainties than previous results from forward modeling of the collective flow in heavy-ion collisions using IQMD, considering the momentum dependence of nucleon potentials, $K=191.3^{+3.7}_{-6.3}$ MeV at 68\\% confidence level, indicating a very soft SNM equation of state, is inferred from the combined data of the rapidity and transverse momentum dependence of the proton elliptic flow in the Au+Au collisions considered.","Ignoring the momentum dependence of single-nucleon potentials, the extracted value for $K$ is $234.7^{+14.6}_{-11.4}$ MeV, in agreement with its fiducial value derived from giant resonance studies."],"url":"http://arxiv.org/abs/2406.07051v1","category":"nucl-th"}
{"created":"2024-06-11 08:25:11","title":"GridPE: Unifying Positional Encoding in Transformers with a Grid Cell-Inspired Framework","abstract":"Understanding spatial location and relationships is a fundamental capability for modern artificial intelligence systems. Insights from human spatial cognition provide valuable guidance in this domain. Recent neuroscientific discoveries have highlighted the role of grid cells as a fundamental neural component for spatial representation, including distance computation, path integration, and scale discernment. In this paper, we introduce a novel positional encoding scheme inspired by Fourier analysis and the latest findings in computational neuroscience regarding grid cells. Assuming that grid cells encode spatial position through a summation of Fourier basis functions, we demonstrate the translational invariance of the grid representation during inner product calculations. Additionally, we derive an optimal grid scale ratio for multi-dimensional Euclidean spaces based on principles of biological efficiency. Utilizing these computational principles, we have developed a **Grid**-cell inspired **Positional Encoding** technique, termed **GridPE**, for encoding locations within high-dimensional spaces. We integrated GridPE into the Pyramid Vision Transformer architecture. Our theoretical analysis shows that GridPE provides a unifying framework for positional encoding in arbitrary high-dimensional spaces. Experimental results demonstrate that GridPE significantly enhances the performance of transformers, underscoring the importance of incorporating neuroscientific insights into the design of artificial intelligence systems.","sentences":["Understanding spatial location and relationships is a fundamental capability for modern artificial intelligence systems.","Insights from human spatial cognition provide valuable guidance in this domain.","Recent neuroscientific discoveries have highlighted the role of grid cells as a fundamental neural component for spatial representation, including distance computation, path integration, and scale discernment.","In this paper, we introduce a novel positional encoding scheme inspired by Fourier analysis and the latest findings in computational neuroscience regarding grid cells.","Assuming that grid cells encode spatial position through a summation of Fourier basis functions, we demonstrate the translational invariance of the grid representation during inner product calculations.","Additionally, we derive an optimal grid scale ratio for multi-dimensional Euclidean spaces based on principles of biological efficiency.","Utilizing these computational principles, we have developed a **Grid**-cell inspired **Positional Encoding** technique, termed **GridPE**, for encoding locations within high-dimensional spaces.","We integrated GridPE into the Pyramid Vision Transformer architecture.","Our theoretical analysis shows that GridPE provides a unifying framework for positional encoding in arbitrary high-dimensional spaces.","Experimental results demonstrate that GridPE significantly enhances the performance of transformers, underscoring the importance of incorporating neuroscientific insights into the design of artificial intelligence systems."],"url":"http://arxiv.org/abs/2406.07049v1","category":"cs.NE"}
{"created":"2024-06-11 07:59:17","title":"Integrating Domain Knowledge for handling Limited Data in Offline RL","abstract":"With the ability to learn from static datasets, Offline Reinforcement Learning (RL) emerges as a compelling avenue for real-world applications. However, state-of-the-art offline RL algorithms perform sub-optimally when confronted with limited data confined to specific regions within the state space. The performance degradation is attributed to the inability of offline RL algorithms to learn appropriate actions for rare or unseen observations. This paper proposes a novel domain knowledge-based regularization technique and adaptively refines the initial domain knowledge to considerably boost performance in limited data with partially omitted states. The key insight is that the regularization term mitigates erroneous actions for sparse samples and unobserved states covered by domain knowledge. Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase of at least 27% compared to existing offline RL algorithms operating on limited data.","sentences":["With the ability to learn from static datasets, Offline Reinforcement Learning (RL) emerges as a compelling avenue for real-world applications.","However, state-of-the-art offline RL algorithms perform sub-optimally when confronted with limited data confined to specific regions within the state space.","The performance degradation is attributed to the inability of offline RL algorithms to learn appropriate actions for rare or unseen observations.","This paper proposes a novel domain knowledge-based regularization technique and adaptively refines the initial domain knowledge to considerably boost performance in limited data with partially omitted states.","The key insight is that the regularization term mitigates erroneous actions for sparse samples and unobserved states covered by domain knowledge.","Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase of at least 27% compared to existing offline RL algorithms operating on limited data."],"url":"http://arxiv.org/abs/2406.07041v1","category":"cs.LG"}
{"created":"2024-06-11 07:49:04","title":"Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model","abstract":"Large language models (LLMs) have showcased impressive multilingual machine translation ability. However, unlike encoder-decoder style models, decoder-only LLMs lack an explicit alignment between source and target contexts. Analyzing contribution scores during generation processes revealed that LLMs can be biased towards previously generated tokens over corresponding source tokens, leading to unfaithful translations. To address this issue, we propose to encourage LLMs to pay more attention to the source context from both source and target perspectives in zeroshot prompting: 1) adjust source context attention weights; 2) suppress irrelevant target prefix influence; Additionally, we propose 3) avoiding over-reliance on the target prefix in instruction tuning. Experimental results from both human-collected unfaithfulness test sets focusing on LLM-generated unfaithful translations and general test sets, verify our methods' effectiveness across multiple language pairs. Further human evaluation shows our method's efficacy in reducing hallucinatory translations and facilitating faithful translation generation.","sentences":["Large language models (LLMs) have showcased impressive multilingual machine translation ability.","However, unlike encoder-decoder style models, decoder-only LLMs lack an explicit alignment between source and target contexts.","Analyzing contribution scores during generation processes revealed that LLMs can be biased towards previously generated tokens over corresponding source tokens, leading to unfaithful translations.","To address this issue, we propose to encourage LLMs to pay more attention to the source context from both source and target perspectives in zeroshot prompting: 1) adjust source context attention weights; 2) suppress irrelevant target prefix influence; Additionally, we propose 3) avoiding over-reliance on the target prefix in instruction tuning.","Experimental results from both human-collected unfaithfulness test sets focusing on LLM-generated unfaithful translations and general test sets, verify our methods' effectiveness across multiple language pairs.","Further human evaluation shows our method's efficacy in reducing hallucinatory translations and facilitating faithful translation generation."],"url":"http://arxiv.org/abs/2406.07036v1","category":"cs.CL"}
{"created":"2024-06-11 07:48:20","title":"Improving Multi-hop Logical Reasoning in Knowledge Graphs with Context-Aware Query Representation Learning","abstract":"Multi-hop logical reasoning on knowledge graphs is a pivotal task in natural language processing, with numerous approaches aiming to answer First-Order Logic (FOL) queries. Recent geometry (e.g., box, cone) and probability (e.g., beta distribution)-based methodologies have effectively addressed complex FOL queries. However, a common challenge across these methods lies in determining accurate geometric bounds or probability parameters for these queries. The challenge arises because existing methods rely on linear sequential operations within their computation graphs, overlooking the logical structure of the query and the relation-induced information that can be gleaned from the relations of the query, which we call the context of the query. To address the problem, we propose a model-agnostic methodology that enhances the effectiveness of existing multi-hop logical reasoning approaches by fully integrating the context of the FOL query graph. Our approach distinctively discerns (1) the structural context inherent to the query structure and (2) the relation-induced context unique to each node in the query graph as delineated in the corresponding knowledge graph. This dual-context paradigm helps nodes within a query graph attain refined internal representations throughout the multi-hop reasoning steps. Through experiments on two datasets, our method consistently enhances the three multi-hop reasoning foundation models, achieving performance improvements of up to 19.5%. Our code is available at https://github.com/kjh9503/caqr.","sentences":["Multi-hop logical reasoning on knowledge graphs is a pivotal task in natural language processing, with numerous approaches aiming to answer First-Order Logic (FOL) queries.","Recent geometry (e.g., box, cone) and probability (e.g., beta distribution)-based methodologies have effectively addressed complex FOL queries.","However, a common challenge across these methods lies in determining accurate geometric bounds or probability parameters for these queries.","The challenge arises because existing methods rely on linear sequential operations within their computation graphs, overlooking the logical structure of the query and the relation-induced information that can be gleaned from the relations of the query, which we call the context of the query.","To address the problem, we propose a model-agnostic methodology that enhances the effectiveness of existing multi-hop logical reasoning approaches by fully integrating the context of the FOL query graph.","Our approach distinctively discerns (1) the structural context inherent to the query structure and (2) the relation-induced context unique to each node in the query graph as delineated in the corresponding knowledge graph.","This dual-context paradigm helps nodes within a query graph attain refined internal representations throughout the multi-hop reasoning steps.","Through experiments on two datasets, our method consistently enhances the three multi-hop reasoning foundation models, achieving performance improvements of up to 19.5%.","Our code is available at https://github.com/kjh9503/caqr."],"url":"http://arxiv.org/abs/2406.07034v1","category":"cs.AI"}
{"created":"2024-06-11 07:32:25","title":"Heterogeneous Learning Rate Scheduling for Neural Architecture Search on Long-Tailed Datasets","abstract":"In this paper, we attempt to address the challenge of applying Neural Architecture Search (NAS) algorithms, specifically the Differentiable Architecture Search (DARTS), to long-tailed datasets where class distribution is highly imbalanced. We observe that traditional re-sampling and re-weighting techniques, which are effective in standard classification tasks, lead to performance degradation when combined with DARTS. To mitigate this, we propose a novel adaptive learning rate scheduling strategy tailored for the architecture parameters of DARTS when integrated with the Bilateral Branch Network (BBN) for handling imbalanced datasets. Our approach dynamically adjusts the learning rate of the architecture parameters based on the training epoch, preventing the disruption of well-trained representations in the later stages of training. Additionally, we explore the impact of branch mixing factors on the algorithm's performance. Through extensive experiments on the CIFAR-10 dataset with an artificially induced long-tailed distribution, we demonstrate that our method achieves comparable accuracy to using DARTS alone. And the experiment results suggest that re-sampling methods inherently harm the performance of the DARTS algorithm. Our findings highlight the importance of careful data augment when applying DNAS to imbalanced learning scenarios.","sentences":["In this paper, we attempt to address the challenge of applying Neural Architecture Search (NAS) algorithms, specifically the Differentiable Architecture Search (DARTS), to long-tailed datasets where class distribution is highly imbalanced.","We observe that traditional re-sampling and re-weighting techniques, which are effective in standard classification tasks, lead to performance degradation when combined with DARTS.","To mitigate this, we propose a novel adaptive learning rate scheduling strategy tailored for the architecture parameters of DARTS when integrated with the Bilateral Branch Network (BBN) for handling imbalanced datasets.","Our approach dynamically adjusts the learning rate of the architecture parameters based on the training epoch, preventing the disruption of well-trained representations in the later stages of training.","Additionally, we explore the impact of branch mixing factors on the algorithm's performance.","Through extensive experiments on the CIFAR-10 dataset with an artificially induced long-tailed distribution, we demonstrate that our method achieves comparable accuracy to using DARTS alone.","And the experiment results suggest that re-sampling methods inherently harm the performance of the DARTS algorithm.","Our findings highlight the importance of careful data augment when applying DNAS to imbalanced learning scenarios."],"url":"http://arxiv.org/abs/2406.07028v1","category":"cs.LG"}
{"created":"2024-06-11 07:29:13","title":"Entropy-Reinforced Planning with Large Language Models for Drug Discovery","abstract":"The objective of drug discovery is to identify chemical compounds that possess specific pharmaceutical properties toward a binding target. Existing large language models (LLMS) can achieve high token matching scores in terms of likelihood for molecule generation. However, relying solely on LLM decoding often results in the generation of molecules that are either invalid due to a single misused token, or suboptimal due to unbalanced exploration and exploitation as a consequence of the LLMs prior experience. Here we propose ERP, Entropy-Reinforced Planning for Transformer Decoding, which employs an entropy-reinforced planning algorithm to enhance the Transformer decoding process and strike a balance between exploitation and exploration. ERP aims to achieve improvements in multiple properties compared to direct sampling from the Transformer. We evaluated ERP on the SARS-CoV-2 virus (3CLPro) and human cancer cell target protein (RTCB) benchmarks and demonstrated that, in both benchmarks, ERP consistently outperforms the current state-of-the-art algorithm by 1-5 percent, and baselines by 5-10 percent, respectively. Moreover, such improvement is robust across Transformer models trained with different objectives. Finally, to further illustrate the capabilities of ERP, we tested our algorithm on three code generation benchmarks and outperformed the current state-of-the-art approach as well. Our code is publicly available at: https://github.com/xuefeng-cs/ERP.","sentences":["The objective of drug discovery is to identify chemical compounds that possess specific pharmaceutical properties toward a binding target.","Existing large language models (LLMS) can achieve high token matching scores in terms of likelihood for molecule generation.","However, relying solely on LLM decoding often results in the generation of molecules that are either invalid due to a single misused token, or suboptimal due to unbalanced exploration and exploitation as a consequence of the LLMs prior experience.","Here we propose ERP, Entropy-Reinforced Planning for Transformer Decoding, which employs an entropy-reinforced planning algorithm to enhance the Transformer decoding process and strike a balance between exploitation and exploration.","ERP aims to achieve improvements in multiple properties compared to direct sampling from the Transformer.","We evaluated ERP on the SARS-CoV-2 virus (3CLPro) and human cancer cell target protein (RTCB) benchmarks and demonstrated that, in both benchmarks, ERP consistently outperforms the current state-of-the-art algorithm by 1-5 percent, and baselines by 5-10 percent, respectively.","Moreover, such improvement is robust across Transformer models trained with different objectives.","Finally, to further illustrate the capabilities of ERP, we tested our algorithm on three code generation benchmarks and outperformed the current state-of-the-art approach as well.","Our code is publicly available at: https://github.com/xuefeng-cs/ERP."],"url":"http://arxiv.org/abs/2406.07025v1","category":"cs.LG"}
{"created":"2024-06-11 07:16:34","title":"Delving into ChatGPT usage in academic writing through excess vocabulary","abstract":"Recent large language models (LLMs) can generate and revise text with human-level performance, and have been widely commercialized in systems like ChatGPT. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists have been using them to assist their scholarly writing. How wide-spread is LLM usage in the academic literature currently? To answer this question, we use an unbiased, large-scale approach, free from any assumptions on academic LLM usage. We study vocabulary changes in 14 million PubMed abstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. Our analysis based on excess words usage suggests that at least 10% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, and was as high as 30% for some PubMed sub-corpora. We show that the appearance of LLM-based writing assistants has had an unprecedented impact in the scientific literature, surpassing the effect of major world events such as the Covid pandemic.","sentences":["Recent large language models (LLMs) can generate and revise text with human-level performance, and have been widely commercialized in systems like ChatGPT.","These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused.","Yet, many scientists have been using them to assist their scholarly writing.","How wide-spread is LLM usage in the academic literature currently?","To answer this question, we use an unbiased, large-scale approach, free from any assumptions on academic LLM usage.","We study vocabulary changes in 14 million PubMed abstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words.","Our analysis based on excess words usage suggests that at least 10% of 2024 abstracts were processed with LLMs.","This lower bound differed across disciplines, countries, and journals, and was as high as 30% for some PubMed sub-corpora.","We show that the appearance of LLM-based writing assistants has had an unprecedented impact in the scientific literature, surpassing the effect of major world events such as the Covid pandemic."],"url":"http://arxiv.org/abs/2406.07016v1","category":"cs.CL"}
{"created":"2024-06-11 07:10:45","title":"Breaking Free: Efficient Multi-Party Private Set Union Without Non-Collusion Assumptions","abstract":"Multi-party private set union (MPSU) protocol enables $m$ $(m > 2)$ parties, each holding a set, to collectively compute the union of their sets without revealing any additional information to other parties. There are two main categories of MPSU protocols: The first builds on public-key techniques. All existing works in this category involve a super-linear number of public-key operations, resulting in poor practical efficiency. The second builds on oblivious transfer and symmetric-key techniques. The only existing work in this category is proposed by Liu and Gao (ASIACRYPT 2023), which features the best concrete performance among all existing protocols, despite its super-linear computation and communication. Unfortunately, it does not achieve the standard semi-honest security, as it inherently relies on a non-collusion assumption, which is unlikely to hold in practice. Therefore, the problem of constructing a practical MPSU protocol based on oblivious transfer and symmetric-key techniques in standard semi-honest model remains open. Furthermore, there is no MPSU protocol achieving both linear computation and linear communication complexity, which leaves another unresolved problem. In this work, we resolve these two open problems. We propose the first MPSU protocol based on oblivious transfer and symmetric-key techniques in the standard semi-honest model. This protocol is $4.9-9.3 \\times$ faster than Liu and Gao in the LAN setting. Concretely, our protocol requires only $3.6$ seconds in online phase for 3 parties with sets of $2^{20}$ items each. We propose the first MPSU protocol achieving both linear computation and linear communication complexity, based on public-key operations. This protocol has the lowest overall communication costs and shows a factor of $3.0-36.5\\times$ improvement in terms of overall communication compared to Liu and Gao.","sentences":["Multi-party private set union (MPSU) protocol enables $m$ $(m > 2)$ parties, each holding a set, to collectively compute the union of their sets without revealing any additional information to other parties.","There are two main categories of MPSU protocols: The first builds on public-key techniques.","All existing works in this category involve a super-linear number of public-key operations, resulting in poor practical efficiency.","The second builds on oblivious transfer and symmetric-key techniques.","The only existing work in this category is proposed by Liu and Gao (ASIACRYPT 2023), which features the best concrete performance among all existing protocols, despite its super-linear computation and communication.","Unfortunately, it does not achieve the standard semi-honest security, as it inherently relies on a non-collusion assumption, which is unlikely to hold in practice.","Therefore, the problem of constructing a practical MPSU protocol based on oblivious transfer and symmetric-key techniques in standard semi-honest model remains open.","Furthermore, there is no MPSU protocol achieving both linear computation and linear communication complexity, which leaves another unresolved problem.","In this work, we resolve these two open problems.","We propose the first MPSU protocol based on oblivious transfer and symmetric-key techniques in the standard semi-honest model.","This protocol is $4.9-9.3 \\times$ faster than Liu and Gao in the LAN setting.","Concretely, our protocol requires only $3.6$ seconds in online phase for 3 parties with sets of $2^{20}$ items each.","We propose the first MPSU protocol achieving both linear computation and linear communication complexity, based on public-key operations.","This protocol has the lowest overall communication costs and shows a factor of $3.0-36.5\\times$ improvement in terms of overall communication compared to Liu and Gao."],"url":"http://arxiv.org/abs/2406.07011v1","category":"cs.CR"}
{"created":"2024-06-11 07:08:48","title":"Eye-for-an-eye: Appearance Transfer with Semantic Correspondence in Diffusion Models","abstract":"As pretrained text-to-image diffusion models have become a useful tool for image synthesis, people want to specify the results in various ways. In this paper, we introduce a method to produce results with the same structure of a target image but painted with colors from a reference image, i.e., appearance transfer, especially following the semantic correspondence between the result and the reference. E.g., the result wing takes color from the reference wing, not the reference head. Existing methods rely on the query-key similarity within self-attention layer, usually producing defective results. To this end, we propose to find semantic correspondences and explicitly rearrange the features according to the semantic correspondences. Extensive experiments show the superiority of our method in various aspects: preserving the structure of the target and reflecting the color from the reference according to the semantic correspondences, even when the two images are not aligned.","sentences":["As pretrained text-to-image diffusion models have become a useful tool for image synthesis, people want to specify the results in various ways.","In this paper, we introduce a method to produce results with the same structure of a target image but painted with colors from a reference image, i.e., appearance transfer, especially following the semantic correspondence between the result and the reference.","E.g., the result wing takes color from the reference wing, not the reference head.","Existing methods rely on the query-key similarity within self-attention layer, usually producing defective results.","To this end, we propose to find semantic correspondences and explicitly rearrange the features according to the semantic correspondences.","Extensive experiments show the superiority of our method in various aspects: preserving the structure of the target and reflecting the color from the reference according to the semantic correspondences, even when the two images are not aligned."],"url":"http://arxiv.org/abs/2406.07008v1","category":"cs.CV"}
{"created":"2024-06-11 06:59:55","title":"MIPI 2024 Challenge on Few-shot RAW Image Denoising: Methods and Results","abstract":"The increasing demand for computational photography and imaging on mobile platforms has led to the widespread development and integration of advanced image sensors with novel algorithms in camera systems. However, the scarcity of high-quality data for research and the rare opportunity for in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI). Building on the achievements of the previous MIPI Workshops held at ECCV 2022 and CVPR 2023, we introduce our third MIPI challenge including three tracks focusing on novel image sensors and imaging algorithms. In this paper, we summarize and review the Few-shot RAW Image Denoising track on MIPI 2024. In total, 165 participants were successfully registered, and 7 teams submitted results in the final testing phase. The developed solutions in this challenge achieved state-of-the-art erformance on Few-shot RAW Image Denoising. More details of this challenge and the link to the dataset can be found at https://mipichallenge.org/MIPI2024.","sentences":["The increasing demand for computational photography and imaging on mobile platforms has led to the widespread development and integration of advanced image sensors with novel algorithms in camera systems.","However, the scarcity of high-quality data for research and the rare opportunity for in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI).","Building on the achievements of the previous MIPI Workshops held at ECCV 2022 and CVPR 2023, we introduce our third MIPI challenge including three tracks focusing on novel image sensors and imaging algorithms.","In this paper, we summarize and review the Few-shot RAW Image Denoising track on MIPI 2024.","In total, 165 participants were successfully registered, and 7 teams submitted results in the final testing phase.","The developed solutions in this challenge achieved state-of-the-art erformance on Few-shot RAW Image Denoising.","More details of this challenge and the link to the dataset can be found at https://mipichallenge.org/MIPI2024."],"url":"http://arxiv.org/abs/2406.07006v1","category":"cs.CV"}
{"created":"2024-06-11 06:53:19","title":"Mitigating Boundary Ambiguity and Inherent Bias for Text Classification in the Era of Large Language Models","abstract":"Text classification is a crucial task encountered frequently in practical scenarios, yet it is still under-explored in the era of large language models (LLMs). This study shows that LLMs are vulnerable to changes in the number and arrangement of options in text classification. Our extensive empirical analyses reveal that the key bottleneck arises from ambiguous decision boundaries and inherent biases towards specific tokens and positions. To mitigate these issues, we make the first attempt and propose a novel two-stage classification framework for LLMs. Our approach is grounded in the empirical observation that pairwise comparisons can effectively alleviate boundary ambiguity and inherent bias. Specifically, we begin with a self-reduction technique to efficiently narrow down numerous options, which contributes to reduced decision space and a faster comparison process. Subsequently, pairwise contrastive comparisons are employed in a chain-of-thought manner to draw out nuances and distinguish confusable options, thus refining the ambiguous decision boundary. Extensive experiments on four datasets (Banking77, HWU64, LIU54, and Clinic150) verify the effectiveness of our framework. Furthermore, benefitting from our framework, various LLMs can achieve consistent improvements. Our code and data are available in \\url{https://github.com/Chuge0335/PC-CoT}.","sentences":["Text classification is a crucial task encountered frequently in practical scenarios, yet it is still under-explored in the era of large language models (LLMs).","This study shows that LLMs are vulnerable to changes in the number and arrangement of options in text classification.","Our extensive empirical analyses reveal that the key bottleneck arises from ambiguous decision boundaries and inherent biases towards specific tokens and positions.","To mitigate these issues, we make the first attempt and propose a novel two-stage classification framework for LLMs.","Our approach is grounded in the empirical observation that pairwise comparisons can effectively alleviate boundary ambiguity and inherent bias.","Specifically, we begin with a self-reduction technique to efficiently narrow down numerous options, which contributes to reduced decision space and a faster comparison process.","Subsequently, pairwise contrastive comparisons are employed in a chain-of-thought manner to draw out nuances and distinguish confusable options, thus refining the ambiguous decision boundary.","Extensive experiments on four datasets (Banking77, HWU64, LIU54, and Clinic150) verify the effectiveness of our framework.","Furthermore, benefitting from our framework, various LLMs can achieve consistent improvements.","Our code and data are available in \\url{https://github.com/Chuge0335/PC-CoT}."],"url":"http://arxiv.org/abs/2406.07001v1","category":"cs.CL"}
{"created":"2024-06-11 06:31:03","title":"DNN Partitioning, Task Offloading, and Resource Allocation in Dynamic Vehicular Networks: A Lyapunov-Guided Diffusion-Based Reinforcement Learning Approach","abstract":"The rapid advancement of Artificial Intelligence (AI) has introduced Deep Neural Network (DNN)-based tasks to the ecosystem of vehicular networks. These tasks are often computation-intensive, requiring substantial computation resources, which are beyond the capability of a single vehicle. To address this challenge, Vehicular Edge Computing (VEC) has emerged as a solution, offering computing services for DNN-based tasks through resource pooling via Vehicle-to-Vehicle/Infrastructure (V2V/V2I) communications. In this paper, we formulate the problem of joint DNN partitioning, task offloading, and resource allocation in VEC as a dynamic long-term optimization. Our objective is to minimize the DNN-based task completion time while guaranteeing the system stability over time. To this end, we first leverage a Lyapunov optimization technique to decouple the original long-term optimization with stability constraints into a per-slot deterministic problem. Afterwards, we propose a Multi-Agent Diffusion-based Deep Reinforcement Learning (MAD2RL) algorithm, incorporating the innovative use of diffusion models to determine the optimal DNN partitioning and task offloading decisions. Furthermore, we integrate convex optimization techniques into MAD2RL as a subroutine to allocate computation resources, enhancing the learning efficiency. Through simulations under real-world movement traces of vehicles, we demonstrate the superior performance of our proposed algorithm compared to existing benchmark solutions.","sentences":["The rapid advancement of Artificial Intelligence (AI) has introduced Deep Neural Network (DNN)-based tasks to the ecosystem of vehicular networks.","These tasks are often computation-intensive, requiring substantial computation resources, which are beyond the capability of a single vehicle.","To address this challenge, Vehicular Edge Computing (VEC) has emerged as a solution, offering computing services for DNN-based tasks through resource pooling via Vehicle-to-Vehicle/Infrastructure (V2V/V2I) communications.","In this paper, we formulate the problem of joint DNN partitioning, task offloading, and resource allocation in VEC as a dynamic long-term optimization.","Our objective is to minimize the DNN-based task completion time while guaranteeing the system stability over time.","To this end, we first leverage a Lyapunov optimization technique to decouple the original long-term optimization with stability constraints into a per-slot deterministic problem.","Afterwards, we propose a Multi-Agent Diffusion-based Deep Reinforcement Learning (MAD2RL) algorithm, incorporating the innovative use of diffusion models to determine the optimal DNN partitioning and task offloading decisions.","Furthermore, we integrate convex optimization techniques into MAD2RL as a subroutine to allocate computation resources, enhancing the learning efficiency.","Through simulations under real-world movement traces of vehicles, we demonstrate the superior performance of our proposed algorithm compared to existing benchmark solutions."],"url":"http://arxiv.org/abs/2406.06986v1","category":"cs.LG"}
{"created":"2024-06-11 06:28:20","title":"Secrecy Energy Efficiency Maximization in RIS-Aided Wireless Networks","abstract":"This work proposes a provably convergent and low complexity optimization algorithm for the maximization of the secrecy energy efficiency in the uplink of a wireless network aided by a Reconfigurable Intelligent Surface (RIS), in the presence of an eavesdropper. The mobil users' transmit powers and the RIS reflection coefficients are optimized. Numerical results show the performance of the proposed methods and compare the use of active and nearly-passive RISs from an energy-efficient perspective.","sentences":["This work proposes a provably convergent and low complexity optimization algorithm for the maximization of the secrecy energy efficiency in the uplink of a wireless network aided by a Reconfigurable Intelligent Surface (RIS), in the presence of an eavesdropper.","The mobil users' transmit powers and the RIS reflection coefficients are optimized.","Numerical results show the performance of the proposed methods and compare the use of active and nearly-passive RISs from an energy-efficient perspective."],"url":"http://arxiv.org/abs/2406.06983v1","category":"eess.SP"}
{"created":"2024-06-11 06:23:29","title":"Sensitivity Analysis for the Test-Negative Design","abstract":"The test-negative design has become popular for evaluating the effectiveness of post-licensure vaccines using observational data. In addition to its logistical convenience on data collection, the design is also believed to control for the differential health-care-seeking behavior between vaccinated and unvaccinated individuals, which is an important while often unmeasured confounder between the vaccination and infection. Hence, the design has been employed routinely to monitor seasonal flu vaccines and more recently to measure the COVID-19 vaccine effectiveness. Despite its popularity, the design has been questioned, in particular about its ability to fully control for the unmeasured confounding. In this paper, we explore deviations from a perfect test-negative design, and propose various sensitivity analysis methods for estimating the effect of vaccination measured by the causal odds ratio on the subpopulation of individuals with good health-care-seeking behavior. We start with point identification of the causal odds ratio under a test-negative design, considering two forms of assumptions on the unmeasured confounder. These assumptions then lead to two approaches for conducting sensitivity analysis, addressing the influence of the unmeasured confounding in different ways. Specifically, one approach investigates partial control for unmeasured confounder in the test-negative design, while the other examines the impact of unmeasured confounder on both vaccination and infection. Furthermore, these approaches can be combined to provide narrower bounds on the true causal odds ratio, and can be further extended to sharpen the bounds by restricting the treatment effect heterogeneity. Finally, we apply the proposed methods to evaluate the effectiveness of COVID-19 vaccines using observational data from test-negative designs.","sentences":["The test-negative design has become popular for evaluating the effectiveness of post-licensure vaccines using observational data.","In addition to its logistical convenience on data collection, the design is also believed to control for the differential health-care-seeking behavior between vaccinated and unvaccinated individuals, which is an important while often unmeasured confounder between the vaccination and infection.","Hence, the design has been employed routinely to monitor seasonal flu vaccines and more recently to measure the COVID-19 vaccine effectiveness.","Despite its popularity, the design has been questioned, in particular about its ability to fully control for the unmeasured confounding.","In this paper, we explore deviations from a perfect test-negative design, and propose various sensitivity analysis methods for estimating the effect of vaccination measured by the causal odds ratio on the subpopulation of individuals with good health-care-seeking behavior.","We start with point identification of the causal odds ratio under a test-negative design, considering two forms of assumptions on the unmeasured confounder.","These assumptions then lead to two approaches for conducting sensitivity analysis, addressing the influence of the unmeasured confounding in different ways.","Specifically, one approach investigates partial control for unmeasured confounder in the test-negative design, while the other examines the impact of unmeasured confounder on both vaccination and infection.","Furthermore, these approaches can be combined to provide narrower bounds on the true causal odds ratio, and can be further extended to sharpen the bounds by restricting the treatment effect heterogeneity.","Finally, we apply the proposed methods to evaluate the effectiveness of COVID-19 vaccines using observational data from test-negative designs."],"url":"http://arxiv.org/abs/2406.06980v1","category":"stat.ME"}
{"created":"2024-06-11 06:18:22","title":"Cross-domain-aware Worker Selection with Training for Crowdsourced Annotation","abstract":"Annotation through crowdsourcing draws incremental attention, which relies on an effective selection scheme given a pool of workers. Existing methods propose to select workers based on their performance on tasks with ground truth, while two important points are missed. 1) The historical performances of workers in other tasks. In real-world scenarios, workers need to solve a new task whose correlation with previous tasks is not well-known before the training, which is called cross-domain. 2) The dynamic worker performance as workers will learn from the ground truth. In this paper, we consider both factors in designing an allocation scheme named cross-domain-aware worker selection with training approach. Our approach proposes two estimation modules to both statistically analyze the cross-domain correlation and simulate the learning gain of workers dynamically. A framework with a theoretical analysis of the worker elimination process is given. To validate the effectiveness of our methods, we collect two novel real-world datasets and generate synthetic datasets. The experiment results show that our method outperforms the baselines on both real-world and synthetic datasets.","sentences":["Annotation through crowdsourcing draws incremental attention, which relies on an effective selection scheme given a pool of workers.","Existing methods propose to select workers based on their performance on tasks with ground truth, while two important points are missed.","1) The historical performances of workers in other tasks.","In real-world scenarios, workers need to solve a new task whose correlation with previous tasks is not well-known before the training, which is called cross-domain.","2)","The dynamic worker performance as workers will learn from the ground truth.","In this paper, we consider both factors in designing an allocation scheme named cross-domain-aware worker selection with training approach.","Our approach proposes two estimation modules to both statistically analyze the cross-domain correlation and simulate the learning gain of workers dynamically.","A framework with a theoretical analysis of the worker elimination process is given.","To validate the effectiveness of our methods, we collect two novel real-world datasets and generate synthetic datasets.","The experiment results show that our method outperforms the baselines on both real-world and synthetic datasets."],"url":"http://arxiv.org/abs/2406.06977v1","category":"cs.LG"}
{"created":"2024-06-11 06:16:33","title":"Discrete Dictionary-based Decomposition Layer for Structured Representation Learning","abstract":"Neuro-symbolic neural networks have been extensively studied to integrate symbolic operations with neural networks, thereby improving systematic generalization. Specifically, Tensor Product Representation (TPR) framework enables neural networks to perform differentiable symbolic operations by encoding the symbolic structure of data within vector spaces. However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations. To address this decomposition problem, we propose a Discrete Dictionary-based Decomposition (D3) layer designed to enhance the decomposition capabilities of TPR-based models. D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations. It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned symbolic features within these dictionaries. D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications. Our experimental results demonstrate that D3 significantly improves the systematic generalization of various TPR-based models while requiring fewer additional parameters. Notably, D3 outperforms baseline models on the synthetic task that demands the systematic decomposition of unseen combinatorial data.","sentences":["Neuro-symbolic neural networks have been extensively studied to integrate symbolic operations with neural networks, thereby improving systematic generalization.","Specifically, Tensor Product Representation (TPR) framework enables neural networks to perform differentiable symbolic operations by encoding the symbolic structure of data within vector spaces.","However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations.","To address this decomposition problem, we propose a Discrete Dictionary-based Decomposition (D3) layer designed to enhance the decomposition capabilities of TPR-based models.","D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations.","It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned symbolic features within these dictionaries.","D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications.","Our experimental results demonstrate that D3 significantly improves the systematic generalization of various TPR-based models while requiring fewer additional parameters.","Notably, D3 outperforms baseline models on the synthetic task that demands the systematic decomposition of unseen combinatorial data."],"url":"http://arxiv.org/abs/2406.06976v1","category":"cs.LG"}
{"created":"2024-06-11 06:13:58","title":"TraceMesh: Scalable and Streaming Sampling for Distributed Traces","abstract":"Distributed tracing serves as a fundamental element in the monitoring of cloud-based and datacenter systems. It provides visibility into the full lifecycle of a request or operation across multiple services, which is essential for understanding system dependencies and performance bottlenecks. To mitigate computational and storage overheads, most tracing frameworks adopt a uniform sampling strategy, which inevitably captures overlapping and redundant information. More advanced methods employ learning-based approaches to bias the sampling toward more informative traces. However, existing methods fall short of considering the high-dimensional and dynamic nature of trace data, which is essential for the production deployment of trace sampling. To address these practical challenges, in this paper we present TraceMesh, a scalable and streaming sampler for distributed traces. TraceMesh employs Locality-Sensitivity Hashing (LSH) to improve sampling efficiency by projecting traces into a low-dimensional space while preserving their similarity. In this process, TraceMesh accommodates previously unseen trace features in a unified and streamlined way. Subsequently, TraceMesh samples traces through evolving clustering, which dynamically adjusts the sampling decision to avoid over-sampling of recurring traces. The proposed method is evaluated with trace data collected from both open-source microservice benchmarks and production service systems. Experimental results demonstrate that TraceMesh outperforms state-of-the-art methods by a significant margin in both sampling accuracy and efficiency.","sentences":["Distributed tracing serves as a fundamental element in the monitoring of cloud-based and datacenter systems.","It provides visibility into the full lifecycle of a request or operation across multiple services, which is essential for understanding system dependencies and performance bottlenecks.","To mitigate computational and storage overheads, most tracing frameworks adopt a uniform sampling strategy, which inevitably captures overlapping and redundant information.","More advanced methods employ learning-based approaches to bias the sampling toward more informative traces.","However, existing methods fall short of considering the high-dimensional and dynamic nature of trace data, which is essential for the production deployment of trace sampling.","To address these practical challenges, in this paper we present TraceMesh, a scalable and streaming sampler for distributed traces.","TraceMesh employs Locality-Sensitivity Hashing (LSH) to improve sampling efficiency by projecting traces into a low-dimensional space while preserving their similarity.","In this process, TraceMesh accommodates previously unseen trace features in a unified and streamlined way.","Subsequently, TraceMesh samples traces through evolving clustering, which dynamically adjusts the sampling decision to avoid over-sampling of recurring traces.","The proposed method is evaluated with trace data collected from both open-source microservice benchmarks and production service systems.","Experimental results demonstrate that TraceMesh outperforms state-of-the-art methods by a significant margin in both sampling accuracy and efficiency."],"url":"http://arxiv.org/abs/2406.06975v1","category":"cs.DC"}
{"created":"2024-06-11 06:09:41","title":"Generative Lifting of Multiview to 3D from Unknown Pose: Wrapping NeRF inside Diffusion","abstract":"We cast multiview reconstruction from unknown pose as a generative modeling problem. From a collection of unannotated 2D images of a scene, our approach simultaneously learns both a network to predict camera pose from 2D image input, as well as the parameters of a Neural Radiance Field (NeRF) for the 3D scene. To drive learning, we wrap both the pose prediction network and NeRF inside a Denoising Diffusion Probabilistic Model (DDPM) and train the system via the standard denoising objective. Our framework requires the system accomplish the task of denoising an input 2D image by predicting its pose and rendering the NeRF from that pose. Learning to denoise thus forces the system to concurrently learn the underlying 3D NeRF representation and a mapping from images to camera extrinsic parameters. To facilitate the latter, we design a custom network architecture to represent pose as a distribution, granting implicit capacity for discovering view correspondences when trained end-to-end for denoising alone. This technique allows our system to successfully build NeRFs, without pose knowledge, for challenging scenes where competing methods fail. At the conclusion of training, our learned NeRF can be extracted and used as a 3D scene model; our full system can be used to sample novel camera poses and generate novel-view images.","sentences":["We cast multiview reconstruction from unknown pose as a generative modeling problem.","From a collection of unannotated 2D images of a scene, our approach simultaneously learns both a network to predict camera pose from 2D image input, as well as the parameters of a Neural Radiance Field (NeRF) for the 3D scene.","To drive learning, we wrap both the pose prediction network and NeRF inside a Denoising Diffusion Probabilistic Model (DDPM) and train the system via the standard denoising objective.","Our framework requires the system accomplish the task of denoising an input 2D image by predicting its pose and rendering the NeRF from that pose.","Learning to denoise thus forces the system to concurrently learn the underlying 3D NeRF representation and a mapping from images to camera extrinsic parameters.","To facilitate the latter, we design a custom network architecture to represent pose as a distribution, granting implicit capacity for discovering view correspondences when trained end-to-end for denoising alone.","This technique allows our system to successfully build NeRFs, without pose knowledge, for challenging scenes where competing methods fail.","At the conclusion of training, our learned NeRF can be extracted and used as a 3D scene model; our full system can be used to sample novel camera poses and generate novel-view images."],"url":"http://arxiv.org/abs/2406.06972v1","category":"cs.CV"}
{"created":"2024-06-11 05:51:44","title":"Beyond the Norms: Detecting Prediction Errors in Regression Models","abstract":"This paper tackles the challenge of detecting unreliable behavior in regression algorithms, which may arise from intrinsic variability (e.g., aleatoric uncertainty) or modeling errors (e.g., model uncertainty). First, we formally introduce the notion of unreliability in regression, i.e., when the output of the regressor exceeds a specified discrepancy (or error). Then, using powerful tools for probabilistic modeling, we estimate the discrepancy density, and we measure its statistical diversity using our proposed metric for statistical dissimilarity. In turn, this allows us to derive a data-driven score that expresses the uncertainty of the regression outcome. We show empirical improvements in error detection for multiple regression tasks, consistently outperforming popular baseline approaches, and contributing to the broader field of uncertainty quantification and safe machine learning systems. Our code is available at https://zenodo.org/records/11281964.","sentences":["This paper tackles the challenge of detecting unreliable behavior in regression algorithms, which may arise from intrinsic variability (e.g., aleatoric uncertainty) or modeling errors (e.g., model uncertainty).","First, we formally introduce the notion of unreliability in regression, i.e., when the output of the regressor exceeds a specified discrepancy (or error).","Then, using powerful tools for probabilistic modeling, we estimate the discrepancy density, and we measure its statistical diversity using our proposed metric for statistical dissimilarity.","In turn, this allows us to derive a data-driven score that expresses the uncertainty of the regression outcome.","We show empirical improvements in error detection for multiple regression tasks, consistently outperforming popular baseline approaches, and contributing to the broader field of uncertainty quantification and safe machine learning systems.","Our code is available at https://zenodo.org/records/11281964."],"url":"http://arxiv.org/abs/2406.06968v1","category":"cs.LG"}
{"created":"2024-06-11 05:50:34","title":"Dual Thinking and Perceptual Analysis of Deep Learning Models using Human Adversarial Examples","abstract":"The dual thinking framework considers fast, intuitive processing and slower, logical processing. The perception of dual thinking in vision requires images where inferences from intuitive and logical processing differ. We introduce an adversarial dataset to provide evidence for the dual thinking framework in human vision, which also aids in studying the qualitative behavior of deep learning models. Our study also addresses a major criticism of using classification models as computational models of human vision by using instance segmentation models that localize objects. The evidence underscores the importance of shape in identifying instances in human vision and shows that deep learning models lack an understanding of sub-structures, as indicated by errors related to the position and number of sub-components. Additionally, the similarity in errors made by models and intuitive human processing indicates that models only address intuitive thinking in human vision.","sentences":["The dual thinking framework considers fast, intuitive processing and slower, logical processing.","The perception of dual thinking in vision requires images where inferences from intuitive and logical processing differ.","We introduce an adversarial dataset to provide evidence for the dual thinking framework in human vision, which also aids in studying the qualitative behavior of deep learning models.","Our study also addresses a major criticism of using classification models as computational models of human vision by using instance segmentation models that localize objects.","The evidence underscores the importance of shape in identifying instances in human vision and shows that deep learning models lack an understanding of sub-structures, as indicated by errors related to the position and number of sub-components.","Additionally, the similarity in errors made by models and intuitive human processing indicates that models only address intuitive thinking in human vision."],"url":"http://arxiv.org/abs/2406.06967v1","category":"cs.CV"}
{"created":"2024-06-11 05:48:04","title":"Evolving from Single-modal to Multi-modal Facial Deepfake Detection: A Survey","abstract":"This survey addresses the critical challenge of deepfake detection amidst the rapid advancements in artificial intelligence. As AI-generated media, including video, audio and text, become more realistic, the risk of misuse to spread misinformation and commit identity fraud increases. Focused on face-centric deepfakes, this work traces the evolution from traditional single-modality methods to sophisticated multi-modal approaches that handle audio-visual and text-visual scenarios. We provide comprehensive taxonomies of detection techniques, discuss the evolution of generative methods from auto-encoders and GANs to diffusion models, and categorize these technologies by their unique attributes. To our knowledge, this is the first survey of its kind. We also explore the challenges of adapting detection methods to new generative models and enhancing the reliability and robustness of deepfake detectors, proposing directions for future research. This survey offers a detailed roadmap for researchers, supporting the development of technologies to counter the deceptive use of AI in media creation, particularly facial forgery. A curated list of all related papers can be found at \\href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection}.","sentences":["This survey addresses the critical challenge of deepfake detection amidst the rapid advancements in artificial intelligence.","As AI-generated media, including video, audio and text, become more realistic, the risk of misuse to spread misinformation and commit identity fraud increases.","Focused on face-centric deepfakes, this work traces the evolution from traditional single-modality methods to sophisticated multi-modal approaches that handle audio-visual and text-visual scenarios.","We provide comprehensive taxonomies of detection techniques, discuss the evolution of generative methods from auto-encoders and GANs to diffusion models, and categorize these technologies by their unique attributes.","To our knowledge, this is the first survey of its kind.","We also explore the challenges of adapting detection methods to new generative models and enhancing the reliability and robustness of deepfake detectors, proposing directions for future research.","This survey offers a detailed roadmap for researchers, supporting the development of technologies to counter the deceptive use of AI in media creation, particularly facial forgery.","A curated list of all related papers can be found at \\href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection}."],"url":"http://arxiv.org/abs/2406.06965v1","category":"cs.CV"}
{"created":"2024-06-11 05:44:56","title":"Evolving Subnetwork Training for Large Language Models","abstract":"Large language models have ushered in a new era of artificial intelligence research. However, their substantial training costs hinder further development and widespread adoption. In this paper, inspired by the redundancy in the parameters of large language models, we propose a novel training paradigm: Evolving Subnetwork Training (EST). EST samples subnetworks from the layers of the large language model and from commonly used modules within each layer, Multi-Head Attention (MHA) and Multi-Layer Perceptron (MLP). By gradually increasing the size of the subnetworks during the training process, EST can save the cost of training. We apply EST to train GPT2 model and TinyLlama model, resulting in 26.7\\% FLOPs saving for GPT2 and 25.0\\% for TinyLlama without an increase in loss on the pre-training dataset. Moreover, EST leads to performance improvements in downstream tasks, indicating that it benefits generalization. Additionally, we provide intuitive theoretical studies based on training dynamics and Dropout theory to ensure the feasibility of EST. Our code is available at https://github.com/OpenDFM/EST.","sentences":["Large language models have ushered in a new era of artificial intelligence research.","However, their substantial training costs hinder further development and widespread adoption.","In this paper, inspired by the redundancy in the parameters of large language models, we propose a novel training paradigm: Evolving Subnetwork Training (EST).","EST samples subnetworks from the layers of the large language model and from commonly used modules within each layer, Multi-Head Attention (MHA) and Multi-Layer Perceptron (MLP).","By gradually increasing the size of the subnetworks during the training process, EST can save the cost of training.","We apply EST to train GPT2 model and TinyLlama model, resulting in 26.7\\% FLOPs saving for GPT2 and 25.0\\% for TinyLlama without an increase in loss on the pre-training dataset.","Moreover, EST leads to performance improvements in downstream tasks, indicating that it benefits generalization.","Additionally, we provide intuitive theoretical studies based on training dynamics and Dropout theory to ensure the feasibility of EST.","Our code is available at https://github.com/OpenDFM/EST."],"url":"http://arxiv.org/abs/2406.06962v1","category":"cs.CL"}
{"created":"2024-06-11 05:35:18","title":"Unleashing the Denoising Capability of Diffusion Prior for Solving Inverse Problems","abstract":"The recent emergence of diffusion models has significantly advanced the precision of learnable priors, presenting innovative avenues for addressing inverse problems. Since inverse problems inherently entail maximum a posteriori estimation, previous works have endeavored to integrate diffusion priors into the optimization frameworks. However, prevailing optimization-based inverse algorithms primarily exploit the prior information within the diffusion models while neglecting their denoising capability. To bridge this gap, this work leverages the diffusion process to reframe noisy inverse problems as a two-variable constrained optimization task by introducing an auxiliary optimization variable. By employing gradient truncation, the projection gradient descent method is efficiently utilized to solve the corresponding optimization problem. The proposed algorithm, termed ProjDiff, effectively harnesses the prior information and the denoising capability of a pre-trained diffusion model within the optimization framework. Extensive experiments on the image restoration tasks and source separation and partial generation tasks demonstrate that ProjDiff exhibits superior performance across various linear and nonlinear inverse problems, highlighting its potential for practical applications. Code is available at https://github.com/weigerzan/ProjDiff/.","sentences":["The recent emergence of diffusion models has significantly advanced the precision of learnable priors, presenting innovative avenues for addressing inverse problems.","Since inverse problems inherently entail maximum a posteriori estimation, previous works have endeavored to integrate diffusion priors into the optimization frameworks.","However, prevailing optimization-based inverse algorithms primarily exploit the prior information within the diffusion models while neglecting their denoising capability.","To bridge this gap, this work leverages the diffusion process to reframe noisy inverse problems as a two-variable constrained optimization task by introducing an auxiliary optimization variable.","By employing gradient truncation, the projection gradient descent method is efficiently utilized to solve the corresponding optimization problem.","The proposed algorithm, termed ProjDiff, effectively harnesses the prior information and the denoising capability of a pre-trained diffusion model within the optimization framework.","Extensive experiments on the image restoration tasks and source separation and partial generation tasks demonstrate that ProjDiff exhibits superior performance across various linear and nonlinear inverse problems, highlighting its potential for practical applications.","Code is available at https://github.com/weigerzan/ProjDiff/."],"url":"http://arxiv.org/abs/2406.06959v1","category":"cs.LG"}
{"created":"2024-06-11 05:23:11","title":"Factors and moderators of ageism: An analysis using data from 55 countries in the World Values Survey Wave 6","abstract":"Today, as the aging of the world accelerates, it is an urgent task to clarify factors that prevent ageism. In this study, using hierarchical multiple regression analysis of data from 40,869 people from 55 countries collected in the World Values Survey Wave 6, we showed that after controlling for demographic factors, stereotypes, a hungry spirit, and male chauvinism are related to ageism, and that altruism, trust within and outside the family, and trust in competition moderate the relationship between the independent and dependent variables. Furthermore, data from Japan, which has the highest aging rate and aging speed in the world, showed that these moderation relationships are moderated.","sentences":["Today, as the aging of the world accelerates, it is an urgent task to clarify factors that prevent ageism.","In this study, using hierarchical multiple regression analysis of data from 40,869 people from 55 countries collected in the World Values Survey Wave 6, we showed that after controlling for demographic factors, stereotypes, a hungry spirit, and male chauvinism are related to ageism, and that altruism, trust within and outside the family, and trust in competition moderate the relationship between the independent and dependent variables.","Furthermore, data from Japan, which has the highest aging rate and aging speed in the world, showed that these moderation relationships are moderated."],"url":"http://arxiv.org/abs/2406.06952v1","category":"econ.GN"}
{"created":"2024-06-11 05:21:30","title":"Triple-domain Feature Learning with Frequency-aware Memory Enhancement for Moving Infrared Small Target Detection","abstract":"Moving infrared small target detection presents significant challenges due to tiny target sizes and low contrast against backgrounds. Currently-existing methods primarily focus on extracting target features only from the spatial-temporal domain. For further enhancing feature representation, more information domains such as frequency are believed to be potentially valuable. To extend target feature learning, we propose a new Triple-domain Strategy (Tridos) with the frequency-aware memory enhancement on the spatial-temporal domain. In our scheme, it effectively detaches and enhances frequency features by a local-global frequency-aware module with Fourier transform. Inspired by the human visual system, our memory enhancement aims to capture the target spatial relations between video frames. Furthermore, it encodes temporal dynamics motion features via differential learning and residual enhancing. Additionally, we further design a residual compensation unit to reconcile possible cross-domain feature mismatches. To our best knowledge, our Tridos is the first work to explore target feature learning comprehensively in spatial-temporal-frequency domains. The extensive experiments on three datasets (DAUB, ITSDT-15K, and IRDST) validate that our triple-domain learning scheme could be obviously superior to state-of-the-art ones. Source codes are available at https://github.com/UESTC-nnLab/Tridos.","sentences":["Moving infrared small target detection presents significant challenges due to tiny target sizes and low contrast against backgrounds.","Currently-existing methods primarily focus on extracting target features only from the spatial-temporal domain.","For further enhancing feature representation, more information domains such as frequency are believed to be potentially valuable.","To extend target feature learning, we propose a new Triple-domain Strategy (Tridos) with the frequency-aware memory enhancement on the spatial-temporal domain.","In our scheme, it effectively detaches and enhances frequency features by a local-global frequency-aware module with Fourier transform.","Inspired by the human visual system, our memory enhancement aims to capture the target spatial relations between video frames.","Furthermore, it encodes temporal dynamics motion features via differential learning and residual enhancing.","Additionally, we further design a residual compensation unit to reconcile possible cross-domain feature mismatches.","To our best knowledge, our Tridos is the first work to explore target feature learning comprehensively in spatial-temporal-frequency domains.","The extensive experiments on three datasets (DAUB, ITSDT-15K, and IRDST) validate that our triple-domain learning scheme could be obviously superior to state-of-the-art ones.","Source codes are available at https://github.com/UESTC-nnLab/Tridos."],"url":"http://arxiv.org/abs/2406.06949v1","category":"cs.CV"}
{"created":"2024-06-11 05:21:20","title":"CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only","abstract":"Software robots have long been deployed in Robotic Process Automation (RPA) to automate mundane and repetitive computer tasks. The advent of Large Language Models (LLMs) with advanced reasoning capabilities has set the stage for these agents to now undertake more complex and even previously unseen tasks. However, the LLM-based automation techniques in recent literature frequently rely on HTML source codes for input, limiting their application to web environments. Moreover, the information contained in HTML codes is often inaccurate or incomplete, making the agent less reliable for practical applications. We propose an LLM-based agent that functions solely on the basis of screenshots for recognizing environments, while leveraging in-context learning to eliminate the need for collecting large datasets of human demonstration. Our strategy, named Context-Aware Action Planning (CAAP) prompting encourages the agent to meticulously review the context in various angles. Through our proposed methodology, we achieve a success rate of 94.4% on 67~types of MiniWoB++ problems, utilizing only 1.48~demonstrations per problem type. Our method offers the potential for broader applications, especially for tasks that require inter-application coordination on computers or smartphones, showcasing a significant advancement in the field of automation agents. Codes and models are accessible at https://github.com/caap-agent/caap-agent.","sentences":["Software robots have long been deployed in Robotic Process Automation (RPA) to automate mundane and repetitive computer tasks.","The advent of Large Language Models (LLMs) with advanced reasoning capabilities has set the stage for these agents to now undertake more complex and even previously unseen tasks.","However, the LLM-based automation techniques in recent literature frequently rely on HTML source codes for input, limiting their application to web environments.","Moreover, the information contained in HTML codes is often inaccurate or incomplete, making the agent less reliable for practical applications.","We propose an LLM-based agent that functions solely on the basis of screenshots for recognizing environments, while leveraging in-context learning to eliminate the need for collecting large datasets of human demonstration.","Our strategy, named Context-Aware Action Planning (CAAP) prompting encourages the agent to meticulously review the context in various angles.","Through our proposed methodology, we achieve a success rate of 94.4% on 67~types of MiniWoB++ problems, utilizing only 1.48~demonstrations per problem type.","Our method offers the potential for broader applications, especially for tasks that require inter-application coordination on computers or smartphones, showcasing a significant advancement in the field of automation agents.","Codes and models are accessible at https://github.com/caap-agent/caap-agent."],"url":"http://arxiv.org/abs/2406.06947v1","category":"cs.AI"}
{"created":"2024-06-11 04:49:04","title":"Efficient combination of observational and experimental datasets under general restrictions on outcome mean functions","abstract":"A researcher collecting data from a randomized controlled trial (RCT) often has access to an auxiliary observational dataset that may be confounded or otherwise biased for estimating causal effects. Common modeling assumptions impose restrictions on the outcome mean function - the conditional expectation of the outcome of interest given observed covariates - in the two datasets. Running examples from the literature include settings where the observational dataset is subject to outcome-mediated selection bias or to confounding bias taking an assumed parametric form. We propose a succinct framework to derive the efficient influence function for any identifiable pathwise differentiable estimand under a general class of restrictions on the outcome mean function. This uncovers surprising results that with homoskedastic outcomes and a constant propensity score in the RCT, even strong parametric assumptions cannot improve the semiparametric lower bound for estimating various average treatment effects. We then leverage double machine learning to construct a one-step estimator that achieves the semiparametric efficiency bound even in cases when the outcome mean function and other nuisance parameters are estimated nonparametrically. The goal is to empower a researcher with custom, previously unstudied modeling restrictions on the outcome mean function to systematically construct causal estimators that maximially leverage their assumptions for variance reduction. We demonstrate the finite sample precision gains of our estimator over existing approaches in extensions of various numerical studies and data examples from the literature.","sentences":["A researcher collecting data from a randomized controlled trial (RCT) often has access to an auxiliary observational dataset that may be confounded or otherwise biased for estimating causal effects.","Common modeling assumptions impose restrictions on the outcome mean function - the conditional expectation of the outcome of interest given observed covariates - in the two datasets.","Running examples from the literature include settings where the observational dataset is subject to outcome-mediated selection bias or to confounding bias taking an assumed parametric form.","We propose a succinct framework to derive the efficient influence function for any identifiable pathwise differentiable estimand under a general class of restrictions on the outcome mean function.","This uncovers surprising results that with homoskedastic outcomes and a constant propensity score in the RCT, even strong parametric assumptions cannot improve the semiparametric lower bound for estimating various average treatment effects.","We then leverage double machine learning to construct a one-step estimator that achieves the semiparametric efficiency bound even in cases when the outcome mean function and other nuisance parameters are estimated nonparametrically.","The goal is to empower a researcher with custom, previously unstudied modeling restrictions on the outcome mean function to systematically construct causal estimators that maximially leverage their assumptions for variance reduction.","We demonstrate the finite sample precision gains of our estimator over existing approaches in extensions of various numerical studies and data examples from the literature."],"url":"http://arxiv.org/abs/2406.06941v1","category":"stat.ME"}
{"created":"2024-06-11 04:25:48","title":"A Non-autoregressive Generation Framework for End-to-End Simultaneous Speech-to-Any Translation","abstract":"Simultaneous translation models play a crucial role in facilitating communication. However, existing research primarily focuses on text-to-text or speech-to-text models, necessitating additional cascade components to achieve speech-to-speech translation. These pipeline methods suffer from error propagation and accumulate delays in each cascade component, resulting in reduced synchronization between the speaker and listener. To overcome these challenges, we propose a novel non-autoregressive generation framework for simultaneous speech translation (NAST-S2X), which integrates speech-to-text and speech-to-speech tasks into a unified end-to-end framework. We develop a non-autoregressive decoder capable of concurrently generating multiple text or acoustic unit tokens upon receiving fixed-length speech chunks. The decoder can generate blank or repeated tokens and employ CTC decoding to dynamically adjust its latency. Experimental results show that NAST-S2X outperforms state-of-the-art models in both speech-to-text and speech-to-speech tasks. It achieves high-quality simultaneous interpretation within a delay of less than 3 seconds and provides a 28 times decoding speedup in offline generation.","sentences":["Simultaneous translation models play a crucial role in facilitating communication.","However, existing research primarily focuses on text-to-text or speech-to-text models, necessitating additional cascade components to achieve speech-to-speech translation.","These pipeline methods suffer from error propagation and accumulate delays in each cascade component, resulting in reduced synchronization between the speaker and listener.","To overcome these challenges, we propose a novel non-autoregressive generation framework for simultaneous speech translation (NAST-S2X), which integrates speech-to-text and speech-to-speech tasks into a unified end-to-end framework.","We develop a non-autoregressive decoder capable of concurrently generating multiple text or acoustic unit tokens upon receiving fixed-length speech chunks.","The decoder can generate blank or repeated tokens and employ CTC decoding to dynamically adjust its latency.","Experimental results show that NAST-S2X outperforms state-of-the-art models in both speech-to-text and speech-to-speech tasks.","It achieves high-quality simultaneous interpretation within a delay of less than 3 seconds and provides a 28 times decoding speedup in offline generation."],"url":"http://arxiv.org/abs/2406.06937v1","category":"cs.CL"}
{"created":"2024-06-11 04:09:27","title":"Operadic structure on Hamiltonian paths and cycles","abstract":"We study Hamiltonian paths and cycles in undirected graphs from an operadic viewpoint. We show that the graphical collection $\\mathsf{Ham}$ encoding directed Hamiltonian paths in connected graphs admits an operad-like structure, called a contractad. Similarly, we construct the graphical collection of Hamiltonian cycles $\\mathsf{CycHam}$ that forms a right module over the contractad $\\mathsf{Ham}$. We use the machinery of contractad generating series for counting Hamiltonian paths/cycles for particular types of graphs.","sentences":["We study Hamiltonian paths and cycles in undirected graphs from an operadic viewpoint.","We show that the graphical collection $\\mathsf{Ham}$ encoding directed Hamiltonian paths in connected graphs admits an operad-like structure, called a contractad.","Similarly, we construct the graphical collection of Hamiltonian cycles $\\mathsf{CycHam}$ that forms a right module over the contractad $\\mathsf{Ham}$.","We use the machinery of contractad generating series for counting Hamiltonian paths/cycles for particular types of graphs."],"url":"http://arxiv.org/abs/2406.06931v1","category":"math.CO"}
{"created":"2024-06-11 03:12:35","title":"Controlling noisy herds","abstract":"The wisdom of the crowd breaks down in small groups. While large flocks exhibit swarm intelligence to evade predators, small groups display erratic behavior, oscillating between unity and discord. We investigate these dynamics using small groups of sheep controlled by shepherd dogs in century-old sheepdog trials, proposing a two-parameter stochastic dynamic framework. Our model employs pressure (stimulus intensity) and lightness (response isotropy) to simulate herding and shedding behaviors. Light sheep rapidly achieve a stable herding state, while heavy sheep exhibit intermittent herding and orthogonal alignment to the dog. High response isotropy enhances group cohesion but complicates group splitting. We construct a unified phase diagram for sheep behavior, identifying three regimes (fleeing, flocking, and grazing) based on group size and stimulus specificity. Increasing stimulus specificity shifts small group behavior from grazing to fleeing, while larger groups exhibit flocking. This transition underscores the challenge of controlling small indecisive collectives. Introducing the Indecisive Collective Algorithm (ICA), we show that deliberate indecisiveness and stochasticity improve control efficiency. ICA outperforms traditional averaging-based algorithms in high-noise settings and excels in tasks requiring group splitting. Our study offers a foundational framework for controlling small, indecisive groups, applicable to biochemical reactions, cell populations, and opinion dynamics.","sentences":["The wisdom of the crowd breaks down in small groups.","While large flocks exhibit swarm intelligence to evade predators, small groups display erratic behavior, oscillating between unity and discord.","We investigate these dynamics using small groups of sheep controlled by shepherd dogs in century-old sheepdog trials, proposing a two-parameter stochastic dynamic framework.","Our model employs pressure (stimulus intensity) and lightness (response isotropy) to simulate herding and shedding behaviors.","Light sheep rapidly achieve a stable herding state, while heavy sheep exhibit intermittent herding and orthogonal alignment to the dog.","High response isotropy enhances group cohesion but complicates group splitting.","We construct a unified phase diagram for sheep behavior, identifying three regimes (fleeing, flocking, and grazing) based on group size and stimulus specificity.","Increasing stimulus specificity shifts small group behavior from grazing to fleeing, while larger groups exhibit flocking.","This transition underscores the challenge of controlling small indecisive collectives.","Introducing the Indecisive Collective Algorithm (ICA), we show that deliberate indecisiveness and stochasticity improve control efficiency.","ICA outperforms traditional averaging-based algorithms in high-noise settings and excels in tasks requiring group splitting.","Our study offers a foundational framework for controlling small, indecisive groups, applicable to biochemical reactions, cell populations, and opinion dynamics."],"url":"http://arxiv.org/abs/2406.06912v1","category":"physics.soc-ph"}
{"created":"2024-06-11 03:09:37","title":"AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising","abstract":"Diffusion models have garnered significant interest from the community for their great generative ability across various applications. However, their typical multi-step sequential-denoising nature gives rise to high cumulative latency, thereby precluding the possibilities of parallel computation. To address this, we introduce AsyncDiff, a universal and plug-and-play acceleration scheme that enables model parallelism across multiple devices. Our approach divides the cumbersome noise prediction model into multiple components, assigning each to a different device. To break the dependency chain between these components, it transforms the conventional sequential denoising into an asynchronous process by exploiting the high similarity between hidden states in consecutive diffusion steps. Consequently, each component is facilitated to compute in parallel on separate devices. The proposed strategy significantly reduces inference latency while minimally impacting the generative quality. Specifically, for the Stable Diffusion v2.1, AsyncDiff achieves a 2.7x speedup with negligible degradation and a 4.0x speedup with only a slight reduction of 0.38 in CLIP Score, on four NVIDIA A5000 GPUs. Our experiments also demonstrate that AsyncDiff can be readily applied to video diffusion models with encouraging performances. The code is available at https://github.com/czg1225/AsyncDiff.","sentences":["Diffusion models have garnered significant interest from the community for their great generative ability across various applications.","However, their typical multi-step sequential-denoising nature gives rise to high cumulative latency, thereby precluding the possibilities of parallel computation.","To address this, we introduce AsyncDiff, a universal and plug-and-play acceleration scheme that enables model parallelism across multiple devices.","Our approach divides the cumbersome noise prediction model into multiple components, assigning each to a different device.","To break the dependency chain between these components, it transforms the conventional sequential denoising into an asynchronous process by exploiting the high similarity between hidden states in consecutive diffusion steps.","Consequently, each component is facilitated to compute in parallel on separate devices.","The proposed strategy significantly reduces inference latency while minimally impacting the generative quality.","Specifically, for the Stable Diffusion v2.1, AsyncDiff achieves a 2.7x speedup with negligible degradation and a 4.0x speedup with only a slight reduction of 0.38 in CLIP Score, on four NVIDIA A5000 GPUs.","Our experiments also demonstrate that AsyncDiff can be readily applied to video diffusion models with encouraging performances.","The code is available at https://github.com/czg1225/AsyncDiff."],"url":"http://arxiv.org/abs/2406.06911v1","category":"cs.CV"}
{"created":"2024-06-11 03:00:41","title":"SignMusketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale","abstract":"A persistent challenge in sign language video processing, including the task of sign language to written language translation, is how we learn representations of sign language in an effective and efficient way that can preserve the important attributes of these languages, while remaining invariant to irrelevant visual differences. Informed by the nature and linguistics of signed languages, our proposed method focuses on just the most relevant parts in a signing video: the face, hands and body posture of the signer. However, instead of using pose estimation coordinates from off-the-shelf pose tracking models, which have inconsistent performance for hands and faces, we propose to learn the complex handshapes and rich facial expressions of sign languages in a self-supervised fashion. Our approach is based on learning from individual frames (rather than video sequences) and is therefore much more efficient than prior work on sign language pre-training. Compared to a recent model that established a new state of the art in sign language translation on the How2Sign dataset, our approach yields similar translation performance, using less than 3\\% of the compute.","sentences":["A persistent challenge in sign language video processing, including the task of sign language to written language translation, is how we learn representations of sign language in an effective and efficient way that can preserve the important attributes of these languages, while remaining invariant to irrelevant visual differences.","Informed by the nature and linguistics of signed languages, our proposed method focuses on just the most relevant parts in a signing video: the face, hands and body posture of the signer.","However, instead of using pose estimation coordinates from off-the-shelf pose tracking models, which have inconsistent performance for hands and faces, we propose to learn the complex handshapes and rich facial expressions of sign languages in a self-supervised fashion.","Our approach is based on learning from individual frames (rather than video sequences) and is therefore much more efficient than prior work on sign language pre-training.","Compared to a recent model that established a new state of the art in sign language translation on the How2Sign dataset, our approach yields similar translation performance, using less than 3\\% of the compute."],"url":"http://arxiv.org/abs/2406.06907v1","category":"cs.CL"}
{"created":"2024-06-11 02:19:31","title":"Nonlinear time-series embedding by monotone variational inequality","abstract":"In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a new method to learn low-dimensional representations of nonlinear time series without supervision and can have provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method is based on the assumption that the observed sequences arise from a common domain, but each sequence obeys its own autoregressive models that are related to each other through low-rank regularization. We cast the problem as a computationally efficient convex matrix parameter recovery problem using monotone Variational Inequality and encode the common domain assumption via low-rank constraint across the learned representations, which can learn the geometry for the entire domain as well as faithful representations for the dynamics of each individual sequence using the domain information in totality. We show the competitive performance of our method on real-world time-series data with the baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering.","sentences":["In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics.","We introduce a new method to learn low-dimensional representations of nonlinear time series without supervision and can have provable recovery guarantees.","The learned representation can be used for downstream machine-learning tasks such as clustering and classification.","The method is based on the assumption that the observed sequences arise from a common domain, but each sequence obeys its own autoregressive models that are related to each other through low-rank regularization.","We cast the problem as a computationally efficient convex matrix parameter recovery problem using monotone Variational Inequality and encode the common domain assumption via low-rank constraint across the learned representations, which can learn the geometry for the entire domain as well as faithful representations for the dynamics of each individual sequence using the domain information in totality.","We show the competitive performance of our method on real-world time-series data with the baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering."],"url":"http://arxiv.org/abs/2406.06894v1","category":"cs.LG"}
{"created":"2024-06-11 02:13:46","title":"Tokenize features, enhancing tables: the FT-TABPFN model for tabular classification","abstract":"Traditional methods for tabular classification usually rely on supervised learning from scratch, which requires extensive training data to determine model parameters. However, a novel approach called Prior-Data Fitted Networks (TabPFN) has changed this paradigm. TabPFN uses a 12-layer transformer trained on large synthetic datasets to learn universal tabular representations. This method enables fast and accurate predictions on new tasks with a single forward pass and no need for additional training. Although TabPFN has been successful on small datasets, it generally shows weaker performance when dealing with categorical features. To overcome this limitation, we propose FT-TabPFN, which is an enhanced version of TabPFN that includes a novel Feature Tokenization layer to better handle classification features. By fine-tuning it for downstream tasks, FT-TabPFN not only expands the functionality of the original model but also significantly improves its applicability and accuracy in tabular classification. Our full source code is available for community use and development.","sentences":["Traditional methods for tabular classification usually rely on supervised learning from scratch, which requires extensive training data to determine model parameters.","However, a novel approach called Prior-Data Fitted Networks (TabPFN) has changed this paradigm.","TabPFN uses a 12-layer transformer trained on large synthetic datasets to learn universal tabular representations.","This method enables fast and accurate predictions on new tasks with a single forward pass and no need for additional training.","Although TabPFN has been successful on small datasets, it generally shows weaker performance when dealing with categorical features.","To overcome this limitation, we propose FT-TabPFN, which is an enhanced version of TabPFN that includes a novel Feature Tokenization layer to better handle classification features.","By fine-tuning it for downstream tasks, FT-TabPFN not only expands the functionality of the original model but also significantly improves its applicability and accuracy in tabular classification.","Our full source code is available for community use and development."],"url":"http://arxiv.org/abs/2406.06891v1","category":"cs.LG"}
{"created":"2024-06-11 02:07:18","title":"PLUM: Preference Learning Plus Test Cases Yields Better Code Language Models","abstract":"Instruction-finetuned code language models (LMs) have shown promise in various programming tasks. They are trained, using a language modeling objective, on natural language instructions and gold code snippet pairs. Recent evidence suggests that these models, never exposed to incorrect solutions during training, often struggle to distinguish between correct and incorrect solutions. This observation raises our inquiry: Can preference learning, which trains models to prefer correct solutions over incorrect ones, help push the boundaries of code LMs even further? We propose PLUM, a novel \\textbf{p}reference \\textbf{l}earning framework a\\textbf{u}gmented with test cases tailored for code L\\textbf{M}s.PLUM aims to investigate the key success factors and potential benefits of preference learning in code LMs, which remain elusive despite its success in aligning LMs with human values. PLUM consists of three stages: (1) Generating test cases for natural language instructions, (2) sampling candidate solutions from the policy and evaluating them against the test cases to create a preference dataset, which is then used to (3) train the policy with a preference learning algorithm. Experiments demonstrate that PLUM substantially improves the performance of existing code LMs on established code generation benchmarks such as HumanEval (+) and MBPP (+), even for the state-of-the-art open-source language model CodeQwen-1.5-7B-Chat. PLUM complements the supervised fine-tuning (SFT) stage, demonstrating synergistic effects.","sentences":["Instruction-finetuned code language models (LMs) have shown promise in various programming tasks.","They are trained, using a language modeling objective, on natural language instructions and gold code snippet pairs.","Recent evidence suggests that these models, never exposed to incorrect solutions during training, often struggle to distinguish between correct and incorrect solutions.","This observation raises our inquiry: Can preference learning, which trains models to prefer correct solutions over incorrect ones, help push the boundaries of code LMs even further?","We propose PLUM, a novel \\textbf{p}reference \\textbf{l}earning framework a\\textbf{u}gmented with test cases tailored for code L\\textbf{M}s.","PLUM aims to investigate the key success factors and potential benefits of preference learning in code LMs, which remain elusive despite its success in aligning LMs with human values.","PLUM consists of three stages: (1) Generating test cases for natural language instructions, (2) sampling candidate solutions from the policy and evaluating them against the test cases to create a preference dataset, which is then used to (3) train the policy with a preference learning algorithm.","Experiments demonstrate that PLUM substantially improves the performance of existing code LMs on established code generation benchmarks such as HumanEval (+) and MBPP (+), even for the state-of-the-art open-source language model CodeQwen-1.5-7B-Chat.","PLUM complements the supervised fine-tuning (SFT) stage, demonstrating synergistic effects."],"url":"http://arxiv.org/abs/2406.06887v1","category":"cs.CL"}
{"created":"2024-06-11 01:20:53","title":"Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback","abstract":"Aligning human preference and value is an important requirement for building contemporary foundation models and embodied AI. However, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into successive stages, such as supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL), each performing one specific learning task. Such a sequential approach results in serious issues such as significant under-utilization of data and distribution mismatch between the learned reward model and generated policy, which eventually lead to poor alignment performance. We develop a single stage approach named Alignment with Integrated Human Feedback (AIHF), capable of integrating both human preference and demonstration to train reward models and the policy. The proposed approach admits a suite of efficient algorithms, which can easily reduce to, and leverage, popular alignment algorithms such as RLHF and Directly Policy Optimization (DPO), and only requires minor changes to the existing alignment pipelines. We demonstrate the efficiency of the proposed solutions with extensive experiments involving alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithms such as RLHF and DPO by large margins, especially when the amount of high-quality preference data is relatively limited.","sentences":["Aligning human preference and value is an important requirement for building contemporary foundation models and embodied AI.","However, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into successive stages, such as supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL), each performing one specific learning task.","Such a sequential approach results in serious issues such as significant under-utilization of data and distribution mismatch between the learned reward model and generated policy, which eventually lead to poor alignment performance.","We develop a single stage approach named Alignment with Integrated Human Feedback (AIHF), capable of integrating both human preference and demonstration to train reward models and the policy.","The proposed approach admits a suite of efficient algorithms, which can easily reduce to, and leverage, popular alignment algorithms such as RLHF and Directly Policy Optimization (DPO), and only requires minor changes to the existing alignment pipelines.","We demonstrate the efficiency of the proposed solutions with extensive experiments involving alignment problems in LLMs and robotic control problems in MuJoCo.","We observe that the proposed solutions outperform the existing alignment algorithms such as RLHF and DPO by large margins, especially when the amount of high-quality preference data is relatively limited."],"url":"http://arxiv.org/abs/2406.06874v1","category":"cs.AI"}
{"created":"2024-06-11 01:17:37","title":"Revolutionizing Wireless Networks with Self-Supervised Learning: A Pathway to Intelligent Communications","abstract":"With the rapid proliferation of mobile devices and data, next-generation wireless communication systems face stringent requirements for ultra-low latency, ultra-high reliability, and massive connectivity. Traditional AI-driven wireless network designs, while promising, often suffer from limitations such as dependency on labeled data and poor generalization. To address these challenges, we present an integration of self-supervised learning (SSL) into wireless networks. SSL leverages large volumes of unlabeled data to train models, enhancing scalability, adaptability, and generalization. This paper offers a comprehensive overview of SSL, categorizing its application scenarios in wireless network optimization and presenting a case study on its impact on semantic communication. Our findings highlight the potentials of SSL to significantly improve wireless network performance without extensive labeled data, paving the way for more intelligent and efficient communication systems.","sentences":["With the rapid proliferation of mobile devices and data, next-generation wireless communication systems face stringent requirements for ultra-low latency, ultra-high reliability, and massive connectivity.","Traditional AI-driven wireless network designs, while promising, often suffer from limitations such as dependency on labeled data and poor generalization.","To address these challenges, we present an integration of self-supervised learning (SSL) into wireless networks.","SSL leverages large volumes of unlabeled data to train models, enhancing scalability, adaptability, and generalization.","This paper offers a comprehensive overview of SSL, categorizing its application scenarios in wireless network optimization and presenting a case study on its impact on semantic communication.","Our findings highlight the potentials of SSL to significantly improve wireless network performance without extensive labeled data, paving the way for more intelligent and efficient communication systems."],"url":"http://arxiv.org/abs/2406.06872v1","category":"eess.SP"}
{"created":"2024-06-11 01:10:40","title":"What's in an embedding? Would a rose by any embedding smell as sweet?","abstract":"Large Language Models (LLMs) are often criticized for lacking true \"understanding\" and an ability to \"reason\" with their knowledge, being seen merely as advanced autocomplete systems. We believe that this perspective might be missing an important insight. We suggest that LLMs do develop a kind of empirical \"understanding\" that is \"geometry\"-like, which seems quite sufficient for a range of applications in NLP, computer vision, coding assistance, etc. However, this \"geometric\" understanding, built from incomplete and noisy data, makes them unreliable, difficult to generalize, and lacking in inference capabilities and explanations, similar to the challenges faced by heuristics-based expert systems decades ago.   To overcome these limitations, we suggest that LLMs should be integrated with an \"algebraic\" representation of knowledge that includes symbolic AI elements used in expert systems. This integration aims to create large knowledge models (LKMs) that not only possess \"deep\" knowledge grounded in first principles, but also have the ability to reason and explain, mimicking human expert capabilities. To harness the full potential of generative AI safely and effectively, a paradigm shift from LLMs to the more comprehensive LKMs is needed.","sentences":["Large Language Models (LLMs) are often criticized for lacking true \"understanding\" and an ability to \"reason\" with their knowledge, being seen merely as advanced autocomplete systems.","We believe that this perspective might be missing an important insight.","We suggest that LLMs do develop a kind of empirical \"understanding\" that is \"geometry\"-like, which seems quite sufficient for a range of applications in NLP, computer vision, coding assistance, etc.","However, this \"geometric\" understanding, built from incomplete and noisy data, makes them unreliable, difficult to generalize, and lacking in inference capabilities and explanations, similar to the challenges faced by heuristics-based expert systems decades ago.   ","To overcome these limitations, we suggest that LLMs should be integrated with an \"algebraic\" representation of knowledge that includes symbolic AI elements used in expert systems.","This integration aims to create large knowledge models (LKMs) that not only possess \"deep\" knowledge grounded in first principles, but also have the ability to reason and explain, mimicking human expert capabilities.","To harness the full potential of generative AI safely and effectively, a paradigm shift from LLMs to the more comprehensive LKMs is needed."],"url":"http://arxiv.org/abs/2406.06870v1","category":"cs.AI"}
{"created":"2024-06-11 00:41:08","title":"Eyeballing Combinatorial Problems: A Case Study of Using Multimodal Large Language Models to Solve Traveling Salesman Problems","abstract":"Multimodal Large Language Models (MLLMs) have demonstrated proficiency in processing di-verse modalities, including text, images, and audio. These models leverage extensive pre-existing knowledge, enabling them to address complex problems with minimal to no specific training examples, as evidenced in few-shot and zero-shot in-context learning scenarios. This paper investigates the use of MLLMs' visual capabilities to 'eyeball' solutions for the Traveling Salesman Problem (TSP) by analyzing images of point distributions on a two-dimensional plane. Our experiments aimed to validate the hypothesis that MLLMs can effectively 'eyeball' viable TSP routes. The results from zero-shot, few-shot, self-ensemble, and self-refine zero-shot evaluations show promising outcomes. We anticipate that these findings will inspire further exploration into MLLMs' visual reasoning abilities to tackle other combinatorial problems.","sentences":["Multimodal Large Language Models (MLLMs) have demonstrated proficiency in processing di-verse modalities, including text, images, and audio.","These models leverage extensive pre-existing knowledge, enabling them to address complex problems with minimal to no specific training examples, as evidenced in few-shot and zero-shot in-context learning scenarios.","This paper investigates the use of MLLMs' visual capabilities to 'eyeball' solutions for the Traveling Salesman Problem (TSP) by analyzing images of point distributions on a two-dimensional plane.","Our experiments aimed to validate the hypothesis that MLLMs can effectively 'eyeball' viable TSP routes.","The results from zero-shot, few-shot, self-ensemble, and self-refine zero-shot evaluations show promising outcomes.","We anticipate that these findings will inspire further exploration into MLLMs' visual reasoning abilities to tackle other combinatorial problems."],"url":"http://arxiv.org/abs/2406.06865v1","category":"cs.AI"}
{"created":"2024-06-11 00:40:17","title":"Validating LLM-Generated Programs with Metamorphic Prompt Testing","abstract":"The latest paradigm shift in software development brings in the innovation and automation afforded by Large Language Models (LLMs), showcased by Generative Pre-trained Transformer (GPT), which has shown remarkable capacity to generate code autonomously, significantly reducing the manual effort required for various programming tasks. Although, the potential benefits of LLM-generated code are vast, most notably in efficiency and rapid prototyping, as LLMs become increasingly integrated into the software development lifecycle and hence the supply chain, complex and multifaceted challenges arise as the code generated from these language models carry profound questions on quality and correctness. Research is required to comprehensively explore these critical concerns surrounding LLM-generated code.   In this paper, we propose a novel solution called metamorphic prompt testing to address these challenges. Our intuitive observation is that intrinsic consistency always exists among correct code pieces but may not exist among flawed code pieces, so we can detect flaws in the code by detecting inconsistencies. Therefore, we can vary a given prompt to multiple prompts with paraphrasing, and to ask the LLM to acquire multiple versions of generated code, so that we can validate whether the semantic relations still hold in the acquired code through cross-validation. Our evaluation on HumanEval shows that metamorphic prompt testing is able to detect 75 percent of the erroneous programs generated by GPT-4, with a false positive rate of 8.6 percent.","sentences":["The latest paradigm shift in software development brings in the innovation and automation afforded by Large Language Models (LLMs), showcased by Generative Pre-trained Transformer (GPT), which has shown remarkable capacity to generate code autonomously, significantly reducing the manual effort required for various programming tasks.","Although, the potential benefits of LLM-generated code are vast, most notably in efficiency and rapid prototyping, as LLMs become increasingly integrated into the software development lifecycle and hence the supply chain, complex and multifaceted challenges arise as the code generated from these language models carry profound questions on quality and correctness.","Research is required to comprehensively explore these critical concerns surrounding LLM-generated code.   ","In this paper, we propose a novel solution called metamorphic prompt testing to address these challenges.","Our intuitive observation is that intrinsic consistency always exists among correct code pieces but may not exist among flawed code pieces, so we can detect flaws in the code by detecting inconsistencies.","Therefore, we can vary a given prompt to multiple prompts with paraphrasing, and to ask the LLM to acquire multiple versions of generated code, so that we can validate whether the semantic relations still hold in the acquired code through cross-validation.","Our evaluation on HumanEval shows that metamorphic prompt testing is able to detect 75 percent of the erroneous programs generated by GPT-4, with a false positive rate of 8.6 percent."],"url":"http://arxiv.org/abs/2406.06864v1","category":"cs.SE"}
{"created":"2024-06-11 00:35:39","title":"Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity","abstract":"Large Language Models (LLMs) have the potential to enhance Agent-Based Modeling by better representing complex interdependent cybersecurity systems, improving cybersecurity threat modeling and risk management. However, evaluating LLMs in this context is crucial for legal compliance and effective application development. Existing LLM evaluation frameworks often overlook the human factor and cognitive computing capabilities essential for interdependent cybersecurity. To address this gap, I propose OllaBench, a novel evaluation framework that assesses LLMs' accuracy, wastefulness, and consistency in answering scenario-based information security compliance and non-compliance questions. OllaBench is built on a foundation of 24 cognitive behavioral theories and empirical evidence from 38 peer-reviewed papers. OllaBench was used to evaluate 21 LLMs, including both open-weight and commercial models from OpenAI, Anthropic, Google, Microsoft, Meta and so on. The results reveal that while commercial LLMs have the highest overall accuracy scores, there is significant room for improvement. Smaller low-resolution open-weight LLMs are not far behind in performance, and there are significant differences in token efficiency and consistency among the evaluated models. OllaBench provides a user-friendly interface and supports a wide range of LLM platforms, making it a valuable tool for researchers and solution developers in the field of human-centric interdependent cybersecurity and beyond.","sentences":["Large Language Models (LLMs) have the potential to enhance Agent-Based Modeling by better representing complex interdependent cybersecurity systems, improving cybersecurity threat modeling and risk management.","However, evaluating LLMs in this context is crucial for legal compliance and effective application development.","Existing LLM evaluation frameworks often overlook the human factor and cognitive computing capabilities essential for interdependent cybersecurity.","To address this gap, I propose OllaBench, a novel evaluation framework that assesses LLMs' accuracy, wastefulness, and consistency in answering scenario-based information security compliance and non-compliance questions.","OllaBench is built on a foundation of 24 cognitive behavioral theories and empirical evidence from 38 peer-reviewed papers.","OllaBench was used to evaluate 21 LLMs, including both open-weight and commercial models from OpenAI, Anthropic, Google, Microsoft, Meta and so on.","The results reveal that while commercial LLMs have the highest overall accuracy scores, there is significant room for improvement.","Smaller low-resolution open-weight LLMs are not far behind in performance, and there are significant differences in token efficiency and consistency among the evaluated models.","OllaBench provides a user-friendly interface and supports a wide range of LLM platforms, making it a valuable tool for researchers and solution developers in the field of human-centric interdependent cybersecurity and beyond."],"url":"http://arxiv.org/abs/2406.06863v1","category":"cs.CR"}
{"created":"2024-06-11 00:02:19","title":"Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning","abstract":"In this paper, we study the non-asymptotic sample complexity for the pure exploration problem in contextual bandits and tabular reinforcement learning (RL): identifying an epsilon-optimal policy from a set of policies with high probability. Existing work in bandits has shown that it is possible to identify the best policy by estimating only the difference between the behaviors of individual policies, which can be substantially cheaper than estimating the behavior of each policy directly. However, the best-known complexities in RL fail to take advantage of this and instead estimate the behavior of each policy directly. Does it suffice to estimate only the differences in the behaviors of policies in RL? We answer this question positively for contextual bandits but in the negative for tabular RL, showing a separation between contextual bandits and RL. However, inspired by this, we show that it almost suffices to estimate only the differences in RL: if we can estimate the behavior of a single reference policy, it suffices to only estimate how any other policy deviates from this reference policy. We develop an algorithm which instantiates this principle and obtains, to the best of our knowledge, the tightest known bound on the sample complexity of tabular RL.","sentences":["In this paper, we study the non-asymptotic sample complexity for the pure exploration problem in contextual bandits and tabular reinforcement learning (RL): identifying an epsilon-optimal policy from a set of policies with high probability.","Existing work in bandits has shown that it is possible to identify the best policy by estimating only the difference between the behaviors of individual policies, which can be substantially cheaper than estimating the behavior of each policy directly.","However, the best-known complexities in RL fail to take advantage of this and instead estimate the behavior of each policy directly.","Does it suffice to estimate only the differences in the behaviors of policies in RL?","We answer this question positively for contextual bandits but in the negative for tabular RL, showing a separation between contextual bandits and RL.","However, inspired by this, we show that it almost suffices to estimate only the differences in RL: if we can estimate the behavior of a single reference policy, it suffices to only estimate how any other policy deviates from this reference policy.","We develop an algorithm which instantiates this principle and obtains, to the best of our knowledge, the tightest known bound on the sample complexity of tabular RL."],"url":"http://arxiv.org/abs/2406.06856v1","category":"cs.LG"}
{"created":"2024-06-10 23:54:21","title":"A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures","abstract":"The large language models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LMMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning. Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms.","sentences":["The large language models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings.","Despite the demonstrable efficacy of LMMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms.","However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks.","Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers.","While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs.","To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods.","Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning.","Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms."],"url":"http://arxiv.org/abs/2406.06852v1","category":"cs.CR"}
{"created":"2024-06-10 23:36:58","title":"Taxes Are All You Need: Integration of Taxonomical Hierarchy Relationships into the Contrastive Loss","abstract":"In this work, we propose a novel supervised contrastive loss that enables the integration of taxonomic hierarchy information during the representation learning process. A supervised contrastive loss operates by enforcing that images with the same class label (positive samples) project closer to each other than images with differing class labels (negative samples). The advantage of this approach is that it directly penalizes the structure of the representation space itself. This enables greater flexibility with respect to encoding semantic concepts. However, the standard supervised contrastive loss only enforces semantic structure based on the downstream task (i.e. the class label). In reality, the class label is only one level of a \\emph{hierarchy of different semantic relationships known as a taxonomy}. For example, the class label is oftentimes the species of an animal, but between different classes there are higher order relationships such as all animals with wings being ``birds\". We show that by explicitly accounting for these relationships with a weighting penalty in the contrastive loss we can out-perform the supervised contrastive loss. Additionally, we demonstrate the adaptability of the notion of a taxonomy by integrating our loss into medical and noise-based settings that show performance improvements by as much as 7%.","sentences":["In this work, we propose a novel supervised contrastive loss that enables the integration of taxonomic hierarchy information during the representation learning process.","A supervised contrastive loss operates by enforcing that images with the same class label (positive samples) project closer to each other than images with differing class labels (negative samples).","The advantage of this approach is that it directly penalizes the structure of the representation space itself.","This enables greater flexibility with respect to encoding semantic concepts.","However, the standard supervised contrastive loss only enforces semantic structure based on the downstream task (i.e. the class label).","In reality, the class label is only one level of a \\emph{hierarchy of different semantic relationships known as a taxonomy}.","For example, the class label is oftentimes the species of an animal, but between different classes there are higher order relationships such as all animals with wings being ``birds\".","We show that by explicitly accounting for these relationships with a weighting penalty in the contrastive loss we can out-perform the supervised contrastive loss.","Additionally, we demonstrate the adaptability of the notion of a taxonomy by integrating our loss into medical and noise-based settings that show performance improvements by as much as 7%."],"url":"http://arxiv.org/abs/2406.06848v1","category":"cs.CV"}
{"created":"2024-06-10 23:25:19","title":"HO-Cap: A Capture System and Dataset for 3D Reconstruction and Pose Tracking of Hand-Object Interaction","abstract":"We introduce a data capture system and a new dataset named HO-Cap that can be used to study 3D reconstruction and pose tracking of hands and objects in videos. The capture system uses multiple RGB-D cameras and a HoloLens headset for data collection, avoiding the use of expensive 3D scanners or mocap systems. We propose a semi-automatic method to obtain annotations of shape and pose of hands and objects in the collected videos, which significantly reduces the required annotation time compared to manual labeling. With this system, we captured a video dataset of humans using objects to perform different tasks, as well as simple pick-and-place and handover of an object from one hand to the other, which can be used as human demonstrations for embodied AI and robot manipulation research. Our data capture setup and annotation framework can be used by the community to reconstruct 3D shapes of objects and human hands and track their poses in videos.","sentences":["We introduce a data capture system and a new dataset named HO-Cap that can be used to study 3D reconstruction and pose tracking of hands and objects in videos.","The capture system uses multiple RGB-D cameras and a HoloLens headset for data collection, avoiding the use of expensive 3D scanners or mocap systems.","We propose a semi-automatic method to obtain annotations of shape and pose of hands and objects in the collected videos, which significantly reduces the required annotation time compared to manual labeling.","With this system, we captured a video dataset of humans using objects to perform different tasks, as well as simple pick-and-place and handover of an object from one hand to the other, which can be used as human demonstrations for embodied AI and robot manipulation research.","Our data capture setup and annotation framework can be used by the community to reconstruct 3D shapes of objects and human hands and track their poses in videos."],"url":"http://arxiv.org/abs/2406.06843v1","category":"cs.CV"}
{"created":"2024-06-10 22:57:27","title":"Stable Minima Cannot Overfit in Univariate ReLU Networks: Generalization by Large Step Sizes","abstract":"We study the generalization of two-layer ReLU neural networks in a univariate nonparametric regression problem with noisy labels. This is a problem where kernels (\\emph{e.g.} NTK) are provably sub-optimal and benign overfitting does not happen, thus disqualifying existing theory for interpolating (0-loss, global optimal) solutions. We present a new theory of generalization for local minima that gradient descent with a constant learning rate can \\emph{stably} converge to. We show that gradient descent with a fixed learning rate $\\eta$ can only find local minima that represent smooth functions with a certain weighted \\emph{first order total variation} bounded by $1/\\eta - 1/2 + \\widetilde{O}(\\sigma + \\sqrt{\\mathrm{MSE}})$ where $\\sigma$ is the label noise level, $\\mathrm{MSE}$ is short for mean squared error against the ground truth, and $\\widetilde{O}(\\cdot)$ hides a logarithmic factor. Under mild assumptions, we also prove a nearly-optimal MSE bound of $\\widetilde{O}(n^{-4/5})$ within the strict interior of the support of the $n$ data points. Our theoretical results are validated by extensive simulation that demonstrates large learning rate training induces sparse linear spline fits. To the best of our knowledge, we are the first to obtain generalization bound via minima stability in the non-interpolation case and the first to show ReLU NNs without regularization can achieve near-optimal rates in nonparametric regression.","sentences":["We study the generalization of two-layer ReLU neural networks in a univariate nonparametric regression problem with noisy labels.","This is a problem where kernels (\\emph{e.g.} NTK) are provably sub-optimal and benign overfitting does not happen, thus disqualifying existing theory for interpolating (0-loss, global optimal) solutions.","We present a new theory of generalization for local minima that gradient descent with a constant learning rate can \\emph{stably} converge to.","We show that gradient descent with a fixed learning rate $\\eta$ can only find local minima that represent smooth functions with a certain weighted \\emph{first order total variation} bounded by $1/\\eta - 1/2 + \\widetilde{O}(\\sigma","+ \\sqrt{\\mathrm{MSE}})$ where $\\sigma$ is the label noise level, $\\mathrm{MSE}$ is short for mean squared error against the ground truth, and $\\widetilde{O}(\\cdot)$ hides a logarithmic factor.","Under mild assumptions, we also prove a nearly-optimal MSE bound of $\\widetilde{O}(n^{-4/5})$ within the strict interior of the support of the $n$ data points.","Our theoretical results are validated by extensive simulation that demonstrates large learning rate training induces sparse linear spline fits.","To the best of our knowledge, we are the first to obtain generalization bound via minima stability in the non-interpolation case and the first to show ReLU NNs without regularization can achieve near-optimal rates in nonparametric regression."],"url":"http://arxiv.org/abs/2406.06838v1","category":"cs.LG"}
{"created":"2024-06-10 22:11:00","title":"Locally Interdependent Multi-Agent MDP: Theoretical Framework for Decentralized Agents with Dynamic Dependencies","abstract":"Many multi-agent systems in practice are decentralized and have dynamically varying dependencies. There has been a lack of attempts in the literature to analyze these systems theoretically. In this paper, we propose and theoretically analyze a decentralized model with dynamically varying dependencies called the Locally Interdependent Multi-Agent MDP. This model can represent problems in many disparate domains such as cooperative navigation, obstacle avoidance, and formation control. Despite the intractability that general partially observable multi-agent systems suffer from, we propose three closed-form policies that are theoretically near-optimal in this setting and can be scalable to compute and store. Consequentially, we reveal a fundamental property of Locally Interdependent Multi-Agent MDP's that the partially observable decentralized solution is exponentially close to the fully observable solution with respect to the visibility radius. We then discuss extensions of our closed-form policies to further improve tractability. We conclude by providing simulations to investigate some long horizon behaviors of our closed-form policies.","sentences":["Many multi-agent systems in practice are decentralized and have dynamically varying dependencies.","There has been a lack of attempts in the literature to analyze these systems theoretically.","In this paper, we propose and theoretically analyze a decentralized model with dynamically varying dependencies called the Locally Interdependent Multi-Agent MDP.","This model can represent problems in many disparate domains such as cooperative navigation, obstacle avoidance, and formation control.","Despite the intractability that general partially observable multi-agent systems suffer from, we propose three closed-form policies that are theoretically near-optimal in this setting and can be scalable to compute and store.","Consequentially, we reveal a fundamental property of Locally Interdependent Multi-Agent MDP's that the partially observable decentralized solution is exponentially close to the fully observable solution with respect to the visibility radius.","We then discuss extensions of our closed-form policies to further improve tractability.","We conclude by providing simulations to investigate some long horizon behaviors of our closed-form policies."],"url":"http://arxiv.org/abs/2406.06823v1","category":"cs.LG"}
{"created":"2024-06-10 22:10:05","title":"An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection","abstract":"Large Language Models (LLMs) have transformed code completion tasks, providing context-based suggestions to boost developer productivity in software engineering. As users often fine-tune these models for specific applications, poisoning and backdoor attacks can covertly alter the model outputs. To address this critical security challenge, we introduce CodeBreaker, a pioneering LLM-assisted backdoor attack framework on code completion models. Unlike recent attacks that embed malicious payloads in detectable or irrelevant sections of the code (e.g., comments), CodeBreaker leverages LLMs (e.g., GPT-4) for sophisticated payload transformation (without affecting functionalities), ensuring that both the poisoned data for fine-tuning and generated code can evade strong vulnerability detection. CodeBreaker stands out with its comprehensive coverage of vulnerabilities, making it the first to provide such an extensive set for evaluation. Our extensive experimental evaluations and user studies underline the strong attack performance of CodeBreaker across various settings, validating its superiority over existing approaches. By integrating malicious payloads directly into the source code with minimal transformation, CodeBreaker challenges current security measures, underscoring the critical need for more robust defenses for code completion.","sentences":["Large Language Models (LLMs) have transformed code completion tasks, providing context-based suggestions to boost developer productivity in software engineering.","As users often fine-tune these models for specific applications, poisoning and backdoor attacks can covertly alter the model outputs.","To address this critical security challenge, we introduce CodeBreaker, a pioneering LLM-assisted backdoor attack framework on code completion models.","Unlike recent attacks that embed malicious payloads in detectable or irrelevant sections of the code (e.g., comments), CodeBreaker leverages LLMs (e.g., GPT-4) for sophisticated payload transformation (without affecting functionalities), ensuring that both the poisoned data for fine-tuning and generated code can evade strong vulnerability detection.","CodeBreaker stands out with its comprehensive coverage of vulnerabilities, making it the first to provide such an extensive set for evaluation.","Our extensive experimental evaluations and user studies underline the strong attack performance of CodeBreaker across various settings, validating its superiority over existing approaches.","By integrating malicious payloads directly into the source code with minimal transformation, CodeBreaker challenges current security measures, underscoring the critical need for more robust defenses for code completion."],"url":"http://arxiv.org/abs/2406.06822v1","category":"cs.CR"}
{"created":"2024-06-10 21:53:55","title":"Evaluating the local bandgap across InxGa1-xAs multiple quantum wells in a metamorphic laser via low-loss EELS","abstract":"We investigate spatially resolved variations in the bandgap energy across multiple InxGa1-xAs quantum wells (QWs) on a GaAs substrate within a metamorphic laser structure. Using high resolution scanning transmission electron microscopy and low-loss electron energy loss spectroscopy, we present a detailed analysis of the local bandgap energy, indium concentration, and strain distribution within the QWs. Our findings reveal significant inhomogeneities, particularly near the interfaces, in both the strain and indium content, and a bandgap variability across QWs. These results are correlated with density functional theory simulations to further elucidate the interplay between strain, composition, and bandgap energy. This work underscores the importance of spatially resolved analysis in understanding, and optimising, the electronic and optical properties of semiconductor devices. The study suggests that the collective impact of individual QWs might affect the emission and performance of the final device, providing insights for the design of next-generation metamorphic lasers with multiple QWs as the active region.","sentences":["We investigate spatially resolved variations in the bandgap energy across multiple InxGa1-xAs quantum wells (QWs) on a GaAs substrate within a metamorphic laser structure.","Using high resolution scanning transmission electron microscopy and low-loss electron energy loss spectroscopy, we present a detailed analysis of the local bandgap energy, indium concentration, and strain distribution within the QWs.","Our findings reveal significant inhomogeneities, particularly near the interfaces, in both the strain and indium content, and a bandgap variability across QWs.","These results are correlated with density functional theory simulations to further elucidate the interplay between strain, composition, and bandgap energy.","This work underscores the importance of spatially resolved analysis in understanding, and optimising, the electronic and optical properties of semiconductor devices.","The study suggests that the collective impact of individual QWs might affect the emission and performance of the final device, providing insights for the design of next-generation metamorphic lasers with multiple QWs as the active region."],"url":"http://arxiv.org/abs/2406.06816v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 21:33:10","title":"Experimental benchmarking of quantum state overlap estimation strategies with photonic systems","abstract":"Accurately estimating the overlap between quantum states is a fundamental task in quantum information processing. While various strategies using distinct quantum measurements have been proposed for overlap estimation, the lack of experimental benchmarks on estimation precision limits strategy selection in different situations. Here we compare the performance of four practical strategies for overlap estimation, including tomography-tomography, tomography-projection, Schur collective measurement and optical swap test. With a photonic system, the overlap-dependent estimation precision for each strategy is quantified in terms of the average estimation variance over uniformly sampled states, which highlight the different performance of different strategies. We further propose an adaptive strategy with optimized precision in full-range overlap estimation. Our results shed new light on extracting the parameter of interest from quantum systems, prompting the design of efficient quantum protocols.","sentences":["Accurately estimating the overlap between quantum states is a fundamental task in quantum information processing.","While various strategies using distinct quantum measurements have been proposed for overlap estimation, the lack of experimental benchmarks on estimation precision limits strategy selection in different situations.","Here we compare the performance of four practical strategies for overlap estimation, including tomography-tomography, tomography-projection, Schur collective measurement and optical swap test.","With a photonic system, the overlap-dependent estimation precision for each strategy is quantified in terms of the average estimation variance over uniformly sampled states, which highlight the different performance of different strategies.","We further propose an adaptive strategy with optimized precision in full-range overlap estimation.","Our results shed new light on extracting the parameter of interest from quantum systems, prompting the design of efficient quantum protocols."],"url":"http://arxiv.org/abs/2406.06810v1","category":"quant-ph"}
{"created":"2024-06-10 21:15:28","title":"Satisficing Exploration in Bandit Optimization","abstract":"Motivated by the concept of satisficing in decision-making, we consider the problem of satisficing exploration in bandit optimization. In this setting, the learner aims at selecting satisficing arms (arms with mean reward exceeding a certain threshold value) as frequently as possible. The performance is measured by satisficing regret, which is the cumulative deficit of the chosen arm's mean reward compared to the threshold. We propose SELECT, a general algorithmic template for Satisficing Exploration via LowEr Confidence bound Testing, that attains constant satisficing regret for a wide variety of bandit optimization problems in the realizable case (i.e., a satisficing arm exists). Specifically, given a class of bandit optimization problems and a corresponding learning oracle with sub-linear (standard) regret upper bound, SELECT iteratively makes use of the oracle to identify a potential satisficing arm with low regret. Then, it collects data samples from this arm, and continuously compares the LCB of the identified arm's mean reward against the threshold value to determine if it is a satisficing arm. As a complement, SELECT also enjoys the same (standard) regret guarantee as the oracle in the non-realizable case. Finally, we conduct numerical experiments to validate the performance of SELECT for several popular bandit optimization settings.","sentences":["Motivated by the concept of satisficing in decision-making, we consider the problem of satisficing exploration in bandit optimization.","In this setting, the learner aims at selecting satisficing arms (arms with mean reward exceeding a certain threshold value) as frequently as possible.","The performance is measured by satisficing regret, which is the cumulative deficit of the chosen arm's mean reward compared to the threshold.","We propose SELECT, a general algorithmic template for Satisficing Exploration via LowEr Confidence bound Testing, that attains constant satisficing regret for a wide variety of bandit optimization problems in the realizable case (i.e., a satisficing arm exists).","Specifically, given a class of bandit optimization problems and a corresponding learning oracle with sub-linear (standard) regret upper bound, SELECT iteratively makes use of the oracle to identify a potential satisficing arm with low regret.","Then, it collects data samples from this arm, and continuously compares the LCB of the identified arm's mean reward against the threshold value to determine if it is a satisficing arm.","As a complement, SELECT also enjoys the same (standard) regret guarantee as the oracle in the non-realizable case.","Finally, we conduct numerical experiments to validate the performance of SELECT for several popular bandit optimization settings."],"url":"http://arxiv.org/abs/2406.06802v1","category":"stat.ML"}
{"created":"2024-06-10 21:10:25","title":"Overcoming Limitations in Artificial Intelligence-based Prostate Cancer Detection through Better Datasets and a Bayesian Approach to Aggregate Panel Predictions","abstract":"Despite considerable progress in developing artificial intelligence (AI) algorithms for prostate cancer detection from whole slide images, the clinical applicability of these models remains limited due to variability in pathological annotations and existing dataset limitations. This article proposes a novel approach to overcome these challenges by leveraging a Bayesian framework to seamlessly integrate new data, and present results as a panel of annotations. The framework is demonstrated by integrating a Bayesian prior with one trained AI model to generate a distribution of Gleason patterns for each pixel of an image. It is shown that using this distribution of Gleason patterns rather than a ground-truth label can improve model applicability, mitigate errors, and highlight areas of interest for pathologists. Additionally, we present a high-quality, hand-curated dataset of prostate histopathological images annotated at the gland level by trained pre-medical students and verified by an expert pathologist. We highlight the potential of this adaptive and uncertainty-aware framework for developing clinically deployable AI tools that can support pathologists in accurate prostate cancer grading, improve diagnostic accuracy, and create positive patient outcomes.","sentences":["Despite considerable progress in developing artificial intelligence (AI) algorithms for prostate cancer detection from whole slide images, the clinical applicability of these models remains limited due to variability in pathological annotations and existing dataset limitations.","This article proposes a novel approach to overcome these challenges by leveraging a Bayesian framework to seamlessly integrate new data, and present results as a panel of annotations.","The framework is demonstrated by integrating a Bayesian prior with one trained AI model to generate a distribution of Gleason patterns for each pixel of an image.","It is shown that using this distribution of Gleason patterns rather than a ground-truth label can improve model applicability, mitigate errors, and highlight areas of interest for pathologists.","Additionally, we present a high-quality, hand-curated dataset of prostate histopathological images annotated at the gland level by trained pre-medical students and verified by an expert pathologist.","We highlight the potential of this adaptive and uncertainty-aware framework for developing clinically deployable AI tools that can support pathologists in accurate prostate cancer grading, improve diagnostic accuracy, and create positive patient outcomes."],"url":"http://arxiv.org/abs/2406.06801v1","category":"q-bio.TO"}
{"created":"2024-06-10 21:02:53","title":"FlexLoc: Conditional Neural Networks for Zero-Shot Sensor Perspective Invariance in Object Localization with Distributed Multimodal Sensors","abstract":"Localization is a critical technology for various applications ranging from navigation and surveillance to assisted living. Localization systems typically fuse information from sensors viewing the scene from different perspectives to estimate the target location while also employing multiple modalities for enhanced robustness and accuracy. Recently, such systems have employed end-to-end deep neural models trained on large datasets due to their superior performance and ability to handle data from diverse sensor modalities. However, such neural models are often trained on data collected from a particular set of sensor poses (i.e., locations and orientations). During real-world deployments, slight deviations from these sensor poses can result in extreme inaccuracies. To address this challenge, we introduce FlexLoc, which employs conditional neural networks to inject node perspective information to adapt the localization pipeline. Specifically, a small subset of model weights are derived from node poses at run time, enabling accurate generalization to unseen perspectives with minimal additional overhead. Our evaluations on a multimodal, multiview indoor tracking dataset showcase that FlexLoc improves the localization accuracy by almost 50% in the zero-shot case (no calibration data available) compared to the baselines. The source code of FlexLoc is available at https://github.com/nesl/FlexLoc.","sentences":["Localization is a critical technology for various applications ranging from navigation and surveillance to assisted living.","Localization systems typically fuse information from sensors viewing the scene from different perspectives to estimate the target location while also employing multiple modalities for enhanced robustness and accuracy.","Recently, such systems have employed end-to-end deep neural models trained on large datasets due to their superior performance and ability to handle data from diverse sensor modalities.","However, such neural models are often trained on data collected from a particular set of sensor poses (i.e., locations and orientations).","During real-world deployments, slight deviations from these sensor poses can result in extreme inaccuracies.","To address this challenge, we introduce FlexLoc, which employs conditional neural networks to inject node perspective information to adapt the localization pipeline.","Specifically, a small subset of model weights are derived from node poses at run time, enabling accurate generalization to unseen perspectives with minimal additional overhead.","Our evaluations on a multimodal, multiview indoor tracking dataset showcase that FlexLoc improves the localization accuracy by almost 50% in the zero-shot case (no calibration data available) compared to the baselines.","The source code of FlexLoc is available at https://github.com/nesl/FlexLoc."],"url":"http://arxiv.org/abs/2406.06796v1","category":"cs.CV"}
{"created":"2024-06-10 20:59:53","title":"PlanDQ: Hierarchical Plan Orchestration via D-Conductor and Q-Performer","abstract":"Despite the recent advancements in offline RL, no unified algorithm could achieve superior performance across a broad range of tasks. Offline \\textit{value function learning}, in particular, struggles with sparse-reward, long-horizon tasks due to the difficulty of solving credit assignment and extrapolation errors that accumulates as the horizon of the task grows.~On the other hand, models that can perform well in long-horizon tasks are designed specifically for goal-conditioned tasks, which commonly perform worse than value function learning methods on short-horizon, dense-reward scenarios. To bridge this gap, we propose a hierarchical planner designed for offline RL called PlanDQ. PlanDQ incorporates a diffusion-based planner at the high level, named D-Conductor, which guides the low-level policy through sub-goals. At the low level, we used a Q-learning based approach called the Q-Performer to accomplish these sub-goals. Our experimental results suggest that PlanDQ can achieve superior or competitive performance on D4RL continuous control benchmark tasks as well as AntMaze, Kitchen, and Calvin as long-horizon tasks.","sentences":["Despite the recent advancements in offline RL, no unified algorithm could achieve superior performance across a broad range of tasks.","Offline \\textit{value function learning}, in particular, struggles with sparse-reward, long-horizon tasks due to the difficulty of solving credit assignment and extrapolation errors that accumulates as the horizon of the task grows.~On","the other hand, models that can perform well in long-horizon tasks are designed specifically for goal-conditioned tasks, which commonly perform worse than value function learning methods on short-horizon, dense-reward scenarios.","To bridge this gap, we propose a hierarchical planner designed for offline RL called PlanDQ.","PlanDQ incorporates a diffusion-based planner at the high level, named D-Conductor, which guides the low-level policy through sub-goals.","At the low level, we used a Q-learning based approach called the Q-Performer to accomplish these sub-goals.","Our experimental results suggest that PlanDQ can achieve superior or competitive performance on D4RL continuous control benchmark tasks as well as AntMaze, Kitchen, and Calvin as long-horizon tasks."],"url":"http://arxiv.org/abs/2406.06793v1","category":"cs.LG"}
{"created":"2024-06-10 20:59:52","title":"Reinforced Compressive Neural Architecture Search for Versatile Adversarial Robustness","abstract":"Prior neural architecture search (NAS) for adversarial robustness works have discovered that a lightweight and adversarially robust neural network architecture could exist in a non-robust large teacher network, generally disclosed by heuristic rules through statistical analysis and neural architecture search, generally disclosed by heuristic rules from neural architecture search. However, heuristic methods cannot uniformly handle different adversarial attacks and \"teacher\" network capacity. To solve this challenge, we propose a Reinforced Compressive Neural Architecture Search (RC-NAS) for Versatile Adversarial Robustness. Specifically, we define task settings that compose datasets, adversarial attacks, and teacher network information. Given diverse tasks, we conduct a novel dual-level training paradigm that consists of a meta-training and a fine-tuning phase to effectively expose the RL agent to diverse attack scenarios (in meta-training), and making it adapt quickly to locate a sub-network (in fine-tuning) for any previously unseen scenarios. Experiments show that our framework could achieve adaptive compression towards different initial teacher networks, datasets, and adversarial attacks, resulting in more lightweight and adversarially robust architectures.","sentences":["Prior neural architecture search (NAS) for adversarial robustness works have discovered that a lightweight and adversarially robust neural network architecture could exist in a non-robust large teacher network, generally disclosed by heuristic rules through statistical analysis and neural architecture search, generally disclosed by heuristic rules from neural architecture search.","However, heuristic methods cannot uniformly handle different adversarial attacks and \"teacher\" network capacity.","To solve this challenge, we propose a Reinforced Compressive Neural Architecture Search (RC-NAS) for Versatile Adversarial Robustness.","Specifically, we define task settings that compose datasets, adversarial attacks, and teacher network information.","Given diverse tasks, we conduct a novel dual-level training paradigm that consists of a meta-training and a fine-tuning phase to effectively expose the RL agent to diverse attack scenarios (in meta-training), and making it adapt quickly to locate a sub-network (in fine-tuning) for any previously unseen scenarios.","Experiments show that our framework could achieve adaptive compression towards different initial teacher networks, datasets, and adversarial attacks, resulting in more lightweight and adversarially robust architectures."],"url":"http://arxiv.org/abs/2406.06792v1","category":"cs.LG"}
{"created":"2024-06-10 20:49:54","title":"BTS: Bridging Text and Sound Modalities for Metadata-Aided Respiratory Sound Classification","abstract":"Respiratory sound classification (RSC) is challenging due to varied acoustic signatures, primarily influenced by patient demographics and recording environments. To address this issue, we introduce a text-audio multimodal model that utilizes metadata of respiratory sounds, which provides useful complementary information for RSC. Specifically, we fine-tune a pretrained text-audio multimodal model using free-text descriptions derived from the sound samples' metadata which includes the gender and age of patients, type of recording devices, and recording location on the patient's body. Our method achieves state-of-the-art performance on the ICBHI dataset, surpassing the previous best result by a notable margin of 1.17%. This result validates the effectiveness of leveraging metadata and respiratory sound samples in enhancing RSC performance. Additionally, we investigate the model performance in the case where metadata is partially unavailable, which may occur in real-world clinical setting.","sentences":["Respiratory sound classification (RSC) is challenging due to varied acoustic signatures, primarily influenced by patient demographics and recording environments.","To address this issue, we introduce a text-audio multimodal model that utilizes metadata of respiratory sounds, which provides useful complementary information for RSC.","Specifically, we fine-tune a pretrained text-audio multimodal model using free-text descriptions derived from the sound samples' metadata which includes the gender and age of patients, type of recording devices, and recording location on the patient's body.","Our method achieves state-of-the-art performance on the ICBHI dataset, surpassing the previous best result by a notable margin of 1.17%.","This result validates the effectiveness of leveraging metadata and respiratory sound samples in enhancing RSC performance.","Additionally, we investigate the model performance in the case where metadata is partially unavailable, which may occur in real-world clinical setting."],"url":"http://arxiv.org/abs/2406.06786v1","category":"cs.SD"}
{"created":"2024-06-10 20:25:18","title":"MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension","abstract":"Recently, Large Language Models (LLMs) with their strong task-handling capabilities have shown remarkable advancements across a spectrum of fields, moving beyond natural language understanding. However, their proficiency within the chemistry domain remains restricted, especially in solving professional molecule-related tasks. This challenge is attributed to their inherent limitations in comprehending molecules using only common textual representations, i.e., SMILES strings. In this study, we seek to enhance the ability of LLMs to comprehend molecules by designing and equipping them with a multi-modal external module, namely MolX. In particular, instead of directly using a SMILES string to represent a molecule, we utilize specific encoders to extract fine-grained features from both SMILES string and 2D molecular graph representations for feeding into an LLM. Moreover, a human-defined molecular fingerprint is incorporated to leverage its embedded domain knowledge. Then, to establish an alignment between MolX and the LLM's textual input space, the whole model in which the LLM is frozen, is pre-trained with a versatile strategy including a diverse set of tasks. Extensive experimental evaluations demonstrate that our proposed method only introduces a small number of trainable parameters while outperforming baselines on various downstream molecule-related tasks ranging from molecule-to-text translation to retrosynthesis, with and without fine-tuning the LLM.","sentences":["Recently, Large Language Models (LLMs) with their strong task-handling capabilities have shown remarkable advancements across a spectrum of fields, moving beyond natural language understanding.","However, their proficiency within the chemistry domain remains restricted, especially in solving professional molecule-related tasks.","This challenge is attributed to their inherent limitations in comprehending molecules using only common textual representations, i.e., SMILES strings.","In this study, we seek to enhance the ability of LLMs to comprehend molecules by designing and equipping them with a multi-modal external module, namely MolX. In particular, instead of directly using a SMILES string to represent a molecule, we utilize specific encoders to extract fine-grained features from both SMILES string and 2D molecular graph representations for feeding into an LLM.","Moreover, a human-defined molecular fingerprint is incorporated to leverage its embedded domain knowledge.","Then, to establish an alignment between MolX and the LLM's textual input space, the whole model in which the LLM is frozen, is pre-trained with a versatile strategy including a diverse set of tasks.","Extensive experimental evaluations demonstrate that our proposed method only introduces a small number of trainable parameters while outperforming baselines on various downstream molecule-related tasks ranging from molecule-to-text translation to retrosynthesis, with and without fine-tuning the LLM."],"url":"http://arxiv.org/abs/2406.06777v1","category":"cs.CV"}
{"created":"2024-06-10 20:24:14","title":"SeeFar: Satellite Agnostic Multi-Resolution Dataset for Geospatial Foundation Models","abstract":"SeeFar is an evolving collection of multi-resolution satellite images from public and commercial satellites. We specifically curated this dataset for training geospatial foundation models, unconstrained by satellite type. In recent years, advances in technology have made satellite imagery more accessible than ever. More earth-observing satellites have been launched in the last five years than in the previous fifty. Modern commercial satellites now offer up to 100 times the spatial resolution of public access satellites. However, the high cost and limited historical availability of commercial satellite imagery is a barrier to the training of foundational models, impacting what images can be used during inference. The SeeFar dataset represents a step towards training models that are satellite-agnostic by combining multi-resolution commercial and public access pre-processed images. This will enable users to utilize historical data alongside higher-resolution, more expensive satellite imagery, offering greater flexibility during inference. To achieve this, we describe a process for standardizing data from diverse satellite sources, normalizing different data formats, and aligning spectral bands to enhance interoperability. The SeeFar dataset includes images at a resolution of 384x384 pixels, spanning four spectral bands (Blue, Green, Red, and Near-Infrared) and expanding spatial resolutions (starting with 30, 10, 1.5, and 1.0 meters), all in cloud-optimized GeoTIFF format. It also provides consistent and comprehensive metadata to enhance data transparency and reliability. By aggregating data from multiple sources, SeeFar makes processed and consistent satellite data accessible to a wider range of users - from researchers to policymakers - fostering competition and innovation in satellite imagery analysis. The dataset is available at \\url{coastalcarbon.ai/seefar}.","sentences":["SeeFar is an evolving collection of multi-resolution satellite images from public and commercial satellites.","We specifically curated this dataset for training geospatial foundation models, unconstrained by satellite type.","In recent years, advances in technology have made satellite imagery more accessible than ever.","More earth-observing satellites have been launched in the last five years than in the previous fifty.","Modern commercial satellites now offer up to 100 times the spatial resolution of public access satellites.","However, the high cost and limited historical availability of commercial satellite imagery is a barrier to the training of foundational models, impacting what images can be used during inference.","The SeeFar dataset represents a step towards training models that are satellite-agnostic by combining multi-resolution commercial and public access pre-processed images.","This will enable users to utilize historical data alongside higher-resolution, more expensive satellite imagery, offering greater flexibility during inference.","To achieve this, we describe a process for standardizing data from diverse satellite sources, normalizing different data formats, and aligning spectral bands to enhance interoperability.","The SeeFar dataset includes images at a resolution of 384x384 pixels, spanning four spectral bands (Blue, Green, Red, and Near-Infrared) and expanding spatial resolutions (starting with 30, 10, 1.5, and 1.0 meters), all in cloud-optimized GeoTIFF format.","It also provides consistent and comprehensive metadata to enhance data transparency and reliability.","By aggregating data from multiple sources, SeeFar makes processed and consistent satellite data accessible to a wider range of users - from researchers to policymakers - fostering competition and innovation in satellite imagery analysis.","The dataset is available at \\url{coastalcarbon.ai/seefar}."],"url":"http://arxiv.org/abs/2406.06776v1","category":"cs.CV"}
{"created":"2024-06-10 20:19:55","title":"Evaluating Zero-Shot Long-Context LLM Compression","abstract":"This study evaluates the effectiveness of zero-shot compression techniques on large language models (LLMs) under long-context. We identify the tendency for computational errors to increase under long-context when employing certain compression methods. We propose a hypothesis to explain the varied behavior of different LLM compression techniques and explore remedies to mitigate the performance decline observed in some techniques under long-context. This is a course report for COS 598D Machine Learning and Systems by Prof. Kai Li at Princeton University. Due to limited computational resources, our experiments were conducted only on LLaMA-2-7B-32K.","sentences":["This study evaluates the effectiveness of zero-shot compression techniques on large language models (LLMs) under long-context.","We identify the tendency for computational errors to increase under long-context when employing certain compression methods.","We propose a hypothesis to explain the varied behavior of different LLM compression techniques and explore remedies to mitigate the performance decline observed in some techniques under long-context.","This is a course report for COS 598D Machine Learning and Systems by Prof. Kai Li at Princeton University.","Due to limited computational resources, our experiments were conducted only on LLaMA-2-7B-32K."],"url":"http://arxiv.org/abs/2406.06773v1","category":"cs.CL"}
{"created":"2024-06-10 20:08:44","title":"DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents","abstract":"Automated scientific discovery promises to accelerate progress across scientific domains. However, developing and evaluating an AI agent's capacity for end-to-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible. In this work we introduce DISCOVERYWORLD, the first virtual environment for developing and benchmarking an agent's ability to perform complete cycles of novel scientific discovery. DISCOVERYWORLD contains a variety of different challenges, covering topics as diverse as radioisotope dating, rocket science, and proteomics, to encourage development of general discovery skills rather than task-specific solutions. DISCOVERYWORLD itself is an inexpensive, simulated, text-based environment (with optional 2D visual overlay). It includes 120 different challenge tasks, spanning eight topics each with three levels of difficulty and several parametric variations. Each task requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. DISCOVERYWORLD further provides three automatic metrics for evaluating performance, based on (a) task completion, (b) task-relevant actions taken, and (c) the discovered explanatory knowledge. We find that strong baseline agents, that perform well in prior published environments, struggle on most DISCOVERYWORLD tasks, suggesting that DISCOVERYWORLD captures some of the novel challenges of discovery, and thus that DISCOVERYWORLD may help accelerate near-term development and assessment of scientific discovery competency in agents. Code available at: www.github.com/allenai/discoveryworld","sentences":["Automated scientific discovery promises to accelerate progress across scientific domains.","However, developing and evaluating an AI agent's capacity for end-to-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible.","In this work we introduce DISCOVERYWORLD, the first virtual environment for developing and benchmarking an agent's ability to perform complete cycles of novel scientific discovery.","DISCOVERYWORLD contains a variety of different challenges, covering topics as diverse as radioisotope dating, rocket science, and proteomics, to encourage development of general discovery skills rather than task-specific solutions.","DISCOVERYWORLD itself is an inexpensive, simulated, text-based environment (with optional 2D visual overlay).","It includes 120 different challenge tasks, spanning eight topics each with three levels of difficulty and several parametric variations.","Each task requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions.","DISCOVERYWORLD further provides three automatic metrics for evaluating performance, based on (a) task completion, (b) task-relevant actions taken, and (c) the discovered explanatory knowledge.","We find that strong baseline agents, that perform well in prior published environments, struggle on most DISCOVERYWORLD tasks, suggesting that DISCOVERYWORLD captures some of the novel challenges of discovery, and thus that DISCOVERYWORLD may help accelerate near-term development and assessment of scientific discovery competency in agents.","Code available at: www.github.com/allenai/discoveryworld"],"url":"http://arxiv.org/abs/2406.06769v1","category":"cs.AI"}
{"created":"2024-06-10 19:29:10","title":"Complexity-Aware Deep Symbolic Regression with Robust Risk-Seeking Policy Gradients","abstract":"This paper proposes a novel deep symbolic regression approach to enhance the robustness and interpretability of data-driven mathematical expression discovery. Despite the success of the state-of-the-art method, DSR, it is built on recurrent neural networks, purely guided by data fitness, and potentially meet tail barriers, which can zero out the policy gradient and cause inefficient model updates. To overcome these limitations, we use transformers in conjunction with breadth-first-search to improve the learning performance. We use Bayesian information criterion (BIC) as the reward function to explicitly account for the expression complexity and optimize the trade-off between interpretability and data fitness. We propose a modified risk-seeking policy that not only ensures the unbiasness of the gradient, but also removes the tail barriers, thus ensuring effective updates from top performers. Through a series of benchmarks and systematic experiments, we demonstrate the advantages of our approach.","sentences":["This paper proposes a novel deep symbolic regression approach to enhance the robustness and interpretability of data-driven mathematical expression discovery.","Despite the success of the state-of-the-art method, DSR, it is built on recurrent neural networks, purely guided by data fitness, and potentially meet tail barriers, which can zero out the policy gradient and cause inefficient model updates.","To overcome these limitations, we use transformers in conjunction with breadth-first-search to improve the learning performance.","We use Bayesian information criterion (BIC) as the reward function to explicitly account for the expression complexity and optimize the trade-off between interpretability and data fitness.","We propose a modified risk-seeking policy that not only ensures the unbiasness of the gradient, but also removes the tail barriers, thus ensuring effective updates from top performers.","Through a series of benchmarks and systematic experiments, we demonstrate the advantages of our approach."],"url":"http://arxiv.org/abs/2406.06751v1","category":"cs.LG"}
{"created":"2024-06-10 19:25:19","title":"Federated Nonparametric Hypothesis Testing with Differential Privacy Constraints: Optimal Rates and Adaptive Tests","abstract":"Federated learning has attracted significant recent attention due to its applicability across a wide range of settings where data is collected and analyzed across disparate locations. In this paper, we study federated nonparametric goodness-of-fit testing in the white-noise-with-drift model under distributed differential privacy (DP) constraints.   We first establish matching lower and upper bounds, up to a logarithmic factor, on the minimax separation rate. This optimal rate serves as a benchmark for the difficulty of the testing problem, factoring in model characteristics such as the number of observations, noise level, and regularity of the signal class, along with the strictness of the $(\\epsilon,\\delta)$-DP requirement. The results demonstrate interesting and novel phase transition phenomena. Furthermore, the results reveal an interesting phenomenon that distributed one-shot protocols with access to shared randomness outperform those without access to shared randomness. We also construct a data-driven testing procedure that possesses the ability to adapt to an unknown regularity parameter over a large collection of function classes with minimal additional cost, all while maintaining adherence to the same set of DP constraints.","sentences":["Federated learning has attracted significant recent attention due to its applicability across a wide range of settings where data is collected and analyzed across disparate locations.","In this paper, we study federated nonparametric goodness-of-fit testing in the white-noise-with-drift model under distributed differential privacy (DP) constraints.   ","We first establish matching lower and upper bounds, up to a logarithmic factor, on the minimax separation rate.","This optimal rate serves as a benchmark for the difficulty of the testing problem, factoring in model characteristics such as the number of observations, noise level, and regularity of the signal class, along with the strictness of the $(\\epsilon,\\delta)$-DP requirement.","The results demonstrate interesting and novel phase transition phenomena.","Furthermore, the results reveal an interesting phenomenon that distributed one-shot protocols with access to shared randomness outperform those without access to shared randomness.","We also construct a data-driven testing procedure that possesses the ability to adapt to an unknown regularity parameter over a large collection of function classes with minimal additional cost, all while maintaining adherence to the same set of DP constraints."],"url":"http://arxiv.org/abs/2406.06749v1","category":"math.ST"}
{"created":"2024-06-10 19:04:39","title":"An Elliptic Kernel Unsupervised Autoencoder-Graph Convolutional Network Ensemble Model for Hyperspectral Unmixing","abstract":"Spectral Unmixing is an important technique in remote sensing used to analyze hyperspectral images to identify endmembers and estimate abundance maps. Over the past few decades, performance of techniques for endmember extraction and fractional abundance map estimation have significantly improved. This article presents an ensemble model workflow called Autoencoder Graph Ensemble Model (AEGEM) designed to extract endmembers and fractional abundance maps. An elliptical kernel is applied to measure spectral distances, generating the adjacency matrix within the elliptical neighborhood. This information is used to construct an elliptical graph, with centroids as senders and remaining pixels within the geometry as receivers. The next step involves stacking abundance maps, senders, and receivers as inputs to a Graph Convolutional Network, which processes this input to refine abundance maps. Finally, an ensemble decision-making process determines the best abundance maps based on root mean square error metric. The proposed AEGEM is assessed with benchmark datasets such as Samson, Jasper, and Urban, outperforming results obtained by baseline algorithms. For the Samson dataset, AEGEM excels in three abundance maps: water, tree and soil yielding values of 0.081, 0.158, and 0.182, respectively. For the Jasper dataset, results are improved for the tree and water endmembers with values of 0.035 and 0.060 in that order, as well as for the mean average of the spectral angle distance metric 0.109. For the Urban dataset, AEGEM outperforms previous results for the abundance maps of roof and asphalt, achieving values of 0.135 and 0.240, respectively. Additionally, for the endmembers of grass and roof, AEGEM achieves values of 0.063 and 0.094.","sentences":["Spectral Unmixing is an important technique in remote sensing used to analyze hyperspectral images to identify endmembers and estimate abundance maps.","Over the past few decades, performance of techniques for endmember extraction and fractional abundance map estimation have significantly improved.","This article presents an ensemble model workflow called Autoencoder Graph Ensemble Model (AEGEM) designed to extract endmembers and fractional abundance maps.","An elliptical kernel is applied to measure spectral distances, generating the adjacency matrix within the elliptical neighborhood.","This information is used to construct an elliptical graph, with centroids as senders and remaining pixels within the geometry as receivers.","The next step involves stacking abundance maps, senders, and receivers as inputs to a Graph Convolutional Network, which processes this input to refine abundance maps.","Finally, an ensemble decision-making process determines the best abundance maps based on root mean square error metric.","The proposed AEGEM is assessed with benchmark datasets such as Samson, Jasper, and Urban, outperforming results obtained by baseline algorithms.","For the Samson dataset, AEGEM excels in three abundance maps: water, tree and soil yielding values of 0.081, 0.158, and 0.182, respectively.","For the Jasper dataset, results are improved for the tree and water endmembers with values of 0.035 and 0.060 in that order, as well as for the mean average of the spectral angle distance metric 0.109.","For the Urban dataset, AEGEM outperforms previous results for the abundance maps of roof and asphalt, achieving values of 0.135 and 0.240, respectively.","Additionally, for the endmembers of grass and roof, AEGEM achieves values of 0.063 and 0.094."],"url":"http://arxiv.org/abs/2406.06742v1","category":"cs.CV"}
{"created":"2024-06-10 18:57:22","title":"Raccoon: Prompt Extraction Benchmark of LLM-Integrated Applications","abstract":"With the proliferation of LLM-integrated applications such as GPT-s, millions are deployed, offering valuable services through proprietary instruction prompts. These systems, however, are prone to prompt extraction attacks through meticulously designed queries. To help mitigate this problem, we introduce the Raccoon benchmark which comprehensively evaluates a model's susceptibility to prompt extraction attacks. Our novel evaluation method assesses models under both defenseless and defended scenarios, employing a dual approach to evaluate the effectiveness of existing defenses and the resilience of the models. The benchmark encompasses 14 categories of prompt extraction attacks, with additional compounded attacks that closely mimic the strategies of potential attackers, alongside a diverse collection of defense templates. This array is, to our knowledge, the most extensive compilation of prompt theft attacks and defense mechanisms to date. Our findings highlight universal susceptibility to prompt theft in the absence of defenses, with OpenAI models demonstrating notable resilience when protected. This paper aims to establish a more systematic benchmark for assessing LLM robustness against prompt extraction attacks, offering insights into their causes and potential countermeasures. Resources of Raccoon are publicly available at https://github.com/M0gician/RaccoonBench.","sentences":["With the proliferation of LLM-integrated applications such as GPT-s, millions are deployed, offering valuable services through proprietary instruction prompts.","These systems, however, are prone to prompt extraction attacks through meticulously designed queries.","To help mitigate this problem, we introduce the Raccoon benchmark which comprehensively evaluates a model's susceptibility to prompt extraction attacks.","Our novel evaluation method assesses models under both defenseless and defended scenarios, employing a dual approach to evaluate the effectiveness of existing defenses and the resilience of the models.","The benchmark encompasses 14 categories of prompt extraction attacks, with additional compounded attacks that closely mimic the strategies of potential attackers, alongside a diverse collection of defense templates.","This array is, to our knowledge, the most extensive compilation of prompt theft attacks and defense mechanisms to date.","Our findings highlight universal susceptibility to prompt theft in the absence of defenses, with OpenAI models demonstrating notable resilience when protected.","This paper aims to establish a more systematic benchmark for assessing LLM robustness against prompt extraction attacks, offering insights into their causes and potential countermeasures.","Resources of Raccoon are publicly available at https://github.com/M0gician/RaccoonBench."],"url":"http://arxiv.org/abs/2406.06737v1","category":"cs.CR"}
{"created":"2024-06-10 18:57:06","title":"Long-Term Fairness Inquiries and Pursuits in Machine Learning: A Survey of Notions, Methods, and Challenges","abstract":"The widespread integration of Machine Learning systems in daily life, particularly in high-stakes domains, has raised concerns about the fairness implications. While prior works have investigated static fairness measures, recent studies reveal that automated decision-making has long-term implications and that off-the-shelf fairness approaches may not serve the purpose of achieving long-term fairness. Additionally, the existence of feedback loops and the interaction between models and the environment introduces additional complexities that may deviate from the initial fairness goals. In this survey, we review existing literature on long-term fairness from different perspectives and present a taxonomy for long-term fairness studies. We highlight key challenges and consider future research directions, analyzing both current issues and potential further explorations.","sentences":["The widespread integration of Machine Learning systems in daily life, particularly in high-stakes domains, has raised concerns about the fairness implications.","While prior works have investigated static fairness measures, recent studies reveal that automated decision-making has long-term implications and that off-the-shelf fairness approaches may not serve the purpose of achieving long-term fairness.","Additionally, the existence of feedback loops and the interaction between models and the environment introduces additional complexities that may deviate from the initial fairness goals.","In this survey, we review existing literature on long-term fairness from different perspectives and present a taxonomy for long-term fairness studies.","We highlight key challenges and consider future research directions, analyzing both current issues and potential further explorations."],"url":"http://arxiv.org/abs/2406.06736v1","category":"cs.LG"}
{"created":"2024-06-10 18:53:09","title":"The Imaging Database for Epilepsy And Surgery (IDEAS)","abstract":"Magnetic resonance imaging (MRI) is a crucial tool to identify brain abnormalities in a wide range of neurological disorders. In focal epilepsy MRI is used to identify structural cerebral abnormalities. For covert lesions, machine learning and artificial intelligence algorithms may improve lesion detection if abnormalities are not evident on visual inspection. The success of this approach depends on the volume and quality of training data.   Herein, we release an open-source dataset of preprocessed MRI scans from 442 individuals with drug-refractory focal epilepsy who had neurosurgical resections, and detailed demographic information. The MRI scan data includes the preoperative 3D T1 and where available 3D FLAIR, as well as a manually inspected complete surface reconstruction and volumetric parcellations. Demographic information includes age, sex, age of onset of epilepsy, location of surgery, histopathology of resected specimen, occurrence and frequency of focal seizures with and without impairment of awareness, focal to bilateral tonic-clonic seizures, number of anti-seizure medications (ASMs) at time of surgery, and a total of 1764 patient years of post-surgical follow up. Crucially, we also include resection masks delineated from post-surgical imaging.   To demonstrate the veracity of our data, we successfully replicated previous studies showing long-term outcomes of seizure freedom in the range of around 50%. Our imaging data replicates findings of group level atrophy in patients compared to controls. Resection locations in the cohort were predominantly in the temporal and frontal lobes.   We envisage our dataset, shared openly with the community, will catalyse the development and application of computational methods in clinical neurology.","sentences":["Magnetic resonance imaging (MRI) is a crucial tool to identify brain abnormalities in a wide range of neurological disorders.","In focal epilepsy MRI is used to identify structural cerebral abnormalities.","For covert lesions, machine learning and artificial intelligence algorithms may improve lesion detection if abnormalities are not evident on visual inspection.","The success of this approach depends on the volume and quality of training data.   ","Herein, we release an open-source dataset of preprocessed MRI scans from 442 individuals with drug-refractory focal epilepsy who had neurosurgical resections, and detailed demographic information.","The MRI scan data includes the preoperative 3D T1 and where available 3D FLAIR, as well as a manually inspected complete surface reconstruction and volumetric parcellations.","Demographic information includes age, sex, age of onset of epilepsy, location of surgery, histopathology of resected specimen, occurrence and frequency of focal seizures with and without impairment of awareness, focal to bilateral tonic-clonic seizures, number of anti-seizure medications (ASMs) at time of surgery, and a total of 1764 patient years of post-surgical follow up.","Crucially, we also include resection masks delineated from post-surgical imaging.   ","To demonstrate the veracity of our data, we successfully replicated previous studies showing long-term outcomes of seizure freedom in the range of around 50%.","Our imaging data replicates findings of group level atrophy in patients compared to controls.","Resection locations in the cohort were predominantly in the temporal and frontal lobes.   ","We envisage our dataset, shared openly with the community, will catalyse the development and application of computational methods in clinical neurology."],"url":"http://arxiv.org/abs/2406.06731v1","category":"q-bio.NC"}
{"created":"2024-06-10 18:52:37","title":"TRINS: Towards Multimodal Language Models that Can Read","abstract":"Large multimodal language models have shown remarkable proficiency in understanding and editing images. However, a majority of these visually-tuned models struggle to comprehend the textual content embedded in images, primarily due to the limitation of training data. In this work, we introduce TRINS: a Text-Rich image INStruction dataset, with the objective of enhancing the reading ability of the multimodal large language model. TRINS is built upon LAION using hybrid data annotation strategies that include machine-assisted and human-assisted annotation processes. It contains 39,153 text-rich images, captions, and 102,437 questions. Specifically, we show that the number of words per annotation in TRINS is significantly longer than that of related datasets, providing new challenges. Furthermore, we introduce a simple and effective architecture, called a Language-vision Reading Assistant (LaRA), which is good at understanding textual content within images. LaRA outperforms existing state-of-the-art multimodal large language models on the TRINS dataset, as well as other classical benchmarks. Lastly, we conducted a comprehensive evaluation with TRINS on various text-rich image understanding and generation tasks, demonstrating its effectiveness.","sentences":["Large multimodal language models have shown remarkable proficiency in understanding and editing images.","However, a majority of these visually-tuned models struggle to comprehend the textual content embedded in images, primarily due to the limitation of training data.","In this work, we introduce TRINS: a Text-Rich image INStruction dataset, with the objective of enhancing the reading ability of the multimodal large language model.","TRINS is built upon LAION using hybrid data annotation strategies that include machine-assisted and human-assisted annotation processes.","It contains 39,153 text-rich images, captions, and 102,437 questions.","Specifically, we show that the number of words per annotation in TRINS is significantly longer than that of related datasets, providing new challenges.","Furthermore, we introduce a simple and effective architecture, called a Language-vision Reading Assistant (LaRA), which is good at understanding textual content within images.","LaRA outperforms existing state-of-the-art multimodal large language models on the TRINS dataset, as well as other classical benchmarks.","Lastly, we conducted a comprehensive evaluation with TRINS on various text-rich image understanding and generation tasks, demonstrating its effectiveness."],"url":"http://arxiv.org/abs/2406.06730v1","category":"cs.CV"}
{"created":"2024-06-10 18:50:57","title":"Synthetic Query Generation using Large Language Models for Virtual Assistants","abstract":"Virtual Assistants (VAs) are important Information Retrieval platforms that help users accomplish various tasks through spoken commands. The speech recognition system (speech-to-text) uses query priors, trained solely on text, to distinguish between phonetically confusing alternatives. Hence, the generation of synthetic queries that are similar to existing VA usage can greatly improve upon the VA's abilities -- especially for use-cases that do not (yet) occur in paired audio/text data.   In this paper, we provide a preliminary exploration of the use of Large Language Models (LLMs) to generate synthetic queries that are complementary to template-based methods. We investigate whether the methods (a) generate queries that are similar to randomly sampled, representative, and anonymized user queries from a popular VA, and (b) whether the generated queries are specific.   We find that LLMs generate more verbose queries, compared to template-based methods, and reference aspects specific to the entity. The generated queries are similar to VA user queries, and are specific enough to retrieve the relevant entity. We conclude that queries generated by LLMs and templates are complementary.","sentences":["Virtual Assistants (VAs) are important Information Retrieval platforms that help users accomplish various tasks through spoken commands.","The speech recognition system (speech-to-text) uses query priors, trained solely on text, to distinguish between phonetically confusing alternatives.","Hence, the generation of synthetic queries that are similar to existing VA usage can greatly improve upon the VA's abilities -- especially for use-cases that do not (yet) occur in paired audio/text data.   ","In this paper, we provide a preliminary exploration of the use of Large Language Models (LLMs) to generate synthetic queries that are complementary to template-based methods.","We investigate whether the methods (a) generate queries that are similar to randomly sampled, representative, and anonymized user queries from a popular VA, and (b) whether the generated queries are specific.   ","We find that LLMs generate more verbose queries, compared to template-based methods, and reference aspects specific to the entity.","The generated queries are similar to VA user queries, and are specific enough to retrieve the relevant entity.","We conclude that queries generated by LLMs and templates are complementary."],"url":"http://arxiv.org/abs/2406.06729v1","category":"cs.IR"}
{"created":"2024-06-10 18:46:14","title":"AI-Driven Predictive Analytics Approach for Early Prognosis of Chronic Kidney Disease Using Ensemble Learning and Explainable AI","abstract":"Chronic Kidney Disease (CKD) is one of the widespread Chronic diseases with no known ultimo cure and high morbidity. Research demonstrates that progressive Chronic Kidney Disease (CKD) is a heterogeneous disorder that significantly impacts kidney structure and functions, eventually leading to kidney failure. With the progression of time, chronic kidney disease has moved from a life-threatening disease affecting few people to a common disorder of varying severity. The goal of this research is to visualize dominating features, feature scores, and values exhibited for early prognosis and detection of CKD using ensemble learning and explainable AI. For that, an AI-driven predictive analytics approach is proposed to aid clinical practitioners in prescribing lifestyle modifications for individual patients to reduce the rate of progression of this disease. Our dataset is collected on body vitals from individuals with CKD and healthy subjects to develop our proposed AI-driven solution accurately. In this regard, blood and urine test results are provided, and ensemble tree-based machine-learning models are applied to predict unseen cases of CKD. Our research findings are validated after lengthy consultations with nephrologists. Our experiments and interpretation results are compared with existing explainable AI applications in various healthcare domains, including CKD. The comparison shows that our developed AI models, particularly the Random Forest model, have identified more features as significant contributors than XgBoost. Interpretability (I), which measures the ratio of important to masked features, indicates that our XgBoost model achieved a higher score, specifically a Fidelity of 98\\%, in this metric and naturally in the FII index compared to competing models.","sentences":["Chronic Kidney Disease (CKD) is one of the widespread Chronic diseases with no known ultimo cure and high morbidity.","Research demonstrates that progressive Chronic Kidney Disease (CKD) is a heterogeneous disorder that significantly impacts kidney structure and functions, eventually leading to kidney failure.","With the progression of time, chronic kidney disease has moved from a life-threatening disease affecting few people to a common disorder of varying severity.","The goal of this research is to visualize dominating features, feature scores, and values exhibited for early prognosis and detection of CKD using ensemble learning and explainable AI.","For that, an AI-driven predictive analytics approach is proposed to aid clinical practitioners in prescribing lifestyle modifications for individual patients to reduce the rate of progression of this disease.","Our dataset is collected on body vitals from individuals with CKD and healthy subjects to develop our proposed AI-driven solution accurately.","In this regard, blood and urine test results are provided, and ensemble tree-based machine-learning models are applied to predict unseen cases of CKD.","Our research findings are validated after lengthy consultations with nephrologists.","Our experiments and interpretation results are compared with existing explainable AI applications in various healthcare domains, including CKD.","The comparison shows that our developed AI models, particularly the Random Forest model, have identified more features as significant contributors than XgBoost.","Interpretability (I), which measures the ratio of important to masked features, indicates that our XgBoost model achieved a higher score, specifically a Fidelity of 98\\%, in this metric and naturally in the FII index compared to competing models."],"url":"http://arxiv.org/abs/2406.06728v1","category":"cs.LG"}
{"created":"2024-06-10 18:35:35","title":"Stochastic Guidance of Buoyancy Controlled Vehicles under Ice Shelves using Ocean Currents","abstract":"We propose a novel technique for guidance of buoyancy-controlled vehicles in uncertain under-ice ocean flows. In-situ melt rate measurements collected at the grounding zone of Antarctic ice shelves, where the ice shelf meets the underlying bedrock, are essential to constrain models of future sea level rise. Buoyancy-controlled vehicles, which control their vertical position in the water column through internal actuation but have no means of horizontal propulsion, offer an affordable and reliable platform for such in-situ data collection. However, reaching the grounding zone requires vehicles to traverse tens of kilometers under the ice shelf, with approximate position knowledge and no means of communication, in highly variable and uncertain ocean currents. To address this challenge, we propose a partially observable MDP approach that exploits model-based knowledge of the under-ice currents and, critically, of their uncertainty, to synthesize effective guidance policies. The approach uses approximate dynamic programming to model uncertainty in the currents, and QMDP to address localization uncertainty. Numerical experiments show that the policy can deliver up to 88.8% of underwater vehicles to the grounding zone -- a 33% improvement compared to state-of-the-art guidance techniques, and a 262% improvement over uncontrolled drifters. Collectively, these results show that model-based under-ice guidance is a highly promising technique for exploration of under-ice cavities, and has the potential to enable cost-effective and scalable access to these challenging and rarely observed environments.","sentences":["We propose a novel technique for guidance of buoyancy-controlled vehicles in uncertain under-ice ocean flows.","In-situ melt rate measurements collected at the grounding zone of Antarctic ice shelves, where the ice shelf meets the underlying bedrock, are essential to constrain models of future sea level rise.","Buoyancy-controlled vehicles, which control their vertical position in the water column through internal actuation but have no means of horizontal propulsion, offer an affordable and reliable platform for such in-situ data collection.","However, reaching the grounding zone requires vehicles to traverse tens of kilometers under the ice shelf, with approximate position knowledge and no means of communication, in highly variable and uncertain ocean currents.","To address this challenge, we propose a partially observable MDP approach that exploits model-based knowledge of the under-ice currents and, critically, of their uncertainty, to synthesize effective guidance policies.","The approach uses approximate dynamic programming to model uncertainty in the currents, and QMDP to address localization uncertainty.","Numerical experiments show that the policy can deliver up to 88.8% of underwater vehicles to the grounding zone -- a 33% improvement compared to state-of-the-art guidance techniques, and a 262% improvement over uncontrolled drifters.","Collectively, these results show that model-based under-ice guidance is a highly promising technique for exploration of under-ice cavities, and has the potential to enable cost-effective and scalable access to these challenging and rarely observed environments."],"url":"http://arxiv.org/abs/2406.06724v1","category":"cs.RO"}
{"created":"2024-06-10 18:23:03","title":"Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation","abstract":"Adaptive brain stimulation can treat neurological conditions such as Parkinson's disease and post-stroke motor deficits by influencing abnormal neural activity. Because of patient heterogeneity, each patient requires a unique stimulation policy to achieve optimal neural responses. Model-free reinforcement learning (MFRL) holds promise in learning effective policies for a variety of similar control tasks, but is limited in domains like brain stimulation by a need for numerous costly environment interactions. In this work we introduce Coprocessor Actor Critic, a novel, model-based reinforcement learning (MBRL) approach for learning neural coprocessor policies for brain stimulation. Our key insight is that coprocessor policy learning is a combination of learning how to act optimally in the world and learning how to induce optimal actions in the world through stimulation of an injured brain. We show that our approach overcomes the limitations of traditional MFRL methods in terms of sample efficiency and task success and outperforms baseline MBRL approaches in a neurologically realistic model of an injured brain.","sentences":["Adaptive brain stimulation can treat neurological conditions such as Parkinson's disease and post-stroke motor deficits by influencing abnormal neural activity.","Because of patient heterogeneity, each patient requires a unique stimulation policy to achieve optimal neural responses.","Model-free reinforcement learning (MFRL) holds promise in learning effective policies for a variety of similar control tasks, but is limited in domains like brain stimulation by a need for numerous costly environment interactions.","In this work we introduce Coprocessor Actor Critic, a novel, model-based reinforcement learning (MBRL) approach for learning neural coprocessor policies for brain stimulation.","Our key insight is that coprocessor policy learning is a combination of learning how to act optimally in the world and learning how to induce optimal actions in the world through stimulation of an injured brain.","We show that our approach overcomes the limitations of traditional MFRL methods in terms of sample efficiency and task success and outperforms baseline MBRL approaches in a neurologically realistic model of an injured brain."],"url":"http://arxiv.org/abs/2406.06714v1","category":"cs.LG"}
{"created":"2024-06-10 18:16:28","title":"Theory of neutrino fast flavor evolution. I. Linear response theory and stability conditions","abstract":"Neutrino-neutrino refraction leads to collective flavor evolution that can include fast flavor conversion, an ingredient still missing in numerical simulations of core-collapse supernovae. We provide a theoretical framework for the linear regime of this phenomenon using the language of response theory. In analogy to electromagnetic waves, we introduce a flavor susceptibility as the linear response to an external flavor field. By requiring self-consistency, this approach leads to the usual dispersion relation for growing modes, but differs from the traditional treatment in that it predicts Landau damping of subluminal collective modes. The new dispersion relation has definite analyticity properties and can be expanded for small growth rates. This approach simplifies and intuitively explains Morinaga's proof of sufficiency for the occurrence of growing modes. We show that weakly growing modes arise as soon as an angular crossing is formed, due to their resonant interaction with individual neutrino modes. For longitudinal plasma waves, a similar resonance causes Landau damping or conversely, the two-stream instability.","sentences":["Neutrino-neutrino refraction leads to collective flavor evolution that can include fast flavor conversion, an ingredient still missing in numerical simulations of core-collapse supernovae.","We provide a theoretical framework for the linear regime of this phenomenon using the language of response theory.","In analogy to electromagnetic waves, we introduce a flavor susceptibility as the linear response to an external flavor field.","By requiring self-consistency, this approach leads to the usual dispersion relation for growing modes, but differs from the traditional treatment in that it predicts Landau damping of subluminal collective modes.","The new dispersion relation has definite analyticity properties and can be expanded for small growth rates.","This approach simplifies and intuitively explains Morinaga's proof of sufficiency for the occurrence of growing modes.","We show that weakly growing modes arise as soon as an angular crossing is formed, due to their resonant interaction with individual neutrino modes.","For longitudinal plasma waves, a similar resonance causes Landau damping or conversely, the two-stream instability."],"url":"http://arxiv.org/abs/2406.06708v1","category":"hep-ph"}
{"created":"2024-06-10 18:02:48","title":"Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics","abstract":"Despite attaining high empirical generalization, the sharpness of models trained with sharpness-aware minimization (SAM) do not always correlate with generalization error. Instead of viewing SAM as minimizing sharpness to improve generalization, our paper considers a new perspective based on SAM's training dynamics. We propose that perturbations in SAM perform perturbed forgetting, where they discard undesirable model biases to exhibit learning signals that generalize better. We relate our notion of forgetting to the information bottleneck principle, use it to explain observations like the better generalization of smaller perturbation batches, and show that perturbed forgetting can exhibit a stronger correlation with generalization than flatness. While standard SAM targets model biases exposed by the steepest ascent directions, we propose a new perturbation that targets biases exposed through the model's outputs. Our output bias forgetting perturbations outperform standard SAM, GSAM, and ASAM on ImageNet, robustness benchmarks, and transfer to CIFAR-{10,100}, while sometimes converging to sharper regions. Our results suggest that the benefits of SAM can be explained by alternative mechanistic principles that do not require flatness of the loss surface.","sentences":["Despite attaining high empirical generalization, the sharpness of models trained with sharpness-aware minimization (SAM) do not always correlate with generalization error.","Instead of viewing SAM as minimizing sharpness to improve generalization, our paper considers a new perspective based on SAM's training dynamics.","We propose that perturbations in SAM perform perturbed forgetting, where they discard undesirable model biases to exhibit learning signals that generalize better.","We relate our notion of forgetting to the information bottleneck principle, use it to explain observations like the better generalization of smaller perturbation batches, and show that perturbed forgetting can exhibit a stronger correlation with generalization than flatness.","While standard SAM targets model biases exposed by the steepest ascent directions, we propose a new perturbation that targets biases exposed through the model's outputs.","Our output bias forgetting perturbations outperform standard SAM, GSAM, and ASAM on ImageNet, robustness benchmarks, and transfer to CIFAR-{10,100}, while sometimes converging to sharper regions.","Our results suggest that the benefits of SAM can be explained by alternative mechanistic principles that do not require flatness of the loss surface."],"url":"http://arxiv.org/abs/2406.06700v1","category":"cs.LG"}
{"created":"2024-06-11 17:57:49","title":"Towards Fundamentally Scalable Model Selection: Asymptotically Fast Update and Selection","abstract":"The advancement of deep learning technologies is bringing new models every day, motivating the study of scalable model selection. An ideal model selection scheme should minimally support two operations efficiently over a large pool of candidate models: update, which involves either adding a new candidate model or removing an existing candidate model, and selection, which involves locating highly performing models for a given task. However, previous solutions to model selection require high computational complexity for at least one of these two operations. In this work, we target fundamentally (more) scalable model selection that supports asymptotically fast update and asymptotically fast selection at the same time. Firstly, we define isolated model embedding, a family of model selection schemes supporting asymptotically fast update and selection: With respect to the number of candidate models $m$, the update complexity is O(1) and the selection consists of a single sweep over $m$ vectors in addition to O(1) model operations. Isolated model embedding also implies several desirable properties for applications. Secondly, we present Standardized Embedder, an empirical realization of isolated model embedding. We assess its effectiveness by using it to select representations from a pool of 100 pre-trained vision models for classification tasks and measuring the performance gaps between the selected models and the best candidates with a linear probing protocol. Experiments suggest our realization is effective in selecting models with competitive performances and highlight isolated model embedding as a promising direction towards model selection that is fundamentally (more) scalable.","sentences":["The advancement of deep learning technologies is bringing new models every day, motivating the study of scalable model selection.","An ideal model selection scheme should minimally support two operations efficiently over a large pool of candidate models: update, which involves either adding a new candidate model or removing an existing candidate model, and selection, which involves locating highly performing models for a given task.","However, previous solutions to model selection require high computational complexity for at least one of these two operations.","In this work, we target fundamentally (more) scalable model selection that supports asymptotically fast update and asymptotically fast selection at the same time.","Firstly, we define isolated model embedding, a family of model selection schemes supporting asymptotically fast update and selection: With respect to the number of candidate models $m$, the update complexity is O(1) and the selection consists of a single sweep over $m$ vectors in addition to O(1) model operations.","Isolated model embedding also implies several desirable properties for applications.","Secondly, we present Standardized Embedder, an empirical realization of isolated model embedding.","We assess its effectiveness by using it to select representations from a pool of 100 pre-trained vision models for classification tasks and measuring the performance gaps between the selected models and the best candidates with a linear probing protocol.","Experiments suggest our realization is effective in selecting models with competitive performances and highlight isolated model embedding as a promising direction towards model selection that is fundamentally (more) scalable."],"url":"http://arxiv.org/abs/2406.07536v1","category":"cs.LG"}
{"created":"2024-06-11 17:55:03","title":"QuickLLaMA: Query-aware Inference Acceleration for Large Language Models","abstract":"The capacity of Large Language Models (LLMs) to comprehend and reason over long contexts is pivotal for advancements in diverse fields. Yet, they still stuggle with capturing long-distance dependencies within sequences to deeply understand semantics. To address this issue, we introduce Query-aware Inference for LLMs (Q-LLM), a system designed to process extensive sequences akin to human cognition. By focusing on memory data relevant to a given query, Q-LLM can accurately capture pertinent information within a fixed window size and provide precise answers to queries. It doesn't require extra training and can be seamlessly integrated with any LLMs. Q-LLM using LLaMA3 (QuickLLaMA) can read Harry Potter within 30s and accurately answer the questions. Q-LLM improved by 7.17% compared to the current state-of-the-art on LLaMA3, and by 3.26% on Mistral on the $\\infty$-bench. In the Needle-in-a-Haystack task, On widely recognized benchmarks, Q-LLM improved upon the current SOTA by 7.0% on Mistral and achieves 100% on LLaMA3. Our code can be found in https://github.com/dvlab-research/Q-LLM.","sentences":["The capacity of Large Language Models (LLMs) to comprehend and reason over long contexts is pivotal for advancements in diverse fields.","Yet, they still stuggle with capturing long-distance dependencies within sequences to deeply understand semantics.","To address this issue, we introduce Query-aware Inference for LLMs (Q-LLM), a system designed to process extensive sequences akin to human cognition.","By focusing on memory data relevant to a given query, Q-LLM can accurately capture pertinent information within a fixed window size and provide precise answers to queries.","It doesn't require extra training and can be seamlessly integrated with any LLMs.","Q-LLM using LLaMA3 (QuickLLaMA) can read Harry Potter within 30s and accurately answer the questions.","Q-LLM improved by 7.17% compared to the current state-of-the-art on LLaMA3, and by 3.26% on Mistral on the $\\infty$-bench.","In the Needle-in-a-Haystack task, On widely recognized benchmarks, Q-LLM improved upon the current SOTA by 7.0% on Mistral and achieves 100% on LLaMA3.","Our code can be found in https://github.com/dvlab-research/Q-LLM."],"url":"http://arxiv.org/abs/2406.07528v1","category":"cs.LG"}
{"created":"2024-06-11 17:50:20","title":"Faster Spectral Density Estimation and Sparsification in the Nuclear Norm","abstract":"We consider the problem of estimating the spectral density of the normalized adjacency matrix of an $n$-node undirected graph. We provide a randomized algorithm that, with $O(n\\epsilon^{-2})$ queries to a degree and neighbor oracle and in $O(n\\epsilon^{-3})$ time, estimates the spectrum up to $\\epsilon$ accuracy in the Wasserstein-1 metric. This improves on previous state-of-the-art methods, including an $O(n\\epsilon^{-7})$ time algorithm from [Braverman et al., STOC 2022] and, for sufficiently small $\\epsilon$, a $2^{O(\\epsilon^{-1})}$ time method from [Cohen-Steiner et al., KDD 2018]. To achieve this result, we introduce a new notion of graph sparsification, which we call nuclear sparsification. We provide an $O(n\\epsilon^{-2})$-query and $O(n\\epsilon^{-2})$-time algorithm for computing $O(n\\epsilon^{-2})$-sparse nuclear sparsifiers. We show that this bound is optimal in both its sparsity and query complexity, and we separate our results from the related notion of additive spectral sparsification. Of independent interest, we show that our sparsification method also yields the first deterministic algorithm for spectral density estimation that scales linearly with $n$ (sublinear in the representation size of the graph).","sentences":["We consider the problem of estimating the spectral density of the normalized adjacency matrix of an $n$-node undirected graph.","We provide a randomized algorithm that, with $O(n\\epsilon^{-2})$ queries to a degree and neighbor oracle and in $O(n\\epsilon^{-3})$ time, estimates the spectrum up to $\\epsilon$ accuracy in the Wasserstein-1 metric.","This improves on previous state-of-the-art methods, including an $O(n\\epsilon^{-7})$ time algorithm from [Braverman et al., STOC 2022] and, for sufficiently small $\\epsilon$, a $2^{O(\\epsilon^{-1})}$ time method from [Cohen-Steiner et al., KDD 2018].","To achieve this result, we introduce a new notion of graph sparsification, which we call nuclear sparsification.","We provide an $O(n\\epsilon^{-2})$-query and $O(n\\epsilon^{-2})$-time algorithm for computing $O(n\\epsilon^{-2})$-sparse nuclear sparsifiers.","We show that this bound is optimal in both its sparsity and query complexity, and we separate our results from the related notion of additive spectral sparsification.","Of independent interest, we show that our sparsification method also yields the first deterministic algorithm for spectral density estimation that scales linearly with $n$ (sublinear in the representation size of the graph)."],"url":"http://arxiv.org/abs/2406.07521v1","category":"cs.DS"}
{"created":"2024-06-11 17:46:12","title":"Scintillation Light in SBND: Simulation, Reconstruction, and Expected Performance of the Photon Detection System","abstract":"SBND is the near detector of the Short-Baseline Neutrino program at Fermilab. Its location near to the Booster Neutrino Beam source and relatively large mass will allow the study of neutrino interactions on argon with unprecedented statistics. This paper describes the expected performance of the SBND photon detection system, using a simulated sample of beam neutrinos and cosmogenic particles. Its design is a dual readout concept combining a system of 120 photomultiplier tubes, used for triggering, with a system of 192 X-ARAPUCA devices, located behind the anode wire planes. Furthermore, covering the cathode plane with highly-reflective panels coated with a wavelength-shifting compound recovers part of the light emitted towards the cathode, where no optical detectors exist. We show how this new design provides a high light yield and a more uniform detection efficiency, an excellent timing resolution and an independent 3D-position reconstruction using only the scintillation light. Finally, the whole reconstruction chain is applied to recover the temporal structure of the beam spill, which is resolved with a resolution on the order of nanoseconds.","sentences":["SBND is the near detector of the Short-Baseline Neutrino program at Fermilab.","Its location near to the Booster Neutrino Beam source and relatively large mass will allow the study of neutrino interactions on argon with unprecedented statistics.","This paper describes the expected performance of the SBND photon detection system, using a simulated sample of beam neutrinos and cosmogenic particles.","Its design is a dual readout concept combining a system of 120 photomultiplier tubes, used for triggering, with a system of 192 X-ARAPUCA devices, located behind the anode wire planes.","Furthermore, covering the cathode plane with highly-reflective panels coated with a wavelength-shifting compound recovers part of the light emitted towards the cathode, where no optical detectors exist.","We show how this new design provides a high light yield and a more uniform detection efficiency, an excellent timing resolution and an independent 3D-position reconstruction using only the scintillation light.","Finally, the whole reconstruction chain is applied to recover the temporal structure of the beam spill, which is resolved with a resolution on the order of nanoseconds."],"url":"http://arxiv.org/abs/2406.07514v1","category":"physics.ins-det"}
{"created":"2024-06-11 17:43:43","title":"Accurate Current Sharing in a DC Microgrid Using Modified Droop Control Algorithm","abstract":"Due to the increasing popularity of DC loads and the potential for higher efficiency, DC microgrids are gaining significant attention. DC microgrids utilize multiple parallel converters to deliver sufficient power to the load. However, a key challenge arises when connecting these converters to a common DC bus: maintaining voltage regulation and accurate current sharing. Unequal cable resistances can cause uneven power sharing and lead to power losses. Conventional droop control methods, which employ a virtual resistor to address this issue, have limitations in achieving good performance across the entire converter operating range. This paper proposes a modified droop control algorithm to address this issue. This method modifies the virtual resistor in a way that ensures power sharing aligns with each converter-rated capacity. The algorithm is simple to implement and uses local measurements to update the droop gain. This paper presents simulation studies and experimental tests to analyze the performance of the proposed method, considering scenarios with equal and unequal converter ratings. The results successfully validate the accuracy and effectiveness of this innovative approach.","sentences":["Due to the increasing popularity of DC loads and the potential for higher efficiency, DC microgrids are gaining significant attention.","DC microgrids utilize multiple parallel converters to deliver sufficient power to the load.","However, a key challenge arises when connecting these converters to a common DC bus: maintaining voltage regulation and accurate current sharing.","Unequal cable resistances can cause uneven power sharing and lead to power losses.","Conventional droop control methods, which employ a virtual resistor to address this issue, have limitations in achieving good performance across the entire converter operating range.","This paper proposes a modified droop control algorithm to address this issue.","This method modifies the virtual resistor in a way that ensures power sharing aligns with each converter-rated capacity.","The algorithm is simple to implement and uses local measurements to update the droop gain.","This paper presents simulation studies and experimental tests to analyze the performance of the proposed method, considering scenarios with equal and unequal converter ratings.","The results successfully validate the accuracy and effectiveness of this innovative approach."],"url":"http://arxiv.org/abs/2406.07513v1","category":"eess.SY"}
{"created":"2024-06-11 17:39:11","title":"Hybrid Machine Learning Approach for Cyberattack Mitigation of Parallel Converters in a DC Microgrid","abstract":"Cyberattack susceptibilities are introduced as the communication requirement increases with the incorporation of more renewable energy sources into DC microgrids. Parallel DC-DC converters are utilized to provide high current and supply the load. Nevertheless, these systems are susceptible to cyberattacks that have the potential to disrupt operations and jeopardize stability. Voltage instability may result from the manipulation of communication commands and low-layer control signals. Therefore, in this paper, a cyberattack that specifically targets parallel DC-DC converters is examined in a DC microgrid. A hybrid machine learning-based detection and mitigation strategy is suggested as a means to counteract this threat. The false data injection (FDI) attack targeting the converters is investigated within a DC microgrid. The efficacy of the suggested approach is verified via simulations executed for various scenarios within the MATLAB/Simulink environment. The technique successfully identifies and blocks FDI attacks, preventing cyberattacks and ensuring the safe operation of the DC microgrid.","sentences":["Cyberattack susceptibilities are introduced as the communication requirement increases with the incorporation of more renewable energy sources into DC microgrids.","Parallel DC-DC converters are utilized to provide high current and supply the load.","Nevertheless, these systems are susceptible to cyberattacks that have the potential to disrupt operations and jeopardize stability.","Voltage instability may result from the manipulation of communication commands and low-layer control signals.","Therefore, in this paper, a cyberattack that specifically targets parallel DC-DC converters is examined in a DC microgrid.","A hybrid machine learning-based detection and mitigation strategy is suggested as a means to counteract this threat.","The false data injection (FDI) attack targeting the converters is investigated within a DC microgrid.","The efficacy of the suggested approach is verified via simulations executed for various scenarios within the MATLAB/Simulink environment.","The technique successfully identifies and blocks FDI attacks, preventing cyberattacks and ensuring the safe operation of the DC microgrid."],"url":"http://arxiv.org/abs/2406.07503v1","category":"eess.SY"}
{"created":"2024-06-11 17:33:10","title":"RaD-Net 2: A causal two-stage repairing and denoising speech enhancement network with knowledge distillation and complex axial self-attention","abstract":"In real-time speech communication systems, speech signals are often degraded by multiple distortions. Recently, a two-stage Repair-and-Denoising network (RaD-Net) was proposed with superior speech quality improvement in the ICASSP 2024 Speech Signal Improvement (SSI) Challenge. However, failure to use future information and constraint receptive field of convolution layers limit the system's performance. To mitigate these problems, we extend RaD-Net to its upgraded version, RaD-Net 2. Specifically, a causality-based knowledge distillation is introduced in the first stage to use future information in a causal way. We use the non-causal repairing network as the teacher to improve the performance of the causal repairing network. In addition, in the second stage, complex axial self-attention is applied in the denoising network's complex feature encoder/decoder. Experimental results on the ICASSP 2024 SSI Challenge blind test set show that RaD-Net 2 brings 0.10 OVRL DNSMOS improvement compared to RaD-Net.","sentences":["In real-time speech communication systems, speech signals are often degraded by multiple distortions.","Recently, a two-stage Repair-and-Denoising network (RaD-Net) was proposed with superior speech quality improvement in the ICASSP 2024 Speech Signal Improvement (SSI) Challenge.","However, failure to use future information and constraint receptive field of convolution layers limit the system's performance.","To mitigate these problems, we extend RaD-Net to its upgraded version, RaD-Net 2.","Specifically, a causality-based knowledge distillation is introduced in the first stage to use future information in a causal way.","We use the non-causal repairing network as the teacher to improve the performance of the causal repairing network.","In addition, in the second stage, complex axial self-attention is applied in the denoising network's complex feature encoder/decoder.","Experimental results on the ICASSP 2024 SSI Challenge blind test set show that RaD-Net 2 brings 0.10 OVRL DNSMOS improvement compared to RaD-Net."],"url":"http://arxiv.org/abs/2406.07498v1","category":"cs.SD"}
{"created":"2024-06-11 17:30:52","title":"Some real Rel trajectories in $\\mathcal{H}(1,1)$ that are not recurrent","abstract":"We study the real Rel orbits of some translation surfaces in the stratum $\\mathcal{H}(1,1)$. Specifically, surfaces that are tremors of the locus of branched double covers of tori. We give necessary and sufficient conditions on tremors of a surface so that the real Rel orbit is recurrent. As a consequence, we are able to provide explicit examples of trajectories of real Rel that are not recurrent.","sentences":["We study the real Rel orbits of some translation surfaces in the stratum $\\mathcal{H}(1,1)$. Specifically, surfaces that are tremors of the locus of branched double covers of tori.","We give necessary and sufficient conditions on tremors of a surface so that the real Rel orbit is recurrent.","As a consequence, we are able to provide explicit examples of trajectories of real Rel that are not recurrent."],"url":"http://arxiv.org/abs/2406.07495v1","category":"math.DS"}
{"created":"2024-06-11 17:27:11","title":"Novel Optimized Designs of Modulo $2n+1$ Adder for Quantum Computing","abstract":"Quantum modular adders are one of the most fundamental yet versatile quantum computation operations. They help implement functions of higher complexity, such as subtraction and multiplication, which are used in applications such as quantum cryptanalysis, quantum image processing, and securing communication. To the best of our knowledge, there is no existing design of quantum modulo $(2n+1)$ adder. In this work, we propose four quantum adders targeted specifically for modulo $(2n+1)$ addition. These adders can provide both regular and modulo $(2n+1)$ sum concurrently, enhancing their application in residue number system based arithmetic. Our first design, QMA1, is a novel quantum modulo $(2n+1)$ adder. The second proposed adder, QMA2, optimizes the utilization of quantum gates within the QMA1, resulting in 37.5% reduced CNOT gate count, 46.15% reduced CNOT depth, and 26.5% decrease in both Toffoli gates and depth. We propose a third adder QMA3 that uses zero resets, a dynamic circuits based feature that reuses qubits, leading to 25% savings in qubit count. Our fourth design, QMA4, demonstrates the benefit of incorporating additional zero resets to achieve a purer zero state, reducing quantum state preparation errors. Notably, we conducted experiments using 5-qubit configurations of the proposed modulo $(2n+1)$ adders on the IBM Washington, a 127-qubit quantum computer based on the Eagle R1 architecture, to demonstrate a 28.8% reduction in QMA1's error of which: (i) 18.63% error reduction happens due to gate and depth reduction in QMA2, and (ii) 2.53% drop in error due to qubit reduction in QMA3, and (iii) 7.64% error decreased due to application of additional zero resets in QMA4.","sentences":["Quantum modular adders are one of the most fundamental yet versatile quantum computation operations.","They help implement functions of higher complexity, such as subtraction and multiplication, which are used in applications such as quantum cryptanalysis, quantum image processing, and securing communication.","To the best of our knowledge, there is no existing design of quantum modulo $(2n+1)$ adder.","In this work, we propose four quantum adders targeted specifically for modulo $(2n+1)$ addition.","These adders can provide both regular and modulo $(2n+1)$ sum concurrently, enhancing their application in residue number system based arithmetic.","Our first design, QMA1, is a novel quantum modulo $(2n+1)$ adder.","The second proposed adder, QMA2, optimizes the utilization of quantum gates within the QMA1, resulting in 37.5% reduced CNOT gate count, 46.15% reduced CNOT depth, and 26.5% decrease in both Toffoli gates and depth.","We propose a third adder QMA3 that uses zero resets, a dynamic circuits based feature that reuses qubits, leading to 25% savings in qubit count.","Our fourth design, QMA4, demonstrates the benefit of incorporating additional zero resets to achieve a purer zero state, reducing quantum state preparation errors.","Notably, we conducted experiments using 5-qubit configurations of the proposed modulo $(2n+1)$ adders on the IBM Washington, a 127-qubit quantum computer based on the Eagle R1 architecture, to demonstrate a 28.8% reduction in QMA1's error of which: (i) 18.63% error reduction happens due to gate and depth reduction in QMA2, and (ii) 2.53% drop in error due to qubit reduction in QMA3, and (iii) 7.64% error decreased due to application of additional zero resets in QMA4."],"url":"http://arxiv.org/abs/2406.07486v1","category":"quant-ph"}
{"created":"2024-06-11 17:24:02","title":"Image Neural Field Diffusion Models","abstract":"Diffusion models have shown an impressive ability to model complex data distributions, with several key advantages over GANs, such as stable training, better coverage of the training distribution's modes, and the ability to solve inverse problems without extra training. However, most diffusion models learn the distribution of fixed-resolution images. We propose to learn the distribution of continuous images by training diffusion models on image neural fields, which can be rendered at any resolution, and show its advantages over fixed-resolution models. To achieve this, a key challenge is to obtain a latent space that represents photorealistic image neural fields. We propose a simple and effective method, inspired by several recent techniques but with key changes to make the image neural fields photorealistic. Our method can be used to convert existing latent diffusion autoencoders into image neural field autoencoders. We show that image neural field diffusion models can be trained using mixed-resolution image datasets, outperform fixed-resolution diffusion models followed by super-resolution models, and can solve inverse problems with conditions applied at different scales efficiently.","sentences":["Diffusion models have shown an impressive ability to model complex data distributions, with several key advantages over GANs, such as stable training, better coverage of the training distribution's modes, and the ability to solve inverse problems without extra training.","However, most diffusion models learn the distribution of fixed-resolution images.","We propose to learn the distribution of continuous images by training diffusion models on image neural fields, which can be rendered at any resolution, and show its advantages over fixed-resolution models.","To achieve this, a key challenge is to obtain a latent space that represents photorealistic image neural fields.","We propose a simple and effective method, inspired by several recent techniques but with key changes to make the image neural fields photorealistic.","Our method can be used to convert existing latent diffusion autoencoders into image neural field autoencoders.","We show that image neural field diffusion models can be trained using mixed-resolution image datasets, outperform fixed-resolution diffusion models followed by super-resolution models, and can solve inverse problems with conditions applied at different scales efficiently."],"url":"http://arxiv.org/abs/2406.07480v1","category":"cs.CV"}
{"created":"2024-06-11 17:23:16","title":"Incompressibility and spectral gaps of random circuits","abstract":"Random reversible and quantum circuits form random walks on the alternating group $\\mathrm{Alt}(2^n)$ and unitary group $\\mathrm{SU}(2^n)$, respectively. Known bounds on the spectral gap for the $t$-th moment of these random walks have inverse-polynomial dependence in both $n$ and $t$. We prove that the gap for random reversible circuits is $\\Omega(n^{-3})$ for all $t\\geq 1$, and the gap for random quantum circuits is $\\Omega(n^{-3})$ for $t \\leq \\Theta(2^{n/2})$. These gaps are independent of $t$ in the respective regimes. We can further improve both gaps to $n^{-1}/\\mathrm{polylog}(n, t)$ for $t\\leq 2^{\\Theta(n)}$, which is tight up to polylog factors. Our spectral gap results have a number of consequences:   1) Random reversible circuits with $\\mathcal{O}(n^4 t)$ gates form multiplicative-error $t$-wise independent (even) permutations for all $t\\geq 1$; for $t \\leq \\Theta(2^{n/6.1})$, we show that $\\tilde{\\mathcal{O}}(n^2 t)$ gates suffice.   2) Random quantum circuits with $\\mathcal{O}(n^4 t)$ gates form multiplicative-error unitary $t$-designs for $t \\leq \\Theta(2^{n/2})$; for $t\\leq \\Theta(2^{2n/5})$, we show that $\\tilde{\\mathcal{O}}(n^2t)$ gates suffice.   3) The robust quantum circuit complexity of random circuits grows linearly for an exponentially long time, proving the robust Brown--Susskind conjecture [BS18,BCHJ+21].   Our spectral gap bounds are proven by reducing random quantum circuits to a more structured walk: a modification of the ``$\\mathrm{PFC}$ ensemble'' from [MPSY24] together with an expander on the alternating group due to Kassabov [Kas07a], for which we give an efficient implementation using reversible circuits. In our reduction, we approximate the structured walk with local random circuits without losing the gap, which uses tools from the study of frustration-free Hamiltonians.","sentences":["Random reversible and quantum circuits form random walks on the alternating group $\\mathrm{Alt}(2^n)$ and unitary group $\\mathrm{SU}(2^n)$, respectively.","Known bounds on the spectral gap for the $t$-th moment of these random walks have inverse-polynomial dependence in both $n$ and $t$. We prove that the gap for random reversible circuits is $\\Omega(n^{-3})$ for all $t\\geq 1$, and the gap for random quantum circuits is $\\Omega(n^{-3})$ for $t \\leq \\Theta(2^{n/2})$. These gaps are independent of $t$ in the respective regimes.","We can further improve both gaps to $n^{-1}/\\mathrm{polylog}(n, t)$ for $t\\leq 2^{\\Theta(n)}$, which is tight up to polylog factors.","Our spectral gap results have a number of consequences:   1) Random reversible circuits with $\\mathcal{O}(n^4 t)$ gates form multiplicative-error $t$-wise independent (even) permutations for all $t\\geq 1$; for $t \\leq \\Theta(2^{n/6.1})$, we show that $\\tilde{\\mathcal{O}}(n^2 t)$ gates suffice.   ","2) Random quantum circuits with $\\mathcal{O}(n^4 t)$ gates form multiplicative-error unitary $t$-designs for $t \\leq \\Theta(2^{n/2})$; for $t\\leq \\Theta(2^{2n/5})$, we show that $\\tilde{\\mathcal{O}}(n^2t)$ gates suffice.   ","3)","The robust quantum circuit complexity of random circuits grows linearly for an exponentially long time, proving the robust Brown--Susskind conjecture [BS18,BCHJ+21].   ","Our spectral gap bounds are proven by reducing random quantum circuits to a more structured walk: a modification of the ``$\\mathrm{PFC}$ ensemble'' from [MPSY24] together with an expander on the alternating group due to Kassabov","[Kas07a], for which we give an efficient implementation using reversible circuits.","In our reduction, we approximate the structured walk with local random circuits without losing the gap, which uses tools from the study of frustration-free Hamiltonians."],"url":"http://arxiv.org/abs/2406.07478v1","category":"quant-ph"}
{"created":"2024-06-11 17:20:01","title":"Choreographing the Rhythms of Observation: Dynamics for Ranged Observer Bipartite-Unipartite SpatioTemporal (ROBUST) Networks","abstract":"Existing network analysis methods struggle to optimize observer placements in dynamic environments with limited visibility. This dissertation introduces the novel ROBUST (Ranged Observer Bipartite-Unipartite SpatioTemporal) framework, offering a significant advancement in modeling, analyzing, and optimizing observer networks within complex spatiotemporal domains. ROBUST leverages a unique bipartite-unipartite approach, distinguishing between observer and observable entities while incorporating spatial constraints and temporal dynamics.   This research extends spatiotemporal network theory by introducing novel graph-based measures, including myopic degree, spatial closeness centrality, and edge length proportion. These measures, coupled with advanced clustering techniques like Proximal Recurrence, provide insights into network structure, resilience, and the effectiveness of observer placements. The ROBUST framework demonstrates superior resource allocation and strategic responsiveness compared to conventional models. Case studies in oceanographic monitoring, urban safety networks, and multi-agent path planning showcases its practical applicability and adaptability. Results demonstrate significant improvements in coverage, response times, and overall network efficiency.   This work paves the way for future research in incorporating imperfect knowledge, refining temporal pathing methodologies, and expanding the scope of applications. By bridging theoretical advancements with practical solutions, ROBUST stands as a significant contribution to the field, promising to inform and inspire ongoing and future endeavors in network optimization and multi-agent system planning.","sentences":["Existing network analysis methods struggle to optimize observer placements in dynamic environments with limited visibility.","This dissertation introduces the novel ROBUST (Ranged Observer Bipartite-Unipartite SpatioTemporal) framework, offering a significant advancement in modeling, analyzing, and optimizing observer networks within complex spatiotemporal domains.","ROBUST leverages a unique bipartite-unipartite approach, distinguishing between observer and observable entities while incorporating spatial constraints and temporal dynamics.   ","This research extends spatiotemporal network theory by introducing novel graph-based measures, including myopic degree, spatial closeness centrality, and edge length proportion.","These measures, coupled with advanced clustering techniques like Proximal Recurrence, provide insights into network structure, resilience, and the effectiveness of observer placements.","The ROBUST framework demonstrates superior resource allocation and strategic responsiveness compared to conventional models.","Case studies in oceanographic monitoring, urban safety networks, and multi-agent path planning showcases its practical applicability and adaptability.","Results demonstrate significant improvements in coverage, response times, and overall network efficiency.   ","This work paves the way for future research in incorporating imperfect knowledge, refining temporal pathing methodologies, and expanding the scope of applications.","By bridging theoretical advancements with practical solutions, ROBUST stands as a significant contribution to the field, promising to inform and inspire ongoing and future endeavors in network optimization and multi-agent system planning."],"url":"http://arxiv.org/abs/2406.07473v1","category":"cs.MA"}
{"created":"2024-06-11 17:13:18","title":"Anomaly Detection on Unstable Logs with GPT Models","abstract":"Log-based anomaly detection has been widely studied in the literature as a way to increase the dependability of software-intensive systems. In reality, logs can be unstable due to changes made to the software during its evolution. This, in turn, degrades the performance of downstream log analysis activities, such as anomaly detection. The critical challenge in detecting anomalies on these unstable logs is the lack of information about the new logs, due to insufficient log data from new software versions. The application of Large Language Models (LLMs) to many software engineering tasks has revolutionized various domains. In this paper, we report on an experimental comparison of a fine-tuned LLM and alternative models for anomaly detection on unstable logs. The main motivation is that the pre-training of LLMs on vast datasets may enable a robust understanding of diverse patterns and contextual information, which can be leveraged to mitigate the data insufficiency issue in the context of software evolution. Our experimental results on the two-version dataset of LOGEVOL-Hadoop show that the fine-tuned LLM (GPT-3) fares slightly better than supervised baselines when evaluated on unstable logs. The difference between GPT-3 and other supervised approaches tends to become more significant as the degree of changes in log sequences increases. However, it is unclear whether the difference is practically significant in all cases. Lastly, our comparison of prompt engineering (with GPT-4) and fine-tuning reveals that the latter provides significantly superior performance on both stable and unstable logs, offering valuable insights into the effective utilization of LLMs in this domain.","sentences":["Log-based anomaly detection has been widely studied in the literature as a way to increase the dependability of software-intensive systems.","In reality, logs can be unstable due to changes made to the software during its evolution.","This, in turn, degrades the performance of downstream log analysis activities, such as anomaly detection.","The critical challenge in detecting anomalies on these unstable logs is the lack of information about the new logs, due to insufficient log data from new software versions.","The application of Large Language Models (LLMs) to many software engineering tasks has revolutionized various domains.","In this paper, we report on an experimental comparison of a fine-tuned LLM and alternative models for anomaly detection on unstable logs.","The main motivation is that the pre-training of LLMs on vast datasets may enable a robust understanding of diverse patterns and contextual information, which can be leveraged to mitigate the data insufficiency issue in the context of software evolution.","Our experimental results on the two-version dataset of LOGEVOL-Hadoop show that the fine-tuned LLM (GPT-3) fares slightly better than supervised baselines when evaluated on unstable logs.","The difference between GPT-3 and other supervised approaches tends to become more significant as the degree of changes in log sequences increases.","However, it is unclear whether the difference is practically significant in all cases.","Lastly, our comparison of prompt engineering (with GPT-4) and fine-tuning reveals that the latter provides significantly superior performance on both stable and unstable logs, offering valuable insights into the effective utilization of LLMs in this domain."],"url":"http://arxiv.org/abs/2406.07467v1","category":"cs.SE"}
{"created":"2024-06-11 17:05:34","title":"Hodge structures on conformal blocks","abstract":"We prove existence and uniqueness of complex Hodge structures on modular functors, under the assumptions that their conformal blocks are semisimple as representations of mapping class groups and that the associated categories are ribbon. These $2$ assumptions are satisfied by modular functors coming from Lie algebras, and it is an open conjecture that they are always satisfied. The proof is based on the non-abelian Hodge correspondence and Ocneanu rigidity. Given a modular functor, we explain how its Hodge numbers fit into a Frobenius algebra and the Chern characters of its Hodge decompositions into a new cohomological field theory (CohFT). In the case of $\\mathrm{SU}(2)$ modular functors of level $2$ times an odd number, we give explicit formulas for all Hodge numbers, in any genus $g$.","sentences":["We prove existence and uniqueness of complex Hodge structures on modular functors, under the assumptions that their conformal blocks are semisimple as representations of mapping class groups and that the associated categories are ribbon.","These $2$ assumptions are satisfied by modular functors coming from Lie algebras, and it is an open conjecture that they are always satisfied.","The proof is based on the non-abelian Hodge correspondence and Ocneanu rigidity.","Given a modular functor, we explain how its Hodge numbers fit into a Frobenius algebra and the Chern characters of its Hodge decompositions into a new cohomological field theory (CohFT).","In the case of $\\mathrm{SU}(2)$ modular functors of level $2$ times an odd number, we give explicit formulas for all Hodge numbers, in any genus $g$."],"url":"http://arxiv.org/abs/2406.07459v1","category":"math.AG"}
{"created":"2024-06-11 17:02:37","title":"Resummation of Multi-Stress Tensors in Higher Dimensions","abstract":"In the context of holographic conformal field theories (CFTs), a system of linear partial differential equations was recently proposed to be the higher-dimensional analogs of the null-state equations in $d=2$ CFTs at large central charge. Solving these equations in a near-lightcone expansion yields solutions that match the minimal-twist multi-stress tensor contributions to a heavy-light four-point correlator (or a thermal two-point correlator) computed using holography, the conformal bootstrap, and other methods. This note explores the exact solutions to these equations. We begin by observing that, in an expansion in terms of the ratio between the heavy operator's dimension and the central charge, the $d=2$ correlator involving the level-two degenerate scalars at each order can be represented as a Bessel function; the resummation yields the Virasoro vacuum block. We next observe a relation between the $d=2$ correlator and the $d=4$ near-lightcone correlator involving light scalars with the same conformal dimension. The resummed $d=4$ correlator takes a simple form in the complex frequency domain. Unlike the Virasoro vacuum block, the resummation in $d=4$ leads to essential singularities. Similar expressions are also obtained when the light scalar's dimension takes other finite values. These CFT results correspond to a holographic computation with a spherical black hole. In addition, using the differential equations, we demonstrate that the correlators can be reconstructed via certain modes. In $d=2$, these modes are related to the Virasoro algebra.","sentences":["In the context of holographic conformal field theories (CFTs), a system of linear partial differential equations was recently proposed to be the higher-dimensional analogs of the null-state equations in $d=2$ CFTs at large central charge.","Solving these equations in a near-lightcone expansion yields solutions that match the minimal-twist multi-stress tensor contributions to a heavy-light four-point correlator (or a thermal two-point correlator) computed using holography, the conformal bootstrap, and other methods.","This note explores the exact solutions to these equations.","We begin by observing that, in an expansion in terms of the ratio between the heavy operator's dimension and the central charge, the $d=2$ correlator involving the level-two degenerate scalars at each order can be represented as a Bessel function; the resummation yields the Virasoro vacuum block.","We next observe a relation between the $d=2$ correlator and the $d=4$ near-lightcone correlator involving light scalars with the same conformal dimension.","The resummed $d=4$ correlator takes a simple form in the complex frequency domain.","Unlike the Virasoro vacuum block, the resummation in $d=4$ leads to essential singularities.","Similar expressions are also obtained when the light scalar's dimension takes other finite values.","These CFT results correspond to a holographic computation with a spherical black hole.","In addition, using the differential equations, we demonstrate that the correlators can be reconstructed via certain modes.","In $d=2$, these modes are related to the Virasoro algebra."],"url":"http://arxiv.org/abs/2406.07458v1","category":"hep-th"}
{"created":"2024-06-11 16:59:18","title":"Reserve Provision from Electric Vehicles: Aggregate Boundaries and Stochastic Model Predictive Control","abstract":"Controlled charging of electric vehicles, EVs, is a major potential source of flexibility to facilitate the integration of variable renewable energy and reduce the need for stationary energy storage. To offer system services from EVs, fleet aggregators must address the uncertainty of individual driving and charging behaviour. This paper introduces a means of forecasting the service volume available from EVs by considering several EV batteries as one conceptual battery with aggregate power and energy boundaries. This avoids the impossible task of predicting individual driving behaviour by taking advantage of the law of large numbers. The forecastability of the boundaries is demonstrated in a multiple linear regression model which achieves an $R^2$ of 0.7 for a fleet of 1,000 EVs. A two-stage stochastic model predictive control algorithm is used to schedule reserve services on a day-ahead basis addressing risk trade-offs by including Conditional Value-at-Risk in the objective function. A case study with 1.2 million domestic EV charge records from Great Britain shows that increasing fleet size improves prediction accuracy, thereby increasing reserve revenues and decreasing effective charging costs. For fleet sizes of 400 or above, charging cost reductions plateau at 60\\%, with an average of 1.8kW of reserve provided per vehicle.","sentences":["Controlled charging of electric vehicles, EVs, is a major potential source of flexibility to facilitate the integration of variable renewable energy and reduce the need for stationary energy storage.","To offer system services from EVs, fleet aggregators must address the uncertainty of individual driving and charging behaviour.","This paper introduces a means of forecasting the service volume available from EVs by considering several EV batteries as one conceptual battery with aggregate power and energy boundaries.","This avoids the impossible task of predicting individual driving behaviour by taking advantage of the law of large numbers.","The forecastability of the boundaries is demonstrated in a multiple linear regression model which achieves an $R^2$ of 0.7 for a fleet of 1,000 EVs.","A two-stage stochastic model predictive control algorithm is used to schedule reserve services on a day-ahead basis addressing risk trade-offs by including Conditional Value-at-Risk in the objective function.","A case study with 1.2 million domestic EV charge records from Great Britain shows that increasing fleet size improves prediction accuracy, thereby increasing reserve revenues and decreasing effective charging costs.","For fleet sizes of 400 or above, charging cost reductions plateau at 60\\%, with an average of 1.8kW of reserve provided per vehicle."],"url":"http://arxiv.org/abs/2406.07454v1","category":"eess.SY"}
{"created":"2024-06-11 16:58:00","title":"HTVM: Efficient Neural Network Deployment On Heterogeneous TinyML Platforms","abstract":"Optimal deployment of deep neural networks (DNNs) on state-of-the-art Systems-on-Chips (SoCs) is crucial for tiny machine learning (TinyML) at the edge. The complexity of these SoCs makes deployment non-trivial, as they typically contain multiple heterogeneous compute cores with limited, programmer-managed memory to optimize latency and energy efficiency. We propose HTVM - a compiler that merges TVM with DORY to maximize the utilization of heterogeneous accelerators and minimize data movements. HTVM allows deploying the MLPerf(TM) Tiny suite on DIANA, an SoC with a RISC-V CPU, and digital and analog compute-in-memory AI accelerators, at 120x improved performance over plain TVM deployment.","sentences":["Optimal deployment of deep neural networks (DNNs) on state-of-the-art Systems-on-Chips (SoCs) is crucial for tiny machine learning (TinyML) at the edge.","The complexity of these SoCs makes deployment non-trivial, as they typically contain multiple heterogeneous compute cores with limited, programmer-managed memory to optimize latency and energy efficiency.","We propose HTVM - a compiler that merges TVM with DORY to maximize the utilization of heterogeneous accelerators and minimize data movements.","HTVM allows deploying the MLPerf(TM)","Tiny suite on DIANA, an SoC with a RISC-V CPU, and digital and analog compute-in-memory AI accelerators, at 120x improved performance over plain TVM deployment."],"url":"http://arxiv.org/abs/2406.07453v1","category":"cs.PL"}
{"created":"2024-06-11 16:53:52","title":"Accuracy and limitations of the bond polarizability model in modeling of Raman scattering from molecular dynamics simulations","abstract":"Calculation of Raman scattering from molecular dynamics (MD) simulations requires accurate modeling of the evolution of the electronic polarizability of the system along its MD trajectory. For large systems, this necessitates the use of atomistic models to represent the dependence of electronic polarizability on atomic coordinates. The bond polarizability model (BPM) is the simplest such model and has been used for modeling the Raman spectra of molecular systems but has not been applied to solid-state systems. Here, we systematically investigate the accuracy and limitations of the BPM parameterized from density functional theory (DFT) results for a series of simple molecules such as CO2, SO2, H2S, H2O, NH3, and CH4, the more complex CH2O, CH3OH and CH3CH2OH and thiophene molecules and the BaTiO3 and CsPbBr3 perovskite solids. We find that BPM can reliably reproduce the overall features of the Raman spectra such as shifts of peak positions. However, with the exception of highly symmetric systems, the assumption of non-interacting bonds limits the quantitative accuracy of the BPM; this assumption also leads to qualitatively inaccurate polarizability evolution and Raman spectra for systems where large deviations from the ground state structure are present.","sentences":["Calculation of Raman scattering from molecular dynamics (MD) simulations requires accurate modeling of the evolution of the electronic polarizability of the system along its MD trajectory.","For large systems, this necessitates the use of atomistic models to represent the dependence of electronic polarizability on atomic coordinates.","The bond polarizability model (BPM) is the simplest such model and has been used for modeling the Raman spectra of molecular systems but has not been applied to solid-state systems.","Here, we systematically investigate the accuracy and limitations of the BPM parameterized from density functional theory (DFT) results for a series of simple molecules such as CO2, SO2, H2S, H2O, NH3, and CH4, the more complex CH2O, CH3OH and CH3CH2OH and thiophene molecules and the BaTiO3 and CsPbBr3 perovskite solids.","We find that BPM can reliably reproduce the overall features of the Raman spectra such as shifts of peak positions.","However, with the exception of highly symmetric systems, the assumption of non-interacting bonds limits the quantitative accuracy of the BPM; this assumption also leads to qualitatively inaccurate polarizability evolution and Raman spectra for systems where large deviations from the ground state structure are present."],"url":"http://arxiv.org/abs/2406.07448v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-11 16:53:33","title":"Calculating the two-photon exchange contribution to $K_L\\rightarrow\u03bc^+\u03bc^-$ decay","abstract":"We present a theoretical framework within which both the real and imaginary parts of the complex, two-photon exchange amplitude contributing to $K_L\\rightarrow\\mu^+\\mu^-$ decay can be calculated using lattice QCD. The real part of this two-photon amplitude is of approximately the same size as that coming from a second-order weak strangeness-changing neutral-current process. Thus a test of the standard model prediction for this second-order weak process depends on an accurate result of this two-photon amplitude. A limiting factor of our proposed method comes from low-energy three-particle $\\pi\\pi\\gamma$ states. The contribution from these states will be significantly distorted by the finite volume of our calculation -- a distortion for which there is no available correction. However, a simple estimate of the contribution of these three-particle states suggests their contribution to be at most a few percent allowing their neglect in a lattice calculation with a 10% target accuracy.","sentences":["We present a theoretical framework within which both the real and imaginary parts of the complex, two-photon exchange amplitude contributing to $K_L\\rightarrow\\mu^+\\mu^-$ decay can be calculated using lattice QCD.","The real part of this two-photon amplitude is of approximately the same size as that coming from a second-order weak strangeness-changing neutral-current process.","Thus a test of the standard model prediction for this second-order weak process depends on an accurate result of this two-photon amplitude.","A limiting factor of our proposed method comes from low-energy three-particle $\\pi\\pi\\gamma$ states.","The contribution from these states will be significantly distorted by the finite volume of our calculation -- a distortion for which there is no available correction.","However, a simple estimate of the contribution of these three-particle states suggests their contribution to be at most a few percent allowing their neglect in a lattice calculation with a 10% target accuracy."],"url":"http://arxiv.org/abs/2406.07447v1","category":"hep-ph"}
{"created":"2024-06-11 16:51:01","title":"Constructions of Tur\u00e1n systems that are tight up to a multiplicative constant","abstract":"For positive integers $n\\ge s> r$, the Tur\\'an function $T(n,s,r)$ is the smallest size of an r-graph with n vertices such that every set of s vertices contains at least one edge. Also, define the Tur\\'an density $t(s,r)$ as the limit of $T(n,s,r)/ {n\\choose r}$ as $n\\to\\infty$. The question of estimating these parameters received a lot of attention after it was first raised by Tur\\'an in 1941. A trivial lower bound is $t(s,r)\\ge 1/{s\\choose s-r}$. In the early 1990s, de Caen conjectured that $r\\cdot t(r+1,r)\\to\\infty$ as $r\\to\\infty$.   We disprove this conjecture by showing more strongly that for every integer $R\\ge1$ there is $\\mu_R$ (in fact, $\\mu_R$ can be taken to grow as $(1+o(1))\\, R\\ln R$) such that   $t(r+R,r)\\le (\\mu_R+o(1))/ {r+R\\choose R}$ as $r\\to\\infty$, that is, the trivial lower bound is tight for every $R$ up to a multiplicative constant $\\mu_R$.","sentences":["For positive integers $n\\ge s> r$, the Tur\\'an function $T(n,s,r)$ is the smallest size of an r-graph with n vertices such that every set of s vertices contains at least one edge.","Also, define the Tur\\'an density $t(s,r)$ as the limit of $T(n,s,r)/ {n\\choose r}$ as $n\\to\\infty$. The question of estimating these parameters received a lot of attention after it was first raised by Tur\\'an in 1941.","A trivial lower bound is $t(s,r)\\ge 1/{s\\choose s-r}$.","In the early 1990s, de Caen conjectured that $r\\cdot t(r+1,r)\\to\\infty$ as $r\\to\\infty$.   We disprove this conjecture by showing more strongly that for every integer $R\\ge1$ there is $\\mu_R$ (in fact, $\\mu_R$ can be taken to grow as $(1+o(1))\\, R\\ln R$) such that   $t(r+R,r)\\le (\\mu_R+o(1))/ {r+R\\choose R}$ as $r\\to\\infty$, that is, the trivial lower bound is tight for every $R$ up to a multiplicative constant $\\mu_R$."],"url":"http://arxiv.org/abs/2406.07443v1","category":"math.CO"}
{"created":"2024-06-11 16:34:21","title":"Matryoshka Representation Learning for Recommendation","abstract":"Representation learning is essential for deep-neural-network-based recommender systems to capture user preferences and item features within fixed-dimensional user and item vectors. Unlike existing representation learning methods that either treat each user preference and item feature uniformly or categorize them into discrete clusters, we argue that in the real world, user preferences and item features are naturally expressed and organized in a hierarchical manner, leading to a new direction for representation learning. In this paper, we introduce a novel matryoshka representation learning method for recommendation (MRL4Rec), by which we restructure user and item vectors into matryoshka representations with incrementally dimensional and overlapping vector spaces to explicitly represent user preferences and item features at different hierarchical levels. We theoretically establish that constructing training triplets specific to each level is pivotal in guaranteeing accurate matryoshka representation learning. Subsequently, we propose the matryoshka negative sampling mechanism to construct training triplets, which further ensures the effectiveness of the matryoshka representation learning in capturing hierarchical user preferences and item features. The experiments demonstrate that MRL4Rec can consistently and substantially outperform a number of state-of-the-art competitors on several real-life datasets. Our code is publicly available at https://github.com/Riwei-HEU/MRL.","sentences":["Representation learning is essential for deep-neural-network-based recommender systems to capture user preferences and item features within fixed-dimensional user and item vectors.","Unlike existing representation learning methods that either treat each user preference and item feature uniformly or categorize them into discrete clusters, we argue that in the real world, user preferences and item features are naturally expressed and organized in a hierarchical manner, leading to a new direction for representation learning.","In this paper, we introduce a novel matryoshka representation learning method for recommendation (MRL4Rec), by which we restructure user and item vectors into matryoshka representations with incrementally dimensional and overlapping vector spaces to explicitly represent user preferences and item features at different hierarchical levels.","We theoretically establish that constructing training triplets specific to each level is pivotal in guaranteeing accurate matryoshka representation learning.","Subsequently, we propose the matryoshka negative sampling mechanism to construct training triplets, which further ensures the effectiveness of the matryoshka representation learning in capturing hierarchical user preferences and item features.","The experiments demonstrate that MRL4Rec can consistently and substantially outperform a number of state-of-the-art competitors on several real-life datasets.","Our code is publicly available at https://github.com/Riwei-HEU/MRL."],"url":"http://arxiv.org/abs/2406.07432v1","category":"cs.IR"}
{"created":"2024-06-11 16:34:16","title":"Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments","abstract":"We study pursuit-evasion games in highly occluded urban environments, e.g. tall buildings in a city, where a scout (quadrotor) tracks multiple dynamic targets on the ground. We show that we can build a neural radiance field (NeRF) representation of the city -- online -- using RGB and depth images from different vantage points. This representation is used to calculate the information gain to both explore unknown parts of the city and track the targets -- thereby giving a completely first-principles approach to actively tracking dynamic targets. We demonstrate, using a custom-built simulator using Open Street Maps data of Philadelphia and New York City, that we can explore and locate 20 stationary targets within 300 steps. This is slower than a greedy baseline which which does not use active perception. But for dynamic targets that actively hide behind occlusions, we show that our approach maintains, at worst, a tracking error of 200m; the greedy baseline can have a tracking error as large as 600m. We observe a number of interesting properties in the scout's policies, e.g., it switches its attention to track a different target periodically, as the quality of the NeRF representation improves over time, the scout also becomes better in terms of target tracking.","sentences":["We study pursuit-evasion games in highly occluded urban environments, e.g. tall buildings in a city, where a scout (quadrotor) tracks multiple dynamic targets on the ground.","We show that we can build a neural radiance field (NeRF) representation of the city -- online -- using RGB and depth images from different vantage points.","This representation is used to calculate the information gain to both explore unknown parts of the city and track the targets -- thereby giving a completely first-principles approach to actively tracking dynamic targets.","We demonstrate, using a custom-built simulator using Open Street Maps data of Philadelphia and New York City, that we can explore and locate 20 stationary targets within 300 steps.","This is slower than a greedy baseline which which does not use active perception.","But for dynamic targets that actively hide behind occlusions, we show that our approach maintains, at worst, a tracking error of 200m; the greedy baseline can have a tracking error as large as 600m. We observe a number of interesting properties in the scout's policies, e.g., it switches its attention to track a different target periodically, as the quality of the NeRF representation improves over time, the scout also becomes better in terms of target tracking."],"url":"http://arxiv.org/abs/2406.07431v1","category":"cs.MA"}
{"created":"2024-06-11 16:33:56","title":"Making 'syscall' a Privilege not a Right","abstract":"Browsers, Library OSes, and system emulators rely on sandboxes and in-process isolation to emulate system resources and securely isolate untrusted components. All access to system resources like system calls (syscall) need to be securely mediated by the application. Otherwise system calls may allow untrusted components to evade the emulator or sandbox monitor, and hence, escape and attack the entire application or system. Existing approaches, such as ptrace, require additional context switches between kernel and userspace, which introduce high performance overhead. And, seccomp-bpf supports only limited policies, which restricts its functionality, or it still requires ptrace to provide assistance.   In this paper, we present nexpoline, a secure syscall interception mechanism combining Memory Protection Keys (MPK) and Seccomp or Syscall User Dispatch (SUD). Our approach transforms an application's syscall instruction into a privilege reserved for the trusted monitor within the address space, allowing flexible user defined policy. To execute a syscall, the application must switch contexts via nexpoline. It offers better efficiency than secure interception techniques like ptrace, as nexpoline can intercept syscalls through binary rewriting securely. Consequently, nexpoline ensures the safety, flexibility and efficiency for syscall interception. Notably, it operates without kernel modifications, making it viable on current Linux systems without needing root privileges. Our benchmarks demonstrate improved performance over ptrace in interception overhead while achieving the same security guarantees. When compared to similarly performing firejail, nexpoline supports more complex policies and enables the possibility to emulate system resources.","sentences":["Browsers, Library OSes, and system emulators rely on sandboxes and in-process isolation to emulate system resources and securely isolate untrusted components.","All access to system resources like system calls (syscall) need to be securely mediated by the application.","Otherwise system calls may allow untrusted components to evade the emulator or sandbox monitor, and hence, escape and attack the entire application or system.","Existing approaches, such as ptrace, require additional context switches between kernel and userspace, which introduce high performance overhead.","And, seccomp-bpf supports only limited policies, which restricts its functionality, or it still requires ptrace to provide assistance.   ","In this paper, we present nexpoline, a secure syscall interception mechanism combining Memory Protection Keys (MPK) and Seccomp or Syscall User Dispatch (SUD).","Our approach transforms an application's syscall instruction into a privilege reserved for the trusted monitor within the address space, allowing flexible user defined policy.","To execute a syscall, the application must switch contexts via nexpoline.","It offers better efficiency than secure interception techniques like ptrace, as nexpoline can intercept syscalls through binary rewriting securely.","Consequently, nexpoline ensures the safety, flexibility and efficiency for syscall interception.","Notably, it operates without kernel modifications, making it viable on current Linux systems without needing root privileges.","Our benchmarks demonstrate improved performance over ptrace in interception overhead while achieving the same security guarantees.","When compared to similarly performing firejail, nexpoline supports more complex policies and enables the possibility to emulate system resources."],"url":"http://arxiv.org/abs/2406.07429v1","category":"cs.CR"}
{"created":"2024-06-11 16:21:57","title":"Graph Reasoning for Explainable Cold Start Recommendation","abstract":"The cold start problem, where new users or items have no interaction history, remains a critical challenge in recommender systems (RS). A common solution involves using Knowledge Graphs (KG) to train entity embeddings or Graph Neural Networks (GNNs). Since KGs incorporate auxiliary data and not just user/item interactions, these methods can make relevant recommendations for cold users or items. Graph Reasoning (GR) methods, however, find paths from users to items to recommend using relations in the KG and, in the context of RS, have been used for interpretability. In this study, we propose GRECS: a framework for adapting GR to cold start recommendations. By utilizing explicit paths starting for users rather than relying only on entity embeddings, GRECS can find items corresponding to users' preferences by navigating the graph, even when limited information about users is available. Our experiments show that GRECS mitigates the cold start problem and outperforms competitive baselines across 5 standard datasets while being explainable. This study highlights the potential of GR for developing explainable recommender systems better suited for managing cold users and items.","sentences":["The cold start problem, where new users or items have no interaction history, remains a critical challenge in recommender systems (RS).","A common solution involves using Knowledge Graphs (KG) to train entity embeddings or Graph Neural Networks (GNNs).","Since KGs incorporate auxiliary data and not just user/item interactions, these methods can make relevant recommendations for cold users or items.","Graph Reasoning (GR) methods, however, find paths from users to items to recommend using relations in the KG and, in the context of RS, have been used for interpretability.","In this study, we propose GRECS: a framework for adapting GR to cold start recommendations.","By utilizing explicit paths starting for users rather than relying only on entity embeddings, GRECS can find items corresponding to users' preferences by navigating the graph, even when limited information about users is available.","Our experiments show that GRECS mitigates the cold start problem and outperforms competitive baselines across 5 standard datasets while being explainable.","This study highlights the potential of GR for developing explainable recommender systems better suited for managing cold users and items."],"url":"http://arxiv.org/abs/2406.07420v1","category":"cs.IR"}
{"created":"2024-06-11 16:21:13","title":"Average-exact mixed anomalies and compatible phases","abstract":"The quantum anomaly of a global symmetry is known to strongly constrain the allowed low-energy physics in a clean and isolated quantum system. However, the effect of quantum anomalies in disordered systems is much less understood, especially when the global symmetry is only preserved on average by the disorder. In this work, we focus on disordered systems with both average and exact symmetries $A\\times K$, where the exact symmetry $K$ is respected in every disorder configuration, and the average $A$ is only preserved on average by the disorder ensemble. When there is a mixed quantum anomaly between the average and exact symmetries, we argue that the mixed state representing the ensemble of disordered ground states cannot be featureless. While disordered mixed states smoothly connected to the anomaly-compatible phases in clean limit are certainly allowed, we also found disordered phases that have no clean-limit counterparts, including the glassy states with strong-to-weak symmetry breaking, and average topological orders for certain anomalies. We construct solvable lattice models to demonstrate each of these possibilities. We also provide a field-theoretic argument to provide a criterion for whether a given average-exact mixed anomaly admits a compatible average topological order.","sentences":["The quantum anomaly of a global symmetry is known to strongly constrain the allowed low-energy physics in a clean and isolated quantum system.","However, the effect of quantum anomalies in disordered systems is much less understood, especially when the global symmetry is only preserved on average by the disorder.","In this work, we focus on disordered systems with both average and exact symmetries $A\\times K$, where the exact symmetry $K$ is respected in every disorder configuration, and the average $A$ is only preserved on average by the disorder ensemble.","When there is a mixed quantum anomaly between the average and exact symmetries, we argue that the mixed state representing the ensemble of disordered ground states cannot be featureless.","While disordered mixed states smoothly connected to the anomaly-compatible phases in clean limit are certainly allowed, we also found disordered phases that have no clean-limit counterparts, including the glassy states with strong-to-weak symmetry breaking, and average topological orders for certain anomalies.","We construct solvable lattice models to demonstrate each of these possibilities.","We also provide a field-theoretic argument to provide a criterion for whether a given average-exact mixed anomaly admits a compatible average topological order."],"url":"http://arxiv.org/abs/2406.07417v1","category":"cond-mat.str-el"}
{"created":"2024-06-11 16:20:45","title":"Heat operators and isometry groups of Cuntz-Krieger algebras","abstract":"This paper introduces heat semigroups of topological Markov chains and Cuntz-Krieger algebras by means of spectral noncommutative geometry. Using recent advances on the logarithmic Dirichlet Laplacian on Ahlfors regular metric-measure spaces, we construct spectral triples on Cuntz-Krieger algebras from singular integral operators. These spectral triples exhaust K-homology and for Cuntz algebras we can compute their heat operators explicitly as Riesz potential operators. We also describe their isometry group in terms of the automorphism group of the underlying directed graph and prove that the Voiculescu noncommutative topological entropy vanishes on isometries.","sentences":["This paper introduces heat semigroups of topological Markov chains and Cuntz-Krieger algebras by means of spectral noncommutative geometry.","Using recent advances on the logarithmic Dirichlet Laplacian on Ahlfors regular metric-measure spaces, we construct spectral triples on Cuntz-Krieger algebras from singular integral operators.","These spectral triples exhaust K-homology and for Cuntz algebras we can compute their heat operators explicitly as Riesz potential operators.","We also describe their isometry group in terms of the automorphism group of the underlying directed graph and prove that the Voiculescu noncommutative topological entropy vanishes on isometries."],"url":"http://arxiv.org/abs/2406.07416v1","category":"math.OA"}
{"created":"2024-06-11 16:13:09","title":"Private Geometric Median","abstract":"In this paper, we study differentially private (DP) algorithms for computing the geometric median (GM) of a dataset: Given $n$ points, $x_1,\\dots,x_n$ in $\\mathbb{R}^d$, the goal is to find a point $\\theta$ that minimizes the sum of the Euclidean distances to these points, i.e., $\\sum_{i=1}^{n} \\|\\theta - x_i\\|_2$. Off-the-shelf methods, such as DP-GD, require strong a priori knowledge locating the data within a ball of radius $R$, and the excess risk of the algorithm depends linearly on $R$. In this paper, we ask: can we design an efficient and private algorithm with an excess error guarantee that scales with the (unknown) radius containing the majority of the datapoints? Our main contribution is a pair of polynomial-time DP algorithms for the task of private GM with an excess error guarantee that scales with the effective diameter of the datapoints. Additionally, we propose an inefficient algorithm based on the inverse smooth sensitivity mechanism, which satisfies the more restrictive notion of pure DP. We complement our results with a lower bound and demonstrate the optimality of our polynomial-time algorithms in terms of sample complexity.","sentences":["In this paper, we study differentially private (DP) algorithms for computing the geometric median (GM) of a dataset: Given $n$ points, $x_1,\\dots,x_n$ in $\\mathbb{R}^d$, the goal is to find a point $\\theta$ that minimizes the sum of the Euclidean distances to these points, i.e., $\\sum_{i=1}^{n} \\|\\theta - x_i\\|_2$. Off-the-shelf methods, such as DP-GD, require strong a priori knowledge locating the data within a ball of radius $R$, and the excess risk of the algorithm depends linearly on $R$. In this paper, we ask: can we design an efficient and private algorithm with an excess error guarantee that scales with the (unknown) radius containing the majority of the datapoints?","Our main contribution is a pair of polynomial-time DP algorithms for the task of private GM with an excess error guarantee that scales with the effective diameter of the datapoints.","Additionally, we propose an inefficient algorithm based on the inverse smooth sensitivity mechanism, which satisfies the more restrictive notion of pure DP.","We complement our results with a lower bound and demonstrate the optimality of our polynomial-time algorithms in terms of sample complexity."],"url":"http://arxiv.org/abs/2406.07407v1","category":"cs.LG"}
{"created":"2024-06-11 16:10:50","title":"Persistent currents in mesoscopic spin-orbit coupled rings due to an applied Zeeman field","abstract":"Persistent currents (PCs) in mesoscopic rings have been a subject of intense investigation since their proposal by B\\\"uttiker, Landauer, and Imry in 1983. In this paper, we explore the behavior of PC in spin-orbit coupled rings under the influence of a Zeeman field, contrasting it with traditional PC observed in rings threaded by magnetic flux. Our study reveals that the emergence of PC in our setup crucially depends on nonzero values of spin-orbit coupling and the Zeeman field. Through theoretical analysis and numerical calculations, we uncover several intriguing phenomena. Specifically, in ballistic rings, we observe an inverse proportionality between PC and system size, with PC being zero at half filling for even numbers of sites. Additionally, the introduction of on-site disorder leads to the suppression of PC, with exponential decay observed for large disorder strengths and quadratic decay for smaller disorder strengths. Notably, disorder can enhance PC in individual samples, albeit with a configuration-averaged PC of zero. Furthermore, we find that the standard deviation of PC increases with disorder strength, reaching a maximum before decreasing to zero at high disorder strengths. Our findings shed light on the intricate interplay between spin-orbit coupling, Zeeman fields, and disorder in mesoscopic quantum systems, offering new avenues for theoretical exploration and experimental verification.","sentences":["Persistent currents (PCs) in mesoscopic rings have been a subject of intense investigation since their proposal by B\\\"uttiker, Landauer, and Imry in 1983.","In this paper, we explore the behavior of PC in spin-orbit coupled rings under the influence of a Zeeman field, contrasting it with traditional PC observed in rings threaded by magnetic flux.","Our study reveals that the emergence of PC in our setup crucially depends on nonzero values of spin-orbit coupling and the Zeeman field.","Through theoretical analysis and numerical calculations, we uncover several intriguing phenomena.","Specifically, in ballistic rings, we observe an inverse proportionality between PC and system size, with PC being zero at half filling for even numbers of sites.","Additionally, the introduction of on-site disorder leads to the suppression of PC, with exponential decay observed for large disorder strengths and quadratic decay for smaller disorder strengths.","Notably, disorder can enhance PC in individual samples, albeit with a configuration-averaged PC of zero.","Furthermore, we find that the standard deviation of PC increases with disorder strength, reaching a maximum before decreasing to zero at high disorder strengths.","Our findings shed light on the intricate interplay between spin-orbit coupling, Zeeman fields, and disorder in mesoscopic quantum systems, offering new avenues for theoretical exploration and experimental verification."],"url":"http://arxiv.org/abs/2406.07405v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-11 16:08:29","title":"The Tannaka group $E_6$ only arises from cubic threefolds","abstract":"We show that under mild assumptions, the Fano surfaces of lines on smooth cubic threefolds are the only smooth subvarieties of abelian varieties whose Tannaka group for the convolution of perverse sheaves is an exceptional simple group. This in particular leads to a considerable strengthening of our previous work on the Shafarevich conjecture. A key idea is to control the Hodge decomposition on cohomology by a cocharacter of the Tannaka group of Hodge modules, and to play this off against an improvement of the Hodge number estimates for irregular varieties by Lazarsfeld-Popa and Lombardi.","sentences":["We show that under mild assumptions, the Fano surfaces of lines on smooth cubic threefolds are the only smooth subvarieties of abelian varieties whose Tannaka group for the convolution of perverse sheaves is an exceptional simple group.","This in particular leads to a considerable strengthening of our previous work on the Shafarevich conjecture.","A key idea is to control the Hodge decomposition on cohomology by a cocharacter of the Tannaka group of Hodge modules, and to play this off against an improvement of the Hodge number estimates for irregular varieties by Lazarsfeld-Popa and Lombardi."],"url":"http://arxiv.org/abs/2406.07401v1","category":"math.AG"}
{"created":"2024-06-11 15:58:22","title":"Any topological recursion on a rational spectral curve is KP integrable","abstract":"We prove that for any initial data on a genus zero spectral curve the corresponding correlation differentials of topological recursion are KP integrable. As an application we prove KP integrability of partition functions associated via ELSV-type formulas to the $r$-th roots of the twisted powers of the log canonical bundles.","sentences":["We prove that for any initial data on a genus zero spectral curve the corresponding correlation differentials of topological recursion are KP integrable.","As an application we prove KP integrability of partition functions associated via ELSV-type formulas to the $r$-th roots of the twisted powers of the log canonical bundles."],"url":"http://arxiv.org/abs/2406.07391v1","category":"math-ph"}
{"created":"2024-06-11 15:50:35","title":"Fast Adaptive Meta-Heuristic for Large-Scale Facility Location Problem","abstract":"Facility location problems have been a major research area of interest in the last several decades. In particular, uncapacitated location problems (ULP) have enormous applications. Variations of ULP often appear, especially as large-scale subproblems in more complex combinatorial optimization problems. Although many researchers have studied different versions of ULP (e.g., uncapacitated facility location problem (UCFLP) and p-Median problem), most of these authors have considered small to moderately sized problems. In this paper, we address the ULP and provide a fast adaptive meta-heuristic for large-scale problems. The approach is based on critical event memory tabu search. For the diversification component of the algorithm, we have chosen a procedure based on a sequencing problem commonly used for traveling salesman-type problems. The efficacy of this approach is evaluated across a diverse range of benchmark problems sourced from the Internet, with a comprehensive comparison against four prominent algorithms in the literature. The proposed adaptive critical event tabu search (ACETS) demonstrates remarkable effectiveness for large-scale problems. The algorithm successfully solved all problems optimally within a short computing time. Notably, ACETS discovered three best new solutions for benchmark problems, specifically for Asymmetric 500A-1, Asymmetric 750A-1, and Symmetric 750B-4, underscoring its innovative and robust nature.","sentences":["Facility location problems have been a major research area of interest in the last several decades.","In particular, uncapacitated location problems (ULP) have enormous applications.","Variations of ULP often appear, especially as large-scale subproblems in more complex combinatorial optimization problems.","Although many researchers have studied different versions of ULP (e.g., uncapacitated facility location problem (UCFLP) and p-Median problem), most of these authors have considered small to moderately sized problems.","In this paper, we address the ULP and provide a fast adaptive meta-heuristic for large-scale problems.","The approach is based on critical event memory tabu search.","For the diversification component of the algorithm, we have chosen a procedure based on a sequencing problem commonly used for traveling salesman-type problems.","The efficacy of this approach is evaluated across a diverse range of benchmark problems sourced from the Internet, with a comprehensive comparison against four prominent algorithms in the literature.","The proposed adaptive critical event tabu search (ACETS) demonstrates remarkable effectiveness for large-scale problems.","The algorithm successfully solved all problems optimally within a short computing time.","Notably, ACETS discovered three best new solutions for benchmark problems, specifically for Asymmetric 500A-1, Asymmetric 750A-1, and Symmetric 750B-4, underscoring its innovative and robust nature."],"url":"http://arxiv.org/abs/2406.07382v1","category":"math.OC"}
{"created":"2024-06-11 15:43:21","title":"Decoding planetary surfaces by counting cracks","abstract":"Planets are often covered with thin cracked shells. From mud films to lithospheres of rock or ice, fracture networks form two-dimensional (2D) tessellations of convex polygons whose geometry encodes their genesis. Here we chart the geometry of 2D fracture mosaics across the solar system, and decode their formative conditions using a new dynamical crack model. We show that mosaics can be projected onto a Symbolic Ternary Diagram, where the relative proportions of ``T'', ``X'' and ``Y'' junctions are uniquely related to contributions from distinct modes of fracture. Most planetary mosaics cluster in a region associated with hierarchical fracture networks, where sequential cracking favors formation of T junctions. Exceptions to this rule may betray the presence of water. Europa's fracture networks stand apart due to the predominance of X junctions; this is a special feature of ice, where healing of cracks by refreezing of water allows new fractures to overprint older ones. Several fracture networks on Mars appear as outliers due the high proportion of Y junctions. These patterns -- previously interpreted as ancient mudcracks and frozen polar terrain, based on geological evidence -- are consistent with the twisting of crack junctions by cyclic volume change. Our findings suggest that counting cracks could aid in the identification of other water-influenced planetary environments.","sentences":["Planets are often covered with thin cracked shells.","From mud films to lithospheres of rock or ice, fracture networks form two-dimensional (2D) tessellations of convex polygons whose geometry encodes their genesis.","Here we chart the geometry of 2D fracture mosaics across the solar system, and decode their formative conditions using a new dynamical crack model.","We show that mosaics can be projected onto a Symbolic Ternary Diagram, where the relative proportions of ``T'', ``X'' and ``Y'' junctions are uniquely related to contributions from distinct modes of fracture.","Most planetary mosaics cluster in a region associated with hierarchical fracture networks, where sequential cracking favors formation of T junctions.","Exceptions to this rule may betray the presence of water.","Europa's fracture networks stand apart due to the predominance of X junctions; this is a special feature of ice, where healing of cracks by refreezing of water allows new fractures to overprint older ones.","Several fracture networks on Mars appear as outliers due the high proportion of Y junctions.","These patterns -- previously interpreted as ancient mudcracks and frozen polar terrain, based on geological evidence -- are consistent with the twisting of crack junctions by cyclic volume change.","Our findings suggest that counting cracks could aid in the identification of other water-influenced planetary environments."],"url":"http://arxiv.org/abs/2406.07376v1","category":"physics.geo-ph"}
{"created":"2024-06-11 15:41:48","title":"Movable-Antenna Array Empowered ISAC Systems for Low-Altitude Economy","abstract":"This paper investigates a movable-antenna (MA) array empowered integrated sensing and communications (ISAC) over low-altitude platform (LAP) system to support low-altitude economy (LAE) applications. In the considered system, an unmanned aerial vehicle (UAV) is dispatched to hover in the air, working as the UAV-enabled LAP (ULAP) to provide information transmission and sensing simultaneously for LAE applications. To improve the throughput capacity, we formulate a data rate maximization problem by jointly optimizing the transmit information and sensing beamforming and the antenna positions of the MA array. Since the data rate maximization problem is non-convex with highly coupled variables, we propose an efficient alternation optimization based algorithm, which iteratively optimizes parts of the variables while fixing others. Numerical results show the superiority of the proposed MA array-based scheme in terms of the achievable data rate and beamforming gain compared with two benchmark schemes.","sentences":["This paper investigates a movable-antenna (MA) array empowered integrated sensing and communications (ISAC) over low-altitude platform (LAP) system to support low-altitude economy (LAE) applications.","In the considered system, an unmanned aerial vehicle (UAV) is dispatched to hover in the air, working as the UAV-enabled LAP (ULAP) to provide information transmission and sensing simultaneously for LAE applications.","To improve the throughput capacity, we formulate a data rate maximization problem by jointly optimizing the transmit information and sensing beamforming and the antenna positions of the MA array.","Since the data rate maximization problem is non-convex with highly coupled variables, we propose an efficient alternation optimization based algorithm, which iteratively optimizes parts of the variables while fixing others.","Numerical results show the superiority of the proposed MA array-based scheme in terms of the achievable data rate and beamforming gain compared with two benchmark schemes."],"url":"http://arxiv.org/abs/2406.07374v1","category":"eess.SP"}
{"created":"2024-06-11 15:40:44","title":"iMESA: Incremental Distributed Optimization for Collaborative Simultaneous Localization and Mapping","abstract":"This paper introduces a novel incremental distributed back-end algorithm for Collaborative Simultaneous Localization and Mapping (C-SLAM). For real-world deployments, robotic teams require algorithms to compute a consistent state estimate accurately, within online runtime constraints, and with potentially limited communication. Existing centralized, decentralized, and distributed approaches to solving C-SLAM problems struggle to achieve all of these goals. To address this capability gap, we present Incremental Manifold Edge-based Separable ADMM (iMESA) a fully distributed C-SLAM back-end algorithm that can provide a multi-robot team with accurate state estimates in real-time with only sparse pair-wise communication between robots. Extensive evaluation on real and synthetic data demonstrates that iMESA is able to outperform comparable state-of-the-art C-SLAM back-ends.","sentences":["This paper introduces a novel incremental distributed back-end algorithm for Collaborative Simultaneous Localization and Mapping (C-SLAM).","For real-world deployments, robotic teams require algorithms to compute a consistent state estimate accurately, within online runtime constraints, and with potentially limited communication.","Existing centralized, decentralized, and distributed approaches to solving C-SLAM problems struggle to achieve all of these goals.","To address this capability gap, we present Incremental Manifold Edge-based Separable ADMM (iMESA) a fully distributed C-SLAM back-end algorithm that can provide a multi-robot team with accurate state estimates in real-time with only sparse pair-wise communication between robots.","Extensive evaluation on real and synthetic data demonstrates that iMESA is able to outperform comparable state-of-the-art C-SLAM back-ends."],"url":"http://arxiv.org/abs/2406.07371v1","category":"cs.RO"}
{"created":"2024-06-11 15:27:02","title":"A mechanical qubit","abstract":"Strong nonlinear interactions between quantized excitations are an important resource for quantum technologies based on bosonic oscillator modes. However, most electromagnetic and mechanical nonlinearities arising from intrinsic material properties are far too weak compared to dissipation in the system to allow for nonlinear effects to be observed on the single-quantum level. To overcome this limitation, electromagnetic resonators in both the optical and microwave frequency regimes have been coupled to other strongly nonlinear quantum systems such as atoms and superconducting qubits, allowing for the demonstration of effects such as photon blockade and coherent quantum protocols using the Kerr effect. Here, we demonstrate the realization of the single-phonon nonlinear regime in a solid-state mechanical system. The single-phonon anharmonicity in our system exceeds the decoherence rate by a factor of 6.8, allowing us to use the lowest two energy levels of the resonator as a mechanical qubit, for which we show initialization, readout, and a complete set of direct single qubit gates. Our work adds another unique capability to a powerful quantum acoustics platform for quantum simulations, sensing, and information processing.","sentences":["Strong nonlinear interactions between quantized excitations are an important resource for quantum technologies based on bosonic oscillator modes.","However, most electromagnetic and mechanical nonlinearities arising from intrinsic material properties are far too weak compared to dissipation in the system to allow for nonlinear effects to be observed on the single-quantum level.","To overcome this limitation, electromagnetic resonators in both the optical and microwave frequency regimes have been coupled to other strongly nonlinear quantum systems such as atoms and superconducting qubits, allowing for the demonstration of effects such as photon blockade and coherent quantum protocols using the Kerr effect.","Here, we demonstrate the realization of the single-phonon nonlinear regime in a solid-state mechanical system.","The single-phonon anharmonicity in our system exceeds the decoherence rate by a factor of 6.8, allowing us to use the lowest two energy levels of the resonator as a mechanical qubit, for which we show initialization, readout, and a complete set of direct single qubit gates.","Our work adds another unique capability to a powerful quantum acoustics platform for quantum simulations, sensing, and information processing."],"url":"http://arxiv.org/abs/2406.07360v1","category":"quant-ph"}
{"created":"2024-06-11 15:26:20","title":"PSMC: Provable and Scalable Algorithms for Motif Conductance Based Graph Clustering","abstract":"Higher-order graph clustering aims to partition the graph using frequently occurring subgraphs. Motif conductance is one of the most promising higher-order graph clustering models due to its strong interpretability. However, existing motif conductance based graph clustering algorithms are mainly limited by a seminal two-stage reweighting computing framework, needing to enumerate all motif instances to obtain an edge-weighted graph for partitioning. However, such a framework has two-fold vital defects: (1) It can only provide a quadratic bound for the motif with three vertices, and whether there is provable clustering quality for other motifs is still an open question. (2) The enumeration procedure of motif instances incurs prohibitively high costs against large motifs or large dense graphs due to combinatorial explosions. Besides, expensive spectral clustering or local graph diffusion on the edge-weighted graph also makes existing methods unable to handle massive graphs with millions of nodes. To overcome these dilemmas, we propose a Provable and Scalable Motif Conductance algorithm PSMC, which has a fixed and motif-independent approximation ratio for any motif. Specifically, PSMC first defines a new vertex metric Motif Resident based on the given motif, which can be computed locally. Then, it iteratively deletes the vertex with the smallest motif resident value very efficiently using novel dynamic update technologies. Finally, it outputs the locally optimal result during the above iterative process. To further boost efficiency, we propose several effective bounds to estimate the motif resident value of each vertex, which can greatly reduce computational costs. Empirical results show that our proposed algorithms achieve 3.2-32 times speedup and improve the quality by at least 12 times than the baselines.","sentences":["Higher-order graph clustering aims to partition the graph using frequently occurring subgraphs.","Motif conductance is one of the most promising higher-order graph clustering models due to its strong interpretability.","However, existing motif conductance based graph clustering algorithms are mainly limited by a seminal two-stage reweighting computing framework, needing to enumerate all motif instances to obtain an edge-weighted graph for partitioning.","However, such a framework has two-fold vital defects: (1) It can only provide a quadratic bound for the motif with three vertices, and whether there is provable clustering quality for other motifs is still an open question.","(2) The enumeration procedure of motif instances incurs prohibitively high costs against large motifs or large dense graphs due to combinatorial explosions.","Besides, expensive spectral clustering or local graph diffusion on the edge-weighted graph also makes existing methods unable to handle massive graphs with millions of nodes.","To overcome these dilemmas, we propose a Provable and Scalable Motif Conductance algorithm PSMC, which has a fixed and motif-independent approximation ratio for any motif.","Specifically, PSMC first defines a new vertex metric Motif Resident based on the given motif, which can be computed locally.","Then, it iteratively deletes the vertex with the smallest motif resident value very efficiently using novel dynamic update technologies.","Finally, it outputs the locally optimal result during the above iterative process.","To further boost efficiency, we propose several effective bounds to estimate the motif resident value of each vertex, which can greatly reduce computational costs.","Empirical results show that our proposed algorithms achieve 3.2-32 times speedup and improve the quality by at least 12 times than the baselines."],"url":"http://arxiv.org/abs/2406.07357v1","category":"cs.CC"}
{"created":"2024-06-11 15:25:14","title":"Insulator-to-Metal Transition and Anomalously Slow Hot Carrier Cooling in a Photo-doped Mott Insulator","abstract":"Photo-doped Mott insulators can exhibit novel photocarrier transport and relaxation dynamics and non-equilibrium phases. However, time-resolved real-space imaging of these processes are still lacking. Here, we use scanning ultrafast electron microscopy (SUEM) to directly visualize the spatial-temporal evolution of photoexcited species in a spin-orbit assisted Mott insulator {\\alpha}-RuCl3. At low optical fluences, we observe extremely long hot photocarrier transport time over one nanosecond, almost an order of magnitude longer than any known values in conventional semiconductors. At higher optical fluences, we observe nonlinear features suggesting a photo-induced insulator-to-metal transition, which is unusual in a large-gap Mott insulator. Our results demonstrate the rich physics in a photo-doped Mott insulator that can be extracted from spatial-temporal imaging and showcase the capability of SUEM to sensitively probe photoexcitations in strongly correlated electron systems.","sentences":["Photo-doped Mott insulators can exhibit novel photocarrier transport and relaxation dynamics and non-equilibrium phases.","However, time-resolved real-space imaging of these processes are still lacking.","Here, we use scanning ultrafast electron microscopy (SUEM) to directly visualize the spatial-temporal evolution of photoexcited species in a spin-orbit assisted Mott insulator {\\alpha}-RuCl3.","At low optical fluences, we observe extremely long hot photocarrier transport time over one nanosecond, almost an order of magnitude longer than any known values in conventional semiconductors.","At higher optical fluences, we observe nonlinear features suggesting a photo-induced insulator-to-metal transition, which is unusual in a large-gap Mott insulator.","Our results demonstrate the rich physics in a photo-doped Mott insulator that can be extracted from spatial-temporal imaging and showcase the capability of SUEM to sensitively probe photoexcitations in strongly correlated electron systems."],"url":"http://arxiv.org/abs/2406.07355v1","category":"cond-mat.str-el"}
{"created":"2024-06-11 15:19:14","title":"Localized Orbital Scaling Correction with Linear Response in Materials","abstract":"Density functional theory (DFT) is a powerful tool for quantum-mechanical calculations, but practical calculations suffer systematic errors like incorrect charge densities and total energies in molecular dissociation, underestimated band gaps in bulk materials, and poor energy level alignment at interfaces. These problems are due to delocalization error. The localized orbital scaling correction (LOSC) removes delocalization error in molecules effectively, but screening of the Hartree-exchange-correlation response is necessary to correct it in materials. We introduce LOSC with system-dependent linear-response screening (lrLOSC), which effectively corrects delocalization error in semiconductors and insulators. After correcting for electron-phonon effects, the band gaps of eleven test systems are predicted with a mean absolute error of 0.28 eV, comparable to self-consistent $GW$. This method represents a significant step forward in correcting densities and total energies across system sizes and solving the band gap and energy level alignment problems entirely within the DFT framework.","sentences":["Density functional theory (DFT) is a powerful tool for quantum-mechanical calculations, but practical calculations suffer systematic errors like incorrect charge densities and total energies in molecular dissociation, underestimated band gaps in bulk materials, and poor energy level alignment at interfaces.","These problems are due to delocalization error.","The localized orbital scaling correction (LOSC) removes delocalization error in molecules effectively, but screening of the Hartree-exchange-correlation response is necessary to correct it in materials.","We introduce LOSC with system-dependent linear-response screening (lrLOSC), which effectively corrects delocalization error in semiconductors and insulators.","After correcting for electron-phonon effects, the band gaps of eleven test systems are predicted with a mean absolute error of 0.28 eV, comparable to self-consistent $GW$. This method represents a significant step forward in correcting densities and total energies across system sizes and solving the band gap and energy level alignment problems entirely within the DFT framework."],"url":"http://arxiv.org/abs/2406.07351v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-11 15:12:42","title":"Machine Learning approaches to classical density functional theory","abstract":"In this chapter, we discuss recent advances and new opportunities through methods of machine learning for the field of classical density functional theory, dealing with the equilibrium properties of thermal nano- and micro-particle systems having classical interactions. Machine learning methods offer the great potential to construct and/or improve the free energy functional (the central object of density functional theory) from simulation data and thus they complement traditional physics- or intuition-based approaches to the free energy construction. We also give an outlook to machine learning efforts in related fields, such as liquid state theory, electron density functional theory and power functional theory as a functionally formulated approach to classical nonequilibrium systems.","sentences":["In this chapter, we discuss recent advances and new opportunities through methods of machine learning for the field of classical density functional theory, dealing with the equilibrium properties of thermal nano- and micro-particle systems having classical interactions.","Machine learning methods offer the great potential to construct and/or improve the free energy functional (the central object of density functional theory) from simulation data and thus they complement traditional physics- or intuition-based approaches to the free energy construction.","We also give an outlook to machine learning efforts in related fields, such as liquid state theory, electron density functional theory and power functional theory as a functionally formulated approach to classical nonequilibrium systems."],"url":"http://arxiv.org/abs/2406.07345v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-11 15:02:16","title":"Global-Regularized Neighborhood Regression for Efficient Zero-Shot Texture Anomaly Detection","abstract":"Texture surface anomaly detection finds widespread applications in industrial settings. However, existing methods often necessitate gathering numerous samples for model training. Moreover, they predominantly operate within a close-set detection framework, limiting their ability to identify anomalies beyond the training dataset. To tackle these challenges, this paper introduces a novel zero-shot texture anomaly detection method named Global-Regularized Neighborhood Regression (GRNR). Unlike conventional approaches, GRNR can detect anomalies on arbitrary textured surfaces without any training data or cost. Drawing from human visual cognition, GRNR derives two intrinsic prior supports directly from the test texture image: local neighborhood priors characterized by coherent similarities and global normality priors featuring typical normal patterns. The fundamental principle of GRNR involves utilizing the two extracted intrinsic support priors for self-reconstructive regression of the query sample. This process employs the transformation facilitated by local neighbor support while being regularized by global normality support, aiming to not only achieve visually consistent reconstruction results but also preserve normality properties. We validate the effectiveness of GRNR across various industrial scenarios using eight benchmark datasets, demonstrating its superior detection performance without the need for training data. Remarkably, our method is applicable for open-set texture defect detection and can even surpass existing vanilla approaches that require extensive training.","sentences":["Texture surface anomaly detection finds widespread applications in industrial settings.","However, existing methods often necessitate gathering numerous samples for model training.","Moreover, they predominantly operate within a close-set detection framework, limiting their ability to identify anomalies beyond the training dataset.","To tackle these challenges, this paper introduces a novel zero-shot texture anomaly detection method named Global-Regularized Neighborhood Regression (GRNR).","Unlike conventional approaches, GRNR can detect anomalies on arbitrary textured surfaces without any training data or cost.","Drawing from human visual cognition, GRNR derives two intrinsic prior supports directly from the test texture image: local neighborhood priors characterized by coherent similarities and global normality priors featuring typical normal patterns.","The fundamental principle of GRNR involves utilizing the two extracted intrinsic support priors for self-reconstructive regression of the query sample.","This process employs the transformation facilitated by local neighbor support while being regularized by global normality support, aiming to not only achieve visually consistent reconstruction results but also preserve normality properties.","We validate the effectiveness of GRNR across various industrial scenarios using eight benchmark datasets, demonstrating its superior detection performance without the need for training data.","Remarkably, our method is applicable for open-set texture defect detection and can even surpass existing vanilla approaches that require extensive training."],"url":"http://arxiv.org/abs/2406.07333v1","category":"cs.CV"}
{"created":"2024-06-11 15:00:24","title":"Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field","abstract":"Radiance field methods represent the state of the art in reconstructing complex scenes from multi-view photos. However, these reconstructions often suffer from one or both of the following limitations: First, they typically represent scenes in low dynamic range (LDR), which restricts their use to evenly lit environments and hinders immersive viewing experiences. Secondly, their reliance on a pinhole camera model, assuming all scene elements are in focus in the input images, presents practical challenges and complicates refocusing during novel-view synthesis. Addressing these limitations, we present a lightweight method based on 3D Gaussian Splatting that utilizes multi-view LDR images of a scene with varying exposure times, apertures, and focus distances as input to reconstruct a high-dynamic-range (HDR) radiance field. By incorporating analytical convolutions of Gaussians based on a thin-lens camera model as well as a tonemapping module, our reconstructions enable the rendering of HDR content with flexible refocusing capabilities. We demonstrate that our combined treatment of HDR and depth of field facilitates real-time cinematic rendering, outperforming the state of the art.","sentences":["Radiance field methods represent the state of the art in reconstructing complex scenes from multi-view photos.","However, these reconstructions often suffer from one or both of the following limitations:","First, they typically represent scenes in low dynamic range (LDR), which restricts their use to evenly lit environments and hinders immersive viewing experiences.","Secondly, their reliance on a pinhole camera model, assuming all scene elements are in focus in the input images, presents practical challenges and complicates refocusing during novel-view synthesis.","Addressing these limitations, we present a lightweight method based on 3D Gaussian Splatting that utilizes multi-view LDR images of a scene with varying exposure times, apertures, and focus distances as input to reconstruct a high-dynamic-range (HDR) radiance field.","By incorporating analytical convolutions of Gaussians based on a thin-lens camera model as well as a tonemapping module, our reconstructions enable the rendering of HDR content with flexible refocusing capabilities.","We demonstrate that our combined treatment of HDR and depth of field facilitates real-time cinematic rendering, outperforming the state of the art."],"url":"http://arxiv.org/abs/2406.07329v1","category":"cs.CV"}
{"created":"2024-06-11 14:58:33","title":"Lyapunov equations: a (fixed) point of view","abstract":"The Lyapunov equation is the gateway drug of nonlinear control theory. In these notes we revisit an elegant statement connecting the concepts of asymptotic stability and observability, to the solvability of Lyapunov equations, and discuss how this statement can be proved using the Brouwer fixed-point theorem.","sentences":["The Lyapunov equation is the gateway drug of nonlinear control theory.","In these notes we revisit an elegant statement connecting the concepts of asymptotic stability and observability, to the solvability of Lyapunov equations, and discuss how this statement can be proved using the Brouwer fixed-point theorem."],"url":"http://arxiv.org/abs/2406.07324v1","category":"eess.SY"}
{"created":"2024-06-11 14:47:36","title":"Embedded Graph Convolutional Networks for Real-Time Event Data Processing on SoC FPGAs","abstract":"The utilisation of event cameras represents an important and swiftly evolving trend aimed at addressing the constraints of traditional video systems. Particularly within the automotive domain, these cameras find significant relevance for their integration into embedded real-time systems due to lower latency and energy consumption. One effective approach to ensure the necessary throughput and latency for event processing systems is through the utilisation of graph convolutional networks (GCNs). In this study, we introduce a series of hardware-aware optimisations tailored for PointNet++, a GCN architecture designed for point cloud processing. The proposed techniques result in more than a 100-fold reduction in model size compared to Asynchronous Event-based GNN (AEGNN), one of the most recent works in the field, with a relatively small decrease in accuracy (2.3% for N-Caltech101 classification, 1.7% for N-Cars classification), thus following the TinyML trend. Based on software research, we designed a custom EFGCN (Event-Based FPGA-accelerated Graph Convolutional Network) and we implemented it on ZCU104 SoC FPGA platform, achieving a throughput of 13.3 million events per second (MEPS) and real-time partially asynchronous processing with a latency of 4.47 ms. We also address the scalability of the proposed hardware model to improve the obtained accuracy score. To the best of our knowledge, this study marks the first endeavour in accelerating PointNet++ networks on SoC FPGAs, as well as the first hardware architecture exploration of graph convolutional networks implementation for real-time continuous event data processing. We publish both software and hardware source code in an open repository: https://github.com/vision-agh/*** (will be published upon acceptance).","sentences":["The utilisation of event cameras represents an important and swiftly evolving trend aimed at addressing the constraints of traditional video systems.","Particularly within the automotive domain, these cameras find significant relevance for their integration into embedded real-time systems due to lower latency and energy consumption.","One effective approach to ensure the necessary throughput and latency for event processing systems is through the utilisation of graph convolutional networks (GCNs).","In this study, we introduce a series of hardware-aware optimisations tailored for PointNet++, a GCN architecture designed for point cloud processing.","The proposed techniques result in more than a 100-fold reduction in model size compared to Asynchronous Event-based GNN (AEGNN), one of the most recent works in the field, with a relatively small decrease in accuracy (2.3% for N-Caltech101 classification, 1.7% for N-Cars classification), thus following the TinyML trend.","Based on software research, we designed a custom EFGCN (Event-Based FPGA-accelerated Graph Convolutional Network) and we implemented it on ZCU104 SoC FPGA platform, achieving a throughput of 13.3 million events per second (MEPS) and real-time partially asynchronous processing with a latency of 4.47 ms.","We also address the scalability of the proposed hardware model to improve the obtained accuracy score.","To the best of our knowledge, this study marks the first endeavour in accelerating PointNet++ networks on SoC FPGAs, as well as the first hardware architecture exploration of graph convolutional networks implementation for real-time continuous event data processing.","We publish both software and hardware source code in an open repository: https://github.com/vision-agh/*** (will be published upon acceptance)."],"url":"http://arxiv.org/abs/2406.07318v1","category":"cs.CV"}
{"created":"2024-06-11 14:45:00","title":"Fetch-A-Set: A Large-Scale OCR-Free Benchmark for Historical Document Retrieval","abstract":"This paper introduces Fetch-A-Set (FAS), a comprehensive benchmark tailored for legislative historical document analysis systems, addressing the challenges of large-scale document retrieval in historical contexts. The benchmark comprises a vast repository of documents dating back to the XVII century, serving both as a training resource and an evaluation benchmark for retrieval systems. It fills a critical gap in the literature by focusing on complex extractive tasks within the domain of cultural heritage. The proposed benchmark tackles the multifaceted problem of historical document analysis, including text-to-image retrieval for queries and image-to-text topic extraction from document fragments, all while accommodating varying levels of document legibility. This benchmark aims to spur advancements in the field by providing baselines and data for the development and evaluation of robust historical document retrieval systems, particularly in scenarios characterized by wide historical spectrum.","sentences":["This paper introduces Fetch-A-Set (FAS), a comprehensive benchmark tailored for legislative historical document analysis systems, addressing the challenges of large-scale document retrieval in historical contexts.","The benchmark comprises a vast repository of documents dating back to the XVII century, serving both as a training resource and an evaluation benchmark for retrieval systems.","It fills a critical gap in the literature by focusing on complex extractive tasks within the domain of cultural heritage.","The proposed benchmark tackles the multifaceted problem of historical document analysis, including text-to-image retrieval for queries and image-to-text topic extraction from document fragments, all while accommodating varying levels of document legibility.","This benchmark aims to spur advancements in the field by providing baselines and data for the development and evaluation of robust historical document retrieval systems, particularly in scenarios characterized by wide historical spectrum."],"url":"http://arxiv.org/abs/2406.07315v1","category":"cs.IR"}
{"created":"2024-06-11 14:29:58","title":"Optimal Scheduling of Battery Storage Systems in the Swedish Multi-FCR Market Incorporating Battery Degradation and Technical Requirements","abstract":"This paper develops a novel mixed-integer linear programming (MILP) model for optimal participation of battery energy storage systems (BESSs) in the Swedish frequency containment reserve (FCR) markets. The developed model aims to maximize the battery owner's potential profit by considering battery degradation and participation in multiple FCR markets, i.e., FCR in normal operation (FCR-N), and FCR in disturbances (FCR-D) for up- and down-regulations. Accordingly, a precise formulation of a detailed battery degradation model and adherence to the technical requirements of the Swedish FCR markets are incorporated into the developed model. To achieve more practical results, simulations are conducted based on one minute time step realistic data for the whole year 2022. The results show a potential profit of 708 thousand Euros for a 1MW/1MWh BESS by participating in multi-FCR market. Analyzing the impact of considering degradation in the optimization problem has shown that the annual battery aging cost could decrease by 5%-29% without a significant effect on profit. The proposed model can be practically used by flexibility asset owners to achieve profitable and sustainable operation strategies that reduce battery degradation.","sentences":["This paper develops a novel mixed-integer linear programming (MILP) model for optimal participation of battery energy storage systems (BESSs) in the Swedish frequency containment reserve (FCR) markets.","The developed model aims to maximize the battery owner's potential profit by considering battery degradation and participation in multiple FCR markets, i.e., FCR in normal operation (FCR-N), and FCR in disturbances (FCR-D) for up- and down-regulations.","Accordingly, a precise formulation of a detailed battery degradation model and adherence to the technical requirements of the Swedish FCR markets are incorporated into the developed model.","To achieve more practical results, simulations are conducted based on one minute time step realistic data for the whole year 2022.","The results show a potential profit of 708 thousand Euros for a 1MW/1MWh BESS by participating in multi-FCR market.","Analyzing the impact of considering degradation in the optimization problem has shown that the annual battery aging cost could decrease by 5%-29% without a significant effect on profit.","The proposed model can be practically used by flexibility asset owners to achieve profitable and sustainable operation strategies that reduce battery degradation."],"url":"http://arxiv.org/abs/2406.07301v1","category":"eess.SY"}
{"created":"2024-06-11 14:27:40","title":"Enhanced In-Flight Connectivity for Urban Air Mobility via LEO Satellite Networks","abstract":"Urban Air Mobility (UAM) is the envisioned future of inter-city aerial transportation. This paper presents a novel, in-flight connectivity link allocation method for UAM, which dynamically switches between terrestrial cellular and Low Earth Orbit (LEO) satellite networks based on real-time conditions. Our approach prefers cellular networks for cost efficiency, switching to LEO satellites under poor cellular conditions to ensure continuous UAM connectivity. By integrating real-time metrics like signal strength, network congestion, and flight trajectory into the selection process, our algorithm effectively balances cost, minimum data rate requirements, and continuity of communication. Numerical results validate minimization of data-loss while ensuring an optimal selection from the set of available above-threshold data rates at every time sample. Furthermore, insights derived from our study emphasize the importance of hybrid connectivity solutions in ensuring seamless, uninterrupted communication for future urban aerial vehicles.","sentences":["Urban Air Mobility (UAM) is the envisioned future of inter-city aerial transportation.","This paper presents a novel, in-flight connectivity link allocation method for UAM, which dynamically switches between terrestrial cellular and Low Earth Orbit (LEO) satellite networks based on real-time conditions.","Our approach prefers cellular networks for cost efficiency, switching to LEO satellites under poor cellular conditions to ensure continuous UAM connectivity.","By integrating real-time metrics like signal strength, network congestion, and flight trajectory into the selection process, our algorithm effectively balances cost, minimum data rate requirements, and continuity of communication.","Numerical results validate minimization of data-loss while ensuring an optimal selection from the set of available above-threshold data rates at every time sample.","Furthermore, insights derived from our study emphasize the importance of hybrid connectivity solutions in ensuring seamless, uninterrupted communication for future urban aerial vehicles."],"url":"http://arxiv.org/abs/2406.07298v1","category":"cs.ET"}
{"created":"2024-06-11 14:24:45","title":"Instruct Large Language Models to Drive like Humans","abstract":"Motion planning in complex scenarios is the core challenge in autonomous driving. Conventional methods apply predefined rules or learn from driving data to plan the future trajectory. Recent methods seek the knowledge preserved in large language models (LLMs) and apply them in the driving scenarios. Despite the promising results, it is still unclear whether the LLM learns the underlying human logic to drive. In this paper, we propose an InstructDriver method to transform LLM into a motion planner with explicit instruction tuning to align its behavior with humans. We derive driving instruction data based on human logic (e.g., do not cause collisions) and traffic rules (e.g., proceed only when green lights). We then employ an interpretable InstructChain module to further reason the final planning reflecting the instructions. Our InstructDriver allows the injection of human rules and learning from driving data, enabling both interpretability and data scalability. Different from existing methods that experimented on closed-loop or simulated settings, we adopt the real-world closed-loop motion planning nuPlan benchmark for better evaluation. InstructDriver demonstrates the effectiveness of the LLM planner in a real-world closed-loop setting. Our code is publicly available at https://github.com/bonbon-rj/InstructDriver.","sentences":["Motion planning in complex scenarios is the core challenge in autonomous driving.","Conventional methods apply predefined rules or learn from driving data to plan the future trajectory.","Recent methods seek the knowledge preserved in large language models (LLMs) and apply them in the driving scenarios.","Despite the promising results, it is still unclear whether the LLM learns the underlying human logic to drive.","In this paper, we propose an InstructDriver method to transform LLM into a motion planner with explicit instruction tuning to align its behavior with humans.","We derive driving instruction data based on human logic (e.g., do not cause collisions) and traffic rules (e.g., proceed only when green lights).","We then employ an interpretable InstructChain module to further reason the final planning reflecting the instructions.","Our InstructDriver allows the injection of human rules and learning from driving data, enabling both interpretability and data scalability.","Different from existing methods that experimented on closed-loop or simulated settings, we adopt the real-world closed-loop motion planning nuPlan benchmark for better evaluation.","InstructDriver demonstrates the effectiveness of the LLM planner in a real-world closed-loop setting.","Our code is publicly available at https://github.com/bonbon-rj/InstructDriver."],"url":"http://arxiv.org/abs/2406.07296v1","category":"cs.RO"}
{"created":"2024-06-11 14:23:48","title":"OTO Planner: An Efficient Only Travelling Once Exploration Planner for Complex and Unknown Environments","abstract":"Autonomous exploration in complex and cluttered environments is essential for various applications. However, there are many challenges due to the lack of global heuristic information. Existing exploration methods suffer from the repeated paths and considerable computational resource requirement in large-scale environments. To address the above issues, this letter proposes an efficient exploration planner that reduces repeated paths in complex environments, hence it is called \"Only Travelling Once Planner\". OTO Planner includes fast frontier updating, viewpoint evaluation and viewpoint refinement. A selective frontier updating mechanism is designed, saving a large amount of computational resources. In addition, a novel viewpoint evaluation system is devised to reduce the repeated paths utilizing the enclosed sub-region detection. Besides, a viewpoint refinement approach is raised to concentrate the redundant viewpoints, leading to smoother paths. We conduct extensive simulation and real-world experiments to validate the proposed method. Compared to the state-of-the-art approach, the proposed method reduces the exploration time and movement distance by 10%-20% and improves the speed of frontier detection by 6-9 times.","sentences":["Autonomous exploration in complex and cluttered environments is essential for various applications.","However, there are many challenges due to the lack of global heuristic information.","Existing exploration methods suffer from the repeated paths and considerable computational resource requirement in large-scale environments.","To address the above issues, this letter proposes an efficient exploration planner that reduces repeated paths in complex environments, hence it is called \"Only Travelling Once Planner\".","OTO Planner includes fast frontier updating, viewpoint evaluation and viewpoint refinement.","A selective frontier updating mechanism is designed, saving a large amount of computational resources.","In addition, a novel viewpoint evaluation system is devised to reduce the repeated paths utilizing the enclosed sub-region detection.","Besides, a viewpoint refinement approach is raised to concentrate the redundant viewpoints, leading to smoother paths.","We conduct extensive simulation and real-world experiments to validate the proposed method.","Compared to the state-of-the-art approach, the proposed method reduces the exploration time and movement distance by 10%-20% and improves the speed of frontier detection by 6-9 times."],"url":"http://arxiv.org/abs/2406.07294v1","category":"cs.RO"}
{"created":"2024-06-11 14:15:33","title":"Bilingual Sexism Classification: Fine-Tuned XLM-RoBERTa and GPT-3.5 Few-Shot Learning","abstract":"Sexism in online content is a pervasive issue that necessitates effective classification techniques to mitigate its harmful impact. Online platforms often have sexist comments and posts that create a hostile environment, especially for women and minority groups. This content not only spreads harmful stereotypes but also causes emotional harm. Reliable methods are essential to find and remove sexist content, making online spaces safer and more welcoming. Therefore, the sEXism Identification in Social neTworks (EXIST) challenge addresses this issue at CLEF 2024. This study aims to improve sexism identification in bilingual contexts (English and Spanish) by leveraging natural language processing models. The tasks are to determine whether a text is sexist and what the source intention behind it is. We fine-tuned the XLM-RoBERTa model and separately used GPT-3.5 with few-shot learning prompts to classify sexist content. The XLM-RoBERTa model exhibited robust performance in handling complex linguistic structures, while GPT-3.5's few-shot learning capability allowed for rapid adaptation to new data with minimal labeled examples. Our approach using XLM-RoBERTa achieved 4th place in the soft-soft evaluation of Task 1 (sexism identification). For Task 2 (source intention), we achieved 2nd place in the soft-soft evaluation.","sentences":["Sexism in online content is a pervasive issue that necessitates effective classification techniques to mitigate its harmful impact.","Online platforms often have sexist comments and posts that create a hostile environment, especially for women and minority groups.","This content not only spreads harmful stereotypes but also causes emotional harm.","Reliable methods are essential to find and remove sexist content, making online spaces safer and more welcoming.","Therefore, the sEXism Identification in Social neTworks (EXIST) challenge addresses this issue at CLEF 2024.","This study aims to improve sexism identification in bilingual contexts (English and Spanish) by leveraging natural language processing models.","The tasks are to determine whether a text is sexist and what the source intention behind it is.","We fine-tuned the XLM-RoBERTa model and separately used GPT-3.5 with few-shot learning prompts to classify sexist content.","The XLM-RoBERTa model exhibited robust performance in handling complex linguistic structures, while GPT-3.5's few-shot learning capability allowed for rapid adaptation to new data with minimal labeled examples.","Our approach using XLM-RoBERTa achieved 4th place in the soft-soft evaluation of Task 1 (sexism identification).","For Task 2 (source intention), we achieved 2nd place in the soft-soft evaluation."],"url":"http://arxiv.org/abs/2406.07287v1","category":"cs.CL"}
{"created":"2024-06-11 14:08:17","title":"Optimal policy design for decision problems under social influence","abstract":"This paper focuses on devising strategies for control-oriented decision-making scenarios, in the presence of social and external influences, e.g. within recommending systems in social contexts. More precisely, we extend the classical Friedkin and Johnsen model of opinion dynamics to incorporate random factors, such as variability in individual predisposition, and uncertainty in social acceptance towards a specific action that a recommending system aims to promote. Furthermore, we formulate an optimization-based control problem aimed at fostering the social acceptance of particular actions within the network. Initially conceptualized as an economic cost minimization, in this preliminary work, we simplify our problem by reformulating it into an MPC framework. Through our analysis and numerical simulations, we illustrate the effectiveness of the proposed methodologies.","sentences":["This paper focuses on devising strategies for control-oriented decision-making scenarios, in the presence of social and external influences, e.g. within recommending systems in social contexts.","More precisely, we extend the classical Friedkin and Johnsen model of opinion dynamics to incorporate random factors, such as variability in individual predisposition, and uncertainty in social acceptance towards a specific action that a recommending system aims to promote.","Furthermore, we formulate an optimization-based control problem aimed at fostering the social acceptance of particular actions within the network.","Initially conceptualized as an economic cost minimization, in this preliminary work, we simplify our problem by reformulating it into an MPC framework.","Through our analysis and numerical simulations, we illustrate the effectiveness of the proposed methodologies."],"url":"http://arxiv.org/abs/2406.07282v1","category":"eess.SY"}
{"created":"2024-06-11 14:04:58","title":"On Kernel's Safety in the Spectre Era (Extended Version)","abstract":"The efficacy of address space layout randomization has been formally demonstrated in a shared-memory model by Abadi et al., contingent on specific assumptions about victim programs. However, modern operating systems, implementing layout randomization in the kernel, diverge from these assumptions and operate on a separate memory model with communication through system calls. In this work, we relax Abadi et al.'s language assumptions while demonstrating that layout randomization offers a comparable safety guarantee in a system with memory separation. However, in practice, speculative execution and side-channels are recognized threats to layout randomization. We show that kernel safety cannot be restored for attackers capable of using side-channels and speculative execution and introduce a new condition, that allows us to formally prove kernel safety in the Spectre era. Our research demonstrates that under this condition, the system remains safe without relying on layout randomization. We also demonstrate that our condition can be sensibly weakened, leading to enforcement mechanisms that can guarantee kernel safety for safe system calls in the Spectre era.","sentences":["The efficacy of address space layout randomization has been formally demonstrated in a shared-memory model by Abadi et al., contingent on specific assumptions about victim programs.","However, modern operating systems, implementing layout randomization in the kernel, diverge from these assumptions and operate on a separate memory model with communication through system calls.","In this work, we relax Abadi et al.'s language assumptions while demonstrating that layout randomization offers a comparable safety guarantee in a system with memory separation.","However, in practice, speculative execution and side-channels are recognized threats to layout randomization.","We show that kernel safety cannot be restored for attackers capable of using side-channels and speculative execution and introduce a new condition, that allows us to formally prove kernel safety in the Spectre era.","Our research demonstrates that under this condition, the system remains safe without relying on layout randomization.","We also demonstrate that our condition can be sensibly weakened, leading to enforcement mechanisms that can guarantee kernel safety for safe system calls in the Spectre era."],"url":"http://arxiv.org/abs/2406.07278v1","category":"cs.CR"}
{"created":"2024-06-11 13:56:12","title":"Exploiting Heterogeneity in the Decentralised Control of Platoons","abstract":"This paper investigates the use of decentralised control architectures with heterogeneous dynamics for improving performance in large-scale systems. Our focus is on two well-known decentralised approaches; the 'predecessor following' and 'bidirectional architectures' for vehicle platooning. The former, utilising homogeneous control dynamics, is known to face exponential growth in disturbance amplification throughout the platoon, resulting in poor scalability properties. We demonstrate that by incorporating heterogeneous control system dynamics, this limitation disappears entirely, even under bandwidth constraints. Furthermore, we reveal that introducing heterogeneity in the bidirectional architecture allows the platoon's behaviour to be rendered independent of its length, allowing for highly scalable performance.","sentences":["This paper investigates the use of decentralised control architectures with heterogeneous dynamics for improving performance in large-scale systems.","Our focus is on two well-known decentralised approaches; the 'predecessor following' and 'bidirectional architectures' for vehicle platooning.","The former, utilising homogeneous control dynamics, is known to face exponential growth in disturbance amplification throughout the platoon, resulting in poor scalability properties.","We demonstrate that by incorporating heterogeneous control system dynamics, this limitation disappears entirely, even under bandwidth constraints.","Furthermore, we reveal that introducing heterogeneity in the bidirectional architecture allows the platoon's behaviour to be rendered independent of its length, allowing for highly scalable performance."],"url":"http://arxiv.org/abs/2406.07271v1","category":"eess.SY"}
{"created":"2024-06-11 13:46:52","title":"Personalisation of d'Hondt's algorithm and its use in recommender ecosystems","abstract":"In the area of recommender systems, we are dealing with aggregations and potential of personalisation in ecosystems. Personalisation is based on separate aggregation models for each user. This approach reveals differences in user preferences, especially when they are in strict disagreement with global preferences. Hybrid models are based on combination of global and personalised model of weights for d'Hondt's voting algorithm. This paper shows that personalisation combined with hybridisation on case-by-case basis outperforms non-personalised d'Hondt's algorithm on datasets RetailRocket and SLANTour. By taking into account voices of minorities we achieved better click through rate.","sentences":["In the area of recommender systems, we are dealing with aggregations and potential of personalisation in ecosystems.","Personalisation is based on separate aggregation models for each user.","This approach reveals differences in user preferences, especially when they are in strict disagreement with global preferences.","Hybrid models are based on combination of global and personalised model of weights for d'Hondt's voting algorithm.","This paper shows that personalisation combined with hybridisation on case-by-case basis outperforms non-personalised d'Hondt's algorithm on datasets RetailRocket and SLANTour.","By taking into account voices of minorities we achieved better click through rate."],"url":"http://arxiv.org/abs/2406.07264v1","category":"cs.HC"}
{"created":"2024-06-11 13:42:49","title":"Active learning for affinity prediction of antibodies","abstract":"The primary objective of most lead optimization campaigns is to enhance the binding affinity of ligands. For large molecules such as antibodies, identifying mutations that enhance antibody affinity is particularly challenging due to the combinatorial explosion of potential mutations. When the structure of the antibody-antigen complex is available, relative binding free energy (RBFE) methods can offer valuable insights into how different mutations will impact the potency and selectivity of a drug candidate, thereby reducing the reliance on costly and time-consuming wet-lab experiments. However, accurately simulating the physics of large molecules is computationally intensive. We present an active learning framework that iteratively proposes promising sequences for simulators to evaluate, thereby accelerating the search for improved binders. We explore different modeling approaches to identify the most effective surrogate model for this task, and evaluate our framework both using pre-computed pools of data and in a realistic full-loop setting.","sentences":["The primary objective of most lead optimization campaigns is to enhance the binding affinity of ligands.","For large molecules such as antibodies, identifying mutations that enhance antibody affinity is particularly challenging due to the combinatorial explosion of potential mutations.","When the structure of the antibody-antigen complex is available, relative binding free energy (RBFE) methods can offer valuable insights into how different mutations will impact the potency and selectivity of a drug candidate, thereby reducing the reliance on costly and time-consuming wet-lab experiments.","However, accurately simulating the physics of large molecules is computationally intensive.","We present an active learning framework that iteratively proposes promising sequences for simulators to evaluate, thereby accelerating the search for improved binders.","We explore different modeling approaches to identify the most effective surrogate model for this task, and evaluate our framework both using pre-computed pools of data and in a realistic full-loop setting."],"url":"http://arxiv.org/abs/2406.07263v1","category":"cs.LG"}
{"created":"2024-06-11 13:32:10","title":"Infinite-Horizon Distributionally Robust Regret-Optimal Control","abstract":"We study the infinite-horizon distributionally robust (DR) control of linear systems with quadratic costs, where disturbances have unknown, possibly time-correlated distribution within a Wasserstein-2 ambiguity set. We aim to minimize the worst-case expected regret-the excess cost of a causal policy compared to a non-causal one with access to future disturbance. Though the optimal policy lacks a finite-order state-space realization (i.e., it is non-rational), it can be characterized by a finite-dimensional parameter. Leveraging this, we develop an efficient frequency-domain algorithm to compute this optimal control policy and present a convex optimization method to construct a near-optimal state-space controller that approximates the optimal non-rational controller in the $\\mathit{H}_\\infty$-norm. This approach avoids solving a computationally expensive semi-definite program (SDP) that scales with the time horizon in the finite-horizon setting.","sentences":["We study the infinite-horizon distributionally robust (DR) control of linear systems with quadratic costs, where disturbances have unknown, possibly time-correlated distribution within a Wasserstein-2 ambiguity set.","We aim to minimize the worst-case expected regret-the excess cost of a causal policy compared to a non-causal one with access to future disturbance.","Though the optimal policy lacks a finite-order state-space realization (i.e., it is non-rational), it can be characterized by a finite-dimensional parameter.","Leveraging this, we develop an efficient frequency-domain algorithm to compute this optimal control policy and present a convex optimization method to construct a near-optimal state-space controller that approximates the optimal non-rational controller in the $\\mathit{H}_\\infty$-norm.","This approach avoids solving a computationally expensive semi-definite program (SDP) that scales with the time horizon in the finite-horizon setting."],"url":"http://arxiv.org/abs/2406.07248v1","category":"math.OC"}
{"created":"2024-06-11 13:22:25","title":"Samelson complex structures for the tangent Lie group","abstract":"It is shown that for any compact Lie group $G$ (odd or even dimensional), the tangent bundle $TG$ admits a left-invariant integrable almost complex structure, where the Lie group structure on $TG$ is the natural one induced from $G$. The aforementioned complex structure on $TG$ is inspired by Samelson's construction for even dimensional compact Lie groups.","sentences":["It is shown that for any compact Lie group $G$ (odd or even dimensional), the tangent bundle $TG$ admits a left-invariant integrable almost complex structure, where the Lie group structure on $TG$ is the natural one induced from $G$. The aforementioned complex structure on $TG$ is inspired by Samelson's construction for even dimensional compact Lie groups."],"url":"http://arxiv.org/abs/2406.07241v1","category":"math.DG"}
{"created":"2024-06-11 13:21:57","title":"Odd and Even Elliptic Curves with Complex Multiplication","abstract":"We call an order $O$ in a quadratic field $K$ odd (resp. even) if its discriminant is an odd (resp. even) integer. We call an elliptic curve $E$ over the field $C$ of complex numbers with CM odd (resp. even) if its endomorphism ring $End(E)$ is an odd (resp. even) order in the corresponding imaginary quadratic field.   Suppose that $j(E)$ is a real number and let us consider the set $J(R,E)$ of all $j(E')$ where $E'$ is any elliptic curve that enjoys the following properties.   1) $E'$ is isogenous to $E$;   2) $j(E')$ is a real number;   3) $E'$ has the same parity as $E$.   We prove that the closure of $J(R,E)$ in the set $R$ of real numbers is the closed semi-infinite interval $(-\\infty,1728]$ (resp. the whole $R$) if $E$ is odd (resp. even).   This paper was inspired by a question of Jean-Louis Colliot-Th\\'el\\`ene and Alena Pirutka about the distribution of $j$-invariants of certain elliptic curves of CM type.","sentences":["We call an order $O$ in a quadratic field $K$ odd (resp.","even) if its discriminant is an odd (resp.","even) integer.","We call an elliptic curve $E$ over the field $C$ of complex numbers with CM odd (resp.","even) if its endomorphism ring $End(E)$ is an odd (resp.","even) order in the corresponding imaginary quadratic field.   Suppose that $j(E)$ is a real number and let us consider the set $J(R,E)$ of all $j(E')$ where $E'$ is any elliptic curve that enjoys the following properties.   ","1) $E'$ is isogenous to $E$;   2) $j(E')$ is a real number;   3) $E'$ has the same parity as $E$.   We prove that the closure of $J(R,E)$ in the set $R$ of real numbers is the closed semi-infinite interval $(-\\infty,1728]$ (resp.","the whole $R$) if $E$ is odd (resp.","even).   ","This paper was inspired by a question of Jean-Louis Colliot-Th\\'el\\`ene and Alena Pirutka about the distribution of $j$-invariants of certain elliptic curves of CM type."],"url":"http://arxiv.org/abs/2406.07240v1","category":"math.NT"}
{"created":"2024-06-11 13:06:09","title":"Which Country Is This? Automatic Country Ranking of Street View Photos","abstract":"In this demonstration, we present Country Guesser, a live system that guesses the country that a photo is taken in. In particular, given a Google Street View image, our federated ranking model uses a combination of computer vision, machine learning and text retrieval methods to compute a ranking of likely countries of the location shown in a given image from Street View. Interestingly, using text-based features to probe large pre-trained language models can assist to provide cross-modal supervision. We are not aware of previous country guessing systems informed by visual and textual features.","sentences":["In this demonstration, we present Country Guesser, a live system that guesses the country that a photo is taken in.","In particular, given a Google Street View image, our federated ranking model uses a combination of computer vision, machine learning and text retrieval methods to compute a ranking of likely countries of the location shown in a given image from Street View.","Interestingly, using text-based features to probe large pre-trained language models can assist to provide cross-modal supervision.","We are not aware of previous country guessing systems informed by visual and textual features."],"url":"http://arxiv.org/abs/2406.07227v1","category":"cs.CV"}
{"created":"2024-06-11 12:57:38","title":"Probabilistic time integration for semi-explicit PDAEs","abstract":"This paper deals with the application of probabilistic time integration methods to semi-explicit partial differential-algebraic equations of parabolic type and its semi-discrete counterparts, namely semi-explicit differential-algebraic equations of index 2. The proposed methods iteratively construct a probability distribution over the solution of deterministic problems, enhancing the information obtained from the numerical simulation. Within this paper, we examine the efficacy of the randomized versions of the implicit Euler method, the midpoint scheme, and exponential integrators of first and second order. By demonstrating the consistency and convergence properties of these solvers, we illustrate their utility in capturing the sensitivity of the solution to numerical errors. Our analysis establishes the theoretical validity of randomized time integration for constrained systems and offers insights into the calibration of probabilistic integrators for practical applications.","sentences":["This paper deals with the application of probabilistic time integration methods to semi-explicit partial differential-algebraic equations of parabolic type and its semi-discrete counterparts, namely semi-explicit differential-algebraic equations of index 2.","The proposed methods iteratively construct a probability distribution over the solution of deterministic problems, enhancing the information obtained from the numerical simulation.","Within this paper, we examine the efficacy of the randomized versions of the implicit Euler method, the midpoint scheme, and exponential integrators of first and second order.","By demonstrating the consistency and convergence properties of these solvers, we illustrate their utility in capturing the sensitivity of the solution to numerical errors.","Our analysis establishes the theoretical validity of randomized time integration for constrained systems and offers insights into the calibration of probabilistic integrators for practical applications."],"url":"http://arxiv.org/abs/2406.07220v1","category":"math.NA"}
{"created":"2024-06-11 12:44:16","title":"DSig: Breaking the Barrier of Signatures in Data Centers","abstract":"Data centers increasingly host mutually distrustful users on shared infrastructure. A powerful tool to safeguard such users are digital signatures. Digital signatures have revolutionized Internet-scale applications, but current signatures are too slow for the growing genre of microsecond-scale systems in modern data centers. We propose DSig, the first digital signature system to achieve single-digit microsecond latency to sign, transmit, and verify signatures in data center systems. DSig is based on the observation that, in many data center applications, the signer of a message knows most of the time who will verify its signature. We introduce a new hybrid signature scheme that combines cheap single-use hash-based signatures verified in the foreground with traditional signatures pre-verified in the background. Compared to prior state-of-the-art signatures, DSig reduces signing time from 18.9 to 0.7 us and verification time from 35.6 to 5.1 us, while keeping signature transmission time below 2.5 us. Moreover, DSig achieves 2.5x higher signing throughput and 6.9x higher verification throughput than the state of the art. We use DSig to (a) bring auditability to two key-value stores (HERD and Redis) and a financial trading system (based on Liquibook) for 86% lower added latency than the state of the art, and (b) replace signatures in BFT broadcast and BFT replication, reducing their latency by 73% and 69%, respectively","sentences":["Data centers increasingly host mutually distrustful users on shared infrastructure.","A powerful tool to safeguard such users are digital signatures.","Digital signatures have revolutionized Internet-scale applications, but current signatures are too slow for the growing genre of microsecond-scale systems in modern data centers.","We propose DSig, the first digital signature system to achieve single-digit microsecond latency to sign, transmit, and verify signatures in data center systems.","DSig is based on the observation that, in many data center applications, the signer of a message knows most of the time who will verify its signature.","We introduce a new hybrid signature scheme that combines cheap single-use hash-based signatures verified in the foreground with traditional signatures pre-verified in the background.","Compared to prior state-of-the-art signatures, DSig reduces signing time from 18.9 to 0.7 us and verification time from 35.6 to 5.1 us, while keeping signature transmission time below 2.5 us.","Moreover, DSig achieves 2.5x higher signing throughput and 6.9x higher verification throughput than the state of the art.","We use DSig to (a) bring auditability to two key-value stores (HERD and Redis) and a financial trading system (based on Liquibook) for 86% lower added latency than the state of the art, and (b) replace signatures in BFT broadcast and BFT replication, reducing their latency by 73% and 69%, respectively"],"url":"http://arxiv.org/abs/2406.07215v1","category":"cs.CR"}
{"created":"2024-06-11 12:43:53","title":"Robustness of perfect transmission resonances to asymmetric perturbation","abstract":"We investigate the perfect transmission resonances (PTRs) of perturbed 1D finite periodic systems with mirror symmetric cells. The unperturbed scattering region consists of $N$ identical cells and the related transmission spectrum possesses at least $N-1$ PTRs in each pass band of the Bloch dispersion of the unit cell. On the other hand, the perturbation is breaking the periodicity and, a priori, is able to eliminate all the PTRs. We show how PTRs could still appear in the perturbed case with a suitable design of the perturbation. We also reveal a connection between two apparently independent PTRs, a connection that lies in the symmetry of the finite Kronig-Penney systems and which implies that if one PTR is preserved then another one, among the $N-1$ PTRs in a pass band, is necessarily also preserved.","sentences":["We investigate the perfect transmission resonances (PTRs) of perturbed 1D finite periodic systems with mirror symmetric cells.","The unperturbed scattering region consists of $N$ identical cells and the related transmission spectrum possesses at least $N-1$ PTRs in each pass band of the Bloch dispersion of the unit cell.","On the other hand, the perturbation is breaking the periodicity and, a priori, is able to eliminate all the PTRs.","We show how PTRs could still appear in the perturbed case with a suitable design of the perturbation.","We also reveal a connection between two apparently independent PTRs, a connection that lies in the symmetry of the finite Kronig-Penney systems and which implies that if one PTR is preserved then another one, among the $N-1$ PTRs in a pass band, is necessarily also preserved."],"url":"http://arxiv.org/abs/2406.07214v1","category":"quant-ph"}
{"created":"2024-06-11 12:31:22","title":"Database-assisted automata learning","abstract":"This paper presents DAALder (Database-Assisted Automata Learning, with Dutch suffix from leerder), a new algorithm for learning state machines, or automata, specifically deterministic finite-state automata (DFA). When learning state machines from log data originating from software systems, the large amount of log data can pose a challenge. Conventional state merging algorithms cannot efficiently deal with this, as they require a large amount of memory. To solve this, we utilized database technologies to efficiently query a big trace dataset and construct a state machine from it, as databases allow to save large amounts of data on disk while still being able to query it efficiently. Building on research in both active learning and passive learning, the proposed algorithm is a combination of the two. It can quickly find a characteristic set of traces from a database using heuristics from a state merging algorithm. Experiments show that our algorithm has similar performance to conventional state merging algorithms on large datasets, but requires far less memory.","sentences":["This paper presents DAALder (Database-Assisted Automata Learning, with Dutch suffix from leerder), a new algorithm for learning state machines, or automata, specifically deterministic finite-state automata (DFA).","When learning state machines from log data originating from software systems, the large amount of log data can pose a challenge.","Conventional state merging algorithms cannot efficiently deal with this, as they require a large amount of memory.","To solve this, we utilized database technologies to efficiently query a big trace dataset and construct a state machine from it, as databases allow to save large amounts of data on disk while still being able to query it efficiently.","Building on research in both active learning and passive learning, the proposed algorithm is a combination of the two.","It can quickly find a characteristic set of traces from a database using heuristics from a state merging algorithm.","Experiments show that our algorithm has similar performance to conventional state merging algorithms on large datasets, but requires far less memory."],"url":"http://arxiv.org/abs/2406.07208v1","category":"cs.FL"}
{"created":"2024-06-11 12:23:21","title":"Manifestation of superfluidity in atom-number-imbalanced two-component Bose-Einstein condensates","abstract":"Superfluid and dissipative regimes in the dynamics of a two-component quasi-one-dimensional Bose-Einstein condensate (BEC) with unequal atom numbers in the components have been explored. The system supports localized waves of the symbiotic type owing to the same-species repulsion and cross-species attraction. The minority BEC component moves through the majority component and creates excitations. To quantify the emerging excitations we introduce a time-dependent function called disturbance. Through numerical simulations of the coupled Gross-Pitaevskii equations with periodic boundary conditions, we have revealed a critical velocity of the localized wave, above which a transition from superfluid to dissipative regime occurs, evidenced by a sharp increase in the disturbance function. The factors responsible for the discrepancy between the actual critical velocity and the speed of sound, expected from theoretical arguments, have been discussed.","sentences":["Superfluid and dissipative regimes in the dynamics of a two-component quasi-one-dimensional Bose-Einstein condensate (BEC) with unequal atom numbers in the components have been explored.","The system supports localized waves of the symbiotic type owing to the same-species repulsion and cross-species attraction.","The minority BEC component moves through the majority component and creates excitations.","To quantify the emerging excitations we introduce a time-dependent function called disturbance.","Through numerical simulations of the coupled Gross-Pitaevskii equations with periodic boundary conditions, we have revealed a critical velocity of the localized wave, above which a transition from superfluid to dissipative regime occurs, evidenced by a sharp increase in the disturbance function.","The factors responsible for the discrepancy between the actual critical velocity and the speed of sound, expected from theoretical arguments, have been discussed."],"url":"http://arxiv.org/abs/2406.07204v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-11 12:20:01","title":"Exact blow-up profiles for the parabolic-elliptic Keller-Segel system in dimensions $N\\ge 3$","abstract":"In this paper, we obtain the exact blow-up profiles of solutions of the Keller-Segel-Patlak system in the space with dimensions $N\\ge 3$, which solves an open problem proposed by P. Souplet and M. Winkler in 2019. To establish this achievement, we develop the zero number argument for nonlinear equations with unbounded coefficients and construct a family of auxiliary backward self-similar solutions through nontrivial ODE analysis.","sentences":["In this paper, we obtain the exact blow-up profiles of solutions of the Keller-Segel-Patlak system in the space with dimensions $N\\ge 3$, which solves an open problem proposed by P. Souplet and M. Winkler in 2019.","To establish this achievement, we develop the zero number argument for nonlinear equations with unbounded coefficients and construct a family of auxiliary backward self-similar solutions through nontrivial ODE analysis."],"url":"http://arxiv.org/abs/2406.07201v1","category":"math.AP"}
{"created":"2024-06-11 12:19:31","title":"A Multi-step Approach for Minimizing Risk in Decentralized Exchanges","abstract":"Decentralized Exchanges are becoming even more predominant in today's finance. Driven by the need to study this phenomenon from an academic perspective, the SIAG/FME Code Quest 2023 was announced. Specifically, participating teams were asked to implement, in Python, the basic functions of an Automated Market Maker and a liquidity provision strategy in an Automated Market Maker to minimize the Conditional Value at Risk, a critical measure of investment risk. As the competition's winning team, we highlight our approach in this work. In particular, as the dependence of the final return on the initial wealth distribution is highly non-linear, we cannot use standard ad-hoc approaches. Additionally, classical minimization techniques would require a significant computational load due to the cost of the target function. For these reasons, we propose a three-step approach. In the first step, the target function is approximated by a Kernel Ridge Regression. Then, the approximating function is minimized. In the final step, the previously discovered minimum is utilized as the starting point for directly optimizing the desired target function. By using this procedure, we can both reduce the computational complexity and increase the accuracy of the solution. Finally, the overall computational load is further reduced thanks to an algorithmic trick concerning the returns simulation and the usage of Cython.","sentences":["Decentralized Exchanges are becoming even more predominant in today's finance.","Driven by the need to study this phenomenon from an academic perspective, the SIAG/FME Code Quest 2023 was announced.","Specifically, participating teams were asked to implement, in Python, the basic functions of an Automated Market Maker and a liquidity provision strategy in an Automated Market Maker to minimize the Conditional Value at Risk, a critical measure of investment risk.","As the competition's winning team, we highlight our approach in this work.","In particular, as the dependence of the final return on the initial wealth distribution is highly non-linear, we cannot use standard ad-hoc approaches.","Additionally, classical minimization techniques would require a significant computational load due to the cost of the target function.","For these reasons, we propose a three-step approach.","In the first step, the target function is approximated by a Kernel Ridge Regression.","Then, the approximating function is minimized.","In the final step, the previously discovered minimum is utilized as the starting point for directly optimizing the desired target function.","By using this procedure, we can both reduce the computational complexity and increase the accuracy of the solution.","Finally, the overall computational load is further reduced thanks to an algorithmic trick concerning the returns simulation and the usage of Cython."],"url":"http://arxiv.org/abs/2406.07200v1","category":"q-fin.PM"}
{"created":"2024-06-11 12:18:18","title":"Target Speech Diarization with Multimodal Prompts","abstract":"Traditional speaker diarization seeks to detect ``who spoke when'' according to speaker characteristics. Extending to target speech diarization, we detect ``when target event occurs'' according to the semantic characteristics of speech. We propose a novel Multimodal Target Speech Diarization (MM-TSD) framework, which accommodates diverse and multi-modal prompts to specify target events in a flexible and user-friendly manner, including semantic language description, pre-enrolled speech, pre-registered face image, and audio-language logical prompts. We further propose a voice-face aligner module to project human voice and face representation into a shared space. We develop a multi-modal dataset based on VoxCeleb2 for MM-TSD training and evaluation. Additionally, we conduct comparative analysis and ablation studies for each category of prompts to validate the efficacy of each component in the proposed framework. Furthermore, our framework demonstrates versatility in performing various signal processing tasks, including speaker diarization and overlap speech detection, using task-specific prompts. MM-TSD achieves robust and comparable performance as a unified system compared to specialized models. Moreover, MM-TSD shows capability to handle complex conversations for real-world dataset.","sentences":["Traditional speaker diarization seeks to detect ``who spoke when'' according to speaker characteristics.","Extending to target speech diarization, we detect ``when target event occurs'' according to the semantic characteristics of speech.","We propose a novel Multimodal Target Speech Diarization (MM-TSD) framework, which accommodates diverse and multi-modal prompts to specify target events in a flexible and user-friendly manner, including semantic language description, pre-enrolled speech, pre-registered face image, and audio-language logical prompts.","We further propose a voice-face aligner module to project human voice and face representation into a shared space.","We develop a multi-modal dataset based on VoxCeleb2 for MM-TSD training and evaluation.","Additionally, we conduct comparative analysis and ablation studies for each category of prompts to validate the efficacy of each component in the proposed framework.","Furthermore, our framework demonstrates versatility in performing various signal processing tasks, including speaker diarization and overlap speech detection, using task-specific prompts.","MM-TSD achieves robust and comparable performance as a unified system compared to specialized models.","Moreover, MM-TSD shows capability to handle complex conversations for real-world dataset."],"url":"http://arxiv.org/abs/2406.07198v1","category":"eess.AS"}
{"created":"2024-06-11 12:16:50","title":"A numerical model for time-multiplexed Ising machines based on delay-line oscillators","abstract":"Ising machines (IM) have recently been proposed as unconventional hardware-based computation accelerators for solving NP-hard problems. In this work, we present a model for a time-multiplexed IM based on the nonlinear oscillations in a delay line-based resonator and numerically study the effects that the circuit parameters, specifically the compression gain $\\beta_r$ and frequency nonlinearity $\\beta_i$, have on the IM solutions. We find that the likelihood of reaching the global minimum -- the global minimum probability (GMP) -- is the highest for a certain range of $\\beta_r$ and $\\beta_i$ located near the edge of the synchronization region of the oscillators. The optimal range remains unchanged for all tested coupling topologies and network connections. We also observe a sharp transition line in the ($\\beta_i, \\beta_r$) space above which the GMP falls to zero. In all cases, small variations in the natural frequency of the oscillators do not modify the results, allowing us to extend this model to realistic systems.","sentences":["Ising machines (IM) have recently been proposed as unconventional hardware-based computation accelerators for solving NP-hard problems.","In this work, we present a model for a time-multiplexed IM based on the nonlinear oscillations in a delay line-based resonator and numerically study the effects that the circuit parameters, specifically the compression gain $\\beta_r$ and frequency nonlinearity $\\beta_i$, have on the IM solutions.","We find that the likelihood of reaching the global minimum -- the global minimum probability (GMP) -- is the highest for a certain range of $\\beta_r$ and $\\beta_i$ located near the edge of the synchronization region of the oscillators.","The optimal range remains unchanged for all tested coupling topologies and network connections.","We also observe a sharp transition line in the ($\\beta_i, \\beta_r$) space above which the GMP falls to zero.","In all cases, small variations in the natural frequency of the oscillators do not modify the results, allowing us to extend this model to realistic systems."],"url":"http://arxiv.org/abs/2406.07197v1","category":"math-ph"}
{"created":"2024-06-11 12:10:57","title":"Binary asteroid candidates in Gaia DR3 astrometry","abstract":"Asteroids with companions constitute an excellent sample for studying the collisional and dynamical evolution of minor planets. The currently known binary population were discovered by different complementary techniques that produce, for the moment, a strongly biased distribution, especially in a range of intermediate asteroid sizes (approximately 20 to 100 km) where both mutual photometric events and high-resolution adaptive optic imaging are poorly efficient. A totally independent technique of binary asteroid discovery, based on astrometry, can help to reveal new binary systems and populate a range of sizes and separations that remain nearly unexplored. In this work, we describe a dedicated period detection method and its results for the Gaia DR3 data set. This method looks for the presence of a periodic signature in the orbit post-fit residuals. After conservative filtering and validation based on statistical and physical criteria, we are able to present a first sample of astrometric binary candidates, to be confirmed by other observation techniques such as photometric light curves and stellar occultations.","sentences":["Asteroids with companions constitute an excellent sample for studying the collisional and dynamical evolution of minor planets.","The currently known binary population were discovered by different complementary techniques that produce, for the moment, a strongly biased distribution, especially in a range of intermediate asteroid sizes (approximately 20 to 100 km) where both mutual photometric events and high-resolution adaptive optic imaging are poorly efficient.","A totally independent technique of binary asteroid discovery, based on astrometry, can help to reveal new binary systems and populate a range of sizes and separations that remain nearly unexplored.","In this work, we describe a dedicated period detection method and its results for the Gaia DR3 data set.","This method looks for the presence of a periodic signature in the orbit post-fit residuals.","After conservative filtering and validation based on statistical and physical criteria, we are able to present a first sample of astrometric binary candidates, to be confirmed by other observation techniques such as photometric light curves and stellar occultations."],"url":"http://arxiv.org/abs/2406.07195v1","category":"astro-ph.EP"}
{"created":"2024-06-11 12:03:57","title":"MeMSVD: Long-Range Temporal Structure Capturing Using Incremental SVD","abstract":"This paper is on long-term video understanding where the goal is to recognise human actions over long temporal windows (up to minutes long). In prior work, long temporal context is captured by constructing a long-term memory bank consisting of past and future video features which are then integrated into standard (short-term) video recognition backbones through the use of attention mechanisms. Two well-known problems related to this approach are the quadratic complexity of the attention operation and the fact that the whole feature bank must be stored in memory for inference. To address both issues, we propose an alternative to attention-based schemes which is based on a low-rank approximation of the memory obtained using Singular Value Decomposition. Our scheme has two advantages: (a) it reduces complexity by more than an order of magnitude, and (b) it is amenable to an efficient implementation for the calculation of the memory bases in an incremental fashion which does not require the storage of the whole feature bank in memory. The proposed scheme matches or surpasses the accuracy achieved by attention-based mechanisms while being memory-efficient. Through extensive experiments, we demonstrate that our framework generalises to different architectures and tasks, outperforming the state-of-the-art in three datasets.","sentences":["This paper is on long-term video understanding where the goal is to recognise human actions over long temporal windows (up to minutes long).","In prior work, long temporal context is captured by constructing a long-term memory bank consisting of past and future video features which are then integrated into standard (short-term) video recognition backbones through the use of attention mechanisms.","Two well-known problems related to this approach are the quadratic complexity of the attention operation and the fact that the whole feature bank must be stored in memory for inference.","To address both issues, we propose an alternative to attention-based schemes which is based on a low-rank approximation of the memory obtained using Singular Value Decomposition.","Our scheme has two advantages: (a) it reduces complexity by more than an order of magnitude, and (b) it is amenable to an efficient implementation for the calculation of the memory bases in an incremental fashion which does not require the storage of the whole feature bank in memory.","The proposed scheme matches or surpasses the accuracy achieved by attention-based mechanisms while being memory-efficient.","Through extensive experiments, we demonstrate that our framework generalises to different architectures and tasks, outperforming the state-of-the-art in three datasets."],"url":"http://arxiv.org/abs/2406.07191v1","category":"cs.CV"}
{"created":"2024-06-11 12:03:57","title":"Convergence of bi-spatial pullback random attractors and stochastic Liouville type equations for nonautonomous stochastic p-Laplacian lattice system","abstract":"We consider convergence properties of the long-term behaviors with respect to the coefficient of the stochastic term for a nonautonomous stochastic $p$-Laplacian lattice equation with multiplicative noise. First, the upper semi-continuity of pullback random $(\\ell^2,\\ell^q)$-attractor is proved for each $q\\in[1,+\\infty)$. Then, a convergence result of the time-dependent invariant sample Borel probability measures is obtained in $\\ell^2$. Next, we show that the invariant sample measures satisfy a stochastic Liouville type equation and a termwise convergence of the stochastic Liouville type equations is verified. Furthermore, each family of the invariant sample measures is turned out to be a sample statistical solution, which hence also fulfills a convergence consequence.","sentences":["We consider convergence properties of the long-term behaviors with respect to the coefficient of the stochastic term for a nonautonomous stochastic $p$-Laplacian lattice equation with multiplicative noise.","First, the upper semi-continuity of pullback random $(\\ell^2,\\ell^q)$-attractor is proved for each $q\\in[1,+\\infty)$. Then, a convergence result of the time-dependent invariant sample Borel probability measures is obtained in $\\ell^2$. Next, we show that the invariant sample measures satisfy a stochastic Liouville type equation and a termwise convergence of the stochastic Liouville type equations is verified.","Furthermore, each family of the invariant sample measures is turned out to be a sample statistical solution, which hence also fulfills a convergence consequence."],"url":"http://arxiv.org/abs/2406.07192v1","category":"math.PR"}
{"created":"2024-06-11 12:00:50","title":"Quantum Speedup of the Dispersion and Codebook Design Problems","abstract":"We propose new formulations of max-sum and max-min dispersion problems that enable solutions via the Grover adaptive search (GAS) quantum algorithm, offering quadratic speedup. Dispersion problems are combinatorial optimization problems classified as NP-hard, which appear often in coding theory and wireless communications applications involving optimal codebook design. In turn, GAS is a quantum exhaustive search algorithm that can be used to implement full-fledged maximum-likelihood optimal solutions. In conventional naive formulations however, it is typical to rely on a binary vector spaces, resulting in search space sizes prohibitive even for GAS. To circumvent this challenge, we instead formulate the search of optimal dispersion problem over Dicke states, an equal superposition of binary vectors with equal Hamming weights, which significantly reduces the search space leading to a simplification of the quantum circuit via the elimination of penalty terms. Additionally, we propose a method to replace distance coefficients with their ranks, contributing to the reduction of the number of qubits. Our analysis demonstrates that as a result of the proposed techniques a reduction in query complexity compared to the conventional GAS using Hadamard transform is achieved, enhancing the feasibility of the quantum-based solution of the dispersion problem.","sentences":["We propose new formulations of max-sum and max-min dispersion problems that enable solutions via the Grover adaptive search (GAS) quantum algorithm, offering quadratic speedup.","Dispersion problems are combinatorial optimization problems classified as NP-hard, which appear often in coding theory and wireless communications applications involving optimal codebook design.","In turn, GAS is a quantum exhaustive search algorithm that can be used to implement full-fledged maximum-likelihood optimal solutions.","In conventional naive formulations however, it is typical to rely on a binary vector spaces, resulting in search space sizes prohibitive even for GAS.","To circumvent this challenge, we instead formulate the search of optimal dispersion problem over Dicke states, an equal superposition of binary vectors with equal Hamming weights, which significantly reduces the search space leading to a simplification of the quantum circuit via the elimination of penalty terms.","Additionally, we propose a method to replace distance coefficients with their ranks, contributing to the reduction of the number of qubits.","Our analysis demonstrates that as a result of the proposed techniques a reduction in query complexity compared to the conventional GAS using Hadamard transform is achieved, enhancing the feasibility of the quantum-based solution of the dispersion problem."],"url":"http://arxiv.org/abs/2406.07187v1","category":"quant-ph"}
{"created":"2024-06-11 11:53:23","title":"A Well-Balanced Method for an Unstaggered Central Scheme, the two-space Dimensional Case","abstract":"We develop a second-order accurate central scheme for the two-dimensional hyperbolic system of in-homogeneous conservation laws. The main idea behind the scheme is that we combine the well-balanced deviation method with the Kurganov-Tadmor (KT) scheme. The approach satisfies the well-balanced property and retains the advantages of KT scheme: Riemann-solver-free and the avoidance of oversampling on the regions between Riemann-fans. The scheme is implemented and applied to a number of numerical experiments for the Euler equations with gravitational source term and the results are non-oscillatory. Based on the same idea, we construct a semi-discrete scheme where we combine the above two methods and illustrate the maximum principle.","sentences":["We develop a second-order accurate central scheme for the two-dimensional hyperbolic system of in-homogeneous conservation laws.","The main idea behind the scheme is that we combine the well-balanced deviation method with the Kurganov-Tadmor (KT) scheme.","The approach satisfies the well-balanced property and retains the advantages of KT scheme: Riemann-solver-free and the avoidance of oversampling on the regions between Riemann-fans.","The scheme is implemented and applied to a number of numerical experiments for the Euler equations with gravitational source term and the results are non-oscillatory.","Based on the same idea, we construct a semi-discrete scheme where we combine the above two methods and illustrate the maximum principle."],"url":"http://arxiv.org/abs/2406.07185v1","category":"math.NA"}
{"created":"2024-06-11 11:49:57","title":"Modelling formation of stationary periodic patterns in growing population of motile bacteria","abstract":"Biological pattern formation is one of the most intriguing phenomena in nature. Simplest examples of such patterns are represented by travelling waves and stationary periodic patterns which occur during various biological processes including morphogenesis and population dynamics. Formation of these patterns in populations of motile microorganisms such as Dictyostelium discoideum and E. coli have been shown in a number of experimental studies. Conditions for formation of various types of patterns are commonly addressed in mathematical studies of dynamical systems containing diffusive and advection terms. In this work, we do mathematical study of spatio-temporal patterns forming in growing population of chemotactically active bacteria. In particular, we perform linear analysis to find conditions for formation of stationary periodic patterns, and nonlinear (Fourier) analysis to find characteristics, such as amplitude and wavelength, of these patterns. We verify our analytical results by means of numerical simulations.","sentences":["Biological pattern formation is one of the most intriguing phenomena in nature.","Simplest examples of such patterns are represented by travelling waves and stationary periodic patterns which occur during various biological processes including morphogenesis and population dynamics.","Formation of these patterns in populations of motile microorganisms such as Dictyostelium discoideum and E. coli have been shown in a number of experimental studies.","Conditions for formation of various types of patterns are commonly addressed in mathematical studies of dynamical systems containing diffusive and advection terms.","In this work, we do mathematical study of spatio-temporal patterns forming in growing population of chemotactically active bacteria.","In particular, we perform linear analysis to find conditions for formation of stationary periodic patterns, and nonlinear (Fourier) analysis to find characteristics, such as amplitude and wavelength, of these patterns.","We verify our analytical results by means of numerical simulations."],"url":"http://arxiv.org/abs/2406.07182v1","category":"math.AP"}
{"created":"2024-06-11 11:03:21","title":"Machine learning potential for the Cu-W system","abstract":"Combining the excellent thermal and electrical properties of Cu with the high abrasion resistance and thermal stability of W, Cu-W nanoparticle-reinforced metal matrix composites and nano-multilayers (NMLs) are finding applications as brazing fillers and shielding material for plasma and radiation. Due to the large lattice mismatch between fcc Cu and bcc W, these systems have complex interfaces that are beyond the scales suitable for ab initio methods, thus motivating the development of chemically accurate interatomic potentials. Here, a neural network potential (NNP) for Cu-W is developed within the Behler-Parrinello framework using a curated training dataset that captures metallurgically-relevant local atomic environments. The Cu-W NNP accurately predicts (i) the metallurgical properties (elasticity, stacking faults, dislocations, thermodynamic behavior) in elemental Cu and W, (ii) energies and structures of Cu-W intermetallics and solid solutions, and (iii) a range of fcc Cu/bcc W interfaces, and exhibits physically-reasonable behavior for solid W/liquid Cu systems. As will be demonstrated in forthcoming work, this near-ab initio-accurate NNP can be applied to understand complex phenomena involving interface-driven processes and properties in Cu-W composites.","sentences":["Combining the excellent thermal and electrical properties of Cu with the high abrasion resistance and thermal stability of W, Cu-W nanoparticle-reinforced metal matrix composites and nano-multilayers (NMLs) are finding applications as brazing fillers and shielding material for plasma and radiation.","Due to the large lattice mismatch between fcc Cu and bcc W, these systems have complex interfaces that are beyond the scales suitable for ab initio methods, thus motivating the development of chemically accurate interatomic potentials.","Here, a neural network potential (NNP) for Cu-W is developed within the Behler-Parrinello framework using a curated training dataset that captures metallurgically-relevant local atomic environments.","The Cu-W NNP accurately predicts (i) the metallurgical properties (elasticity, stacking faults, dislocations, thermodynamic behavior) in elemental Cu and W, (ii) energies and structures of Cu-W intermetallics and solid solutions, and (iii) a range of fcc Cu/bcc W interfaces, and exhibits physically-reasonable behavior for solid W/liquid Cu systems.","As will be demonstrated in forthcoming work, this near-ab initio-accurate NNP can be applied to understand complex phenomena involving interface-driven processes and properties in Cu-W composites."],"url":"http://arxiv.org/abs/2406.07157v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-11 10:59:23","title":"A portrait of the rotation of Ultra-Cool Dwarfs revealed by TESS","abstract":"This study presents the results of a search for rotation signature in 250 Gaia DR3 Ultra-Cool Dwarfs (UCDs) with TESS light curves. We identified 71 targets with unambiguous periodicities, of which 61 present rotation signatures and a single source behavior, with periods between 0.133 and 5.81 days. Five UCDs show double-dip features, namely variations with two periods, one approximately double or half the other. The remaining ten UCDs with unambiguous variability present a likely non-single behavior. We also found 20 UCDs showing complex behavior in their light curves, with noticeable fluctuations and irregular structure, with a few exhibiting apparent changes in their temporal structure. The remaining 159 targets show noisy light curves corresponding to low-amplitude signals, whose temporal variation cannot be easily identified. The distribution of the UCDs with rotation signature in the CMD diagram points to a lack of rotating objects within about $11.5<M_{G}<12.5$ and $G-G_{RP}<1.5$ separating them into two regimes, one mainly composed of less massive late-M stars with $P_{rot} \\geq 1.0$ d, and another mainly composed of more massive early-M stars with $P_{rot}<1.0$ d. It is important to emphasize that by separating stars into age intervals, one observes that UCDs with $P_{rot} \\geq 1.0$ d tend to be located in regions of younger objects, and, in contrast, those with $P_{rot}<1.0$ d are mainly concentrated in regions of older objects. Whether these trends of stars contrasting the sample separation is physical or produced by observational biases is a question to be verified in future studies.","sentences":["This study presents the results of a search for rotation signature in 250 Gaia DR3 Ultra-Cool Dwarfs (UCDs) with TESS light curves.","We identified 71 targets with unambiguous periodicities, of which 61 present rotation signatures and a single source behavior, with periods between 0.133 and 5.81 days.","Five UCDs show double-dip features, namely variations with two periods, one approximately double or half the other.","The remaining ten UCDs with unambiguous variability present a likely non-single behavior.","We also found 20 UCDs showing complex behavior in their light curves, with noticeable fluctuations and irregular structure, with a few exhibiting apparent changes in their temporal structure.","The remaining 159 targets show noisy light curves corresponding to low-amplitude signals, whose temporal variation cannot be easily identified.","The distribution of the UCDs with rotation signature in the CMD diagram points to a lack of rotating objects within about $11.5<M_{G}<12.5$ and $G-G_{RP}<1.5$ separating them into two regimes, one mainly composed of less massive late-M stars with $P_{rot} \\geq 1.0$ d, and another mainly composed of more massive early-M stars with $P_{rot}<1.0$ d. It is important to emphasize that by separating stars into age intervals, one observes that UCDs with $P_{rot} \\geq 1.0$ d tend to be located in regions of younger objects, and, in contrast, those with $P_{rot}<1.0$ d are mainly concentrated in regions of older objects.","Whether these trends of stars contrasting the sample separation is physical or produced by observational biases is a question to be verified in future studies."],"url":"http://arxiv.org/abs/2406.07154v1","category":"astro-ph.SR"}
{"created":"2024-06-11 10:57:27","title":"High-performance in-vacuum optical system for quantum optics experiments in a Penning-trap","abstract":"Accurate measurements with implications in many branches in Physics have been accessed using Penning traps and conventional techniques within a temperature regime where each eigenmotion of a charged particle is still a classical harmonic oscillator. Cooling the particle directly or indirectly with lasers allows reaching the quantum regime of each oscillator, controlling subtle effects in the precision frontier by detecting photons instead of electric current. In this paper, we present a new in-vacuum optical system designed to detecting 397-nm fluorescence photons from individual calcium ions and Coulomb crystals in a 7-T Penning trap. Based on the outcome of computer simulations, our design shows diffraction-limited performance. The system has been characterized using a single laser-cooled ion as a point-like source, reaching a final resolution of 3.69(3) $\\mu$m and 2.75(3) $\\mu$m for the trap's axial and radial directions, respectively, after correcting aberrations.","sentences":["Accurate measurements with implications in many branches in Physics have been accessed using Penning traps and conventional techniques within a temperature regime where each eigenmotion of a charged particle is still a classical harmonic oscillator.","Cooling the particle directly or indirectly with lasers allows reaching the quantum regime of each oscillator, controlling subtle effects in the precision frontier by detecting photons instead of electric current.","In this paper, we present a new in-vacuum optical system designed to detecting 397-nm fluorescence photons from individual calcium ions and Coulomb crystals in a 7-T Penning trap.","Based on the outcome of computer simulations, our design shows diffraction-limited performance.","The system has been characterized using a single laser-cooled ion as a point-like source, reaching a final resolution of 3.69(3) $\\mu$m and 2.75(3) $\\mu$m for the trap's axial and radial directions, respectively, after correcting aberrations."],"url":"http://arxiv.org/abs/2406.07152v1","category":"quant-ph"}
{"created":"2024-06-11 10:49:11","title":"Astrocytic NMDA Receptors Modulate the Dynamics of Continuous Attractors","abstract":"Neuronal networking supports complex brain functions, with neurotransmitters facilitating communication through chemical synapses. The release probability of neurotransmitters varies and is influenced by pre-synaptic neuronal activity. Recent findings suggest that blocking astrocytic N-Methyl-D-Aspartate (NMDA) receptors reduces this variation. However, the theoretical implications of this reduction on neuronal dynamics have not been thoroughly investigated. Utilizing continuous attractor neural network (CANN) models with short-term synaptic depression (STD), we explore the effects of reduced release probability variation. Our results show that blocking astrocytic NMDA receptors stabilizes attractor states and diminishes their mobility. These insights enhance our understanding of NMDA receptors' role in astrocytes and their broader impact on neural computation and memory, with potential implications for neurological conditions involving NMDA receptor antagonists.","sentences":["Neuronal networking supports complex brain functions, with neurotransmitters facilitating communication through chemical synapses.","The release probability of neurotransmitters varies and is influenced by pre-synaptic neuronal activity.","Recent findings suggest that blocking astrocytic N-Methyl-D-Aspartate (NMDA) receptors reduces this variation.","However, the theoretical implications of this reduction on neuronal dynamics have not been thoroughly investigated.","Utilizing continuous attractor neural network (CANN) models with short-term synaptic depression (STD), we explore the effects of reduced release probability variation.","Our results show that blocking astrocytic NMDA receptors stabilizes attractor states and diminishes their mobility.","These insights enhance our understanding of NMDA receptors' role in astrocytes and their broader impact on neural computation and memory, with potential implications for neurological conditions involving NMDA receptor antagonists."],"url":"http://arxiv.org/abs/2406.07148v1","category":"q-bio.NC"}
{"created":"2024-06-11 10:42:11","title":"From many-body ab initio to effective excitonic models: a versatile mapping approach including environmental embedding effects","abstract":"We present an original multi-state projective diabatization scheme based on the Green's function formalism that allows the systematic mapping of many-body ab initio calculations onto effective excitonic models. This method inherits the ability of the Bethe-Salpeter equation to describe Frenkel molecular excitons and intermolecular charge-transfer states equally well, as well as the possibility for an effective description of environmental effects in a QM/MM framework. The latter is found to be a crucial element in order to obtain accurate model parameters for condensed phases and to ensure their transferability to excitonic models for extended systems. The method is presented through a series of examples illustrating its quality, robustness, and internal consistency.","sentences":["We present an original multi-state projective diabatization scheme based on the Green's function formalism that allows the systematic mapping of many-body ab initio calculations onto effective excitonic models.","This method inherits the ability of the Bethe-Salpeter equation to describe Frenkel molecular excitons and intermolecular charge-transfer states equally well, as well as the possibility for an effective description of environmental effects in a QM/MM framework.","The latter is found to be a crucial element in order to obtain accurate model parameters for condensed phases and to ensure their transferability to excitonic models for extended systems.","The method is presented through a series of examples illustrating its quality, robustness, and internal consistency."],"url":"http://arxiv.org/abs/2406.07143v1","category":"physics.chem-ph"}
{"created":"2024-06-11 10:41:20","title":"Local Time Statistics and Permeable Barrier Crossing: from Poisson to Birth-Death Diffusion Equations","abstract":"Barrier crossing is a widespread phenomenon across natural and engineering systems. While an abundant cross-disciplinary literature on the topic has emerged over the years, the stochastic underpinnings of the process are yet to be fully understood. We fill this knowledge gap by considering a diffusing particle and presenting a stochastic definition of Brownian motion in the presence of a permeable barrier. This definition relies on reflected Brownian motion and on the crossing events being Poisson processes subordinated by the local time of the underlying motion at the barrier. Within this paradigm we derive the exact expression for the distribution of the number of crossings, and find an experimentally measurable statistical definition of permeability. We employ Feynman-Kac theory to derive and solve a set of governing birth-death diffusion equations and extend them to when barrier permeability is asymmetric. As an application we study a system of infinite, identical and periodically placed asymmetric barriers for which we derive analytically effective transport parameters. This periodic arrangement induces an effective drift at long times whose magnitude depends on the difference in the permeability on either side of the barrier as well as on their absolute values. As the asymmetric permeabilities act akin to localised ``ratchet'' potentials that break spatial symmetry and detailed balance, the proposed arrangement of asymmetric barriers provides an example of a noise-induced drift without the need to time-modulate any external force or create temporal correlations on the motion of a diffusing particle.","sentences":["Barrier crossing is a widespread phenomenon across natural and engineering systems.","While an abundant cross-disciplinary literature on the topic has emerged over the years, the stochastic underpinnings of the process are yet to be fully understood.","We fill this knowledge gap by considering a diffusing particle and presenting a stochastic definition of Brownian motion in the presence of a permeable barrier.","This definition relies on reflected Brownian motion and on the crossing events being Poisson processes subordinated by the local time of the underlying motion at the barrier.","Within this paradigm we derive the exact expression for the distribution of the number of crossings, and find an experimentally measurable statistical definition of permeability.","We employ Feynman-Kac theory to derive and solve a set of governing birth-death diffusion equations and extend them to when barrier permeability is asymmetric.","As an application we study a system of infinite, identical and periodically placed asymmetric barriers for which we derive analytically effective transport parameters.","This periodic arrangement induces an effective drift at long times whose magnitude depends on the difference in the permeability on either side of the barrier as well as on their absolute values.","As the asymmetric permeabilities act akin to localised ``ratchet'' potentials that break spatial symmetry and detailed balance, the proposed arrangement of asymmetric barriers provides an example of a noise-induced drift without the need to time-modulate any external force or create temporal correlations on the motion of a diffusing particle."],"url":"http://arxiv.org/abs/2406.07142v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-11 10:12:06","title":"Study of selected mild steels for application in vacuum systems of future gravitational wave detectors","abstract":"Next-generation gravitational wave detectors (GWDs) like the Cosmic Explorer and Einstein Telescope require extensive vacuum tubing, necessitating cost-effective materials. This study explores the viability of mild steel as an alternative to austenitic stainless steel for UHV beampipes, focusing on outgassing rates and surface chemistry after low-temperature bakeouts. Mild steels exhibit significantly lower hydrogen outgassing rates, below 10$^{-14}$ mbar l s$^{-1}$ cm$^{-2}$ after bakeouts at 80{\\deg}C for 48 hours. While water vapor is the primary residual gas after such low-temperature bakeouts, repeated treatments reduce its outgassing rate and modify surface conditions so that such benefit is preserved after at least six months of exposure to laboratory air. These findings position mild steel as an economical and efficient material for future GWD beampipes.","sentences":["Next-generation gravitational wave detectors (GWDs) like the Cosmic Explorer and Einstein Telescope require extensive vacuum tubing, necessitating cost-effective materials.","This study explores the viability of mild steel as an alternative to austenitic stainless steel for UHV beampipes, focusing on outgassing rates and surface chemistry after low-temperature bakeouts.","Mild steels exhibit significantly lower hydrogen outgassing rates, below 10$^{-14}$ mbar l s$^{-1}$ cm$^{-2}$","after bakeouts at 80{\\deg}C for 48 hours.","While water vapor is the primary residual gas after such low-temperature bakeouts, repeated treatments reduce its outgassing rate and modify surface conditions so that such benefit is preserved after at least six months of exposure to laboratory air.","These findings position mild steel as an economical and efficient material for future GWD beampipes."],"url":"http://arxiv.org/abs/2406.07123v1","category":"physics.acc-ph"}
{"created":"2024-06-11 10:01:51","title":"Transport of low regularity Gaussian measures for the 1d quintic nonlinear Schr\u00f6dinger equation","abstract":"We consider the 1d nonlinear Schr\\\"odinger equation (NLS) on the torus with initial data distributed according to the Gaussian measure with covariance operator $(1 - \\Delta)^{-s}$, where $\\Delta$ is the Laplace operator. We prove that the Gaussian measures are quasi-invariant along the flow of (NLS) for the full range $s > \\frac{3}{2}$. This improves a previous result obtained by Planchon, Tzvetkov and Visciglia (in 2019), where the quasi-invariance is proven for $s=2k$, for all integers $k\\geq 1$. In our approach, to prove the quasi-invariance, we directly establish an explicit formula for the Radon-Nikodym derivative $G_s(t,.)$ of the transported measures, which is obtained as the limit of truncated Radon-Nikodym derivatives $G_{s,N}(t,.)$ for transported measures associated with a truncated system. We also prove that the Radon-Nikodym derivatives belong to $L^p$, $p>1$, with respect to $H^1(\\mathbb{T})$-cutoff Gaussian measures, relying on the introduction of weighted Gaussian measures produced by a normal form reduction, following a recent work by Sun and Tzvetkov (in 2023). Additionally, we prove that the truncated densities $G_{s,N}(t,.)$ converges to $G_s(t,.)$ in $L^p$ (with respect to the $H^1(\\mathbb{T})$-cutoff Gaussian measures).","sentences":["We consider the 1d nonlinear Schr\\\"odinger equation (NLS) on the torus with initial data distributed according to the Gaussian measure with covariance operator $(1 - \\Delta)^{-s}$, where $\\Delta$ is the Laplace operator.","We prove that the Gaussian measures are quasi-invariant along the flow of (NLS) for the full range $s > \\frac{3}{2}$.","This improves a previous result obtained by Planchon, Tzvetkov and Visciglia (in 2019), where the quasi-invariance is proven for $s=2k$, for all integers $k\\geq 1$.","In our approach, to prove the quasi-invariance, we directly establish an explicit formula for the Radon-Nikodym derivative $G_s(t,.)$ of the transported measures, which is obtained as the limit of truncated Radon-Nikodym derivatives $G_{s,N}(t,.)$ for transported measures associated with a truncated system.","We also prove that the Radon-Nikodym derivatives belong to $L^p$, $p>1$, with respect to $H^1(\\mathbb{T})$-cutoff Gaussian measures, relying on the introduction of weighted Gaussian measures produced by a normal form reduction, following a recent work by Sun and Tzvetkov (in 2023).","Additionally, we prove that the truncated densities $G_{s,N}(t,.)$ converges to $G_s(t,.)$ in $L^p$ (with respect to the $H^1(\\mathbb{T})$-cutoff Gaussian measures)."],"url":"http://arxiv.org/abs/2406.07116v1","category":"math.AP"}
{"created":"2024-06-11 09:50:42","title":"Solving singular generalized eigenvalue problems. Part III: structure preservation","abstract":"In Parts I and II of this series of papers, three new methods for the computation of eigenvalues of singular pencils were developed: rank-completing perturbations, rank-projections, and augmentation. It was observed that a straightforward structure-preserving adaption for symmetric pencils was not possible and it was left as an open question how to address this challenge. In this Part III, it is shown how the observed issue can be circumvented by using Hermitian perturbations. This leads to structure-preserving analogues of the three techniques from Parts I and II for Hermitian pencils (including real symmetric pencils) as well as for related structures. It is an important feature of these methods that the sign characteristic of the given pencil is preserved. As an application, it is shown that the resulting methods can be used to solve systems of bivariate polynomials.","sentences":["In Parts I and II of this series of papers, three new methods for the computation of eigenvalues of singular pencils were developed: rank-completing perturbations, rank-projections, and augmentation.","It was observed that a straightforward structure-preserving adaption for symmetric pencils was not possible and it was left as an open question how to address this challenge.","In this Part III, it is shown how the observed issue can be circumvented by using Hermitian perturbations.","This leads to structure-preserving analogues of the three techniques from Parts I and II for Hermitian pencils (including real symmetric pencils) as well as for related structures.","It is an important feature of these methods that the sign characteristic of the given pencil is preserved.","As an application, it is shown that the resulting methods can be used to solve systems of bivariate polynomials."],"url":"http://arxiv.org/abs/2406.07109v1","category":"math.NA"}
{"created":"2024-06-11 09:49:47","title":"On the power of adaption and randomization","abstract":"We present bounds between different widths of convex subsets of Banach spaces, including Gelfand and Bernstein widths. Using this, and some relations between widths and minimal errors, we obtain bounds on the maximal gain of adaptive and randomized algorithms over non-adaptive, deterministic ones for approximating linear operators on convex sets. Our results also apply to the approximation of embeddings into the space of bounded functions based on function evaluations, i.e., to sampling recovery in the uniform norm. We conclude with a list of open problems.","sentences":["We present bounds between different widths of convex subsets of Banach spaces, including Gelfand and Bernstein widths.","Using this, and some relations between widths and minimal errors, we obtain bounds on the maximal gain of adaptive and randomized algorithms over non-adaptive, deterministic ones for approximating linear operators on convex sets.","Our results also apply to the approximation of embeddings into the space of bounded functions based on function evaluations, i.e., to sampling recovery in the uniform norm.","We conclude with a list of open problems."],"url":"http://arxiv.org/abs/2406.07108v1","category":"math.NA"}
{"created":"2024-06-11 09:44:22","title":"The Study of the Canonical forms of Killing tensor in vacuum with \u039b","abstract":"This paper is the initial part of a comprehensive study of spacetimes that admit the canonical forms of Killing tensor in General Relativity. Our scope is to derive either new exact solutions of Einstein's equations or to determine the hidden symmetries of the already known ones. In this preliminary work we first introduce the canonical forms of Killing tensor. Subsequently, we employ the integrability conditions of each canonical form along with the Einstein field equations (in vacuum with {\\Lambda}) and the Bianchi identities in an attempt to create a solvable yet overdetermined system of equations. Finally, we obtain multiple special algebraic solutions according to the Petrov classification (D, III, N, O). The latter becomes possible since our analysis is embodied with the usage of the Newman-Penrose formalism of null tetrads.","sentences":["This paper is the initial part of a comprehensive study of spacetimes that admit the canonical forms of Killing tensor in General Relativity.","Our scope is to derive either new exact solutions of Einstein's equations or to determine the hidden symmetries of the already known ones.","In this preliminary work we first introduce the canonical forms of Killing tensor.","Subsequently, we employ the integrability conditions of each canonical form along with the Einstein field equations (in vacuum with {\\Lambda}) and the Bianchi identities in an attempt to create a solvable yet overdetermined system of equations.","Finally, we obtain multiple special algebraic solutions according to the Petrov classification (D, III, N, O).","The latter becomes possible since our analysis is embodied with the usage of the Newman-Penrose formalism of null tetrads."],"url":"http://arxiv.org/abs/2406.07105v1","category":"gr-qc"}
{"created":"2024-06-11 09:43:20","title":"On the extension for Toeplitz matrices of certain Markov inequalities","abstract":"Starting from a doubly infinite sequence of complex numbers, the aim of this paper is to extend certain Markov inequalities for the determinant of Hankel matrices and the zeros of the corresponding orthogonal polynomials on the real line (A. Markov in Notes of the Imperial Academy of Sciences, St. Petersburg, 74 (Appendix n. 2) (1894) 1-30. English translation, by J. Shohat, Duke Math. J. 7 (1940), 85-96) to the Toeplitz case, where the central role is played by CD kernels and paraorthogonal polynomials on the unit circle. In particular, we consider the case in which the starting sequence is a two-sided P\\'olya frequency sequence.","sentences":["Starting from a doubly infinite sequence of complex numbers, the aim of this paper is to extend certain Markov inequalities for the determinant of Hankel matrices and the zeros of the corresponding orthogonal polynomials on the real line (A. Markov in Notes of the Imperial Academy of Sciences, St. Petersburg, 74 (Appendix n. 2) (1894) 1-30.","English translation, by J. Shohat, Duke Math.","J. 7 (1940), 85-96) to the Toeplitz case, where the central role is played by CD kernels and paraorthogonal polynomials on the unit circle.","In particular, we consider the case in which the starting sequence is a two-sided P\\'olya frequency sequence."],"url":"http://arxiv.org/abs/2406.07104v1","category":"math.CA"}
{"created":"2024-06-11 09:40:29","title":"Large amplitude quasi-periodic traveling waves in two dimensional forced rotating fluids","abstract":"We establish the existence of quasi-periodic traveling wave solutions for the $\\beta$-plane equation on $\\mathbb{T}^2$ with a large quasi-periodic traveling wave external force. These solutions exhibit large sizes, which depend on the frequency of oscillations of the external force. Due to the presence of small divisors, the proof relies on a nonlinear Nash-Moser scheme tailored to construct nonlinear waves of large size. To our knowledge, this is the first instance of constructing quasi-periodic solutions for a quasilinear PDE in dimensions greater than one, with a 1-smoothing dispersion relation that is highly degenerate - indicating an infinite-dimensional kernel for the linear principal operator. This degeneracy challenge is overcome by preserving the traveling-wave structure, the conservation of momentum and by implementing normal form methods for the linearized system with sublinear dispersion relation in higher space dimension.","sentences":["We establish the existence of quasi-periodic traveling wave solutions for the $\\beta$-plane equation on $\\mathbb{T}^2$ with a large quasi-periodic traveling wave external force.","These solutions exhibit large sizes, which depend on the frequency of oscillations of the external force.","Due to the presence of small divisors, the proof relies on a nonlinear Nash-Moser scheme tailored to construct nonlinear waves of large size.","To our knowledge, this is the first instance of constructing quasi-periodic solutions for a quasilinear PDE in dimensions greater than one, with a 1-smoothing dispersion relation that is highly degenerate - indicating an infinite-dimensional kernel for the linear principal operator.","This degeneracy challenge is overcome by preserving the traveling-wave structure, the conservation of momentum and by implementing normal form methods for the linearized system with sublinear dispersion relation in higher space dimension."],"url":"http://arxiv.org/abs/2406.07099v1","category":"math.AP"}
{"created":"2024-06-11 09:33:15","title":"Grapevine Disease Prediction Using Climate Variables from Multi-Sensor Remote Sensing Imagery via a Transformer Model","abstract":"Early detection and management of grapevine diseases are important in pursuing sustainable viticulture. This paper introduces a novel framework leveraging the TabPFN model to forecast blockwise grapevine diseases using climate variables from multi-sensor remote sensing imagery. By integrating advanced machine learning techniques with detailed environmental data, our approach significantly enhances the accuracy and efficiency of disease prediction in vineyards. The TabPFN model's experimental evaluations showcase comparable performance to traditional gradient-boosted decision trees, such as XGBoost, CatBoost, and LightGBM. The model's capability to process complex data and provide per-pixel disease-affecting probabilities enables precise, targeted interventions, contributing to more sustainable disease management practices. Our findings underscore the transformative potential of combining Transformer models with remote sensing data in precision agriculture, offering a scalable solution for improving crop health and productivity while reducing environmental impact.","sentences":["Early detection and management of grapevine diseases are important in pursuing sustainable viticulture.","This paper introduces a novel framework leveraging the TabPFN model to forecast blockwise grapevine diseases using climate variables from multi-sensor remote sensing imagery.","By integrating advanced machine learning techniques with detailed environmental data, our approach significantly enhances the accuracy and efficiency of disease prediction in vineyards.","The TabPFN model's experimental evaluations showcase comparable performance to traditional gradient-boosted decision trees, such as XGBoost, CatBoost, and LightGBM.","The model's capability to process complex data and provide per-pixel disease-affecting probabilities enables precise, targeted interventions, contributing to more sustainable disease management practices.","Our findings underscore the transformative potential of combining Transformer models with remote sensing data in precision agriculture, offering a scalable solution for improving crop health and productivity while reducing environmental impact."],"url":"http://arxiv.org/abs/2406.07094v1","category":"cs.IR"}
{"created":"2024-06-11 09:31:39","title":"Truncated pushforwards and refined unramified cohomology","abstract":"For a large class of cohomology theories, we prove that refined unramified cohomology is canonically isomorphic to the hypercohomology of a natural truncated complex of Zariski sheaves. This generalizes a classical result of Bloch and Ogus and solves a conjecture of Kok and Zhou.","sentences":["For a large class of cohomology theories, we prove that refined unramified cohomology is canonically isomorphic to the hypercohomology of a natural truncated complex of Zariski sheaves.","This generalizes a classical result of Bloch and Ogus and solves a conjecture of Kok and Zhou."],"url":"http://arxiv.org/abs/2406.07092v1","category":"math.AG"}
{"created":"2024-06-11 09:31:22","title":"Spoken Language Corpora Augmentation with Domain-Specific Voice-Cloned Speech","abstract":"In this paper we study the impact of augmenting spoken language corpora with domain-specific synthetic samples for the purpose of training a speech recognition system. Using both a conventional neural TTS system and a zero-shot one with voice cloning ability we generate speech corpora that vary in the number of voices. We compare speech recognition models trained with addition of different amounts of synthetic data generated using these two methods with a baseline model trained solely on voice recordings. We show that while the quality of voice-cloned dataset is lower, its increased multivoiceity makes it much more effective than the one with only a few voices synthesized with the use of a conventional neural TTS system. Furthermore, our experiments indicate that using low variability synthetic speech quickly leads to saturation in the quality of the ASR whereas high variability speech provides improvement even when increasing total amount of data used for training by 30%.","sentences":["In this paper we study the impact of augmenting spoken language corpora with domain-specific synthetic samples for the purpose of training a speech recognition system.","Using both a conventional neural TTS system and a zero-shot one with voice cloning ability we generate speech corpora that vary in the number of voices.","We compare speech recognition models trained with addition of different amounts of synthetic data generated using these two methods with a baseline model trained solely on voice recordings.","We show that while the quality of voice-cloned dataset is lower, its increased multivoiceity makes it much more effective than the one with only a few voices synthesized with the use of a conventional neural TTS system.","Furthermore, our experiments indicate that using low variability synthetic speech quickly leads to saturation in the quality of the ASR whereas high variability speech provides improvement even when increasing total amount of data used for training by 30%."],"url":"http://arxiv.org/abs/2406.07090v1","category":"eess.AS"}
{"created":"2024-06-11 09:24:10","title":"One-particle operator representation over two-particle basis sets for relativistic QED computations","abstract":"This work is concerned with two-spin-1/2-fermion relativistic quantum mechanics, and it is about the construction of one-particle projectors using an inherently two(many)-particle, `explicitly correlated' basis representation, necessary for good numerical convergence of the interaction energy. It is demonstrated that a faithful representation of the one-particle operators, which appear in intermediate but essential computational steps, can be constructed over a many-particle basis set by accounting for the full Hilbert space beyond the physically relevant anti-symmetric subspace. Applications of this development can be foreseen for the computation of quantum-electrodynamics corrections for a correlated relativistic reference state and high-precision relativistic computations of medium-to-high-$Z$ helium-like systems, for which other two-particle projection techniques are unreliable.","sentences":["This work is concerned with two-spin-1/2-fermion relativistic quantum mechanics, and it is about the construction of one-particle projectors using an inherently two(many)-particle, `explicitly correlated' basis representation, necessary for good numerical convergence of the interaction energy.","It is demonstrated that a faithful representation of the one-particle operators, which appear in intermediate but essential computational steps, can be constructed over a many-particle basis set by accounting for the full Hilbert space beyond the physically relevant anti-symmetric subspace.","Applications of this development can be foreseen for the computation of quantum-electrodynamics corrections for a correlated relativistic reference state and high-precision relativistic computations of medium-to-high-$Z$ helium-like systems, for which other two-particle projection techniques are unreliable."],"url":"http://arxiv.org/abs/2406.07086v1","category":"physics.chem-ph"}
{"created":"2024-06-11 09:22:39","title":"CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor Segmentation","abstract":"Existing promptable segmentation methods in the medical imaging field primarily consider either textual or visual prompts to segment relevant objects, yet they often fall short when addressing anomalies in medical images, like tumors, which may vary greatly in shape, size, and appearance. Recognizing the complexity of medical scenarios and the limitations of textual or visual prompts, we propose a novel dual-prompt schema that leverages the complementary strengths of visual and textual prompts for segmenting various organs and tumors. Specifically, we introduce CAT, an innovative model that Coordinates Anatomical prompts derived from 3D cropped images with Textual prompts enriched by medical domain knowledge. The model architecture adopts a general query-based design, where prompt queries facilitate segmentation queries for mask prediction. To synergize two types of prompts within a unified framework, we implement a ShareRefiner, which refines both segmentation and prompt queries while disentangling the two types of prompts. Trained on a consortium of 10 public CT datasets, CAT demonstrates superior performance in multiple segmentation tasks. Further validation on a specialized in-house dataset reveals the remarkable capacity of segmenting tumors across multiple cancer stages. This approach confirms that coordinating multimodal prompts is a promising avenue for addressing complex scenarios in the medical domain.","sentences":["Existing promptable segmentation methods in the medical imaging field primarily consider either textual or visual prompts to segment relevant objects, yet they often fall short when addressing anomalies in medical images, like tumors, which may vary greatly in shape, size, and appearance.","Recognizing the complexity of medical scenarios and the limitations of textual or visual prompts, we propose a novel dual-prompt schema that leverages the complementary strengths of visual and textual prompts for segmenting various organs and tumors.","Specifically, we introduce CAT, an innovative model that Coordinates Anatomical prompts derived from 3D cropped images with Textual prompts enriched by medical domain knowledge.","The model architecture adopts a general query-based design, where prompt queries facilitate segmentation queries for mask prediction.","To synergize two types of prompts within a unified framework, we implement a ShareRefiner, which refines both segmentation and prompt queries while disentangling the two types of prompts.","Trained on a consortium of 10 public CT datasets, CAT demonstrates superior performance in multiple segmentation tasks.","Further validation on a specialized in-house dataset reveals the remarkable capacity of segmenting tumors across multiple cancer stages.","This approach confirms that coordinating multimodal prompts is a promising avenue for addressing complex scenarios in the medical domain."],"url":"http://arxiv.org/abs/2406.07085v1","category":"cs.CV"}
{"created":"2024-06-11 09:04:23","title":"Meta-Backscatter: A New ISAC Paradigm for Battery-Free Internet of Things","abstract":"The meta-material sensor has been regarded as a next-generation sensing technology for the battery-free Internet of Things (IoT) due to its battery-free characteristic and improved sensing performance. The meta-material sensors function as backscatter tags that change their reflection coefficients with the conditions of sensing targets such as temperature and gas concentration, allowing transceivers to perform sensing by analyzing the reflected signals from the sensors. Simultaneously, the sensors also function as environmental scatterers, creating additional signal paths to enhance communication performance. Therefore, the meta-material sensor potentially provides a new paradigm of Integrated Sensing and Communication (ISAC) for the battery-free IoT system. In this article, we first propose a Meta-Backscatter system that utilizes meta-material sensors to achieve diverse sensing functionalities and improved communication performance. We begin with the introduction of the metamaterial sensor and further elaborate on the Meta-Backscatter system. Subsequently, we present optimization strategies for meta-material sensors, transmitters, and receivers to strike a balance between sensing and communication. Furthermore, this article provides a case study of the system and examines the feasibility and trade-off through the simulation results. Finally, potential extensions of the system and their related research challenges are addressed.","sentences":["The meta-material sensor has been regarded as a next-generation sensing technology for the battery-free Internet of Things (IoT) due to its battery-free characteristic and improved sensing performance.","The meta-material sensors function as backscatter tags that change their reflection coefficients with the conditions of sensing targets such as temperature and gas concentration, allowing transceivers to perform sensing by analyzing the reflected signals from the sensors.","Simultaneously, the sensors also function as environmental scatterers, creating additional signal paths to enhance communication performance.","Therefore, the meta-material sensor potentially provides a new paradigm of Integrated Sensing and Communication (ISAC) for the battery-free IoT system.","In this article, we first propose a Meta-Backscatter system that utilizes meta-material sensors to achieve diverse sensing functionalities and improved communication performance.","We begin with the introduction of the metamaterial sensor and further elaborate on the Meta-Backscatter system.","Subsequently, we present optimization strategies for meta-material sensors, transmitters, and receivers to strike a balance between sensing and communication.","Furthermore, this article provides a case study of the system and examines the feasibility and trade-off through the simulation results.","Finally, potential extensions of the system and their related research challenges are addressed."],"url":"http://arxiv.org/abs/2406.07077v1","category":"eess.SY"}
{"created":"2024-06-11 08:59:34","title":"Adaptive Control: Algorithms, Analysis and Applications","abstract":"Adaptive control provides techniques for adjusting control parameters in real time to maintain system performance despite unknown or changing process parameters. These methods use real data to tune controllers and adjust plant models or controller parameters. The field has progressed significantly since the 1970s, helped by digital computers. Early applications offered essential feedback, and theoretical advances solved many basic problems.   This book comprehensively treats adaptive control, guiding readers from basic problems to analytical solutions with practical applications. Presenting a unified view is challenging due to various design steps and applications. However, a coherent presentation of basic techniques is now possible. The book uses a discrete-time approach to reflect the role of digital computers and shares practical experiences and understanding of different control designs.   Mathematical aspects of synthesizing and analyzing algorithms are emphasized, though they alone may not solve practical problems. The book includes applications of control techniques but stresses that a solid mathematical understanding is crucial for creatively applying them to new challenges. Mathematical synthesis and analysis are highlighted, but they must be supplemented with practical problem-solving and algorithm modifications for specific applications.","sentences":["Adaptive control provides techniques for adjusting control parameters in real time to maintain system performance despite unknown or changing process parameters.","These methods use real data to tune controllers and adjust plant models or controller parameters.","The field has progressed significantly since the 1970s, helped by digital computers.","Early applications offered essential feedback, and theoretical advances solved many basic problems.   ","This book comprehensively treats adaptive control, guiding readers from basic problems to analytical solutions with practical applications.","Presenting a unified view is challenging due to various design steps and applications.","However, a coherent presentation of basic techniques is now possible.","The book uses a discrete-time approach to reflect the role of digital computers and shares practical experiences and understanding of different control designs.   ","Mathematical aspects of synthesizing and analyzing algorithms are emphasized, though they alone may not solve practical problems.","The book includes applications of control techniques but stresses that a solid mathematical understanding is crucial for creatively applying them to new challenges.","Mathematical synthesis and analysis are highlighted, but they must be supplemented with practical problem-solving and algorithm modifications for specific applications."],"url":"http://arxiv.org/abs/2406.07073v1","category":"eess.SY"}
{"created":"2024-06-11 08:58:59","title":"Is Stateful Fuzzing Really Challenging?","abstract":"Fuzzing has been proven extremely effective in finding vulnerabilities in software. When it comes to fuzz stateless systems, analysts have no doubts about the choice to make. In fact, among the plethora of stateless fuzzers devised in the last 20 years, AFL (with its descendants AFL++ and LibAFL) stood up for its effectiveness, speed and ability to find bugs. On the other hand, when dealing with stateful systems, it is not clear what is the best tool to use. In fact, the research community struggles to devise (and benchmark) effective and generic stateful fuzzers. In this short paper, we discuss the reasons that make stateful fuzzers difficult to devise and benchmark.","sentences":["Fuzzing has been proven extremely effective in finding vulnerabilities in software.","When it comes to fuzz stateless systems, analysts have no doubts about the choice to make.","In fact, among the plethora of stateless fuzzers devised in the last 20 years, AFL (with its descendants AFL++","and LibAFL) stood up for its effectiveness, speed and ability to find bugs.","On the other hand, when dealing with stateful systems, it is not clear what is the best tool to use.","In fact, the research community struggles to devise (and benchmark) effective and generic stateful fuzzers.","In this short paper, we discuss the reasons that make stateful fuzzers difficult to devise and benchmark."],"url":"http://arxiv.org/abs/2406.07071v1","category":"cs.SE"}
{"created":"2024-06-11 08:56:08","title":"Optimal Gait Control for a Tendon-driven Soft Quadruped Robot by Model-based Reinforcement Learning","abstract":"This study presents an innovative approach to optimal gait control for a soft quadruped robot enabled by four Compressible Tendon-driven Soft Actuators (CTSAs). Improving our previous studies of using model-free reinforcement learning for gait control, we employ model-based reinforcement learning (MBRL) to further enhance the performance of the gait controller. Compared to rigid robots, the proposed soft quadruped robot has better safety, less weight, and a simpler mechanism for fabrication and control. However, the primary challenge lies in developing sophisticated control algorithms to attain optimal gait control for fast and stable locomotion. The research employs a multi-stage methodology, including state space restriction, data-driven model training, and reinforcement learning algorithm development. Compared to benchmark methods, the proposed MBRL algorithm, combined with post-training, significantly improves the efficiency and performance of gait control policies. The developed policy is both robust and adaptable to the robot's deformable morphology. The study concludes by highlighting the practical applicability of these findings in real-world scenarios.","sentences":["This study presents an innovative approach to optimal gait control for a soft quadruped robot enabled by four Compressible Tendon-driven Soft Actuators (CTSAs).","Improving our previous studies of using model-free reinforcement learning for gait control, we employ model-based reinforcement learning (MBRL) to further enhance the performance of the gait controller.","Compared to rigid robots, the proposed soft quadruped robot has better safety, less weight, and a simpler mechanism for fabrication and control.","However, the primary challenge lies in developing sophisticated control algorithms to attain optimal gait control for fast and stable locomotion.","The research employs a multi-stage methodology, including state space restriction, data-driven model training, and reinforcement learning algorithm development.","Compared to benchmark methods, the proposed MBRL algorithm, combined with post-training, significantly improves the efficiency and performance of gait control policies.","The developed policy is both robust and adaptable to the robot's deformable morphology.","The study concludes by highlighting the practical applicability of these findings in real-world scenarios."],"url":"http://arxiv.org/abs/2406.07069v1","category":"cs.RO"}
{"created":"2024-06-11 08:55:17","title":"Emergent Moir\u00e9 fringes in direct-grown quasicrystal","abstract":"Quasicrystals represent a category of rarely structured solids that challenge traditional periodicity in crystal materials. Recent advancements in the synthesis of two-dimensional (2D) van der Waals materials have paved the way for exploring the unique physical properties of these systems. Here, we report on the synthesis of 2D quasicrystals featuring 30{\\deg} alternating twist angles between multiple graphene layers, using chemical vapor deposition (CVD). Strikingly, we observed periodic Moir\\'e patterns in the quasicrystal, a finding that has not been previously reported in traditional alloy-based quasicrystals. The Moir\\'e periodicity, varying with the parity of the constituent layers, aligns with the theoretical predictions that suggest a stress cancellation mechanism in force. The emergence of Moir\\'e fringes is attributed to the spontaneous mismatched lattice constant in the oriented graphene layers, proving the existence of atomic relaxation. This phenomenon, which has been largely understudied in graphene systems with large twist angles, has now been validated through our use of scanning transmission electron microscopy (STEM). Our CVD-grown Moir\\'e quasicrystal provides an ideal platform for exploring the unusual physical properties that arise from Moir\\'e periodicity within quasicrystals.","sentences":["Quasicrystals represent a category of rarely structured solids that challenge traditional periodicity in crystal materials.","Recent advancements in the synthesis of two-dimensional (2D) van der Waals materials have paved the way for exploring the unique physical properties of these systems.","Here, we report on the synthesis of 2D quasicrystals featuring 30{\\deg} alternating twist angles between multiple graphene layers, using chemical vapor deposition (CVD).","Strikingly, we observed periodic Moir\\'e patterns in the quasicrystal, a finding that has not been previously reported in traditional alloy-based quasicrystals.","The Moir\\'e periodicity, varying with the parity of the constituent layers, aligns with the theoretical predictions that suggest a stress cancellation mechanism in force.","The emergence of Moir\\'e fringes is attributed to the spontaneous mismatched lattice constant in the oriented graphene layers, proving the existence of atomic relaxation.","This phenomenon, which has been largely understudied in graphene systems with large twist angles, has now been validated through our use of scanning transmission electron microscopy (STEM).","Our CVD-grown Moir\\'e quasicrystal provides an ideal platform for exploring the unusual physical properties that arise from Moir\\'e periodicity within quasicrystals."],"url":"http://arxiv.org/abs/2406.07068v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-11 08:50:55","title":"Inferring the dependence graph density of binary graphical models in high dimension","abstract":"We consider a system of binary interacting chains describing the dynamics of a group of $N$ components that, at each time unit, either send some signal to the others or remain silent otherwise. The interactions among the chains are encoded by a directed Erd\\\"os-R\\'enyi random graph with unknown parameter $ p \\in (0, 1) .$ Moreover, the system is structured within two populations (excitatory chains versus inhibitory ones) which are coupled via a mean field interaction on the underlying Erd\\\"os-R\\'enyi graph. In this paper, we address the question of inferring the connectivity parameter $p$ based only on the observation of the interacting chains over $T$ time units. In our main result, we show that the connectivity parameter $p$ can be estimated with rate $N^{-1/2}+N^{1/2}/T+(\\log(T)/T)^{1/2}$ through an easy-to-compute estimator. Our analysis relies on a precise study of the spatio-temporal decay of correlations of the interacting chains. This is done through the study of coalescing random walks defining a backward regeneration representation of the system. Interestingly, we also show that this backward regeneration representation allows us to perfectly sample the system of interacting chains (conditionally on each realization of the underlying Erd\\\"os-R\\'enyi graph) from its stationary distribution. These probabilistic results have an interest in its own.","sentences":["We consider a system of binary interacting chains describing the dynamics of a group of $N$ components that, at each time unit, either send some signal to the others or remain silent otherwise.","The interactions among the chains are encoded by a directed Erd\\\"os-R\\'enyi random graph with unknown parameter $ p \\in (0, 1) .$","Moreover, the system is structured within two populations (excitatory chains versus inhibitory ones) which are coupled via a mean field interaction on the underlying Erd\\\"os-R\\'enyi graph.","In this paper, we address the question of inferring the connectivity parameter $p$ based only on the observation of the interacting chains over $T$ time units.","In our main result, we show that the connectivity parameter $p$ can be estimated with rate $N^{-1/2}+N^{1/2}/T+(\\log(T)/T)^{1/2}$ through an easy-to-compute estimator.","Our analysis relies on a precise study of the spatio-temporal decay of correlations of the interacting chains.","This is done through the study of coalescing random walks defining a backward regeneration representation of the system.","Interestingly, we also show that this backward regeneration representation allows us to perfectly sample the system of interacting chains (conditionally on each realization of the underlying Erd\\\"os-R\\'enyi graph) from its stationary distribution.","These probabilistic results have an interest in its own."],"url":"http://arxiv.org/abs/2406.07066v1","category":"math.ST"}
{"created":"2024-06-11 08:47:02","title":"Optimal Gait Design for a Soft Quadruped Robot via Multi-fidelity Bayesian Optimization","abstract":"This study focuses on the locomotion capability improvement in a tendon-driven soft quadruped robot through an online adaptive learning approach. Leveraging the inverse kinematics model of the soft quadruped robot, we employ a central pattern generator to design a parametric gait pattern, and use Bayesian optimization (BO) to find the optimal parameters. Further, to address the challenges of modeling discrepancies, we implement a multi-fidelity BO approach, combining data from both simulation and physical experiments throughout training and optimization. This strategy enables the adaptive refinement of the gait pattern and ensures a smooth transition from simulation to real-world deployment for the controller. Moreover, we integrate a computational task off-loading architecture by edge computing, which reduces the onboard computational and memory overhead, to improve real-time control performance and facilitate an effective online learning process. The proposed approach successfully achieves optimal walking gait design for physical deployment with high efficiency, effectively addressing challenges related to the reality gap in soft robotics.","sentences":["This study focuses on the locomotion capability improvement in a tendon-driven soft quadruped robot through an online adaptive learning approach.","Leveraging the inverse kinematics model of the soft quadruped robot, we employ a central pattern generator to design a parametric gait pattern, and use Bayesian optimization (BO) to find the optimal parameters.","Further, to address the challenges of modeling discrepancies, we implement a multi-fidelity BO approach, combining data from both simulation and physical experiments throughout training and optimization.","This strategy enables the adaptive refinement of the gait pattern and ensures a smooth transition from simulation to real-world deployment for the controller.","Moreover, we integrate a computational task off-loading architecture by edge computing, which reduces the onboard computational and memory overhead, to improve real-time control performance and facilitate an effective online learning process.","The proposed approach successfully achieves optimal walking gait design for physical deployment with high efficiency, effectively addressing challenges related to the reality gap in soft robotics."],"url":"http://arxiv.org/abs/2406.07065v1","category":"cs.RO"}
{"created":"2024-06-11 08:37:31","title":"Adaptive quantum optimization algorithms for programmable atom-cavity systems","abstract":"Developing quantum algorithms adaptive to specific constraints of near-term devices is an essential step towards practical quantum advantage. In a recent work [Phys. Rev. Lett. 131, 103601(2023)], we show cold atoms in an optical cavity can be built as a universal quantum optimizer with programmable all-to-all interactions, and the effective Hamiltonian for atoms directly encodes number partitioning problems (NPPs). Here, we numerically investigate the performance of quantum annealing (QA) and quantum approximate optimization algorithm (QAOA) to find the solution of NPP that is encoded in the ground state of atomic qubits. We find the success probability of the standard QA decays rapidly with the problem size. The optimized annealing path or inhomogeneous driving fields only lead to mild improvement on the success probability. Similarly, the standard QAOA always gets trapped in a false local minimum, and there is no significant performance improvement as we increase the depth of the quantum circuit. Inspired by the counterdiabatic driving, we propose an adaptive ansatz of QAOA which releases the parameter freedom of the NPP Hamiltonian to match higher-order counterdiabatic terms. Through numerical simulations, we find that our adaptive QAOA can achieve the optimal solution within very small circuit depth. It is thus worth paying the extra optimization cost of additional parameters for improving QAOA performance. Therefore, our adaptive QAOA provides a promising choice for programmable atom-cavity systems to demonstrate competitive computational power within its quantum coherence time.","sentences":["Developing quantum algorithms adaptive to specific constraints of near-term devices is an essential step towards practical quantum advantage.","In a recent work [Phys. Rev. Lett.","131, 103601(2023)], we show cold atoms in an optical cavity can be built as a universal quantum optimizer with programmable all-to-all interactions, and the effective Hamiltonian for atoms directly encodes number partitioning problems (NPPs).","Here, we numerically investigate the performance of quantum annealing (QA) and quantum approximate optimization algorithm (QAOA) to find the solution of NPP that is encoded in the ground state of atomic qubits.","We find the success probability of the standard QA decays rapidly with the problem size.","The optimized annealing path or inhomogeneous driving fields only lead to mild improvement on the success probability.","Similarly, the standard QAOA always gets trapped in a false local minimum, and there is no significant performance improvement as we increase the depth of the quantum circuit.","Inspired by the counterdiabatic driving, we propose an adaptive ansatz of QAOA which releases the parameter freedom of the NPP Hamiltonian to match higher-order counterdiabatic terms.","Through numerical simulations, we find that our adaptive QAOA can achieve the optimal solution within very small circuit depth.","It is thus worth paying the extra optimization cost of additional parameters for improving QAOA performance.","Therefore, our adaptive QAOA provides a promising choice for programmable atom-cavity systems to demonstrate competitive computational power within its quantum coherence time."],"url":"http://arxiv.org/abs/2406.07055v1","category":"quant-ph"}
{"created":"2024-06-11 08:35:23","title":"TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs","abstract":"Large Language Models (LLMs) have immense potential to transform the telecommunications industry. They could help professionals understand complex standards, generate code, and accelerate development. However, traditional LLMs struggle with the precision and source verification essential for telecom work. To address this, specialized LLM-based solutions tailored to telecommunication standards are needed. Retrieval-augmented generation (RAG) offers a way to create precise, fact-based answers. This paper proposes TelecomRAG, a framework for a Telecommunication Standards Assistant that provides accurate, detailed, and verifiable responses. Our implementation, using a knowledge base built from 3GPP Release 16 and Release 18 specification documents, demonstrates how this assistant surpasses generic LLMs, offering superior accuracy, technical depth, and verifiability, and thus significant value to the telecommunications field.","sentences":["Large Language Models (LLMs) have immense potential to transform the telecommunications industry.","They could help professionals understand complex standards, generate code, and accelerate development.","However, traditional LLMs struggle with the precision and source verification essential for telecom work.","To address this, specialized LLM-based solutions tailored to telecommunication standards are needed.","Retrieval-augmented generation (RAG) offers a way to create precise, fact-based answers.","This paper proposes TelecomRAG, a framework for a Telecommunication Standards Assistant that provides accurate, detailed, and verifiable responses.","Our implementation, using a knowledge base built from 3GPP Release 16 and Release 18 specification documents, demonstrates how this assistant surpasses generic LLMs, offering superior accuracy, technical depth, and verifiability, and thus significant value to the telecommunications field."],"url":"http://arxiv.org/abs/2406.07053v1","category":"cs.NI"}
{"created":"2024-06-11 08:34:59","title":"MPSDynamics.jl: Tensor network simulations for finite-temperature (non-Markovian) open quantum system dynamics","abstract":"The MPSDynamics.jl package provides an easy to use interface for performing open quantum systems simulations at zero and finite temperatures. The package has been developed with the aim of studying non-Markovian open system dynamics using the state-of-the-art numerically exact Thermalized-Time Evolving Density operator with Orthonormal Polynomials Algorithm (T-TEDOPA) based on environment chain mapping. The simulations rely on a tensor network representation of the quantum states as matrix product states (MPS) and tree tensor network (TTN) states. Written in the Julia programming language, MPSDynamics.jl is a versatile open-source package providing a choice of several variants of the Time-Dependent Variational Principle (TDVP) method for time evolution (including novel bond-adaptive one-site algorithms). The package also provides strong support for the measurement of single and multi-site observables, as well as the storing and logging of data, which makes it a useful tool for the study of many-body physics. It currently handles long-range interactions, time-dependent Hamiltonians, multiple environments, bosonic and fermionic environments, and joint system-environment observables.","sentences":["The MPSDynamics.jl package provides an easy to use interface for performing open quantum systems simulations at zero and finite temperatures.","The package has been developed with the aim of studying non-Markovian open system dynamics using the state-of-the-art numerically exact Thermalized-Time Evolving Density operator with Orthonormal Polynomials Algorithm (T-TEDOPA) based on environment chain mapping.","The simulations rely on a tensor network representation of the quantum states as matrix product states (MPS) and tree tensor network (TTN) states.","Written in the Julia programming language, MPSDynamics.jl is a versatile open-source package providing a choice of several variants of the Time-Dependent Variational Principle (TDVP) method for time evolution (including novel bond-adaptive one-site algorithms).","The package also provides strong support for the measurement of single and multi-site observables, as well as the storing and logging of data, which makes it a useful tool for the study of many-body physics.","It currently handles long-range interactions, time-dependent Hamiltonians, multiple environments, bosonic and fermionic environments, and joint system-environment observables."],"url":"http://arxiv.org/abs/2406.07052v1","category":"quant-ph"}
{"created":"2024-06-11 08:26:42","title":"DualMamba: A Lightweight Spectral-Spatial Mamba-Convolution Network for Hyperspectral Image Classification","abstract":"The effectiveness and efficiency of modeling complex spectral-spatial relations are both crucial for Hyperspectral image (HSI) classification. Most existing methods based on CNNs and transformers still suffer from heavy computational burdens and have room for improvement in capturing the global-local spectral-spatial feature representation. To this end, we propose a novel lightweight parallel design called lightweight dual-stream Mamba-convolution network (DualMamba) for HSI classification. Specifically, a parallel lightweight Mamba and CNN block are first developed to extract global and local spectral-spatial features. First, the cross-attention spectral-spatial Mamba module is proposed to leverage the global modeling of Mamba at linear complexity. Within this module, dynamic positional embedding is designed to enhance the spatial location information of visual sequences. The lightweight spectral/spatial Mamba blocks comprise an efficient scanning strategy and a lightweight Mamba design to efficiently extract global spectral-spatial features. And the cross-attention spectral-spatial fusion is designed to learn cross-correlation and fuse spectral-spatial features. Second, the lightweight spectral-spatial residual convolution module is proposed with lightweight spectral and spatial branches to extract local spectral-spatial features through residual learning. Finally, the adaptive global-local fusion is proposed to dynamically combine global Mamba features and local convolution features for a global-local spectral-spatial representation. Compared with state-of-the-art HSI classification methods, experimental results demonstrate that DualMamba achieves significant classification accuracy on three public HSI datasets and a superior reduction in model parameters and floating point operations (FLOPs).","sentences":["The effectiveness and efficiency of modeling complex spectral-spatial relations are both crucial for Hyperspectral image (HSI) classification.","Most existing methods based on CNNs and transformers still suffer from heavy computational burdens and have room for improvement in capturing the global-local spectral-spatial feature representation.","To this end, we propose a novel lightweight parallel design called lightweight dual-stream Mamba-convolution network (DualMamba) for HSI classification.","Specifically, a parallel lightweight Mamba and CNN block are first developed to extract global and local spectral-spatial features.","First, the cross-attention spectral-spatial Mamba module is proposed to leverage the global modeling of Mamba at linear complexity.","Within this module, dynamic positional embedding is designed to enhance the spatial location information of visual sequences.","The lightweight spectral/spatial Mamba blocks comprise an efficient scanning strategy and a lightweight Mamba design to efficiently extract global spectral-spatial features.","And the cross-attention spectral-spatial fusion is designed to learn cross-correlation and fuse spectral-spatial features.","Second, the lightweight spectral-spatial residual convolution module is proposed with lightweight spectral and spatial branches to extract local spectral-spatial features through residual learning.","Finally, the adaptive global-local fusion is proposed to dynamically combine global Mamba features and local convolution features for a global-local spectral-spatial representation.","Compared with state-of-the-art HSI classification methods, experimental results demonstrate that DualMamba achieves significant classification accuracy on three public HSI datasets and a superior reduction in model parameters and floating point operations (FLOPs)."],"url":"http://arxiv.org/abs/2406.07050v1","category":"cs.CV"}
{"created":"2024-06-11 17:58:53","title":"Transforming a rare event search into a not-so-rare event search in real-time with deep learning-based object detection","abstract":"Deep learning-based object detection algorithms enable the simultaneous classification and localization of any number of objects in image data. Many of these algorithms are capable of operating in real-time on high resolution images, attributing to their widespread usage across many fields. We present an end-to-end object detection pipeline designed for real-time rare event searches for the Migdal effect, using high-resolution image data from a state-of-the-art scientific CMOS camera in the MIGDAL experiment. The Migdal effect in nuclear scattering, crucial for sub-GeV dark matter searches, has yet to be experimentally confirmed, making its detection a primary goal of the MIGDAL experiment. Our pipeline employs the YOLOv8 object detection algorithm and is trained on real data to enhance the detection efficiency of nuclear and electronic recoils, particularly those exhibiting overlapping tracks that are indicative of the Migdal effect. When deployed online on the MIGDAL readout PC, we demonstrate our pipeline to process and perform the rare event search on 2D image data faster than the peak 120 frame per second acquisition rate of the CMOS camera. Applying these same steps offline, we demonstrate that we can reduce a sample of 20 million camera frames to around 1000 frames while maintaining nearly all signal that YOLOv8 is able to detect, thereby transforming a rare search into a much more manageable search. Our studies highlight the potential of pipelines similar to ours significantly improving the detection capabilities of experiments requiring rapid and precise object identification in high-throughput data environments.","sentences":["Deep learning-based object detection algorithms enable the simultaneous classification and localization of any number of objects in image data.","Many of these algorithms are capable of operating in real-time on high resolution images, attributing to their widespread usage across many fields.","We present an end-to-end object detection pipeline designed for real-time rare event searches for the Migdal effect, using high-resolution image data from a state-of-the-art scientific CMOS camera in the MIGDAL experiment.","The Migdal effect in nuclear scattering, crucial for sub-GeV dark matter searches, has yet to be experimentally confirmed, making its detection a primary goal of the MIGDAL experiment.","Our pipeline employs the YOLOv8 object detection algorithm and is trained on real data to enhance the detection efficiency of nuclear and electronic recoils, particularly those exhibiting overlapping tracks that are indicative of the Migdal effect.","When deployed online on the MIGDAL readout PC, we demonstrate our pipeline to process and perform the rare event search on 2D image data faster than the peak 120 frame per second acquisition rate of the CMOS camera.","Applying these same steps offline, we demonstrate that we can reduce a sample of 20 million camera frames to around 1000 frames while maintaining nearly all signal that YOLOv8 is able to detect, thereby transforming a rare search into a much more manageable search.","Our studies highlight the potential of pipelines similar to ours significantly improving the detection capabilities of experiments requiring rapid and precise object identification in high-throughput data environments."],"url":"http://arxiv.org/abs/2406.07538v1","category":"hep-ex"}
{"created":"2024-06-11 17:55:25","title":"MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation","abstract":"Model merging has emerged as an effective approach to combine multiple single-task models, fine-tuned from the same pre-trained model, into a multitask model. This process typically involves computing a weighted average of the model parameters without any additional training. Existing model-merging methods focus on enhancing average task accuracy. However, interference and conflicts between the objectives of different tasks can lead to trade-offs during model merging. In real-world applications, a set of solutions with various trade-offs can be more informative, helping practitioners make decisions based on diverse preferences. In this paper, we introduce a novel low-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP identifies a Pareto set of scaling coefficients for merging multiple models to reflect the trade-offs. The core component of MAP is approximating the evaluation metrics of the various tasks using a quadratic approximation surrogate model derived from a pre-selected set of scaling coefficients, enabling amortized inference. Experimental results on vision and natural language processing tasks show that MAP can accurately identify the Pareto front. To further reduce the required computation of MAP, we propose (1) a Bayesian adaptive sampling algorithm and (2) a nested merging scheme with multiple stages.","sentences":["Model merging has emerged as an effective approach to combine multiple single-task models, fine-tuned from the same pre-trained model, into a multitask model.","This process typically involves computing a weighted average of the model parameters without any additional training.","Existing model-merging methods focus on enhancing average task accuracy.","However, interference and conflicts between the objectives of different tasks can lead to trade-offs during model merging.","In real-world applications, a set of solutions with various trade-offs can be more informative, helping practitioners make decisions based on diverse preferences.","In this paper, we introduce a novel low-compute algorithm, Model Merging with Amortized Pareto Front (MAP).","MAP identifies a Pareto set of scaling coefficients for merging multiple models to reflect the trade-offs.","The core component of MAP is approximating the evaluation metrics of the various tasks using a quadratic approximation surrogate model derived from a pre-selected set of scaling coefficients, enabling amortized inference.","Experimental results on vision and natural language processing tasks show that MAP can accurately identify the Pareto front.","To further reduce the required computation of MAP, we propose (1) a Bayesian adaptive sampling algorithm and (2) a nested merging scheme with multiple stages."],"url":"http://arxiv.org/abs/2406.07529v1","category":"cs.LG"}
{"created":"2024-06-11 17:51:12","title":"Change of numeraire for weak martingale transport","abstract":"Change of numeraire is a classical tool in mathematical finance. Campi-Laachir-Martini established its applicability to martingale optimal transport. We note that the results of Campi-Laachir-Martini extend to the case of weak martingale transport. We apply this to shadow couplings, continuous time martingale transport problems in the framework of Huesmann-Trevisan and in particular to establish the correspondence between stretched Brownian motion with its geometric counterpart.   Note: We emphasize that we learned about the geometric stretched Brownian motion gSBM (defined in PDE terms) in a presentation of Loeper \\cite{Lo23} before our work on this topic started. We noticed that a change of numeraire transformation in the spirit of \\cite{CaLaMa14} allows for an alternative viewpoint in the weak optimal transport framework. We make our work public following the publication of Backhoff-Loeper-Obloj's work \\cite{BaLoOb24} on arxiv.org. The article \\cite{BaLoOb24} derives gSBM using PDE techniques as well as through an independent probabilistic approach which is close to the one we give in the present article.","sentences":["Change of numeraire is a classical tool in mathematical finance.","Campi-Laachir-Martini established its applicability to martingale optimal transport.","We note that the results of Campi-Laachir-Martini extend to the case of weak martingale transport.","We apply this to shadow couplings, continuous time martingale transport problems in the framework of Huesmann-Trevisan and in particular to establish the correspondence between stretched Brownian motion with its geometric counterpart.   ","Note: We emphasize that we learned about the geometric stretched Brownian motion gSBM (defined in PDE terms) in a presentation of Loeper \\cite{Lo23} before our work on this topic started.","We noticed that a change of numeraire transformation in the spirit of \\cite{CaLaMa14} allows for an alternative viewpoint in the weak optimal transport framework.","We make our work public following the publication of Backhoff-Loeper-Obloj's work \\cite{BaLoOb24} on arxiv.org.","The article \\cite{BaLoOb24} derives gSBM using PDE techniques as well as through an independent probabilistic approach which is close to the one we give in the present article."],"url":"http://arxiv.org/abs/2406.07523v1","category":"math.PR"}
{"created":"2024-06-11 17:43:19","title":"COMAP Pathfinder -- Season 2 results I. Improved data selection and processing","abstract":"The CO Mapping Array Project (COMAP) Pathfinder is performing line intensity mapping of CO emission to trace the distribution of unresolved galaxies at redshift $z \\sim 3$. We present an improved version of the COMAP data processing pipeline and apply this to the first two seasons of observations. This analysis improves on the COMAP Early Science (ES) results in several key aspects. On the observational side, all second season scans were made in constant-elevation mode, after noting that the previous Lissajous scans were associated with increased systematic errors; those scans accounted for 50% of the total Season 1 data volume. Secondly, all new observations were restricted to an elevation range of 35-65 degrees, to minimize sidelobe ground pickup. On the data processing side, more effective data cleaning in both the time- and map-domain has allowed us to eliminate all data-driven power spectrum-based cuts. This increases the overall data retention and reduces the risk of signal subtraction bias. On the other hand, due to the increased sensitivity, two new pointing-correlated systematic errors have emerged, and we introduce a new map-domain PCA filter to suppress these. Subtracting only 5 out of 256 PCA modes, we find that the standard deviation of the cleaned maps decreases by 67% on large angular scales, and after applying this filter, the maps appear consistent with instrumental noise. Combining all these improvements, we find that each hour of raw Season 2 observations yields on average 3.2 times more cleaned data compared to ES analysis. Combining this with the increase in raw observational hours, the effective amount of data available for high-level analysis is a factor of 8 higher than in ES. The resulting maps have reached an uncertainty of $25$-$50\\,\\mu K$ per voxel, providing by far the strongest constraints on cosmological CO line emission published to date.","sentences":["The CO Mapping Array Project (COMAP) Pathfinder is performing line intensity mapping of CO emission to trace the distribution of unresolved galaxies at redshift $z \\sim 3$.","We present an improved version of the COMAP data processing pipeline and apply this to the first two seasons of observations.","This analysis improves on the COMAP Early Science (ES) results in several key aspects.","On the observational side, all second season scans were made in constant-elevation mode, after noting that the previous Lissajous scans were associated with increased systematic errors; those scans accounted for 50% of the total Season 1 data volume.","Secondly, all new observations were restricted to an elevation range of 35-65 degrees, to minimize sidelobe ground pickup.","On the data processing side, more effective data cleaning in both the time- and map-domain has allowed us to eliminate all data-driven power spectrum-based cuts.","This increases the overall data retention and reduces the risk of signal subtraction bias.","On the other hand, due to the increased sensitivity, two new pointing-correlated systematic errors have emerged, and we introduce a new map-domain PCA filter to suppress these.","Subtracting only 5 out of 256 PCA modes, we find that the standard deviation of the cleaned maps decreases by 67% on large angular scales, and after applying this filter, the maps appear consistent with instrumental noise.","Combining all these improvements, we find that each hour of raw Season 2 observations yields on average 3.2 times more cleaned data compared to ES analysis.","Combining this with the increase in raw observational hours, the effective amount of data available for high-level analysis is a factor of 8 higher than in ES.","The resulting maps have reached an uncertainty of $25$-$50\\,\\mu K$ per voxel, providing by far the strongest constraints on cosmological CO line emission published to date."],"url":"http://arxiv.org/abs/2406.07510v1","category":"astro-ph.CO"}
{"created":"2024-06-11 17:30:03","title":"Paraphrasing in Affirmative Terms Improves Negation Understanding","abstract":"Negation is a common linguistic phenomenon. Yet language models face challenges with negation in many natural language understanding tasks such as question answering and natural language inference. In this paper, we experiment with seamless strategies that incorporate affirmative interpretations (i.e., paraphrases without negation) to make models more robust against negation. Crucially, our affirmative interpretations are obtained automatically. We show improvements with CondaQA, a large corpus requiring reasoning with negation, and five natural language understanding tasks.","sentences":["Negation is a common linguistic phenomenon.","Yet language models face challenges with negation in many natural language understanding tasks such as question answering and natural language inference.","In this paper, we experiment with seamless strategies that incorporate affirmative interpretations (i.e., paraphrases without negation) to make models more robust against negation.","Crucially, our affirmative interpretations are obtained automatically.","We show improvements with CondaQA, a large corpus requiring reasoning with negation, and five natural language understanding tasks."],"url":"http://arxiv.org/abs/2406.07492v1","category":"cs.CL"}
{"created":"2024-06-11 17:26:07","title":"Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing","abstract":"In the rapidly evolving landscape of Natural Language Processing (NLP), the use of Large Language Models (LLMs) for automated text annotation in social media posts has garnered significant interest. Despite the impressive innovations in developing LLMs like ChatGPT, their efficacy, and accuracy as annotation tools are not well understood. In this paper, we analyze the performance of eight open-source and proprietary LLMs for annotating the stance expressed in social media posts, benchmarking their performance against human annotators' (i.e., crowd-sourced) judgments. Additionally, we investigate the conditions under which LLMs are likely to disagree with human judgment. A significant finding of our study is that the explicitness of text expressing a stance plays a critical role in how faithfully LLMs' stance judgments match humans'. We argue that LLMs perform well when human annotators do, and when LLMs fail, it often corresponds to situations in which human annotators struggle to reach an agreement. We conclude with recommendations for a comprehensive approach that combines the precision of human expertise with the scalability of LLM predictions. This study highlights the importance of improving the accuracy and comprehensiveness of automated stance detection, aiming to advance these technologies for more efficient and unbiased analysis of social media.","sentences":["In the rapidly evolving landscape of Natural Language Processing (NLP), the use of Large Language Models (LLMs) for automated text annotation in social media posts has garnered significant interest.","Despite the impressive innovations in developing LLMs like ChatGPT, their efficacy, and accuracy as annotation tools are not well understood.","In this paper, we analyze the performance of eight open-source and proprietary LLMs for annotating the stance expressed in social media posts, benchmarking their performance against human annotators' (i.e., crowd-sourced) judgments.","Additionally, we investigate the conditions under which LLMs are likely to disagree with human judgment.","A significant finding of our study is that the explicitness of text expressing a stance plays a critical role in how faithfully LLMs' stance judgments match humans'.","We argue that LLMs perform well when human annotators do, and when LLMs fail, it often corresponds to situations in which human annotators struggle to reach an agreement.","We conclude with recommendations for a comprehensive approach that combines the precision of human expertise with the scalability of LLM predictions.","This study highlights the importance of improving the accuracy and comprehensiveness of automated stance detection, aiming to advance these technologies for more efficient and unbiased analysis of social media."],"url":"http://arxiv.org/abs/2406.07483v1","category":"cs.CL"}
{"created":"2024-06-11 17:25:46","title":"Comparing Deep Learning Models for Rice Mapping in Bhutan Using High Resolution Satellite Imagery","abstract":"The Bhutanese government is increasing its utilization of technological approaches such as including Remote Sensing-based knowledge in their decision-making process. This study focuses on crop type and crop extent in Paro, one of the top rice-yielding districts in Bhutan, and employs publicly available NICFI high-resolution satellite imagery from Planet. Two Deep Learning (DL) approaches, point-based (DNN) and patch-based (U-Net), models were used in conjunction with cloud-computing platforms. Three different models per DL approaches (DNN and U-Net) were trained: 1) RGBN channels from Planet; 2) RGBN and elevation data (RGBNE); 3) RGBN and Sentinel-1 (S1) data (RGBNS), and RGBN with E and S1 data (RGBNES). From this comprehensive analysis, the U-Net displayed higher performance metrics across both model training and model validation efforts. Among the U-Net model sets, the RGBN, RGBNE, RGBNS, and RGBNES models had an F1-score of 0.8546, 0.8563, 0.8467, and 0.8500 respectively. An independent model evaluation was performed and found a high level of performance variation across all the metrics. For this independent model evaluation, the U-Net RGBN, RGBNE, RGBNES, and RGBN models displayed the F1-scores of 0.5935, 0.6154, 0.5882, and 0.6582, suggesting U-Net RGBNES as the best model. The study shows that the DL approaches can predict rice. Also, DL methods can be used with the survey-based approaches currently utilized by the Bhutan Department of Agriculture. Further, this study demonstrated the usage of regional land cover products such as SERVIR's RLCMS as a weak label approach to capture different strata addressing the class imbalance problem and improving the sampling design for DL application. Finally, through preliminary model testing and comparisons outlined it was shown that using additional features such as NDVI, EVI, and NDWI did not drastically improve model performance.","sentences":["The Bhutanese government is increasing its utilization of technological approaches such as including Remote Sensing-based knowledge in their decision-making process.","This study focuses on crop type and crop extent in Paro, one of the top rice-yielding districts in Bhutan, and employs publicly available NICFI high-resolution satellite imagery from Planet.","Two Deep Learning (DL) approaches, point-based (DNN) and patch-based (U-Net), models were used in conjunction with cloud-computing platforms.","Three different models per DL approaches (DNN and U-Net) were trained: 1) RGBN channels from Planet; 2) RGBN and elevation data (RGBNE); 3) RGBN and Sentinel-1 (S1) data (RGBNS), and RGBN with E and S1 data (RGBNES).","From this comprehensive analysis, the U-Net displayed higher performance metrics across both model training and model validation efforts.","Among the U-Net model sets, the RGBN, RGBNE, RGBNS, and RGBNES models had an F1-score of 0.8546, 0.8563, 0.8467, and 0.8500 respectively.","An independent model evaluation was performed and found a high level of performance variation across all the metrics.","For this independent model evaluation, the U-Net RGBN, RGBNE, RGBNES, and RGBN models displayed the F1-scores of 0.5935, 0.6154, 0.5882, and 0.6582, suggesting U-Net RGBNES as the best model.","The study shows that the DL approaches can predict rice.","Also, DL methods can be used with the survey-based approaches currently utilized by the Bhutan Department of Agriculture.","Further, this study demonstrated the usage of regional land cover products such as SERVIR's RLCMS as a weak label approach to capture different strata addressing the class imbalance problem and improving the sampling design for DL application.","Finally, through preliminary model testing and comparisons outlined it was shown that using additional features such as NDVI, EVI, and NDWI did not drastically improve model performance."],"url":"http://arxiv.org/abs/2406.07482v1","category":"cs.CV"}
{"created":"2024-06-11 17:21:15","title":"Partially Observed Trajectory Inference using Optimal Transport and a Dynamics Prior","abstract":"Trajectory inference seeks to recover the temporal dynamics of a population from snapshots of its (uncoupled) temporal marginals, i.e. where observed particles are not tracked over time. Lavenant et al. arXiv:2102.09204 addressed this challenging problem under a stochastic differential equation (SDE) model with a gradient-driven drift in the observed space, introducing a minimum entropy estimator relative to the Wiener measure. Chizat et al. arXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL) algorithm using Schr\\\"odinger bridges. Motivated by the overwhelming success of observable state space models in the traditional paired trajectory inference problem (e.g. target tracking), we extend the above framework to a class of latent SDEs in the form of observable state space models. In this setting, we use partial observations to infer trajectories in the latent space under a specified dynamics model (e.g. the constant velocity/acceleration models from target tracking). We introduce PO-MFL to solve this latent trajectory inference problem and provide theoretical guarantees by extending the results of arXiv:2102.09204 to the partially observed setting. We leverage the MFL framework of arXiv:2205.07146, yielding an algorithm based on entropic OT between dynamics-adjusted adjacent time marginals. Experiments validate the robustness of our method and the exponential convergence of the MFL dynamics, and demonstrate significant outperformance over the latent-free method of arXiv:2205.07146 in key scenarios.","sentences":["Trajectory inference seeks to recover the temporal dynamics of a population from snapshots of its (uncoupled) temporal marginals, i.e. where observed particles are not tracked over time.","Lavenant et al. arXiv:2102.09204 addressed this challenging problem under a stochastic differential equation (SDE) model with a gradient-driven drift in the observed space, introducing a minimum entropy estimator relative to the Wiener measure.","Chizat et al.","arXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL) algorithm using Schr\\\"odinger bridges.","Motivated by the overwhelming success of observable state space models in the traditional paired trajectory inference problem (e.g. target tracking), we extend the above framework to a class of latent SDEs in the form of observable state space models.","In this setting, we use partial observations to infer trajectories in the latent space under a specified dynamics model (e.g. the constant velocity/acceleration models from target tracking).","We introduce PO-MFL to solve this latent trajectory inference problem and provide theoretical guarantees by extending the results of arXiv:2102.09204 to the partially observed setting.","We leverage the MFL framework of arXiv:2205.07146, yielding an algorithm based on entropic OT between dynamics-adjusted adjacent time marginals.","Experiments validate the robustness of our method and the exponential convergence of the MFL dynamics, and demonstrate significant outperformance over the latent-free method of arXiv:2205.07146 in key scenarios."],"url":"http://arxiv.org/abs/2406.07475v1","category":"cs.LG"}
{"created":"2024-06-11 17:05:44","title":"Existence and asymptotic autonomous robustness of random attractors for three-dimensional stochastic globally modified Navier-Stokes equations on unbounded domains","abstract":"In this article, we discuss the existence and asymptotically autonomous robustness (AAR) (almost surely) of random attractors for 3D stochastic globally modified Navier-Stokes equations (SGMNSE) on Poincar\\'e domains (which may be bounded or unbounded). Our aim is to investigate the existence and AAR of random attractors for 3D SGMNSE when the time-dependent forcing converges to a time-independent function under the perturbation of linear multiplicative noise as well as additive noise. The main approach is to provide a way to justify that, on some uniformly tempered universe, the usual pullback asymptotic compactness of the solution operators is uniform across an infinite time-interval $(-\\infty,\\tau]$. The backward uniform ``tail-smallness'' and ``flattening-property'' of the solutions over $(-\\infty,\\tau]$ have been demonstrated to achieve this goal. To the best of our knowledge, this is the first attempt to establish the existence as well as AAR of random attractors for 3D SGMNSE on unbounded domains.","sentences":["In this article, we discuss the existence and asymptotically autonomous robustness (AAR) (almost surely) of random attractors for 3D stochastic globally modified Navier-Stokes equations (SGMNSE) on Poincar\\'e domains (which may be bounded or unbounded).","Our aim is to investigate the existence and AAR of random attractors for 3D SGMNSE when the time-dependent forcing converges to a time-independent function under the perturbation of linear multiplicative noise as well as additive noise.","The main approach is to provide a way to justify that, on some uniformly tempered universe, the usual pullback asymptotic compactness of the solution operators is uniform across an infinite time-interval $(-\\infty,\\tau]$. The backward uniform ``tail-smallness'' and ``flattening-property'' of the solutions over $(-\\infty,\\tau]$ have been demonstrated to achieve this goal.","To the best of our knowledge, this is the first attempt to establish the existence as well as AAR of random attractors for 3D SGMNSE on unbounded domains."],"url":"http://arxiv.org/abs/2406.07460v1","category":"math.PR"}
{"created":"2024-06-11 16:14:30","title":"Accelerating Ill-conditioned Hankel Matrix Recovery via Structured Newton-like Descent","abstract":"This paper studies the robust Hankel recovery problem, which simultaneously removes the sparse outliers and fulfills missing entries from the partial observation. We propose a novel non-convex algorithm, coined Hankel Structured Newton-Like Descent (HSNLD), to tackle the robust Hankel recovery problem. HSNLD is highly efficient with linear convergence, and its convergence rate is independent of the condition number of the underlying Hankel matrix. The recovery guarantee has been established under some mild conditions. Numerical experiments on both synthetic and real datasets show the superior performance of HSNLD against state-of-the-art algorithms.","sentences":["This paper studies the robust Hankel recovery problem, which simultaneously removes the sparse outliers and fulfills missing entries from the partial observation.","We propose a novel non-convex algorithm, coined Hankel Structured Newton-Like Descent (HSNLD), to tackle the robust Hankel recovery problem.","HSNLD is highly efficient with linear convergence, and its convergence rate is independent of the condition number of the underlying Hankel matrix.","The recovery guarantee has been established under some mild conditions.","Numerical experiments on both synthetic and real datasets show the superior performance of HSNLD against state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2406.07409v1","category":"stat.ML"}
{"created":"2024-06-11 15:37:31","title":"A qualitative field study on explainable AI for lay users subjected to AI cyberattacks","abstract":"In this paper we present results from a qualitative field study on explainable AI (XAI) for lay users (n = 18) who were subjected to AI cyberattacks. The study was based on a custom-built smart heating application called Squid and was conducted over seven weeks in early 2023. Squid combined a smart radiator valve installed in participant homes with a web application that implemented an AI feature known as setpoint learning, which is commonly available in consumer smart thermostats. Development of Squid followed the XAI principle of interpretability-by-design where the AI feature was implemented using a simple glass-box machine learning model with the model subsequently exposed to users via the web interface (e.g. as interactive visualisations). AI attacks on users were simulated by injecting malicious training data and by manipulating data used for model predictions. Research data consisted of semi-structured interviews, researcher field notes, participant diaries, and application logs. In our analysis we reflect on the impact of XAI on user satisfaction and user comprehension as well as its use as a tool for diagnosing AI attacks. Our results show only limited engagement with XAI features and suggest that, for Squid users, common assumptions found in the XAI literature were not aligned to reality. On the positive side, users appear to have developed better mental models of the AI feature compared to previous work, and there is evidence that users did make some use of XAI as a diagnostic tool.","sentences":["In this paper we present results from a qualitative field study on explainable AI (XAI) for lay users (n = 18) who were subjected to AI cyberattacks.","The study was based on a custom-built smart heating application called Squid and was conducted over seven weeks in early 2023.","Squid combined a smart radiator valve installed in participant homes with a web application that implemented an AI feature known as setpoint learning, which is commonly available in consumer smart thermostats.","Development of Squid followed the XAI principle of interpretability-by-design where the AI feature was implemented using a simple glass-box machine learning model with the model subsequently exposed to users via the web interface (e.g. as interactive visualisations).","AI attacks on users were simulated by injecting malicious training data and by manipulating data used for model predictions.","Research data consisted of semi-structured interviews, researcher field notes, participant diaries, and application logs.","In our analysis we reflect on the impact of XAI on user satisfaction and user comprehension as well as its use as a tool for diagnosing AI attacks.","Our results show only limited engagement with XAI features and suggest that, for Squid users, common assumptions found in the XAI literature were not aligned to reality.","On the positive side, users appear to have developed better mental models of the AI feature compared to previous work, and there is evidence that users did make some use of XAI as a diagnostic tool."],"url":"http://arxiv.org/abs/2406.07369v1","category":"cs.HC"}
{"created":"2024-06-11 15:11:42","title":"Characterizing GPROF Regional Bias Using Radar-Derived Hydrometeor Information","abstract":"Current satellite precipitation retrievals like GPROF assume that brightness temperature is sufficient to constrain rainfall. This information, however, often represents multiple rain states, resulting in rainfall estimate uncertainties. These uncertainties, while dominated by random variability, can also exhibit substantial regional biases, complicating the use of traditional ground validation techniques which seek to understand these uncertainties. This study aims to characterize the physical contributors to these biases for use in uncertainty quantification. To do this, coincident GPROF Version 7, GMI, and GPM Combined observations were examined over three tropical land regions, the Amazon, Congo, and Southeast Asia, which are known to exhibit distinct biases relative to one another when comparing GPROF with GPM Combined. Rain intensity and ice-rain ratio were identified as the primary sources of GPROF regional biases. By incorporating the information from these sources, the self-similarity between these three regions was brought from within 13 percent to within 7 percent, reducing the interregional bias by half. Including a third constraint based on the polarization-corrected 37-GHz brightness temperature further improved this self-similarity to within 4 percent by accounting for second-order hydrometeor profile differences which were underutilized by GPROF. Comparing the effects of these three constraints between GPROF Version 7 with the 1D version of GPROF-NN showed similar improvements, indicating the utility of this uncertainty quantification and adjustment method across precipitation products. With these constraints, regional GPROF biases can be made more consistent, improving the fidelity of the precipitation climate data records and operational precipitation products which utilize this information.","sentences":["Current satellite precipitation retrievals like GPROF assume that brightness temperature is sufficient to constrain rainfall.","This information, however, often represents multiple rain states, resulting in rainfall estimate uncertainties.","These uncertainties, while dominated by random variability, can also exhibit substantial regional biases, complicating the use of traditional ground validation techniques which seek to understand these uncertainties.","This study aims to characterize the physical contributors to these biases for use in uncertainty quantification.","To do this, coincident GPROF Version 7, GMI, and GPM Combined observations were examined over three tropical land regions, the Amazon, Congo, and Southeast Asia, which are known to exhibit distinct biases relative to one another when comparing GPROF with GPM Combined.","Rain intensity and ice-rain ratio were identified as the primary sources of GPROF regional biases.","By incorporating the information from these sources, the self-similarity between these three regions was brought from within 13 percent to within 7 percent, reducing the interregional bias by half.","Including a third constraint based on the polarization-corrected 37-GHz brightness temperature further improved this self-similarity to within 4 percent by accounting for second-order hydrometeor profile differences which were underutilized by GPROF.","Comparing the effects of these three constraints between GPROF Version 7 with the 1D version of GPROF-NN showed similar improvements, indicating the utility of this uncertainty quantification and adjustment method across precipitation products.","With these constraints, regional GPROF biases can be made more consistent, improving the fidelity of the precipitation climate data records and operational precipitation products which utilize this information."],"url":"http://arxiv.org/abs/2406.07344v1","category":"physics.ao-ph"}
{"created":"2024-06-11 15:06:50","title":"Remarks on second and third weights of Projective Reed-Muller codes","abstract":"Determining the weight distributions of the projective Reed-Muller codes is a very hard problem and has been studied extensively in the literature. In this article, we provide an alternative proof of the second weight of the projective Reed-Muller codes $\\PRM (d, m)$ where $m \\ge 3$ and $3 \\le d \\le \\frac{q+3}{2}$. We show that the second weight is attained by codewords that correspond to hypersurfaces containing a hyperplane under the hypothesis on $d$. Furthermore, we compute the second weight of $\\PRM (d, 2)$ for $3 \\le d \\le q-1$. Furthermore, we give an upper bound for the third weight of $\\PRM(d, 2)$.","sentences":["Determining the weight distributions of the projective Reed-Muller codes is a very hard problem and has been studied extensively in the literature.","In this article, we provide an alternative proof of the second weight of the projective Reed-Muller codes $\\PRM (d, m)$ where $m \\ge 3$ and $3 \\le d \\le \\frac{q+3}{2}$.","We show that the second weight is attained by codewords that correspond to hypersurfaces containing a hyperplane under the hypothesis on $d$.","Furthermore, we compute the second weight of $\\PRM (d, 2)$ for $3 \\le d \\le q-1$. Furthermore, we give an upper bound for the third weight of $\\PRM(d, 2)$."],"url":"http://arxiv.org/abs/2406.07339v1","category":"math.AG"}
{"created":"2024-06-11 14:23:01","title":"Convergence rate of random scan Coordinate Ascent Variational Inference under log-concavity","abstract":"The Coordinate Ascent Variational Inference scheme is a popular algorithm used to compute the mean-field approximation of a probability distribution of interest. We analyze its random scan version, under log-concavity assumptions on the target density. Our approach builds on the recent work of M. Arnese and D. Lacker, \\emph{Convergence of coordinate ascent variational inference for log-concave measures via optimal transport} [arXiv:2404.08792] which studies the deterministic scan version of the algorithm, phrasing it as a block-coordinate descent algorithm in the space of probability distributions endowed with the geometry of optimal transport. We obtain tight rates for the random scan version, which imply that the total number of factor updates required to converge scales linearly with the condition number and the number of blocks of the target distribution. By contrast, available bounds for the deterministic scan case scale quadratically in the same quantities, which is analogue to what happens for optimization of convex functions in Euclidean spaces.","sentences":["The Coordinate Ascent Variational Inference scheme is a popular algorithm used to compute the mean-field approximation of a probability distribution of interest.","We analyze its random scan version, under log-concavity assumptions on the target density.","Our approach builds on the recent work of M. Arnese and D. Lacker, \\emph{Convergence of coordinate ascent variational inference for log-concave measures via optimal transport}","[arXiv:2404.08792] which studies the deterministic scan version of the algorithm, phrasing it as a block-coordinate descent algorithm in the space of probability distributions endowed with the geometry of optimal transport.","We obtain tight rates for the random scan version, which imply that the total number of factor updates required to converge scales linearly with the condition number and the number of blocks of the target distribution.","By contrast, available bounds for the deterministic scan case scale quadratically in the same quantities, which is analogue to what happens for optimization of convex functions in Euclidean spaces."],"url":"http://arxiv.org/abs/2406.07292v1","category":"stat.ML"}
{"created":"2024-06-11 14:13:28","title":"From rank-based models with common noise to pathwise entropy solutions of SPDEs","abstract":"We study the mean field limit of a rank-based model with common noise, which arises as an extension to models for the market capitalization of firms in stochastic portfolio theory. We show that, under certain conditions on the drift and diffusion coefficients, the empirical cumulative distribution function converges to the solution of a stochastic PDE. A key step in the proof, which is of independent interest, is to show that any solution to an associated martingale problem is also a pathwise entropy solution to the stochastic PDE, a notion introduced in a recent series of papers [32, 33, 19, 16, 17].","sentences":["We study the mean field limit of a rank-based model with common noise, which arises as an extension to models for the market capitalization of firms in stochastic portfolio theory.","We show that, under certain conditions on the drift and diffusion coefficients, the empirical cumulative distribution function converges to the solution of a stochastic PDE.","A key step in the proof, which is of independent interest, is to show that any solution to an associated martingale problem is also a pathwise entropy solution to the stochastic PDE, a notion introduced in a recent series of papers","[32, 33, 19, 16, 17]."],"url":"http://arxiv.org/abs/2406.07286v1","category":"math.PR"}
{"created":"2024-06-11 14:12:23","title":"Sum the Probabilities to $m$ and Stop","abstract":"This work investigates the optimal selection of the last $m$th success in a sequence of $n$ independent Bernoulli trials. We propose a threshold strategy that is $\\varepsilon$-optimal under minimal assumptions about the monotonicity of the trials' success probabilities. This new strategy ensures stopping at most one step earlier than the optimal rule. Specifically, the new threshold coincides with the point where the sum of success probabilities in the remaining trials equals $m$. We show that the underperformance of the new rule, in comparison to the optimal one, is of the order $O(n^{-2})$ in the case of a Karamata-Stirling success profile with parameter $\\theta > 0$ where $p_k = \\theta / (\\theta + k - 1)$ for the $k$th trial. We further leverage the classical weak convergence of the number of successes in the trials to a Poisson random variable to derive the asymptotic solution of the stopping problem. Finally, we present illustrative results highlighting the close performance between the two rules.","sentences":["This work investigates the optimal selection of the last $m$th success in a sequence of $n$ independent Bernoulli trials.","We propose a threshold strategy that is $\\varepsilon$-optimal under minimal assumptions about the monotonicity of the trials' success probabilities.","This new strategy ensures stopping at most one step earlier than the optimal rule.","Specifically, the new threshold coincides with the point where the sum of success probabilities in the remaining trials equals $m$. We show that the underperformance of the new rule, in comparison to the optimal one, is of the order $O(n^{-2})$ in the case of a Karamata-Stirling success profile with parameter $\\theta > 0$ where $p_k","=","\\theta / (\\theta + k - 1)$ for the $k$th trial.","We further leverage the classical weak convergence of the number of successes in the trials to a Poisson random variable to derive the asymptotic solution of the stopping problem.","Finally, we present illustrative results highlighting the close performance between the two rules."],"url":"http://arxiv.org/abs/2406.07283v1","category":"math.PR"}
{"created":"2024-06-11 13:51:52","title":"High-fidelity single-spin shuttling in silicon","abstract":"The computational power and fault-tolerance of future large-scale quantum processors derive in large part from the connectivity between the qubits. One approach to increase connectivity is to engineer qubit-qubit interactions at a distance. Alternatively, the connectivity can be increased by physically displacing the qubits. This has been explored in trapped-ion experiments and using neutral atoms trapped with optical tweezers. For semiconductor spin qubits, several studies have investigated spin coherent shuttling of individual electrons, but high-fidelity transport over extended distances remains to be demonstrated. Here we report shuttling of an electron inside an isotopically purified Si/SiGe heterostructure using electric gate potentials. First, we form static quantum dots, and study how spin coherence decays as we repeatedly move a single electron between up to five dots. Next, we create a traveling wave potential to transport an electron in a moving quantum dot. This second method shows substantially better spin coherence than the first. It allows us to displace an electron over an effective distance of 10 {\\mu}m in under 200 ns with an average fidelity of 99%. These results will guide future efforts to realize large-scale semiconductor quantum processors, making use of electron shuttling both within and between qubit arrays.","sentences":["The computational power and fault-tolerance of future large-scale quantum processors derive in large part from the connectivity between the qubits.","One approach to increase connectivity is to engineer qubit-qubit interactions at a distance.","Alternatively, the connectivity can be increased by physically displacing the qubits.","This has been explored in trapped-ion experiments and using neutral atoms trapped with optical tweezers.","For semiconductor spin qubits, several studies have investigated spin coherent shuttling of individual electrons, but high-fidelity transport over extended distances remains to be demonstrated.","Here we report shuttling of an electron inside an isotopically purified Si/SiGe heterostructure using electric gate potentials.","First, we form static quantum dots, and study how spin coherence decays as we repeatedly move a single electron between up to five dots.","Next, we create a traveling wave potential to transport an electron in a moving quantum dot.","This second method shows substantially better spin coherence than the first.","It allows us to displace an electron over an effective distance of 10 {\\mu}m in under 200 ns with an average fidelity of 99%.","These results will guide future efforts to realize large-scale semiconductor quantum processors, making use of electron shuttling both within and between qubit arrays."],"url":"http://arxiv.org/abs/2406.07267v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-11 13:28:43","title":"Marginalization Consistent Mixture of Separable Flows for Probabilistic Irregular Time Series Forecasting","abstract":"Probabilistic forecasting models for joint distributions of targets in irregular time series are a heavily under-researched area in machine learning with, to the best of our knowledge, only three models researched so far: GPR, the Gaussian Process Regression model~\\citep{Durichen2015.Multitask}, TACTiS, the Transformer-Attentional Copulas for Time Series~\\cite{Drouin2022.Tactis, ashok2024tactis} and ProFITi \\citep{Yalavarthi2024.Probabilistica}, a multivariate normalizing flow model based on invertible attention layers. While ProFITi, thanks to using multivariate normalizing flows, is the more expressive model with better predictive performance, we will show that it suffers from marginalization inconsistency: it does not guarantee that the marginal distributions of a subset of variables in its predictive distributions coincide with the directly predicted distributions of these variables. Also, TACTiS does not provide any guarantees for marginalization consistency. We develop a novel probabilistic irregular time series forecasting model, Marginalization Consistent Mixtures of Separable Flows (moses), that mixes several normalizing flows with (i) Gaussian Processes with full covariance matrix as source distributions and (ii) a separable invertible transformation, aiming to combine the expressivity of normalizing flows with the marginalization consistency of Gaussians. In experiments on four different datasets we show that moses outperforms other state-of-the-art marginalization consistent models, performs on par with ProFITi, but different from ProFITi, guarantee marginalization consistency.","sentences":["Probabilistic forecasting models for joint distributions of targets in irregular time series are a heavily under-researched area in machine learning with, to the best of our knowledge, only three models researched so far: GPR, the Gaussian Process Regression model~\\citep{Durichen2015.Multitask}, TACTiS, the Transformer-Attentional Copulas for Time Series~\\cite{Drouin2022.Tactis, ashok2024tactis} and ProFITi \\citep{Yalavarthi2024.Probabilistica}, a multivariate normalizing flow model based on invertible attention layers.","While ProFITi, thanks to using multivariate normalizing flows, is the more expressive model with better predictive performance, we will show that it suffers from marginalization inconsistency: it does not guarantee that the marginal distributions of a subset of variables in its predictive distributions coincide with the directly predicted distributions of these variables.","Also, TACTiS does not provide any guarantees for marginalization consistency.","We develop a novel probabilistic irregular time series forecasting model, Marginalization Consistent Mixtures of Separable Flows (moses), that mixes several normalizing flows with (i) Gaussian Processes with full covariance matrix as source distributions and (ii) a separable invertible transformation, aiming to combine the expressivity of normalizing flows with the marginalization consistency of Gaussians.","In experiments on four different datasets we show that moses outperforms other state-of-the-art marginalization consistent models, performs on par with ProFITi, but different from ProFITi, guarantee marginalization consistency."],"url":"http://arxiv.org/abs/2406.07246v1","category":"cs.LG"}
{"created":"2024-06-11 13:22:52","title":"Variational inequalities and smooth-fit principle for singular stochastic control problems in Hilbert spaces","abstract":"We consider a class of infinite-dimensional singular stochastic control problems. These can be thought of as spatial monotone follower problems and find applications in spatial models of production and climate transition. Let $(D,\\mathcal{M},\\mu)$ be a finite measure space and consider the Hilbert space $H:=L^2(D,\\mathcal{M},\\mu; \\mathbb{R})$. Let then $X$ be an $H$-valued stochastic process on a suitable complete probability space, whose evolution is determined through an SPDE driven by a self-adjoint linear operator $\\mathcal{A}$ and affected by a cylindrical Brownian motion. The evolution of $X$ is controlled linearly via an $H$-valued control consisting of the direction and the intensity of action, a real-valued nondecreasing right-continuous stochastic process, adapted to the underlying filtration. The goal is to minimize a discounted convex cost-functional over an infinite time-horizon. By combining properties of semiconcave functions and techniques from viscosity theory, we first show that the value function of the problem $V$ is a $C^{1,Lip}(H)$-viscosity solution to the corresponding dynamic programming equation, which here takes the form of a variational inequality with gradient constraint. Then, by allowing the decision maker to choose only the intensity of the control and requiring that the given control direction $\\hat{n}$ is an eigenvector of the linear operator $\\mathcal{A}$, we establish that the directional derivative $V_{\\hat{n}}$ is of class $C^1(H)$, hence a second-order smooth-fit principle in the controlled direction holds for $V$. This result is obtained by exploiting a connection to optimal stopping and combining results and techniques from convex analysis and viscosity theory.","sentences":["We consider a class of infinite-dimensional singular stochastic control problems.","These can be thought of as spatial monotone follower problems and find applications in spatial models of production and climate transition.","Let $(D,\\mathcal{M},\\mu)$ be a finite measure space and consider the Hilbert space $H:=L^2(D,\\mathcal{M},\\mu; \\mathbb{R})$. Let then $X$ be an $H$-valued stochastic process on a suitable complete probability space, whose evolution is determined through an SPDE driven by a self-adjoint linear operator $\\mathcal{A}$ and affected by a cylindrical Brownian motion.","The evolution of $X$ is controlled linearly via an $H$-valued control consisting of the direction and the intensity of action, a real-valued nondecreasing right-continuous stochastic process, adapted to the underlying filtration.","The goal is to minimize a discounted convex cost-functional over an infinite time-horizon.","By combining properties of semiconcave functions and techniques from viscosity theory, we first show that the value function of the problem $V$ is a $C^{1,Lip}(H)$-viscosity solution to the corresponding dynamic programming equation, which here takes the form of a variational inequality with gradient constraint.","Then, by allowing the decision maker to choose only the intensity of the control and requiring that the given control direction $\\hat{n}$ is an eigenvector of the linear operator $\\mathcal{A}$, we establish that the directional derivative $V_{\\hat{n}}$ is of class $C^1(H)$, hence a second-order smooth-fit principle in the controlled direction holds for $V$. This result is obtained by exploiting a connection to optimal stopping and combining results and techniques from convex analysis and viscosity theory."],"url":"http://arxiv.org/abs/2406.07242v1","category":"math.OC"}
{"created":"2024-06-11 13:20:01","title":"Structures and Superconductivity of Hydrogen and Hydrides under Extreme Pressure","abstract":"Metallic hydrogen, existing in remarkably extreme environments, was predicted to exhibit long-sought room-temperature superconductivity. Although the superconductivity of metallic hydrogen has not been confirmed experimentally, superconductivity of hydrogen in hydrides was recently discovered with remarkably high critical temperature as theoretically predicted. In recent years, theoretical simulations have become a new paradigm for material science, especially exploration of material at extreme pressure. As the typical high-pressure material, metallic hydrogen has been providing a fertile playground for advanced simulations for long time. Simulations not only provide the substitute of experiments for hydrogen at high-pressure, but also encouraged the discovery of almost all the experimentally discovered superconducting hydrides with the record high superconducting transition temperature. This work reviews recent progress in hydrogen and hydrides under extreme pressure, focusing on phase diagram, structures and the long-sought goal of high-temperature superconductivity. In the end, we highlight structural features of hydrides for realization of hydrogen-driven superconducting hydrides near ambient pressure.","sentences":["Metallic hydrogen, existing in remarkably extreme environments, was predicted to exhibit long-sought room-temperature superconductivity.","Although the superconductivity of metallic hydrogen has not been confirmed experimentally, superconductivity of hydrogen in hydrides was recently discovered with remarkably high critical temperature as theoretically predicted.","In recent years, theoretical simulations have become a new paradigm for material science, especially exploration of material at extreme pressure.","As the typical high-pressure material, metallic hydrogen has been providing a fertile playground for advanced simulations for long time.","Simulations not only provide the substitute of experiments for hydrogen at high-pressure, but also encouraged the discovery of almost all the experimentally discovered superconducting hydrides with the record high superconducting transition temperature.","This work reviews recent progress in hydrogen and hydrides under extreme pressure, focusing on phase diagram, structures and the long-sought goal of high-temperature superconductivity.","In the end, we highlight structural features of hydrides for realization of hydrogen-driven superconducting hydrides near ambient pressure."],"url":"http://arxiv.org/abs/2406.07238v1","category":"cond-mat.supr-con"}
{"created":"2024-06-11 13:11:13","title":"On Lie n-centralizers, n-commuting linear maps and related mappings","abstract":"In this article, we consider several local conditions under which linear mappings on algebras act like Lie n-centralizers and we study these linear mappings, Lie n-centralizers and n-commuting linear maps.","sentences":["In this article, we consider several local conditions under which linear mappings on algebras act like Lie n-centralizers and we study these linear mappings, Lie n-centralizers and n-commuting linear maps."],"url":"http://arxiv.org/abs/2406.07233v1","category":"math.RA"}
{"created":"2024-06-11 12:42:41","title":"Semantic-Aware Spectrum Sharing in Internet of Vehicles Based on Deep Reinforcement Learning","abstract":"This work aims to investigate semantic communication in high-speed mobile Internet of vehicles (IoV) environments, with a focus on the spectrum sharing between vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications. We specifically address spectrum scarcity and network traffic and then propose a semantic-aware spectrum sharing algorithm (SSS) based on the deep reinforcement learning (DRL) soft actor-critic (SAC) approach. Firstly, we delve into the extraction of semantic information. Secondly, we redefine metrics for semantic information in V2V and V2I spectrum sharing in IoV environments, introducing high-speed semantic spectrum efficiency (HSSE) and semantic transmission rate (HSR). Finally, we employ the SAC algorithm for decision optimization in V2V and V2I spectrum sharing based on semantic information. This optimization encompasses the optimal link of V2V and V2I sharing strategies, the transmission power for vehicles sending semantic information and the length of transmitted semantic symbols, aiming at maximizing HSSE of V2I and enhancing success rate of effective semantic information transmission (SRS) of V2V. Experimental results demonstrate that the SSS algorithm outperforms other baseline algorithms, including other traditional-communication-based spectrum sharing algorithms and spectrum sharing algorithm using other reinforcement learning approaches. The SSS algorithm exhibits a 15% increase in HSSE and approximately a 7% increase in SRS.","sentences":["This work aims to investigate semantic communication in high-speed mobile Internet of vehicles (IoV) environments, with a focus on the spectrum sharing between vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications.","We specifically address spectrum scarcity and network traffic and then propose a semantic-aware spectrum sharing algorithm (SSS) based on the deep reinforcement learning (DRL) soft actor-critic (SAC) approach.","Firstly, we delve into the extraction of semantic information.","Secondly, we redefine metrics for semantic information in V2V and V2I spectrum sharing in IoV environments, introducing high-speed semantic spectrum efficiency (HSSE) and semantic transmission rate (HSR).","Finally, we employ the SAC algorithm for decision optimization in V2V and V2I spectrum sharing based on semantic information.","This optimization encompasses the optimal link of V2V and V2I sharing strategies, the transmission power for vehicles sending semantic information and the length of transmitted semantic symbols, aiming at maximizing HSSE of V2I and enhancing success rate of effective semantic information transmission (SRS) of V2V. Experimental results demonstrate that the SSS algorithm outperforms other baseline algorithms, including other traditional-communication-based spectrum sharing algorithms and spectrum sharing algorithm using other reinforcement learning approaches.","The SSS algorithm exhibits a 15% increase in HSSE and approximately a 7% increase in SRS."],"url":"http://arxiv.org/abs/2406.07213v1","category":"cs.LG"}
{"created":"2024-06-11 11:40:52","title":"Gelfand--Phillips type properties of locally convex spaces","abstract":"Let $1\\leq p\\leq q\\leq\\infty.$ Being motivated by the classical notions of the Gelfand--Phillips property and the (coarse) Gelfand--Phillips property of order $p$ of Banach spaces, we introduce and study different types of the Gelfand--Phillips property of order $(p,q)$ (the $GP_{(p,q)}$ property) and the coarse Gelfand--Phillips property of order $p$ in the realm of all locally convex spaces. We compare these classes and show that they are stable under taking direct product, direct sums and closed subspaces. It is shown that any locally convex space is a quotient space of a locally convex space with the $GP_{(p,q)}$ property. Characterizations of locally convex spaces with the introduced Gelfand--Phillips type properties are given.","sentences":["Let $1\\leq p\\leq q\\leq\\infty.$ Being motivated by the classical notions of the Gelfand--Phillips property and the (coarse) Gelfand--Phillips property of order $p$ of Banach spaces, we introduce and study different types of the Gelfand--Phillips property of order $(p,q)$ (the $GP_{(p,q)}$ property) and the coarse Gelfand--Phillips property of order $p$ in the realm of all locally convex spaces.","We compare these classes and show that they are stable under taking direct product, direct sums and closed subspaces.","It is shown that any locally convex space is a quotient space of a locally convex space with the $GP_{(p,q)}$ property.","Characterizations of locally convex spaces with the introduced Gelfand--Phillips type properties are given."],"url":"http://arxiv.org/abs/2406.07178v1","category":"math.FA"}
{"created":"2024-06-11 11:36:02","title":"Phase Diagram of growth modes in Graphene Growth on Cooper by Vapor Deposition","abstract":"Understanding the atomistic mechanism in graphene growth is crucial for controlling the number of layers or domain sizes to meet practical needs. In this work, focusing on the growth of graphene by chemical vapor deposition on copper substrates, the surface kinetics in the growth are systematically investigated by first-principles calculations. The phase diagram, predicting whether the growth mode is monolayer graphene or bilayer graphene under various experimental conditions, is constructed based on classical nucleation theory. Our phase diagram well illustrates the effect of high hydrogen pressure on bilayer graphene growth and clarifies the mechanism of the most widely used experimental growth approaches. The phase diagram can provide guidance and predictions for experiments and inspires the study of other two-dimensional materials with graphene-like growth mechanisms.","sentences":["Understanding the atomistic mechanism in graphene growth is crucial for controlling the number of layers or domain sizes to meet practical needs.","In this work, focusing on the growth of graphene by chemical vapor deposition on copper substrates, the surface kinetics in the growth are systematically investigated by first-principles calculations.","The phase diagram, predicting whether the growth mode is monolayer graphene or bilayer graphene under various experimental conditions, is constructed based on classical nucleation theory.","Our phase diagram well illustrates the effect of high hydrogen pressure on bilayer graphene growth and clarifies the mechanism of the most widely used experimental growth approaches.","The phase diagram can provide guidance and predictions for experiments and inspires the study of other two-dimensional materials with graphene-like growth mechanisms."],"url":"http://arxiv.org/abs/2406.07175v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-11 11:08:33","title":"Deep Learning-Based Approach for User Activity Detection with Grant-Free Random Access in Cell-Free Massive MIMO","abstract":"Modern wireless networks must reliably support a wide array of connectivity demands, encompassing various user needs across diverse scenarios. Machine-Type Communication (mMTC) is pivotal in these networks, particularly given the challenges posed by massive connectivity and sporadic device activation patterns. Traditional grant-based random access (GB-RA) protocols face limitations due to constrained orthogonal preamble resources. In response, the adoption of grant-free random access (GF-RA) protocols offers a promising solution. This paper explores the application of supervised machine learning models to tackle activity detection issues in scenarios where non-orthogonal preamble design is considered. We introduce a data-driven algorithm specifically designed for user activity detection in Cell-Free Massive Multiple-Input Multiple-Output (CF-mMIMO) networks operating under GF-RA protocols. Additionally, this study presents a novel clustering strategy that simplifies and enhances activity detection accuracy, assesses the resilience of the algorithm to input perturbations, and investigates the effects of adopting floating-to-fixed-point conversion on algorithm performance. Simulations conducted adhere to 3GPP standards, ensuring accurate channel modeling, and employ a deep learning approach to boost the detection capabilities of mMTC GF-RA devices. The results are compelling: the algorithm achieves an exceptional 99\\% accuracy rate, confirming its efficacy in real-world applications.","sentences":["Modern wireless networks must reliably support a wide array of connectivity demands, encompassing various user needs across diverse scenarios.","Machine-Type Communication (mMTC) is pivotal in these networks, particularly given the challenges posed by massive connectivity and sporadic device activation patterns.","Traditional grant-based random access (GB-RA) protocols face limitations due to constrained orthogonal preamble resources.","In response, the adoption of grant-free random access (GF-RA) protocols offers a promising solution.","This paper explores the application of supervised machine learning models to tackle activity detection issues in scenarios where non-orthogonal preamble design is considered.","We introduce a data-driven algorithm specifically designed for user activity detection in Cell-Free Massive Multiple-Input Multiple-Output (CF-mMIMO) networks operating under GF-RA protocols.","Additionally, this study presents a novel clustering strategy that simplifies and enhances activity detection accuracy, assesses the resilience of the algorithm to input perturbations, and investigates the effects of adopting floating-to-fixed-point conversion on algorithm performance.","Simulations conducted adhere to 3GPP standards, ensuring accurate channel modeling, and employ a deep learning approach to boost the detection capabilities of mMTC GF-RA devices.","The results are compelling: the algorithm achieves an exceptional 99\\% accuracy rate, confirming its efficacy in real-world applications."],"url":"http://arxiv.org/abs/2406.07160v1","category":"cs.LG"}
{"created":"2024-06-11 10:50:50","title":"Partial yet definite emergence of the Kardar-Parisi-Zhang class in isotropic spin chains","abstract":"Integrable spin chains with a continuous non-Abelian symmetry, such as the one-dimensional isotropic Heisenberg model, show superdiffusive transport with little theoretical understanding. Although recent studies reported a surprising connection to the Kardar-Parisi-Zhang (KPZ) universality class in that case, this view was most recently questioned by discrepancies in full counting statistics. Here, by combining extensive numerical simulations of the Landau-Lifshitz one-dimensional magnet, with a framework developed by exact studies of the KPZ class, we characterize various two-point quantities that remain hitherto unexplored in spin chains, and find full agreement with KPZ scaling laws. This establishes the partial emergence of the KPZ class in isotropic spin chains. Moreover, we reveal that the KPZ scaling laws are intact in the presence of an energy current, under the appropriate Galilean boost required by the propagation of spacetime correlation.","sentences":["Integrable spin chains with a continuous non-Abelian symmetry, such as the one-dimensional isotropic Heisenberg model, show superdiffusive transport with little theoretical understanding.","Although recent studies reported a surprising connection to the Kardar-Parisi-Zhang (KPZ) universality class in that case, this view was most recently questioned by discrepancies in full counting statistics.","Here, by combining extensive numerical simulations of the Landau-Lifshitz one-dimensional magnet, with a framework developed by exact studies of the KPZ class, we characterize various two-point quantities that remain hitherto unexplored in spin chains, and find full agreement with KPZ scaling laws.","This establishes the partial emergence of the KPZ class in isotropic spin chains.","Moreover, we reveal that the KPZ scaling laws are intact in the presence of an energy current, under the appropriate Galilean boost required by the propagation of spacetime correlation."],"url":"http://arxiv.org/abs/2406.07150v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-11 10:43:07","title":"Ion Energy Tuning for Enhanced sp3 Carbon Fraction in Electron Cyclotron Resonance Ion Beam Deposited Diamond-Like Carbon Coatings: a Computational and Experimental Approach","abstract":"A novel high-energy electron cyclotron resonance (ECR) ion beam deposition (IBD) technique was used to fabricate DLC films at different ion beam energies. The ratios of sp2/sp3 bonding in the DLC coatings were determined by Raman spectroscopy and XPS, with the confirmation of being hydrogen-free due to the lack of photoluminescence (PL) background in the Raman spectra. The results indicate that the sp3 percentage ranges from 45% - 85% for the ECR-IBD fabricated DLC films in this study. Monte-Carlo based SRIM simulation was used to extract the energy and angular distribution of the sputtered particles from the carbon target and correlate it to the highest sp3 fraction in the manufactured ECR-IBD DLCs. This study demonstrates a method of depositing DLC thin films under ambient conditions (room temperature with no post-annealing or additional bias voltage applied) which produces high-sp3 coatings (higher than those traditionally reported for other sputtering methods) suitable for applications where high quality DLC coatings are required.","sentences":["A novel high-energy electron cyclotron resonance (ECR) ion beam deposition (IBD) technique was used to fabricate DLC films at different ion beam energies.","The ratios of sp2/sp3 bonding in the DLC coatings were determined by Raman spectroscopy and XPS, with the confirmation of being hydrogen-free due to the lack of photoluminescence (PL) background in the Raman spectra.","The results indicate that the sp3 percentage ranges from 45% - 85% for the ECR-IBD fabricated DLC films in this study.","Monte-Carlo based SRIM simulation was used to extract the energy and angular distribution of the sputtered particles from the carbon target and correlate it to the highest sp3 fraction in the manufactured ECR-IBD DLCs.","This study demonstrates a method of depositing DLC thin films under ambient conditions (room temperature with no post-annealing or additional bias voltage applied) which produces high-sp3 coatings (higher than those traditionally reported for other sputtering methods) suitable for applications where high quality DLC coatings are required."],"url":"http://arxiv.org/abs/2406.07144v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-11 09:49:00","title":"Agnostic Sharpness-Aware Minimization","abstract":"Sharpness-aware minimization (SAM) has been instrumental in improving deep neural network training by minimizing both the training loss and the sharpness of the loss landscape, leading the model into flatter minima that are associated with better generalization properties. In another aspect, Model-Agnostic Meta-Learning (MAML) is a framework designed to improve the adaptability of models. MAML optimizes a set of meta-models that are specifically tailored for quick adaptation to multiple tasks with minimal fine-tuning steps and can generalize well with limited data. In this work, we explore the connection between SAM and MAML, particularly in terms of enhancing model generalization. We introduce Agnostic-SAM, a novel approach that combines the principles of both SAM and MAML. Agnostic-SAM adapts the core idea of SAM by optimizing the model towards wider local minima using training data, while concurrently maintaining low loss values on validation data. By doing so, it seeks flatter minima that are not only robust to small perturbations but also less vulnerable to data distributional shift problems. Our experimental results demonstrate that Agnostic-SAM significantly improves generalization over baselines across a range of datasets and under challenging conditions such as noisy labels and data limitation.","sentences":["Sharpness-aware minimization (SAM) has been instrumental in improving deep neural network training by minimizing both the training loss and the sharpness of the loss landscape, leading the model into flatter minima that are associated with better generalization properties.","In another aspect, Model-Agnostic Meta-Learning (MAML) is a framework designed to improve the adaptability of models.","MAML optimizes a set of meta-models that are specifically tailored for quick adaptation to multiple tasks with minimal fine-tuning steps and can generalize well with limited data.","In this work, we explore the connection between SAM and MAML, particularly in terms of enhancing model generalization.","We introduce Agnostic-SAM, a novel approach that combines the principles of both SAM and MAML.","Agnostic-SAM adapts the core idea of SAM by optimizing the model towards wider local minima using training data, while concurrently maintaining low loss values on validation data.","By doing so, it seeks flatter minima that are not only robust to small perturbations but also less vulnerable to data distributional shift problems.","Our experimental results demonstrate that Agnostic-SAM significantly improves generalization over baselines across a range of datasets and under challenging conditions such as noisy labels and data limitation."],"url":"http://arxiv.org/abs/2406.07107v1","category":"cs.LG"}
{"created":"2024-06-11 09:47:13","title":"Displacement versus velocity memory effects from a gravitational plane wave","abstract":"This article demonstrates that additionally to the well-known velocity memory effect, a vacuum gravitational plane wave can also induce a displacement memory on a couple of test particles. A complete classification of the conditions under which a velocity or a displacement memory effect occur is established. These conditions depend both the initial conditions of the relative motion and on the wave profile. The two cases where the wave admits a pulse or a step profile are treated. Our analytical expressions are then compared to numerical integrations to exhibit either a velocity or a displacement memory, in the case of these two families of profiles. Additionally to this classification, the existence of a new symmetry of polarized vacuum gravitational plane wave under M\\\"{o}bius reparametrization of the null time is demonstrated. Finally, we discuss the resolution of the geodesic deviation equation by means of the underlying symmetries of vacuum gravitational plane wave.","sentences":["This article demonstrates that additionally to the well-known velocity memory effect, a vacuum gravitational plane wave can also induce a displacement memory on a couple of test particles.","A complete classification of the conditions under which a velocity or a displacement memory effect occur is established.","These conditions depend both the initial conditions of the relative motion and on the wave profile.","The two cases where the wave admits a pulse or a step profile are treated.","Our analytical expressions are then compared to numerical integrations to exhibit either a velocity or a displacement memory, in the case of these two families of profiles.","Additionally to this classification, the existence of a new symmetry of polarized vacuum gravitational plane wave under M\\\"{o}bius reparametrization of the null time is demonstrated.","Finally, we discuss the resolution of the geodesic deviation equation by means of the underlying symmetries of vacuum gravitational plane wave."],"url":"http://arxiv.org/abs/2406.07106v1","category":"gr-qc"}
{"created":"2024-06-11 09:37:59","title":"High-purity and stable single-photon emission in bilayer WSe$_2$ via phonon-assisted excitation","abstract":"The excitation scheme is essential for single-photon sources as it prepares the exciton state, defines the decay dynamics, and influences the spectral diffusion of the emitted single photons. Here, we investigate the impact of different optical excitation strategies on the single-photon emission characteristics of bilayer WSe$_2$ quantum emitters. Under phonon-assisted excitation, we achieve narrow and stable single-photon emission with an excellent purity reaching $ 0.94\\pm 0.02\\,$. Furthermore, the decay time is reduced by more than an order of magnitude from $(16.65 \\pm 2.39)\\,$ns for above-band excitation to $(1.33 \\pm 0.04)\\,$ns for phonon-assisted excitation. Finally, we observe a suppressed spectral wandering along with a two-fold reduction of the spectral linewidth. Our comprehensive investigation highlights the critical role of the excitation method in optimizing the performance of WSe$_2$-based quantum emitters.","sentences":["The excitation scheme is essential for single-photon sources as it prepares the exciton state, defines the decay dynamics, and influences the spectral diffusion of the emitted single photons.","Here, we investigate the impact of different optical excitation strategies on the single-photon emission characteristics of bilayer WSe$_2$ quantum emitters.","Under phonon-assisted excitation, we achieve narrow and stable single-photon emission with an excellent purity reaching $ 0.94\\pm 0.02\\,$.","Furthermore, the decay time is reduced by more than an order of magnitude from $(16.65 \\pm 2.39)\\,$ns for above-band excitation to $(1.33 \\pm 0.04)\\,$ns for phonon-assisted excitation.","Finally, we observe a suppressed spectral wandering along with a two-fold reduction of the spectral linewidth.","Our comprehensive investigation highlights the critical role of the excitation method in optimizing the performance of WSe$_2$-based quantum emitters."],"url":"http://arxiv.org/abs/2406.07097v1","category":"quant-ph"}
{"created":"2024-06-11 09:31:37","title":"AutoTVG: A New Vision-language Pre-training Paradigm for Temporal Video Grounding","abstract":"Temporal Video Grounding (TVG) aims to localize a moment from an untrimmed video given the language description. Since the annotation of TVG is labor-intensive, TVG under limited supervision has accepted attention in recent years. The great success of vision-language pre-training guides TVG to follow the traditional \"pre-training + fine-tuning\" paradigm, however, the pre-training process would suffer from a lack of temporal modeling and fine-grained alignment due to the difference of data nature between pre-train and test. Besides, the large gap between pretext and downstream tasks makes zero-shot testing impossible for the pre-trained model. To avoid the drawbacks of the traditional paradigm, we propose AutoTVG, a new vision-language pre-training paradigm for TVG that enables the model to learn semantic alignment and boundary regression from automatically annotated untrimmed videos. To be specific, AutoTVG consists of a novel Captioned Moment Generation (CMG) module to generate captioned moments from untrimmed videos, and TVGNet with a regression head to predict localization results. Experimental results on Charades-STA and ActivityNet Captions show that, regarding zero-shot temporal video grounding, AutoTVG achieves highly competitive performance with in-distribution methods under out-of-distribution testing, and is superior to existing pre-training frameworks with much less training data.","sentences":["Temporal Video Grounding (TVG) aims to localize a moment from an untrimmed video given the language description.","Since the annotation of TVG is labor-intensive, TVG under limited supervision has accepted attention in recent years.","The great success of vision-language pre-training guides TVG to follow the traditional \"pre-training + fine-tuning\" paradigm, however, the pre-training process would suffer from a lack of temporal modeling and fine-grained alignment due to the difference of data nature between pre-train and test.","Besides, the large gap between pretext and downstream tasks makes zero-shot testing impossible for the pre-trained model.","To avoid the drawbacks of the traditional paradigm, we propose AutoTVG, a new vision-language pre-training paradigm for TVG that enables the model to learn semantic alignment and boundary regression from automatically annotated untrimmed videos.","To be specific, AutoTVG consists of a novel Captioned Moment Generation (CMG) module to generate captioned moments from untrimmed videos, and TVGNet with a regression head to predict localization results.","Experimental results on Charades-STA and ActivityNet Captions show that, regarding zero-shot temporal video grounding, AutoTVG achieves highly competitive performance with in-distribution methods under out-of-distribution testing, and is superior to existing pre-training frameworks with much less training data."],"url":"http://arxiv.org/abs/2406.07091v1","category":"cs.CV"}
{"created":"2024-06-11 09:09:37","title":"DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs","abstract":"Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning autonomous language agents in various real-life applications. To improve the neural-symbolic reasoning capabilities of language agents powered by Large Language Models (LLMs) in KGQA, we propose the DecompositionAlignment-Reasoning Agent (DARA) framework. DARA effectively parses questions into formal queries through a dual mechanism: high-level iterative task decomposition and low-level task grounding. Importantly, DARA can be efficiently trained with a small number of high-quality reasoning trajectories. Our experimental results demonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms both in-context learning-based agents with GPT-4 and alternative fine-tuned agents, across different benchmarks in zero-shot evaluation, making such models more accessible for real-life applications. We also show that DARA attains performance comparable to state-of-the-art enumerating-and-ranking-based methods for KGQA.","sentences":["Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning autonomous language agents in various real-life applications.","To improve the neural-symbolic reasoning capabilities of language agents powered by Large Language Models (LLMs) in KGQA, we propose the DecompositionAlignment-Reasoning Agent (DARA) framework.","DARA effectively parses questions into formal queries through a dual mechanism: high-level iterative task decomposition and low-level task grounding.","Importantly, DARA can be efficiently trained with a small number of high-quality reasoning trajectories.","Our experimental results demonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms both in-context learning-based agents with GPT-4 and alternative fine-tuned agents, across different benchmarks in zero-shot evaluation, making such models more accessible for real-life applications.","We also show that DARA attains performance comparable to state-of-the-art enumerating-and-ranking-based methods for KGQA."],"url":"http://arxiv.org/abs/2406.07080v1","category":"cs.CL"}
{"created":"2024-06-11 08:45:46","title":"Modeling fibrous tissue in vascular fluid-structure interaction: a morphology-based pipeline and biomechanical significance","abstract":"We propose a suite of technologies for analyzing the interaction between anisotropic arterial walls and blood flow for subject-specific geometries. Utilizing an established lumen modeling strategy, we present a comprehensive pipeline for generating the thick-walled artery models. Through a specialized mesh generation procedure, we obtain the meshes for the arterial lumen and wall with mesh continuity across the interface ensured. Exploiting the centerline information, a series of procedures is introduced for generating local basis vectors within the arterial wall. The procedures are tailored to handle thick-walled and, in particular, aneurysmatic tissues in which the basis vectors may exhibit transmural variations. Additionally, we propose methods to accurately identify the centerline in multi-branched vessels and bifurcating regions. The developed fiber generation method is evaluated against the strategy using linear elastic analysis, demonstrating that the proposed approach yields satisfactory fiber definitions in the considered benchmark. Finally, we examine the impact of anisotropic arterial wall models on the vascular fluid-structure interaction analysis through numerical examples. For comparison purposes, the neo-Hookean model is considered. The first case involves an idealized curved geometry, while the second case studies an image-based abdominal aorta model. The numerical results reveal that the deformation and stress distribution are critically related to the constitutive model of the wall, while the hemodynamic factors are less sensitive to the wall model. This work paves the way for more accurate image-based vascular modeling and enhances the prediction of arterial behavior under physiologically realistic conditions.","sentences":["We propose a suite of technologies for analyzing the interaction between anisotropic arterial walls and blood flow for subject-specific geometries.","Utilizing an established lumen modeling strategy, we present a comprehensive pipeline for generating the thick-walled artery models.","Through a specialized mesh generation procedure, we obtain the meshes for the arterial lumen and wall with mesh continuity across the interface ensured.","Exploiting the centerline information, a series of procedures is introduced for generating local basis vectors within the arterial wall.","The procedures are tailored to handle thick-walled and, in particular, aneurysmatic tissues in which the basis vectors may exhibit transmural variations.","Additionally, we propose methods to accurately identify the centerline in multi-branched vessels and bifurcating regions.","The developed fiber generation method is evaluated against the strategy using linear elastic analysis, demonstrating that the proposed approach yields satisfactory fiber definitions in the considered benchmark.","Finally, we examine the impact of anisotropic arterial wall models on the vascular fluid-structure interaction analysis through numerical examples.","For comparison purposes, the neo-Hookean model is considered.","The first case involves an idealized curved geometry, while the second case studies an image-based abdominal aorta model.","The numerical results reveal that the deformation and stress distribution are critically related to the constitutive model of the wall, while the hemodynamic factors are less sensitive to the wall model.","This work paves the way for more accurate image-based vascular modeling and enhances the prediction of arterial behavior under physiologically realistic conditions."],"url":"http://arxiv.org/abs/2406.07064v1","category":"physics.med-ph"}
{"created":"2024-06-11 08:42:07","title":"Triage of 3D pathology data via 2.5D multiple-instance learning to guide pathologist assessments","abstract":"Accurate patient diagnoses based on human tissue biopsies are hindered by current clinical practice, where pathologists assess only a limited number of thin 2D tissue slices sectioned from 3D volumetric tissue. Recent advances in non-destructive 3D pathology, such as open-top light-sheet microscopy, enable comprehensive imaging of spatially heterogeneous tissue morphologies, offering the feasibility to improve diagnostic determinations. A potential early route towards clinical adoption for 3D pathology is to rely on pathologists for final diagnosis based on viewing familiar 2D H&E-like image sections from the 3D datasets. However, manual examination of the massive 3D pathology datasets is infeasible. To address this, we present CARP3D, a deep learning triage approach that automatically identifies the highest-risk 2D slices within 3D volumetric biopsy, enabling time-efficient review by pathologists. For a given slice in the biopsy, we estimate its risk by performing attention-based aggregation of 2D patches within each slice, followed by pooling of the neighboring slices to compute a context-aware 2.5D risk score. For prostate cancer risk stratification, CARP3D achieves an area under the curve (AUC) of 90.4% for triaging slices, outperforming methods relying on independent analysis of 2D sections (AUC=81.3%). These results suggest that integrating additional depth context enhances the model's discriminative capabilities. In conclusion, CARP3D has the potential to improve pathologist diagnosis via accurate triage of high-risk slices within large-volume 3D pathology datasets.","sentences":["Accurate patient diagnoses based on human tissue biopsies are hindered by current clinical practice, where pathologists assess only a limited number of thin 2D tissue slices sectioned from 3D volumetric tissue.","Recent advances in non-destructive 3D pathology, such as open-top light-sheet microscopy, enable comprehensive imaging of spatially heterogeneous tissue morphologies, offering the feasibility to improve diagnostic determinations.","A potential early route towards clinical adoption for 3D pathology is to rely on pathologists for final diagnosis based on viewing familiar 2D H&E-like image sections from the 3D datasets.","However, manual examination of the massive 3D pathology datasets is infeasible.","To address this, we present CARP3D, a deep learning triage approach that automatically identifies the highest-risk 2D slices within 3D volumetric biopsy, enabling time-efficient review by pathologists.","For a given slice in the biopsy, we estimate its risk by performing attention-based aggregation of 2D patches within each slice, followed by pooling of the neighboring slices to compute a context-aware 2.5D risk score.","For prostate cancer risk stratification, CARP3D achieves an area under the curve (AUC) of 90.4% for triaging slices, outperforming methods relying on independent analysis of 2D sections (AUC=81.3%).","These results suggest that integrating additional depth context enhances the model's discriminative capabilities.","In conclusion, CARP3D has the potential to improve pathologist diagnosis via accurate triage of high-risk slices within large-volume 3D pathology datasets."],"url":"http://arxiv.org/abs/2406.07061v1","category":"eess.IV"}
{"created":"2024-06-11 08:38:24","title":"The evolution of coronal shock wave properties and their relation with solar energetic particles","abstract":"Shock waves driven by fast and wide coronal mass ejections (CMEs) are highly efficient particle accelerators involved in the production of solar energetic particle (SEP) events. The gradual SEP event measured by STEREO-A and B on October 11, 2013 had notable properties: (1) it occurred in isolation with very low background particle intensities, (2) it had a clear onset of SEPs measured in situ allowing detailed timing analyses, and (3) it was associated with a fast CME event magnetically connected with STA and B. These allowed us to investigate the temporal connection between the rapidly evolving shock properties, such as compression ratio, Mach number and geometry, and the intensity and composition of SEPs measured in situ. We use shock reconstruction techniques and multi-viewpoint imaging data from STA and B, SOHO, and SDO spacecraft to determine the kinematic evolution of the expanding shock wave. Using 3D magneto-hydrodynamic modelling we obtained shock wave properties along an ensemble of magnetic field lines connected to STA and B, estimating their uncertainties. Using a velocity dispersion analysis of the SEP data, we time shift the SEP time series and analyze the relations between their properties and the modeled shock ones, as well as the energy dependence of these relations. We find a very good temporal agreement between the formation of the modelled shock wave and the estimated release times for both electrons and protons. This simultaneous release suggests a common acceleration process. This early phase is marked at both STEREOs by elevated electron-to-proton ratios that coincide with the highly quasi-perpendicular phase of the shock, suggesting that the rapid evolution of the shock as it transits from the low to the high corona modifies the conditions under which particles are accelerated. We discuss these findings in terms of basic geometry and acceleration processes.","sentences":["Shock waves driven by fast and wide coronal mass ejections (CMEs) are highly efficient particle accelerators involved in the production of solar energetic particle (SEP) events.","The gradual SEP event measured by STEREO-A and B on October 11, 2013 had notable properties: (1) it occurred in isolation with very low background particle intensities, (2) it had a clear onset of SEPs measured in situ allowing detailed timing analyses, and (3) it was associated with a fast CME event magnetically connected with STA and B.","These allowed us to investigate the temporal connection between the rapidly evolving shock properties, such as compression ratio, Mach number and geometry, and the intensity and composition of SEPs measured in situ.","We use shock reconstruction techniques and multi-viewpoint imaging data from STA and B, SOHO, and SDO spacecraft to determine the kinematic evolution of the expanding shock wave.","Using 3D magneto-hydrodynamic modelling we obtained shock wave properties along an ensemble of magnetic field lines connected to STA and B, estimating their uncertainties.","Using a velocity dispersion analysis of the SEP data, we time shift the SEP time series and analyze the relations between their properties and the modeled shock ones, as well as the energy dependence of these relations.","We find a very good temporal agreement between the formation of the modelled shock wave and the estimated release times for both electrons and protons.","This simultaneous release suggests a common acceleration process.","This early phase is marked at both STEREOs by elevated electron-to-proton ratios that coincide with the highly quasi-perpendicular phase of the shock, suggesting that the rapid evolution of the shock as it transits from the low to the high corona modifies the conditions under which particles are accelerated.","We discuss these findings in terms of basic geometry and acceleration processes."],"url":"http://arxiv.org/abs/2406.07058v1","category":"astro-ph.SR"}
{"created":"2024-06-11 08:21:30","title":"GPU-Accelerated Optimization-Based Collision Avoidance","abstract":"This paper proposes a GPU-accelerated optimization framework for collision avoidance problems where the controlled objects and the obstacles can be modeled as the finite union of convex polyhedra. A novel collision avoidance constraint is proposed based on scale-based collision detection and the strong duality of convex optimization. Under this constraint, the high-dimensional non-convex optimization problems of collision avoidance can be decomposed into several low-dimensional quadratic programmings (QPs) following the paradigm of alternating direction method of multipliers (ADMM). Furthermore, these low-dimensional QPs can be solved parallel with GPUs, significantly reducing computational time. High-fidelity simulations are conducted to validate the proposed method's effectiveness and practicality.","sentences":["This paper proposes a GPU-accelerated optimization framework for collision avoidance problems where the controlled objects and the obstacles can be modeled as the finite union of convex polyhedra.","A novel collision avoidance constraint is proposed based on scale-based collision detection and the strong duality of convex optimization.","Under this constraint, the high-dimensional non-convex optimization problems of collision avoidance can be decomposed into several low-dimensional quadratic programmings (QPs) following the paradigm of alternating direction method of multipliers (ADMM).","Furthermore, these low-dimensional QPs can be solved parallel with GPUs, significantly reducing computational time.","High-fidelity simulations are conducted to validate the proposed method's effectiveness and practicality."],"url":"http://arxiv.org/abs/2406.07048v1","category":"cs.RO"}
{"created":"2024-06-11 08:01:02","title":"EFFOcc: A Minimal Baseline for EFficient Fusion-based 3D Occupancy Network","abstract":"3D occupancy prediction (Occ) is a rapidly rising challenging perception task in the field of autonomous driving which represents the driving scene as uniformly partitioned 3D voxel grids with semantics. Compared to 3D object detection, grid perception has great advantage of better recognizing irregularly shaped, unknown category, or partially occluded general objects. However, existing 3D occupancy networks (occnets) are both computationally heavy and label-hungry. In terms of model complexity, occnets are commonly composed of heavy Conv3D modules or transformers on the voxel level. In terms of label annotations requirements, occnets are supervised with large-scale expensive dense voxel labels. Model and data inefficiency, caused by excessive network parameters and label annotations requirement, severely hinder the onboard deployment of occnets. This paper proposes an efficient 3d occupancy network (EFFOcc), that targets the minimal network complexity and label requirement while achieving state-of-the-art accuracy. EFFOcc only uses simple 2D operators, and improves Occ accuracy to the state-of-the-art on multiple large-scale benchmarks: Occ3D-nuScenes, Occ3D-Waymo, and OpenOccupancy-nuScenes. On Occ3D-nuScenes benchmark, EFFOcc has only 18.4M parameters, and achieves 50.46 in terms of mean IoU (mIoU), to our knowledge, it is the occnet with minimal parameters compared with related occnets. Moreover, we propose a two-stage active learning strategy to reduce the requirements of labelled data. Active EFFOcc trained with 6\\% labelled voxels achieves 47.19 mIoU, which is 95.7% fully supervised performance. The proposed EFFOcc also supports improved vision-only occupancy prediction with the aid of region-decomposed distillation. Code and demo videos will be available at https://github.com/synsin0/EFFOcc.","sentences":["3D occupancy prediction (Occ) is a rapidly rising challenging perception task in the field of autonomous driving which represents the driving scene as uniformly partitioned 3D voxel grids with semantics.","Compared to 3D object detection, grid perception has great advantage of better recognizing irregularly shaped, unknown category, or partially occluded general objects.","However, existing 3D occupancy networks (occnets) are both computationally heavy and label-hungry.","In terms of model complexity, occnets are commonly composed of heavy Conv3D modules or transformers on the voxel level.","In terms of label annotations requirements, occnets are supervised with large-scale expensive dense voxel labels.","Model and data inefficiency, caused by excessive network parameters and label annotations requirement, severely hinder the onboard deployment of occnets.","This paper proposes an efficient 3d occupancy network (EFFOcc), that targets the minimal network complexity and label requirement while achieving state-of-the-art accuracy.","EFFOcc only uses simple 2D operators, and improves Occ accuracy to the state-of-the-art on multiple large-scale benchmarks: Occ3D-nuScenes, Occ3D-Waymo, and OpenOccupancy-nuScenes.","On Occ3D-nuScenes benchmark, EFFOcc has only 18.4M parameters, and achieves 50.46 in terms of mean IoU (mIoU), to our knowledge, it is the occnet with minimal parameters compared with related occnets.","Moreover, we propose a two-stage active learning strategy to reduce the requirements of labelled data.","Active EFFOcc trained with 6\\% labelled voxels achieves 47.19 mIoU, which is 95.7% fully supervised performance.","The proposed EFFOcc also supports improved vision-only occupancy prediction with the aid of region-decomposed distillation.","Code and demo videos will be available at https://github.com/synsin0/EFFOcc."],"url":"http://arxiv.org/abs/2406.07042v1","category":"cs.CV"}
{"created":"2024-06-11 07:48:20","title":"Random centers of localization for random operators","abstract":"We propose a new random process to construct the eigenvectors of some random operators which make a short and clean connection with the resolvent. In this process the center of localization has to be chosen randomly.","sentences":["We propose a new random process to construct the eigenvectors of some random operators which make a short and clean connection with the resolvent.","In this process the center of localization has to be chosen randomly."],"url":"http://arxiv.org/abs/2406.07035v1","category":"math.PR"}
{"created":"2024-06-11 07:40:46","title":"Arbitrary-Order Distributed Finite-Time Differentiator for Multi-Agent Systems","abstract":"This paper proposes arbitrary-order distributed finite-time differentiator (AODFD) for leader-follower multi-agent systems (MAS) under directed graph by only using relative or absolute output information. By using arbitrary-order distributed finite-time differentiator via relative output information (AODFD-R), each follower agent can obtain the relative output information between itself and leader and the relative output's arbitrary-order derivatives, where the information to be measured is only the local relative output information between each follower agent and its neighboring agents. As a simple extension of AODFD-R, the arbitrary-order distributed finite-time differentiator via absolute output information (AODFD-A) is also given. The finite-time stability of the closed-loop system under AODFD is proved by constructing a Lyapunov function skillfully. Finally, several simulation examples are given to verify the effectiveness of the AODFD.","sentences":["This paper proposes arbitrary-order distributed finite-time differentiator (AODFD) for leader-follower multi-agent systems (MAS) under directed graph by only using relative or absolute output information.","By using arbitrary-order distributed finite-time differentiator via relative output information (AODFD-R), each follower agent can obtain the relative output information between itself and leader and the relative output's arbitrary-order derivatives, where the information to be measured is only the local relative output information between each follower agent and its neighboring agents.","As a simple extension of AODFD-R, the arbitrary-order distributed finite-time differentiator via absolute output information (AODFD-A) is also given.","The finite-time stability of the closed-loop system under AODFD is proved by constructing a Lyapunov function skillfully.","Finally, several simulation examples are given to verify the effectiveness of the AODFD."],"url":"http://arxiv.org/abs/2406.07031v1","category":"cs.MA"}
{"created":"2024-06-11 07:31:38","title":"Majority Dynamics and Internal Partitions of Random Regular Graphs: Experimental Results","abstract":"This paper focuses on Majority Dynamics in sparse graphs, in particular, as a tool to study internal cuts. It is known that, in Majority Dynamics on a finite graph, each vertex eventually either comes to a fixed state, or oscillates with period two. The empirical evidence acquired by simulations suggests that for random odd-regular graphs, approximately half of the vertices end up oscillating with high probability. We notice a local symmetry between oscillating and non-oscillating vertices, that potentially can explain why the fraction of the oscillating vertices is concentrated around $\\frac{1}{2}$. In our simulations, we observe that the parts of random odd-regular graph under Majority Dynamics with high probability do not contain $\\lceil \\frac{d}{2} \\rceil$-cores at any timestep, and thus, one cannot use Majority Dynamics to prove that internal cuts exist in odd-regular graphs almost surely. However, we suggest a modification of Majority Dynamics, that yields parts with desired cores with high probability.","sentences":["This paper focuses on Majority Dynamics in sparse graphs, in particular, as a tool to study internal cuts.","It is known that, in Majority Dynamics on a finite graph, each vertex eventually either comes to a fixed state, or oscillates with period two.","The empirical evidence acquired by simulations suggests that for random odd-regular graphs, approximately half of the vertices end up oscillating with high probability.","We notice a local symmetry between oscillating and non-oscillating vertices, that potentially can explain why the fraction of the oscillating vertices is concentrated around $\\frac{1}{2}$. In our simulations, we observe that the parts of random odd-regular graph under Majority Dynamics with high probability do not contain $\\lceil \\frac{d}{2} \\rceil$-cores at any timestep, and thus, one cannot use Majority Dynamics to prove that internal cuts exist in odd-regular graphs almost surely.","However, we suggest a modification of Majority Dynamics, that yields parts with desired cores with high probability."],"url":"http://arxiv.org/abs/2406.07026v1","category":"math.CO"}
{"created":"2024-06-11 07:25:17","title":"Learning Discrete Latent Variable Structures with Tensor Rank Conditions","abstract":"Unobserved discrete data are ubiquitous in many scientific disciplines, and how to learn the causal structure of these latent variables is crucial for uncovering data patterns. Most studies focus on the linear latent variable model or impose strict constraints on latent structures, which fail to address cases in discrete data involving non-linear relationships or complex latent structures. To achieve this, we explore a tensor rank condition on contingency tables for an observed variable set $\\mathbf{X}_p$, showing that the rank is determined by the minimum support of a specific conditional set (not necessary in $\\mathbf{X}_p$) that d-separates all variables in $\\mathbf{X}_p$. By this, one can locate the latent variable through probing the rank on different observed variables set, and further identify the latent causal structure under some structure assumptions. We present the corresponding identification algorithm and conduct simulated experiments to verify the effectiveness of our method. In general, our results elegantly extend the identification boundary for causal discovery with discrete latent variables and expand the application scope of causal discovery with latent variables.","sentences":["Unobserved discrete data are ubiquitous in many scientific disciplines, and how to learn the causal structure of these latent variables is crucial for uncovering data patterns.","Most studies focus on the linear latent variable model or impose strict constraints on latent structures, which fail to address cases in discrete data involving non-linear relationships or complex latent structures.","To achieve this, we explore a tensor rank condition on contingency tables for an observed variable set $\\mathbf{X}_p$, showing that the rank is determined by the minimum support of a specific conditional set (not necessary in $\\mathbf{X}_p$) that d-separates all variables in $\\mathbf{X}_p$. By this, one can locate the latent variable through probing the rank on different observed variables set, and further identify the latent causal structure under some structure assumptions.","We present the corresponding identification algorithm and conduct simulated experiments to verify the effectiveness of our method.","In general, our results elegantly extend the identification boundary for causal discovery with discrete latent variables and expand the application scope of causal discovery with latent variables."],"url":"http://arxiv.org/abs/2406.07020v1","category":"cs.LG"}
{"created":"2024-06-11 07:19:04","title":"MoreauPruner: Robust Pruning of Large Language Models against Weight Perturbations","abstract":"Few-shot gradient methods have been extensively utilized in existing model pruning methods, where the model weights are regarded as static values and the effects of potential weight perturbations are not considered. However, the widely used large language models (LLMs) have several billion model parameters, which could increase the fragility of few-shot gradient pruning. In this work, we experimentally show that one-shot gradient pruning algorithms could lead to unstable results under perturbations to model weights. And the minor error of switching between data formats bfloat16 and float16 could result in drastically different outcomes. To address such instabilities, we leverage optimization analysis and propose an LLM structural pruning method, called MoreauPruner, with provable robustness against weight perturbations. In MoreauPruner, the model weight importance is estimated based on the neural network's Moreau envelope, which can be flexibly combined with $\\ell_1$-norm regularization techniques to induce the sparsity required in the pruning task. We extensively evaluate the MoreauPruner algorithm on several well-known LLMs, including LLaMA-7B, LLaMA-13B, LLaMA3-8B, and Vicuna-7B. Our numerical results suggest the robustness of MoreauPruner against weight perturbations, and indicate the MoreauPruner's successful accuracy-based scores in comparison to several existing pruning methods. We have released the code in \\url{https://github.com/ShiningSord/MoreauPruner}.","sentences":["Few-shot gradient methods have been extensively utilized in existing model pruning methods, where the model weights are regarded as static values and the effects of potential weight perturbations are not considered.","However, the widely used large language models (LLMs) have several billion model parameters, which could increase the fragility of few-shot gradient pruning.","In this work, we experimentally show that one-shot gradient pruning algorithms could lead to unstable results under perturbations to model weights.","And the minor error of switching between data formats bfloat16 and float16 could result in drastically different outcomes.","To address such instabilities, we leverage optimization analysis and propose an LLM structural pruning method, called MoreauPruner, with provable robustness against weight perturbations.","In MoreauPruner, the model weight importance is estimated based on the neural network's Moreau envelope, which can be flexibly combined with $\\ell_1$-norm regularization techniques to induce the sparsity required in the pruning task.","We extensively evaluate the MoreauPruner algorithm on several well-known LLMs, including LLaMA-7B, LLaMA-13B, LLaMA3-8B, and Vicuna-7B. Our numerical results suggest the robustness of MoreauPruner against weight perturbations, and indicate the MoreauPruner's successful accuracy-based scores in comparison to several existing pruning methods.","We have released the code in \\url{https://github.com/ShiningSord/MoreauPruner}."],"url":"http://arxiv.org/abs/2406.07017v1","category":"cs.LG"}
{"created":"2024-06-11 07:16:16","title":"On symmetric plane quartic curves","abstract":"In the present paper we study the geometry of plane quartics with large automorphism groups. We show results devoted to smooth plane quartics that are invariant under the action of the elementary abelian group of type $[2,2,2]$, and we study geometric properties of the smooth plane quartic having automorphism group of order $48$.","sentences":["In the present paper we study the geometry of plane quartics with large automorphism groups.","We show results devoted to smooth plane quartics that are invariant under the action of the elementary abelian group of type $[2,2,2]$, and we study geometric properties of the smooth plane quartic having automorphism group of order $48$."],"url":"http://arxiv.org/abs/2406.07015v1","category":"math.AG"}
{"created":"2024-06-11 07:10:03","title":"Topological phase transition in fluctuating imaginary gauge fields","abstract":"We investigate the exact solvability and point-gap topological phase transitions in non-Hermitian lattice models. These models incorporate site-dependent nonreciprocal hoppings $J e^{\\pm g_n}$, facilitated by a spatially fluctuating imaginary gauge field $ig_n \\hat~x$ that disrupts translational symmetry. By employing suitable imaginary gauge transformations, it is revealed that a lattice characterized by any given $g_n$ is spectrally equivalent to a lattice devoid of fields, under open boundary conditions. Furthermore, a system with closed boundaries can be simplified to a spectrally equivalent lattice featuring a uniform mean field $i\\bar{g}\\hat~x$. This framework offers a comprehensive method for analytically predicting spectral topological invariance and associated boundary localization phenomena for bond-disordered nonperiodic lattices. These predictions are made by analyzing gauge-transformed isospectral periodic lattices. Notably, for a lattice with quasiperiodic $g_n= \\ln |\\lambda \\cos 2\\pi \\alpha n|$ and an irrational $\\alpha$, a previously unknown topological phase transition is unveiled. It is observed that the topological spectral index $W$ assumes values of $-N$ or $+N$, leading to all $N$ open-boundary eigenstates localizing either at the right or left edge, solely dependent on the strength of the gauge field, where $\\lambda<2$ or $\\lambda>2$. A phase transition is identified at the critical point $\\lambda\\approx2$, at which all eigenstates undergo delocalization. The theory has been shown to be relevant for long-range hopping models and for higher dimensions.","sentences":["We investigate the exact solvability and point-gap topological phase transitions in non-Hermitian lattice models.","These models incorporate site-dependent nonreciprocal hoppings $J e^{\\pm g_n}$, facilitated by a spatially fluctuating imaginary gauge field $ig_n \\hat~x$ that disrupts translational symmetry.","By employing suitable imaginary gauge transformations, it is revealed that a lattice characterized by any given $g_n$ is spectrally equivalent to a lattice devoid of fields, under open boundary conditions.","Furthermore, a system with closed boundaries can be simplified to a spectrally equivalent lattice featuring a uniform mean field $i\\bar{g}\\hat~x$.","This framework offers a comprehensive method for analytically predicting spectral topological invariance and associated boundary localization phenomena for bond-disordered nonperiodic lattices.","These predictions are made by analyzing gauge-transformed isospectral periodic lattices.","Notably, for a lattice with quasiperiodic $g_n= \\ln |\\lambda \\cos 2\\pi \\alpha n|$ and an irrational $\\alpha$, a previously unknown topological phase transition is unveiled.","It is observed that the topological spectral index $W$ assumes values of $-N$ or $+N$, leading to all $N$ open-boundary eigenstates localizing either at the right or left edge, solely dependent on the strength of the gauge field, where $\\lambda<2$ or $\\lambda>2$. A phase transition is identified at the critical point $\\lambda\\approx2$, at which all eigenstates undergo delocalization.","The theory has been shown to be relevant for long-range hopping models and for higher dimensions."],"url":"http://arxiv.org/abs/2406.07009v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-11 06:59:17","title":"DecoR: Deconfounding Time Series with Robust Regression","abstract":"Causal inference on time series data is a challenging problem, especially in the presence of unobserved confounders. This work focuses on estimating the causal effect between two time series, which are confounded by a third, unobserved time series. Assuming spectral sparsity of the confounder, we show how in the frequency domain this problem can be framed as an adversarial outlier problem. We introduce Deconfounding by Robust regression (DecoR), a novel approach that estimates the causal effect using robust linear regression in the frequency domain. Considering two different robust regression techniques, we first improve existing bounds on the estimation error for such techniques. Crucially, our results do not require distributional assumptions on the covariates. We can therefore use them in time series settings. Applying these results to DecoR, we prove, under suitable assumptions, upper bounds for the estimation error of DecoR that imply consistency. We show DecoR's effectiveness through experiments on synthetic data. Our experiments furthermore suggest that our method is robust with respect to model misspecification.","sentences":["Causal inference on time series data is a challenging problem, especially in the presence of unobserved confounders.","This work focuses on estimating the causal effect between two time series, which are confounded by a third, unobserved time series.","Assuming spectral sparsity of the confounder, we show how in the frequency domain this problem can be framed as an adversarial outlier problem.","We introduce Deconfounding by Robust regression (DecoR), a novel approach that estimates the causal effect using robust linear regression in the frequency domain.","Considering two different robust regression techniques, we first improve existing bounds on the estimation error for such techniques.","Crucially, our results do not require distributional assumptions on the covariates.","We can therefore use them in time series settings.","Applying these results to DecoR, we prove, under suitable assumptions, upper bounds for the estimation error of DecoR that imply consistency.","We show DecoR's effectiveness through experiments on synthetic data.","Our experiments furthermore suggest that our method is robust with respect to model misspecification."],"url":"http://arxiv.org/abs/2406.07005v1","category":"stat.ML"}
{"created":"2024-06-11 06:53:39","title":"Atmospheric Centers of Action: current features and expected changes from simulations with CMIP5 and CMIP6 models","abstract":"The results of an analysis of changes in the characteristics of atmospheric centers of action (ACAs) in the Northern (NH) and Southern (SH) hemispheres using results of simulations with the CMIP5 and CMIP6 ensembles of climate models are presented. The ability of models to simulate ACA features is estimated for the historical scenario in comparison with ERA5 reanalysis data. The projected changes are evaluated under RCP8.5 and SSP5-8.5 scenarios for CMIP5 and CMIP6 models, respectively. The ACA intensity is evaluated that defined as the difference in sea level pressure averaged over the ACA region and the entire hemisphere. In NH, reanalysis and models show greater intensity of subtropical oceanic anticyclonic ACAs in summer than in winter. The opposite is found for the intensity of NH subpolar oceanic cyclonic ACAs. The interannual variability of the ACA intensity in winter is generally greater than in summer. In SH, the season with greater intensity of oceanic anticyclonic and cyclonic ACAs and its interannual variability varies from ocean to ocean. CMIP5 and CMIP6 models show substantial changes of ACAs characteristics in the XXI century. More significant trends in the strengthening of ACAs in the 21st century appear in the SH, especially in the winter seasons. The most consistent weakening trends are found over continents for winter North American maximum and the summer Asian minimum. For the winter Siberian maximum, the weakening trend is found more pronounced in CMIP6 models than in CMIP5.","sentences":["The results of an analysis of changes in the characteristics of atmospheric centers of action (ACAs) in the Northern (NH) and Southern (SH) hemispheres using results of simulations with the CMIP5 and CMIP6 ensembles of climate models are presented.","The ability of models to simulate ACA features is estimated for the historical scenario in comparison with ERA5 reanalysis data.","The projected changes are evaluated under RCP8.5 and SSP5-8.5 scenarios for CMIP5 and CMIP6 models, respectively.","The ACA intensity is evaluated that defined as the difference in sea level pressure averaged over the ACA region and the entire hemisphere.","In NH, reanalysis and models show greater intensity of subtropical oceanic anticyclonic ACAs in summer than in winter.","The opposite is found for the intensity of NH subpolar oceanic cyclonic ACAs.","The interannual variability of the ACA intensity in winter is generally greater than in summer.","In SH, the season with greater intensity of oceanic anticyclonic and cyclonic ACAs and its interannual variability varies from ocean to ocean.","CMIP5 and CMIP6 models show substantial changes of ACAs characteristics in the XXI century.","More significant trends in the strengthening of ACAs in the 21st century appear in the SH, especially in the winter seasons.","The most consistent weakening trends are found over continents for winter North American maximum and the summer Asian minimum.","For the winter Siberian maximum, the weakening trend is found more pronounced in CMIP6 models than in CMIP5."],"url":"http://arxiv.org/abs/2406.07002v1","category":"physics.ao-ph"}
{"created":"2024-06-11 06:51:02","title":"Teaching with Uncertainty: Unleashing the Potential of Knowledge Distillation in Object Detection","abstract":"Knowledge distillation (KD) is a widely adopted and effective method for compressing models in object detection tasks. Particularly, feature-based distillation methods have shown remarkable performance. Existing approaches often ignore the uncertainty in the teacher model's knowledge, which stems from data noise and imperfect training. This limits the student model's ability to learn latent knowledge, as it may overly rely on the teacher's imperfect guidance. In this paper, we propose a novel feature-based distillation paradigm with knowledge uncertainty for object detection, termed \"Uncertainty Estimation-Discriminative Knowledge Extraction-Knowledge Transfer (UET)\", which can seamlessly integrate with existing distillation methods. By leveraging the Monte Carlo dropout technique, we introduce knowledge uncertainty into the training process of the student model, facilitating deeper exploration of latent knowledge. Our method performs effectively during the KD process without requiring intricate structures or extensive computational resources. Extensive experiments validate the effectiveness of our proposed approach across various distillation strategies, detectors, and backbone architectures. Specifically, following our proposed paradigm, the existing FGD method achieves state-of-the-art (SoTA) performance, with ResNet50-based GFL achieving 44.1% mAP on the COCO dataset, surpassing the baselines by 3.9%.","sentences":["Knowledge distillation (KD) is a widely adopted and effective method for compressing models in object detection tasks.","Particularly, feature-based distillation methods have shown remarkable performance.","Existing approaches often ignore the uncertainty in the teacher model's knowledge, which stems from data noise and imperfect training.","This limits the student model's ability to learn latent knowledge, as it may overly rely on the teacher's imperfect guidance.","In this paper, we propose a novel feature-based distillation paradigm with knowledge uncertainty for object detection, termed \"Uncertainty Estimation-Discriminative Knowledge Extraction-Knowledge Transfer (UET)\", which can seamlessly integrate with existing distillation methods.","By leveraging the Monte Carlo dropout technique, we introduce knowledge uncertainty into the training process of the student model, facilitating deeper exploration of latent knowledge.","Our method performs effectively during the KD process without requiring intricate structures or extensive computational resources.","Extensive experiments validate the effectiveness of our proposed approach across various distillation strategies, detectors, and backbone architectures.","Specifically, following our proposed paradigm, the existing FGD method achieves state-of-the-art (SoTA) performance, with ResNet50-based GFL achieving 44.1% mAP on the COCO dataset, surpassing the baselines by 3.9%."],"url":"http://arxiv.org/abs/2406.06999v1","category":"cs.CV"}
{"created":"2024-06-11 06:25:41","title":"Open Packing in Graphs: Bounds and Complexity","abstract":"Given a graph $G(V,E)$, a vertex subset S of G is called an open packing in G if no pair of distinct vertices in S have a common neighbour in G. The size of a largest open packing in G is called the open packing number of G and is denoted by $\\rho^o(G)$. It would be interesting to note that the open packing number is a lower bound for the total domination number in graphs with no isolated vertices [Henning and Slater, 1999]. Given a graph G and a positive integer k, the decision problem OPEN PACKING tests whether G has an open packing of size at least k. The optimization problem MAX-OPEN PACKING takes a graph G as input and finds the open packing number of G. It is known that OPEN PACKING is NP-complete on split graphs (i.e., the class of $\\{2K_2,C_4,C_5\\}$-free graphs) [Ramos et al., 2014]. In this work, we complete the study on the complexity (P vs NPC) of OPEN PACKING on H-free graphs for every graph H with at least three vertices by proving that OPEN PACKING is (i) NP-complete on $K_{1,3}$-free graphs and (ii) polynomial time solvable on $(P_4\\cup rK_1)$-free graphs for every $r\\geq 1$. In the course of proving (ii), we show that for every $t\\in {2,3,4}$ and $r\\geq 1$, if G is a $(P_t\\cup rK_1)$-free graph, then $\\rho^o(G)$ is bounded above by a linear function of r. Moreover, we show that OPEN PACKING parameterized by solution size is W[1]-complete on $K_{1,3}$-free graphs and MAX-OPEN PACKING is hard to approximate within a factor of $n^{(\\frac{1}{2}-\\delta)}$ for any $\\delta>0$ on $K_{1,3}$-free graphs unless P=NP. Further, we prove that OPEN PACKING is (a) NP-complete on $K_{1,4}$-free split graphs and (b) polynomial time solvable on $K_{1,3}$-free split graphs. We prove a similar dichotomy result on split graphs with degree restrictions on the vertices in the independent set of the clique-independent set partition of the split graphs.","sentences":["Given a graph $G(V,E)$, a vertex subset S of G is called an open packing in G if no pair of distinct vertices in S have a common neighbour in G. The size of a largest open packing in G is called the open packing number of G and is denoted by $\\rho^o(G)$. It would be interesting to note that the open packing number is a lower bound for the total domination number in graphs with no isolated vertices [Henning and Slater, 1999].","Given a graph G and a positive integer k, the decision problem OPEN PACKING tests whether G has an open packing of size at least k. The optimization problem MAX-OPEN PACKING takes a graph G as input and finds the open packing number of G.","It is known that OPEN PACKING is NP-complete on split graphs (i.e., the class of $\\{2K_2,C_4,C_5\\}$-free graphs)","[Ramos et al., 2014].","In this work, we complete the study on the complexity (P vs NPC) of OPEN PACKING on H-free graphs for every graph H with at least three vertices by proving that OPEN PACKING is (i) NP-complete on $K_{1,3}$-free graphs and (ii) polynomial time solvable on $(P_4\\cup rK_1)$-free graphs for every $r\\geq 1$.","In the course of proving (ii), we show that for every $t\\in {2,3,4}$ and $r\\geq 1$, if G is a $(P_t\\cup rK_1)$-free graph, then $\\rho^o(G)$ is bounded above by a linear function of r.","Moreover, we show that OPEN PACKING parameterized by solution size is W[1]-complete on $K_{1,3}$-free graphs and MAX-OPEN PACKING is hard to approximate within a factor of $n^{(\\frac{1}{2}-\\delta)}$ for any $\\delta>0$ on $K_{1,3}$-free graphs unless P=NP.","Further, we prove that OPEN PACKING is (a) NP-complete on $K_{1,4}$-free split graphs and (b) polynomial time solvable on $K_{1,3}$-free split graphs.","We prove a similar dichotomy result on split graphs with degree restrictions on the vertices in the independent set of the clique-independent set partition of the split graphs."],"url":"http://arxiv.org/abs/2406.06982v1","category":"cs.DM"}
{"created":"2024-06-11 05:46:35","title":"DHR+S: Distributed Hybrid Rendering with Realistic Real-time Shadows for Interactive Thin Client Metaverse and Game Applications","abstract":"Distributed hybrid rendering (DHR) is a real-time rendering approach that incorporates cloud-based ray tracing with locally rasterized graphics for interactive thin client metaverse and game applications. With cloud assistance, DHR can generate high-fidelity ray-traced graphics contents remotely and deliver them to thin clients with low graphics capability, including standalone extended reality devices and mobile phones, while maintaining interactive frame rates for users under adverse network conditions. DHR can already achieve the effect of ray-traced hard shadows that form with the occlusion of direct illumination. We enhance the realism of these shadows by softening their edges with the direction of rays traced and approximating the occlusion of indirect illumination by reconstructing ray-traced ambient occlusion with a modified version of spatiotemporal variance-guided filtering. Our technique uses only 20-30% of the bandwidth of remote rendering and is also tolerant of delays of up to 200 ms with only slight distortion to the shadows along object edges.","sentences":["Distributed hybrid rendering (DHR) is a real-time rendering approach that incorporates cloud-based ray tracing with locally rasterized graphics for interactive thin client metaverse and game applications.","With cloud assistance, DHR can generate high-fidelity ray-traced graphics contents remotely and deliver them to thin clients with low graphics capability, including standalone extended reality devices and mobile phones, while maintaining interactive frame rates for users under adverse network conditions.","DHR can already achieve the effect of ray-traced hard shadows that form with the occlusion of direct illumination.","We enhance the realism of these shadows by softening their edges with the direction of rays traced and approximating the occlusion of indirect illumination by reconstructing ray-traced ambient occlusion with a modified version of spatiotemporal variance-guided filtering.","Our technique uses only 20-30% of the bandwidth of remote rendering and is also tolerant of delays of up to 200 ms with only slight distortion to the shadows along object edges."],"url":"http://arxiv.org/abs/2406.06963v1","category":"cs.GR"}
{"created":"2024-06-11 05:40:45","title":"Low Rank Multi-Dictionary Selection at Scale","abstract":"The sparse dictionary coding framework represents signals as a linear combination of a few predefined dictionary atoms. It has been employed for images, time series, graph signals and recently for 2-way (or 2D) spatio-temporal data employing jointly temporal and spatial dictionaries. Large and over-complete dictionaries enable high-quality models, but also pose scalability challenges which are exacerbated in multi-dictionary settings. Hence, an important problem that we address in this paper is: How to scale multi-dictionary coding for large dictionaries and datasets?   We propose a multi-dictionary atom selection technique for low-rank sparse coding named LRMDS. To enable scalability to large dictionaries and datasets, it progressively selects groups of row-column atom pairs based on their alignment with the data and performs convex relaxation coding via the corresponding sub-dictionaries. We demonstrate both theoretically and experimentally that when the data has a low-rank encoding with a sparse subset of the atoms, LRMDS is able to select them with strong guarantees under mild assumptions. Furthermore, we demonstrate the scalability and quality of LRMDS in both synthetic and real-world datasets and for a range of coding dictionaries. It achieves 3X to 10X speed-up compared to baselines, while obtaining up to two orders of magnitude improvement in representation quality on some of the real world datasets given a fixed target number of atoms.","sentences":["The sparse dictionary coding framework represents signals as a linear combination of a few predefined dictionary atoms.","It has been employed for images, time series, graph signals and recently for 2-way (or 2D) spatio-temporal data employing jointly temporal and spatial dictionaries.","Large and over-complete dictionaries enable high-quality models, but also pose scalability challenges which are exacerbated in multi-dictionary settings.","Hence, an important problem that we address in this paper is: How to scale multi-dictionary coding for large dictionaries and datasets?   ","We propose a multi-dictionary atom selection technique for low-rank sparse coding named LRMDS.","To enable scalability to large dictionaries and datasets, it progressively selects groups of row-column atom pairs based on their alignment with the data and performs convex relaxation coding via the corresponding sub-dictionaries.","We demonstrate both theoretically and experimentally that when the data has a low-rank encoding with a sparse subset of the atoms, LRMDS is able to select them with strong guarantees under mild assumptions.","Furthermore, we demonstrate the scalability and quality of LRMDS in both synthetic and real-world datasets and for a range of coding dictionaries.","It achieves 3X to 10X speed-up compared to baselines, while obtaining up to two orders of magnitude improvement in representation quality on some of the real world datasets given a fixed target number of atoms."],"url":"http://arxiv.org/abs/2406.06960v1","category":"cs.LG"}
{"created":"2024-06-11 05:21:37","title":"A Probabilistic Framework for LLM Hallucination Detection via Belief Tree Propagation","abstract":"This paper focuses on the task of hallucination detection, which aims to determine the truthfulness of LLM-generated statements. To address this problem, a popular class of methods utilize the LLM's self-consistencies in its beliefs in a set of logically related augmented statements generated by the LLM, which does not require external knowledge databases and can work with both white-box and black-box LLMs. However, in many existing approaches, the augmented statements tend to be very monotone and unstructured, which makes it difficult to integrate meaningful information from the LLM beliefs in these statements. Also, many methods work with the binarized version of the LLM's belief, instead of the continuous version, which significantly loses information. To overcome these limitations, in this paper, we propose Belief Tree Propagation (BTProp), a probabilistic framework for LLM hallucination detection. BTProp introduces a belief tree of logically related statements by recursively decomposing a parent statement into child statements with three decomposition strategies, and builds a hidden Markov tree model to integrate the LLM's belief scores in these statements in a principled way. Experiment results show that our method improves baselines by 3%-9% (evaluated by AUROC and AUC-PR) on multiple hallucination detection benchmarks. Code is available at https://github.com/UCSB-NLP-Chang/BTProp.","sentences":["This paper focuses on the task of hallucination detection, which aims to determine the truthfulness of LLM-generated statements.","To address this problem, a popular class of methods utilize the LLM's self-consistencies in its beliefs in a set of logically related augmented statements generated by the LLM, which does not require external knowledge databases and can work with both white-box and black-box LLMs.","However, in many existing approaches, the augmented statements tend to be very monotone and unstructured, which makes it difficult to integrate meaningful information from the LLM beliefs in these statements.","Also, many methods work with the binarized version of the LLM's belief, instead of the continuous version, which significantly loses information.","To overcome these limitations, in this paper, we propose Belief Tree Propagation (BTProp), a probabilistic framework for LLM hallucination detection.","BTProp introduces a belief tree of logically related statements by recursively decomposing a parent statement into child statements with three decomposition strategies, and builds a hidden Markov tree model to integrate the LLM's belief scores in these statements in a principled way.","Experiment results show that our method improves baselines by 3%-9% (evaluated by AUROC and AUC-PR) on multiple hallucination detection benchmarks.","Code is available at https://github.com/UCSB-NLP-Chang/BTProp."],"url":"http://arxiv.org/abs/2406.06950v1","category":"cs.CL"}
{"created":"2024-06-11 05:21:21","title":"Neural Visibility Field for Uncertainty-Driven Active Mapping","abstract":"This paper presents Neural Visibility Field (NVF), a novel uncertainty quantification method for Neural Radiance Fields (NeRF) applied to active mapping. Our key insight is that regions not visible in the training views lead to inherently unreliable color predictions by NeRF at this region, resulting in increased uncertainty in the synthesized views. To address this, we propose to use Bayesian Networks to composite position-based field uncertainty into ray-based uncertainty in camera observations. Consequently, NVF naturally assigns higher uncertainty to unobserved regions, aiding robots to select the most informative next viewpoints. Extensive evaluations show that NVF excels not only in uncertainty quantification but also in scene reconstruction for active mapping, outperforming existing methods.","sentences":["This paper presents Neural Visibility Field (NVF), a novel uncertainty quantification method for Neural Radiance Fields (NeRF) applied to active mapping.","Our key insight is that regions not visible in the training views lead to inherently unreliable color predictions by NeRF at this region, resulting in increased uncertainty in the synthesized views.","To address this, we propose to use Bayesian Networks to composite position-based field uncertainty into ray-based uncertainty in camera observations.","Consequently, NVF naturally assigns higher uncertainty to unobserved regions, aiding robots to select the most informative next viewpoints.","Extensive evaluations show that NVF excels not only in uncertainty quantification but also in scene reconstruction for active mapping, outperforming existing methods."],"url":"http://arxiv.org/abs/2406.06948v1","category":"cs.CV"}
{"created":"2024-06-11 05:12:00","title":"Sparse Bayesian Networks: Efficient Uncertainty Quantification in Medical Image Analysis","abstract":"Efficiently quantifying predictive uncertainty in medical images remains a challenge. While Bayesian neural networks (BNN) offer predictive uncertainty, they require substantial computational resources to train. Although Bayesian approximations such as ensembles have shown promise, they still suffer from high training and inference costs. Existing approaches mainly address the costs of BNN inference post-training, with little focus on improving training efficiency and reducing parameter complexity. This study introduces a training procedure for a sparse (partial) Bayesian network. Our method selectively assigns a subset of parameters as Bayesian by assessing their deterministic saliency through gradient sensitivity analysis. The resulting network combines deterministic and Bayesian parameters, exploiting the advantages of both representations to achieve high task-specific performance and minimize predictive uncertainty. Demonstrated on multi-label ChestMNIST for classification and ISIC, LIDC-IDRI for segmentation, our approach achieves competitive performance and predictive uncertainty estimation by reducing Bayesian parameters by over 95\\%, significantly reducing computational expenses compared to fully Bayesian and ensemble methods.","sentences":["Efficiently quantifying predictive uncertainty in medical images remains a challenge.","While Bayesian neural networks (BNN) offer predictive uncertainty, they require substantial computational resources to train.","Although Bayesian approximations such as ensembles have shown promise, they still suffer from high training and inference costs.","Existing approaches mainly address the costs of BNN inference post-training, with little focus on improving training efficiency and reducing parameter complexity.","This study introduces a training procedure for a sparse (partial) Bayesian network.","Our method selectively assigns a subset of parameters as Bayesian by assessing their deterministic saliency through gradient sensitivity analysis.","The resulting network combines deterministic and Bayesian parameters, exploiting the advantages of both representations to achieve high task-specific performance and minimize predictive uncertainty.","Demonstrated on multi-label ChestMNIST for classification and ISIC, LIDC-IDRI for segmentation, our approach achieves competitive performance and predictive uncertainty estimation by reducing Bayesian parameters by over 95\\%, significantly reducing computational expenses compared to fully Bayesian and ensemble methods."],"url":"http://arxiv.org/abs/2406.06946v1","category":"cs.CV"}
{"created":"2024-06-11 04:18:53","title":"Decentralized Social Networks and the Future of Free Speech Online","abstract":"Decentralized social networks like Mastodon and BlueSky are trending topics that have drawn much attention and discussion in recent years. By devolving powers from the central node to the end users, decentralized social networks aim to cure existing pathologies on the centralized platforms and have been viewed by many as the future of the Internet. This article critically and systematically assesses the decentralization project's prospect for communications online. It uses normative theories of free speech to examine whether and how the decentralization design could facilitate users' freedom of expression online. The analysis shows that both promises and pitfalls exist, highlighting the importance of value-based design in this area. Two most salient issues for the design of the decentralized networks are: how to balance the decentralization ideal with constant needs of centralization on the network, and how to empower users to make them truly capable of exercising their control. The article then uses some design examples, such as the shared blocklist and the opt-in search function, to illustrate the value considerations underlying the design choices. Some tentative proposals for law and policy interventions are offered to better facilitate the design of the new network. Rather than providing clear answers, the article seeks to map the value implications of the design choices, highlight the stakes, and point directions for future research.","sentences":["Decentralized social networks like Mastodon and BlueSky are trending topics that have drawn much attention and discussion in recent years.","By devolving powers from the central node to the end users, decentralized social networks aim to cure existing pathologies on the centralized platforms and have been viewed by many as the future of the Internet.","This article critically and systematically assesses the decentralization project's prospect for communications online.","It uses normative theories of free speech to examine whether and how the decentralization design could facilitate users' freedom of expression online.","The analysis shows that both promises and pitfalls exist, highlighting the importance of value-based design in this area.","Two most salient issues for the design of the decentralized networks are: how to balance the decentralization ideal with constant needs of centralization on the network, and how to empower users to make them truly capable of exercising their control.","The article then uses some design examples, such as the shared blocklist and the opt-in search function, to illustrate the value considerations underlying the design choices.","Some tentative proposals for law and policy interventions are offered to better facilitate the design of the new network.","Rather than providing clear answers, the article seeks to map the value implications of the design choices, highlight the stakes, and point directions for future research."],"url":"http://arxiv.org/abs/2406.06934v1","category":"cs.CY"}
{"created":"2024-06-11 04:08:37","title":"Explaining Representation Learning with Perceptual Components","abstract":"Self-supervised models create representation spaces that lack clear semantic meaning. This interpretability problem of representations makes traditional explainability methods ineffective in this context. In this paper, we introduce a novel method to analyze representation spaces using three key perceptual components: color, shape, and texture. We employ selective masking of these components to observe changes in representations, resulting in distinct importance maps for each. In scenarios, where labels are absent, these importance maps provide more intuitive explanations as they are integral to the human visual system. Our approach enhances the interpretability of the representation space, offering explanations that resonate with human visual perception. We analyze how different training objectives create distinct representation spaces using perceptual components. Additionally, we examine the representation of images across diverse image domains, providing insights into the role of these components in different contexts.","sentences":["Self-supervised models create representation spaces that lack clear semantic meaning.","This interpretability problem of representations makes traditional explainability methods ineffective in this context.","In this paper, we introduce a novel method to analyze representation spaces using three key perceptual components: color, shape, and texture.","We employ selective masking of these components to observe changes in representations, resulting in distinct importance maps for each.","In scenarios, where labels are absent, these importance maps provide more intuitive explanations as they are integral to the human visual system.","Our approach enhances the interpretability of the representation space, offering explanations that resonate with human visual perception.","We analyze how different training objectives create distinct representation spaces using perceptual components.","Additionally, we examine the representation of images across diverse image domains, providing insights into the role of these components in different contexts."],"url":"http://arxiv.org/abs/2406.06930v1","category":"cs.CV"}
{"created":"2024-06-11 04:04:50","title":"Social Learning with Bounded Rationality: Negative Reviews Persist under Newest First","abstract":"We study a model of social learning from reviews where customers are computationally limited and make purchases based on reading only the first few reviews displayed by the platform. Under this bounded rationality, we establish that the review ordering policy can have a significant impact. In particular, the popular Newest First ordering induces a negative review to persist as the most recent review longer than a positive review. This phenomenon, which we term the Cost of Newest First, can make the long-term revenue unboundedly lower than a counterpart where reviews are exogenously drawn for each customer.   We show that the impact of the Cost of Newest First can be mitigated under dynamic pricing, which allows the price to depend on the set of displayed reviews. Under the optimal dynamic pricing policy, the revenue loss is at most a factor of 2. On the way, we identify a structural property for this optimal dynamic pricing: the prices should ensure that the probability of a purchase is always the same, regardless of the state of reviews. We also study an extension of the model where customers put more weight on more recent reviews (and discount older reviews based on their time of posting), and we show that Newest First is still not the optimal ordering policy if customers discount slowly.   Lastly, we corroborate our theoretical findings using a real-world review dataset. We find that the average rating of the first page of reviews is statistically significantly smaller than the overall average rating, which is in line with our theoretical results.","sentences":["We study a model of social learning from reviews where customers are computationally limited and make purchases based on reading only the first few reviews displayed by the platform.","Under this bounded rationality, we establish that the review ordering policy can have a significant impact.","In particular, the popular Newest First ordering induces a negative review to persist as the most recent review longer than a positive review.","This phenomenon, which we term the Cost of Newest First, can make the long-term revenue unboundedly lower than a counterpart where reviews are exogenously drawn for each customer.   ","We show that the impact of the Cost of Newest First can be mitigated under dynamic pricing, which allows the price to depend on the set of displayed reviews.","Under the optimal dynamic pricing policy, the revenue loss is at most a factor of 2.","On the way, we identify a structural property for this optimal dynamic pricing: the prices should ensure that the probability of a purchase is always the same, regardless of the state of reviews.","We also study an extension of the model where customers put more weight on more recent reviews (and discount older reviews based on their time of posting), and we show that Newest First is still not the optimal ordering policy if customers discount slowly.   ","Lastly, we corroborate our theoretical findings using a real-world review dataset.","We find that the average rating of the first page of reviews is statistically significantly smaller than the overall average rating, which is in line with our theoretical results."],"url":"http://arxiv.org/abs/2406.06929v1","category":"cs.GT"}
{"created":"2024-06-11 03:34:39","title":"Where to place a mosquito trap for West Nile Virus surveillance?","abstract":"The rapid spread of West Nile Virus (WNV) is a growing concern. With no vaccines or specific medications available, prevention through mosquito control is the only solution to curb the spread. Mosquito traps, used to detect viral presence in mosquito populations, are essential tools for WNV surveillance. But how do we decide where to place a mosquito trap? And what makes a good trap location, anyway?   We present a robust statistical approach to determine a mosquito trap's ability to predict human WNV cases in the Chicago metropolitan area and its suburbs. We then use this value to detect the landscape, demographic, and socioeconomic factors associated with a mosquito trap's predictive ability. This approach enables resource-limited mosquito control programs to identify better trap locations while reducing trap numbers to increase trap-based surveillance efficiency. The approach can also be applied to a wide range of different environmental surveillance programs.","sentences":["The rapid spread of West Nile Virus (WNV) is a growing concern.","With no vaccines or specific medications available, prevention through mosquito control is the only solution to curb the spread.","Mosquito traps, used to detect viral presence in mosquito populations, are essential tools for WNV surveillance.","But how do we decide where to place a mosquito trap?","And what makes a good trap location, anyway?   ","We present a robust statistical approach to determine a mosquito trap's ability to predict human WNV cases in the Chicago metropolitan area and its suburbs.","We then use this value to detect the landscape, demographic, and socioeconomic factors associated with a mosquito trap's predictive ability.","This approach enables resource-limited mosquito control programs to identify better trap locations while reducing trap numbers to increase trap-based surveillance efficiency.","The approach can also be applied to a wide range of different environmental surveillance programs."],"url":"http://arxiv.org/abs/2406.06920v1","category":"stat.AP"}
{"created":"2024-06-11 03:19:06","title":"Monadic ortholattices: completions and duality","abstract":"We show that the variety of monadic ortholattices is closed under MacNeille and canonical completions. In each case, the completion of $L$ is obtained by forming an associated dual space $X$ that is a monadic orthoframe. This is a set with an orthogonality relation and an additional binary relation satisfying certain conditions. For the MacNeille completion, $X$ is formed from the non-zero elements of $L$, and for the canonical completion, $X$ is formed from the proper filters of $L$. The corresponding completion of $L$ is then obtained as the ortholattice of bi-orthogonally closed subsets of $X$ with an additional operation defined through the binary relation of $X$.   With the introduction of a suitable topology on an orthoframe, as was done by Goldblatt and Bimb\\'o, we obtain a dual adjunction between the categories of monadic ortholattices and monadic orthospaces. A restriction of this dual adjunction provides a dual equivalence.","sentences":["We show that the variety of monadic ortholattices is closed under MacNeille and canonical completions.","In each case, the completion of $L$ is obtained by forming an associated dual space $X$ that is a monadic orthoframe.","This is a set with an orthogonality relation and an additional binary relation satisfying certain conditions.","For the MacNeille completion, $X$ is formed from the non-zero elements of $L$, and for the canonical completion, $X$ is formed from the proper filters of $L$. The corresponding completion of $L$ is then obtained as the ortholattice of bi-orthogonally closed subsets of $X$ with an additional operation defined through the binary relation of $X$.   With the introduction of a suitable topology on an orthoframe, as was done by Goldblatt and Bimb\\'o, we obtain a dual adjunction between the categories of monadic ortholattices and monadic orthospaces.","A restriction of this dual adjunction provides a dual equivalence."],"url":"http://arxiv.org/abs/2406.06917v1","category":"math.LO"}
{"created":"2024-06-11 03:18:55","title":"On regularity of a Kinetic Boundary layer","abstract":"We study the nonlinear steady Boltzmann equation in the half space, with phase transition and Dirichlet boundary condition. In particular, we study the regularity of the solution to the half-space problem in the situation that the gas is in contact with its condensed phase. We propose a novel kinetic weight and establish a weighted $C^1$ estimate under the spatial domain $x\\in [0,\\infty)$, which is unbounded and not strictly convex. Additionally, we prove the $W^{1,p}$ estimate without any weight for $p<2$.","sentences":["We study the nonlinear steady Boltzmann equation in the half space, with phase transition and Dirichlet boundary condition.","In particular, we study the regularity of the solution to the half-space problem in the situation that the gas is in contact with its condensed phase.","We propose a novel kinetic weight and establish a weighted $C^1$ estimate under the spatial domain $x\\in [0,\\infty)$, which is unbounded and not strictly convex.","Additionally, we prove the $W^{1,p}$ estimate without any weight for $p<2$."],"url":"http://arxiv.org/abs/2406.06916v1","category":"math.AP"}
{"created":"2024-06-11 03:15:39","title":"On the Communication Complexity of Secure Multi-Party Computation With Aborts","abstract":"A central goal of cryptography is Secure Multi-party Computation (MPC), where $n$ parties desire to compute a function of their joint inputs without letting any party learn about the inputs of its peers. Unfortunately, it is well-known that MPC guaranteeing output delivery to every party is infeasible when a majority of the parties are malicious. In fact, parties operating over a point-to-point network (i.e. without access to a broadcast channel) cannot even reach an agreement on the output when more than one third of the parties are malicious (Lamport, Shostak, and Pease, JACM 1980).   Motivated by this infeasibility in the point-to-point model, Goldwasser and Lindell (J. Cryptol 2005) introduced a definition of MPC that does not require agreement, referred to as MPC with selective abort. Under this definition, any party may abort the protocol if they detect malicious behavior. They showed that MPC with selective abort is feasible for any number of malicious parties by implementing a broadcast functionality with abort.   While the model of MPC with abort has attracted much attention over the years, little is known about its communication complexity over point-to-point networks. In this work, we study the communication complexity of MPC with abort and devise nearly-optimal communication efficient protocols in this model. Namely, we prove trade-offs between the number of honest parties $h$, the communication complexity, and the locality of the protocols. Here, locality is a bound on the number of peers with which each party must communicate.","sentences":["A central goal of cryptography is Secure Multi-party Computation (MPC), where $n$ parties desire to compute a function of their joint inputs without letting any party learn about the inputs of its peers.","Unfortunately, it is well-known that MPC guaranteeing output delivery to every party is infeasible when a majority of the parties are malicious.","In fact, parties operating over a point-to-point network (i.e. without access to a broadcast channel) cannot even reach an agreement on the output when more than one third of the parties are malicious (Lamport, Shostak, and Pease, JACM 1980).   ","Motivated by this infeasibility in the point-to-point model, Goldwasser and Lindell (J. Cryptol 2005) introduced a definition of MPC that does not require agreement, referred to as MPC with selective abort.","Under this definition, any party may abort the protocol if they detect malicious behavior.","They showed that MPC with selective abort is feasible for any number of malicious parties by implementing a broadcast functionality with abort.   ","While the model of MPC with abort has attracted much attention over the years, little is known about its communication complexity over point-to-point networks.","In this work, we study the communication complexity of MPC with abort and devise nearly-optimal communication efficient protocols in this model.","Namely, we prove trade-offs between the number of honest parties $h$, the communication complexity, and the locality of the protocols.","Here, locality is a bound on the number of peers with which each party must communicate."],"url":"http://arxiv.org/abs/2406.06914v1","category":"cs.CR"}
{"created":"2024-06-11 03:09:20","title":"Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large Language Models","abstract":"Simultaneous Machine Translation (SiMT) generates target translations while reading the source sentence. It relies on a policy to determine the optimal timing for reading sentences and generating translations. Existing SiMT methods generally adopt the traditional Transformer architecture, which concurrently determines the policy and generates translations. While they excel at determining policies, their translation performance is suboptimal. Conversely, Large Language Models (LLMs), trained on extensive corpora, possess superior generation capabilities, but it is difficult for them to acquire translation policy through the training methods of SiMT. Therefore, we introduce Agent-SiMT, a framework combining the strengths of LLMs and traditional SiMT methods. Agent-SiMT contains the policy-decision agent and the translation agent. The policy-decision agent is managed by a SiMT model, which determines the translation policy using partial source sentence and translation. The translation agent, leveraging an LLM, generates translation based on the partial source sentence. The two agents collaborate to accomplish SiMT. Experiments demonstrate that Agent-SiMT attains state-of-the-art performance.","sentences":["Simultaneous Machine Translation (SiMT) generates target translations while reading the source sentence.","It relies on a policy to determine the optimal timing for reading sentences and generating translations.","Existing SiMT methods generally adopt the traditional Transformer architecture, which concurrently determines the policy and generates translations.","While they excel at determining policies, their translation performance is suboptimal.","Conversely, Large Language Models (LLMs), trained on extensive corpora, possess superior generation capabilities, but it is difficult for them to acquire translation policy through the training methods of SiMT.","Therefore, we introduce Agent-SiMT, a framework combining the strengths of LLMs and traditional SiMT methods.","Agent-SiMT contains the policy-decision agent and the translation agent.","The policy-decision agent is managed by a SiMT model, which determines the translation policy using partial source sentence and translation.","The translation agent, leveraging an LLM, generates translation based on the partial source sentence.","The two agents collaborate to accomplish SiMT.","Experiments demonstrate that Agent-SiMT attains state-of-the-art performance."],"url":"http://arxiv.org/abs/2406.06910v1","category":"cs.CL"}
{"created":"2024-06-11 03:07:41","title":"Training Dynamics of Nonlinear Contrastive Learning Model in the High Dimensional Limit","abstract":"This letter presents a high-dimensional analysis of the training dynamics for a single-layer nonlinear contrastive learning model. The empirical distribution of the model weights converges to a deterministic measure governed by a McKean-Vlasov nonlinear partial differential equation (PDE). Under L2 regularization, this PDE reduces to a closed set of low-dimensional ordinary differential equations (ODEs), reflecting the evolution of the model performance during the training process. We analyze the fixed point locations and their stability of the ODEs unveiling several interesting findings. First, only the hidden variable's second moment affects feature learnability at the state with uninformative initialization. Second, higher moments influence the probability of feature selection by controlling the attraction region, rather than affecting local stability. Finally, independent noises added in the data argumentation degrade performance but negatively correlated noise can reduces the variance of gradient estimation yielding better performance. Despite of the simplicity of the analyzed model, it exhibits a rich phenomena of training dynamics, paving a way to understand more complex mechanism behind practical large models.","sentences":["This letter presents a high-dimensional analysis of the training dynamics for a single-layer nonlinear contrastive learning model.","The empirical distribution of the model weights converges to a deterministic measure governed by a McKean-Vlasov nonlinear partial differential equation (PDE).","Under L2 regularization, this PDE reduces to a closed set of low-dimensional ordinary differential equations (ODEs), reflecting the evolution of the model performance during the training process.","We analyze the fixed point locations and their stability of the ODEs unveiling several interesting findings.","First, only the hidden variable's second moment affects feature learnability at the state with uninformative initialization.","Second, higher moments influence the probability of feature selection by controlling the attraction region, rather than affecting local stability.","Finally, independent noises added in the data argumentation degrade performance but negatively correlated noise can reduces the variance of gradient estimation yielding better performance.","Despite of the simplicity of the analyzed model, it exhibits a rich phenomena of training dynamics, paving a way to understand more complex mechanism behind practical large models."],"url":"http://arxiv.org/abs/2406.06909v1","category":"cs.LG"}
{"created":"2024-06-11 03:00:03","title":"Some quenched and annealed limit theorems of superprocesses in random environments","abstract":"Let $X=(X_t, t\\geq 0)$ be a superprocess in a random environment described by a Gaussian noise $W=\\{W(t,x), t\\geq 0, x\\in \\mathbb{R}^d\\}$ white in time and colored in space with correlation kernel $g(x,y)$. When $d\\geq 3$, under the condition that the correlation function $g(x,y)$ is bounded above by some appropriate function $\\bar{g}(x-y)$, we present the quenched and annealed Strong Law of Large Numbers and the Central Limit Theorems regarding the weighted occupation measure $\\int_0^t X_s ds$ as $t\\to \\infty$.","sentences":["Let $X=(X_t, t\\geq 0)$ be a superprocess in a random environment described by a Gaussian noise $W=\\{W(t,x), t\\geq 0, x\\in \\mathbb{R}^d\\}$ white in time and colored in space with correlation kernel $g(x,y)$. When $d\\geq 3$, under the condition that the correlation function $g(x,y)$ is bounded above by some appropriate function $\\bar{g}(x-y)$, we present the quenched and annealed Strong Law of Large Numbers and the Central Limit Theorems regarding the weighted occupation measure $\\int_0^t X_s ds$ as $t\\to \\infty$."],"url":"http://arxiv.org/abs/2406.06905v1","category":"math.PR"}
{"created":"2024-06-11 02:33:47","title":"SmartPQ: An Adaptive Concurrent Priority Queue for NUMA Architectures","abstract":"Concurrent priority queues are widely used in important workloads, such as graph applications and discrete event simulations. However, designing scalable concurrent priority queues for NUMA architectures is challenging. Even though several NUMA-oblivious implementations can scale up to a high number of threads, exploiting the potential parallelism of insert operation, NUMA-oblivious implementations scale poorly in deleteMin-dominated workloads. This is because all threads compete for accessing the same memory locations, i.e., the highest-priority element of the queue, thus incurring excessive cache coherence traffic and non-uniform memory accesses between nodes of a NUMA system. In such scenarios, NUMA-aware implementations are typically used to improve system performance on a NUMA system.   In this work, we propose an adaptive priority queue, called SmartPQ. SmartPQ tunes itself by switching between a NUMA-oblivious and a NUMA-aware algorithmic mode to achieve high performance under all various contention scenarios. SmartPQ has two key components. First, it is built on top of NUMA Node Delegation (Nuddle), a generic low-overhead technique to construct efficient NUMA-aware data structures using any arbitrary concurrent NUMA-oblivious implementation as its backbone. Second, SmartPQ integrates a lightweight decision making mechanism to decide when to switch between NUMA-oblivious and NUMA-aware algorithmic modes. Our evaluation shows that, in NUMA systems, SmartPQ performs best in all various contention scenarios with 87.9% success rate, and dynamically adapts between NUMA-aware and NUMA-oblivious algorithmic mode, with negligible performance overheads. SmartPQ improves performance by 1.87x on average over SprayList, the state-of-theart NUMA-oblivious priority queue.","sentences":["Concurrent priority queues are widely used in important workloads, such as graph applications and discrete event simulations.","However, designing scalable concurrent priority queues for NUMA architectures is challenging.","Even though several NUMA-oblivious implementations can scale up to a high number of threads, exploiting the potential parallelism of insert operation, NUMA-oblivious implementations scale poorly in deleteMin-dominated workloads.","This is because all threads compete for accessing the same memory locations, i.e., the highest-priority element of the queue, thus incurring excessive cache coherence traffic and non-uniform memory accesses between nodes of a NUMA system.","In such scenarios, NUMA-aware implementations are typically used to improve system performance on a NUMA system.   ","In this work, we propose an adaptive priority queue, called SmartPQ. SmartPQ tunes itself by switching between a NUMA-oblivious and a NUMA-aware algorithmic mode to achieve high performance under all various contention scenarios.","SmartPQ has two key components.","First, it is built on top of NUMA Node Delegation (Nuddle), a generic low-overhead technique to construct efficient NUMA-aware data structures using any arbitrary concurrent NUMA-oblivious implementation as its backbone.","Second, SmartPQ integrates a lightweight decision making mechanism to decide when to switch between NUMA-oblivious and NUMA-aware algorithmic modes.","Our evaluation shows that, in NUMA systems, SmartPQ performs best in all various contention scenarios with 87.9% success rate, and dynamically adapts between NUMA-aware and NUMA-oblivious algorithmic mode, with negligible performance overheads.","SmartPQ improves performance by 1.87x on average over SprayList, the state-of-theart NUMA-oblivious priority queue."],"url":"http://arxiv.org/abs/2406.06900v1","category":"cs.DC"}
{"created":"2024-06-11 02:31:34","title":"Developing, Analyzing, and Evaluating Vehicular Lane Keeping Algorithms Under Dynamic Lighting and Weather Conditions Using Electric Vehicles","abstract":"Self-driving vehicles have the potential to reduce accidents and fatalities on the road. Many production vehicles already come equipped with basic self-driving capabilities, but have trouble following lanes in adverse lighting and weather conditions. Therefore, we develop, analyze, and evaluate two vehicular lane-keeping algorithms under dynamic weather conditions using a combined deep learning- and hand-crafted approach and an end-to-end deep learning approach. We use image segmentation- and linear-regression based deep learning to drive the vehicle toward the center of the lane, measuring the amount of laps completed, average speed, and average steering error per lap. Our hybrid model completes more laps than our end-to-end deep learning model. In the future, we are interested in combining our algorithms to form one cohesive approach to lane-following.","sentences":["Self-driving vehicles have the potential to reduce accidents and fatalities on the road.","Many production vehicles already come equipped with basic self-driving capabilities, but have trouble following lanes in adverse lighting and weather conditions.","Therefore, we develop, analyze, and evaluate two vehicular lane-keeping algorithms under dynamic weather conditions using a combined deep learning- and hand-crafted approach and an end-to-end deep learning approach.","We use image segmentation- and linear-regression based deep learning to drive the vehicle toward the center of the lane, measuring the amount of laps completed, average speed, and average steering error per lap.","Our hybrid model completes more laps than our end-to-end deep learning model.","In the future, we are interested in combining our algorithms to form one cohesive approach to lane-following."],"url":"http://arxiv.org/abs/2406.06899v1","category":"cs.RO"}
{"created":"2024-06-11 02:29:22","title":"Bifurcations and multistability in empirical mutualistic networks","abstract":"Individual species may experience diverse outcomes, from prosperity to extinction, in an ecological community subject to external and internal variations. Despite the wealth of theoretical results derived from random matrix ensembles, a theoretical framework still remains to be developed to understand species-level dynamical heterogeneity within a given community, hampering real-world ecosystems' theoretical assessment and management. Here, we consider empirical plant-pollinator mutualistic networks, additionally including all-to-all intragroup competition, where species abundance evolves under a Lotka-Volterra-type equation. Setting the strengths of competition and mutualism to be uniform, we investigate how individual species persist or go extinct under varying the interaction strengths. By employing bifurcation theory in tandem with numerical continuation, we elucidate transcritical bifurcations underlying species extinction and demonstrate that the Hopf bifurcation of unfeasible equilibria and degenerate transcritical bifurcations give rise to multistability, i.e., the coexistence of multiple attracting feasible equilibria. These bifurcations allow us to partition the parameter space into different regimes, each with distinct sets of extinct species, offering insights into how interspecific interactions generate one or multiple extinction scenarios within an ecological network.","sentences":["Individual species may experience diverse outcomes, from prosperity to extinction, in an ecological community subject to external and internal variations.","Despite the wealth of theoretical results derived from random matrix ensembles, a theoretical framework still remains to be developed to understand species-level dynamical heterogeneity within a given community, hampering real-world ecosystems' theoretical assessment and management.","Here, we consider empirical plant-pollinator mutualistic networks, additionally including all-to-all intragroup competition, where species abundance evolves under a Lotka-Volterra-type equation.","Setting the strengths of competition and mutualism to be uniform, we investigate how individual species persist or go extinct under varying the interaction strengths.","By employing bifurcation theory in tandem with numerical continuation, we elucidate transcritical bifurcations underlying species extinction and demonstrate that the Hopf bifurcation of unfeasible equilibria and degenerate transcritical bifurcations give rise to multistability, i.e., the coexistence of multiple attracting feasible equilibria.","These bifurcations allow us to partition the parameter space into different regimes, each with distinct sets of extinct species, offering insights into how interspecific interactions generate one or multiple extinction scenarios within an ecological network."],"url":"http://arxiv.org/abs/2406.06897v1","category":"q-bio.PE"}
{"created":"2024-06-11 01:52:04","title":"Enabling Data Dependency-based Query Optimization","abstract":"Data dependency-based query optimization techniques can considerably improve database system performance: we apply three such optimization techniques to five database management systems (DBMSs) and observe throughput improvements between 5 % and 33 %. We address two key challenges to achieve these results: (i) efficiently identifying and extracting relevant dependencies from the data, and (ii) making use of the dependencies through SQL rewrites or as transformation rules in the optimizer.   First, the schema does not provide all relevant dependencies. We present a workload-driven dependency discovery approach to find additional dependencies within milliseconds. Second, the throughput improvement of a state-of-the-art DBMS is 13 % using only SQL rewrites, but 20 % when we integrate dependency-based optimization into the optimizer and execution engine, e. g., by employing dependency propagation and subquery handling. Using all relevant dependencies, the runtime of four standard benchmarks improves by up to 10 % compared to using only primary and foreign keys, and up to 22 % compared to not using dependencies. The dependency discovery overhead amortizes after a single workload execution.","sentences":["Data dependency-based query optimization techniques can considerably improve database system performance: we apply three such optimization techniques to five database management systems (DBMSs) and observe throughput improvements between 5 % and 33 %.","We address two key challenges to achieve these results: (i) efficiently identifying and extracting relevant dependencies from the data, and (ii) making use of the dependencies through SQL rewrites or as transformation rules in the optimizer.   ","First, the schema does not provide all relevant dependencies.","We present a workload-driven dependency discovery approach to find additional dependencies within milliseconds.","Second, the throughput improvement of a state-of-the-art DBMS is 13 % using only SQL rewrites, but 20 % when we integrate dependency-based optimization into the optimizer and execution engine, e. g., by employing dependency propagation and subquery handling.","Using all relevant dependencies, the runtime of four standard benchmarks improves by up to 10 % compared to using only primary and foreign keys, and up to 22 % compared to not using dependencies.","The dependency discovery overhead amortizes after a single workload execution."],"url":"http://arxiv.org/abs/2406.06886v1","category":"cs.DB"}
{"created":"2024-06-11 01:44:57","title":"A Characterization for Tightness of the Sparse Moment-SOS Hierarchy","abstract":"This paper studies the sparse Moment-SOS hierarchy of relaxations for solving sparse polynomial optimization problems. We show that this sparse hierarchy is tight if and only if the objective can be written as a sum of sparse nonnegative polynomials, each of which belongs to the sum of the ideal and quadratic module generated by the corresponding sparse constraints. Based on this characterization, we give several sufficient conditions for the sparse Moment-SOS hierarchy to be tight. In particular, we show that this sparse hierarchy is tight under some assumptions such as convexity, optimality conditions or finiteness of constraining sets.","sentences":["This paper studies the sparse Moment-SOS hierarchy of relaxations for solving sparse polynomial optimization problems.","We show that this sparse hierarchy is tight if and only if the objective can be written as a sum of sparse nonnegative polynomials, each of which belongs to the sum of the ideal and quadratic module generated by the corresponding sparse constraints.","Based on this characterization, we give several sufficient conditions for the sparse Moment-SOS hierarchy to be tight.","In particular, we show that this sparse hierarchy is tight under some assumptions such as convexity, optimality conditions or finiteness of constraining sets."],"url":"http://arxiv.org/abs/2406.06882v1","category":"math.OC"}
{"created":"2024-06-11 01:44:16","title":"Pseudo-Entanglement is Necessary for EFI Pairs","abstract":"Regarding minimal assumptions, most of classical cryptography is known to depend on the existence of One-Way Functions (OWFs). However, recent evidence has shown that this is not the case when considering quantum resources. Besides the well known unconditional security of Quantum Key Distribution, it is now known that computational cryptography may be built on weaker primitives than OWFs, e.g., pseudo-random states [JLS18], one-way state generators [MY23], or EFI pairs of states [BCQ23]. We consider a new quantum resource, pseudo-entanglement, and show that the existence of EFI pairs, one of the current main candidates for the weakest computational assumption for cryptography (necessary for commitments, oblivious transfer, secure multi-party computation, computational zero-knowledge proofs), implies the existence of pseudo-entanglement, as defined by [ABF+24, ABV23] under some reasonable adaptations. We prove this by constructing a new family of pseudo-entangled quantum states given only EFI pairs. Our result has important implications for the field of computational cryptography. It shows that if pseudo-entanglement does not exist, then most of cryptography cannot exist either. Moreover, it establishes pseudo-entanglement as a new minimal assumption for most of computational cryptography, which may pave the way for the unification of other assumptions into a single primitive. Finally, pseudo-entanglement connects physical phenomena and efficient computation, thus, our result strengthens the connection between cryptography and the physical world.","sentences":["Regarding minimal assumptions, most of classical cryptography is known to depend on the existence of One-Way Functions (OWFs).","However, recent evidence has shown that this is not the case when considering quantum resources.","Besides the well known unconditional security of Quantum Key Distribution, it is now known that computational cryptography may be built on weaker primitives than OWFs, e.g., pseudo-random states","[JLS18], one-way state generators [MY23], or EFI pairs of states","[BCQ23].","We consider a new quantum resource, pseudo-entanglement, and show that the existence of EFI pairs, one of the current main candidates for the weakest computational assumption for cryptography (necessary for commitments, oblivious transfer, secure multi-party computation, computational zero-knowledge proofs), implies the existence of pseudo-entanglement, as defined by [ABF+24, ABV23] under some reasonable adaptations.","We prove this by constructing a new family of pseudo-entangled quantum states given only EFI pairs.","Our result has important implications for the field of computational cryptography.","It shows that if pseudo-entanglement does not exist, then most of cryptography cannot exist either.","Moreover, it establishes pseudo-entanglement as a new minimal assumption for most of computational cryptography, which may pave the way for the unification of other assumptions into a single primitive.","Finally, pseudo-entanglement connects physical phenomena and efficient computation, thus, our result strengthens the connection between cryptography and the physical world."],"url":"http://arxiv.org/abs/2406.06881v1","category":"quant-ph"}
{"created":"2024-06-11 00:50:28","title":"Causality for Complex Continuous-time Functional Longitudinal Studies with Dynamic Treatment Regimes","abstract":"Causal inference in longitudinal studies is often hampered by treatment-confounder feedback. Existing methods typically assume discrete time steps or step-like data changes, which we term ``regular and irregular functional studies,'' limiting their applicability to studies with continuous monitoring data, like intensive care units or continuous glucose monitoring. These studies, which we formally term ``functional longitudinal studies,'' require new approaches. Moreover, existing methods tailored for ``functional longitudinal studies'' can only investigate static treatment regimes, which are independent of historical covariates or treatments, leading to either stringent parametric assumptions or strong positivity assumptions. This restriction has limited the range of causal questions these methods can answer and their practicality. We address these limitations by developing a nonparametric framework for functional longitudinal data, accommodating dynamic treatment regimes that depend on historical covariates or treatments, and may or may not depend on the actual treatment administered. To build intuition and explain our approach, we provide a comprehensive review of existing methods for regular and irregular longitudinal studies. We then formally define the potential outcomes and causal effects of interest, develop identification assumptions, and derive g-computation and inverse probability weighting formulas through novel applications of stochastic process and measure theory. Additionally, we compute the efficient influence curve using semiparametric theory. Our framework generalizes existing literature, and achieves double robustness under specific conditions. Finally, to aid interpretation, we provide sufficient and intuitive conditions for our identification assumptions, enhancing the applicability of our methodology to real-world scenarios.","sentences":["Causal inference in longitudinal studies is often hampered by treatment-confounder feedback.","Existing methods typically assume discrete time steps or step-like data changes, which we term ``regular and irregular functional studies,'' limiting their applicability to studies with continuous monitoring data, like intensive care units or continuous glucose monitoring.","These studies, which we formally term ``functional longitudinal studies,'' require new approaches.","Moreover, existing methods tailored for ``functional longitudinal studies'' can only investigate static treatment regimes, which are independent of historical covariates or treatments, leading to either stringent parametric assumptions or strong positivity assumptions.","This restriction has limited the range of causal questions these methods can answer and their practicality.","We address these limitations by developing a nonparametric framework for functional longitudinal data, accommodating dynamic treatment regimes that depend on historical covariates or treatments, and may or may not depend on the actual treatment administered.","To build intuition and explain our approach, we provide a comprehensive review of existing methods for regular and irregular longitudinal studies.","We then formally define the potential outcomes and causal effects of interest, develop identification assumptions, and derive g-computation and inverse probability weighting formulas through novel applications of stochastic process and measure theory.","Additionally, we compute the efficient influence curve using semiparametric theory.","Our framework generalizes existing literature, and achieves double robustness under specific conditions.","Finally, to aid interpretation, we provide sufficient and intuitive conditions for our identification assumptions, enhancing the applicability of our methodology to real-world scenarios."],"url":"http://arxiv.org/abs/2406.06868v1","category":"math.ST"}
{"created":"2024-06-11 00:01:42","title":"Design and Scheduling of an AI-based Queueing System","abstract":"To leverage prediction models to make optimal scheduling decisions in service systems, we must understand how predictive errors impact congestion due to externalities on the delay of other jobs. Motivated by applications where prediction models interact with human servers (e.g., content moderation), we consider a large queueing system comprising of many single server queues where the class of a job is estimated using a prediction model. By characterizing the impact of mispredictions on congestion cost in heavy traffic, we design an index-based policy that incorporates the predicted class information in a near-optimal manner. Our theoretical results guide the design of predictive models by providing a simple model selection procedure with downstream queueing performance as a central concern, and offer novel insights on how to design queueing systems with AI-based triage. We illustrate our framework on a content moderation task based on real online comments, where we construct toxicity classifiers by finetuning large language models.","sentences":["To leverage prediction models to make optimal scheduling decisions in service systems, we must understand how predictive errors impact congestion due to externalities on the delay of other jobs.","Motivated by applications where prediction models interact with human servers (e.g., content moderation), we consider a large queueing system comprising of many single server queues where the class of a job is estimated using a prediction model.","By characterizing the impact of mispredictions on congestion cost in heavy traffic, we design an index-based policy that incorporates the predicted class information in a near-optimal manner.","Our theoretical results guide the design of predictive models by providing a simple model selection procedure with downstream queueing performance as a central concern, and offer novel insights on how to design queueing systems with AI-based triage.","We illustrate our framework on a content moderation task based on real online comments, where we construct toxicity classifiers by finetuning large language models."],"url":"http://arxiv.org/abs/2406.06855v1","category":"math.OC"}
{"created":"2024-06-11 00:01:32","title":"Current-readout technique for ultra-high-rate experiments","abstract":"This study developed a new current-readout technique capable of handling measurements with high count rates exceeding 1 Gcps. By directly capturing the output current of a photomultiplier as a digitized waveform, we estimated event rates, overcoming the limitations imposed by pulse pileup constraints. This innovative method was applied to a muon spin rotation/relaxation/resonance experiment at the Japan Proton Accelerator Research Complex, demonstrating its anticipated performance. Furthermore, we explored methods for estimating statistical uncertainty and investigated potential applications in analog-logic OR/AND gates. Overall, our findings reveal that the developed technique opens up avenues for the development of future non-binary logic circuits operating based on n-adic numbers.","sentences":["This study developed a new current-readout technique capable of handling measurements with high count rates exceeding 1 Gcps.","By directly capturing the output current of a photomultiplier as a digitized waveform, we estimated event rates, overcoming the limitations imposed by pulse pileup constraints.","This innovative method was applied to a muon spin rotation/relaxation/resonance experiment at the Japan Proton Accelerator Research Complex, demonstrating its anticipated performance.","Furthermore, we explored methods for estimating statistical uncertainty and investigated potential applications in analog-logic OR/AND gates.","Overall, our findings reveal that the developed technique opens up avenues for the development of future non-binary logic circuits operating based on n-adic numbers."],"url":"http://arxiv.org/abs/2406.06854v1","category":"physics.ins-det"}
{"created":"2024-06-10 23:23:59","title":"Aerial Relay to Achieve Covertness and Security","abstract":"In this work, a delay-tolerant unmanned aerial vehicle (UAV) relayed covert and secure communication framework is investigated. In this framework, a legitimate UAV serves as an aerial relay to realize communication when the direct link between the terrestrial transmitter and receiver is blocked and also acts as a friendly jammer to suppress the malicious nodes presented on the ground. Subsequently, considering the uncertainty of malicious nodes' positions, a robust fractional programming optimization problem is built to maximize energy efficiency by jointly optimizing the trajectory of the UAV, the transmit power of the transmitter, and the time-switching factor. For the extremely complicated covert constraint, Pinsker's inequality, Jensen's inequality, and the bisection search method are employed to construct a tractable shrunken one. After this, an alternate optimization-based algorithm is proposed to solve the fractional programming optimization problem. To achieve low complexity, we design the primal-dual search-based algorithm and the successive convex approximation-based algorithm, respectively, for each sub-problem. Numerical results show the effectiveness of our proposed algorithm.","sentences":["In this work, a delay-tolerant unmanned aerial vehicle (UAV) relayed covert and secure communication framework is investigated.","In this framework, a legitimate UAV serves as an aerial relay to realize communication when the direct link between the terrestrial transmitter and receiver is blocked and also acts as a friendly jammer to suppress the malicious nodes presented on the ground.","Subsequently, considering the uncertainty of malicious nodes' positions, a robust fractional programming optimization problem is built to maximize energy efficiency by jointly optimizing the trajectory of the UAV, the transmit power of the transmitter, and the time-switching factor.","For the extremely complicated covert constraint, Pinsker's inequality, Jensen's inequality, and the bisection search method are employed to construct a tractable shrunken one.","After this, an alternate optimization-based algorithm is proposed to solve the fractional programming optimization problem.","To achieve low complexity, we design the primal-dual search-based algorithm and the successive convex approximation-based algorithm, respectively, for each sub-problem.","Numerical results show the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2406.06842v1","category":"cs.IT"}
{"created":"2024-06-10 22:56:56","title":"A Dynamic Likelihood Approach to Filtering for advection-diffusion1 Dynamics","abstract":"A Bayesian data assimilation scheme is formulated for advection-dominated advective and diffusive evolutionary problems, based upon the Dynamic Likelihood (DLF) approach to filtering. The DLF was developed specifically for hyperbolic problems -waves-, and in this paper, it is extended via a split step formulation, to handle advection-diffusion problems. In the dynamic likelihood approach, observations and their statistics are used to propagate probabilities along characteristics, evolving the likelihood in time. The estimate posterior thus inherits phase information. For advection-diffusion the advective part of the time evolution is handled on the basis of observations alone, while the diffusive part is informed through the model as well as observations. We expect, and indeed show here, that in advection-dominated problems, the DLF approach produces better estimates than other assimilation approaches, particularly when the observations are sparse and have low uncertainty. The added computational expense of the method is cubic in the total number of observations over time, which is on the same order of magnitude as a standard Kalman filter and can be mitigated by bounding the number of forward propagated observations, discarding the least informative data.","sentences":["A Bayesian data assimilation scheme is formulated for advection-dominated advective and diffusive evolutionary problems, based upon the Dynamic Likelihood (DLF) approach to filtering.","The DLF was developed specifically for hyperbolic problems -waves-, and in this paper, it is extended via a split step formulation, to handle advection-diffusion problems.","In the dynamic likelihood approach, observations and their statistics are used to propagate probabilities along characteristics, evolving the likelihood in time.","The estimate posterior thus inherits phase information.","For advection-diffusion the advective part of the time evolution is handled on the basis of observations alone, while the diffusive part is informed through the model as well as observations.","We expect, and indeed show here, that in advection-dominated problems, the DLF approach produces better estimates than other assimilation approaches, particularly when the observations are sparse and have low uncertainty.","The added computational expense of the method is cubic in the total number of observations over time, which is on the same order of magnitude as a standard Kalman filter and can be mitigated by bounding the number of forward propagated observations, discarding the least informative data."],"url":"http://arxiv.org/abs/2406.06837v1","category":"math.DS"}
{"created":"2024-06-10 22:43:34","title":"Data-driven Power Flow Linearization: Simulation","abstract":"Building on the theoretical insights of Part I, this paper, as the second part of the tutorial, dives deeper into data-driven power flow linearization (DPFL), focusing on comprehensive numerical testing. The necessity of these simulations stems from the theoretical analysis's inherent limitations, particularly the challenge of identifying the differences in real-world performance among DPFL methods with overlapping theoretical capabilities and/or limitations. The absence of a comprehensive numerical comparison of DPFL approaches in the literature also motivates this paper, especially given the fact that over 95% of existing DPFL studies have not provided any open-source codes. To bridge the gap, this paper first reviews existing DPFL experiments, examining the adopted test scenarios, load fluctuation settings, data sources, considerations for data noise/outliers, and the comparison made so far. Subsequently, this paper evaluates a total of 44 methods, containing over 30 existing DPFL approaches, some innovative DPFL techniques, and several classic physics-driven power flow linearization methods for benchmarking. The evaluation spans various dimensions, including generalizability, applicability, accuracy, and computational efficiency, using various different test cases scaling from 9-bus to 1354-bus systems. The numerical analysis identifies and examines significant trends and consistent findings across all methods under various test cases. It also offers theoretical insights into phenomena like under-performance, failure, excessive computation times, etc. Overall, this paper identifies the differences in the performances of the wide range of DPFL methods, reveals gaps not evident from theoretical discussions, assists in method selection for real-world applications, and provides thorough discussions on open questions within DPFL research, indicating ten potential future directions.","sentences":["Building on the theoretical insights of Part I, this paper, as the second part of the tutorial, dives deeper into data-driven power flow linearization (DPFL), focusing on comprehensive numerical testing.","The necessity of these simulations stems from the theoretical analysis's inherent limitations, particularly the challenge of identifying the differences in real-world performance among DPFL methods with overlapping theoretical capabilities and/or limitations.","The absence of a comprehensive numerical comparison of DPFL approaches in the literature also motivates this paper, especially given the fact that over 95% of existing DPFL studies have not provided any open-source codes.","To bridge the gap, this paper first reviews existing DPFL experiments, examining the adopted test scenarios, load fluctuation settings, data sources, considerations for data noise/outliers, and the comparison made so far.","Subsequently, this paper evaluates a total of 44 methods, containing over 30 existing DPFL approaches, some innovative DPFL techniques, and several classic physics-driven power flow linearization methods for benchmarking.","The evaluation spans various dimensions, including generalizability, applicability, accuracy, and computational efficiency, using various different test cases scaling from 9-bus to 1354-bus systems.","The numerical analysis identifies and examines significant trends and consistent findings across all methods under various test cases.","It also offers theoretical insights into phenomena like under-performance, failure, excessive computation times, etc.","Overall, this paper identifies the differences in the performances of the wide range of DPFL methods, reveals gaps not evident from theoretical discussions, assists in method selection for real-world applications, and provides thorough discussions on open questions within DPFL research, indicating ten potential future directions."],"url":"http://arxiv.org/abs/2406.06833v1","category":"eess.SY"}
{"created":"2024-06-10 22:15:55","title":"A local squared Wasserstein-2 method for efficient reconstruction of models with uncertainty","abstract":"In this paper, we propose a local squared Wasserstein-2 (W_2) method to solve the inverse problem of reconstructing models with uncertain latent variables or parameters. A key advantage of our approach is that it does not require prior information on the distribution of the latent variables or parameters in the underlying models. Instead, our method can efficiently reconstruct the distributions of the output associated with different inputs based on empirical distributions of observation data. We demonstrate the effectiveness of our proposed method across several uncertainty quantification (UQ) tasks, including linear regression with coefficient uncertainty, training neural networks with weight uncertainty, and reconstructing ordinary differential equations (ODEs) with a latent random variable.","sentences":["In this paper, we propose a local squared Wasserstein-2 (W_2) method to solve the inverse problem of reconstructing models with uncertain latent variables or parameters.","A key advantage of our approach is that it does not require prior information on the distribution of the latent variables or parameters in the underlying models.","Instead, our method can efficiently reconstruct the distributions of the output associated with different inputs based on empirical distributions of observation data.","We demonstrate the effectiveness of our proposed method across several uncertainty quantification (UQ) tasks, including linear regression with coefficient uncertainty, training neural networks with weight uncertainty, and reconstructing ordinary differential equations (ODEs) with a latent random variable."],"url":"http://arxiv.org/abs/2406.06825v1","category":"stat.ML"}
{"created":"2024-06-10 22:15:22","title":"Modified Legendre-Gauss Collocation Method for Solving Optimal Control Problems with Nonsmooth Solutions","abstract":"A modified form of Legendre-Gauss orthogonal direct collocation is developed for solving optimal control problems whose solutions are nonsmooth due to control discontinuities. This new method adds switch-time variables, control variables, and collocation conditions at both endpoints of a mesh interval, whereas these new variables and collocation conditions are not included in standard Legendre-Gauss orthogonal collocation. The modified Legendre-Gauss collocation method alters the search space of the resulting nonlinear programming problem and enables determining accurately the location of the nonsmoothness in the optimal control. The transformed adjoint system of the modified Legendre-Gauss collocation method is then derived and shown to satisfy a discrete form of the continuous variational necessary conditions for optimality. The method is motivated via a control-constrained triple-integrator minimum-time optimal control problem where the solution possesses a two-switch bang-bang optimal control structure. In addition, the method developed in this paper is compared with existing Gaussian quadrature collocation methods. The method developed in this paper is shown to be capable of accurately solving optimal control problems with a discontinuous optimal control.","sentences":["A modified form of Legendre-Gauss orthogonal direct collocation is developed for solving optimal control problems whose solutions are nonsmooth due to control discontinuities.","This new method adds switch-time variables, control variables, and collocation conditions at both endpoints of a mesh interval, whereas these new variables and collocation conditions are not included in standard Legendre-Gauss orthogonal collocation.","The modified Legendre-Gauss collocation method alters the search space of the resulting nonlinear programming problem and enables determining accurately the location of the nonsmoothness in the optimal control.","The transformed adjoint system of the modified Legendre-Gauss collocation method is then derived and shown to satisfy a discrete form of the continuous variational necessary conditions for optimality.","The method is motivated via a control-constrained triple-integrator minimum-time optimal control problem where the solution possesses a two-switch bang-bang optimal control structure.","In addition, the method developed in this paper is compared with existing Gaussian quadrature collocation methods.","The method developed in this paper is shown to be capable of accurately solving optimal control problems with a discontinuous optimal control."],"url":"http://arxiv.org/abs/2406.06824v1","category":"math.OC"}
{"created":"2024-06-10 22:09:42","title":"Streaming Algorithms with Few State Changes","abstract":"In this paper, we study streaming algorithms that minimize the number of changes made to their internal state (i.e., memory contents). While the design of streaming algorithms typically focuses on minimizing space and update time, these metrics fail to capture the asymmetric costs, inherent in modern hardware and database systems, of reading versus writing to memory. In fact, most streaming algorithms write to their memory on every update, which is undesirable when writing is significantly more expensive than reading. This raises the question of whether streaming algorithms with small space and number of memory writes are possible.   We first demonstrate that, for the fundamental $F_p$ moment estimation problem with $p\\ge 1$, any streaming algorithm that achieves a constant factor approximation must make $\\Omega(n^{1-1/p})$ internal state changes, regardless of how much space it uses. Perhaps surprisingly, we show that this lower bound can be matched by an algorithm that also has near-optimal space complexity. Specifically, we give a $(1+\\varepsilon)$-approximation algorithm for $F_p$ moment estimation that uses a near-optimal $\\widetilde{\\mathcal{O}}_\\varepsilon(n^{1-1/p})$ number of state changes, while simultaneously achieving near-optimal space, i.e., for $p\\in[1,2]$, our algorithm uses $\\text{poly}\\left(\\log n,\\frac{1}{\\varepsilon}\\right)$ bits of space, while for $p>2$, the algorithm uses $\\widetilde{\\mathcal{O}}_\\varepsilon(n^{1-2/p})$ space. We similarly design streaming algorithms that are simultaneously near-optimal in both space complexity and the number of state changes for the heavy-hitters problem, sparse support recovery, and entropy estimation. Our results demonstrate that an optimal number of state changes can be achieved without sacrificing space complexity.","sentences":["In this paper, we study streaming algorithms that minimize the number of changes made to their internal state (i.e., memory contents).","While the design of streaming algorithms typically focuses on minimizing space and update time, these metrics fail to capture the asymmetric costs, inherent in modern hardware and database systems, of reading versus writing to memory.","In fact, most streaming algorithms write to their memory on every update, which is undesirable when writing is significantly more expensive than reading.","This raises the question of whether streaming algorithms with small space and number of memory writes are possible.   ","We first demonstrate that, for the fundamental $F_p$ moment estimation problem with $p\\ge 1$, any streaming algorithm that achieves a constant factor approximation must make $\\Omega(n^{1-1/p})$ internal state changes, regardless of how much space it uses.","Perhaps surprisingly, we show that this lower bound can be matched by an algorithm that also has near-optimal space complexity.","Specifically, we give a $(1+\\varepsilon)$-approximation algorithm for $F_p$ moment estimation that uses a near-optimal $\\widetilde{\\mathcal{O}}_\\varepsilon(n^{1-1/p})$ number of state changes, while simultaneously achieving near-optimal space, i.e., for $p\\in[1,2]$, our algorithm uses $\\text{poly}\\left(\\log n,\\frac{1}{\\varepsilon}\\right)$ bits of space, while for $p>2$, the algorithm uses $\\widetilde{\\mathcal{O}}_\\varepsilon(n^{1-2/p})$ space.","We similarly design streaming algorithms that are simultaneously near-optimal in both space complexity and the number of state changes for the heavy-hitters problem, sparse support recovery, and entropy estimation.","Our results demonstrate that an optimal number of state changes can be achieved without sacrificing space complexity."],"url":"http://arxiv.org/abs/2406.06821v1","category":"cs.DS"}
{"created":"2024-06-10 22:01:34","title":"Conformal Prediction for Class-wise Coverage via Augmented Label Rank Calibration","abstract":"Conformal prediction (CP) is an emerging uncertainty quantification framework that allows us to construct a prediction set to cover the true label with a pre-specified marginal or conditional probability. Although the valid coverage guarantee has been extensively studied for classification problems, CP often produces large prediction sets which may not be practically useful. This issue is exacerbated for the setting of class-conditional coverage on imbalanced classification tasks. This paper proposes the Rank Calibrated Class-conditional CP (RC3P) algorithm to reduce the prediction set sizes to achieve class-conditional coverage, where the valid coverage holds for each class. In contrast to the standard class-conditional CP (CCP) method that uniformly thresholds the class-wise conformity score for each class, the augmented label rank calibration step allows RC3P to selectively iterate this class-wise thresholding subroutine only for a subset of classes whose class-wise top-k error is small. We prove that agnostic to the classifier and data distribution, RC3P achieves class-wise coverage. We also show that RC3P reduces the size of prediction sets compared to the CCP method. Comprehensive experiments on multiple real-world datasets demonstrate that RC3P achieves class-wise coverage and 26.25% reduction in prediction set sizes on average.","sentences":["Conformal prediction (CP) is an emerging uncertainty quantification framework that allows us to construct a prediction set to cover the true label with a pre-specified marginal or conditional probability.","Although the valid coverage guarantee has been extensively studied for classification problems, CP often produces large prediction sets which may not be practically useful.","This issue is exacerbated for the setting of class-conditional coverage on imbalanced classification tasks.","This paper proposes the Rank Calibrated Class-conditional CP (RC3P) algorithm to reduce the prediction set sizes to achieve class-conditional coverage, where the valid coverage holds for each class.","In contrast to the standard class-conditional CP (CCP) method that uniformly thresholds the class-wise conformity score for each class, the augmented label rank calibration step allows RC3P to selectively iterate this class-wise thresholding subroutine only for a subset of classes whose class-wise top-k error is small.","We prove that agnostic to the classifier and data distribution, RC3P achieves class-wise coverage.","We also show that RC3P reduces the size of prediction sets compared to the CCP method.","Comprehensive experiments on multiple real-world datasets demonstrate that RC3P achieves class-wise coverage and 26.25% reduction in prediction set sizes on average."],"url":"http://arxiv.org/abs/2406.06818v1","category":"cs.LG"}
{"created":"2024-06-10 21:56:38","title":"The Legal Duty to Search for Less Discriminatory Algorithms","abstract":"Work in computer science has established that, contrary to conventional wisdom, for a given prediction problem there are almost always multiple possible models with equivalent performance--a phenomenon often termed model multiplicity. Critically, different models of equivalent performance can produce different predictions for the same individual, and, in aggregate, exhibit different levels of impacts across demographic groups. Thus, when an algorithmic system displays a disparate impact, model multiplicity suggests that developers could discover an alternative model that performs equally well, but has less discriminatory impact. Indeed, the promise of model multiplicity is that an equally accurate, but less discriminatory algorithm (LDA) almost always exists. But without dedicated exploration, it is unlikely developers will discover potential LDAs. Model multiplicity and the availability of LDAs have significant ramifications for the legal response to discriminatory algorithms, in particular for disparate impact doctrine, which has long taken into account the availability of alternatives with less disparate effect when assessing liability. A close reading of legal authorities over the decades reveals that the law has on numerous occasions recognized that the existence of a less discriminatory alternative is sometimes relevant to a defendant's burden of justification at the second step of disparate impact analysis. Indeed, under disparate impact doctrine, it makes little sense to say that a given algorithmic system used by an employer, creditor, or housing provider is \"necessary\" if an equally accurate model that exhibits less disparate effect is available and possible to discover with reasonable effort. As a result, we argue that the law should place a duty of a reasonable search for LDAs on entities that develop and deploy predictive models in covered civil rights domains.","sentences":["Work in computer science has established that, contrary to conventional wisdom, for a given prediction problem there are almost always multiple possible models with equivalent performance--a phenomenon often termed model multiplicity.","Critically, different models of equivalent performance can produce different predictions for the same individual, and, in aggregate, exhibit different levels of impacts across demographic groups.","Thus, when an algorithmic system displays a disparate impact, model multiplicity suggests that developers could discover an alternative model that performs equally well, but has less discriminatory impact.","Indeed, the promise of model multiplicity is that an equally accurate, but less discriminatory algorithm (LDA) almost always exists.","But without dedicated exploration, it is unlikely developers will discover potential LDAs.","Model multiplicity and the availability of LDAs have significant ramifications for the legal response to discriminatory algorithms, in particular for disparate impact doctrine, which has long taken into account the availability of alternatives with less disparate effect when assessing liability.","A close reading of legal authorities over the decades reveals that the law has on numerous occasions recognized that the existence of a less discriminatory alternative is sometimes relevant to a defendant's burden of justification at the second step of disparate impact analysis.","Indeed, under disparate impact doctrine, it makes little sense to say that a given algorithmic system used by an employer, creditor, or housing provider is \"necessary\" if an equally accurate model that exhibits less disparate effect is available and possible to discover with reasonable effort.","As a result, we argue that the law should place a duty of a reasonable search for LDAs on entities that develop and deploy predictive models in covered civil rights domains."],"url":"http://arxiv.org/abs/2406.06817v1","category":"cs.CY"}
{"created":"2024-06-10 21:47:24","title":"Bounds on the fractal uncertainty exponent and a spectral gap","abstract":"We prove two results on Fractal Uncertainty Principle (FUP) for discrete Cantor sets with large alphabets. First, we give an example of an alphabet with dimension $\\delta \\in (\\frac12,1)$ where the FUP exponent is exponentially small as the size of the alphabet grows. Secondly, for $\\delta \\in (0,\\frac12]$ we show that a similar alphabet has a large FUP exponent, arbitrarily close to the optimal upper bound of $\\frac12-\\frac\\delta2$, if we dilate the Fourier transform by a factor satisfying a generic Diophantine condition. We give an application of the latter result to spectral gaps for open quantum baker's maps.","sentences":["We prove two results on Fractal Uncertainty Principle (FUP) for discrete Cantor sets with large alphabets.","First, we give an example of an alphabet with dimension $\\delta \\in (\\frac12,1)$ where the FUP exponent is exponentially small as the size of the alphabet grows.","Secondly, for $\\delta \\in (0,\\frac12]$ we show that a similar alphabet has a large FUP exponent, arbitrarily close to the optimal upper bound of $\\frac12-\\frac\\delta2$, if we dilate the Fourier transform by a factor satisfying a generic Diophantine condition.","We give an application of the latter result to spectral gaps for open quantum baker's maps."],"url":"http://arxiv.org/abs/2406.06815v1","category":"math.AP"}
{"created":"2024-06-10 21:44:52","title":"Stable Neighbor Denoising for Source-free Domain Adaptive Segmentation","abstract":"We study source-free unsupervised domain adaptation (SFUDA) for semantic segmentation, which aims to adapt a source-trained model to the target domain without accessing the source data. Many works have been proposed to address this challenging problem, among which uncertainty-based self-training is a predominant approach. However, without comprehensive denoising mechanisms, they still largely fall into biased estimates when dealing with different domains and confirmation bias. In this paper, we observe that pseudo-label noise is mainly contained in unstable samples in which the predictions of most pixels undergo significant variations during self-training. Inspired by this, we propose a novel mechanism to denoise unstable samples with stable ones. Specifically, we introduce the Stable Neighbor Denoising (SND) approach, which effectively discovers highly correlated stable and unstable samples by nearest neighbor retrieval and guides the reliable optimization of unstable samples by bi-level learning. Moreover, we compensate for the stable set by object-level object paste, which can further eliminate the bias caused by less learned classes. Our SND enjoys two advantages. First, SND does not require a specific segmentor structure, endowing its universality. Second, SND simultaneously addresses the issues of class, domain, and confirmation biases during adaptation, ensuring its effectiveness. Extensive experiments show that SND consistently outperforms state-of-the-art methods in various SFUDA semantic segmentation settings. In addition, SND can be easily integrated with other approaches, obtaining further improvements.","sentences":["We study source-free unsupervised domain adaptation (SFUDA) for semantic segmentation, which aims to adapt a source-trained model to the target domain without accessing the source data.","Many works have been proposed to address this challenging problem, among which uncertainty-based self-training is a predominant approach.","However, without comprehensive denoising mechanisms, they still largely fall into biased estimates when dealing with different domains and confirmation bias.","In this paper, we observe that pseudo-label noise is mainly contained in unstable samples in which the predictions of most pixels undergo significant variations during self-training.","Inspired by this, we propose a novel mechanism to denoise unstable samples with stable ones.","Specifically, we introduce the Stable Neighbor Denoising (SND) approach, which effectively discovers highly correlated stable and unstable samples by nearest neighbor retrieval and guides the reliable optimization of unstable samples by bi-level learning.","Moreover, we compensate for the stable set by object-level object paste, which can further eliminate the bias caused by less learned classes.","Our SND enjoys two advantages.","First, SND does not require a specific segmentor structure, endowing its universality.","Second, SND simultaneously addresses the issues of class, domain, and confirmation biases during adaptation, ensuring its effectiveness.","Extensive experiments show that SND consistently outperforms state-of-the-art methods in various SFUDA semantic segmentation settings.","In addition, SND can be easily integrated with other approaches, obtaining further improvements."],"url":"http://arxiv.org/abs/2406.06813v1","category":"cs.CV"}
{"created":"2024-06-10 21:27:13","title":"AGB-DE: A Corpus for the Automated Legal Assessment of Clauses in German Consumer Contracts","abstract":"Legal tasks and datasets are often used as benchmarks for the capabilities of language models. However, openly available annotated datasets are rare. In this paper, we introduce AGB-DE, a corpus of 3,764 clauses from German consumer contracts that have been annotated and legally assessed by legal experts. Together with the data, we present a first baseline for the task of detecting potentially void clauses, comparing the performance of an SVM baseline with three fine-tuned open language models and the performance of GPT-3.5. Our results show the challenging nature of the task, with no approach exceeding an F1-score of 0.54. While the fine-tuned models often performed better with regard to precision, GPT-3.5 outperformed the other approaches with regard to recall. An analysis of the errors indicates that one of the main challenges could be the correct interpretation of complex clauses, rather than the decision boundaries of what is permissible and what is not.","sentences":["Legal tasks and datasets are often used as benchmarks for the capabilities of language models.","However, openly available annotated datasets are rare.","In this paper, we introduce AGB-DE, a corpus of 3,764 clauses from German consumer contracts that have been annotated and legally assessed by legal experts.","Together with the data, we present a first baseline for the task of detecting potentially void clauses, comparing the performance of an SVM baseline with three fine-tuned open language models and the performance of GPT-3.5.","Our results show the challenging nature of the task, with no approach exceeding an F1-score of 0.54.","While the fine-tuned models often performed better with regard to precision, GPT-3.5 outperformed the other approaches with regard to recall.","An analysis of the errors indicates that one of the main challenges could be the correct interpretation of complex clauses, rather than the decision boundaries of what is permissible and what is not."],"url":"http://arxiv.org/abs/2406.06809v1","category":"cs.CL"}
{"created":"2024-06-10 21:23:12","title":"Additive engineering for Sb$_2$S$_3$ indoor photovoltaics with efficiency exceeding 17%","abstract":"Indoor photovoltaics (IPVs) have attracted increasing attention for sustainably powering Internet of Things (IoT) electronics. Sb$_2$S$_3$ is a promising IPV candidate material with a bandgap of ~1.75 eV, which is near the optimal value for indoor energy harvesting. However, the performance of Sb$_2$S$_3$ solar cells is limited by nonradiative recombination, closely associated with the poor-quality absorber films. Additive engineering is an effective strategy to improved the properties of solution-processed films. This work shows that the addition of monoethanolamine (MEA) into the precursor solution allows the nucleation and growth of Sb$_2$S$_3$ films to be controlled, enabling the deposition of high-quality Sb$_2$S$_3$ absorbers with reduced grain boundary density, optimized band positions and increased carrier concentration. Complemented with computations, it is revealed that the incorporation of MEA leads to a more efficient and energetically favorable deposition for enhanced heterogeneous nucleation on the substrate, which increases the grain size and accelerates the deposition rate of Sb$_2$S$_3$ films. Due to suppressed carrier recombination and improved charge-carrier transport in Sb$_2$S$_3$ absorber films, the MEA-modulated Sb$_2$S$_3$ solar cell yields a power conversion efficiency (PCE) of 7.22% under AM1.5G illumination, and an IPV PCE of 17.55% under 1000 lux white light emitting diode (WLED) illumination, which is the highest yet reported for Sb$_2$S$_3$ IPVs. Furthermore, we construct high performance large-area Sb$_2$S$_3$ IPV modules to power IoT wireless sensors, and realize the long-term continuous recording of environmental parameters under WLED illumination in an office. This work highlights the great prospect of Sb$_2$S$_3$ photovoltaics for indoor energy harvesting.","sentences":["Indoor photovoltaics (IPVs) have attracted increasing attention for sustainably powering Internet of Things (IoT) electronics.","Sb$_2$S$_3$ is a promising IPV candidate material with a bandgap of ~1.75 eV, which is near the optimal value for indoor energy harvesting.","However, the performance of Sb$_2$S$_3$ solar cells is limited by nonradiative recombination, closely associated with the poor-quality absorber films.","Additive engineering is an effective strategy to improved the properties of solution-processed films.","This work shows that the addition of monoethanolamine (MEA) into the precursor solution allows the nucleation and growth of Sb$_2$S$_3$ films to be controlled, enabling the deposition of high-quality Sb$_2$S$_3$ absorbers with reduced grain boundary density, optimized band positions and increased carrier concentration.","Complemented with computations, it is revealed that the incorporation of MEA leads to a more efficient and energetically favorable deposition for enhanced heterogeneous nucleation on the substrate, which increases the grain size and accelerates the deposition rate of Sb$_2$S$_3$ films.","Due to suppressed carrier recombination and improved charge-carrier transport in Sb$_2$S$_3$ absorber films, the MEA-modulated Sb$_2$S$_3$ solar cell yields a power conversion efficiency (PCE) of 7.22% under AM1.5G illumination, and an IPV PCE of 17.55% under 1000 lux white light emitting diode (WLED) illumination, which is the highest yet reported for Sb$_2$S$_3$ IPVs.","Furthermore, we construct high performance large-area Sb$_2$S$_3$ IPV modules to power IoT wireless sensors, and realize the long-term continuous recording of environmental parameters under WLED illumination in an office.","This work highlights the great prospect of Sb$_2$S$_3$ photovoltaics for indoor energy harvesting."],"url":"http://arxiv.org/abs/2406.06807v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 21:19:23","title":"Lookback Prophet Inequalities","abstract":"Prophet inequalities are fundamental optimal stopping problems, where a decision-maker observes sequentially items with values sampled independently from known distributions, and must decide at each new observation to either stop and gain the current value or reject it irrevocably and move to the next step. This model is often too pessimistic and does not adequately represent real-world online selection processes. Potentially, rejected items can be revisited and a fraction of their value can be recovered. To analyze this problem, we consider general decay functions $D_1,D_2,\\ldots$, quantifying the value to be recovered from a rejected item, depending on how far it has been observed in the past. We analyze how lookback improves, or not, the competitive ratio in prophet inequalities in different order models. We show that, under mild monotonicity assumptions on the decay functions, the problem can be reduced to the case where all the decay functions are equal to the same function $x \\mapsto \\gamma x$, where $\\gamma = \\inf_{x>0} \\inf_{j \\geq 1} D_j(x)/x$. Consequently, we focus on this setting and refine the analyses of the competitive ratios, with upper and lower bounds expressed as increasing functions of $\\gamma$.","sentences":["Prophet inequalities are fundamental optimal stopping problems, where a decision-maker observes sequentially items with values sampled independently from known distributions, and must decide at each new observation to either stop and gain the current value or reject it irrevocably and move to the next step.","This model is often too pessimistic and does not adequately represent real-world online selection processes.","Potentially, rejected items can be revisited and a fraction of their value can be recovered.","To analyze this problem, we consider general decay functions $D_1,D_2,\\ldots$, quantifying the value to be recovered from a rejected item, depending on how far it has been observed in the past.","We analyze how lookback improves, or not, the competitive ratio in prophet inequalities in different order models.","We show that, under mild monotonicity assumptions on the decay functions, the problem can be reduced to the case where all the decay functions are equal to the same function $x \\mapsto \\gamma x$, where $\\gamma = \\inf_{x>0} \\inf_{j \\geq 1} D_j(x)/x$. Consequently, we focus on this setting and refine the analyses of the competitive ratios, with upper and lower bounds expressed as increasing functions of $\\gamma$."],"url":"http://arxiv.org/abs/2406.06805v1","category":"cs.DS"}
{"created":"2024-06-10 21:17:29","title":"Robustness to Missing Data: Breakdown Point Analysis","abstract":"Missing data is pervasive in econometric applications, and rarely is it plausible that the data are missing (completely) at random. This paper proposes a methodology for studying the robustness of results drawn from incomplete datasets. Selection is measured as the squared Hellinger divergence between the distributions of complete and incomplete observations, which has a natural interpretation. The breakdown point is defined as the minimal amount of selection needed to overturn a given result. Reporting point estimates and lower confidence intervals of the breakdown point is a simple, concise way to communicate the robustness of a result. An estimator of the breakdown point of a result drawn from a generalized method of moments model is proposed and shown root-n consistent and asymptotically normal under mild assumptions. Lower confidence intervals of the breakdown point are simple to construct. The paper concludes with a simulation study illustrating the finite sample performance of the estimators in several common models.","sentences":["Missing data is pervasive in econometric applications, and rarely is it plausible that the data are missing (completely) at random.","This paper proposes a methodology for studying the robustness of results drawn from incomplete datasets.","Selection is measured as the squared Hellinger divergence between the distributions of complete and incomplete observations, which has a natural interpretation.","The breakdown point is defined as the minimal amount of selection needed to overturn a given result.","Reporting point estimates and lower confidence intervals of the breakdown point is a simple, concise way to communicate the robustness of a result.","An estimator of the breakdown point of a result drawn from a generalized method of moments model is proposed and shown root-n consistent and asymptotically normal under mild assumptions.","Lower confidence intervals of the breakdown point are simple to construct.","The paper concludes with a simulation study illustrating the finite sample performance of the estimators in several common models."],"url":"http://arxiv.org/abs/2406.06804v1","category":"econ.EM"}
{"created":"2024-06-10 21:08:39","title":"LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data Caching","abstract":"As Large Language Models (LLMs) broaden their capabilities to manage thousands of API calls, they are confronted with complex data operations across vast datasets with significant overhead to the underlying system. In this work, we introduce LLM-dCache to optimize data accesses by treating cache operations as callable API functions exposed to the tool-augmented agent. We grant LLMs the autonomy to manage cache decisions via prompting, seamlessly integrating with existing function-calling mechanisms. Tested on an industry-scale massively parallel platform that spans hundreds of GPT endpoints and terabytes of imagery, our method improves Copilot times by an average of 1.24x across various LLMs and prompting techniques.","sentences":["As Large Language Models (LLMs) broaden their capabilities to manage thousands of API calls, they are confronted with complex data operations across vast datasets with significant overhead to the underlying system.","In this work, we introduce LLM-dCache to optimize data accesses by treating cache operations as callable API functions exposed to the tool-augmented agent.","We grant LLMs the autonomy to manage cache decisions via prompting, seamlessly integrating with existing function-calling mechanisms.","Tested on an industry-scale massively parallel platform that spans hundreds of GPT endpoints and terabytes of imagery, our method improves Copilot times by an average of 1.24x across various LLMs and prompting techniques."],"url":"http://arxiv.org/abs/2406.06799v1","category":"cs.DC"}
{"created":"2024-06-10 20:55:15","title":"On the coupled hydraulic and dielectric material properties of soils: combined numerical and experimental investigations","abstract":"Precise knowledge of the frequency dependent electromagnetic properties of porous media is urgently necessary for successful utilization of high frequency electromagnetic measurement techniques for near and subsurface sensing. Thus, there is a need of systematic investigations by means of dielectric spectroscopy of unsaturated and saturated soils under controlled hydraulic conditions. In this context, two-port rod based transmission lines (R-TMLs) were characterized in the frequency range from 1 MHz to 10 GHz by combined theoretical, numerical, and experimental investigations. To analyze coupled hydraulic and dielectric soil properties a slightly plastic clay soil was investigated. There is evidence that the bound water contribution of the soil is substantially lower than expected.","sentences":["Precise knowledge of the frequency dependent electromagnetic properties of porous media is urgently necessary for successful utilization of high frequency electromagnetic measurement techniques for near and subsurface sensing.","Thus, there is a need of systematic investigations by means of dielectric spectroscopy of unsaturated and saturated soils under controlled hydraulic conditions.","In this context, two-port rod based transmission lines (R-TMLs) were characterized in the frequency range from 1 MHz to 10 GHz by combined theoretical, numerical, and experimental investigations.","To analyze coupled hydraulic and dielectric soil properties a slightly plastic clay soil was investigated.","There is evidence that the bound water contribution of the soil is substantially lower than expected."],"url":"http://arxiv.org/abs/2406.06789v1","category":"physics.geo-ph"}
{"created":"2024-06-10 20:52:12","title":"Gradient ascend method for fully nonlinear parabolic differential equations with convex nonlinearity","abstract":"We introduce a generic numerical schemes for fully nonlinear parabolic PDEs on the full domain, where the nonlinearity is convex on the Hessian of the solution. The main idea behind this paper is reduction: we reduce the fully nonlinear problem to a series of simpler semilinear ones parameterized by the diffusion term and take the maximum over the solution of those problems through a novel gradient ascend method on the diffusion coefficient that we introduced for such semilinear PDEs. The promise of this study is a step toward the proof-of-concept that for any arbitrary numerical scheme for semilinear parabolic PDEs, one can build an iterative methods based on gradient ascent to converge to the solution of a fully nonlinear problem. Therefore, investment in our effort to create more efficient semilinear PDEs leads to efficiency for fully nonlinear PDEs. We leverage the theoretical results of Soner et al. [2012] to obtain numerical schemes for fully nonlinear PDEs and to justify the convergence in our methodology. The method is tested in a numerical experiment on the fully problem of portfolio optimization under stochastic volatility model.","sentences":["We introduce a generic numerical schemes for fully nonlinear parabolic PDEs on the full domain, where the nonlinearity is convex on the Hessian of the solution.","The main idea behind this paper is reduction: we reduce the fully nonlinear problem to a series of simpler semilinear ones parameterized by the diffusion term and take the maximum over the solution of those problems through a novel gradient ascend method on the diffusion coefficient that we introduced for such semilinear PDEs.","The promise of this study is a step toward the proof-of-concept that for any arbitrary numerical scheme for semilinear parabolic PDEs, one can build an iterative methods based on gradient ascent to converge to the solution of a fully nonlinear problem.","Therefore, investment in our effort to create more efficient semilinear PDEs leads to efficiency for fully nonlinear PDEs.","We leverage the theoretical results of Soner et al.","[2012] to obtain numerical schemes for fully nonlinear PDEs and to justify the convergence in our methodology.","The method is tested in a numerical experiment on the fully problem of portfolio optimization under stochastic volatility model."],"url":"http://arxiv.org/abs/2406.06787v1","category":"math.AP"}
{"created":"2024-06-10 20:39:02","title":"Optimal Demodulation Domain for Microwave SQUID Multiplexers in Presence of Readout System Noise","abstract":"The Microwave SQUID Multiplexer ({\\mu}MUX) is the device of choice for the readout of a large number of Low-Temperature Detectors in a wide variety of experiments within the fields of astronomy and particle physics. While it offers large multiplexing factors, the system noise performance is highly dependent on the cold and warm-readout electronic systems used to read it out, as well as the demodulation domain and parameters chosen. In order to understand the impact of the readout systems in the overall noise performance, first we extended the available {\\mu}MUX simulation frameworks including additive and multiplicative noise sources in the probing tones (i.e. phase and amplitude noise), along with the capability of demodulating the scientific data, either in resonator's phase or amplitude. Then, considering the additive noise as a dominant noise source, the optimum readout parameters to achieve minimum system noise were found for both open-loop and flux-ramp demodulation schemes in the aforementioned domains. Later, we evaluated the system noise sensitivity to multiplicative noise sources under the optimum readout parameters. Finally, as a case study, we evaluated the optimal demodulation domain and expected system noise level for a typical Software-Defined Radio (SDR) readout system. This work leads to an improved system performance prediction and noise engineering based on the available readout electronics and selected demodulation domain.","sentences":["The Microwave SQUID Multiplexer ({\\mu}MUX) is the device of choice for the readout of a large number of Low-Temperature Detectors in a wide variety of experiments within the fields of astronomy and particle physics.","While it offers large multiplexing factors, the system noise performance is highly dependent on the cold and warm-readout electronic systems used to read it out, as well as the demodulation domain and parameters chosen.","In order to understand the impact of the readout systems in the overall noise performance, first we extended the available {\\mu}MUX simulation frameworks including additive and multiplicative noise sources in the probing tones (i.e. phase and amplitude noise), along with the capability of demodulating the scientific data, either in resonator's phase or amplitude.","Then, considering the additive noise as a dominant noise source, the optimum readout parameters to achieve minimum system noise were found for both open-loop and flux-ramp demodulation schemes in the aforementioned domains.","Later, we evaluated the system noise sensitivity to multiplicative noise sources under the optimum readout parameters.","Finally, as a case study, we evaluated the optimal demodulation domain and expected system noise level for a typical Software-Defined Radio (SDR) readout system.","This work leads to an improved system performance prediction and noise engineering based on the available readout electronics and selected demodulation domain."],"url":"http://arxiv.org/abs/2406.06782v1","category":"physics.ins-det"}
{"created":"2024-06-10 20:32:08","title":"Bijections Between Sets of Invariant Ideals, Via the Ladder Technique","abstract":"We present a new method of establishing a bijective correspondence - in fact, a lattice isomorphism - between action- and coaction-invariant ideals of C*-algebras and their crossed products by a fixed locally compact group. It is known that such a correspondence exists whenever the group is amenable; our results hold for any locally compact group under a natural form of coaction invariance.","sentences":["We present a new method of establishing a bijective correspondence - in fact, a lattice isomorphism - between action- and coaction-invariant ideals of C*-algebras and their crossed products by a fixed locally compact group.","It is known that such a correspondence exists whenever the group is amenable; our results hold for any locally compact group under a natural form of coaction invariance."],"url":"http://arxiv.org/abs/2406.06780v1","category":"math.OA"}
{"created":"2024-06-10 20:08:49","title":"Optimal control for a SIR model with limited hospitalised patients","abstract":"This paper analyses the optimal control of infectious disease propagation using a classic susceptible-infected-recovered (SIR) model characterised by permanent immunity and the absence of available vaccines. The control is performed over a time-dependent mean reproduction number, in order to minimise the cumulative number of ever-infected individuals (recovered), under different constraints. We consider constraints on isolation measures ranging from partial lockdown to non-intervention, as well as the social and economic costs associated with such isolation, and the capacity limitations of intensive care units that limits the number of infected individuals to a maximum allowed value. We rigorously derive an optimal quarantine strategy based on necessary optimality conditions. The obtained optimal strategy is of a boundary-bang type, comprising three phases: an initial phase with no intervention, a second phase maintaining the infected population at its maximum possible value, and a final phase of partial lockdown applied over a single interval. The optimal policy is further refined by optimising the transition times between these phases. We show that these results are in excellent agreement with the numerical solution of the problem.","sentences":["This paper analyses the optimal control of infectious disease propagation using a classic susceptible-infected-recovered (SIR) model characterised by permanent immunity and the absence of available vaccines.","The control is performed over a time-dependent mean reproduction number, in order to minimise the cumulative number of ever-infected individuals (recovered), under different constraints.","We consider constraints on isolation measures ranging from partial lockdown to non-intervention, as well as the social and economic costs associated with such isolation, and the capacity limitations of intensive care units that limits the number of infected individuals to a maximum allowed value.","We rigorously derive an optimal quarantine strategy based on necessary optimality conditions.","The obtained optimal strategy is of a boundary-bang type, comprising three phases: an initial phase with no intervention, a second phase maintaining the infected population at its maximum possible value, and a final phase of partial lockdown applied over a single interval.","The optimal policy is further refined by optimising the transition times between these phases.","We show that these results are in excellent agreement with the numerical solution of the problem."],"url":"http://arxiv.org/abs/2406.06770v1","category":"math.OC"}
{"created":"2024-06-10 19:43:28","title":"Ricci curvature bounds and rigidity for non-smooth Riemannian and semi-Riemannian metrics","abstract":"We study rigidity problems for Riemannian and semi-Riemannian manifolds with metrics of low regularity. Specifically, we prove a version of the Cheeger-Gromoll splitting theorem \\cite{CheegerGromoll72splitting} for Riemannian metrics and the flatness criterion for semi-Riemannian metrics of regularity $C^1$. With our proof of the splitting theorem, we are able to obtain an isometry of higher regularity than the Lipschitz regularity guaranteed by the $\\mathsf{RCD}$-splitting theorem \\cite{gigli2013splitting, gigli2014splitoverview}. Along the way, we establish a Bochner-Weitzenb\\\"ock identity which permits %incorporates both the non-smoothness of the metric and of the vector fields, complementing a recent similar result in \\cite{mondino2024equivalence}. The last section of the article is dedicated to the discussion of various notions of Sobolev spaces in low regularity, as well as an alternative proof of the equivalence (see \\cite{mondino2024equivalence}) between distributional Ricci curvature bounds and $\\mathsf{RCD}$-type bounds, using in part the stability of the variable $\\mathsf{CD}$-condition under suitable limits \\cite{ketterer2017variableCD}.","sentences":["We study rigidity problems for Riemannian and semi-Riemannian manifolds with metrics of low regularity.","Specifically, we prove a version of the Cheeger-Gromoll splitting theorem \\cite{CheegerGromoll72splitting} for Riemannian metrics and the flatness criterion for semi-Riemannian metrics of regularity $C^1$. With our proof of the splitting theorem, we are able to obtain an isometry of higher regularity than the Lipschitz regularity guaranteed by the $\\mathsf{RCD}$-splitting theorem \\cite{gigli2013splitting, gigli2014splitoverview}.","Along the way, we establish a Bochner-Weitzenb\\\"ock identity which permits %incorporates both the non-smoothness of the metric and of the vector fields, complementing a recent similar result in \\cite{mondino2024equivalence}.","The last section of the article is dedicated to the discussion of various notions of Sobolev spaces in low regularity, as well as an alternative proof of the equivalence (see \\cite{mondino2024equivalence}) between distributional Ricci curvature bounds and $\\mathsf{RCD}$-type bounds, using in part the stability of the variable $\\mathsf{CD}$-condition under suitable limits \\cite{ketterer2017variableCD}."],"url":"http://arxiv.org/abs/2406.06762v1","category":"math.DG"}
{"created":"2024-06-10 19:41:25","title":"Scalable Private Search with Wally","abstract":"This paper presents Wally, a private search system that supports efficient semantic and keyword search queries against large databases. When sufficient clients are making the queries, Wally performance is significantly better than previous systems. In previous private search systems, for each client query, the server must perform at least one expensive cryptographic operation per database entry. As a result, performance degraded proportionally with the number of entries in the database. In Wally we get rid of this limitation. Specifically, for each query the server performs cryptographic operations only against a few database entries. We achieve these results by requiring each client to add a few fake queries, and sends each query via an anonymous network to the server at independently chosen random instants. Additionally, each client also uses somewhat homomorphic encryption (SHE) to hide whether a query is real or fake, Wally provides $(\\epsilon, \\delta)$-differential privacy guarantee, which is an accepted standard for strong privacy. The number of fake queries each client makes depends inversely on the number of clients making queries. Therefore, the fake queries' overhead vanishes as the number of clients increases, enabling scalability to millions of queries and large databases. Concretely, Wally can serve $8$M requests at a rate of 3,000 queries per second. That is around 60x higher than the state-of-the-art scheme.","sentences":["This paper presents Wally, a private search system that supports efficient semantic and keyword search queries against large databases.","When sufficient clients are making the queries, Wally performance is significantly better than previous systems.","In previous private search systems, for each client query, the server must perform at least one expensive cryptographic operation per database entry.","As a result, performance degraded proportionally with the number of entries in the database.","In Wally we get rid of this limitation.","Specifically, for each query the server performs cryptographic operations only against a few database entries.","We achieve these results by requiring each client to add a few fake queries, and sends each query via an anonymous network to the server at independently chosen random instants.","Additionally, each client also uses somewhat homomorphic encryption (SHE) to hide whether a query is real or fake, Wally provides $(\\epsilon, \\delta)$-differential privacy guarantee, which is an accepted standard for strong privacy.","The number of fake queries each client makes depends inversely on the number of clients making queries.","Therefore, the fake queries' overhead vanishes as the number of clients increases, enabling scalability to millions of queries and large databases.","Concretely, Wally can serve $8$M requests at a rate of 3,000 queries per second.","That is around 60x higher than the state-of-the-art scheme."],"url":"http://arxiv.org/abs/2406.06761v1","category":"cs.CR"}
{"created":"2024-06-10 19:38:04","title":"Decentralized Reliability Estimation for Mixnets","abstract":"Continuous-time decryption mixnets can anonymously route data packets with end to end latency that can be as low as a second, making them usable for a variety of applications. Such mixnets however lack verifiable reliability properties that ensure the correct processing and delivery of packets, while existing verifiability mechanisms are incompatible with scalable low latency continuous-time mixnets due to imposing overheads measuring in minutes to hours. This work addresses this gap by proposing a scheme that can estimate reliability scores for links and nodes forming a continuous-time mixnet where some form of credentials authorize clients to send traffic. The scores can be computed publicly by all participants from a set of measurement packets that are eventually revealed and act as a random sample of the traffic, without affecting mixnet transmission latency for client packets. Our scheme relies on VRF-based routing, a novel primitive that ensures that legitimate client packets follow the routing policy of the mixnet, as well as randomly generating unforgeable measurement packets. We experimentally validate our construction both in unreliable and adversarial settings, demonstrating its feasibility.","sentences":["Continuous-time decryption mixnets can anonymously route data packets with end to end latency that can be as low as a second, making them usable for a variety of applications.","Such mixnets however lack verifiable reliability properties that ensure the correct processing and delivery of packets, while existing verifiability mechanisms are incompatible with scalable low latency continuous-time mixnets due to imposing overheads measuring in minutes to hours.","This work addresses this gap by proposing a scheme that can estimate reliability scores for links and nodes forming a continuous-time mixnet where some form of credentials authorize clients to send traffic.","The scores can be computed publicly by all participants from a set of measurement packets that are eventually revealed and act as a random sample of the traffic, without affecting mixnet transmission latency for client packets.","Our scheme relies on VRF-based routing, a novel primitive that ensures that legitimate client packets follow the routing policy of the mixnet, as well as randomly generating unforgeable measurement packets.","We experimentally validate our construction both in unreliable and adversarial settings, demonstrating its feasibility."],"url":"http://arxiv.org/abs/2406.06760v1","category":"cs.CR"}
{"created":"2024-06-10 19:34:22","title":"Characterization of wormhole space-times supported by a covariant action-dependent Lagrangian theory","abstract":"In this work, we undertake an analysis of new wormhole solutions within an action-dependent Lagrangian framework. These geometries can be traversable and supported by a positive energy density. The modification of the gravitational field equations is produced by the inclusion in the gravitational Lagrangian linear of a background four-vector $\\lambda_{\\mu}$. This new term expands significantly the conventional description of gravity making it highly non-linear, and therefore drawing general conclusions about legitimate forms of $\\lambda_{\\mu}$ proves a formidable task in general. It is, then, customary to adopt an ansatz that strikes a balance between enabling new phenomenology while retaining a significant degree of generality on $\\lambda_\\mu$. Ours is given by the choice $\\lambda_\\mu=(0,\\lambda_1(r), 0, 0)$, with an arbitrary $\\lambda_1(r)$. By setting $\\lambda_1(r)=-1/r$ we craft new families with physically desirable properties, but the wormholes thus generated turn out to be conical, as evidenced by an angle deficit, in a similar fashion to other known solution families. Under the general shape of $\\lambda_1(r)$, we demonstrate that these solutions are not compatible with the Null Energy Condition (NEC) in general, as it happens to their General Relativity counterparts, except on specific occasions where the derivative of the redshift function of the metric diverges at the throat (however, in these latter cases, the traversability of the wormhole will be disrupted). On the other hand, it is possible to solve the conical character and satisfies the flatness condition for more general functions of $\\lambda_1(r)$.","sentences":["In this work, we undertake an analysis of new wormhole solutions within an action-dependent Lagrangian framework.","These geometries can be traversable and supported by a positive energy density.","The modification of the gravitational field equations is produced by the inclusion in the gravitational Lagrangian linear of a background four-vector $\\lambda_{\\mu}$.","This new term expands significantly the conventional description of gravity making it highly non-linear, and therefore drawing general conclusions about legitimate forms of $\\lambda_{\\mu}$ proves a formidable task in general.","It is, then, customary to adopt an ansatz that strikes a balance between enabling new phenomenology while retaining a significant degree of generality on $\\lambda_\\mu$. Ours is given by the choice $\\lambda_\\mu=(0,\\lambda_1(r), 0, 0)$, with an arbitrary $\\lambda_1(r)$. By setting $\\lambda_1(r)=-1/r$ we craft new families with physically desirable properties, but the wormholes thus generated turn out to be conical, as evidenced by an angle deficit, in a similar fashion to other known solution families.","Under the general shape of $\\lambda_1(r)$, we demonstrate that these solutions are not compatible with the Null Energy Condition (NEC) in general, as it happens to their General Relativity counterparts, except on specific occasions where the derivative of the redshift function of the metric diverges at the throat (however, in these latter cases, the traversability of the wormhole will be disrupted).","On the other hand, it is possible to solve the conical character and satisfies the flatness condition for more general functions of $\\lambda_1(r)$."],"url":"http://arxiv.org/abs/2406.06756v1","category":"gr-qc"}
{"created":"2024-06-11 17:59:53","title":"Image and Video Tokenization with Binary Spherical Quantization","abstract":"We propose a new transformer-based image and video tokenizer with Binary Spherical Quantization (BSQ). BSQ projects the high-dimensional visual embedding to a lower-dimensional hypersphere and then applies binary quantization. BSQ is (1) parameter-efficient without an explicit codebook, (2) scalable to arbitrary token dimensions, and (3) compact: compressing visual data by up to 100$\\times$ with minimal distortion. Our tokenizer uses a transformer encoder and decoder with simple block-wise causal masking to support variable-length videos as input. The resulting BSQ-ViT achieves state-of-the-art visual reconstruction quality on image and video reconstruction benchmarks with 2.4$\\times$ throughput compared to the best prior methods. Furthermore, by learning an autoregressive prior for adaptive arithmetic coding, BSQ-ViT achieves comparable results on video compression with state-of-the-art video compression standards. BSQ-ViT also enables masked language models to achieve competitive image synthesis quality to GAN- and diffusion-based methods.","sentences":["We propose a new transformer-based image and video tokenizer with Binary Spherical Quantization (BSQ).","BSQ projects the high-dimensional visual embedding to a lower-dimensional hypersphere and then applies binary quantization.","BSQ is (1) parameter-efficient without an explicit codebook, (2) scalable to arbitrary token dimensions, and (3) compact: compressing visual data by up to 100$\\times$ with minimal distortion.","Our tokenizer uses a transformer encoder and decoder with simple block-wise causal masking to support variable-length videos as input.","The resulting BSQ-ViT achieves state-of-the-art visual reconstruction quality on image and video reconstruction benchmarks with 2.4$\\times$ throughput compared to the best prior methods.","Furthermore, by learning an autoregressive prior for adaptive arithmetic coding, BSQ-ViT achieves comparable results on video compression with state-of-the-art video compression standards.","BSQ-ViT also enables masked language models to achieve competitive image synthesis quality to GAN- and diffusion-based methods."],"url":"http://arxiv.org/abs/2406.07548v1","category":"cs.CV"}
{"created":"2024-06-11 17:29:51","title":"Transition from decaying to decayless kink oscillations of solar coronal loops","abstract":"The transition of an impulsively excited kink oscillation of a solar coronal loop to an oscillation with a stationary amplitude, i.e., the damping pattern, is determined using the low-dimensional self-oscillation model. In the model, the decayless kink oscillations are sustained by the interaction of the oscillating loop with an external quasi-steady flow. The analytical solution is based on the assumption that the combined effect of the effective dissipation, for example, by resonant absorption, and interaction with an external flow, is weak. The effect is characterised by a dimensionless coupling parameter. The damping pattern is found to depend upon the initial amplitude and the coupling parameter. The approximate expression shows a good agreement with a numerical solution of the self-oscillation equation. The plausibility of the established damping pattern is demonstrated by an observational example. Notably, the damping pattern is not exponential, and the characteristic decay time is different from the time determined by the traditionally used exponential damping fit. Implications of this finding for seismology of the solar coronal plasmas are discussed. In particular, it is suggested that a very rapid, in less than the oscillation period, decay of the oscillation to the stationary level, achieved for larger values of the coupling parameter, can explain the relative rareness of the kink oscillation events.","sentences":["The transition of an impulsively excited kink oscillation of a solar coronal loop to an oscillation with a stationary amplitude, i.e., the damping pattern, is determined using the low-dimensional self-oscillation model.","In the model, the decayless kink oscillations are sustained by the interaction of the oscillating loop with an external quasi-steady flow.","The analytical solution is based on the assumption that the combined effect of the effective dissipation, for example, by resonant absorption, and interaction with an external flow, is weak.","The effect is characterised by a dimensionless coupling parameter.","The damping pattern is found to depend upon the initial amplitude and the coupling parameter.","The approximate expression shows a good agreement with a numerical solution of the self-oscillation equation.","The plausibility of the established damping pattern is demonstrated by an observational example.","Notably, the damping pattern is not exponential, and the characteristic decay time is different from the time determined by the traditionally used exponential damping fit.","Implications of this finding for seismology of the solar coronal plasmas are discussed.","In particular, it is suggested that a very rapid, in less than the oscillation period, decay of the oscillation to the stationary level, achieved for larger values of the coupling parameter, can explain the relative rareness of the kink oscillation events."],"url":"http://arxiv.org/abs/2406.07490v1","category":"astro-ph.SR"}
{"created":"2024-06-11 17:27:23","title":"GLAD: Towards Better Reconstruction with Global and Local Adaptive Diffusion Models for Unsupervised Anomaly Detection","abstract":"Diffusion models have shown superior performance on unsupervised anomaly detection tasks. Since trained with normal data only, diffusion models tend to reconstruct normal counterparts of test images with certain noises added. However, these methods treat all potential anomalies equally, which may cause two main problems. From the global perspective, the difficulty of reconstructing images with different anomalies is uneven. Therefore, instead of utilizing the same setting for all samples, we propose to predict a particular denoising step for each sample by evaluating the difference between image contents and the priors extracted from diffusion models. From the local perspective, reconstructing abnormal regions differs from normal areas even in the same image. Theoretically, the diffusion model predicts a noise for each step, typically following a standard Gaussian distribution. However, due to the difference between the anomaly and its potential normal counterpart, the predicted noise in abnormal regions will inevitably deviate from the standard Gaussian distribution. To this end, we propose introducing synthetic abnormal samples in training to encourage the diffusion models to break through the limitation of standard Gaussian distribution, and a spatial-adaptive feature fusion scheme is utilized during inference. With the above modifications, we propose a global and local adaptive diffusion model (abbreviated to GLAD) for unsupervised anomaly detection, which introduces appealing flexibility and achieves anomaly-free reconstruction while retaining as much normal information as possible. Extensive experiments are conducted on three commonly used anomaly detection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board dataset (PCB-Bank) we integrated, showing the effectiveness of the proposed method.","sentences":["Diffusion models have shown superior performance on unsupervised anomaly detection tasks.","Since trained with normal data only, diffusion models tend to reconstruct normal counterparts of test images with certain noises added.","However, these methods treat all potential anomalies equally, which may cause two main problems.","From the global perspective, the difficulty of reconstructing images with different anomalies is uneven.","Therefore, instead of utilizing the same setting for all samples, we propose to predict a particular denoising step for each sample by evaluating the difference between image contents and the priors extracted from diffusion models.","From the local perspective, reconstructing abnormal regions differs from normal areas even in the same image.","Theoretically, the diffusion model predicts a noise for each step, typically following a standard Gaussian distribution.","However, due to the difference between the anomaly and its potential normal counterpart, the predicted noise in abnormal regions will inevitably deviate from the standard Gaussian distribution.","To this end, we propose introducing synthetic abnormal samples in training to encourage the diffusion models to break through the limitation of standard Gaussian distribution, and a spatial-adaptive feature fusion scheme is utilized during inference.","With the above modifications, we propose a global and local adaptive diffusion model (abbreviated to GLAD) for unsupervised anomaly detection, which introduces appealing flexibility and achieves anomaly-free reconstruction while retaining as much normal information as possible.","Extensive experiments are conducted on three commonly used anomaly detection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board dataset (PCB-Bank) we integrated, showing the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2406.07487v1","category":"cs.CV"}
{"created":"2024-06-11 17:01:45","title":"fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis functions","abstract":"Recent advancements in neural network design have given rise to the development of Kolmogorov-Arnold Networks (KANs), which enhance speed, interpretability, and precision. This paper presents the Fractional Kolmogorov-Arnold Network (fKAN), a novel neural network architecture that incorporates the distinctive attributes of KANs with a trainable adaptive fractional-orthogonal Jacobi function as its basis function. By leveraging the unique mathematical properties of fractional Jacobi functions, including simple derivative formulas, non-polynomial behavior, and activity for both positive and negative input values, this approach ensures efficient learning and enhanced accuracy. The proposed architecture is evaluated across a range of tasks in deep learning and physics-informed deep learning. Precision is tested on synthetic regression data, image classification, image denoising, and sentiment analysis. Additionally, the performance is measured on various differential equations, including ordinary, partial, and fractional delay differential equations. The results demonstrate that integrating fractional Jacobi functions into KANs significantly improves training speed and performance across diverse fields and applications.","sentences":["Recent advancements in neural network design have given rise to the development of Kolmogorov-Arnold Networks (KANs), which enhance speed, interpretability, and precision.","This paper presents the Fractional Kolmogorov-Arnold Network (fKAN), a novel neural network architecture that incorporates the distinctive attributes of KANs with a trainable adaptive fractional-orthogonal Jacobi function as its basis function.","By leveraging the unique mathematical properties of fractional Jacobi functions, including simple derivative formulas, non-polynomial behavior, and activity for both positive and negative input values, this approach ensures efficient learning and enhanced accuracy.","The proposed architecture is evaluated across a range of tasks in deep learning and physics-informed deep learning.","Precision is tested on synthetic regression data, image classification, image denoising, and sentiment analysis.","Additionally, the performance is measured on various differential equations, including ordinary, partial, and fractional delay differential equations.","The results demonstrate that integrating fractional Jacobi functions into KANs significantly improves training speed and performance across diverse fields and applications."],"url":"http://arxiv.org/abs/2406.07456v1","category":"cs.LG"}
{"created":"2024-06-11 16:34:02","title":"Learning Domain-Invariant Features for Out-of-Context News Detection","abstract":"Multimodal out-of-context news is a common type of misinformation on online media platforms. This involves posting a caption, alongside an invalid out-of-context news image. Reflecting its importance, researchers have developed models to detect such misinformation. However, a common limitation of these models is that they only consider the scenario where pre-labeled data is available for each domain, failing to address the out-of-context news detection on unlabeled domains (e.g., unverified news on new topics or agencies). In this work, we therefore focus on domain adaptive out-of-context news detection. In order to effectively adapt the detection model to unlabeled news topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation with Test-Time Adaptation) which applies contrastive learning and maximum mean discrepancy (MMD) to learn the domain-invariant feature. In addition, it leverages target domain statistics during test-time to further assist domain adaptation. Experimental results show that our approach outperforms baselines in 5 out of 7 domain adaptation settings on two public datasets, by as much as 2.93% in F1 and 2.08% in accuracy.","sentences":["Multimodal out-of-context news is a common type of misinformation on online media platforms.","This involves posting a caption, alongside an invalid out-of-context news image.","Reflecting its importance, researchers have developed models to detect such misinformation.","However, a common limitation of these models is that they only consider the scenario where pre-labeled data is available for each domain, failing to address the out-of-context news detection on unlabeled domains (e.g., unverified news on new topics or agencies).","In this work, we therefore focus on domain adaptive out-of-context news detection.","In order to effectively adapt the detection model to unlabeled news topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation with Test-Time Adaptation) which applies contrastive learning and maximum mean discrepancy (MMD) to learn the domain-invariant feature.","In addition, it leverages target domain statistics during test-time to further assist domain adaptation.","Experimental results show that our approach outperforms baselines in 5 out of 7 domain adaptation settings on two public datasets, by as much as 2.93% in F1 and 2.08% in accuracy."],"url":"http://arxiv.org/abs/2406.07430v1","category":"cs.CL"}
{"created":"2024-06-11 16:21:48","title":"Single and merger soliton dynamics in scalar field dark matter with and without self-interactions","abstract":"(abridged)Scalar field dark matter (SFDM) made of bosons has become a popular alternative to the CDM paradigm, especially for its potential to cure the so-called \"small-scale problems\" of CDM. Cosmological simulations have determined that SFDM halos exhibit a core-envelope structure, but they are computationally expensive. Halo cores have been found to be well approximated by \"solitons\". The study of single soliton and multiple soliton merger dynamics constitutes a more feasible approach to investigate in detail the genuine quantum dynamics of SFDM and its interplay with self-gravity for a multitude of free boson parameters. In this paper, we present dedicated simulations of single solitons and binary soliton mergers, for models without and with a 2-boson, repulsive, weak to intermediate self-interaction (SI), as well as multiple soliton mergers without SI. We adapt the open-source code Pyultralight to simulate solitons with SI. We derive numerical scaling relations between the central density and mass of solitons for several values of SI and find deviations from the monotonic relations known from fuzzy dark matter (no SI), or the strongly repulsive Thomas-Fermi regime. Solitons with SI exemplify larger cores and lower central densities, compared to solitons without SI. Using our simulations, we extract numerical density profiles for solitons and post-merger objects, and fit them to analytic functions of previous literature. We find a mild preference for Gaussian cores for objects with SI, while the envelopes of post-mergers can be fit to NFW profiles albeit with some caution as we discuss. Similar to previous work, we find global, persistent oscillations for solitons as well as post-mergers, confirming that self-gravitating SFDM has very long relaxation times, although objects with SI exhibit oscillations of comparatively smaller amplitude.","sentences":["(abridged)Scalar field dark matter (SFDM) made of bosons has become a popular alternative to the CDM paradigm, especially for its potential to cure the so-called \"small-scale problems\" of CDM.","Cosmological simulations have determined that SFDM halos exhibit a core-envelope structure, but they are computationally expensive.","Halo cores have been found to be well approximated by \"solitons\".","The study of single soliton and multiple soliton merger dynamics constitutes a more feasible approach to investigate in detail the genuine quantum dynamics of SFDM and its interplay with self-gravity for a multitude of free boson parameters.","In this paper, we present dedicated simulations of single solitons and binary soliton mergers, for models without and with a 2-boson, repulsive, weak to intermediate self-interaction (SI), as well as multiple soliton mergers without SI.","We adapt the open-source code Pyultralight to simulate solitons with SI.","We derive numerical scaling relations between the central density and mass of solitons for several values of SI and find deviations from the monotonic relations known from fuzzy dark matter (no SI), or the strongly repulsive Thomas-Fermi regime.","Solitons with SI exemplify larger cores and lower central densities, compared to solitons without SI.","Using our simulations, we extract numerical density profiles for solitons and post-merger objects, and fit them to analytic functions of previous literature.","We find a mild preference for Gaussian cores for objects with SI, while the envelopes of post-mergers can be fit to NFW profiles albeit with some caution as we discuss.","Similar to previous work, we find global, persistent oscillations for solitons as well as post-mergers, confirming that self-gravitating SFDM has very long relaxation times, although objects with SI exhibit oscillations of comparatively smaller amplitude."],"url":"http://arxiv.org/abs/2406.07419v1","category":"astro-ph.CO"}
{"created":"2024-06-11 16:07:08","title":"Redefining Automotive Radar Imaging: A Domain-Informed 1D Deep Learning Approach for High-Resolution and Efficient Performance","abstract":"Millimeter-wave (mmWave) radars are indispensable for perception tasks of autonomous vehicles, thanks to their resilience in challenging weather conditions. Yet, their deployment is often limited by insufficient spatial resolution for precise semantic scene interpretation. Classical super-resolution techniques adapted from optical imaging inadequately address the distinct characteristics of radar signal data. In response, our study redefines radar imaging super-resolution as a one-dimensional (1D) signal super-resolution spectra estimation problem by harnessing the radar signal processing domain knowledge, introducing innovative data normalization and a domain-informed signal-to-noise ratio (SNR)-guided loss function. Our tailored deep learning network for automotive radar imaging exhibits remarkable scalability, parameter efficiency and fast inference speed, alongside enhanced performance in terms of radar imaging quality and resolution. Extensive testing confirms that our SR-SPECNet sets a new benchmark in producing high-resolution radar range-azimuth images, outperforming existing methods across varied antenna configurations and dataset sizes. Source code and new radar dataset will be made publicly available online.","sentences":["Millimeter-wave (mmWave) radars are indispensable for perception tasks of autonomous vehicles, thanks to their resilience in challenging weather conditions.","Yet, their deployment is often limited by insufficient spatial resolution for precise semantic scene interpretation.","Classical super-resolution techniques adapted from optical imaging inadequately address the distinct characteristics of radar signal data.","In response, our study redefines radar imaging super-resolution as a one-dimensional (1D) signal super-resolution spectra estimation problem by harnessing the radar signal processing domain knowledge, introducing innovative data normalization and a domain-informed signal-to-noise ratio (SNR)-guided loss function.","Our tailored deep learning network for automotive radar imaging exhibits remarkable scalability, parameter efficiency and fast inference speed, alongside enhanced performance in terms of radar imaging quality and resolution.","Extensive testing confirms that our SR-SPECNet sets a new benchmark in producing high-resolution radar range-azimuth images, outperforming existing methods across varied antenna configurations and dataset sizes.","Source code and new radar dataset will be made publicly available online."],"url":"http://arxiv.org/abs/2406.07399v1","category":"cs.LG"}
{"created":"2024-06-11 15:06:15","title":"Transferring Knowledge from Large Foundation Models to Small Downstream Models","abstract":"How do we transfer the relevant knowledge from ever larger foundation models into small, task-specific downstream models that can run at much lower costs? Standard transfer learning using pre-trained weights as the initialization transfers limited information and commits us to often massive pre-trained architectures. This procedure also precludes combining multiple pre-trained models that learn complementary information. To address these shortcomings, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the smaller downstream model. Rather than indiscriminately compressing all pre-trained features, AFT adaptively transfers pre-trained features that are most useful for performing the downstream task, using a simple regularization that adds minimal overhead. Across multiple vision, language, and multi-modal datasets, AFT achieves significantly better downstream performance compared to alternatives with a similar computational cost. Furthermore, AFT reliably translates improvement in pre-trained models into improvement in downstream performance, even if the downstream model is over $50\\times$ smaller, and can effectively transfer complementary information learned by multiple pre-trained models.","sentences":["How do we transfer the relevant knowledge from ever larger foundation models into small, task-specific downstream models that can run at much lower costs?","Standard transfer learning using pre-trained weights as the initialization transfers limited information and commits us to often massive pre-trained architectures.","This procedure also precludes combining multiple pre-trained models that learn complementary information.","To address these shortcomings, we introduce Adaptive Feature Transfer (AFT).","Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the smaller downstream model.","Rather than indiscriminately compressing all pre-trained features, AFT adaptively transfers pre-trained features that are most useful for performing the downstream task, using a simple regularization that adds minimal overhead.","Across multiple vision, language, and multi-modal datasets, AFT achieves significantly better downstream performance compared to alternatives with a similar computational cost.","Furthermore, AFT reliably translates improvement in pre-trained models into improvement in downstream performance, even if the downstream model is over $50\\times$ smaller, and can effectively transfer complementary information learned by multiple pre-trained models."],"url":"http://arxiv.org/abs/2406.07337v1","category":"cs.LG"}
{"created":"2024-06-11 12:24:52","title":"Mean-Field Magnetohydrodynamics Models as Scaling Limits of Stochastic Induction Equations","abstract":"We study the asymptotic properties of a stochastic model for the induction equations of the magnetic field in a three dimensional periodic domain. The turbulent velocity field driving the electromotive force on the magnetic field is modeled by a noise white in time. For this model we rigorously take a scaling limit leading to a deterministic model. While in case of isotropic turbulence this produces an additional dissipation in the limit model which influences also the decay rate of the Magnetic field in the stochastic model, the case of turbulence devoloped in a preferential direction allows us to find a dynamo effect.","sentences":["We study the asymptotic properties of a stochastic model for the induction equations of the magnetic field in a three dimensional periodic domain.","The turbulent velocity field driving the electromotive force on the magnetic field is modeled by a noise white in time.","For this model we rigorously take a scaling limit leading to a deterministic model.","While in case of isotropic turbulence this produces an additional dissipation in the limit model which influences also the decay rate of the Magnetic field in the stochastic model, the case of turbulence devoloped in a preferential direction allows us to find a dynamo effect."],"url":"http://arxiv.org/abs/2406.07206v1","category":"math.PR"}
{"created":"2024-06-11 07:37:33","title":"Exoplanets in reflected starlight with dual-field interferometry: A case for shorter wavelengths and a fifth Unit Telescope at VLTI/Paranal","abstract":"The direct observation of cold and temperate planets within 1 to 10 AU would be extremely valuable for uncovering their atmospheric compositions but remains a formidable challenge with current astronomical methods. Ground-based optical interferometry, capable of high angular-resolution imaging, offers a promising avenue for studying these exoplanets, complementing space-based observations. Our objective is to explore the fundamental limits of dual-field interferometry and assess its potential for characterizing exoplanets in reflected light using the Very Large Telescope Interferometer (VLTI). We developed analytical expressions to describe the performance of dual-field interferometry and integrated these with simulations of atmospheric wavefronts corrected by extreme Adaptive Optics. An analytical solution for optimal phase apodization was formulated to enhance starlight rejection when injected into a single-mode fibre. This framework was applied to determine the detectability of known exoplanets in reflected light across various wavelength bands for both the current VLTI and a proposed extended version. Our results indicate that employing shorter wavelengths improves detectability, enabling at least seven Jupiter-mass exoplanets to be observed in the J band with current VLTI's baselines. Adding new baselines with lengths beyond 200 meters significantly enhances VLTI's capabilities, increasing the number of detectable exoplanets and revealing potential habitable zone candidates such as $\\tau$ Ceti e and Proxima Centauri b. To substantially improve the VLTI's exoplanet characterization capabilities, we recommend developing instrumentation at wavelengths shorter than 1$\\,\\mu$m, as well as the addition of a fifth Unit Telescope (UT5).","sentences":["The direct observation of cold and temperate planets within 1 to 10 AU would be extremely valuable for uncovering their atmospheric compositions but remains a formidable challenge with current astronomical methods.","Ground-based optical interferometry, capable of high angular-resolution imaging, offers a promising avenue for studying these exoplanets, complementing space-based observations.","Our objective is to explore the fundamental limits of dual-field interferometry and assess its potential for characterizing exoplanets in reflected light using the Very Large Telescope Interferometer (VLTI).","We developed analytical expressions to describe the performance of dual-field interferometry and integrated these with simulations of atmospheric wavefronts corrected by extreme Adaptive Optics.","An analytical solution for optimal phase apodization was formulated to enhance starlight rejection when injected into a single-mode fibre.","This framework was applied to determine the detectability of known exoplanets in reflected light across various wavelength bands for both the current VLTI and a proposed extended version.","Our results indicate that employing shorter wavelengths improves detectability, enabling at least seven Jupiter-mass exoplanets to be observed in the J band with current VLTI's baselines.","Adding new baselines with lengths beyond 200 meters significantly enhances VLTI's capabilities, increasing the number of detectable exoplanets and revealing potential habitable zone candidates such as $\\tau$ Ceti e and Proxima Centauri b.","To substantially improve the VLTI's exoplanet characterization capabilities, we recommend developing instrumentation at wavelengths shorter than 1$\\,\\mu$m, as well as the addition of a fifth Unit Telescope (UT5)."],"url":"http://arxiv.org/abs/2406.07030v1","category":"astro-ph.EP"}
{"created":"2024-06-11 07:00:08","title":"Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference","abstract":"The customization of large language models (LLMs) for user-specified tasks gets important. However, maintaining all the customized LLMs on cloud servers incurs substantial memory and computational overheads, and uploading user data can also lead to privacy concerns. On-device LLMs can offer a promising solution by mitigating these issues. Yet, the performance of on-device LLMs is inherently constrained by the limitations of small-scaled models. To overcome these restrictions, we first propose Crayon, a novel approach for on-device LLM customization. Crayon begins by constructing a pool of diverse base adapters, and then we instantly blend them into a customized adapter without extra training. In addition, we develop a device-server hybrid inference strategy, which deftly allocates more demanding queries or non-customized tasks to a larger, more capable LLM on a server. This ensures optimal performance without sacrificing the benefits of on-device customization. We carefully craft a novel benchmark from multiple question-answer datasets, and show the efficacy of our method in the LLM customization.","sentences":["The customization of large language models (LLMs) for user-specified tasks gets important.","However, maintaining all the customized LLMs on cloud servers incurs substantial memory and computational overheads, and uploading user data can also lead to privacy concerns.","On-device LLMs can offer a promising solution by mitigating these issues.","Yet, the performance of on-device LLMs is inherently constrained by the limitations of small-scaled models.","To overcome these restrictions, we first propose Crayon, a novel approach for on-device LLM customization.","Crayon begins by constructing a pool of diverse base adapters, and then we instantly blend them into a customized adapter without extra training.","In addition, we develop a device-server hybrid inference strategy, which deftly allocates more demanding queries or non-customized tasks to a larger, more capable LLM on a server.","This ensures optimal performance without sacrificing the benefits of on-device customization.","We carefully craft a novel benchmark from multiple question-answer datasets, and show the efficacy of our method in the LLM customization."],"url":"http://arxiv.org/abs/2406.07007v1","category":"cs.CL"}
{"created":"2024-06-11 06:28:21","title":"On the H\u00f6lder Stability of Multiset and Graph Neural Networks","abstract":"Famously, multiset neural networks based on sum-pooling can separate all distinct multisets, and as a result can be used by message passing neural networks (MPNNs) to separate all pairs of graphs that can be separated by the 1-WL graph isomorphism test. However, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" multisets and graphs might even be considered identical when using fixed finite precision.   In this work, we propose to fully analyze the separation quality of multiset models and MPNNs via a novel adaptation of Lipschitz and H\\\"{o}lder continuity to parametric functions. We prove that common sum-based models are lower-H\\\"{o}lder continuous, with a H\\\"{o}lder exponent that decays rapidly with the network's depth. Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful MPNNs. To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz continuous. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks.","sentences":["Famously, multiset neural networks based on sum-pooling can separate all distinct multisets, and as a result can be used by message passing neural networks (MPNNs) to separate all pairs of graphs that can be separated by the 1-WL graph isomorphism test.","However, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" multisets and graphs might even be considered identical when using fixed finite precision.   ","In this work, we propose to fully analyze the separation quality of multiset models and MPNNs via a novel adaptation of Lipschitz and H\\\"{o}lder continuity to parametric functions.","We prove that common sum-based models are lower-H\\\"{o}lder continuous, with a H\\\"{o}lder exponent that decays rapidly with the network's depth.","Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful MPNNs.","To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz continuous.","We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks."],"url":"http://arxiv.org/abs/2406.06984v1","category":"cs.LG"}
{"created":"2024-06-11 05:25:25","title":"Stepwise Regression and Pre-trained Edge for Robust Stereo Matching","abstract":"Due to the difficulty in obtaining real samples and ground truth, the generalization performance and the fine-tuned performance are critical for the feasibility of stereo matching methods in real-world applications. However, the presence of substantial disparity distributions and density variations across different datasets presents significant challenges for the generalization and fine-tuning of the model. In this paper, we propose a novel stereo matching method, called SR-Stereo, which mitigates the distributional differences across different datasets by predicting the disparity clips and uses a loss weight related to the regression target scale to improve the accuracy of the disparity clips. Moreover, this stepwise regression architecture can be easily extended to existing iteration-based methods to improve the performance without changing the structure. In addition, to mitigate the edge blurring of the fine-tuned model on sparse ground truth, we propose Domain Adaptation Based on Pre-trained Edges (DAPE). Specifically, we use the predicted disparity and RGB image to estimate the edge map of the target domain image. The edge map is filtered to generate edge map background pseudo-labels, which together with the sparse ground truth disparity on the target domain are used as a supervision to jointly fine-tune the pre-trained stereo matching model. These proposed methods are extensively evaluated on SceneFlow, KITTI, Middbury 2014 and ETH3D. The SR-Stereo achieves competitive disparity estimation performance and state-of-the-art cross-domain generalisation performance. Meanwhile, the proposed DAPE significantly improves the disparity estimation performance of fine-tuned models, especially in the textureless and detail regions.","sentences":["Due to the difficulty in obtaining real samples and ground truth, the generalization performance and the fine-tuned performance are critical for the feasibility of stereo matching methods in real-world applications.","However, the presence of substantial disparity distributions and density variations across different datasets presents significant challenges for the generalization and fine-tuning of the model.","In this paper, we propose a novel stereo matching method, called SR-Stereo, which mitigates the distributional differences across different datasets by predicting the disparity clips and uses a loss weight related to the regression target scale to improve the accuracy of the disparity clips.","Moreover, this stepwise regression architecture can be easily extended to existing iteration-based methods to improve the performance without changing the structure.","In addition, to mitigate the edge blurring of the fine-tuned model on sparse ground truth, we propose Domain Adaptation Based on Pre-trained Edges (DAPE).","Specifically, we use the predicted disparity and RGB image to estimate the edge map of the target domain image.","The edge map is filtered to generate edge map background pseudo-labels, which together with the sparse ground truth disparity on the target domain are used as a supervision to jointly fine-tune the pre-trained stereo matching model.","These proposed methods are extensively evaluated on SceneFlow, KITTI, Middbury 2014 and ETH3D. The SR-Stereo achieves competitive disparity estimation performance and state-of-the-art cross-domain generalisation performance.","Meanwhile, the proposed DAPE significantly improves the disparity estimation performance of fine-tuned models, especially in the textureless and detail regions."],"url":"http://arxiv.org/abs/2406.06953v1","category":"cs.CV"}
{"created":"2024-06-11 03:15:47","title":"Scalability in Workforce Management: Applying Scalability Principles to Foster a Four-Day Work Week","abstract":"The traditional five-day workweek faces mounting challenges, prompting exploration of alternative models like the four-day workweek. This research explores the transformative potential of scalability principles derived from cloud computing and IT in redefining workforce management for a four-day workweek. The study employs a Multivocal Literacy Research methodology, combining grey literature and systematic review approaches. Through a comprehensive review of related work, the challenges, and benefits of transitioning to a four-day workweek are explored. Pilot programs, clear communication, and agility are identified as critical success factors. The synthesis of scalability principles in workforce management serves as a powerful framework for a smooth transition towards a four-day workweek. By prioritizing adaptability, dynamic resource allocation, and data-driven insights, organizations can unlock the full potential of a compressed work schedule. This research contributes valuable insights for organizations seeking to thrive in the evolving landscape of modern work structures and prioritizing employee well-being.","sentences":["The traditional five-day workweek faces mounting challenges, prompting exploration of alternative models like the four-day workweek.","This research explores the transformative potential of scalability principles derived from cloud computing and IT in redefining workforce management for a four-day workweek.","The study employs a Multivocal Literacy Research methodology, combining grey literature and systematic review approaches.","Through a comprehensive review of related work, the challenges, and benefits of transitioning to a four-day workweek are explored.","Pilot programs, clear communication, and agility are identified as critical success factors.","The synthesis of scalability principles in workforce management serves as a powerful framework for a smooth transition towards a four-day workweek.","By prioritizing adaptability, dynamic resource allocation, and data-driven insights, organizations can unlock the full potential of a compressed work schedule.","This research contributes valuable insights for organizations seeking to thrive in the evolving landscape of modern work structures and prioritizing employee well-being."],"url":"http://arxiv.org/abs/2406.06915v1","category":"cs.CY"}
{"created":"2024-06-11 02:07:47","title":"A Subjective Quality Evaluation of 3D Mesh with Dynamic Level of Detail in Virtual Reality","abstract":"3D meshes are one of the main components of Virtual Reality applications. However, many network and computational resources are required to process 3D meshes in real-time. A potential solution to this challenge is to dynamically adapt the Level of Detail (LoD) of a 3D mesh based on the object's position and the user's viewpoint. In this paper, we conduct a subjective study to investigate users' quality perception of 3D meshes with dynamic Level of Detail in a Virtual Reality environment. The subjective experiment is carried out with five 3D meshes of different characteristics, four Levels of Detail, and four distance settings. The results of the experiment show that the impact of the dynamic level of detail depends on both the position of the 3D object in the virtual world and the number of vertices of the original mesh. In addition, we present a quality model that can accurately predict the MOS score of a LoD version of a 3D mesh from the number of vertices and the distance from the viewpoint.","sentences":["3D meshes are one of the main components of Virtual Reality applications.","However, many network and computational resources are required to process 3D meshes in real-time.","A potential solution to this challenge is to dynamically adapt the Level of Detail (LoD) of a 3D mesh based on the object's position and the user's viewpoint.","In this paper, we conduct a subjective study to investigate users' quality perception of 3D meshes with dynamic Level of Detail in a Virtual Reality environment.","The subjective experiment is carried out with five 3D meshes of different characteristics, four Levels of Detail, and four distance settings.","The results of the experiment show that the impact of the dynamic level of detail depends on both the position of the 3D object in the virtual world and the number of vertices of the original mesh.","In addition, we present a quality model that can accurately predict the MOS score of a LoD version of a 3D mesh from the number of vertices and the distance from the viewpoint."],"url":"http://arxiv.org/abs/2406.06888v1","category":"cs.MM"}
{"created":"2024-06-11 01:43:50","title":"Multi-Objective Sizing Optimization Method of Microgrid Considering Cost and Carbon Emissions","abstract":"Microgrid serves as a promising solution to integrate and manage distributed renewable energy resources. In this paper, we establish a stochastic multi-objective sizing optimization (SMOSO) model for microgrid planning, which fully captures the battery degradation characteristics and the total carbon emissions. The microgrid operator aims to simultaneously maximize the economic benefits and minimize carbon emissions, and the degradation of the battery energy storage system (BESS) is modeled as a nonlinear function of power throughput. A self-adaptive multi-objective genetic algorithm (SAMOGA) is proposed to solve the SMOSO model, and this algorithm is enhanced by pre-grouped hierarchical selection and self-adaptive probabilities of crossover and mutation. Several case studies are conducted to determine the microgrid size by analyzing Pareto frontiers, and the simulation results validate that the proposed method has superior performance over other algorithms on the solution quality of optimum and diversity.","sentences":["Microgrid serves as a promising solution to integrate and manage distributed renewable energy resources.","In this paper, we establish a stochastic multi-objective sizing optimization (SMOSO) model for microgrid planning, which fully captures the battery degradation characteristics and the total carbon emissions.","The microgrid operator aims to simultaneously maximize the economic benefits and minimize carbon emissions, and the degradation of the battery energy storage system (BESS) is modeled as a nonlinear function of power throughput.","A self-adaptive multi-objective genetic algorithm (SAMOGA) is proposed to solve the SMOSO model, and this algorithm is enhanced by pre-grouped hierarchical selection and self-adaptive probabilities of crossover and mutation.","Several case studies are conducted to determine the microgrid size by analyzing Pareto frontiers, and the simulation results validate that the proposed method has superior performance over other algorithms on the solution quality of optimum and diversity."],"url":"http://arxiv.org/abs/2406.06880v1","category":"eess.SY"}
{"created":"2024-06-10 23:31:36","title":"Generalized W-Net: Arbitrary-style Chinese Character Synthesization","abstract":"Synthesizing Chinese characters with consistent style using few stylized examples is challenging. Existing models struggle to generate arbitrary style characters with limited examples. In this paper, we propose the Generalized W-Net, a novel class of W-shaped architectures that addresses this. By incorporating Adaptive Instance Normalization and introducing multi-content, our approach can synthesize Chinese characters in any desired style, even with limited examples. It handles seen and unseen styles during training and can generate new character contents. Experimental results demonstrate the effectiveness of our approach.","sentences":["Synthesizing Chinese characters with consistent style using few stylized examples is challenging.","Existing models struggle to generate arbitrary style characters with limited examples.","In this paper, we propose the Generalized W-Net, a novel class of W-shaped architectures that addresses this.","By incorporating Adaptive Instance Normalization and introducing multi-content, our approach can synthesize Chinese characters in any desired style, even with limited examples.","It handles seen and unseen styles during training and can generate new character contents.","Experimental results demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2406.06847v1","category":"cs.CV"}
{"created":"2024-06-10 22:07:57","title":"Adapters Strike Back","abstract":"Adapters provide an efficient and lightweight mechanism for adapting trained transformer models to a variety of different tasks. However, they have often been found to be outperformed by other adaptation mechanisms, including low-rank adaptation. In this paper, we provide an in-depth study of adapters, their internal structure, as well as various implementation choices. We uncover pitfalls for using adapters and suggest a concrete, improved adapter architecture, called Adapter+, that not only outperforms previous adapter implementations but surpasses a number of other, more complex adaptation mechanisms in several challenging settings. Despite this, our suggested adapter is highly robust and, unlike previous work, requires little to no manual intervention when addressing a novel scenario. Adapter+ reaches state-of-the-art average accuracy on the VTAB benchmark, even without a per-task hyperparameter optimization.","sentences":["Adapters provide an efficient and lightweight mechanism for adapting trained transformer models to a variety of different tasks.","However, they have often been found to be outperformed by other adaptation mechanisms, including low-rank adaptation.","In this paper, we provide an in-depth study of adapters, their internal structure, as well as various implementation choices.","We uncover pitfalls for using adapters and suggest a concrete, improved adapter architecture, called Adapter+, that not only outperforms previous adapter implementations but surpasses a number of other, more complex adaptation mechanisms in several challenging settings.","Despite this, our suggested adapter is highly robust and, unlike previous work, requires little to no manual intervention when addressing a novel scenario.","Adapter+ reaches state-of-the-art average accuracy on the VTAB benchmark, even without a per-task hyperparameter optimization."],"url":"http://arxiv.org/abs/2406.06820v1","category":"cs.CV"}
{"created":"2024-06-10 19:17:09","title":"Multi-Objective Neural Architecture Search for In-Memory Computing","abstract":"In this work, we employ neural architecture search (NAS) to enhance the efficiency of deploying diverse machine learning (ML) tasks on in-memory computing (IMC) architectures. Initially, we design three fundamental components inspired by the convolutional layers found in VGG and ResNet models. Subsequently, we utilize Bayesian optimization to construct a convolutional neural network (CNN) model with adaptable depths, employing these components. Through the Bayesian search algorithm, we explore a vast search space comprising over 640 million network configurations to identify the optimal solution, considering various multi-objective cost functions like accuracy/latency and accuracy/energy. Our evaluation of this NAS approach for IMC architecture deployment spans three distinct image classification datasets, demonstrating the effectiveness of our method in achieving a balanced solution characterized by high accuracy and reduced latency and energy consumption.","sentences":["In this work, we employ neural architecture search (NAS) to enhance the efficiency of deploying diverse machine learning (ML) tasks on in-memory computing (IMC) architectures.","Initially, we design three fundamental components inspired by the convolutional layers found in VGG and ResNet models.","Subsequently, we utilize Bayesian optimization to construct a convolutional neural network (CNN) model with adaptable depths, employing these components.","Through the Bayesian search algorithm, we explore a vast search space comprising over 640 million network configurations to identify the optimal solution, considering various multi-objective cost functions like accuracy/latency and accuracy/energy.","Our evaluation of this NAS approach for IMC architecture deployment spans three distinct image classification datasets, demonstrating the effectiveness of our method in achieving a balanced solution characterized by high accuracy and reduced latency and energy consumption."],"url":"http://arxiv.org/abs/2406.06746v1","category":"cs.LG"}
{"created":"2024-06-10 18:00:01","title":"Systematic Collapse of the Accretion Disc Across the Supermassive Black Hole Population","abstract":"The structure of the accretion flow onto supermassive black holes (SMBH) is not well understood. Standard disc models match to zeroth order in predicting substantial energy dissipation within optically-thick material producing a characteristic strong blue/UV continuum. However they fail at reproducing more detailed comparisons to the observed spectral shapes along with their observed variability. Based on stellar mass black holes within our galaxy, accretion discs should undergo a transition into an X-ray hot, radiatively inefficient flow, below a (mass scaled) luminosity of $\\sim 0.02\\,L_{\\rm{Edd}}$. While this has been seen in limited samples of nearby low-luminosity active galactic nuclei (AGN) and a few rare changing-look AGN, it is not at all clear whether this transition is present in the wider AGN population across cosmic time. A key issue is the difficulty in disentangling a change in spectral state from increased dust obscuration and/or host galaxy contamination, effectively drowning out the AGN emission. Here we use the new eROSITA eFEDS Survey to identify unobscured AGN from their X-ray emission, matched to excellent optical imaging from Subaru's Hyper Suprime-Cam; allowing the subtraction of the host galaxy contamination. The resulting, uncontaminated, AGN spectra reveal a smooth transition from a strongly disc dominated state in bright AGN, to the collapse of the disc into an inefficient X-ray plasma in the low luminosity AGN, with the transition occurring at $\\sim 0.02\\,L_{\\rm{Edd}}$; revealing fundamental aspects of accretion physics in AGN.","sentences":["The structure of the accretion flow onto supermassive black holes (SMBH) is not well understood.","Standard disc models match to zeroth order in predicting substantial energy dissipation within optically-thick material producing a characteristic strong blue/UV continuum.","However they fail at reproducing more detailed comparisons to the observed spectral shapes along with their observed variability.","Based on stellar mass black holes within our galaxy, accretion discs should undergo a transition into an X-ray hot, radiatively inefficient flow, below a (mass scaled) luminosity of $\\sim 0.02\\,L_{\\rm{Edd}}$.","While this has been seen in limited samples of nearby low-luminosity active galactic nuclei (AGN) and a few rare changing-look AGN, it is not at all clear whether this transition is present in the wider AGN population across cosmic time.","A key issue is the difficulty in disentangling a change in spectral state from increased dust obscuration and/or host galaxy contamination, effectively drowning out the AGN emission.","Here we use the new eROSITA eFEDS","Survey to identify unobscured AGN from their X-ray emission, matched to excellent optical imaging from Subaru's Hyper Suprime-Cam; allowing the subtraction of the host galaxy contamination.","The resulting, uncontaminated, AGN spectra reveal a smooth transition from a strongly disc dominated state in bright AGN, to the collapse of the disc into an inefficient X-ray plasma in the low luminosity AGN, with the transition occurring at $\\sim 0.02\\,L_{\\rm{Edd}}$; revealing fundamental aspects of accretion physics in AGN."],"url":"http://arxiv.org/abs/2406.06674v1","category":"astro-ph.HE"}
{"created":"2024-06-10 16:01:05","title":"Enrolment-based personalisation for improving individual-level fairness in speech emotion recognition","abstract":"The expression of emotion is highly individualistic. However, contemporary speech emotion recognition (SER) systems typically rely on population-level models that adopt a `one-size-fits-all' approach for predicting emotion. Moreover, standard evaluation practices measure performance also on the population level, thus failing to characterise how models work across different speakers. In the present contribution, we present a new method for capitalising on individual differences to adapt an SER model to each new speaker using a minimal set of enrolment utterances. In addition, we present novel evaluation schemes for measuring fairness across different speakers. Our findings show that aggregated evaluation metrics may obfuscate fairness issues on the individual-level, which are uncovered by our evaluation, and that our proposed method can improve performance both in aggregated and disaggregated terms.","sentences":["The expression of emotion is highly individualistic.","However, contemporary speech emotion recognition (SER) systems typically rely on population-level models that adopt a `one-size-fits-all' approach for predicting emotion.","Moreover, standard evaluation practices measure performance also on the population level, thus failing to characterise how models work across different speakers.","In the present contribution, we present a new method for capitalising on individual differences to adapt an SER model to each new speaker using a minimal set of enrolment utterances.","In addition, we present novel evaluation schemes for measuring fairness across different speakers.","Our findings show that aggregated evaluation metrics may obfuscate fairness issues on the individual-level, which are uncovered by our evaluation, and that our proposed method can improve performance both in aggregated and disaggregated terms."],"url":"http://arxiv.org/abs/2406.06665v1","category":"cs.CL"}
{"created":"2024-06-10 11:23:30","title":"Link Prediction in Bipartite Networks","abstract":"Bipartite networks serve as highly suitable models to represent systems involving interactions between two distinct types of entities, such as online dating platforms, job search services, or ecommerce websites. These models can be leveraged to tackle a number of tasks, including link prediction among the most useful ones, especially to design recommendation systems. However, if this task has garnered much interest when conducted on unipartite (i.e. standard) networks, it is far from being the case for bipartite ones. In this study, we address this gap by performing an experimental comparison of 19 link prediction methods able to handle bipartite graphs. Some come directly from the literature, and some are adapted by us from techniques originally designed for unipartite networks. We also propose to repurpose recommendation systems based on graph convolutional networks (GCN) as a novel link prediction solution for bipartite networks. To conduct our experiments, we constitute a benchmark of 3 real-world bipartite network datasets with various topologies. Our results indicate that GCN-based personalized recommendation systems, which have received significant attention in recent years, can produce successful results for link prediction in bipartite networks. Furthermore, purely heuristic metrics that do not rely on any learning process, like the Structural Perturbation Method (SPM), can also achieve success.","sentences":["Bipartite networks serve as highly suitable models to represent systems involving interactions between two distinct types of entities, such as online dating platforms, job search services, or ecommerce websites.","These models can be leveraged to tackle a number of tasks, including link prediction among the most useful ones, especially to design recommendation systems.","However, if this task has garnered much interest when conducted on unipartite (i.e. standard) networks, it is far from being the case for bipartite ones.","In this study, we address this gap by performing an experimental comparison of 19 link prediction methods able to handle bipartite graphs.","Some come directly from the literature, and some are adapted by us from techniques originally designed for unipartite networks.","We also propose to repurpose recommendation systems based on graph convolutional networks (GCN) as a novel link prediction solution for bipartite networks.","To conduct our experiments, we constitute a benchmark of 3 real-world bipartite network datasets with various topologies.","Our results indicate that GCN-based personalized recommendation systems, which have received significant attention in recent years, can produce successful results for link prediction in bipartite networks.","Furthermore, purely heuristic metrics that do not rely on any learning process, like the Structural Perturbation Method (SPM), can also achieve success."],"url":"http://arxiv.org/abs/2406.06658v1","category":"cs.SI"}
{"created":"2024-06-10 09:09:08","title":"DKDL-Net: A Lightweight Bearing Fault Detection Model via Decoupled Knowledge Distillation and Low-Rank Adaptation Fine-tuning","abstract":"Rolling bearing fault detection has developed rapidly in the field of fault diagnosis technology, and it occupies a very important position in this field. Deep learning-based bearing fault diagnosis models have achieved significant success. At the same time, with the continuous improvement of new signal processing technologies such as Fourier transform, wavelet transform and empirical mode decomposition, the fault diagnosis technology of rolling bearings has also been greatly developed, and it can be said that it has entered a new research stage. However, most of the existing methods are limited to varying degrees in the industrial field. The main ones are fast feature extraction and computational complexity. The key to this paper is to propose a lightweight bearing fault diagnosis model DKDL-Net to solve these challenges. The model is trained on the CWRU data set by decoupling knowledge distillation and low rank adaptive fine tuning. Specifically, we built and trained a teacher model based on a 6-layer neural network with 69,626 trainable parameters, and on this basis, using decoupling knowledge distillation (DKD) and Low-Rank adaptive (LoRA) fine-tuning, we trained the student sag model DKDL-Net, which has only 6838 parameters. Experiments show that DKDL-Net achieves 99.48\\% accuracy in computational complexity on the test set while maintaining model performance, which is 0.58\\% higher than the state-of-the-art (SOTA) model, and our model has lower parameters. Our code is available at Github link: https://github.com/SPBU-LiPengyi/DKDL-Net.git.","sentences":["Rolling bearing fault detection has developed rapidly in the field of fault diagnosis technology, and it occupies a very important position in this field.","Deep learning-based bearing fault diagnosis models have achieved significant success.","At the same time, with the continuous improvement of new signal processing technologies such as Fourier transform, wavelet transform and empirical mode decomposition, the fault diagnosis technology of rolling bearings has also been greatly developed, and it can be said that it has entered a new research stage.","However, most of the existing methods are limited to varying degrees in the industrial field.","The main ones are fast feature extraction and computational complexity.","The key to this paper is to propose a lightweight bearing fault diagnosis model DKDL-Net to solve these challenges.","The model is trained on the CWRU data set by decoupling knowledge distillation and low rank adaptive fine tuning.","Specifically, we built and trained a teacher model based on a 6-layer neural network with 69,626 trainable parameters, and on this basis, using decoupling knowledge distillation (DKD) and Low-Rank adaptive (LoRA) fine-tuning, we trained the student sag model DKDL-Net, which has only 6838 parameters.","Experiments show that DKDL-Net achieves 99.48\\% accuracy in computational complexity on the test set while maintaining model performance, which is 0.58\\% higher than the state-of-the-art (SOTA) model, and our model has lower parameters.","Our code is available at Github link: https://github.com/SPBU-LiPengyi/DKDL-Net.git."],"url":"http://arxiv.org/abs/2406.06653v1","category":"cs.LG"}
{"created":"2024-06-09 23:39:31","title":"Latent Diffusion Model-Enabled Real-Time Semantic Communication Considering Semantic Ambiguities and Channel Noises","abstract":"Semantic communication (SemCom) has emerged as a new paradigm for communication systems, with deep learning (DL) models being one of the key drives to shift from the accuracy of bit/symbol to the semantics and pragmatics of data. Nevertheless, DL-based SemCom systems often face performance bottlenecks due to overfitting, poor generalization, and sensitivity to outliers. Furthermore, the varying-fading gains and noises with uncertain signal-to-noise ratios (SNRs) commonly present in wireless channels usually restrict the accuracy of semantic information transmission. Consequently, to address the aforementioned issues, this paper constructs a SemCom system based on the latent diffusion model, and proposes three improvements compared to existing works: i) To handle potential outliers in the source data, semantic errors obtained by projected gradient descent based on the vulnerabilities of DL models, are utilized to update the parameters and obtain an outlier-robust encoder. ii) A lightweight single-layer latent space transformation adapter completes one-shot learning at transmitter and is placed before the decoder at receiver, enabling adaptation for out-of-distribution data or enhancing human-perceptual quality. iii) An end-to-end consistency distillation (EECD) strategy is used to distill the diffusion models trained in latent space, enabling deterministic single or few-step real-time denoising in various noisy channels while maintaining high semantic quality. Extensive numerical experiments across different datasets demonstrate the superiority of the proposed SemCom system, consistently proving its robustness to outliers, the capability to transmit data with unknown distributions, and the ability to perform real-time channel denoising tasks while preserving high human perceptual quality, outperforming the existing denoising approaches in semantic metrics such as MS-SSIM and LPIPS.","sentences":["Semantic communication (SemCom) has emerged as a new paradigm for communication systems, with deep learning (DL) models being one of the key drives to shift from the accuracy of bit/symbol to the semantics and pragmatics of data.","Nevertheless, DL-based SemCom systems often face performance bottlenecks due to overfitting, poor generalization, and sensitivity to outliers.","Furthermore, the varying-fading gains and noises with uncertain signal-to-noise ratios (SNRs) commonly present in wireless channels usually restrict the accuracy of semantic information transmission.","Consequently, to address the aforementioned issues, this paper constructs a SemCom system based on the latent diffusion model, and proposes three improvements compared to existing works: i) To handle potential outliers in the source data, semantic errors obtained by projected gradient descent based on the vulnerabilities of DL models, are utilized to update the parameters and obtain an outlier-robust encoder.","ii)","A lightweight single-layer latent space transformation adapter completes one-shot learning at transmitter and is placed before the decoder at receiver, enabling adaptation for out-of-distribution data or enhancing human-perceptual quality.","iii)","An end-to-end consistency distillation (EECD) strategy is used to distill the diffusion models trained in latent space, enabling deterministic single or few-step real-time denoising in various noisy channels while maintaining high semantic quality.","Extensive numerical experiments across different datasets demonstrate the superiority of the proposed SemCom system, consistently proving its robustness to outliers, the capability to transmit data with unknown distributions, and the ability to perform real-time channel denoising tasks while preserving high human perceptual quality, outperforming the existing denoising approaches in semantic metrics such as MS-SSIM and LPIPS."],"url":"http://arxiv.org/abs/2406.06644v1","category":"cs.LG"}
{"created":"2024-06-09 18:31:19","title":"TopoBenchmarkX: A Framework for Benchmarking Topological Deep Learning","abstract":"This work introduces TopoBenchmarkX, a modular open-source library designed to standardize benchmarking and accelerate research in Topological Deep Learning (TDL). TopoBenchmarkX maps the TDL pipeline into a sequence of independent and modular components for data loading and processing, as well as model training, optimization, and evaluation. This modular organization provides flexibility for modifications and facilitates the adaptation and optimization of various TDL pipelines. A key feature of TopoBenchmarkX is that it allows for the transformation and lifting between topological domains. This enables, for example, to obtain richer data representations and more fine-grained analyses by mapping the topology and features of a graph to higher-order topological domains such as simplicial and cell complexes. The range of applicability of TopoBenchmarkX is demonstrated by benchmarking several TDL architectures for various tasks and datasets.","sentences":["This work introduces TopoBenchmarkX, a modular open-source library designed to standardize benchmarking and accelerate research in Topological Deep Learning (TDL).","TopoBenchmarkX maps the TDL pipeline into a sequence of independent and modular components for data loading and processing, as well as model training, optimization, and evaluation.","This modular organization provides flexibility for modifications and facilitates the adaptation and optimization of various TDL pipelines.","A key feature of TopoBenchmarkX is that it allows for the transformation and lifting between topological domains.","This enables, for example, to obtain richer data representations and more fine-grained analyses by mapping the topology and features of a graph to higher-order topological domains such as simplicial and cell complexes.","The range of applicability of TopoBenchmarkX is demonstrated by benchmarking several TDL architectures for various tasks and datasets."],"url":"http://arxiv.org/abs/2406.06642v1","category":"cs.LG"}
{"created":"2024-06-11 17:57:32","title":"Dynamics of the non-radial energy-critical inhomogeneous NLS","abstract":"We consider the focusing inhomogeneous nonlinear Schr\\\"odinger equation \\[ i\\partial_t u + \\Delta u + |x|^{-b}|u|^\\alpha u = 0\\qtq{on}\\R\\times\\R^N, \\] with $\\alpha=\\tfrac{4-2b}{N-2}$, $N=\\{3,4,5\\}$ and $0<b\\leq \\min\\Big\\{\\tfrac{6-N}{2},\\tfrac{4}{N}$\\Big\\}. This paper establishes global well-posedness and scattering for the non-radial energy-critical case in $\\dot{H}^1(\\R^N)$. It extends the previous research by Murphy and the first author \\cite{GM}, which focused on the case $(N,\\alpha,b)=(3,2,1)$. The novelty here, beyond considering higher dimensions, lies in our assumption of the condition $\\sup_{t\\in I}\\|\\nabla u(t)\\|_{L^2}<\\|\\nabla Q\\|_{L^2}$, which is weaker than the condition stated in \\cite{Guzman}. Consequently, if a solution has energy and kinetic energy less than the ground state $Q$ at some point, then the solution is global and scatters. Moreover, we show scattering for the defocusing case. On the other hand, in this work, we also investigate the blow-up issue with nonradial data for $N\\geq 3$ in $H^1(\\mathbb{R}^N)$. This implies that our result holds without classical assumptions such as spherically symmetric data or $|x|u_0 \\in L^2(\\mathbb{R}^N)$.   \\   \\noindent Mathematics Subject Classification. 35A01, 35QA55, 35P25.","sentences":["We consider the focusing inhomogeneous nonlinear Schr\\\"odinger equation \\","[ i\\partial_t u + \\Delta u + |x|^{-b}|u|^\\alpha u = 0\\qtq{on}\\R\\times\\R^N, \\] with $\\alpha=\\tfrac{4-2b}{N-2}$, $N=\\{3,4,5\\}$ and $0<b\\leq \\min\\Big\\{\\tfrac{6-N}{2},\\tfrac{4}{N}$\\Big\\}.","This paper establishes global well-posedness and scattering for the non-radial energy-critical case in $\\dot{H}^1(\\R^N)$. It extends the previous research by Murphy and the first author \\cite{GM}, which focused on the case $(N,\\alpha,b)=(3,2,1)$. The novelty here, beyond considering higher dimensions, lies in our assumption of the condition $\\sup_{t\\in I}\\|\\nabla u(t)\\|_{L^2}<\\|\\nabla Q\\|_{L^2}$, which is weaker than the condition stated in \\cite{Guzman}.","Consequently, if a solution has energy and kinetic energy less than the ground state $Q$ at some point, then the solution is global and scatters.","Moreover, we show scattering for the defocusing case.","On the other hand, in this work, we also investigate the blow-up issue with nonradial data for $N\\geq 3$ in $H^1(\\mathbb{R}^N)$. This implies that our result holds without classical assumptions such as spherically symmetric data or $|x|u_0 \\in L^2(\\mathbb{R}^N)$.   \\   \\noindent Mathematics Subject Classification.","35A01, 35QA55, 35P25."],"url":"http://arxiv.org/abs/2406.07535v1","category":"math.AP"}
{"created":"2024-06-11 17:56:31","title":"Interpreting DESI 2024 BAO: late-time dynamical dark energy or a local effect?","abstract":"We perform fits to DESI, CMB and supernova data to understand the physical origin of the DESI hint for dynamical dark energy. We find that the linear parametrization of the equation of state $w$ may guide to misleading interpretations, such as the hint for a phantom Universe, which are not preferred by the data. Instead, physical quintessence models fit the data well. Model-independently, present observations prefer deviations from the constant dark energy, $w=-1$, only at very low redshifts, $z < \\mathcal{O}(0.1)$. We find that this result is driven by low-$z$ supernova data. Therefore, either the fundamental properties of our Universe, characterised by the equation of state $w$ and the Hubble parameter $H$, underwent dramatic changes very recently or, alternatively, we do not fully understand the systematics of our local Universe in a radius of about $300\\,h^{-1}\\rm Mpc$.","sentences":["We perform fits to DESI, CMB and supernova data to understand the physical origin of the DESI hint for dynamical dark energy.","We find that the linear parametrization of the equation of state $w$ may guide to misleading interpretations, such as the hint for a phantom Universe, which are not preferred by the data.","Instead, physical quintessence models fit the data well.","Model-independently, present observations prefer deviations from the constant dark energy, $w=-1$, only at very low redshifts, $z <","\\mathcal{O}(0.1)$. We find that this result is driven by low-$z$ supernova data.","Therefore, either the fundamental properties of our Universe, characterised by the equation of state $w$ and the Hubble parameter $H$, underwent dramatic changes very recently or, alternatively, we do not fully understand the systematics of our local Universe in a radius of about $300\\,h^{-1}\\rm Mpc$."],"url":"http://arxiv.org/abs/2406.07533v1","category":"astro-ph.CO"}
{"created":"2024-06-11 17:43:18","title":"Uniqueness on average of large isoperimetric sets in noncompact manifolds with nonnegative Ricci curvature","abstract":"Let $(M^n,g)$ be a complete Riemannian manifold which is not isometric to $\\mathbb{R}^n$, has nonnegative Ricci curvature, Euclidean volume growth, and quadratic Riemann curvature decay. We prove that there is a set $\\mathcal{G}\\subset (0,\\infty)$ with density $1$ at infinity such that for every $V\\in \\mathcal{G}$ there is a unique isoperimetric set of volume $V$ in $M$, and its boundary is strictly volume preserving stable.   The latter result cannot be improved to uniqueness or strict stability for every large volume. Indeed, we construct a complete Riemannian surface that satisfies the previous assumptions, and with the following property: there are arbitrarily large and diverging intervals $I_n\\subset (0,\\infty)$ such that isoperimetric sets with volumes $V\\in I_n$ exist, but they are neither unique nor they have strictly volume preserving stable boundaries.   The proof relies on a set of new ideas, as the present setting goes beyond the range of applicability of the methods based on the implicit function theorem, and no symmetry is assumed.","sentences":["Let $(M^n,g)$ be a complete Riemannian manifold which is not isometric to $\\mathbb{R}^n$, has nonnegative Ricci curvature, Euclidean volume growth, and quadratic Riemann curvature decay.","We prove that there is a set $\\mathcal{G}\\subset (0,\\infty)$ with density $1$ at infinity such that for every $V\\in \\mathcal{G}$ there is a unique isoperimetric set of volume $V$ in $M$, and its boundary is strictly volume preserving stable.   ","The latter result cannot be improved to uniqueness or strict stability for every large volume.","Indeed, we construct a complete Riemannian surface that satisfies the previous assumptions, and with the following property: there are arbitrarily large and diverging intervals $I_n\\subset (0,\\infty)$ such that isoperimetric sets with volumes $V\\in I_n$ exist, but they are neither unique nor they have strictly volume preserving stable boundaries.   ","The proof relies on a set of new ideas, as the present setting goes beyond the range of applicability of the methods based on the implicit function theorem, and no symmetry is assumed."],"url":"http://arxiv.org/abs/2406.07509v1","category":"math.DG"}
{"created":"2024-06-11 17:41:31","title":"$(J/\u03c8, J/\u03c8)$, and $(\u03b7_c, \u03b7_c)$ production through two intermediate photons in electron-positron annihilation at B-factories","abstract":"We study the processes, $e^- e^+ \\rightarrow \\gamma^* \\gamma^* \\rightarrow J/\\psi +J/\\psi$, and $e^- e^+ \\rightarrow \\gamma^* \\gamma^* \\rightarrow \\eta_c+ \\eta_c$ at $\\sqrt{s}=10.6$ GeV in the framework of $4\\times 4$ Bethe-Salpeter equation. For $J/\\psi+J/\\psi$ production, the dominant contribution is through fragmentation process, while for $\\eta_c+\\eta_c$ production, the quark rearrangement diagrams contribute. Our results of cross section for $J/\\psi+J/\\psi$ and $\\psi(2S)+\\psi(2S)$ are compatible with the experimental upper limits set by Belle Collaboration, while in the absence of experimental data for $\\eta_c(1S)+\\eta_c(1S)$, and $\\eta_c(2S)+\\eta_c(2S)$ production, we have given theoretical prediction of their cross sections, and compared with NRQCD prediction.","sentences":["We study the processes, $e^- e^+ \\rightarrow \\gamma^* \\gamma^*","\\rightarrow J/\\psi +J/\\psi$, and $e^- e^+ \\rightarrow \\gamma^*","\\gamma^*","\\rightarrow \\eta_c+ \\eta_c$ at $\\sqrt{s}=10.6$ GeV in the framework of $4\\times 4$ Bethe-Salpeter equation.","For $J/\\psi+J/\\psi$ production, the dominant contribution is through fragmentation process, while for $\\eta_c+\\eta_c$ production, the quark rearrangement diagrams contribute.","Our results of cross section for $J/\\psi+J/\\psi$ and $\\psi(2S)+\\psi(2S)$ are compatible with the experimental upper limits set by Belle Collaboration, while in the absence of experimental data for $\\eta_c(1S)+\\eta_c(1S)$, and $\\eta_c(2S)+\\eta_c(2S)$ production, we have given theoretical prediction of their cross sections, and compared with NRQCD prediction."],"url":"http://arxiv.org/abs/2406.07508v1","category":"hep-ph"}
{"created":"2024-06-11 17:16:58","title":"Exploring non-radial oscillation modes in dark matter admixed neutron stars","abstract":"Because of their extreme densities and consequently, gravitational potential, compact objects such as neutron stars can prove to be excellent captors of dark matter particles. Considering purely gravitational interactions between dark and hadronic matter, we construct dark matter admixed stars composed of two-fluid matter subject to current astrophysical constraints of maximum mass and tidal deformability. We choose a wide range of parameters to construct the dark matter equation of state, and the DDME2 parameterization for the hadronic equation of state. We then examine the effect of dark matter on the stellar structure, tidal deformability and non-radial modes considering the relativistic Cowling approximation. We find the effect on $p$-modes is substantial, with frequencies decreasing up to the typical $f-$mode frequency range for most stars with a dark matter halo. The effects on the $f-$mode frequency are less extreme. Finally, we find the most probable and $1\\sigma$ values of the dark matter parameters used in this study.","sentences":["Because of their extreme densities and consequently, gravitational potential, compact objects such as neutron stars can prove to be excellent captors of dark matter particles.","Considering purely gravitational interactions between dark and hadronic matter, we construct dark matter admixed stars composed of two-fluid matter subject to current astrophysical constraints of maximum mass and tidal deformability.","We choose a wide range of parameters to construct the dark matter equation of state, and the DDME2 parameterization for the hadronic equation of state.","We then examine the effect of dark matter on the stellar structure, tidal deformability and non-radial modes considering the relativistic Cowling approximation.","We find the effect on $p$-modes is substantial, with frequencies decreasing up to the typical $f-$mode frequency range for most stars with a dark matter halo.","The effects on the $f-$mode frequency are less extreme.","Finally, we find the most probable and $1\\sigma$ values of the dark matter parameters used in this study."],"url":"http://arxiv.org/abs/2406.07470v1","category":"astro-ph.HE"}
{"created":"2024-06-11 17:13:18","title":"On functions of low differential uniformity in characteristic 2: A close look (I)","abstract":"We introduce a new concept, the APN-defect, which can be thought of as measuring the distance of a given function $G:\\mathbb{F}_{2^n} \\rightarrow \\mathbb{F}_{2^n}$ to the set of almost perfect nonlinear (APN) functions. This concept is motivated by the detailed analysis of the differential behaviour of non-APN functions (of low differential uniformity) $G$ using the so-called difference squares. We describe the relations between the APN-defect and other recent concepts of similar nature. Upper and lower bounds for the values of APN-defect for several classes of functions of interest, including Dembowski-Ostrom polynomials are given. Its exact values in some cases are also calculated. The difference square corresponding to a modification of the inverse function is determined, its APN-defect depending on $n$ is evaluated and the implications are discussed.   In the forthcoming second part of this work we further examine modifications of the inverse function. We also study modifications of classes of functions of low uniformity over infinitely many extensions of $\\mathbb{F}_{2^n}$. We present quantitative results on their differential behaviour, especially in connection with their APN-defects.","sentences":["We introduce a new concept, the APN-defect, which can be thought of as measuring the distance of a given function $G:\\mathbb{F}_{2^n} \\rightarrow \\mathbb{F}_{2^n}$ to the set of almost perfect nonlinear (APN) functions.","This concept is motivated by the detailed analysis of the differential behaviour of non-APN functions (of low differential uniformity) $G$ using the so-called difference squares.","We describe the relations between the APN-defect and other recent concepts of similar nature.","Upper and lower bounds for the values of APN-defect for several classes of functions of interest, including Dembowski-Ostrom polynomials are given.","Its exact values in some cases are also calculated.","The difference square corresponding to a modification of the inverse function is determined, its APN-defect depending on $n$ is evaluated and the implications are discussed.   ","In the forthcoming second part of this work we further examine modifications of the inverse function.","We also study modifications of classes of functions of low uniformity over infinitely many extensions of $\\mathbb{F}_{2^n}$. We present quantitative results on their differential behaviour, especially in connection with their APN-defects."],"url":"http://arxiv.org/abs/2406.07468v1","category":"cs.IT"}
{"created":"2024-06-11 16:49:00","title":"GPU Accelerated Implicit Kinetic Meshfree Method based on Modified LU-SGS","abstract":"This report presents the GPU acceleration of implicit kinetic meshfree methods using modified LU-SGS algorithms. The meshfree scheme is based on the least squares kinetic upwind method (LSKUM). In the existing matrix-free LU-SGS approaches for kinetic meshfree methods, the products of split flux Jacobians and increments in conserved vectors are approximated by increments in the split fluxes. In our modified LU-SGS approach, the Jacobian vector products are computed exactly using algorithmic differentiation (AD). The implicit GPU solvers with exact and approximate computation of the Jacobian vector products are applied to the standard test cases for two-dimensional inviscid flows. Numerical results have shown that the GPU solvers with the exact computation of the Jacobian vector products are computationally more efficient and yield better convergence rates than the solvers with approximations to the Jacobian vector products. Benchmarks are presented to assess the performance of implicit GPU solvers compared to the explicit GPU solver and the implicit serial LSKUM solver.","sentences":["This report presents the GPU acceleration of implicit kinetic meshfree methods using modified LU-SGS algorithms.","The meshfree scheme is based on the least squares kinetic upwind method (LSKUM).","In the existing matrix-free LU-SGS approaches for kinetic meshfree methods, the products of split flux Jacobians and increments in conserved vectors are approximated by increments in the split fluxes.","In our modified LU-SGS approach, the Jacobian vector products are computed exactly using algorithmic differentiation (AD).","The implicit GPU solvers with exact and approximate computation of the Jacobian vector products are applied to the standard test cases for two-dimensional inviscid flows.","Numerical results have shown that the GPU solvers with the exact computation of the Jacobian vector products are computationally more efficient and yield better convergence rates than the solvers with approximations to the Jacobian vector products.","Benchmarks are presented to assess the performance of implicit GPU solvers compared to the explicit GPU solver and the implicit serial LSKUM solver."],"url":"http://arxiv.org/abs/2406.07441v1","category":"cs.DC"}
{"created":"2024-06-11 16:45:48","title":"DeformTime: Capturing Variable Dependencies with Deformable Attention for Time Series Forecasting","abstract":"In multivariate time series (MTS) forecasting, existing state-of-the-art deep learning approaches tend to focus on autoregressive formulations and overlook the information within exogenous indicators. To address this limitation, we present DeformTime, a neural network architecture that attempts to capture correlated temporal patterns from the input space, and hence, improve forecasting accuracy. It deploys two core operations performed by deformable attention blocks (DABs): learning dependencies across variables from different time steps (variable DAB), and preserving temporal dependencies in data from previous time steps (temporal DAB). Input data transformation is explicitly designed to enhance learning from the deformed series of information while passing through a DAB. We conduct extensive experiments on 6 MTS data sets, using previously established benchmarks as well as challenging infectious disease modelling tasks with more exogenous variables. The results demonstrate that DeformTime improves accuracy against previous competitive methods across the vast majority of MTS forecasting tasks, reducing the mean absolute error by 10% on average. Notably, performance gains remain consistent across longer forecasting horizons.","sentences":["In multivariate time series (MTS) forecasting, existing state-of-the-art deep learning approaches tend to focus on autoregressive formulations and overlook the information within exogenous indicators.","To address this limitation, we present DeformTime, a neural network architecture that attempts to capture correlated temporal patterns from the input space, and hence, improve forecasting accuracy.","It deploys two core operations performed by deformable attention blocks (DABs): learning dependencies across variables from different time steps (variable DAB), and preserving temporal dependencies in data from previous time steps (temporal DAB).","Input data transformation is explicitly designed to enhance learning from the deformed series of information while passing through a DAB.","We conduct extensive experiments on 6 MTS data sets, using previously established benchmarks as well as challenging infectious disease modelling tasks with more exogenous variables.","The results demonstrate that DeformTime improves accuracy against previous competitive methods across the vast majority of MTS forecasting tasks, reducing the mean absolute error by 10% on average.","Notably, performance gains remain consistent across longer forecasting horizons."],"url":"http://arxiv.org/abs/2406.07438v1","category":"cs.LG"}
{"created":"2024-06-11 16:11:32","title":"Entropy, slicing problem and functional Mahler's conjecture","abstract":"In a recent work, Bo'az Klartag showed that, given a convex body with minimal volume product, its isotropic constant is related to its volume product. As a consequence, he obtained that a strong version of the slicing conjecture implies Mahler's conjecture. In this work, we extend these geometrical results to the realm of log-concave functions. In this regard, the functional analogues of the projective perturbations of the body are the log-Laplace perturbations of the function. The differentiation along these transformations is simplified thanks to the known properties of the log-Laplace transform. Moreover, we show that achieving such an analogous result requires the consideration of the suitable version of the isotropic constant, notably the one incorporating the entropy. Finally, an investigation into the equivalences between the functional and geometrical strong forms of the slicing conjecture is provided.","sentences":["In a recent work, Bo'az Klartag showed that, given a convex body with minimal volume product, its isotropic constant is related to its volume product.","As a consequence, he obtained that a strong version of the slicing conjecture implies Mahler's conjecture.","In this work, we extend these geometrical results to the realm of log-concave functions.","In this regard, the functional analogues of the projective perturbations of the body are the log-Laplace perturbations of the function.","The differentiation along these transformations is simplified thanks to the known properties of the log-Laplace transform.","Moreover, we show that achieving such an analogous result requires the consideration of the suitable version of the isotropic constant, notably the one incorporating the entropy.","Finally, an investigation into the equivalences between the functional and geometrical strong forms of the slicing conjecture is provided."],"url":"http://arxiv.org/abs/2406.07406v1","category":"math.MG"}
{"created":"2024-06-11 16:09:27","title":"Optimal Marital Strategies: How Couples Develop Successful Interaction Styles","abstract":"The study of marriage dynamics and of strategies to reduce the likelihood of divorce has been an important research area for many years. Gottman's research on successful marriages revealed three matched interaction styles: conflict-avoiding, validating, and volatile. There has, however, been little progress in explaining how couples develop these styles of interaction and why failure to do so leads to failed marriages. In this paper, we show that these interaction styles arise as solutions to an optimal control problem where the couples jointly maximize a common goal. The validating style arises when the benefit from achieving joint happiness is balanced by the emotional cost of adopting a particular style. The ubiquitous conflict-avoider style arises naturally when the couple does not care about the cost. The volatile style is not an optimal solution, but volatile marriages may still be successful for couples with highly positive natural dispositions. The problem of the spouses having different goals in marriage is relevant to marriage repair, and this problem will be studied in the next paper using differential game theory.","sentences":["The study of marriage dynamics and of strategies to reduce the likelihood of divorce has been an important research area for many years.","Gottman's research on successful marriages revealed three matched interaction styles: conflict-avoiding, validating, and volatile.","There has, however, been little progress in explaining how couples develop these styles of interaction and why failure to do so leads to failed marriages.","In this paper, we show that these interaction styles arise as solutions to an optimal control problem where the couples jointly maximize a common goal.","The validating style arises when the benefit from achieving joint happiness is balanced by the emotional cost of adopting a particular style.","The ubiquitous conflict-avoider style arises naturally when the couple does not care about the cost.","The volatile style is not an optimal solution, but volatile marriages may still be successful for couples with highly positive natural dispositions.","The problem of the spouses having different goals in marriage is relevant to marriage repair, and this problem will be studied in the next paper using differential game theory."],"url":"http://arxiv.org/abs/2406.07403v1","category":"math.OC"}
{"created":"2024-06-11 16:03:25","title":"Impact of the nuclear equation of state on the formation of twin stars","abstract":"Twin stars-two stable neutron stars (NSs) with the same mass but different radii have long been proposed to appear as a consequence of a possible first-order phase transition in NS matter. Within a meta-model for the EOS of hybrid stars, we revisit the viability of twin stars and its dependence on numerous parameters characterizing the EOS of nuclear matter, quark matter, and the phase transition between them. While essentially no experimental constraint exists for the last two, parameters characterizing the EOS of neutron-rich nucleonic matter have been constrained within various ranges by terrestrial experiments and astrophysical observations. Within these ranges, the impact of nuclear EOS on the formation of twin stars is studied. It is found that the symmetry energy of neutron-rich nucleonic matter notably influences the formation of twin stars, particularly through its slope $L$ and curvature $K_{\\rm sym}$. Conversely, the EOS of symmetric nuclear matter shows minimal influence on the formation of twin stars, partially owing to its relatively well-constrained parameters.","sentences":["Twin stars-two stable neutron stars (NSs) with the same mass but different radii have long been proposed to appear as a consequence of a possible first-order phase transition in NS matter.","Within a meta-model for the EOS of hybrid stars, we revisit the viability of twin stars and its dependence on numerous parameters characterizing the EOS of nuclear matter, quark matter, and the phase transition between them.","While essentially no experimental constraint exists for the last two, parameters characterizing the EOS of neutron-rich nucleonic matter have been constrained within various ranges by terrestrial experiments and astrophysical observations.","Within these ranges, the impact of nuclear EOS on the formation of twin stars is studied.","It is found that the symmetry energy of neutron-rich nucleonic matter notably influences the formation of twin stars, particularly through its slope $L$ and curvature $K_{\\rm sym}$. Conversely, the EOS of symmetric nuclear matter shows minimal influence on the formation of twin stars, partially owing to its relatively well-constrained parameters."],"url":"http://arxiv.org/abs/2406.07396v1","category":"nucl-th"}
{"created":"2024-06-11 15:28:48","title":"Deep Implicit Optimization for Robust and Flexible Image Registration","abstract":"Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time. However, DLIR methods forego many of the benefits of classical optimization-based methods. The functional nature of deep networks do not guarantee that the predicted transformation is a local minima of the registration objective, the representation of the transformation (displacement/velocity field/affine) is fixed, and the networks are not robust to domain shift. Our method aims to bridge this gap between classical and learning methods by incorporating optimization as a layer in a deep network. A deep network is trained to predict multi-scale dense feature images that are registered using a black box iterative optimization solver. This optimal warp is then used to minimize image and label alignment errors. By implicitly differentiating end-to-end through an iterative optimization solver, our learned features are registration and label-aware, and the warp functions are guaranteed to be local minima of the registration objective in the feature space. Our framework shows excellent performance on in-domain datasets, and is agnostic to domain shift such as anisotropy and varying intensity profiles. For the first time, our method allows switching between arbitrary transformation representations (free-form to diffeomorphic) at test time with zero retraining. End-to-end feature learning also facilitates interpretability of features, and out-of-the-box promptability using additional label-fidelity terms at inference.","sentences":["Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time.","However, DLIR methods forego many of the benefits of classical optimization-based methods.","The functional nature of deep networks do not guarantee that the predicted transformation is a local minima of the registration objective, the representation of the transformation (displacement/velocity field/affine) is fixed, and the networks are not robust to domain shift.","Our method aims to bridge this gap between classical and learning methods by incorporating optimization as a layer in a deep network.","A deep network is trained to predict multi-scale dense feature images that are registered using a black box iterative optimization solver.","This optimal warp is then used to minimize image and label alignment errors.","By implicitly differentiating end-to-end through an iterative optimization solver, our learned features are registration and label-aware, and the warp functions are guaranteed to be local minima of the registration objective in the feature space.","Our framework shows excellent performance on in-domain datasets, and is agnostic to domain shift such as anisotropy and varying intensity profiles.","For the first time, our method allows switching between arbitrary transformation representations (free-form to diffeomorphic) at test time with zero retraining.","End-to-end feature learning also facilitates interpretability of features, and out-of-the-box promptability using additional label-fidelity terms at inference."],"url":"http://arxiv.org/abs/2406.07361v1","category":"cs.CV"}
{"created":"2024-06-11 15:07:40","title":"Analytical Delta-V Approximation for Nonlinear Programming of Multi-target Rendezvous and Flyby Trajectories","abstract":"This study proposes an analytical Delta-V approximation of short-time transfers based on the linear relative motion and a gradient-based nonlinear programming model of multi-target rendezvous and flyby trajectories. In previous studies, the Lambert's solution is commonly used to evaluate Delta-V of short-duration transfers. In this study, to avoid the iteration process for obtaining the Lambert's solution and its gradient, the linear relative motion equations are applied to form an analytical two-point boundary value model for the near-circular orbit rendezvous problems. Although the relative motion equations are usually applicable when the two orbits are close enough, and the position and velocity errors would become more significant as the orbital differences increase, the errors of the velocity increments were proved acceptable in our simulations. Moreover, the analytical formula facilitates the calculation of the gradients to the start epoch and flight time, which are used to establish a nonlinear programming model for sequence optimization that gradient-based algorithms can easily solve. Simulation results demonstrated that the analytical Delta-V approximation requires much less calculation than the Lambert's solution, and the proposed gradient-based nonlinear programming algorithms can obtain similar results in less time than previous methods.","sentences":["This study proposes an analytical Delta-V approximation of short-time transfers based on the linear relative motion and a gradient-based nonlinear programming model of multi-target rendezvous and flyby trajectories.","In previous studies, the Lambert's solution is commonly used to evaluate Delta-V of short-duration transfers.","In this study, to avoid the iteration process for obtaining the Lambert's solution and its gradient, the linear relative motion equations are applied to form an analytical two-point boundary value model for the near-circular orbit rendezvous problems.","Although the relative motion equations are usually applicable when the two orbits are close enough, and the position and velocity errors would become more significant as the orbital differences increase, the errors of the velocity increments were proved acceptable in our simulations.","Moreover, the analytical formula facilitates the calculation of the gradients to the start epoch and flight time, which are used to establish a nonlinear programming model for sequence optimization that gradient-based algorithms can easily solve.","Simulation results demonstrated that the analytical Delta-V approximation requires much less calculation than the Lambert's solution, and the proposed gradient-based nonlinear programming algorithms can obtain similar results in less time than previous methods."],"url":"http://arxiv.org/abs/2406.07341v1","category":"astro-ph.EP"}
{"created":"2024-06-11 14:47:30","title":"Morse Index Stability for the Ginzburg-Landau Approximation","abstract":"In this paper we study the behaviour of critical points of the Ginzburg-Landau perturbation of the Dirichlet energy into the sphere $E_\\varepsilon(u):=\\int_\\Sigma \\frac{1}{2}|du|^2_h\\ \\,dvol_h +\\frac{1}{4\\varepsilon^2}(1-|u|^2)^2\\,dvol_h=\\int_{\\Sigma}e_{\\varepsilon}(u)$. Our first main result is a precise point-wise estimate for $e_\\varepsilon(u_k)$ in the regions where compactness fails, which also implies the $L^{2,1}$ quantization in the bubbling process. Our second main result consists in applying the method developed in a previous joint paper with T. Rivi\\`ere to study the upper-semi-continuity of the extended Morse index to sequences of critical points of $E_{\\epsilon}$: given a sequence of critical points $u_{\\varepsilon_k}:\\Sigma\\to \\mathbb{R}^{n+1}$ of $E_\\varepsilon$ that converges in the bubble tree sense to a harmonic map $u_\\infty\\in W^{1,2}(\\Sigma,{S}^{n})$ and bubbles $v^i_{\\infty}:\\mathbb{R}^2\\to {S}^{n}$, we show that the extended Morse indices of the maps $v^i,u_\\infty$ control the extended Morse index of the sequence $u_{\\varepsilon_k}$ for $k$ large enough.","sentences":["In this paper we study the behaviour of critical points of the Ginzburg-Landau perturbation of the Dirichlet energy into the sphere $E_\\varepsilon(u):=\\int_\\Sigma \\frac{1}{2}|du|^2_h\\ \\,dvol_h +\\frac{1}{4\\varepsilon^2}(1-|u|^2)^2\\,dvol_h=\\int_{\\Sigma}e_{\\varepsilon}(u)$.","Our first main result is a precise point-wise estimate for $e_\\varepsilon(u_k)$ in the regions where compactness fails, which also implies the $L^{2,1}$ quantization in the bubbling process.","Our second main result consists in applying the method developed in a previous joint paper with T. Rivi\\`ere to study the upper-semi-continuity of the extended Morse index to sequences of critical points of $E_{\\epsilon}$: given a sequence of critical points $u_{\\varepsilon_k}:\\Sigma\\to \\mathbb{R}^{n+1}$ of $E_\\varepsilon$ that converges in the bubble tree sense to a harmonic map $u_\\infty\\in W^{1,2}(\\Sigma,{S}^{n})$ and bubbles $v^i_{\\infty}:\\mathbb{R}^2\\to {S}^{n}$, we show that the extended Morse indices of the maps $v^i,u_\\infty$ control the extended Morse index of the sequence $u_{\\varepsilon_k}$ for $k$ large enough."],"url":"http://arxiv.org/abs/2406.07317v1","category":"math.DG"}
{"created":"2024-06-11 14:30:55","title":"Minimal-norm solution to the Fredholm integral equations of the first kind via the H-HK formulation","abstract":"The Fredholm integral equations of the first kind is a typical ill-posed problem, so that it is usually difficult to obtain its analytical minimal-norm solution. This paper gives a closed-form minimal-norm solution for the degenerate kernel equations based on the H-HK formulation. Furthermore, it has been shown that the structure of solutions to degenerate kernel equations and matrix equations are consistent. Subsequently, the obtained results are extended to non-degenerate integral equations. Finally, the validity and applicability of the proposed method are demonstrated by some examples.","sentences":["The Fredholm integral equations of the first kind is a typical ill-posed problem, so that it is usually difficult to obtain its analytical minimal-norm solution.","This paper gives a closed-form minimal-norm solution for the degenerate kernel equations based on the H-HK formulation.","Furthermore, it has been shown that the structure of solutions to degenerate kernel equations and matrix equations are consistent.","Subsequently, the obtained results are extended to non-degenerate integral equations.","Finally, the validity and applicability of the proposed method are demonstrated by some examples."],"url":"http://arxiv.org/abs/2406.07303v1","category":"math.NA"}
{"created":"2024-06-11 14:23:36","title":"Exploring Cognitive Bias Triggers in COVID-19 Misinformation Tweets: A Bot vs. Human Perspective","abstract":"During the COVID-19 pandemic, the proliferation of misinformation on social media has been rapidly increasing. Automated Bot authors are believed to be significant contributors of this surge. It is hypothesized that Bot authors deliberately craft online misinformation aimed at triggering and exploiting human cognitive biases, thereby enhancing tweet engagement and persuasive influence. This study investigates this hypothesis by studying triggers of biases embedded in Bot-authored misinformation and comparing them with their counterparts, Human-authored misinformation. We complied a Misinfo Dataset that contains COVID-19 vaccine-related misinformation tweets annotated by author identities, Bots vs Humans, from Twitter during the vaccination period from July 2020 to July 2021. We developed an algorithm to computationally automate the extraction of triggers for eight cognitive biase. Our analysis revealed that the Availability Bias, Cognitive Dissonance, and Confirmation Bias were most commonly present in misinformation, with Bot-authored tweets exhibiting a greater prevalence, with distinct patterns in utilizing bias triggers between Humans and Bots. We further linked these bias triggers with engagement metrics, inferring their potential influence on tweet engagement and persuasiveness. Overall, our findings indicate that bias-triggering tactics have been more influential on Bot-authored tweets than Human-authored tweets. While certain bias triggers boosted engagement for Bot-authored tweets, some other bias triggers unexpectedly decreased it. Conversely, triggers of most biases appeared to be unrelated to the engagement of Human-authored tweets. Our work sheds light on the differential utilization and effect of persuasion strategies between Bot-authored and Human-authored misinformation from the lens of human biases, offering insights for the development of effective counter-measures.","sentences":["During the COVID-19 pandemic, the proliferation of misinformation on social media has been rapidly increasing.","Automated Bot authors are believed to be significant contributors of this surge.","It is hypothesized that Bot authors deliberately craft online misinformation aimed at triggering and exploiting human cognitive biases, thereby enhancing tweet engagement and persuasive influence.","This study investigates this hypothesis by studying triggers of biases embedded in Bot-authored misinformation and comparing them with their counterparts, Human-authored misinformation.","We complied a Misinfo Dataset that contains COVID-19 vaccine-related misinformation tweets annotated by author identities, Bots vs Humans, from Twitter during the vaccination period from July 2020 to July 2021.","We developed an algorithm to computationally automate the extraction of triggers for eight cognitive biase.","Our analysis revealed that the Availability Bias, Cognitive Dissonance, and Confirmation Bias were most commonly present in misinformation, with Bot-authored tweets exhibiting a greater prevalence, with distinct patterns in utilizing bias triggers between Humans and Bots.","We further linked these bias triggers with engagement metrics, inferring their potential influence on tweet engagement and persuasiveness.","Overall, our findings indicate that bias-triggering tactics have been more influential on Bot-authored tweets than Human-authored tweets.","While certain bias triggers boosted engagement for Bot-authored tweets, some other bias triggers unexpectedly decreased it.","Conversely, triggers of most biases appeared to be unrelated to the engagement of Human-authored tweets.","Our work sheds light on the differential utilization and effect of persuasion strategies between Bot-authored and Human-authored misinformation from the lens of human biases, offering insights for the development of effective counter-measures."],"url":"http://arxiv.org/abs/2406.07293v1","category":"cs.SI"}
{"created":"2024-06-11 14:22:10","title":"Semiclassical orthogonal polynomials on the unit circle: A Riemann-Hilbert perspective","abstract":"In this work we apply Riemann-Hilbert problem to study two families of orthogonal polynomials on the unit circle, named of modified Jacobi and Bessel. We derive first and second order differential equations, for these families of orthogonal polynomials and functions of second kind, as well as nonlinear equations that the Verblunsky coefficients satisfy.","sentences":["In this work we apply Riemann-Hilbert problem to study two families of orthogonal polynomials on the unit circle, named of modified Jacobi and Bessel.","We derive first and second order differential equations, for these families of orthogonal polynomials and functions of second kind, as well as nonlinear equations that the Verblunsky coefficients satisfy."],"url":"http://arxiv.org/abs/2406.07290v1","category":"math.CA"}
{"created":"2024-06-11 14:07:05","title":"Noise-Robust Voice Conversion by Conditional Denoising Training Using Latent Variables of Recording Quality and Environment","abstract":"We propose noise-robust voice conversion (VC) which takes into account the recording quality and environment of noisy source speech. Conventional denoising training improves the noise robustness of a VC model by learning noisy-to-clean VC process. However, the naturalness of the converted speech is limited when the noise of the source speech is unseen during the training. To this end, our proposed training conditions a VC model on two latent variables representing the recording quality and environment of the source speech. These latent variables are derived from deep neural networks pre-trained on recording quality assessment and acoustic scene classification and calculated in an utterance-wise or frame-wise manner. As a result, the trained VC model can explicitly learn information about speech degradation during the training. Objective and subjective evaluations show that our training improves the quality of the converted speech compared to the conventional training.","sentences":["We propose noise-robust voice conversion (VC) which takes into account the recording quality and environment of noisy source speech.","Conventional denoising training improves the noise robustness of a VC model by learning noisy-to-clean VC process.","However, the naturalness of the converted speech is limited when the noise of the source speech is unseen during the training.","To this end, our proposed training conditions a VC model on two latent variables representing the recording quality and environment of the source speech.","These latent variables are derived from deep neural networks pre-trained on recording quality assessment and acoustic scene classification and calculated in an utterance-wise or frame-wise manner.","As a result, the trained VC model can explicitly learn information about speech degradation during the training.","Objective and subjective evaluations show that our training improves the quality of the converted speech compared to the conventional training."],"url":"http://arxiv.org/abs/2406.07280v1","category":"cs.SD"}
{"created":"2024-06-11 14:02:43","title":"The New Worlds Simulations: Large-scale Simulations across Three Cosmologies","abstract":"In this paper we describe the set of ``New Worlds Simulations'', three very large cosmology simulations, Qo'noS, Vulcan, and Ferenginar, that were carried out on the Summit supercomputer with the Hardware/Hybrid Cosmology Code, HACC. The gravity-only simulations follow the evolution of structure in the Universe by each employing 12,288^3 particles in (3 Gpc/h)^3 volumes, leading to a mass resolution of m_p~10^9 Msun/h. The simulations cover three different cosmologies, one LambdaCDM model, consistent with measurements from Planck, one simulation with massive neutrinos, and one simulation with a varying dark energy equation of state. All simulations have the same phases to allow a detailed comparison of the results and the investigation of the impact of different cosmological parameters. We present measurements of some basic statistics, such as matter power spectra, correlation function, halo mass function and concentration-mass relation and investigate the differences due to the varying cosmologies. Given the large volume and high resolution, these simulations provide excellent bases for creating synthetic skies. A subset of the data is made publicly available as part of this paper.","sentences":["In this paper we describe the set of ``New Worlds Simulations'', three very large cosmology simulations, Qo'noS, Vulcan, and Ferenginar, that were carried out on the Summit supercomputer with the Hardware/Hybrid Cosmology Code, HACC.","The gravity-only simulations follow the evolution of structure in the Universe by each employing 12,288^3 particles in (3 Gpc/h)^3 volumes, leading to a mass resolution of m_p~10^9","Msun/h.","The simulations cover three different cosmologies, one LambdaCDM model, consistent with measurements from Planck, one simulation with massive neutrinos, and one simulation with a varying dark energy equation of state.","All simulations have the same phases to allow a detailed comparison of the results and the investigation of the impact of different cosmological parameters.","We present measurements of some basic statistics, such as matter power spectra, correlation function, halo mass function and concentration-mass relation and investigate the differences due to the varying cosmologies.","Given the large volume and high resolution, these simulations provide excellent bases for creating synthetic skies.","A subset of the data is made publicly available as part of this paper."],"url":"http://arxiv.org/abs/2406.07276v1","category":"astro-ph.CO"}
{"created":"2024-06-11 12:19:04","title":"Spectral Methods for Coastal-Trapped Waves and Instabilities in a Background Flow","abstract":"Here we present a numerical method for finding non-hydrostatic coastal-trapped wave and instability solutions to the non-hydrostatic Boussinesq equations in the presence of a background flow and complicated coastal topography. We use spectral methods to discretise the two-dimensional eigenvalue problem and solve the resulting discrete problem by standard methods. Our approach is applied to three examples and shown to be consistent with previous numerical and analytical results. In particular, we show that our method is able to reliably identify coastal-trapped wave solutions that correspond to waves seen in realistic simulations of the Southeast Greenland shelf.","sentences":["Here we present a numerical method for finding non-hydrostatic coastal-trapped wave and instability solutions to the non-hydrostatic Boussinesq equations in the presence of a background flow and complicated coastal topography.","We use spectral methods to discretise the two-dimensional eigenvalue problem and solve the resulting discrete problem by standard methods.","Our approach is applied to three examples and shown to be consistent with previous numerical and analytical results.","In particular, we show that our method is able to reliably identify coastal-trapped wave solutions that correspond to waves seen in realistic simulations of the Southeast Greenland shelf."],"url":"http://arxiv.org/abs/2406.07199v1","category":"physics.flu-dyn"}
{"created":"2024-06-11 12:07:04","title":"Accurate estimate of the ESPRESSO fiber-injection losses inferred from integrated field-stabilization images","abstract":"Ground-based astronomy is unavoidably subject to the adverse effect of atmospheric turbulence, a.k.a. the seeing, which blurs the images and limits the achievable spatial resolution. For spectroscopic observations, it leads to slit or fiber-injection losses, since not all photons distributed over the extended seeing disk can be captured. These losses might have a very substantial impact on the overall efficiency of a spectrograph and are naturally highly variable. Assessing the fiber-injection losses requires accurate information about the image quality (IQ) delivered by the telescope to the instrument over the course of the observations, which, however, is often not directly available. ESPRESSO provides acquisition and field-stabilization images attached to the science data and thus offers the opportunity for a post-processing analysis. Here, we present a novel method to infer the IQ profile and fiber-injection losses from the integrated field-stabilization images, utilizing the spill-over light that does not get injected into the fiber. We validate these measurements against the IQ observed in the acquisition images and determine that our method delivers unbiased estimates with a scatter of 0.11\" for the FWHM of the profile and 15% in terms of fiber-injection losses. This compares favorably to the estimates derived from either the differential image motion monitor (DIMM) or the telescope guide probe sensors and therefore represents a valuable tool to characterize the instrument efficiency and to correct raw spectra for fiber-injection losses.","sentences":["Ground-based astronomy is unavoidably subject to the adverse effect of atmospheric turbulence, a.k.a.","the seeing, which blurs the images and limits the achievable spatial resolution.","For spectroscopic observations, it leads to slit or fiber-injection losses, since not all photons distributed over the extended seeing disk can be captured.","These losses might have a very substantial impact on the overall efficiency of a spectrograph and are naturally highly variable.","Assessing the fiber-injection losses requires accurate information about the image quality (IQ) delivered by the telescope to the instrument over the course of the observations, which, however, is often not directly available.","ESPRESSO provides acquisition and field-stabilization images attached to the science data and thus offers the opportunity for a post-processing analysis.","Here, we present a novel method to infer the IQ profile and fiber-injection losses from the integrated field-stabilization images, utilizing the spill-over light that does not get injected into the fiber.","We validate these measurements against the IQ observed in the acquisition images and determine that our method delivers unbiased estimates with a scatter of 0.11\" for the FWHM of the profile and 15% in terms of fiber-injection losses.","This compares favorably to the estimates derived from either the differential image motion monitor (DIMM) or the telescope guide probe sensors and therefore represents a valuable tool to characterize the instrument efficiency and to correct raw spectra for fiber-injection losses."],"url":"http://arxiv.org/abs/2406.07193v1","category":"astro-ph.IM"}
{"created":"2024-06-11 11:26:27","title":"VoxNeuS: Enhancing Voxel-Based Neural Surface Reconstruction via Gradient Interpolation","abstract":"Neural Surface Reconstruction learns a Signed Distance Field~(SDF) to reconstruct the 3D model from multi-view images. Previous works adopt voxel-based explicit representation to improve efficiency. However, they ignored the gradient instability of interpolation in the voxel grid, leading to degradation on convergence and smoothness. Besides, previous works entangled the optimization of geometry and radiance, which leads to the deformation of geometry to explain radiance, causing artifacts when reconstructing textured planes.   In this work, we reveal that the instability of gradient comes from its discontinuity during trilinear interpolation, and propose to use the interpolated gradient instead of the original analytical gradient to eliminate the discontinuity. Based on gradient interpolation, we propose VoxNeuS, a lightweight surface reconstruction method for computational and memory efficient neural surface reconstruction. Thanks to the explicit representation, the gradient of regularization terms, i.e. Eikonal and curvature loss, are directly solved, avoiding computation and memory-access overhead.   Further, VoxNeuS adopts a geometry-radiance disentangled architecture to handle the geometry deformation from radiance optimization.   The experimental results show that VoxNeuS achieves better reconstruction quality than previous works. The entire training process takes 15 minutes and less than 3 GB of memory on a single 2080ti GPU.","sentences":["Neural Surface Reconstruction learns a Signed Distance Field~(SDF) to reconstruct the 3D model from multi-view images.","Previous works adopt voxel-based explicit representation to improve efficiency.","However, they ignored the gradient instability of interpolation in the voxel grid, leading to degradation on convergence and smoothness.","Besides, previous works entangled the optimization of geometry and radiance, which leads to the deformation of geometry to explain radiance, causing artifacts when reconstructing textured planes.   ","In this work, we reveal that the instability of gradient comes from its discontinuity during trilinear interpolation, and propose to use the interpolated gradient instead of the original analytical gradient to eliminate the discontinuity.","Based on gradient interpolation, we propose VoxNeuS, a lightweight surface reconstruction method for computational and memory efficient neural surface reconstruction.","Thanks to the explicit representation, the gradient of regularization terms, i.e. Eikonal and curvature loss, are directly solved, avoiding computation and memory-access overhead.   ","Further, VoxNeuS adopts a geometry-radiance disentangled architecture to handle the geometry deformation from radiance optimization.   ","The experimental results show that VoxNeuS achieves better reconstruction quality than previous works.","The entire training process takes 15 minutes and less than 3 GB of memory on a single 2080ti GPU."],"url":"http://arxiv.org/abs/2406.07170v1","category":"cs.CV"}
{"created":"2024-06-11 11:18:10","title":"On the pathwise uniqueness of stochastic 2D Euler equations with Kraichnan noise and $L^p$-data","abstract":"In the recent work [arXiv:2308.03216], Coghi and Maurelli proved pathwise uniqueness of solutions to the vorticity form of stochastic 2D Euler equation, with Kraichnan transport noise and initial data in $L^1\\cap L^p$ for $p>3/2$. The aim of this note is to remove the constraint on $p$, showing that pathwise uniqueness holds for all $L^1\\cap L^p$ initial data with arbitrary $p>1$.","sentences":["In the recent work [arXiv:2308.03216], Coghi and Maurelli proved pathwise uniqueness of solutions to the vorticity form of stochastic 2D Euler equation, with Kraichnan transport noise and initial data in $L^1\\cap L^p$ for $p>3/2$. The aim of this note is to remove the constraint on $p$, showing that pathwise uniqueness holds for all $L^1\\cap L^p$ initial data with arbitrary $p>1$."],"url":"http://arxiv.org/abs/2406.07167v1","category":"math.PR"}
{"created":"2024-06-11 10:37:36","title":"Increased accuracy and signal-to-noise ratio through recent improvements in Infra-Red Video Bolometer fabrication and calibration","abstract":"The Infra-Red Video Bolometer (IRVB) is a diagnostic equipped with an infra-red camera that measures the total radiated power in thousands of LOSs within a large field of view (FOV). Recently validated in MAST-U, it offers a high spatial resolution map of the radiated power in the divertor region, where large gradients are expected. The IRVB's sensing element comprises a thin layer of high Z absorbing material, typically Platinum, usually coated with Carbon to reduce reflections. It is here explored the possibility of using a relatively inert material like Titanium, that can be produced in layers up to 1mum compared to 2.5mum for Pt, and then coat it with Pt of the desired thickness (0.3mum per side here) and Carbon. This leads to a higher temperature signal (2 to 3 times), and better spatial resolution (about 4 times), resulting in higher accuracy in the measured power. This assembly is also expected to improve foil uniformity, as the Pt layer is obtained via deposition rather than mechanical processes. Given its multi-material composition, measuring the thermal properties of the foil assembly is vital. Various methods using a calibrated laser as a heat source have been developed, analysing the temperature profile shape or fitting the calculated laser power for different intensities and frequencies. It is here presented a simpler approach, that relies on analysing the separate components of the foil heat equation for a single laser exposure in a given area. This can then be iterated over the entire foil to capture local deviations.","sentences":["The Infra-Red Video Bolometer (IRVB) is a diagnostic equipped with an infra-red camera that measures the total radiated power in thousands of LOSs within a large field of view (FOV).","Recently validated in MAST-U, it offers a high spatial resolution map of the radiated power in the divertor region, where large gradients are expected.","The IRVB's sensing element comprises a thin layer of high Z absorbing material, typically Platinum, usually coated with Carbon to reduce reflections.","It is here explored the possibility of using a relatively inert material like Titanium, that can be produced in layers up to 1mum compared to 2.5mum for Pt, and then coat it with Pt of the desired thickness (0.3mum per side here) and Carbon.","This leads to a higher temperature signal (2 to 3 times), and better spatial resolution (about 4 times), resulting in higher accuracy in the measured power.","This assembly is also expected to improve foil uniformity, as the Pt layer is obtained via deposition rather than mechanical processes.","Given its multi-material composition, measuring the thermal properties of the foil assembly is vital.","Various methods using a calibrated laser as a heat source have been developed, analysing the temperature profile shape or fitting the calculated laser power for different intensities and frequencies.","It is here presented a simpler approach, that relies on analysing the separate components of the foil heat equation for a single laser exposure in a given area.","This can then be iterated over the entire foil to capture local deviations."],"url":"http://arxiv.org/abs/2406.07139v1","category":"physics.plasm-ph"}
{"created":"2024-06-11 09:53:18","title":"NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse Polarized Images","abstract":"We present NeRSP, a Neural 3D reconstruction technique for Reflective surfaces with Sparse Polarized images. Reflective surface reconstruction is extremely challenging as specular reflections are view-dependent and thus violate the multiview consistency for multiview stereo. On the other hand, sparse image inputs, as a practical capture setting, commonly cause incomplete or distorted results due to the lack of correspondence matching. This paper jointly handles the challenges from sparse inputs and reflective surfaces by leveraging polarized images. We derive photometric and geometric cues from the polarimetric image formation model and multiview azimuth consistency, which jointly optimize the surface geometry modeled via implicit neural representation. Based on the experiments on our synthetic and real datasets, we achieve the state-of-the-art surface reconstruction results with only 6 views as input.","sentences":["We present NeRSP, a Neural 3D reconstruction technique for Reflective surfaces with Sparse Polarized images.","Reflective surface reconstruction is extremely challenging as specular reflections are view-dependent and thus violate the multiview consistency for multiview stereo.","On the other hand, sparse image inputs, as a practical capture setting, commonly cause incomplete or distorted results due to the lack of correspondence matching.","This paper jointly handles the challenges from sparse inputs and reflective surfaces by leveraging polarized images.","We derive photometric and geometric cues from the polarimetric image formation model and multiview azimuth consistency, which jointly optimize the surface geometry modeled via implicit neural representation.","Based on the experiments on our synthetic and real datasets, we achieve the state-of-the-art surface reconstruction results with only 6 views as input."],"url":"http://arxiv.org/abs/2406.07111v1","category":"cs.CV"}
{"created":"2024-06-11 08:59:20","title":"On the relation between trainability and dequantization of variational quantum learning models","abstract":"The quest for successful variational quantum machine learning (QML) relies on the design of suitable parametrized quantum circuits (PQCs), as analogues to neural networks in classical machine learning. Successful QML models must fulfill the properties of trainability and non-dequantization, among others. Recent works have highlighted an intricate interplay between trainability and dequantization of such models, which is still unresolved. In this work we contribute to this debate from the perspective of machine learning, proving a number of results identifying, among others when trainability and non-dequantization are not mutually exclusive. We begin by providing a number of new somewhat broader definitions of the relevant concepts, compared to what is found in other literature, which are operationally motivated, and consistent with prior art. With these precise definitions given and motivated, we then study the relation between trainability and dequantization of variational QML. Next, we also discuss the degrees of \"variationalness\" of QML models, where we distinguish between models like the hardware efficient ansatz and quantum kernel methods. Finally, we introduce recipes for building PQC-based QML models which are both trainable and nondequantizable, and corresponding to different degrees of variationalness. We do not address the practical utility for such models. Our work however does point toward a way forward for finding more general constructions, for which finding applications may become feasible.","sentences":["The quest for successful variational quantum machine learning (QML) relies on the design of suitable parametrized quantum circuits (PQCs), as analogues to neural networks in classical machine learning.","Successful QML models must fulfill the properties of trainability and non-dequantization, among others.","Recent works have highlighted an intricate interplay between trainability and dequantization of such models, which is still unresolved.","In this work we contribute to this debate from the perspective of machine learning, proving a number of results identifying, among others when trainability and non-dequantization are not mutually exclusive.","We begin by providing a number of new somewhat broader definitions of the relevant concepts, compared to what is found in other literature, which are operationally motivated, and consistent with prior art.","With these precise definitions given and motivated, we then study the relation between trainability and dequantization of variational QML.","Next, we also discuss the degrees of \"variationalness\" of QML models, where we distinguish between models like the hardware efficient ansatz and quantum kernel methods.","Finally, we introduce recipes for building PQC-based QML models which are both trainable and nondequantizable, and corresponding to different degrees of variationalness.","We do not address the practical utility for such models.","Our work however does point toward a way forward for finding more general constructions, for which finding applications may become feasible."],"url":"http://arxiv.org/abs/2406.07072v1","category":"quant-ph"}
{"created":"2024-06-11 08:44:52","title":"Anomalous propagators and the particle-particle channel: Hedin's equations","abstract":"Hedin's equations provide an elegant route to compute the exact one-body Green's function (or propagator) via the self-consistent iteration of a set of non-linear equations.   Its first-order approximation, known as $GW$, corresponds to a resummation of ring diagrams and has shown to be extremely successful in physics and chemistry. Systematic improvement is possible, although challenging, via the introduction of vertex corrections.   Considering anomalous propagators and an external pairing potential, we derive a new self-consistent set of closed equations equivalent to the famous Hedin equations but having as a first-order approximation the particle-particle (pp) $T$-matrix approximation where one performs a resummation of the ladder diagrams.   This pp version of Hedin's equations offers a way to go systematically beyond the $T$-matrix approximation by accounting for low-order pp vertex corrections.","sentences":["Hedin's equations provide an elegant route to compute the exact one-body Green's function (or propagator) via the self-consistent iteration of a set of non-linear equations.   ","Its first-order approximation, known as $GW$, corresponds to a resummation of ring diagrams and has shown to be extremely successful in physics and chemistry.","Systematic improvement is possible, although challenging, via the introduction of vertex corrections.   ","Considering anomalous propagators and an external pairing potential, we derive a new self-consistent set of closed equations equivalent to the famous Hedin equations but having as a first-order approximation the particle-particle (pp) $T$-matrix approximation where one performs a resummation of the ladder diagrams.   ","This pp version of Hedin's equations offers a way to go systematically beyond the $T$-matrix approximation by accounting for low-order pp vertex corrections."],"url":"http://arxiv.org/abs/2406.07062v1","category":"physics.chem-ph"}
{"created":"2024-06-11 08:20:50","title":"Photo-induced dynamics with continuous and discrete quantum baths","abstract":"The ultrafast quantum dynamics of photophysical processes in complex molecules is an extremely challenging computational problem with a wide variety of fascinating applications in quantum chemistry and biology. Inspired by recent developments in open quantum systems, we introduce a pure-state unraveled hybrid-bath method that describes a continuous environment via a set of discrete, effective bosonic degrees of freedom using a Markovian embedding. Our method is capable of describing both, a continuous spectral density and sharp peaks embedded into it. Thereby, we overcome the limitations of previous methods, which either capture long-time memory effects using the unitary dynamics of a set of discrete vibrational modes or use memoryless Markovian environments employing a Lindblad or Redfield master equation. We benchmark our method against two paradigmatic problems from quantum chemistry and biology. We demonstrate that compared to unitary descriptions, a significantly smaller number of bosonic modes suffices to describe the excitonic dynamics accurately, yielding a computational speed-up of nearly an order of magnitude. Furthermore, we take into account explicitly the effect of a $\\delta$-peak in the spectral density of a light-harvesting complex, demonstrating the strong impact of the long-time memory of the environment on the dynamics.","sentences":["The ultrafast quantum dynamics of photophysical processes in complex molecules is an extremely challenging computational problem with a wide variety of fascinating applications in quantum chemistry and biology.","Inspired by recent developments in open quantum systems, we introduce a pure-state unraveled hybrid-bath method that describes a continuous environment via a set of discrete, effective bosonic degrees of freedom using a Markovian embedding.","Our method is capable of describing both, a continuous spectral density and sharp peaks embedded into it.","Thereby, we overcome the limitations of previous methods, which either capture long-time memory effects using the unitary dynamics of a set of discrete vibrational modes or use memoryless Markovian environments employing a Lindblad or Redfield master equation.","We benchmark our method against two paradigmatic problems from quantum chemistry and biology.","We demonstrate that compared to unitary descriptions, a significantly smaller number of bosonic modes suffices to describe the excitonic dynamics accurately, yielding a computational speed-up of nearly an order of magnitude.","Furthermore, we take into account explicitly the effect of a $\\delta$-peak in the spectral density of a light-harvesting complex, demonstrating the strong impact of the long-time memory of the environment on the dynamics."],"url":"http://arxiv.org/abs/2406.07047v1","category":"quant-ph"}
{"created":"2024-06-11 08:17:56","title":"Research on High-precision Detection Technology for Un-derground Space Information Data","abstract":"The quality of underground space information data has become a major problem endangering the safety of underground space. After research and analysis, we found that the current high accuracy information data remote detection methods are limited to the detection of overground spaces objects, and are not applicable to the detection of various information data in underground space. In this paper, we analyze the spectral properties of the fractional-order differential (FDO) operator, and establish mathematical model of remote transmission and high-precision detection of information data, which realizes the functions of high-precision and remote detection of information data. By fusing the information data to detect the mathematical model in a long distance and with high accuracy, A mathematical model has been established to improve the quality of underground spatial information data. Through the application in engineering practice, the effectiveness of this method in underground space information data detection is verified.","sentences":["The quality of underground space information data has become a major problem endangering the safety of underground space.","After research and analysis, we found that the current high accuracy information data remote detection methods are limited to the detection of overground spaces objects, and are not applicable to the detection of various information data in underground space.","In this paper, we analyze the spectral properties of the fractional-order differential (FDO) operator, and establish mathematical model of remote transmission and high-precision detection of information data, which realizes the functions of high-precision and remote detection of information data.","By fusing the information data to detect the mathematical model in a long distance and with high accuracy, A mathematical model has been established to improve the quality of underground spatial information data.","Through the application in engineering practice, the effectiveness of this method in underground space information data detection is verified."],"url":"http://arxiv.org/abs/2406.07045v1","category":"math.NA"}
{"created":"2024-06-11 08:11:22","title":"On inertial Levenberg-Marquardt type methods for solving nonlinear ill-posed operator equations","abstract":"In these notes we propose and analyze an inertial type method for obtaining stable approximate solutions to nonlinear ill-posed operator equations. The method is based on the Levenberg-Marquardt (LM) iteration. The main obtained results are: monotonicity and convergence for exact data, stability and semi-convergence for noisy data. Regarding numerical experiments we consider: i) a parameter identification problem in elliptic PDEs, ii) a parameter identification problem in machine learning; the computational efficiency of the proposed method is compared with canonical implementations of the LM method.","sentences":["In these notes we propose and analyze an inertial type method for obtaining stable approximate solutions to nonlinear ill-posed operator equations.","The method is based on the Levenberg-Marquardt (LM) iteration.","The main obtained results are: monotonicity and convergence for exact data, stability and semi-convergence for noisy data.","Regarding numerical experiments we consider: i) a parameter identification problem in elliptic PDEs, ii) a parameter identification problem in machine learning; the computational efficiency of the proposed method is compared with canonical implementations of the LM method."],"url":"http://arxiv.org/abs/2406.07044v1","category":"math.NA"}
{"created":"2024-06-11 07:55:05","title":"Pluriclosed manifolds with parallel Bismut torsion","abstract":"We present a complete classification of simply-connected pluriclosed manifolds with parallel Bismut torsion, extending previously known results in the literature. Consequently, we also establish a splitting theorem for compact manifolds that are both pluriclosed with parallel Bismut torsion and Calabi-Yau with torsion.","sentences":["We present a complete classification of simply-connected pluriclosed manifolds with parallel Bismut torsion, extending previously known results in the literature.","Consequently, we also establish a splitting theorem for compact manifolds that are both pluriclosed with parallel Bismut torsion and Calabi-Yau with torsion."],"url":"http://arxiv.org/abs/2406.07039v1","category":"math.DG"}
{"created":"2024-06-11 07:54:09","title":"A Multi-Scale Boltzmann Equation for Complex Systems of Neutral Gases across All Flow Regimes","abstract":"A Multi-scale Boltzmann Equation (MBE) is found from the gas-kinetic theory and the direct modeling philosophy as a master equation for complex physical systems of neutral gases across all flow regimes, which locates between the continuum limit and the free-molecular limit, covering a vast range of applications such as hypersonic flows over aerospace crafts and delicate flows around MEMS. The most explicit characteristic of MBE is evolving the variable observation time in the expression, which distinguishes the MBE from the single-scale master or governing equation where a fixed scale is implied in the assumptions. The fundamental properties of MBE, such as the asymptotic property, are proved theoretically, while a concise numerical scheme is developed for MBE to demonstrate its validity by benchmark multi-scale problems.","sentences":["A Multi-scale Boltzmann Equation (MBE) is found from the gas-kinetic theory and the direct modeling philosophy as a master equation for complex physical systems of neutral gases across all flow regimes, which locates between the continuum limit and the free-molecular limit, covering a vast range of applications such as hypersonic flows over aerospace crafts and delicate flows around MEMS.","The most explicit characteristic of MBE is evolving the variable observation time in the expression, which distinguishes the MBE from the single-scale master or governing equation where a fixed scale is implied in the assumptions.","The fundamental properties of MBE, such as the asymptotic property, are proved theoretically, while a concise numerical scheme is developed for MBE to demonstrate its validity by benchmark multi-scale problems."],"url":"http://arxiv.org/abs/2406.07038v1","category":"physics.comp-ph"}
{"created":"2024-06-11 07:47:28","title":"Solving the index problem for (curved) Bernstein-Gelfand-Gelfand sequences","abstract":"We study the index theory of curved Bernstein-Gelfand-Gelfand (BGG) sequences in parabolic geometry and their role in $K$-homology and noncommutative geometry. The BGG-sequences fit into $K$-homology, and we solve their index problem. We provide a condition for when the BGG-complex on the flat parabolic geometry $G/P$ of a semisimple Lie group $G$ fits into $G$-equivariant $K$-homology by means of Heisenberg calculus. For higher rank Lie groups, we prove a no-go theorem showing that the approach fails.","sentences":["We study the index theory of curved Bernstein-Gelfand-Gelfand (BGG) sequences in parabolic geometry and their role in $K$-homology and noncommutative geometry.","The BGG-sequences fit into $K$-homology, and we solve their index problem.","We provide a condition for when the BGG-complex on the flat parabolic geometry $G/P$ of a semisimple Lie group $G$ fits into $G$-equivariant $K$-homology by means of Heisenberg calculus.","For higher rank Lie groups, we prove a no-go theorem showing that the approach fails."],"url":"http://arxiv.org/abs/2406.07033v1","category":"math.KT"}
{"created":"2024-06-11 06:47:40","title":"On isometry groups of gradient Ricci solitons","abstract":"We give a result estimating the dimension of the Lie algebra of Killing vector fields on an irreducible non-trivial gradient Ricci soliton. Then we study the structure of this manifold when the maximal dimension is attained. There are local and global implications.","sentences":["We give a result estimating the dimension of the Lie algebra of Killing vector fields on an irreducible non-trivial gradient Ricci soliton.","Then we study the structure of this manifold when the maximal dimension is attained.","There are local and global implications."],"url":"http://arxiv.org/abs/2406.06997v1","category":"math.DG"}
{"created":"2024-06-11 06:46:08","title":"Lorentz-violating extension of Wigner function formalism and chiral kinetic theory","abstract":"The quantum kinetic equation for the gauge-invariant Wigner function, constructed from spinor fields that obey the Dirac equation modified by CPT and Lorentz symmetry-violating terms, is presented. The equations for the components of Wigner function in the Clifford algebra basis are accomplished. Focusing on the massless case, an extended semiclassical chiral kinetic theory in the presence of external electromagnetic fields is developed. We calculate the chiral currents and establish the anomalous magnetic and separation effects in a Lorentz-violating background. The chiral anomaly within the context of extended Quantum Electrodynamics is elucidated. Finally, we derive the semiclassical Lorentz-violating extended chiral transport equation.","sentences":["The quantum kinetic equation for the gauge-invariant Wigner function, constructed from spinor fields that obey the Dirac equation modified by CPT and Lorentz symmetry-violating terms, is presented.","The equations for the components of Wigner function in the Clifford algebra basis are accomplished.","Focusing on the massless case, an extended semiclassical chiral kinetic theory in the presence of external electromagnetic fields is developed.","We calculate the chiral currents and establish the anomalous magnetic and separation effects in a Lorentz-violating background.","The chiral anomaly within the context of extended Quantum Electrodynamics is elucidated.","Finally, we derive the semiclassical Lorentz-violating extended chiral transport equation."],"url":"http://arxiv.org/abs/2406.06996v1","category":"hep-th"}
{"created":"2024-06-11 06:36:32","title":"AC False Data Injection Attacks in Power Systems: Design and Optimization","abstract":"False Data Injection (FDI) attacks are one of the challenges that the modern power system, as a cyber-physical system, is encountering. Designing AC FDI attacks that accurately address the physics of the power systems could jeopardize the security of power systems as they can easily bypass the traditional Bad Data Detection (BDD) algorithm. Knowing the essence of the AC FDI attack and how they can be designed gives insight about detecting the system again these attacks. Moreover, recognition of the nature of these attacks, especially when they are designed optimally, is essential for benchmarking various defensive approaches to increase the resilience of power systems. This paper presents a unified approach to demonstrate the process of designing optimal AC FDI attack. In this connection, we first define the process of designing an AC-based FDI attack that satisfies AC power flow equations. We then formulate an optimization problem to design an optimal AC FDI attack that both satisfies AC power flow equations and overloads a specific line in the system. The objective function is defined to optimize the magnitude of the attack vector in such a way that it can evade residue-based BDD approaches. The proposed approach for designing AC FDI attacks is applied to the IEEE 118-bus test case system. Various comparisons are conducted to elaborate on the impact of optimally designing AC FDI attacks on the residual for the AC state estimation algorithm. Comparing the results of optimal and non-optimal AC FDI attacks demonstrates the impact on the difficulty of detecting FDI attacks and the importance of optimally designing these attacks.","sentences":["False Data Injection (FDI) attacks are one of the challenges that the modern power system, as a cyber-physical system, is encountering.","Designing AC FDI attacks that accurately address the physics of the power systems could jeopardize the security of power systems as they can easily bypass the traditional Bad Data Detection (BDD) algorithm.","Knowing the essence of the AC FDI attack and how they can be designed gives insight about detecting the system again these attacks.","Moreover, recognition of the nature of these attacks, especially when they are designed optimally, is essential for benchmarking various defensive approaches to increase the resilience of power systems.","This paper presents a unified approach to demonstrate the process of designing optimal AC FDI attack.","In this connection, we first define the process of designing an AC-based FDI attack that satisfies AC power flow equations.","We then formulate an optimization problem to design an optimal AC FDI attack that both satisfies AC power flow equations and overloads a specific line in the system.","The objective function is defined to optimize the magnitude of the attack vector in such a way that it can evade residue-based BDD approaches.","The proposed approach for designing AC FDI attacks is applied to the IEEE 118-bus test case system.","Various comparisons are conducted to elaborate on the impact of optimally designing AC FDI attacks on the residual for the AC state estimation algorithm.","Comparing the results of optimal and non-optimal AC FDI attacks demonstrates the impact on the difficulty of detecting FDI attacks and the importance of optimally designing these attacks."],"url":"http://arxiv.org/abs/2406.06988v1","category":"math.OC"}
{"created":"2024-06-11 06:18:26","title":"Hydra-MDP: End-to-end Multimodal Planning with Multi-target Hydra-Distillation","abstract":"We propose Hydra-MDP, a novel paradigm employing multiple teachers in a teacher-student model. This approach uses knowledge distillation from both human and rule-based teachers to train the student model, which features a multi-head decoder to learn diverse trajectory candidates tailored to various evaluation metrics. With the knowledge of rule-based teachers, Hydra-MDP learns how the environment influences the planning in an end-to-end manner instead of resorting to non-differentiable post-processing. This method achieves the $1^{st}$ place in the Navsim challenge, demonstrating significant improvements in generalization across diverse driving environments and conditions. Code will be available at \\url{https://github.com/woxihuanjiangguo/Hydra-MDP}","sentences":["We propose Hydra-MDP, a novel paradigm employing multiple teachers in a teacher-student model.","This approach uses knowledge distillation from both human and rule-based teachers to train the student model, which features a multi-head decoder to learn diverse trajectory candidates tailored to various evaluation metrics.","With the knowledge of rule-based teachers, Hydra-MDP learns how the environment influences the planning in an end-to-end manner instead of resorting to non-differentiable post-processing.","This method achieves the $1^{st}$ place in the Navsim challenge, demonstrating significant improvements in generalization across diverse driving environments and conditions.","Code will be available at \\url{https://github.com/woxihuanjiangguo/Hydra-MDP}"],"url":"http://arxiv.org/abs/2406.06978v1","category":"cs.CV"}
{"created":"2024-06-11 03:50:47","title":"Average speeds of time almost periodic traveling waves for rapidly/slowly oscillating reaction-diffusion equations","abstract":"This paper is concerned with the propagation dynamics of time almost periodic reaction-diffusion equations. Assuming the existence of a time almost periodic traveling wave connecting two stable steady states, we focus especially on the asymptotic behavior of average wave speeds in both rapidly oscillating and slowly oscillating environments. We prove that, in the rapidly oscillating case, the average speed converges to the constant wave speed of the homogenized equation; while in the slowly oscillating case, it approximates the arithmetic mean of the constant wave speeds for a family of equations with frozen coefficients. In both cases, we provide estimates on the convergence rates showing that, in comparison to the limiting speeds, the deviations of average speeds for almost periodic traveling waves are at most linear in certain sense. Furthermore, our explicit formulas for the limiting speeds indicate that temporal variations have significant influences on wave propagation. Even in periodic environments, it can alter the propagation direction of bistable equations.","sentences":["This paper is concerned with the propagation dynamics of time almost periodic reaction-diffusion equations.","Assuming the existence of a time almost periodic traveling wave connecting two stable steady states, we focus especially on the asymptotic behavior of average wave speeds in both rapidly oscillating and slowly oscillating environments.","We prove that, in the rapidly oscillating case, the average speed converges to the constant wave speed of the homogenized equation; while in the slowly oscillating case, it approximates the arithmetic mean of the constant wave speeds for a family of equations with frozen coefficients.","In both cases, we provide estimates on the convergence rates showing that, in comparison to the limiting speeds, the deviations of average speeds for almost periodic traveling waves are at most linear in certain sense.","Furthermore, our explicit formulas for the limiting speeds indicate that temporal variations have significant influences on wave propagation.","Even in periodic environments, it can alter the propagation direction of bistable equations."],"url":"http://arxiv.org/abs/2406.06928v1","category":"math.AP"}
{"created":"2024-06-11 03:44:17","title":"Non-autoregressive Personalized Bundle Generation","abstract":"The personalized bundle generation problem, which aims to create a preferred bundle for user from numerous candidate items, receives increasing attention in recommendation. However, existing works ignore the order-invariant nature of the bundle and adopt sequential modeling methods as the solution, which might introduce inductive bias and cause a large latency in prediction. To address this problem, we propose to perform the bundle generation via non-autoregressive mechanism and design a novel encoder-decoder framework named BundleNAT, which can effectively output the targeted bundle in one-shot without relying on any inherent order. In detail, instead of learning sequential dependency, we propose to adopt pre-training techniques and graph neural network to fully embed user-based preference and item-based compatibility information, and use a self-attention based encoder to further extract global dependency pattern. We then design a permutation-equivariant decoding architecture that is able to directly output the desired bundle in a one-shot manner. Experiments on three real-world datasets from Youshu and Netease show the proposed BundleNAT significantly outperforms the current state-of-the-art methods in average by up to 35.92%, 10.97% and 23.67% absolute improvements in Precision, Precision+, and Recall, respectively.","sentences":["The personalized bundle generation problem, which aims to create a preferred bundle for user from numerous candidate items, receives increasing attention in recommendation.","However, existing works ignore the order-invariant nature of the bundle and adopt sequential modeling methods as the solution, which might introduce inductive bias and cause a large latency in prediction.","To address this problem, we propose to perform the bundle generation via non-autoregressive mechanism and design a novel encoder-decoder framework named BundleNAT, which can effectively output the targeted bundle in one-shot without relying on any inherent order.","In detail, instead of learning sequential dependency, we propose to adopt pre-training techniques and graph neural network to fully embed user-based preference and item-based compatibility information, and use a self-attention based encoder to further extract global dependency pattern.","We then design a permutation-equivariant decoding architecture that is able to directly output the desired bundle in a one-shot manner.","Experiments on three real-world datasets from Youshu and Netease show the proposed BundleNAT significantly outperforms the current state-of-the-art methods in average by up to 35.92%, 10.97% and 23.67% absolute improvements in Precision, Precision+, and Recall, respectively."],"url":"http://arxiv.org/abs/2406.06925v1","category":"cs.LG"}
{"created":"2024-06-11 03:27:33","title":"Existence and uniqueness of ground state solutions for the planar Schr\u00f6dinger-Newton equation on the disc","abstract":"This paper is concerned with the existence and qualitative properties of positive ground state solutions for the planar Schr\\\"odinger-Newton equation on the disc. First, we prove the existence and radial symmetry of all the positive ground state solutions by employing the symmetric decreasing rearrangement and Talenti's inequality. Next, we develop Newton's theorem and then use the contraction mapping principle to establish the uniqueness of the positive ground state solution for the Schr\\\"odinger-Newton equation on the disc in the two dimensional case. Finally, we show that the unique positive ground state solution converges to the trivial solution as the radius $R$ tending to infinity, which is totally different from the higher dimensional case in \\cite{Guo-Wang-Yi}.","sentences":["This paper is concerned with the existence and qualitative properties of positive ground state solutions for the planar Schr\\\"odinger-Newton equation on the disc.","First, we prove the existence and radial symmetry of all the positive ground state solutions by employing the symmetric decreasing rearrangement and Talenti's inequality.","Next, we develop Newton's theorem and then use the contraction mapping principle to establish the uniqueness of the positive ground state solution for the Schr\\\"odinger-Newton equation on the disc in the two dimensional case.","Finally, we show that the unique positive ground state solution converges to the trivial solution as the radius $R$ tending to infinity, which is totally different from the higher dimensional case in \\cite{Guo-Wang-Yi}."],"url":"http://arxiv.org/abs/2406.06919v1","category":"math.AP"}
{"created":"2024-06-11 02:50:09","title":"On Stewart's Perturbation Theorem for SVD","abstract":"This paper establishes a variant of Stewart's theorem (Theorem~6.4 of Stewart, {\\em SIAM Rev.}, 15:727--764, 1973) for the singular subspaces associated with the SVD of a matrix subject to perturbations. Stewart's original version uses both the Frobenius and spectral norms, whereas the new variant uses the spectral norm and any unitarily invariant norm that offer choices per convenience of particular applications and lead to sharper bounds than that straightforwardly derived from Stewart's original theorem with the help of the well-known equivalence inequalities between matrix norms. Of interest in their own right, bounds on the solution to two couple Sylvester equations are established for a few different circumstances.","sentences":["This paper establishes a variant of Stewart's theorem (Theorem~6.4 of Stewart, {\\em SIAM Rev.}, 15:727--764, 1973) for the singular subspaces associated with the SVD of a matrix subject to perturbations.","Stewart's original version uses both the Frobenius and spectral norms, whereas the new variant uses the spectral norm and any unitarily invariant norm that offer choices per convenience of particular applications and lead to sharper bounds than that straightforwardly derived from Stewart's original theorem with the help of the well-known equivalence inequalities between matrix norms.","Of interest in their own right, bounds on the solution to two couple Sylvester equations are established for a few different circumstances."],"url":"http://arxiv.org/abs/2406.06901v1","category":"math.NA"}
{"created":"2024-06-11 02:29:59","title":"Conformal metrics of constant scalar curvature with unbounded volumes","abstract":"For $n\\geq 25$, we construct a smooth metric $\\tilde{g}$ on the standard $n$-dimensional sphere $\\mathbb{S}^n$ such that there exists a sequence of smooth metrics $\\{\\tilde{g}_k\\}_{k\\in\\mathbb{N}}$ conformal to $\\tilde g$ where each $\\tilde g_k$ has scalar curvature $R_{\\tilde{g}_k}\\equiv 1$ and their volumes $\\text{Vol}(\\mathbb{S}^n,\\tilde{g}_k)$ tend to infinity as $k$ approaches infinity.","sentences":["For $n\\geq 25$, we construct a smooth metric $\\tilde{g}$ on the standard $n$-dimensional sphere $\\mathbb{S}^n$ such that there exists a sequence of smooth metrics $\\{\\tilde{g}_k\\}_{k\\in\\mathbb{N}}$ conformal to $\\tilde g$ where each $\\tilde g_k$ has scalar curvature $R_{\\tilde{g}_k}\\equiv 1$ and their volumes $\\text{Vol}(\\mathbb{S}^n,\\tilde{g}_k)$ tend to infinity as $k$ approaches infinity."],"url":"http://arxiv.org/abs/2406.06898v1","category":"math.AP"}
{"created":"2024-06-11 02:23:04","title":"Simultaneous global inviscid Burgers flows with periodic Poisson forcing","abstract":"We study the inviscid Burgers equation on the circle $\\mathbb{T}:=\\mathbb{R}/\\mathbb{Z}$ forced by the derivative of a Poisson point process on $\\mathbb{R}\\times\\mathbb{T}$. We construct global solutions with mean $\\theta$ simultaneously for all $\\theta\\in\\mathbb{R}$, and in addition construct their associated global shocks (which are unique except on a countable set of $\\theta$). We then show that as $\\theta$ changes, the solution only changes through the movement of the global shock, and give precise formulas for this movement. This is an analogue of previous results by the author and Yu Gu in the viscous case with white-in-time forcing, which related the derivative of the solution in $\\theta$ to the density of a particle diffusing in the Burgers flow.","sentences":["We study the inviscid Burgers equation on the circle $\\mathbb{T}:=\\mathbb{R}/\\mathbb{Z}$ forced by the derivative of a Poisson point process on $\\mathbb{R}\\times\\mathbb{T}$. We construct global solutions with mean $\\theta$ simultaneously for all $\\theta\\in\\mathbb{R}$, and in addition construct their associated global shocks (which are unique except on a countable set of $\\theta$).","We then show that as $\\theta$ changes, the solution only changes through the movement of the global shock, and give precise formulas for this movement.","This is an analogue of previous results by the author and Yu Gu in the viscous case with white-in-time forcing, which related the derivative of the solution in $\\theta$ to the density of a particle diffusing in the Burgers flow."],"url":"http://arxiv.org/abs/2406.06896v1","category":"math.PR"}
{"created":"2024-06-11 02:21:51","title":"On a nonhomogeneous heat equation on the complex plane","abstract":"In this article, we investigate the existence, uniqueness, and asymptotic behaviors of mild solutions of a parabolic evolution equations on complex plane, in which the diffusion operator has the form \\(\\overline{\\Box}_{\\varphi} = \\overline{D}\\, \\overline{D}^{\\ast}\\), where \\(\\overline{D} f = \\bar{\\partial}f + \\varphi_{\\bar{z}} f\\), the function \\(\\varphi\\) is smooth and subharmonic on \\(\\mathbb{C}\\), and \\(\\overline{D}^{\\ast}\\) is the formal adjoint of \\(\\overline{D}\\). Our method combines certain estimates of heat kernel associating with the homogeneous linear equation of Raich \\cite{raich06} and a fixed point argument.","sentences":["In this article, we investigate the existence, uniqueness, and asymptotic behaviors of mild solutions of a parabolic evolution equations on complex plane, in which the diffusion operator has the form \\(\\overline{\\Box}_{\\varphi} = \\overline{D}\\, \\overline{D}^{\\ast}\\), where \\(\\overline{D} f = \\bar{\\partial}f + \\varphi_{\\bar{z}} f\\), the function \\(\\varphi\\) is smooth and subharmonic on \\(\\mathbb{C}\\), and \\(\\overline{D}^{\\ast}\\) is the formal adjoint of \\(\\overline{D}\\).","Our method combines certain estimates of heat kernel associating with the homogeneous linear equation of Raich \\cite{raich06} and a fixed point argument."],"url":"http://arxiv.org/abs/2406.06895v1","category":"math.AP"}
{"created":"2024-06-11 01:43:45","title":"SpikePipe: Accelerated Training of Spiking Neural Networks via Inter-Layer Pipelining and Multiprocessor Scheduling","abstract":"Spiking Neural Networks (SNNs) have gained popularity due to their high energy efficiency. Prior works have proposed various methods for training SNNs, including backpropagation-based methods. Training SNNs is computationally expensive compared to their conventional counterparts and would benefit from multiprocessor hardware acceleration. This is the first paper to propose inter-layer pipelining to accelerate training in SNNs using systolic array-based processors and multiprocessor scheduling. The impact of training using delayed gradients is observed using three networks training on different datasets, showing no degradation for small networks and < 10% degradation for large networks. The mapping of various training tasks of the SNN onto systolic arrays is formulated, and the proposed scheduling method is evaluated on the three networks. The results are compared against standard pipelining algorithms. The results show that the proposed method achieves an average speedup of 1.6X compared to standard pipelining algorithms, with an upwards of 2X improvement in some cases. The incurred communication overhead due to the proposed method is less than 0.5% of the total required communication of training.","sentences":["Spiking Neural Networks (SNNs) have gained popularity due to their high energy efficiency.","Prior works have proposed various methods for training SNNs, including backpropagation-based methods.","Training SNNs is computationally expensive compared to their conventional counterparts and would benefit from multiprocessor hardware acceleration.","This is the first paper to propose inter-layer pipelining to accelerate training in SNNs using systolic array-based processors and multiprocessor scheduling.","The impact of training using delayed gradients is observed using three networks training on different datasets, showing no degradation for small networks and < 10% degradation for large networks.","The mapping of various training tasks of the SNN onto systolic arrays is formulated, and the proposed scheduling method is evaluated on the three networks.","The results are compared against standard pipelining algorithms.","The results show that the proposed method achieves an average speedup of 1.6X compared to standard pipelining algorithms, with an upwards of 2X improvement in some cases.","The incurred communication overhead due to the proposed method is less than 0.5% of the total required communication of training."],"url":"http://arxiv.org/abs/2406.06879v1","category":"eess.SP"}
{"created":"2024-06-11 01:01:46","title":"Experimentally Tractable Generation of High-Order Rogue Waves in Bose-Einstein Condensates","abstract":"In this work, we study a prototypical, experimentally accessible scenario that enables the systematic generation of so-called high-order rogue waves in atomic Bose-Einstein condensates. These waveforms lead to significantly and controllably more extreme focusing events than the famous Peregrine soliton. In one spatial dimension, we showcase conclusive numerical evidence that our scheme generates the focusing behavior associated with the first four rogue waves from the relevant hierarchy. We then extend considerations to anisotropic two-dimensional and even three-dimensional settings, establishing that the scheme can generate second order rogue waves despite the well-known limitation of finite-time blow up of focusing nonlinear Schr\\\"odinger equations.","sentences":["In this work, we study a prototypical, experimentally accessible scenario that enables the systematic generation of so-called high-order rogue waves in atomic Bose-Einstein condensates.","These waveforms lead to significantly and controllably more extreme focusing events than the famous Peregrine soliton.","In one spatial dimension, we showcase conclusive numerical evidence that our scheme generates the focusing behavior associated with the first four rogue waves from the relevant hierarchy.","We then extend considerations to anisotropic two-dimensional and even three-dimensional settings, establishing that the scheme can generate second order rogue waves despite the well-known limitation of finite-time blow up of focusing nonlinear Schr\\\"odinger equations."],"url":"http://arxiv.org/abs/2406.06869v1","category":"nlin.PS"}
{"created":"2024-06-10 23:59:02","title":"Gap theorems in Yang-Mills theory for complete four-dimensional manifolds with positive Yamabe constant","abstract":"In this paper we prove gap theorems in Yang-Mills theory for complete four-dimensional manifolds with positive Yamabe constant. We extend the results of Gursky-Kelleher-Streets to complete manifolds. We also describe the equality in the gap theorem in terms of the basic instanton, which is interesting even for compact manifolds.","sentences":["In this paper we prove gap theorems in Yang-Mills theory for complete four-dimensional manifolds with positive Yamabe constant.","We extend the results of Gursky-Kelleher-Streets to complete manifolds.","We also describe the equality in the gap theorem in terms of the basic instanton, which is interesting even for compact manifolds."],"url":"http://arxiv.org/abs/2406.06853v1","category":"math.DG"}
{"created":"2024-06-10 23:27:30","title":"Time-dependent Relativistic Hartree-Fock model with spherical symmetry","abstract":"This work establishes the time-dependent relativistic Hartree-Fock (TD-RHF) model with spherical symmetry for the first time. The time-dependent integro-differential Dirac equations are solved by expanding Dirac spinors on the spherical Dirac Woods-Saxon (DWS) basis. The numerical verification demonstrates the high conservation qualities for both the total binding energy and the particle number, as well as the time-reversal invariance of the system, which ensures the precision and reliability of the newly developed TD-RHF model. Subsequently, the isoscalar giant monopole resonance (ISGMR) mode of $^{208}$Pb is investigated using the RHF Lagrangian PKO1. The constrained energy of the ISGMR calculated by PKO1 is found to be in close agreement with the experimental data, and the strength function is similar to the results given by the relativistic Hartree-Fock plus random phase approximation. Based on the advantage of the TD-RHF model in avoiding complicated calculations of the residual interactions, the ISGMR mode of $^{208}$Pb is calculated by twelve relativistic effective Lagrangians. The results indicate that the value of the incompressibility of nuclear matter $K_\\infty$ constrained by relativistic effective Lagrangians is in the range of $237\\sim246$ MeV, which is lower than the previous investigations based on the relativistic models.","sentences":["This work establishes the time-dependent relativistic Hartree-Fock (TD-RHF) model with spherical symmetry for the first time.","The time-dependent integro-differential Dirac equations are solved by expanding Dirac spinors on the spherical Dirac Woods-Saxon (DWS) basis.","The numerical verification demonstrates the high conservation qualities for both the total binding energy and the particle number, as well as the time-reversal invariance of the system, which ensures the precision and reliability of the newly developed TD-RHF model.","Subsequently, the isoscalar giant monopole resonance (ISGMR) mode of $^{208}$Pb is investigated using the RHF Lagrangian PKO1.","The constrained energy of the ISGMR calculated by PKO1 is found to be in close agreement with the experimental data, and the strength function is similar to the results given by the relativistic Hartree-Fock plus random phase approximation.","Based on the advantage of the TD-RHF model in avoiding complicated calculations of the residual interactions, the ISGMR mode of $^{208}$Pb is calculated by twelve relativistic effective Lagrangians.","The results indicate that the value of the incompressibility of nuclear matter $K_\\infty$ constrained by relativistic effective Lagrangians is in the range of $237\\sim246$ MeV, which is lower than the previous investigations based on the relativistic models."],"url":"http://arxiv.org/abs/2406.06845v1","category":"nucl-th"}
{"created":"2024-06-10 22:39:38","title":"Gielis superformula and wildfire models","abstract":"Experimental data on the propagation of wildfires show that its short-time spread has a double semi-elliptical shape. Our main goal is to show that this shape can be accurately approximated in polar coordinates by choosing suitable parameters in Gielis superformula and, then, implemented in a geometrical model for wildfire propagation. In this model, the firefront can be determined by computing the lightlike geodesics of a specific Finsler spacetime, and we derive a concise and efficient expression of the geodesic equations in these coordinates.","sentences":["Experimental data on the propagation of wildfires show that its short-time spread has a double semi-elliptical shape.","Our main goal is to show that this shape can be accurately approximated in polar coordinates by choosing suitable parameters in Gielis superformula and, then, implemented in a geometrical model for wildfire propagation.","In this model, the firefront can be determined by computing the lightlike geodesics of a specific Finsler spacetime, and we derive a concise and efficient expression of the geodesic equations in these coordinates."],"url":"http://arxiv.org/abs/2406.06831v1","category":"math.DG"}
{"created":"2024-06-10 22:38:45","title":"Cosmological Stasis from Dynamical Scalars: Tracking Solutions and the Possibility of a Stasis-Induced Inflation","abstract":"It has recently been realized that many theories of physics beyond the Standard Model give rise to cosmological histories exhibiting extended epochs of cosmological stasis. During such epochs, the abundances of different energy components such as matter, radiation, and vacuum energy each remain fixed despite cosmological expansion. In previous analyses of the stasis phenomenon, these different energy components were modeled as fluids with fixed, unchanging equations of state. In this paper, by contrast, we consider more realistic systems involving dynamical scalars which pass through underdamping transitions as the universe expands. Indeed, such systems might be highly relevant for BSM scenarios involving higher-dimensional bulk moduli and inflatons. Remarkably, we find that stasis emerges even in such situations, despite the appearance of time-varying equations of state. Moreover, this stasis includes several new features which might have important phenomenological implications and applications. For example, in the presence of an additional \"background\" energy component, we find that the scalars evolve into a \"tracking\" stasis in which the stasis equation of state automatically tracks that of the background. This phenomenon exists even if the background has only a small initial abundance. We also discuss the intriguing possibility that our results might form the basis of a new \"Stasis Inflation\" scenario in which no ad-hoc inflaton potential is needed and in which there is no graceful-exit problem. Within such a scenario, the number of e-folds of cosmological expansion produced is directly related to the hierarchies between physical BSM mass scales. Moreover, non-zero matter and radiation abundances can be sustained throughout the inflationary epoch.","sentences":["It has recently been realized that many theories of physics beyond the Standard Model give rise to cosmological histories exhibiting extended epochs of cosmological stasis.","During such epochs, the abundances of different energy components such as matter, radiation, and vacuum energy each remain fixed despite cosmological expansion.","In previous analyses of the stasis phenomenon, these different energy components were modeled as fluids with fixed, unchanging equations of state.","In this paper, by contrast, we consider more realistic systems involving dynamical scalars which pass through underdamping transitions as the universe expands.","Indeed, such systems might be highly relevant for BSM scenarios involving higher-dimensional bulk moduli and inflatons.","Remarkably, we find that stasis emerges even in such situations, despite the appearance of time-varying equations of state.","Moreover, this stasis includes several new features which might have important phenomenological implications and applications.","For example, in the presence of an additional \"background\" energy component, we find that the scalars evolve into a \"tracking\" stasis in which the stasis equation of state automatically tracks that of the background.","This phenomenon exists even if the background has only a small initial abundance.","We also discuss the intriguing possibility that our results might form the basis of a new \"Stasis Inflation\" scenario in which no ad-hoc inflaton potential is needed and in which there is no graceful-exit problem.","Within such a scenario, the number of e-folds of cosmological expansion produced is directly related to the hierarchies between physical BSM mass scales.","Moreover, non-zero matter and radiation abundances can be sustained throughout the inflationary epoch."],"url":"http://arxiv.org/abs/2406.06830v1","category":"astro-ph.CO"}
{"created":"2024-06-10 22:27:09","title":"Predictions for a Low-mass Cutoff for the Primordial Black Hole Mass Spectrum","abstract":"In this note we outline how a modest violation in the conservation of mass during the merger of two PBHs affects the PBH mass spectrum that we previously obtained using a Boltzmann equation model for the evolution of the mass spectrum with no mass loss. We find that if the initial cosmological redshift is on the order 10$^{12}$, then the fraction of primordial holes with masses greater than $10^{3}$ solar masses appears be close to what is required to provide the seeds for galaxies. In addition we note that as a result of rapid collisions and strong coupling to electromagnetic radiation for temperatures $>$ GeV (Chapline 2018), there will be an effective low mass cutoff in the mass spectrum for PBH masses less than a certain PBH mass less than than $0.1M_{\\odot}$. We also point out that this cutoff in the mass spectrum below $\\sim 0.1 M_\\odot$ can be confirmed by combining future microlensing observations from the Roman Space Telescope and the Vera C. Rubin Observatory with astrometric observations.","sentences":["In this note we outline how a modest violation in the conservation of mass during the merger of two PBHs affects the PBH mass spectrum that we previously obtained using a Boltzmann equation model for the evolution of the mass spectrum with no mass loss.","We find that if the initial cosmological redshift is on the order 10$^{12}$, then the fraction of primordial holes with masses greater than $10^{3}$ solar masses appears be close to what is required to provide the seeds for galaxies.","In addition we note that as a result of rapid collisions and strong coupling to electromagnetic radiation for temperatures $>$ GeV (Chapline 2018), there will be an effective low mass cutoff in the mass spectrum for PBH masses less than a certain PBH mass less than than $0.1M_{\\odot}$. We also point out that this cutoff in the mass spectrum below $\\sim 0.1 M_\\odot$ can be confirmed by combining future microlensing observations from the Roman Space Telescope and the Vera C. Rubin Observatory with astrometric observations."],"url":"http://arxiv.org/abs/2406.06827v1","category":"astro-ph.CO"}
{"created":"2024-06-10 22:03:45","title":"Classification of nilpotent almost abelian Lie groups admitting left-invariant complex or symplectic structures","abstract":"We classify the nilpotent almost abelian Lie algebras admitting complex or symplectic structures. It turns out that if a nilpotent almost abelian Lie algebra admits a complex structure, then it necessarily admits a symplectic structure. Given an even dimensional almost abelian Lie algebra, we show that it always admits a complex structure when it is 2-step nilpotent and it always admits a symplectic structure when it is $k$-step nilpotent for $k=2, \\, 3$ or $4$. Several consequences of the classification theorems are obtained.","sentences":["We classify the nilpotent almost abelian Lie algebras admitting complex or symplectic structures.","It turns out that if a nilpotent almost abelian Lie algebra admits a complex structure, then it necessarily admits a symplectic structure.","Given an even dimensional almost abelian Lie algebra, we show that it always admits a complex structure when it is 2-step nilpotent and it always admits a symplectic structure when it is $k$-step nilpotent for $k=2, \\, 3$ or $4$. Several consequences of the classification theorems are obtained."],"url":"http://arxiv.org/abs/2406.06819v1","category":"math.DG"}
{"created":"2024-06-10 21:37:36","title":"On Learning what to Learn: heterogeneous observations of dynamics and establishing (possibly causal) relations among them","abstract":"Before we attempt to learn a function between two (sets of) observables of a physical process, we must first decide what the inputs and what the outputs of the desired function are going to be. Here we demonstrate two distinct, data-driven ways of initially deciding ``the right quantities'' to relate through such a function, and then proceed to learn it. This is accomplished by processing multiple simultaneous heterogeneous data streams (ensembles of time series) from observations of a physical system: multiple observation processes of the system. We thus determine (a) what subsets of observables are common between the observation processes (and therefore observable from each other, relatable through a function); and (b) what information is unrelated to these common observables, and therefore particular to each observation process, and not contributing to the desired function. Any data-driven function approximation technique can subsequently be used to learn the input-output relation, from k-nearest neighbors and Geometric Harmonics to Gaussian Processes and Neural Networks. Two particular ``twists'' of the approach are discussed. The first has to do with the identifiability of particular quantities of interest from the measurements. We now construct mappings from a single set of observations of one process to entire level sets of measurements of the process, consistent with this single set. The second attempts to relate our framework to a form of causality: if one of the observation processes measures ``now'', while the second observation process measures ``in the future'', the function to be learned among what is common across observation processes constitutes a dynamical model for the system evolution.","sentences":["Before we attempt to learn a function between two (sets of) observables of a physical process, we must first decide what the inputs and what the outputs of the desired function are going to be.","Here we demonstrate two distinct, data-driven ways of initially deciding ``the right quantities'' to relate through such a function, and then proceed to learn it.","This is accomplished by processing multiple simultaneous heterogeneous data streams (ensembles of time series) from observations of a physical system: multiple observation processes of the system.","We thus determine (a) what subsets of observables are common between the observation processes (and therefore observable from each other, relatable through a function); and (b) what information is unrelated to these common observables, and therefore particular to each observation process, and not contributing to the desired function.","Any data-driven function approximation technique can subsequently be used to learn the input-output relation, from k-nearest neighbors and Geometric Harmonics to Gaussian Processes and Neural Networks.","Two particular ``twists'' of the approach are discussed.","The first has to do with the identifiability of particular quantities of interest from the measurements.","We now construct mappings from a single set of observations of one process to entire level sets of measurements of the process, consistent with this single set.","The second attempts to relate our framework to a form of causality: if one of the observation processes measures ``now'', while the second observation process measures ``in the future'', the function to be learned among what is common across observation processes constitutes a dynamical model for the system evolution."],"url":"http://arxiv.org/abs/2406.06812v1","category":"cs.LG"}
{"created":"2024-06-10 21:34:43","title":"Learning Continually by Spectral Regularization","abstract":"Loss of plasticity is a phenomenon where neural networks become more difficult to train during the course of learning. Continual learning algorithms seek to mitigate this effect by sustaining good predictive performance while maintaining network trainability. We develop new techniques for improving continual learning by first reconsidering how initialization can ensure trainability during early phases of learning. From this perspective, we derive new regularization strategies for continual learning that ensure beneficial initialization properties are better maintained throughout training. In particular, we investigate two new regularization techniques for continual learning: (i) Wasserstein regularization toward the initial weight distribution, which is less restrictive than regularizing toward initial weights; and (ii) regularizing weight matrix singular values, which directly ensures gradient diversity is maintained throughout training. We present an experimental analysis that shows these alternative regularizers can improve continual learning performance across a range of supervised learning tasks and model architectures. The alternative regularizers prove to be less sensitive to hyperparameters while demonstrating better training in individual tasks, sustaining trainability as new tasks arrive, and achieving better generalization performance.","sentences":["Loss of plasticity is a phenomenon where neural networks become more difficult to train during the course of learning.","Continual learning algorithms seek to mitigate this effect by sustaining good predictive performance while maintaining network trainability.","We develop new techniques for improving continual learning by first reconsidering how initialization can ensure trainability during early phases of learning.","From this perspective, we derive new regularization strategies for continual learning that ensure beneficial initialization properties are better maintained throughout training.","In particular, we investigate two new regularization techniques for continual learning: (i) Wasserstein regularization toward the initial weight distribution, which is less restrictive than regularizing toward initial weights; and (ii) regularizing weight matrix singular values, which directly ensures gradient diversity is maintained throughout training.","We present an experimental analysis that shows these alternative regularizers can improve continual learning performance across a range of supervised learning tasks and model architectures.","The alternative regularizers prove to be less sensitive to hyperparameters while demonstrating better training in individual tasks, sustaining trainability as new tasks arrive, and achieving better generalization performance."],"url":"http://arxiv.org/abs/2406.06811v1","category":"cs.LG"}
{"created":"2024-06-10 20:23:04","title":"ComFeAT: Combination of Neural and Spectral Features for Improved Depression Detection","abstract":"In this work, we focus on the detection of depression through speech analysis. Previous research has widely explored features extracted from pre-trained models (PTMs) primarily trained for paralinguistic tasks. Although these features have led to sufficient advances in speech-based depression detection, their performance declines in real-world settings. To address this, in this paper, we introduce ComFeAT, an application that employs a CNN model trained on a combination of features extracted from PTMs, a.k.a. neural features and spectral features to enhance depression detection. Spectral features are robust to domain variations, but, they are not as good as neural features in performance, suprisingly, combining them shows complementary behavior and improves over both neural and spectral features individually. The proposed method also improves over previous state-of-the-art (SOTA) works on E-DAIC benchmark.","sentences":["In this work, we focus on the detection of depression through speech analysis.","Previous research has widely explored features extracted from pre-trained models (PTMs) primarily trained for paralinguistic tasks.","Although these features have led to sufficient advances in speech-based depression detection, their performance declines in real-world settings.","To address this, in this paper, we introduce ComFeAT, an application that employs a CNN model trained on a combination of features extracted from PTMs, a.k.a. neural features and spectral features to enhance depression detection.","Spectral features are robust to domain variations, but, they are not as good as neural features in performance, suprisingly, combining them shows complementary behavior and improves over both neural and spectral features individually.","The proposed method also improves over previous state-of-the-art (SOTA) works on E-DAIC benchmark."],"url":"http://arxiv.org/abs/2406.06774v1","category":"eess.AS"}
{"created":"2024-06-10 20:03:51","title":"ULV: A robust statistical method for clustered data, with applications to multisubject, single-cell omics data","abstract":"Molecular and genomic technological advancements have greatly enhanced our understanding of biological processes by allowing us to quantify key biological variables such as gene expression, protein levels, and microbiome compositions. These breakthroughs have enabled us to achieve increasingly higher levels of resolution in our measurements, exemplified by our ability to comprehensively profile biological information at the single-cell level. However, the analysis of such data faces several critical challenges: limited number of individuals, non-normality, potential dropouts, outliers, and repeated measurements from the same individual. In this article, we propose a novel method, which we call U-statistic based latent variable (ULV). Our proposed method takes advantage of the robustness of rank-based statistics and exploits the statistical efficiency of parametric methods for small sample sizes. It is a computationally feasible framework that addresses all the issues mentioned above simultaneously. An additional advantage of ULV is its flexibility in modeling various types of single-cell data, including both RNA and protein abundance. The usefulness of our method is demonstrated in two studies: a single-cell proteomics study of acute myelogenous leukemia (AML) and a single-cell RNA study of COVID-19 symptoms. In the AML study, ULV successfully identified differentially expressed proteins that would have been missed by the pseudobulk version of the Wilcoxon rank-sum test. In the COVID-19 study, ULV identified genes associated with covariates such as age and gender, and genes that would be missed without adjusting for covariates. The differentially expressed genes identified by our method are less biased toward genes with high expression levels. Furthermore, ULV identified additional gene pathways likely contributing to the mechanisms of COVID-19 severity.","sentences":["Molecular and genomic technological advancements have greatly enhanced our understanding of biological processes by allowing us to quantify key biological variables such as gene expression, protein levels, and microbiome compositions.","These breakthroughs have enabled us to achieve increasingly higher levels of resolution in our measurements, exemplified by our ability to comprehensively profile biological information at the single-cell level.","However, the analysis of such data faces several critical challenges: limited number of individuals, non-normality, potential dropouts, outliers, and repeated measurements from the same individual.","In this article, we propose a novel method, which we call U-statistic based latent variable (ULV).","Our proposed method takes advantage of the robustness of rank-based statistics and exploits the statistical efficiency of parametric methods for small sample sizes.","It is a computationally feasible framework that addresses all the issues mentioned above simultaneously.","An additional advantage of ULV is its flexibility in modeling various types of single-cell data, including both RNA and protein abundance.","The usefulness of our method is demonstrated in two studies: a single-cell proteomics study of acute myelogenous leukemia (AML) and a single-cell RNA study of COVID-19 symptoms.","In the AML study, ULV successfully identified differentially expressed proteins that would have been missed by the pseudobulk version of the Wilcoxon rank-sum test.","In the COVID-19 study, ULV identified genes associated with covariates such as age and gender, and genes that would be missed without adjusting for covariates.","The differentially expressed genes identified by our method are less biased toward genes with high expression levels.","Furthermore, ULV identified additional gene pathways likely contributing to the mechanisms of COVID-19 severity."],"url":"http://arxiv.org/abs/2406.06767v1","category":"stat.ME"}
{"created":"2024-06-10 20:01:14","title":"Duality and the equations of Rees rings and tangent algebras","abstract":"Let $E$ be a module of projective dimension one over a Noetherian ring $R$ and consider its Rees algebra $\\mathcal{R}(E)$. We study this ring as a quotient of the symmetric algebra $\\mathcal{S}(E)$ and consider the ideal $\\mathcal{A}$ defining this quotient. In the case that $\\mathcal{S}(E)$ is a complete intersection ring, we employ a duality between $\\mathcal{A}$ and $\\mathcal{S}(E)$ in order to study the Rees ring $\\mathcal{R}(E)$ in multiple settings. In particular, when $R$ is a complete intersection ring defined by quadrics, we consider its module of K\\\"ahler differentials $\\Omega_{R/k}$ and its associated tangent algebras.","sentences":["Let $E$ be a module of projective dimension one over a Noetherian ring $R$ and consider its Rees algebra $\\mathcal{R}(E)$. We study this ring as a quotient of the symmetric algebra $\\mathcal{S}(E)$ and consider the ideal $\\mathcal{A}$ defining this quotient.","In the case that $\\mathcal{S}(E)$ is a complete intersection ring, we employ a duality between $\\mathcal{A}$ and $\\mathcal{S}(E)$ in order to study the Rees ring $\\mathcal{R}(E)$ in multiple settings.","In particular, when $R$ is a complete intersection ring defined by quadrics, we consider its module of K\\\"ahler differentials $\\Omega_{R/k}$ and its associated tangent algebras."],"url":"http://arxiv.org/abs/2406.06766v1","category":"math.AC"}
{"created":"2024-06-10 19:34:07","title":"Optimal Federated Learning for Nonparametric Regression with Heterogeneous Distributed Differential Privacy Constraints","abstract":"This paper studies federated learning for nonparametric regression in the context of distributed samples across different servers, each adhering to distinct differential privacy constraints. The setting we consider is heterogeneous, encompassing both varying sample sizes and differential privacy constraints across servers. Within this framework, both global and pointwise estimation are considered, and optimal rates of convergence over the Besov spaces are established.   Distributed privacy-preserving estimators are proposed and their risk properties are investigated. Matching minimax lower bounds, up to a logarithmic factor, are established for both global and pointwise estimation. Together, these findings shed light on the tradeoff between statistical accuracy and privacy preservation. In particular, we characterize the compromise not only in terms of the privacy budget but also concerning the loss incurred by distributing data within the privacy framework as a whole. This insight captures the folklore wisdom that it is easier to retain privacy in larger samples, and explores the differences between pointwise and global estimation under distributed privacy constraints.","sentences":["This paper studies federated learning for nonparametric regression in the context of distributed samples across different servers, each adhering to distinct differential privacy constraints.","The setting we consider is heterogeneous, encompassing both varying sample sizes and differential privacy constraints across servers.","Within this framework, both global and pointwise estimation are considered, and optimal rates of convergence over the Besov spaces are established.   ","Distributed privacy-preserving estimators are proposed and their risk properties are investigated.","Matching minimax lower bounds, up to a logarithmic factor, are established for both global and pointwise estimation.","Together, these findings shed light on the tradeoff between statistical accuracy and privacy preservation.","In particular, we characterize the compromise not only in terms of the privacy budget but also concerning the loss incurred by distributing data within the privacy framework as a whole.","This insight captures the folklore wisdom that it is easier to retain privacy in larger samples, and explores the differences between pointwise and global estimation under distributed privacy constraints."],"url":"http://arxiv.org/abs/2406.06755v1","category":"math.ST"}
{"created":"2024-06-10 19:32:12","title":"Homogeneous G2 and Sasakian instantons on the Stiefel 7-manifold","abstract":"We study homogeneous instantons on the seven dimensional Stiefel manifold V in the context of G2 and Sasakian geometry. According to the reductive decomposition of V we provide an explicit description of all invariant G2 and Sasakian structures. In particular, we characterise the invariant G2- structures inducing a Sasakian metric, among which the well known nearly parallel G2-structure (Sasaki- Einstein) is included. As a consequence, we classify the invariant connections on homogeneous principal bundles over V with gauge group U(1) and SO(3), satisfying either the G2 or the Sasakian instanton condition. Finally, we analyse the Yang Mills condition for those invariant connections.","sentences":["We study homogeneous instantons on the seven dimensional Stiefel manifold V in the context of G2 and Sasakian geometry.","According to the reductive decomposition of V we provide an explicit description of all invariant G2 and Sasakian structures.","In particular, we characterise the invariant G2- structures inducing a Sasakian metric, among which the well known nearly parallel G2-structure (Sasaki- Einstein) is included.","As a consequence, we classify the invariant connections on homogeneous principal bundles over V with gauge group U(1) and SO(3), satisfying either the G2 or the Sasakian instanton condition.","Finally, we analyse the Yang Mills condition for those invariant connections."],"url":"http://arxiv.org/abs/2406.06753v1","category":"math.DG"}
{"created":"2024-06-10 19:12:14","title":"Universal properties of the evolution of the Universe in modified loop quantum cosmology","abstract":"In this paper, we systematically study the evolution of the Universe in the framework of a modified loop quantum cosmological model (mLQC-I) with various inflationary potentials, including chaotic, Starobinsky, generalized Starobinsky, polynomials of the first and second kinds, generalized T- models and natural inflation. In all these models, the big bang singularity is represented by a quantum bounce, and the evolution of the Universe both before and after the bounce is universal and weakly depends on the inflationary potentials, as long as the evolution is dominated by the kinetic energy of the inflaton at the bounce. In particular, the evolution in the pre-bounce region can be universally divided into three different phases: pre-bouncing, pre-transition, and pre-de Sitter. The pre-bouncing phase occurs immediately before the quantum bounce, during which the evolution of the Universe is dominated by the kinetic energy of the inflaton. Thus, the equation of state of the inflaton is about one, w = 1. Soon, the inflation potential takes over, so w rapidly falls from one to negative one. This pre-transition phase is very short and quickly turns into the pre-de Sitter phase, whereby the effective cosmological constant with a Planck size takes over and dominates the rest of the contracting phase. In the entire pre-bounce regime, the evolution of the expansion factor and the inflaton can be approximated by analytical solutions, which are universal and independent of the inflation potentials.","sentences":["In this paper, we systematically study the evolution of the Universe in the framework of a modified loop quantum cosmological model (mLQC-I) with various inflationary potentials, including chaotic, Starobinsky, generalized Starobinsky, polynomials of the first and second kinds, generalized T- models and natural inflation.","In all these models, the big bang singularity is represented by a quantum bounce, and the evolution of the Universe both before and after the bounce is universal and weakly depends on the inflationary potentials, as long as the evolution is dominated by the kinetic energy of the inflaton at the bounce.","In particular, the evolution in the pre-bounce region can be universally divided into three different phases: pre-bouncing, pre-transition, and pre-de Sitter.","The pre-bouncing phase occurs immediately before the quantum bounce, during which the evolution of the Universe is dominated by the kinetic energy of the inflaton.","Thus, the equation of state of the inflaton is about one, w = 1.","Soon, the inflation potential takes over, so w rapidly falls from one to negative one.","This pre-transition phase is very short and quickly turns into the pre-de Sitter phase, whereby the effective cosmological constant with a Planck size takes over and dominates the rest of the contracting phase.","In the entire pre-bounce regime, the evolution of the expansion factor and the inflaton can be approximated by analytical solutions, which are universal and independent of the inflation potentials."],"url":"http://arxiv.org/abs/2406.06745v1","category":"gr-qc"}
{"created":"2024-06-10 18:54:47","title":"Space of circle patterns on tori and its symplectic form","abstract":"We consider circle patterns on closed tori equipped with complex projective structures. There is an embedding of the space of circle patterns to the Teichm\\\"{u}ller space of a punctured surface. Via the embedding, the Weil-Petersson symplectic form is pulled back to the space of circle patterns. We investigate its non-degeneracy. On the other hand, we also complete a conjecture that the space of circle patterns is homeomorphic to the Teichm\\\"{u}ller space of the closed torus.","sentences":["We consider circle patterns on closed tori equipped with complex projective structures.","There is an embedding of the space of circle patterns to the Teichm\\\"{u}ller space of a punctured surface.","Via the embedding, the Weil-Petersson symplectic form is pulled back to the space of circle patterns.","We investigate its non-degeneracy.","On the other hand, we also complete a conjecture that the space of circle patterns is homeomorphic to the Teichm\\\"{u}ller space of the closed torus."],"url":"http://arxiv.org/abs/2406.06733v1","category":"math.GT"}
{"created":"2024-06-10 18:41:57","title":"Full transmission of vectorial waves through 3D multiple-scattering media","abstract":"A striking prediction from the random matrix theory in mesoscopic physics is the existence of \"open channels\": waves that can use multipath interference to achieve perfect transmission across an opaque disordered medium even in the multiple-scattering regime. Realization of such open channels requires a coherent control of the complete incident wavefront. To date, the open channels have only been demonstrated in scalar two-dimensional (2D) structures, both experimentally and with numerical studies. Here, we utilize a recently proposed \"augmented partial factorization\" full-wave simulation method to compute the scattering matrix from 3D vectorial Maxwell's equations and demonstrate the existence of open channels in 3D disordered media. We examine the spatial profile of such open channels, demonstrate the existence of a bimodal transmission eigenvalue distribution with full control, and study the effects of incomplete polarization control and of a finite illumination area. This study confirms the validity of the random matrix theory in vectorial systems. The simulation framework provides full access to the complex multi-channel wave transport in 3D disordered systems, filling the gap left by experimental capabilities.","sentences":["A striking prediction from the random matrix theory in mesoscopic physics is the existence of \"open channels\": waves that can use multipath interference to achieve perfect transmission across an opaque disordered medium even in the multiple-scattering regime.","Realization of such open channels requires a coherent control of the complete incident wavefront.","To date, the open channels have only been demonstrated in scalar two-dimensional (2D) structures, both experimentally and with numerical studies.","Here, we utilize a recently proposed \"augmented partial factorization\" full-wave simulation method to compute the scattering matrix from 3D vectorial Maxwell's equations and demonstrate the existence of open channels in 3D disordered media.","We examine the spatial profile of such open channels, demonstrate the existence of a bimodal transmission eigenvalue distribution with full control, and study the effects of incomplete polarization control and of a finite illumination area.","This study confirms the validity of the random matrix theory in vectorial systems.","The simulation framework provides full access to the complex multi-channel wave transport in 3D disordered systems, filling the gap left by experimental capabilities."],"url":"http://arxiv.org/abs/2406.06727v1","category":"physics.optics"}
{"created":"2024-06-10 18:41:45","title":"The Price of Cognition and Replicator Equations in Parallel Neural Networks","abstract":"In this paper, we are aiming to propose a novel mathematical model that studies the dynamics of synaptic damage in terms of concentrations of toxic neuropeptides/neurotransmitters during neurotransmission processes. Our primary objective is to employ Wardrop's first and second principles within a neural network of the brain. In order to comprehensively incorporate Wardrop's first and second principles into the neural network of the brain, we introduce two novel concepts: \\textit{neuropeptide's (neurotransmitter's) equilibrium} and \\textit{synapses optimum}. The \\textit{neuropeptide/neurotransmitter equilibrium} refers to \\textit{a distribution of toxic neuropeptides/neurotransmitters that leads to uniform damage across all synaptic links}. Meanwhile, \\textit{synapses optimum} is \\textit{the most desirable distribution of toxic neuropeptides/neurotransmitters that minimizes the cumulative damage experienced by all synapses}. In the context of a neural network within the brain, an analogue of the price of anarchy is \\textit{the price of cognition} which is \\textit{the most unfavorable ratio between the overall impairment caused by toxic neuropeptide's (neurotransmitter's) equilibrium in comparison to the optimal state of synapses (synapses optimum)}. To put it differently, \\textit{the price of cognition} measures \\textit{the loss of cognitive ability resulting from increased concentrations of toxic neuropeptides/neurotransmitters}. Additionally, a replicator equation is proposed within this framework that leads to the establishment of the synapses optimum during the neurotransmission process.","sentences":["In this paper, we are aiming to propose a novel mathematical model that studies the dynamics of synaptic damage in terms of concentrations of toxic neuropeptides/neurotransmitters during neurotransmission processes.","Our primary objective is to employ Wardrop's first and second principles within a neural network of the brain.","In order to comprehensively incorporate Wardrop's first and second principles into the neural network of the brain, we introduce two novel concepts: \\textit{neuropeptide's (neurotransmitter's) equilibrium} and \\textit{synapses optimum}.","The \\textit{neuropeptide/neurotransmitter equilibrium} refers to \\textit{a distribution of toxic neuropeptides/neurotransmitters that leads to uniform damage across all synaptic links}.","Meanwhile, \\textit{synapses optimum} is \\textit{the most desirable distribution of toxic neuropeptides/neurotransmitters that minimizes the cumulative damage experienced by all synapses}.","In the context of a neural network within the brain, an analogue of the price of anarchy is \\textit{the price of cognition} which is \\textit{the most unfavorable ratio between the overall impairment caused by toxic neuropeptide's (neurotransmitter's) equilibrium in comparison to the optimal state of synapses (synapses optimum)}.","To put it differently, \\textit{the price of cognition} measures \\textit{the loss of cognitive ability resulting from increased concentrations of toxic neuropeptides/neurotransmitters}.","Additionally, a replicator equation is proposed within this framework that leads to the establishment of the synapses optimum during the neurotransmission process."],"url":"http://arxiv.org/abs/2406.06726v1","category":"q-bio.NC"}
{"created":"2024-06-10 18:33:39","title":"On smooth and peaked traveling waves in a local model for shallow water waves","abstract":"We introduce a new model equation for Stokes gravity waves based on conformal transformations of Euler's equations. The local version of the model equation is relevant for dynamics of shallow water waves. It allows us to characterize the traveling periodic waves both in the case of smooth and peaked waves and to solve the existence problem exactly, albeit not in elementary functions. Spectral stability of smooth waves with respect to co-periodic perturbations is proven analytically based on the exact count of eigenvalues in a constrained spectral problem.","sentences":["We introduce a new model equation for Stokes gravity waves based on conformal transformations of Euler's equations.","The local version of the model equation is relevant for dynamics of shallow water waves.","It allows us to characterize the traveling periodic waves both in the case of smooth and peaked waves and to solve the existence problem exactly, albeit not in elementary functions.","Spectral stability of smooth waves with respect to co-periodic perturbations is proven analytically based on the exact count of eigenvalues in a constrained spectral problem."],"url":"http://arxiv.org/abs/2406.06722v1","category":"math.AP"}
{"created":"2024-06-10 18:28:01","title":"TORAX: A Fast and Differentiable Tokamak Transport Simulator in JAX","abstract":"We present TORAX, a new, open-source, differentiable tokamak core transport simulator implemented in Python using the JAX framework. TORAX solves the coupled equations for ion heat transport, electron heat transport, particle transport, and current diffusion, incorporating modular physics-based and ML models. JAX's just-in-time compilation ensures fast runtimes, while its automatic differentiation capability enables gradient-based optimization workflows and simplifies the use of Jacobian-based PDE solvers. Coupling to ML-surrogates of physics models is greatly facilitated by JAX's intrinsic support for neural network development and inference. TORAX is verified against the established RAPTOR code, demonstrating agreement in simulated plasma profiles. TORAX provides a powerful and versatile tool for accelerating research in tokamak scenario modeling, pulse design, and control.","sentences":["We present TORAX, a new, open-source, differentiable tokamak core transport simulator implemented in Python using the JAX framework.","TORAX solves the coupled equations for ion heat transport, electron heat transport, particle transport, and current diffusion, incorporating modular physics-based and ML models.","JAX's just-in-time compilation ensures fast runtimes, while its automatic differentiation capability enables gradient-based optimization workflows and simplifies the use of Jacobian-based PDE solvers.","Coupling to ML-surrogates of physics models is greatly facilitated by JAX's intrinsic support for neural network development and inference.","TORAX is verified against the established RAPTOR code, demonstrating agreement in simulated plasma profiles.","TORAX provides a powerful and versatile tool for accelerating research in tokamak scenario modeling, pulse design, and control."],"url":"http://arxiv.org/abs/2406.06718v1","category":"physics.plasm-ph"}
{"created":"2024-06-10 18:17:05","title":"Geometry of Carrollian Stretched Horizons","abstract":"In this paper, we present a comprehensive toolbox for studying Carrollian stretched horizons, encompassing their geometry, dynamics, symplectic geometry, symmetries, and corresponding Noether charges. We introduce a precise definition of ruled stretched Carrollian structures (sCarrollian structures) on any surface, generalizing the conventional Carrollian structures of null surfaces, along with the notions of sCarrollian connection and sCarrollian stress tensor. Our approach unifies the sCarrollian (intrinsic) and stretched horizon (embedding) perspectives, providing a universal framework for any causal surface, whether timelike or null. We express the Einstein equations in sCarrollian variables and discuss the phase space symplectic structure of the sCarrollian geometry. Through Noether's theorem, we derive the Einstein equation and canonical charge and compute the evolution of the canonical charge along the transverse (radial) direction. The latter can be interpreted as a spin-2 symmetry charge.   Our framework establishes a novel link between gravity on stretched horizons and Carrollian fluid dynamics and unifies various causal surfaces studied in the literature, including non-expanding and isolated horizons. We expect this work to provide insights into the hydrodynamical description of black holes and the quantization of null surfaces.","sentences":["In this paper, we present a comprehensive toolbox for studying Carrollian stretched horizons, encompassing their geometry, dynamics, symplectic geometry, symmetries, and corresponding Noether charges.","We introduce a precise definition of ruled stretched Carrollian structures (sCarrollian structures) on any surface, generalizing the conventional Carrollian structures of null surfaces, along with the notions of sCarrollian connection and sCarrollian stress tensor.","Our approach unifies the sCarrollian (intrinsic) and stretched horizon (embedding) perspectives, providing a universal framework for any causal surface, whether timelike or null.","We express the Einstein equations in sCarrollian variables and discuss the phase space symplectic structure of the sCarrollian geometry.","Through Noether's theorem, we derive the Einstein equation and canonical charge and compute the evolution of the canonical charge along the transverse (radial) direction.","The latter can be interpreted as a spin-2 symmetry charge.   ","Our framework establishes a novel link between gravity on stretched horizons and Carrollian fluid dynamics and unifies various causal surfaces studied in the literature, including non-expanding and isolated horizons.","We expect this work to provide insights into the hydrodynamical description of black holes and the quantization of null surfaces."],"url":"http://arxiv.org/abs/2406.06709v1","category":"gr-qc"}
{"created":"2024-06-10 18:15:32","title":"Discovery of differential equations using sparse state and parameter regression","abstract":"This paper proposes a sparse regression strategy for discovery of ordinary and partial differential equations from incomplete and noisy data. Inference is performed over both equation parameters and state variables using a statistically motivated likelihood function. Sparsity is enforced by a selection algorithm which iteratively removes terms and compares models using statistical information criteria. Large scale optimization is performed using a second-order variant of the Levenberg-Marquardt method, where the gradient and Hessian are computed via automatic differentiation. Illustrations involving canonical systems of ordinary and partial differential equations are used to demonstrate the flexibility and robustness of the approach. Accurate reconstruction of systems is found to be possible even in extreme cases of limited data and large observation noise.","sentences":["This paper proposes a sparse regression strategy for discovery of ordinary and partial differential equations from incomplete and noisy data.","Inference is performed over both equation parameters and state variables using a statistically motivated likelihood function.","Sparsity is enforced by a selection algorithm which iteratively removes terms and compares models using statistical information criteria.","Large scale optimization is performed using a second-order variant of the Levenberg-Marquardt method, where the gradient and Hessian are computed via automatic differentiation.","Illustrations involving canonical systems of ordinary and partial differential equations are used to demonstrate the flexibility and robustness of the approach.","Accurate reconstruction of systems is found to be possible even in extreme cases of limited data and large observation noise."],"url":"http://arxiv.org/abs/2406.06707v1","category":"math.DS"}
{"created":"2024-06-10 18:08:07","title":"The largest metallicity difference in twin systems: high-precision abundance analysis of the benchmark pair Krios & Kronos","abstract":"Aims.We conduct a high-precision differential abundance analysis of the remarkable binary system HD 240429/30 (Krios and Kronos, respectively), whose difference in metallicity is one of the highest detected in systems with similar components to date (approximately 0.20 dex). A condensation temperature TC trend study was performed to search for possible chemical signatures of planet formation. In addition, other potential scenarios have been proposed to explain this disparity. Methods. Fundamental atmospheric parameters (Te f f , log g, [Fe/H], vturb) were calculated using the latest version of the FUNDPAR code employing high resolution MAROON-X spectra. We applied a full line-by-line differential technique to measure the abundances of 26 elements in both stars with equivalent widths and spectral synthesis taking advantage of the non-solar scaled opacities. Results.We found a difference in metallicity of approximately 0.230 dex, being Kronos more metal rich than Krios. The analysis encompassed the examination of the diffusion effect and primordial chemical differences, concluding that the observed chemical discrepancies in the binary system cannot be solely attributed to any of these processes. The results also shown a noticeable excess of Li by approximately 0.56 dex in Kronos, and an enhancement of refractories with respect to Krios. A photometric study with TESS data was carried out, without finding any signal of possible transiting planets around the stars. Several potential planet formation scenarios were also explored to account for the observed excess in both metallicity and lithium in Kronos. Planetary engulfment is a plausible explanation, considering the ingestion of an exceptionally large mass, approximately approximately 27.8M_Earth, but no scenario is definitively ruled out.","sentences":["Aims.","We conduct a high-precision differential abundance analysis of the remarkable binary system HD 240429/30 (Krios and Kronos, respectively), whose difference in metallicity is one of the highest detected in systems with similar components to date (approximately 0.20 dex).","A condensation temperature TC trend study was performed to search for possible chemical signatures of planet formation.","In addition, other potential scenarios have been proposed to explain this disparity.","Methods.","Fundamental atmospheric parameters (Te f f , log g, [Fe/H], vturb) were calculated using the latest version of the FUNDPAR code employing high resolution MAROON-X spectra.","We applied a full line-by-line differential technique to measure the abundances of 26 elements in both stars with equivalent widths and spectral synthesis taking advantage of the non-solar scaled opacities.","Results.","We found a difference in metallicity of approximately 0.230 dex, being Kronos more metal rich than Krios.","The analysis encompassed the examination of the diffusion effect and primordial chemical differences, concluding that the observed chemical discrepancies in the binary system cannot be solely attributed to any of these processes.","The results also shown a noticeable excess of Li by approximately 0.56 dex in Kronos, and an enhancement of refractories with respect to Krios.","A photometric study with TESS data was carried out, without finding any signal of possible transiting planets around the stars.","Several potential planet formation scenarios were also explored to account for the observed excess in both metallicity and lithium in Kronos.","Planetary engulfment is a plausible explanation, considering the ingestion of an exceptionally large mass, approximately approximately 27.8M_Earth, but no scenario is definitively ruled out."],"url":"http://arxiv.org/abs/2406.06705v1","category":"astro-ph.EP"}
{"created":"2024-06-10 18:00:29","title":"On the Equivalence of Generalized Ricci Curvatures","abstract":"We prove the equivalence between the several notions of generalized Ricci curvature found in the literature. As an application, we characterize when the total generalized Ricci tensor is symmetric.","sentences":["We prove the equivalence between the several notions of generalized Ricci curvature found in the literature.","As an application, we characterize when the total generalized Ricci tensor is symmetric."],"url":"http://arxiv.org/abs/2406.06695v1","category":"math.DG"}
{"created":"2024-06-10 18:00:02","title":"Kinematics and Dynamics of the Galactic Bar revealed by Gaia Long Period Variables","abstract":"We take low-amplitude, long period variable (LA-LPV) candidates in Gaia DR3 as tracers of the kinematics and dynamics of the Milky Way bar. LA-LPVs, like other LPVs, have high luminosities and follow a tight period-luminosity relation, but unlike e.g. Mira variables, their radial velocity measurements are reliable due to their smaller pulsation amplitudes. We supplement the Gaia astrometric and radial velocity measurements with distance moduli assigned using a period-luminosity relation to acquire full 6D phase space information. The assigned distances are validated by comparing to geometric distances and StarHorse distances, which shows biases less than $\\sim5\\%$. Our sample provides an unprecedented panoramic picture of the inner Galaxy with minimal selection function effects. We map the kinematics of the inner Milky Way and find a significant kinematic signature corresponding to the Galactic bar. We measure the pattern speed of the Galactic bar using the continuity equation and find $\\Omega_{\\rm b}=34.1\\pm2.4$ km s$^{-1}$ kpc$^{-1}$. We develop a simple, robust and model-independent method to measure the dynamical length of the bar using only kinematics and find $R_{\\rm b}\\sim4.0$ kpc. We validate both measurements using N-body simulations. Assuming knowledge of the gravitational potential of the inner Milky Way, we analyse the orbital structure of the Galactic bar using orbital frequency ratios. The $x_1$ orbits are the dominant bar-supporting orbital family in our sample. Amongst the selected bar stars, the $x_1 v_1$ or \"banana\" orbits constitute a larger fraction ($\\sim 15\\%$) than other orbital families in the bar, implying that they are the dominant family contributing to the Galactic X-shape, although contributions from other orbital families are present.","sentences":["We take low-amplitude, long period variable (LA-LPV) candidates in Gaia DR3 as tracers of the kinematics and dynamics of the Milky Way bar.","LA-LPVs, like other LPVs, have high luminosities and follow a tight period-luminosity relation, but unlike e.g. Mira variables, their radial velocity measurements are reliable due to their smaller pulsation amplitudes.","We supplement the Gaia astrometric and radial velocity measurements with distance moduli assigned using a period-luminosity relation to acquire full 6D phase space information.","The assigned distances are validated by comparing to geometric distances and StarHorse distances, which shows biases less than $\\sim5\\%$. Our sample provides an unprecedented panoramic picture of the inner Galaxy with minimal selection function effects.","We map the kinematics of the inner Milky Way and find a significant kinematic signature corresponding to the Galactic bar.","We measure the pattern speed of the Galactic bar using the continuity equation and find $\\Omega_{\\rm b}=34.1\\pm2.4$ km s$^{-1}$ kpc$^{-1}$. We develop a simple, robust and model-independent method to measure the dynamical length of the bar using only kinematics and find $R_{\\rm b}\\sim4.0$ kpc.","We validate both measurements using N-body simulations.","Assuming knowledge of the gravitational potential of the inner Milky Way, we analyse the orbital structure of the Galactic bar using orbital frequency ratios.","The $x_1$ orbits are the dominant bar-supporting orbital family in our sample.","Amongst the selected bar stars, the $x_1 v_1$ or \"banana\" orbits constitute a larger fraction ($\\sim 15\\%$) than other orbital families in the bar, implying that they are the dominant family contributing to the Galactic X-shape, although contributions from other orbital families are present."],"url":"http://arxiv.org/abs/2406.06678v1","category":"astro-ph.GA"}
{"created":"2024-06-10 18:00:01","title":"Critical Filaments and Superconductivity in Quasiperiodic Twisted Bilayer Graphene","abstract":"Moir\\'e materials can exhibit electronic topological features yet are inherently quasiperiodic. Nonetheless, the localizing tendency of quasiperiodicity can be prevented by topology. We consider a quasiperiodic variant of the chiral Bistritzer-MacDonald model for twisted bilayer graphene with two incommensurate moir\\'e potentials. We observe \"filaments\" linking magic angles with enhanced density of states and fractal wave functions that evade localization; states away from the filaments mimic fractal surface states of dirty topological superconductors. We demonstrate that topological quasiperiodicity can broadly enhance superconductivity without magic-angle fine-tuning.","sentences":["Moir\\'e materials can exhibit electronic topological features yet are inherently quasiperiodic.","Nonetheless, the localizing tendency of quasiperiodicity can be prevented by topology.","We consider a quasiperiodic variant of the chiral Bistritzer-MacDonald model for twisted bilayer graphene with two incommensurate moir\\'e potentials.","We observe \"filaments\" linking magic angles with enhanced density of states and fractal wave functions that evade localization; states away from the filaments mimic fractal surface states of dirty topological superconductors.","We demonstrate that topological quasiperiodicity can broadly enhance superconductivity without magic-angle fine-tuning."],"url":"http://arxiv.org/abs/2406.06676v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-11 17:58:54","title":"BAKU: An Efficient Transformer for Multi-Task Policy Learning","abstract":"Training generalist agents capable of solving diverse tasks is challenging, often requiring large datasets of expert demonstrations. This is particularly problematic in robotics, where each data point requires physical execution of actions in the real world. Thus, there is a pressing need for architectures that can effectively leverage the available training data. In this work, we present BAKU, a simple transformer architecture that enables efficient learning of multi-task robot policies. BAKU builds upon recent advancements in offline imitation learning and meticulously combines observation trunks, action chunking, multi-sensory observations, and action heads to substantially improve upon prior work. Our experiments on 129 simulated tasks across LIBERO, Meta-World suite, and the Deepmind Control suite exhibit an overall 18% absolute improvement over RT-1 and MT-ACT, with a 36% improvement on the harder LIBERO benchmark. On 30 real-world manipulation tasks, given an average of just 17 demonstrations per task, BAKU achieves a 91% success rate. Videos of the robot are best viewed at https://baku-robot.github.io/.","sentences":["Training generalist agents capable of solving diverse tasks is challenging, often requiring large datasets of expert demonstrations.","This is particularly problematic in robotics, where each data point requires physical execution of actions in the real world.","Thus, there is a pressing need for architectures that can effectively leverage the available training data.","In this work, we present BAKU, a simple transformer architecture that enables efficient learning of multi-task robot policies.","BAKU builds upon recent advancements in offline imitation learning and meticulously combines observation trunks, action chunking, multi-sensory observations, and action heads to substantially improve upon prior work.","Our experiments on 129 simulated tasks across LIBERO, Meta-World suite, and the Deepmind Control suite exhibit an overall 18% absolute improvement over RT-1 and MT-ACT, with a 36% improvement on the harder LIBERO benchmark.","On 30 real-world manipulation tasks, given an average of just 17 demonstrations per task, BAKU achieves a 91% success rate.","Videos of the robot are best viewed at https://baku-robot.github.io/."],"url":"http://arxiv.org/abs/2406.07539v1","category":"cs.RO"}
{"created":"2024-06-11 17:30:09","title":"A model-independent test of pre-recombination New Physics: Machine Learning based estimate of the Sound Horizon from Gravitational Wave Standard Sirens and the Baryon Acoustic Oscillation Angular Scale","abstract":"In any cosmological model where spacetime is described by a pseudo-Riemannian manifold, photons propagate along null geodesics, and their number is conserved, upcoming Gravitational Wave (GW) observations can be combined with measurements of the Baryon Acoustic Oscillation (BAO) angular scale to provide model-independent estimates of the sound horizon at the baryon-drag epoch. By focusing on the accuracy expected from forthcoming surveys such as LISA GW standard sirens and DESI or Euclid angular BAO measurements, we forecast a relative precision of $\\sigma_{r_{\\rm d}} /r_{\\rm d} \\sim 1.5\\%$ within the redshift range $z \\lesssim 1$. This approach will offer a unique model-independent measure of a fundamental scale characterizing the early universe, which is competitive with model-dependent values inferred within specific theoretical frameworks. These measurements can serve as a consistency test for $\\Lambda$CDM, potentially clarifying the nature of the Hubble tension and confirming or ruling out new physics prior to recombination with a statistical significance of $\\sim 4\\sigma$.","sentences":["In any cosmological model where spacetime is described by a pseudo-Riemannian manifold, photons propagate along null geodesics, and their number is conserved, upcoming Gravitational Wave (GW) observations can be combined with measurements of the Baryon Acoustic Oscillation (BAO) angular scale to provide model-independent estimates of the sound horizon at the baryon-drag epoch.","By focusing on the accuracy expected from forthcoming surveys such as LISA GW standard sirens and DESI or Euclid angular BAO measurements, we forecast a relative precision of $\\sigma_{r_{\\rm d}} /r_{\\rm d} \\sim 1.5\\%$ within the redshift range $z \\lesssim 1$.","This approach will offer a unique model-independent measure of a fundamental scale characterizing the early universe, which is competitive with model-dependent values inferred within specific theoretical frameworks.","These measurements can serve as a consistency test for $\\Lambda$CDM, potentially clarifying the nature of the Hubble tension and confirming or ruling out new physics prior to recombination with a statistical significance of $\\sim 4\\sigma$."],"url":"http://arxiv.org/abs/2406.07493v1","category":"astro-ph.CO"}
{"created":"2024-06-11 17:12:41","title":"Multimodal Belief Prediction","abstract":"Recognizing a speaker's level of commitment to a belief is a difficult task; humans do not only interpret the meaning of the words in context, but also understand cues from intonation and other aspects of the audio signal. Many papers and corpora in the NLP community have approached the belief prediction task using text-only approaches. We are the first to frame and present results on the multimodal belief prediction task. We use the CB-Prosody corpus (CBP), containing aligned text and audio with speaker belief annotations. We first report baselines and significant features using acoustic-prosodic features and traditional machine learning methods. We then present text and audio baselines for the CBP corpus fine-tuning on BERT and Whisper respectively. Finally, we present our multimodal architecture which fine-tunes on BERT and Whisper and uses multiple fusion methods, improving on both modalities alone.","sentences":["Recognizing a speaker's level of commitment to a belief is a difficult task; humans do not only interpret the meaning of the words in context, but also understand cues from intonation and other aspects of the audio signal.","Many papers and corpora in the NLP community have approached the belief prediction task using text-only approaches.","We are the first to frame and present results on the multimodal belief prediction task.","We use the CB-Prosody corpus (CBP), containing aligned text and audio with speaker belief annotations.","We first report baselines and significant features using acoustic-prosodic features and traditional machine learning methods.","We then present text and audio baselines for the CBP corpus fine-tuning on BERT and Whisper respectively.","Finally, we present our multimodal architecture which fine-tunes on BERT and Whisper and uses multiple fusion methods, improving on both modalities alone."],"url":"http://arxiv.org/abs/2406.07466v1","category":"cs.CL"}
{"created":"2024-06-11 16:54:51","title":"Boosted Conformal Prediction Intervals","abstract":"This paper introduces a boosted conformal procedure designed to tailor conformalized prediction intervals toward specific desired properties, such as enhanced conditional coverage or reduced interval length. We employ machine learning techniques, notably gradient boosting, to systematically improve upon a predefined conformity score function. This process is guided by carefully constructed loss functions that measure the deviation of prediction intervals from the targeted properties. The procedure operates post-training, relying solely on model predictions and without modifying the trained model (e.g., the deep network). Systematic experiments demonstrate that starting from conventional conformal methods, our boosted procedure achieves substantial improvements in reducing interval length and decreasing deviation from target conditional coverage.","sentences":["This paper introduces a boosted conformal procedure designed to tailor conformalized prediction intervals toward specific desired properties, such as enhanced conditional coverage or reduced interval length.","We employ machine learning techniques, notably gradient boosting, to systematically improve upon a predefined conformity score function.","This process is guided by carefully constructed loss functions that measure the deviation of prediction intervals from the targeted properties.","The procedure operates post-training, relying solely on model predictions and without modifying the trained model (e.g., the deep network).","Systematic experiments demonstrate that starting from conventional conformal methods, our boosted procedure achieves substantial improvements in reducing interval length and decreasing deviation from target conditional coverage."],"url":"http://arxiv.org/abs/2406.07449v1","category":"stat.ME"}
{"created":"2024-06-11 16:14:46","title":"Clever Hans Effect Found in Automatic Detection of Alzheimer's Disease through Speech","abstract":"We uncover an underlying bias present in the audio recordings produced from the picture description task of the Pitt corpus, the largest publicly accessible database for Alzheimer's Disease (AD) detection research. Even by solely utilizing the silent segments of these audio recordings, we achieve nearly 100% accuracy in AD detection. However, employing the same methods to other datasets and preprocessed Pitt recordings results in typical levels (approximately 80%) of AD detection accuracy. These results demonstrate a Clever Hans effect in AD detection on the Pitt corpus. Our findings emphasize the crucial importance of maintaining vigilance regarding inherent biases in datasets utilized for training deep learning models, and highlight the necessity for a better understanding of the models' performance.","sentences":["We uncover an underlying bias present in the audio recordings produced from the picture description task of the Pitt corpus, the largest publicly accessible database for Alzheimer's Disease (AD) detection research.","Even by solely utilizing the silent segments of these audio recordings, we achieve nearly 100% accuracy in AD detection.","However, employing the same methods to other datasets and preprocessed Pitt recordings results in typical levels (approximately 80%) of AD detection accuracy.","These results demonstrate a Clever Hans effect in AD detection on the Pitt corpus.","Our findings emphasize the crucial importance of maintaining vigilance regarding inherent biases in datasets utilized for training deep learning models, and highlight the necessity for a better understanding of the models' performance."],"url":"http://arxiv.org/abs/2406.07410v1","category":"eess.AS"}
{"created":"2024-06-11 16:08:39","title":"A Survey on Recent Random Walk-based Methods for Embedding Knowledge Graphs","abstract":"Machine learning, deep learning, and NLP methods on knowledge graphs are present in different fields and have important roles in various domains from self-driving cars to friend recommendations on social media platforms. However, to apply these methods to knowledge graphs, the data usually needs to be in an acceptable size and format. In fact, knowledge graphs normally have high dimensions and therefore we need to transform them to a low-dimensional vector space. An embedding is a low-dimensional space into which you can translate high dimensional vectors in a way that intrinsic features of the input data are preserved. In this review, we first explain knowledge graphs and their embedding and then review some of the random walk-based embedding methods that have been developed recently.","sentences":["Machine learning, deep learning, and NLP methods on knowledge graphs are present in different fields and have important roles in various domains from self-driving cars to friend recommendations on social media platforms.","However, to apply these methods to knowledge graphs, the data usually needs to be in an acceptable size and format.","In fact, knowledge graphs normally have high dimensions and therefore we need to transform them to a low-dimensional vector space.","An embedding is a low-dimensional space into which you can translate high dimensional vectors in a way that intrinsic features of the input data are preserved.","In this review, we first explain knowledge graphs and their embedding and then review some of the random walk-based embedding methods that have been developed recently."],"url":"http://arxiv.org/abs/2406.07402v1","category":"cs.LG"}
{"created":"2024-06-11 15:41:48","title":"Closing the Computational-Query Depth Gap in Parallel Stochastic Convex Optimization","abstract":"We develop a new parallel algorithm for minimizing Lipschitz, convex functions with a stochastic subgradient oracle. The total number of queries made and the query depth, i.e., the number of parallel rounds of queries, match the prior state-of-the-art, [CJJLLST23], while improving upon the computational depth by a polynomial factor for sufficiently small accuracy. When combined with previous state-of-the-art methods our result closes a gap between the best-known query depth and the best-known computational depth of parallel algorithms.   Our method starts with a ball acceleration framework of previous parallel methods, i.e., [CJJJLST20, ACJJS21], which reduce the problem to minimizing a regularized Gaussian convolution of the function constrained to Euclidean balls. By developing and leveraging new stability properties of the Hessian of this induced function, we depart from prior parallel algorithms and reduce these ball-constrained optimization problems to stochastic unconstrained quadratic minimization problems. Although we are unable to prove concentration of the asymmetric matrices that we use to approximate this Hessian, we nevertheless develop an efficient parallel method for solving these quadratics. Interestingly, our algorithms can be improved using fast matrix multiplication and use nearly-linear work if the matrix multiplication exponent is 2.","sentences":["We develop a new parallel algorithm for minimizing Lipschitz, convex functions with a stochastic subgradient oracle.","The total number of queries made and the query depth, i.e., the number of parallel rounds of queries, match the prior state-of-the-art, [CJJLLST23], while improving upon the computational depth by a polynomial factor for sufficiently small accuracy.","When combined with previous state-of-the-art methods our result closes a gap between the best-known query depth and the best-known computational depth of parallel algorithms.   ","Our method starts with a ball acceleration framework of previous parallel methods, i.e., [CJJJLST20, ACJJS21], which reduce the problem to minimizing a regularized Gaussian convolution of the function constrained to Euclidean balls.","By developing and leveraging new stability properties of the Hessian of this induced function, we depart from prior parallel algorithms and reduce these ball-constrained optimization problems to stochastic unconstrained quadratic minimization problems.","Although we are unable to prove concentration of the asymmetric matrices that we use to approximate this Hessian, we nevertheless develop an efficient parallel method for solving these quadratics.","Interestingly, our algorithms can be improved using fast matrix multiplication and use nearly-linear work if the matrix multiplication exponent is 2."],"url":"http://arxiv.org/abs/2406.07373v1","category":"math.OC"}
{"created":"2024-06-11 14:24:00","title":"Multi-objective Reinforcement learning from AI Feedback","abstract":"This paper presents Multi-Objective Reinforcement Learning from AI Feedback (MORLAIF), a novel approach to improving the alignment and performance of language models trained using reinforcement learning from AI feedback (RLAIF). In contrast to standard approaches that train a single preference model to represent all human preferences, MORLAIF decomposes this task into multiple simpler principles, such as toxicity, factuality, and sycophancy. Separate preference models are trained for each principle using feedback from GPT-3.5-Turbo. These preference model scores are then combined using different scalarization functions to provide a reward signal for Proximal Policy Optimization (PPO) training of the target language model. Our experiments indicate that MORLAIF outperforms the standard RLAIF baselines and that MORLAIF can be used to align larger language models using smaller ones. Surprisingly, the choice of scalarization function does not appear to significantly impact the results.","sentences":["This paper presents Multi-Objective Reinforcement Learning from AI Feedback (MORLAIF), a novel approach to improving the alignment and performance of language models trained using reinforcement learning from AI feedback (RLAIF).","In contrast to standard approaches that train a single preference model to represent all human preferences, MORLAIF decomposes this task into multiple simpler principles, such as toxicity, factuality, and sycophancy.","Separate preference models are trained for each principle using feedback from GPT-3.5-Turbo.","These preference model scores are then combined using different scalarization functions to provide a reward signal for Proximal Policy Optimization (PPO) training of the target language model.","Our experiments indicate that MORLAIF outperforms the standard RLAIF baselines and that MORLAIF can be used to align larger language models using smaller ones.","Surprisingly, the choice of scalarization function does not appear to significantly impact the results."],"url":"http://arxiv.org/abs/2406.07295v1","category":"cs.LG"}
{"created":"2024-06-11 13:14:04","title":"Let Go of Your Labels with Unsupervised Transfer","abstract":"Foundation vision-language models have enabled remarkable zero-shot transferability of the pre-trained representations to a wide range of downstream tasks. However, to solve a new task, zero-shot transfer still necessitates human guidance to define visual categories that appear in the data. Here, we show that fully unsupervised transfer emerges when searching for the labeling of a dataset that induces maximal margin classifiers in representation spaces of different foundation models. We present TURTLE, a fully unsupervised method that effectively employs this guiding principle to uncover the underlying labeling of a downstream dataset without any supervision and task-specific representation learning. We evaluate TURTLE on a diverse benchmark suite of 26 datasets and show that it achieves new state-of-the-art unsupervised performance. Furthermore, TURTLE, although being fully unsupervised, outperforms zero-shot transfer baselines on a wide range of datasets. In particular, TURTLE matches the average performance of CLIP zero-shot on 26 datasets by employing the same representation space, spanning a wide range of architectures and model sizes. By guiding the search for the underlying labeling using the representation spaces of two foundation models, TURTLE surpasses zero-shot transfer and unsupervised prompt tuning baselines, demonstrating the surprising power and effectiveness of unsupervised transfer.","sentences":["Foundation vision-language models have enabled remarkable zero-shot transferability of the pre-trained representations to a wide range of downstream tasks.","However, to solve a new task, zero-shot transfer still necessitates human guidance to define visual categories that appear in the data.","Here, we show that fully unsupervised transfer emerges when searching for the labeling of a dataset that induces maximal margin classifiers in representation spaces of different foundation models.","We present TURTLE, a fully unsupervised method that effectively employs this guiding principle to uncover the underlying labeling of a downstream dataset without any supervision and task-specific representation learning.","We evaluate TURTLE on a diverse benchmark suite of 26 datasets and show that it achieves new state-of-the-art unsupervised performance.","Furthermore, TURTLE, although being fully unsupervised, outperforms zero-shot transfer baselines on a wide range of datasets.","In particular, TURTLE matches the average performance of CLIP zero-shot on 26 datasets by employing the same representation space, spanning a wide range of architectures and model sizes.","By guiding the search for the underlying labeling using the representation spaces of two foundation models, TURTLE surpasses zero-shot transfer and unsupervised prompt tuning baselines, demonstrating the surprising power and effectiveness of unsupervised transfer."],"url":"http://arxiv.org/abs/2406.07236v1","category":"cs.LG"}
{"created":"2024-06-11 13:10:30","title":"Decipherment-Aware Multilingual Learning in Jointly Trained Language Models","abstract":"The principle that governs unsupervised multilingual learning (UCL) in jointly trained language models (mBERT as a popular example) is still being debated. Many find it surprising that one can achieve UCL with multiple monolingual corpora. In this work, we anchor UCL in the context of language decipherment and show that the joint training methodology is a decipherment process pivotal for UCL. In a controlled setting, we investigate the effect of different decipherment settings on the multilingual learning performance and consolidate the existing opinions on the contributing factors to multilinguality. From an information-theoretic perspective we draw a limit to the UCL performance and demonstrate the importance of token alignment in challenging decipherment settings caused by differences in the data domain, language order and tokenization granularity. Lastly, we apply lexical alignment to mBERT and investigate the contribution of aligning different lexicon groups to downstream performance.","sentences":["The principle that governs unsupervised multilingual learning (UCL) in jointly trained language models (mBERT as a popular example) is still being debated.","Many find it surprising that one can achieve UCL with multiple monolingual corpora.","In this work, we anchor UCL in the context of language decipherment and show that the joint training methodology is a decipherment process pivotal for UCL.","In a controlled setting, we investigate the effect of different decipherment settings on the multilingual learning performance and consolidate the existing opinions on the contributing factors to multilinguality.","From an information-theoretic perspective we draw a limit to the UCL performance and demonstrate the importance of token alignment in challenging decipherment settings caused by differences in the data domain, language order and tokenization granularity.","Lastly, we apply lexical alignment to mBERT and investigate the contribution of aligning different lexicon groups to downstream performance."],"url":"http://arxiv.org/abs/2406.07231v1","category":"cs.CL"}
{"created":"2024-06-11 12:01:11","title":"RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer Tracker","abstract":"Vision camera and sonar are naturally complementary in the underwater environment. Combining the information from two modalities will promote better observation of underwater targets. However, this problem has not received sufficient attention in previous research. Therefore, this paper introduces a new challenging RGB-Sonar (RGB-S) tracking task and investigates how to achieve efficient tracking of an underwater target through the interaction of RGB and sonar modalities. Specifically, we first propose an RGBS50 benchmark dataset containing 50 sequences and more than 87000 high-quality annotated bounding boxes. Experimental results show that the RGBS50 benchmark poses a challenge to currently popular SOT trackers. Second, we propose an RGB-S tracker called SCANet, which includes a spatial cross-attention module (SCAM) consisting of a novel spatial cross-attention layer and two independent global integration modules. The spatial cross-attention is used to overcome the problem of spatial misalignment of between RGB and sonar images. Third, we propose a SOT data-based RGB-S simulation training method (SRST) to overcome the lack of RGB-S training datasets. It converts RGB images into sonar-like saliency images to construct pseudo-data pairs, enabling the model to learn the semantic structure of RGB-S-like data. Comprehensive experiments show that the proposed spatial cross-attention effectively achieves the interaction between RGB and sonar modalities and SCANet achieves state-of-the-art performance on the proposed benchmark. The code is available at https://github.com/LiYunfengLYF/RGBS50.","sentences":["Vision camera and sonar are naturally complementary in the underwater environment.","Combining the information from two modalities will promote better observation of underwater targets.","However, this problem has not received sufficient attention in previous research.","Therefore, this paper introduces a new challenging RGB-Sonar (RGB-S) tracking task and investigates how to achieve efficient tracking of an underwater target through the interaction of RGB and sonar modalities.","Specifically, we first propose an RGBS50 benchmark dataset containing 50 sequences and more than 87000 high-quality annotated bounding boxes.","Experimental results show that the RGBS50 benchmark poses a challenge to currently popular SOT trackers.","Second, we propose an RGB-S tracker called SCANet, which includes a spatial cross-attention module (SCAM) consisting of a novel spatial cross-attention layer and two independent global integration modules.","The spatial cross-attention is used to overcome the problem of spatial misalignment of between RGB and sonar images.","Third, we propose a SOT data-based RGB-S simulation training method (SRST) to overcome the lack of RGB-S training datasets.","It converts RGB images into sonar-like saliency images to construct pseudo-data pairs, enabling the model to learn the semantic structure of RGB-S-like data.","Comprehensive experiments show that the proposed spatial cross-attention effectively achieves the interaction between RGB and sonar modalities and SCANet achieves state-of-the-art performance on the proposed benchmark.","The code is available at https://github.com/LiYunfengLYF/RGBS50."],"url":"http://arxiv.org/abs/2406.07189v1","category":"cs.CV"}
{"created":"2024-06-11 09:21:50","title":"Leveraging Large Language Models for Efficient Failure Analysis in Game Development","abstract":"In games, and more generally in the field of software development, early detection of bugs is vital to maintain a high quality of the final product. Automated tests are a powerful tool that can catch a problem earlier in development by executing periodically. As an example, when new code is submitted to the code base, a new automated test verifies these changes. However, identifying the specific change responsible for a test failure becomes harder when dealing with batches of changes -- especially in the case of a large-scale project such as a AAA game, where thousands of people contribute to a single code base. This paper proposes a new approach to automatically identify which change in the code caused a test to fail. The method leverages Large Language Models (LLMs) to associate error messages with the corresponding code changes causing the failure. We investigate the effectiveness of our approach with quantitative and qualitative evaluations. Our approach reaches an accuracy of 71% in our newly created dataset, which comprises issues reported by developers at EA over a period of one year. We further evaluated our model through a user study to assess the utility and usability of the tool from a developer perspective, resulting in a significant reduction in time -- up to 60% -- spent investigating issues.","sentences":["In games, and more generally in the field of software development, early detection of bugs is vital to maintain a high quality of the final product.","Automated tests are a powerful tool that can catch a problem earlier in development by executing periodically.","As an example, when new code is submitted to the code base, a new automated test verifies these changes.","However, identifying the specific change responsible for a test failure becomes harder when dealing with batches of changes -- especially in the case of a large-scale project such as a AAA game, where thousands of people contribute to a single code base.","This paper proposes a new approach to automatically identify which change in the code caused a test to fail.","The method leverages Large Language Models (LLMs) to associate error messages with the corresponding code changes causing the failure.","We investigate the effectiveness of our approach with quantitative and qualitative evaluations.","Our approach reaches an accuracy of 71% in our newly created dataset, which comprises issues reported by developers at EA over a period of one year.","We further evaluated our model through a user study to assess the utility and usability of the tool from a developer perspective, resulting in a significant reduction in time -- up to 60% -- spent investigating issues."],"url":"http://arxiv.org/abs/2406.07084v1","category":"cs.LG"}
{"created":"2024-06-11 07:55:09","title":"Learning EFSM Models with Registers in Guards","abstract":"This paper presents an active inference method for Extended Finite State Machines, where inputs and outputs are parametrized, and transitions can be conditioned by guards involving input parameters and internal variables called registers. The method applies to (software) systems that cannot be reset, so it learns an EFSM model of the system on a single trace.","sentences":["This paper presents an active inference method for Extended Finite State Machines, where inputs and outputs are parametrized, and transitions can be conditioned by guards involving input parameters and internal variables called registers.","The method applies to (software) systems that cannot be reset, so it learns an EFSM model of the system on a single trace."],"url":"http://arxiv.org/abs/2406.07040v1","category":"cs.FL"}
{"created":"2024-06-11 07:34:15","title":"Fairness-Aware Meta-Learning via Nash Bargaining","abstract":"To address issues of group-level fairness in machine learning, it is natural to adjust model parameters based on specific fairness objectives over a sensitive-attributed validation set. Such an adjustment procedure can be cast within a meta-learning framework. However, naive integration of fairness goals via meta-learning can cause hypergradient conflicts for subgroups, resulting in unstable convergence and compromising model performance and fairness. To navigate this issue, we frame the resolution of hypergradient conflicts as a multi-player cooperative bargaining game. We introduce a two-stage meta-learning framework in which the first stage involves the use of a Nash Bargaining Solution (NBS) to resolve hypergradient conflicts and steer the model toward the Pareto front, and the second stage optimizes with respect to specific fairness goals. Our method is supported by theoretical results, notably a proof of the NBS for gradient aggregation free from linear independence assumptions, a proof of Pareto improvement, and a proof of monotonic improvement in validation loss. We also show empirical effects across various fairness objectives in six key fairness datasets and two image classification tasks.","sentences":["To address issues of group-level fairness in machine learning, it is natural to adjust model parameters based on specific fairness objectives over a sensitive-attributed validation set.","Such an adjustment procedure can be cast within a meta-learning framework.","However, naive integration of fairness goals via meta-learning can cause hypergradient conflicts for subgroups, resulting in unstable convergence and compromising model performance and fairness.","To navigate this issue, we frame the resolution of hypergradient conflicts as a multi-player cooperative bargaining game.","We introduce a two-stage meta-learning framework in which the first stage involves the use of a Nash Bargaining Solution (NBS) to resolve hypergradient conflicts and steer the model toward the Pareto front, and the second stage optimizes with respect to specific fairness goals.","Our method is supported by theoretical results, notably a proof of the NBS for gradient aggregation free from linear independence assumptions, a proof of Pareto improvement, and a proof of monotonic improvement in validation loss.","We also show empirical effects across various fairness objectives in six key fairness datasets and two image classification tasks."],"url":"http://arxiv.org/abs/2406.07029v1","category":"cs.LG"}
{"created":"2024-06-11 07:28:09","title":"Plant-and-Steal: Truthful Fair Allocations via Predictions","abstract":"We study truthful mechanisms for approximating the Maximin-Share (MMS) allocation of agents with additive valuations for indivisible goods. Algorithmically, constant factor approximations exist for the problem for any number of agents. When adding incentives to the mix, a jarring result by Amanatidis, Birmpas, Christodoulou, and Markakis [EC 2017] shows that the best possible approximation for two agents and $m$ items is $\\lfloor \\frac{m}{2} \\rfloor$. We adopt a learning-augmented framework to investigate what is possible when some prediction on the input is given. For two agents, we give a truthful mechanism that takes agents' ordering over items as prediction. When the prediction is accurate, we give a $2$-approximation to the MMS (consistency), and when the prediction is off, we still get an $\\lceil \\frac{m}{2} \\rceil$-approximation to the MMS (robustness). We further show that the mechanism's performance degrades gracefully in the number of ``mistakes\" in the prediction; i.e., we interpolate (up to constant factors) between the two extremes: when there are no mistakes, and when there is a maximum number of mistakes. We also show an impossibility result on the obtainable consistency for mechanisms with finite robustness. For the general case of $n\\ge 2$ agents, we give a 2-approximation mechanism for accurate predictions, with relaxed fallback guarantees. Finally, we give experimental results which illustrate when different components of our framework, made to insure consistency and robustness, come into play.","sentences":["We study truthful mechanisms for approximating the Maximin-Share (MMS) allocation of agents with additive valuations for indivisible goods.","Algorithmically, constant factor approximations exist for the problem for any number of agents.","When adding incentives to the mix, a jarring result by Amanatidis, Birmpas, Christodoulou, and Markakis [EC 2017] shows that the best possible approximation for two agents and $m$ items is $\\lfloor \\frac{m}{2} \\rfloor$. We adopt a learning-augmented framework to investigate what is possible when some prediction on the input is given.","For two agents, we give a truthful mechanism that takes agents' ordering over items as prediction.","When the prediction is accurate, we give a $2$-approximation to the MMS (consistency), and when the prediction is off, we still get an $\\lceil \\frac{m}{2} \\rceil$-approximation to the MMS (robustness).","We further show that the mechanism's performance degrades gracefully in the number of ``mistakes\" in the prediction; i.e., we interpolate (up to constant factors) between the two extremes: when there are no mistakes, and when there is a maximum number of mistakes.","We also show an impossibility result on the obtainable consistency for mechanisms with finite robustness.","For the general case of $n\\ge 2$ agents, we give a 2-approximation mechanism for accurate predictions, with relaxed fallback guarantees.","Finally, we give experimental results which illustrate when different components of our framework, made to insure consistency and robustness, come into play."],"url":"http://arxiv.org/abs/2406.07024v1","category":"cs.GT"}
{"created":"2024-06-11 07:26:54","title":"LiSD: An Efficient Multi-Task Learning Framework for LiDAR Segmentation and Detection","abstract":"With the rapid proliferation of autonomous driving, there has been a heightened focus on the research of lidar-based 3D semantic segmentation and object detection methodologies, aiming to ensure the safety of traffic participants. In recent decades, learning-based approaches have emerged, demonstrating remarkable performance gains in comparison to conventional algorithms. However, the segmentation and detection tasks have traditionally been examined in isolation to achieve the best precision. To this end, we propose an efficient multi-task learning framework named LiSD which can address both segmentation and detection tasks, aiming to optimize the overall performance. Our proposed LiSD is a voxel-based encoder-decoder framework that contains a hierarchical feature collaboration module and a holistic information aggregation module. Different integration methods are adopted to keep sparsity in segmentation while densifying features for query initialization in detection. Besides, cross-task information is utilized in an instance-aware refinement module to obtain more accurate predictions. Experimental results on the nuScenes dataset and Waymo Open Dataset demonstrate the effectiveness of our proposed model. It is worth noting that LiSD achieves the state-of-the-art performance of 83.3% mIoU on the nuScenes segmentation benchmark for lidar-only methods.","sentences":["With the rapid proliferation of autonomous driving, there has been a heightened focus on the research of lidar-based 3D semantic segmentation and object detection methodologies, aiming to ensure the safety of traffic participants.","In recent decades, learning-based approaches have emerged, demonstrating remarkable performance gains in comparison to conventional algorithms.","However, the segmentation and detection tasks have traditionally been examined in isolation to achieve the best precision.","To this end, we propose an efficient multi-task learning framework named LiSD which can address both segmentation and detection tasks, aiming to optimize the overall performance.","Our proposed LiSD is a voxel-based encoder-decoder framework that contains a hierarchical feature collaboration module and a holistic information aggregation module.","Different integration methods are adopted to keep sparsity in segmentation while densifying features for query initialization in detection.","Besides, cross-task information is utilized in an instance-aware refinement module to obtain more accurate predictions.","Experimental results on the nuScenes dataset and Waymo Open Dataset demonstrate the effectiveness of our proposed model.","It is worth noting that LiSD achieves the state-of-the-art performance of 83.3% mIoU on the nuScenes segmentation benchmark for lidar-only methods."],"url":"http://arxiv.org/abs/2406.07023v1","category":"cs.CV"}
{"created":"2024-06-11 06:44:54","title":"Scaling up masked audio encoder learning for general audio classification","abstract":"Despite progress in audio classification, a generalization gap remains between speech and other sound domains, such as environmental sounds and music. Models trained for speech tasks often fail to perform well on environmental or musical audio tasks, and vice versa. While self-supervised (SSL) audio representations offer an alternative, there has been limited exploration of scaling both model and dataset sizes for SSL-based general audio classification. We introduce Dasheng, a simple SSL audio encoder, based on the efficient masked autoencoder framework. Trained with 1.2 billion parameters on 272,356 hours of diverse audio, Dasheng obtains significant performance gains on the HEAR benchmark. It outperforms previous works on CREMA-D, LibriCount, Speech Commands, VoxLingua, and competes well in music and environment classification. Dasheng features inherently contain rich speech, music, and environmental information, as shown in nearest-neighbor classification experiments. Code is available https://github.com/richermans/dasheng/.","sentences":["Despite progress in audio classification, a generalization gap remains between speech and other sound domains, such as environmental sounds and music.","Models trained for speech tasks often fail to perform well on environmental or musical audio tasks, and vice versa.","While self-supervised (SSL) audio representations offer an alternative, there has been limited exploration of scaling both model and dataset sizes for SSL-based general audio classification.","We introduce Dasheng, a simple SSL audio encoder, based on the efficient masked autoencoder framework.","Trained with 1.2 billion parameters on 272,356 hours of diverse audio, Dasheng obtains significant performance gains on the HEAR benchmark.","It outperforms previous works on CREMA-D, LibriCount, Speech Commands, VoxLingua, and competes well in music and environment classification.","Dasheng features inherently contain rich speech, music, and environmental information, as shown in nearest-neighbor classification experiments.","Code is available https://github.com/richermans/dasheng/."],"url":"http://arxiv.org/abs/2406.06992v1","category":"cs.SD"}
{"created":"2024-06-11 06:32:28","title":"Position Paper: Technical Research and Talent is Needed for Effective AI Governance","abstract":"In light of recent advancements in AI capabilities and the increasingly widespread integration of AI systems into society, governments worldwide are actively seeking to mitigate the potential harms and risks associated with these technologies through regulation and other governance tools. However, there exist significant gaps between governance aspirations and the current state of the technical tooling necessary for their realisation. In this position paper, we survey policy documents published by public-sector institutions in the EU, US, and China to highlight specific areas of disconnect between the technical requirements necessary for enacting proposed policy actions, and the current technical state of the art. Our analysis motivates a call for tighter integration of the AI/ML research community within AI governance in order to i) catalyse technical research aimed at bridging the gap between current and supposed technical underpinnings of regulatory action, as well as ii) increase the level of technical expertise within governing institutions so as to inform and guide effective governance of AI.","sentences":["In light of recent advancements in AI capabilities and the increasingly widespread integration of AI systems into society, governments worldwide are actively seeking to mitigate the potential harms and risks associated with these technologies through regulation and other governance tools.","However, there exist significant gaps between governance aspirations and the current state of the technical tooling necessary for their realisation.","In this position paper, we survey policy documents published by public-sector institutions in the EU, US, and China to highlight specific areas of disconnect between the technical requirements necessary for enacting proposed policy actions, and the current technical state of the art.","Our analysis motivates a call for tighter integration of the AI/ML research community within AI governance in order to i) catalyse technical research aimed at bridging the gap between current and supposed technical underpinnings of regulatory action, as well as ii) increase the level of technical expertise within governing institutions so as to inform and guide effective governance of AI."],"url":"http://arxiv.org/abs/2406.06987v1","category":"cs.CY"}
{"created":"2024-06-11 06:18:29","title":"AudioMarkBench: Benchmarking Robustness of Audio Watermarking","abstract":"The increasing realism of synthetic speech, driven by advancements in text-to-speech models, raises ethical concerns regarding impersonation and disinformation. Audio watermarking offers a promising solution via embedding human-imperceptible watermarks into AI-generated audios. However, the robustness of audio watermarking against common/adversarial perturbations remains understudied. We present AudioMarkBench, the first systematic benchmark for evaluating the robustness of audio watermarking against watermark removal and watermark forgery. AudioMarkBench includes a new dataset created from Common-Voice across languages, biological sexes, and ages, 3 state-of-the-art watermarking methods, and 15 types of perturbations. We benchmark the robustness of these methods against the perturbations in no-box, black-box, and white-box settings. Our findings highlight the vulnerabilities of current watermarking techniques and emphasize the need for more robust and fair audio watermarking solutions. Our dataset and code are publicly available at \\url{https://github.com/moyangkuo/AudioMarkBench}.","sentences":["The increasing realism of synthetic speech, driven by advancements in text-to-speech models, raises ethical concerns regarding impersonation and disinformation.","Audio watermarking offers a promising solution via embedding human-imperceptible watermarks into AI-generated audios.","However, the robustness of audio watermarking against common/adversarial perturbations remains understudied.","We present AudioMarkBench, the first systematic benchmark for evaluating the robustness of audio watermarking against watermark removal and watermark forgery.","AudioMarkBench includes a new dataset created from Common-Voice across languages, biological sexes, and ages, 3 state-of-the-art watermarking methods, and 15 types of perturbations.","We benchmark the robustness of these methods against the perturbations in no-box, black-box, and white-box settings.","Our findings highlight the vulnerabilities of current watermarking techniques and emphasize the need for more robust and fair audio watermarking solutions.","Our dataset and code are publicly available at \\url{https://github.com/moyangkuo/AudioMarkBench}."],"url":"http://arxiv.org/abs/2406.06979v1","category":"cs.LG"}
{"created":"2024-06-11 06:10:46","title":"RWKV-CLIP: A Robust Vision-Language Representation Learner","abstract":"Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from websites. This paper further explores CLIP from the perspectives of data and model architecture. To address the prevalence of noisy data and enhance the quality of large-scale image-text data crawled from the internet, we introduce a diverse description generation framework that can leverage Large Language Models (LLMs) to synthesize and refine content from web-based texts, synthetic captions, and detection tags. Furthermore, we propose RWKV-CLIP, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs. Comprehensive experiments across various model scales and pre-training datasets demonstrate that RWKV-CLIP is a robust and efficient vision-language representation learner, it achieves state-of-the-art performance in several downstream tasks, including linear probe, zero-shot classification, and zero-shot image-text retrieval. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/RWKV-CLIP","sentences":["Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from websites.","This paper further explores CLIP from the perspectives of data and model architecture.","To address the prevalence of noisy data and enhance the quality of large-scale image-text data crawled from the internet, we introduce a diverse description generation framework that can leverage Large Language Models (LLMs) to synthesize and refine content from web-based texts, synthetic captions, and detection tags.","Furthermore, we propose RWKV-CLIP, the first RWKV-driven vision-language representation learning model that combines the effective parallel training of transformers with the efficient inference of RNNs.","Comprehensive experiments across various model scales and pre-training datasets demonstrate that RWKV-CLIP is a robust and efficient vision-language representation learner, it achieves state-of-the-art performance in several downstream tasks, including linear probe, zero-shot classification, and zero-shot image-text retrieval.","To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/RWKV-CLIP"],"url":"http://arxiv.org/abs/2406.06973v1","category":"cs.CV"}
{"created":"2024-06-11 05:47:16","title":"Missingness-resilient Video-enhanced Multimodal Disfluency Detection","abstract":"Most existing speech disfluency detection techniques only rely upon acoustic data. In this work, we present a practical multimodal disfluency detection approach that leverages available video data together with audio. We curate an audiovisual dataset and propose a novel fusion technique with unified weight-sharing modality-agnostic encoders to learn the temporal and semantic context. Our resilient design accommodates real-world scenarios where the video modality may sometimes be missing during inference. We also present alternative fusion strategies when both modalities are assured to be complete. In experiments across five disfluency-detection tasks, our unified multimodal approach significantly outperforms Audio-only unimodal methods, yielding an average absolute improvement of 10% (i.e., 10 percentage point increase) when both video and audio modalities are always available, and 7% even when video modality is missing in half of the samples.","sentences":["Most existing speech disfluency detection techniques only rely upon acoustic data.","In this work, we present a practical multimodal disfluency detection approach that leverages available video data together with audio.","We curate an audiovisual dataset and propose a novel fusion technique with unified weight-sharing modality-agnostic encoders to learn the temporal and semantic context.","Our resilient design accommodates real-world scenarios where the video modality may sometimes be missing during inference.","We also present alternative fusion strategies when both modalities are assured to be complete.","In experiments across five disfluency-detection tasks, our unified multimodal approach significantly outperforms Audio-only unimodal methods, yielding an average absolute improvement of 10% (i.e., 10 percentage point increase) when both video and audio modalities are always available, and 7% even when video modality is missing in half of the samples."],"url":"http://arxiv.org/abs/2406.06964v1","category":"cs.CL"}
{"created":"2024-06-11 05:25:48","title":"ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models","abstract":"With the increasing popularity of recommendation systems (RecSys), the demand for compute resources in datacenters has surged. However, the model-wise resource allocation employed in current RecSys model serving architectures falls short in effectively utilizing resources, leading to sub-optimal total cost of ownership. We propose ElasticRec, a model serving architecture for RecSys providing resource elasticity and high memory efficiency. ElasticRec is based on a microservice-based software architecture for fine-grained resource allocation, tailored to the heterogeneous resource demands of RecSys. Additionally, ElasticRec achieves high memory efficiency via our utility-based resource allocation. Overall, ElasticRec achieves an average 3.3x reduction in memory allocation size and 8.1x increase in memory utility, resulting in an average 1.6x reduction in deployment cost compared to state-of-the-art RecSys inference serving system.","sentences":["With the increasing popularity of recommendation systems (RecSys), the demand for compute resources in datacenters has surged.","However, the model-wise resource allocation employed in current RecSys model serving architectures falls short in effectively utilizing resources, leading to sub-optimal total cost of ownership.","We propose ElasticRec, a model serving architecture for RecSys providing resource elasticity and high memory efficiency.","ElasticRec is based on a microservice-based software architecture for fine-grained resource allocation, tailored to the heterogeneous resource demands of RecSys.","Additionally, ElasticRec achieves high memory efficiency via our utility-based resource allocation.","Overall, ElasticRec achieves an average 3.3x reduction in memory allocation size and 8.1x increase in memory utility, resulting in an average 1.6x reduction in deployment cost compared to state-of-the-art RecSys inference serving system."],"url":"http://arxiv.org/abs/2406.06955v1","category":"cs.DC"}
{"created":"2024-06-11 05:25:38","title":"Distributional MIPLIB: a Multi-Domain Library for Advancing ML-Guided MILP Methods","abstract":"Mixed Integer Linear Programming (MILP) is a fundamental tool for modeling combinatorial optimization problems. Recently, a growing body of research has used machine learning to accelerate MILP solving. Despite the increasing popularity of this approach, there is a lack of a common repository that provides distributions of similar MILP instances across different domains, at different hardness levels, with standardized test sets. In this paper, we introduce Distributional MIPLIB, a multi-domain library of problem distributions for advancing ML-guided MILP methods. We curate MILP distributions from existing work in this area as well as real-world problems that have not been used, and classify them into different hardness levels. It will facilitate research in this area by enabling comprehensive evaluation on diverse and realistic domains. We empirically illustrate the benefits of using Distributional MIPLIB as a research vehicle in two ways. We evaluate the performance of ML-guided variable branching on previously unused distributions to identify potential areas for improvement. Moreover, we propose to learn branching policies from a mix of distributions, demonstrating that mixed distributions achieve better performance compared to homogeneous distributions when there is limited data and generalize well to larger instances.","sentences":["Mixed Integer Linear Programming (MILP) is a fundamental tool for modeling combinatorial optimization problems.","Recently, a growing body of research has used machine learning to accelerate MILP solving.","Despite the increasing popularity of this approach, there is a lack of a common repository that provides distributions of similar MILP instances across different domains, at different hardness levels, with standardized test sets.","In this paper, we introduce Distributional MIPLIB, a multi-domain library of problem distributions for advancing ML-guided MILP methods.","We curate MILP distributions from existing work in this area as well as real-world problems that have not been used, and classify them into different hardness levels.","It will facilitate research in this area by enabling comprehensive evaluation on diverse and realistic domains.","We empirically illustrate the benefits of using Distributional MIPLIB as a research vehicle in two ways.","We evaluate the performance of ML-guided variable branching on previously unused distributions to identify potential areas for improvement.","Moreover, we propose to learn branching policies from a mix of distributions, demonstrating that mixed distributions achieve better performance compared to homogeneous distributions when there is limited data and generalize well to larger instances."],"url":"http://arxiv.org/abs/2406.06954v1","category":"cs.LG"}
{"created":"2024-06-11 04:52:23","title":"Optimal Matrix-Mimetic Tensor Algebras via Variable Projection","abstract":"Recent advances in {matrix-mimetic} tensor frameworks have made it possible to preserve linear algebraic properties for multilinear data analysis and, as a result, to obtain optimal representations of multiway data. Matrix mimeticity arises from interpreting tensors as operators that can be multiplied, factorized, and analyzed analogous to matrices. Underlying the tensor operation is an algebraic framework parameterized by an invertible linear transformation. The choice of linear mapping is crucial to representation quality and, in practice, is made heuristically based on expected correlations in the data. However, in many cases, these correlations are unknown and common heuristics lead to suboptimal performance. In this work, we simultaneously learn optimal linear mappings and corresponding tensor representations without relying on prior knowledge of the data. Our new framework explicitly captures the coupling between the transformation and representation using variable projection. We preserve the invertibility of the linear mapping by learning orthogonal transformations with Riemannian optimization. We provide original theory of uniqueness of the transformation and convergence analysis of our variable-projection-based algorithm. We demonstrate the generality of our framework through numerical experiments on a wide range of applications, including financial index tracking, image compression, and reduced order modeling. We have published all the code related to this work at https://github.com/elizabethnewman/star-M-opt.","sentences":["Recent advances in {matrix-mimetic} tensor frameworks have made it possible to preserve linear algebraic properties for multilinear data analysis and, as a result, to obtain optimal representations of multiway data.","Matrix mimeticity arises from interpreting tensors as operators that can be multiplied, factorized, and analyzed analogous to matrices.","Underlying the tensor operation is an algebraic framework parameterized by an invertible linear transformation.","The choice of linear mapping is crucial to representation quality and, in practice, is made heuristically based on expected correlations in the data.","However, in many cases, these correlations are unknown and common heuristics lead to suboptimal performance.","In this work, we simultaneously learn optimal linear mappings and corresponding tensor representations without relying on prior knowledge of the data.","Our new framework explicitly captures the coupling between the transformation and representation using variable projection.","We preserve the invertibility of the linear mapping by learning orthogonal transformations with Riemannian optimization.","We provide original theory of uniqueness of the transformation and convergence analysis of our variable-projection-based algorithm.","We demonstrate the generality of our framework through numerical experiments on a wide range of applications, including financial index tracking, image compression, and reduced order modeling.","We have published all the code related to this work at https://github.com/elizabethnewman/star-M-opt."],"url":"http://arxiv.org/abs/2406.06942v1","category":"math.NA"}
{"created":"2024-06-11 02:56:13","title":"On the Limitation of Kernel Dependence Maximization for Feature Selection","abstract":"A simple and intuitive method for feature selection consists of choosing the feature subset that maximizes a nonparametric measure of dependence between the response and the features. A popular proposal from the literature uses the Hilbert-Schmidt Independence Criterion (HSIC) as the nonparametric dependence measure. The rationale behind this approach to feature selection is that important features will exhibit a high dependence with the response and their inclusion in the set of selected features will increase the HSIC. Through counterexamples, we demonstrate that this rationale is flawed and that feature selection via HSIC maximization can miss critical features.","sentences":["A simple and intuitive method for feature selection consists of choosing the feature subset that maximizes a nonparametric measure of dependence between the response and the features.","A popular proposal from the literature uses the Hilbert-Schmidt Independence Criterion (HSIC) as the nonparametric dependence measure.","The rationale behind this approach to feature selection is that important features will exhibit a high dependence with the response and their inclusion in the set of selected features will increase the HSIC.","Through counterexamples, we demonstrate that this rationale is flawed and that feature selection via HSIC maximization can miss critical features."],"url":"http://arxiv.org/abs/2406.06903v1","category":"stat.ML"}
{"created":"2024-06-11 02:51:17","title":"CodeScore-R: An Automated Robustness Metric for Assessing the FunctionalCorrectness of Code Synthesis","abstract":"Evaluation metrics are crucial in the field of code synthesis. Commonly used code evaluation metrics canbe classified into three types: match-based, semantic-based, and execution-based. Among them, the execution-basedPass@k metric accurately assesses the functionality of predicted code by executing test cases. However, calculatingthis metric requires a significant amount of overhead, necessitating the design of an automated evaluation metric thatcan assess the functionality of predicted code without the need for test cases. Additionally, a good evaluation metricshould be robust, that is the metric can maintain its accuracy even when the predicted code undergoes minor changes.To address these challenges, we propose an automated robust metric, called CodeScore-R, based on UniXcoder andcontrastive learning, for evaluating the functionality of code synthesis. CodeScore-R employs techniques such assketch-based processing, syntactic-equivalent transformations, and mutation testing to effectively mitigate theinterference caused by identifiers, syntax structures, and operators on evaluation results. Experimental resultsdemonstrate that in the tasks of code generation and migration in Java and Python, CodeScore-R outperforms otherevaluation metrics and is more closely aligned with the Pass@k metric, while exhibiting stronger robustness.","sentences":["Evaluation metrics are crucial in the field of code synthesis.","Commonly used code evaluation metrics canbe classified into three types: match-based, semantic-based, and execution-based.","Among them, the execution-basedPass@k metric accurately assesses the functionality of predicted code by executing test cases.","However, calculatingthis metric requires a significant amount of overhead, necessitating the design of an automated evaluation metric thatcan assess the functionality of predicted code without the need for test cases.","Additionally, a good evaluation metricshould be robust, that is the metric can maintain its accuracy even when the predicted code undergoes minor changes.","To address these challenges, we propose an automated robust metric, called CodeScore-R, based on UniXcoder andcontrastive learning, for evaluating the functionality of code synthesis.","CodeScore-R employs techniques such assketch-based processing, syntactic-equivalent transformations, and mutation testing to effectively mitigate theinterference caused by identifiers, syntax structures, and operators on evaluation results.","Experimental resultsdemonstrate that in the tasks of code generation and migration in Java and Python, CodeScore-R outperforms otherevaluation metrics and is more closely aligned with the Pass@k metric, while exhibiting stronger robustness."],"url":"http://arxiv.org/abs/2406.06902v1","category":"cs.SE"}
{"created":"2024-06-11 02:15:53","title":"Transformers Provably Learn Sparse Token Selection While Fully-Connected Nets Cannot","abstract":"The transformer architecture has prevailed in various deep learning settings due to its exceptional capabilities to select and compose structural information. Motivated by these capabilities, Sanford et al. proposed the sparse token selection task, in which transformers excel while fully-connected networks (FCNs) fail in the worst case. Building upon that, we strengthen the FCN lower bound to an average-case setting and establish an algorithmic separation of transformers over FCNs. Specifically, a one-layer transformer trained with gradient descent provably learns the sparse token selection task and, surprisingly, exhibits strong out-of-distribution length generalization. We provide empirical simulations to justify our theoretical findings.","sentences":["The transformer architecture has prevailed in various deep learning settings due to its exceptional capabilities to select and compose structural information.","Motivated by these capabilities, Sanford et al. proposed the sparse token selection task, in which transformers excel while fully-connected networks (FCNs) fail in the worst case.","Building upon that, we strengthen the FCN lower bound to an average-case setting and establish an algorithmic separation of transformers over FCNs.","Specifically, a one-layer transformer trained with gradient descent provably learns the sparse token selection task and, surprisingly, exhibits strong out-of-distribution length generalization.","We provide empirical simulations to justify our theoretical findings."],"url":"http://arxiv.org/abs/2406.06893v1","category":"stat.ML"}
{"created":"2024-06-11 17:34:46","title":"Trim 3D Gaussian Splatting for Accurate Geometry Representation","abstract":"In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct accurate 3D geometry from images. Previous arts for geometry reconstruction from 3D Gaussians mainly focus on exploring strong geometry regularization. Instead, from a fresh perspective, we propose to obtain accurate 3D geometry of a scene by Gaussian trimming, which selectively removes the inaccurate geometry while preserving accurate structures. To achieve this, we analyze the contributions of individual 3D Gaussians and propose a contribution-based trimming strategy to remove the redundant or inaccurate Gaussians. Furthermore, our experimental and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible factor in representing and optimizing the intricate details. Therefore the proposed TrimGS maintains relatively small Gaussian scales. In addition, TrimGS is also compatible with the effective geometry regularization strategies in previous arts. When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently yields more accurate geometry and higher perceptual quality. Our project page is https://trimgs.github.io","sentences":["In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct accurate 3D geometry from images.","Previous arts for geometry reconstruction from 3D Gaussians mainly focus on exploring strong geometry regularization.","Instead, from a fresh perspective, we propose to obtain accurate 3D geometry of a scene by Gaussian trimming, which selectively removes the inaccurate geometry while preserving accurate structures.","To achieve this, we analyze the contributions of individual 3D Gaussians and propose a contribution-based trimming strategy to remove the redundant or inaccurate Gaussians.","Furthermore, our experimental and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible factor in representing and optimizing the intricate details.","Therefore the proposed TrimGS maintains relatively small Gaussian scales.","In addition, TrimGS is also compatible with the effective geometry regularization strategies in previous arts.","When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently yields more accurate geometry and higher perceptual quality.","Our project page is https://trimgs.github.io"],"url":"http://arxiv.org/abs/2406.07499v1","category":"cs.CV"}
{"created":"2024-06-11 17:28:09","title":"ReduceFormer: Attention with Tensor Reduction by Summation","abstract":"Transformers have excelled in many tasks including vision. However, efficient deployment of transformer models in low-latency or high-throughput applications is hindered by the computation in the attention mechanism which involves expensive operations such as matrix multiplication and Softmax. To address this, we introduce ReduceFormer, a family of models optimized for efficiency with the spirit of attention. ReduceFormer leverages only simple operations such as reduction and element-wise multiplication, leading to greatly simplified architecture and improved inference performance, with up to 37% reduction in latency and 44% improvement in throughput, while maintaining competitive accuracy comparable to other recent methods. The proposed model family is suitable for edge devices where compute resource and memory bandwidth are limited, as well as for cloud computing where high throughput is sought after.","sentences":["Transformers have excelled in many tasks including vision.","However, efficient deployment of transformer models in low-latency or high-throughput applications is hindered by the computation in the attention mechanism which involves expensive operations such as matrix multiplication and Softmax.","To address this, we introduce ReduceFormer, a family of models optimized for efficiency with the spirit of attention.","ReduceFormer leverages only simple operations such as reduction and element-wise multiplication, leading to greatly simplified architecture and improved inference performance, with up to 37% reduction in latency and 44% improvement in throughput, while maintaining competitive accuracy comparable to other recent methods.","The proposed model family is suitable for edge devices where compute resource and memory bandwidth are limited, as well as for cloud computing where high throughput is sought after."],"url":"http://arxiv.org/abs/2406.07488v1","category":"cs.CV"}
{"created":"2024-06-11 17:10:25","title":"Convex ordering for stochastic control: the swing contracts case","abstract":"We investigate propagation of convexity and convex ordering on a typical stochastic optimal control problem, namely the pricing of \\q{\\emph{Take-or-Pay}} swing option, a financial derivative product commonly traded on energy markets. The dynamics of the underlying asset is modelled by an \\emph{ARCH} model with convex coefficients. We prove that the value function associated to the stochastic optimal control problem is a convex function of the underlying asset price. We also introduce a domination criterion offering insights into the monotonicity of the value function with respect to parameters of the underlying \\emph{ARCH} coefficients. We particularly focus on the one-dimensional setting where, by means of Stein's formula and regularization techniques, we show that the convexity assumption for the \\emph{ARCH} coefficients can be relaxed with a semi-convexity assumption. To validate the results presented in this paper, we also conduct numerical illustrations.","sentences":["We investigate propagation of convexity and convex ordering on a typical stochastic optimal control problem, namely the pricing of \\q{\\emph{Take-or-Pay}} swing option, a financial derivative product commonly traded on energy markets.","The dynamics of the underlying asset is modelled by an \\emph{ARCH} model with convex coefficients.","We prove that the value function associated to the stochastic optimal control problem is a convex function of the underlying asset price.","We also introduce a domination criterion offering insights into the monotonicity of the value function with respect to parameters of the underlying \\emph{ARCH} coefficients.","We particularly focus on the one-dimensional setting where, by means of Stein's formula and regularization techniques, we show that the convexity assumption for the \\emph{ARCH} coefficients can be relaxed with a semi-convexity assumption.","To validate the results presented in this paper, we also conduct numerical illustrations."],"url":"http://arxiv.org/abs/2406.07464v1","category":"q-fin.MF"}
{"created":"2024-06-11 14:33:31","title":"A directional total variation minimization algorithm for isotropic resolution in digital breast tomosynthesis","abstract":"An optimization-based image reconstruction algorithm is developed for contrast enhanced digital breast tomosynthesis (DBT) using dual-energy scanning. The algorithm minimizes directional total variation (TV) with a data discrepancy and non-negativity constraints. Iodinated contrast agent (ICA) imaging is performed by reconstructing images from dual-energy DBT data followed by weighted subtraction. Physical DBT data is acquired with a Siemens Mammomat scanner of a structured breast phantom with ICA inserts. Results are shown for both directional TV minimization and filtered back-projection for reference. It is seen that directional TV is able to substantially reduce depth blur for the ICA objects.","sentences":["An optimization-based image reconstruction algorithm is developed for contrast enhanced digital breast tomosynthesis (DBT) using dual-energy scanning.","The algorithm minimizes directional total variation (TV) with a data discrepancy and non-negativity constraints.","Iodinated contrast agent (ICA) imaging is performed by reconstructing images from dual-energy DBT data followed by weighted subtraction.","Physical DBT data is acquired with a Siemens Mammomat scanner of a structured breast phantom with ICA inserts.","Results are shown for both directional TV minimization and filtered back-projection for reference.","It is seen that directional TV is able to substantially reduce depth blur for the ICA objects."],"url":"http://arxiv.org/abs/2406.07306v1","category":"physics.med-ph"}
{"created":"2024-06-11 13:33:45","title":"Optimal Electrical Oblivious Routing on Expanders","abstract":"In this paper, we investigate the question of whether the electrical flow routing is a good oblivious routing scheme on an $m$-edge graph $G = (V, E)$ that is a $\\Phi$-expander, i.e. where $\\lvert \\partial S \\rvert \\geq \\Phi \\cdot \\mathrm{vol}(S)$ for every $S \\subseteq V, \\mathrm{vol}(S) \\leq \\mathrm{vol}(V)/2$. Beyond its simplicity and structural importance, this question is well-motivated by the current state-of-the-art of fast algorithms for $\\ell_{\\infty}$ oblivious routings that reduce to the expander-case which is in turn solved by electrical flow routing.   Our main result proves that the electrical routing is an $O(\\Phi^{-1} \\log m)$-competitive oblivious routing in the $\\ell_1$- and $\\ell_\\infty$-norms. We further observe that the oblivious routing is $O(\\log^2 m)$-competitive in the $\\ell_2$-norm and, in fact, $O(\\log m)$-competitive if $\\ell_2$-localization is $O(\\log m)$ which is widely believed.   Using these three upper bounds, we can smoothly interpolate to obtain upper bounds for every $p \\in [2, \\infty]$ and $q$ given by $1/p + 1/q = 1$. Assuming $\\ell_2$-localization in $O(\\log m)$, we obtain that in $\\ell_p$ and $\\ell_q$, the electrical oblivious routing is $O(\\Phi^{-(1-2/p)}\\log m)$ competitive. Using the currently known result for $\\ell_2$-localization, this ratio deteriorates by at most a sublogarithmic factor for every $p, q \\neq 2$.   We complement our upper bounds with lower bounds that show that the electrical routing for any such $p$ and $q$ is $\\Omega(\\Phi^{-(1-2/p)}\\log m)$-competitive. This renders our results in $\\ell_1$ and $\\ell_{\\infty}$ unconditionally tight up to constants, and the result in any $\\ell_p$- and $\\ell_q$-norm to be tight in case of $\\ell_2$-localization in $O(\\log m)$.","sentences":["In this paper, we investigate the question of whether the electrical flow routing is a good oblivious routing scheme on an $m$-edge graph $G = (V, E)$ that is a $\\Phi$-expander, i.e. where $\\lvert \\partial S","\\rvert \\geq \\Phi \\cdot \\mathrm{vol}(S)$ for every $S \\subseteq V, \\mathrm{vol}(S) \\leq \\mathrm{vol}(V)/2$.","Beyond its simplicity and structural importance, this question is well-motivated by the current state-of-the-art of fast algorithms for $\\ell_{\\infty}$ oblivious routings that reduce to the expander-case which is in turn solved by electrical flow routing.   ","Our main result proves that the electrical routing is an $O(\\Phi^{-1} \\log m)$-competitive oblivious routing in the $\\ell_1$- and $\\ell_\\infty$-norms.","We further observe that the oblivious routing is $O(\\log^2 m)$-competitive in the $\\ell_2$-norm and, in fact, $O(\\log m)$-competitive if $\\ell_2$-localization is $O(\\log m)$ which is widely believed.   ","Using these three upper bounds, we can smoothly interpolate to obtain upper bounds for every $p \\in","[2, \\infty]$ and $q$ given by $1/p + 1/q = 1$.","Assuming $\\ell_2$-localization in $O(\\log m)$, we obtain that in $\\ell_p$ and $\\ell_q$, the electrical oblivious routing is $O(\\Phi^{-(1-2/p)}\\log m)$ competitive.","Using the currently known result for $\\ell_2$-localization, this ratio deteriorates by at most a sublogarithmic factor for every $p, q \\neq 2$.   ","We complement our upper bounds with lower bounds that show that the electrical routing for any such $p$ and $q$ is $\\Omega(\\Phi^{-(1-2/p)}\\log m)$-competitive.","This renders our results in $\\ell_1$ and $\\ell_{\\infty}$ unconditionally tight up to constants, and the result in any $\\ell_p$- and $\\ell_q$-norm to be tight in case of $\\ell_2$-localization in $O(\\log m)$."],"url":"http://arxiv.org/abs/2406.07252v1","category":"cs.DS"}
{"created":"2024-06-11 12:07:38","title":"Supporting Changes in Digital Ownership and Data Sovereignty Across the Automotive Value Chain with Catena-X","abstract":"Digital Twins have evolved as a concept describing digital representations of physical assets. They can be used to facilitate simulations, monitoring, or optimization of product lifecycles. Considering the concept of a Circular Economy, which entails several lifecycles of, e.g., vehicles, their components, and materials, it is important to investigate how the respective Digital Twins are managed over the lifecycle of their physical assets. This publication presents and compares three approaches for managing Digital Twins in industrial use cases. The analysis considers aspects such as updates, data ownership, and data sovereignty. The results based on the research project Catena-X","sentences":["Digital Twins have evolved as a concept describing digital representations of physical assets.","They can be used to facilitate simulations, monitoring, or optimization of product lifecycles.","Considering the concept of a Circular Economy, which entails several lifecycles of, e.g., vehicles, their components, and materials, it is important to investigate how the respective Digital Twins are managed over the lifecycle of their physical assets.","This publication presents and compares three approaches for managing Digital Twins in industrial use cases.","The analysis considers aspects such as updates, data ownership, and data sovereignty.","The results based on the research project Catena-X"],"url":"http://arxiv.org/abs/2406.07194v1","category":"cs.CY"}
{"created":"2024-06-11 09:55:57","title":"Linear Codes from Projective Linear Anticodes Revisited","abstract":"An anticode ${\\bf C} \\subset {\\bf F}_q^n$ with the diameter $\\delta$ is a code in ${\\bf F}_q^n$ such that the distance between any two distinct codewords in ${\\bf C}$ is at most $\\delta$. The famous Erd\\\"{o}s-Kleitman bound for a binary anticode ${\\bf C}$ of the length $n$ and the diameter $\\delta$ asserts that $$|{\\bf C}| \\leq \\Sigma_{i=0}^{\\frac{\\delta}{2}} \\displaystyle{n \\choose i}.$$ In this paper, we give an antiGriesmer bound for $q$-ary projective linear anticodes, which is stronger than the above Erd\\\"{o}s-Kleitman bound for binary anticodes. The antiGriesmer bound is a lower bound on diameters of projective linear anticodes. From some known projective linear anticodes, we construct some linear codes with optimal or near optimal minimum distances. A complementary theorem constructing infinitely many new projective linear $(t+1)$-weight code from a known $t$-weight linear code is presented. Then many new optimal or almost optimal few-weight linear codes are given and their weight distributions are determined. As a by-product, we also construct several infinite families of three-weight binary linear codes, which lead to $l$-strongly regular graphs for each odd integer $l \\geq 3$.","sentences":["An anticode ${\\bf C} \\subset {\\bf F}_q^n$ with the diameter $\\delta$ is a code in ${\\bf F}_q^n$ such that the distance between any two distinct codewords in ${\\bf C}$ is at most","$\\delta$. The famous Erd\\\"{o}s-Kleitman bound for a binary anticode ${\\bf C}$ of the length $n$ and the diameter $\\delta$ asserts that $$|{\\bf C}| \\leq \\Sigma_{i=0}^{\\frac{\\delta}{2}} \\displaystyle{n \\choose i}.$$ In this paper, we give an antiGriesmer bound for $q$-ary projective linear anticodes, which is stronger than the above Erd\\\"{o}s-Kleitman bound for binary anticodes.","The antiGriesmer bound is a lower bound on diameters of projective linear anticodes.","From some known projective linear anticodes, we construct some linear codes with optimal or near optimal minimum distances.","A complementary theorem constructing infinitely many new projective linear $(t+1)$-weight code from a known $t$-weight linear code is presented.","Then many new optimal or almost optimal few-weight linear codes are given and their weight distributions are determined.","As a by-product, we also construct several infinite families of three-weight binary linear codes, which lead to $l$-strongly regular graphs for each odd integer $l \\geq 3$."],"url":"http://arxiv.org/abs/2406.07112v1","category":"cs.IT"}
{"created":"2024-06-11 09:29:33","title":"The Influence of Placement on Transmission in Distributed Computing of Boolean Functions","abstract":"In this paper, we explore a distributed setting, where a user seeks to compute a linearly-separable Boolean function of degree $M$ from $N$ servers, each with a cache size $M$. Exploiting the fundamental concepts of sensitivity and influences of Boolean functions, we devise a novel approach to capture the interplay between dataset placement across servers and server transmissions and to determine the optimal solution for dataset placement that minimizes the communication cost. In particular, we showcase the achievability of the minimum average joint sensitivity, $\\frac{N}{2^{M-1}}$, as a measure for the communication cost.","sentences":["In this paper, we explore a distributed setting, where a user seeks to compute a linearly-separable Boolean function of degree $M$ from $N$ servers, each with a cache size $M$. Exploiting the fundamental concepts of sensitivity and influences of Boolean functions, we devise a novel approach to capture the interplay between dataset placement across servers and server transmissions and to determine the optimal solution for dataset placement that minimizes the communication cost.","In particular, we showcase the achievability of the minimum average joint sensitivity, $\\frac{N}{2^{M-1}}$, as a measure for the communication cost."],"url":"http://arxiv.org/abs/2406.07088v1","category":"cs.IT"}
{"created":"2024-06-11 09:08:09","title":"Convex hull of Brownian motion and Brownian bridge","abstract":"In this article we study the convex hull spanned by the union of trajectories of a standard planar Brownian motion, and an independent standard planar Brownian bridge. We find exact values of the expectation of perimeter and area of such a convex hull. As an auxiliary result, that is of interest in its own right, we provide an explicit shape of the probability density function of a random variable that represents the time when combined maximum of a standard one-dimensional Brownian motion, and an independent standard one-dimensional Brownian bridge is attained. At the end, we generalize our results to the case of multiple independent standard planar Brownian motions and Brownian bridges.","sentences":["In this article we study the convex hull spanned by the union of trajectories of a standard planar Brownian motion, and an independent standard planar Brownian bridge.","We find exact values of the expectation of perimeter and area of such a convex hull.","As an auxiliary result, that is of interest in its own right, we provide an explicit shape of the probability density function of a random variable that represents the time when combined maximum of a standard one-dimensional Brownian motion, and an independent standard one-dimensional Brownian bridge is attained.","At the end, we generalize our results to the case of multiple independent standard planar Brownian motions and Brownian bridges."],"url":"http://arxiv.org/abs/2406.07079v1","category":"math.PR"}
{"created":"2024-06-11 08:37:33","title":"Effectively Compress KV Heads for LLM","abstract":"The advent of pre-trained large language models (LLMs) has revolutionized various natural language processing tasks. These models predominantly employ an auto-regressive decoding mechanism that utilizes Key-Value (KV) caches to eliminate redundant calculations for previous tokens. Nevertheless, as context lengths and batch sizes increase, the linear expansion in memory footprint of KV caches becomes a key bottleneck of LLM deployment, which decreases generation speeds significantly. To mitigate this issue, previous techniques like multi-query attention (MQA) and grouped-query attention (GQA) have been developed, in order to reduce KV heads to accelerate inference with comparable accuracy to multi-head attention (MHA). Despite their effectiveness, existing strategies for compressing MHA often overlook the intrinsic properties of the KV caches. In this work, we explore the low-rank characteristics of the KV caches and propose a novel approach for compressing KV heads. In particular, we carefully optimize the MHA-to-GQA transformation to minimize compression error, and to remain compatible with rotary position embeddings (RoPE), we also introduce specialized strategies for key caches with RoPE. We demonstrate that our method can compress half or even three-quarters of KV heads while maintaining performance comparable to the original LLMs, which presents a promising direction for more efficient LLM deployment in resource-constrained environments.","sentences":["The advent of pre-trained large language models (LLMs) has revolutionized various natural language processing tasks.","These models predominantly employ an auto-regressive decoding mechanism that utilizes Key-Value (KV) caches to eliminate redundant calculations for previous tokens.","Nevertheless, as context lengths and batch sizes increase, the linear expansion in memory footprint of KV caches becomes a key bottleneck of LLM deployment, which decreases generation speeds significantly.","To mitigate this issue, previous techniques like multi-query attention (MQA) and grouped-query attention (GQA) have been developed, in order to reduce KV heads to accelerate inference with comparable accuracy to multi-head attention (MHA).","Despite their effectiveness, existing strategies for compressing MHA often overlook the intrinsic properties of the KV caches.","In this work, we explore the low-rank characteristics of the KV caches and propose a novel approach for compressing KV heads.","In particular, we carefully optimize the MHA-to-GQA transformation to minimize compression error, and to remain compatible with rotary position embeddings (RoPE), we also introduce specialized strategies for key caches with RoPE.","We demonstrate that our method can compress half or even three-quarters of KV heads while maintaining performance comparable to the original LLMs, which presents a promising direction for more efficient LLM deployment in resource-constrained environments."],"url":"http://arxiv.org/abs/2406.07056v1","category":"cs.CL"}
{"created":"2024-06-11 07:46:47","title":"RS-DFM: A Remote Sensing Distributed Foundation Model for Diverse Downstream Tasks","abstract":"Remote sensing lightweight foundation models have achieved notable success in online perception within remote sensing. However, their capabilities are restricted to performing online inference solely based on their own observations and models, thus lacking a comprehensive understanding of large-scale remote sensing scenarios. To overcome this limitation, we propose a Remote Sensing Distributed Foundation Model (RS-DFM) based on generalized information mapping and interaction. This model can realize online collaborative perception across multiple platforms and various downstream tasks by mapping observations into a unified space and implementing a task-agnostic information interaction strategy. Specifically, we leverage the ground-based geometric prior of remote sensing oblique observations to transform the feature mapping from absolute depth estimation to relative depth estimation, thereby enhancing the model's ability to extract generalized features across diverse heights and perspectives. Additionally, we present a dual-branch information compression module to decouple high-frequency and low-frequency feature information, achieving feature-level compression while preserving essential task-agnostic details. In support of our research, we create a multi-task simulation dataset named AirCo-MultiTasks for multi-UAV collaborative observation. We also conduct extensive experiments, including 3D object detection, instance segmentation, and trajectory prediction. The numerous results demonstrate that our RS-DFM achieves state-of-the-art performance across various downstream tasks.","sentences":["Remote sensing lightweight foundation models have achieved notable success in online perception within remote sensing.","However, their capabilities are restricted to performing online inference solely based on their own observations and models, thus lacking a comprehensive understanding of large-scale remote sensing scenarios.","To overcome this limitation, we propose a Remote Sensing Distributed Foundation Model (RS-DFM) based on generalized information mapping and interaction.","This model can realize online collaborative perception across multiple platforms and various downstream tasks by mapping observations into a unified space and implementing a task-agnostic information interaction strategy.","Specifically, we leverage the ground-based geometric prior of remote sensing oblique observations to transform the feature mapping from absolute depth estimation to relative depth estimation, thereby enhancing the model's ability to extract generalized features across diverse heights and perspectives.","Additionally, we present a dual-branch information compression module to decouple high-frequency and low-frequency feature information, achieving feature-level compression while preserving essential task-agnostic details.","In support of our research, we create a multi-task simulation dataset named AirCo-MultiTasks for multi-UAV collaborative observation.","We also conduct extensive experiments, including 3D object detection, instance segmentation, and trajectory prediction.","The numerous results demonstrate that our RS-DFM achieves state-of-the-art performance across various downstream tasks."],"url":"http://arxiv.org/abs/2406.07032v1","category":"cs.CV"}
{"created":"2024-06-11 07:12:12","title":"Bridging Language Gaps in Audio-Text Retrieval","abstract":"Audio-text retrieval is a challenging task, requiring the search for an audio clip or a text caption within a database. The predominant focus of existing research on English descriptions poses a limitation on the applicability of such models, given the abundance of non-English content in real-world data. To address these linguistic disparities, we propose a language enhancement (LE), using a multilingual text encoder (SONAR) to encode the text data with language-specific information. Additionally, we optimize the audio encoder through the application of consistent ensemble distillation (CED), enhancing support for variable-length audio-text retrieval. Our methodology excels in English audio-text retrieval, demonstrating state-of-the-art (SOTA) performance on commonly used datasets such as AudioCaps and Clotho. Simultaneously, the approach exhibits proficiency in retrieving content in seven other languages with only 10% of additional language-enhanced training data, yielding promising results. The source code is publicly available https://github.com/zyyan4/ml-clap.","sentences":["Audio-text retrieval is a challenging task, requiring the search for an audio clip or a text caption within a database.","The predominant focus of existing research on English descriptions poses a limitation on the applicability of such models, given the abundance of non-English content in real-world data.","To address these linguistic disparities, we propose a language enhancement (LE), using a multilingual text encoder (SONAR) to encode the text data with language-specific information.","Additionally, we optimize the audio encoder through the application of consistent ensemble distillation (CED), enhancing support for variable-length audio-text retrieval.","Our methodology excels in English audio-text retrieval, demonstrating state-of-the-art (SOTA) performance on commonly used datasets such as AudioCaps and Clotho.","Simultaneously, the approach exhibits proficiency in retrieving content in seven other languages with only 10% of additional language-enhanced training data, yielding promising results.","The source code is publicly available https://github.com/zyyan4/ml-clap."],"url":"http://arxiv.org/abs/2406.07012v1","category":"cs.SD"}
{"created":"2024-06-11 06:50:28","title":"Movable Antenna Enhanced NOMA Short-Packet Transmission","abstract":"This letter investigates a short-packet downlink transmission system using non-orthogonal multiple access (NOMA) enhanced via movable antenna (MA). We focuses on maximizing the effective throughput for a core user while ensuring reliable communication for an edge user by optimizing the MAs' coordinates and the power and rate allocations from the access point (AP). The optimization challenge is approached by decomposing it into two subproblems, utilizing successive convex approximation (SCA) to handle the highly non-concave nature of channel gains. Numerical results confirm that the proposed solution offers substantial improvements in effective throughput compared to NOMA short-packet communication with fixed position antennas (FPAs).","sentences":["This letter investigates a short-packet downlink transmission system using non-orthogonal multiple access (NOMA) enhanced via movable antenna (MA).","We focuses on maximizing the effective throughput for a core user while ensuring reliable communication for an edge user by optimizing the MAs' coordinates and the power and rate allocations from the access point (AP).","The optimization challenge is approached by decomposing it into two subproblems, utilizing successive convex approximation (SCA) to handle the highly non-concave nature of channel gains.","Numerical results confirm that the proposed solution offers substantial improvements in effective throughput compared to NOMA short-packet communication with fixed position antennas (FPAs)."],"url":"http://arxiv.org/abs/2406.06998v1","category":"eess.SP"}
{"created":"2024-06-11 06:39:57","title":"Privacy-Utility Tradeoff Based on $\u03b1$-lift","abstract":"Information density and its exponential form, known as lift, play a central role in information privacy leakage measures. $\\alpha$-lift is the power-mean of lift, which is tunable between the worst-case measure max-lift ($\\alpha=\\infty$) and more relaxed versions ($\\alpha<\\infty$). This paper investigates the optimization problem of the privacy-utility tradeoff where $\\alpha$-lift and mutual information are privacy and utility measures, respectively. Due to the nonlinear nature of $\\alpha$-lift for $\\alpha<\\infty$, finding the optimal solution is challenging. Therefore, we propose a heuristic algorithm to estimate the optimal utility for each value of $\\alpha$, inspired by the optimal solution for $\\alpha=\\infty$. In proposing the algorithm, we prove and use the convexity of $\\alpha$-lift with respect to the lift.","sentences":["Information density and its exponential form, known as lift, play a central role in information privacy leakage measures.","$\\alpha$-lift is the power-mean of lift, which is tunable between the worst-case measure max-lift ($\\alpha=\\infty$) and more relaxed versions ($\\alpha<\\infty$).","This paper investigates the optimization problem of the privacy-utility tradeoff where $\\alpha$-lift and mutual information are privacy and utility measures, respectively.","Due to the nonlinear nature of $\\alpha$-lift for $\\alpha<\\infty$, finding the optimal solution is challenging.","Therefore, we propose a heuristic algorithm to estimate the optimal utility for each value of $\\alpha$, inspired by the optimal solution for $\\alpha=\\infty$. In proposing the algorithm, we prove and use the convexity of $\\alpha$-lift with respect to the lift."],"url":"http://arxiv.org/abs/2406.06990v1","category":"cs.IT"}
{"created":"2024-06-11 06:24:16","title":"Half Heusler alloy CoVSn as self-supported electrocatalyst for hydrogen evolution reaction","abstract":"Despite significant advancements in electrocatalysis for clean hydrogen fuel generation, the transition from concept to commercialization faces challenges due to the instability of electrocatalysts. This study delves into the exploration of a structurally and mechanically robust half-Heusler alloy, CoVSn, as an efficient electrocatalyst for hydrogen production. The synthesis of CoVSn was achieved using the arc-melting technique and optimized successfully into a cubic structure - a previously unattained and highly challenging feat. The resulting electrode, cut from the obtained CoVSn pellet, served as a self-supported electrocatalyst and initially generates a current density of 10 mA cm-2 at an overpotential of 244 mV. Remarkably, this overpotential decreased uniquely over time, reaches 202 mV after a durability testing of 12 hours, while maintaining its crystal structure integrity after the electrocatalysis process. This progressive enhancement in catalytic activity and structural stability underscores the significance of this research. The synergistic effect between Co and V atoms as pivotal active centers for hydrogen generation was evident, further enhanced by formation of high valance metal sites Co2O3 and V2O3 during the hydrogen evolution reaction. In essence, this study confirms the stability and promise of CoVSn in hydrogen generation, paving the way for exploring additional self-supported ternary intermetallics to enhance water-splitting efficiency.","sentences":["Despite significant advancements in electrocatalysis for clean hydrogen fuel generation, the transition from concept to commercialization faces challenges due to the instability of electrocatalysts.","This study delves into the exploration of a structurally and mechanically robust half-Heusler alloy, CoVSn, as an efficient electrocatalyst for hydrogen production.","The synthesis of CoVSn was achieved using the arc-melting technique and optimized successfully into a cubic structure - a previously unattained and highly challenging feat.","The resulting electrode, cut from the obtained CoVSn pellet, served as a self-supported electrocatalyst and initially generates a current density of 10 mA cm-2 at an overpotential of 244 mV. Remarkably, this overpotential decreased uniquely over time, reaches 202 mV after a durability testing of 12 hours, while maintaining its crystal structure integrity after the electrocatalysis process.","This progressive enhancement in catalytic activity and structural stability underscores the significance of this research.","The synergistic effect between Co and V atoms as pivotal active centers for hydrogen generation was evident, further enhanced by formation of high valance metal sites Co2O3 and V2O3 during the hydrogen evolution reaction.","In essence, this study confirms the stability and promise of CoVSn in hydrogen generation, paving the way for exploring additional self-supported ternary intermetallics to enhance water-splitting efficiency."],"url":"http://arxiv.org/abs/2406.06981v1","category":"physics.chem-ph"}
{"created":"2024-06-11 06:13:17","title":"Constructions, bounds, and algorithms for peaceable queens","abstract":"The peaceable queens problem asks to determine the maximum number $a(n)$ such that there is a placement of $a(n)$ white queens and $a(n)$ black queens on an $n \\times n$ chessboard so that no queen can capture any queen of the opposite color.   In this paper, we consider the peaceable queens problem and its variant on the toroidal board. For the toroidal board, we provide new upper and lower bounds. Somewhat surprisingly, our bounds show that there is a sharp contrast in behaviour between the odd torus and the even torus. Our lower bounds are given by explicit constructions. For the upper bounds, we formulate the problem as a quadratic optimization problem with at most $70$ variables, regardless of the size of the board. We solve our quadratic program exactly using modern optimization software. Our method is quite robust. For example, with very minor changes, it also provides upper bounds for the regular board. In particular, we show that $a(n) \\leq 0.1641n^2$, for all $n$. This improves on the bound $a(n) \\leq 0.25n^2$ of van Bommel and MacEachern.   We also provide a local search algorithm and a software implementation which converges very rapidly to solutions which appear optimal. Our algorithm is sufficiently robust that it works on both the classical and toroidal boards. For example, for the classical board, the algorithm quickly finds the so-called Ainley construction. Thus, our work provides some further evidence that the Ainley construction is indeed optimal.","sentences":["The peaceable queens problem asks to determine the maximum number $a(n)$ such that there is a placement of $a(n)$ white queens and $a(n)$ black queens on an $n \\times n$ chessboard","so that no queen can capture any queen of the opposite color.   ","In this paper, we consider the peaceable queens problem and its variant on the toroidal board.","For the toroidal board, we provide new upper and lower bounds.","Somewhat surprisingly, our bounds show that there is a sharp contrast in behaviour between the odd torus and the even torus.","Our lower bounds are given by explicit constructions.","For the upper bounds, we formulate the problem as a quadratic optimization problem with at most $70$ variables, regardless of the size of the board.","We solve our quadratic program exactly using modern optimization software.","Our method is quite robust.","For example, with very minor changes, it also provides upper bounds for the regular board.","In particular, we show that $a(n) \\leq 0.1641n^2$, for all $n$. This improves on the bound $a(n) \\leq 0.25n^2$ of van Bommel and MacEachern.   ","We also provide a local search algorithm and a software implementation which converges very rapidly to solutions which appear optimal.","Our algorithm is sufficiently robust that it works on both the classical and toroidal boards.","For example, for the classical board, the algorithm quickly finds the so-called Ainley construction.","Thus, our work provides some further evidence that the Ainley construction is indeed optimal."],"url":"http://arxiv.org/abs/2406.06974v1","category":"math.CO"}
{"created":"2024-06-11 04:31:54","title":"Post-Hoc Answer Attribution for Grounded and Trustworthy Long Document Comprehension: Task, Insights, and Challenges","abstract":"Attributing answer text to its source document for information-seeking questions is crucial for building trustworthy, reliable, and accountable systems. We formulate a new task of post-hoc answer attribution for long document comprehension (LDC). Owing to the lack of long-form abstractive and information-seeking LDC datasets, we refactor existing datasets to assess the strengths and weaknesses of existing retrieval-based and proposed answer decomposition and textual entailment-based optimal selection attribution systems for this task. We throw light on the limitations of existing datasets and the need for datasets to assess the actual performance of systems on this task.","sentences":["Attributing answer text to its source document for information-seeking questions is crucial for building trustworthy, reliable, and accountable systems.","We formulate a new task of post-hoc answer attribution for long document comprehension (LDC).","Owing to the lack of long-form abstractive and information-seeking LDC datasets, we refactor existing datasets to assess the strengths and weaknesses of existing retrieval-based and proposed answer decomposition and textual entailment-based optimal selection attribution systems for this task.","We throw light on the limitations of existing datasets and the need for datasets to assess the actual performance of systems on this task."],"url":"http://arxiv.org/abs/2406.06938v1","category":"cs.CL"}
{"created":"2024-06-11 04:21:54","title":"Random Shadows of Fixed Polytopes","abstract":"Estimating the number of vertices of a two dimensional projection, called a shadow, of a polytope is a fundamental tool for understanding the performance of the shadow simplex method for linear programming among other applications. We prove multiple upper bounds on the expected number of vertices of a random shadow of a fixed polytope. Our bounds are in terms of various parameters in the literature including geometric diameter and edge lengths, minimal and maximal slack, maximal coordinates for lattice polytopes, and maximum absolute values of subdeterminants. For the case of geometric diameter and edge lengths, we prove lower bounds and argue that our upper and lower bounds are both tight for zonotopes.","sentences":["Estimating the number of vertices of a two dimensional projection, called a shadow, of a polytope is a fundamental tool for understanding the performance of the shadow simplex method for linear programming among other applications.","We prove multiple upper bounds on the expected number of vertices of a random shadow of a fixed polytope.","Our bounds are in terms of various parameters in the literature including geometric diameter and edge lengths, minimal and maximal slack, maximal coordinates for lattice polytopes, and maximum absolute values of subdeterminants.","For the case of geometric diameter and edge lengths, we prove lower bounds and argue that our upper and lower bounds are both tight for zonotopes."],"url":"http://arxiv.org/abs/2406.06936v1","category":"math.CO"}
{"created":"2024-06-11 04:19:05","title":"Optimal Qubit Mapping Search for Encoding Classical Data into Matrix Product State Representation with Minimal Loss","abstract":"Matrix product state (MPS) offers a framework for encoding classical data into quantum states, enabling the efficient utilization of quantum resources for data representation and processing. This research paper investigates techniques to enhance the efficiency and accuracy of MPS representations specifically designed for encoding classical data. Based on the observations that MPS truncation error depends on the pattern of the classical data, we devised an algorithm that finds optimal qubit mapping for given classical data, thereby improving the efficiency and fidelity of the MPS representation. Furthermore, we evaluate the impact of the optimized MPS in the context of quantum classifiers, demonstrating their enhanced performance compared to the conventional mapping. This improvement confirms the efficacy of the proposed techniques for encoding classical data into quantum states. MPS representation combined with optimal qubit mapping can pave a new way for more efficient and accurate quantum data representation and processing.","sentences":["Matrix product state (MPS) offers a framework for encoding classical data into quantum states, enabling the efficient utilization of quantum resources for data representation and processing.","This research paper investigates techniques to enhance the efficiency and accuracy of MPS representations specifically designed for encoding classical data.","Based on the observations that MPS truncation error depends on the pattern of the classical data, we devised an algorithm that finds optimal qubit mapping for given classical data, thereby improving the efficiency and fidelity of the MPS representation.","Furthermore, we evaluate the impact of the optimized MPS in the context of quantum classifiers, demonstrating their enhanced performance compared to the conventional mapping.","This improvement confirms the efficacy of the proposed techniques for encoding classical data into quantum states.","MPS representation combined with optimal qubit mapping can pave a new way for more efficient and accurate quantum data representation and processing."],"url":"http://arxiv.org/abs/2406.06935v1","category":"quant-ph"}
{"created":"2024-06-11 03:45:03","title":"Gagliardo-Nirenberg Inequalities in Fractional Coulomb-Sobolev spaces for Radial functions","abstract":"We extend the range of parameters associated with the Gagliardo-Nirenberg interpolation inequalities in the fractional Coulomb-Sobolev spaces for radial functions. We also study the optimality of this newly extended range of parameters.","sentences":["We extend the range of parameters associated with the Gagliardo-Nirenberg interpolation inequalities in the fractional Coulomb-Sobolev spaces for radial functions.","We also study the optimality of this newly extended range of parameters."],"url":"http://arxiv.org/abs/2406.06926v1","category":"math.AP"}
{"created":"2024-06-11 03:00:09","title":"John property of anisotropic minimal surfaces","abstract":"For a convex set $K\\subset \\mathbb R^n$ and the associated anisotropic perimeter $P_K$, we establish that every $(\\epsilon,\\,r)$-minimizer for $P_K$ satisfies a local John property. Furthermore, we prove that a certain class of John domains, including $(\\epsilon,\\,r)$-minimizers close to $K$, admits a trace inequality. As a consequence, we provide a more concrete proof for a crucial step in the quantitative Wulff inequality, thereby complementing the seminal work of Figalli, Maggi, and Pratelli.","sentences":["For a convex set $K\\subset \\mathbb R^n$ and the associated anisotropic perimeter $P_K$, we establish that every $(\\epsilon,\\,r)$-minimizer for $P_K$ satisfies a local John property.","Furthermore, we prove that a certain class of John domains, including $(\\epsilon,\\,r)$-minimizers close to $K$, admits a trace inequality.","As a consequence, we provide a more concrete proof for a crucial step in the quantitative Wulff inequality, thereby complementing the seminal work of Figalli, Maggi, and Pratelli."],"url":"http://arxiv.org/abs/2406.06906v1","category":"math.OC"}
{"created":"2024-06-11 02:09:46","title":"Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation","abstract":"Image diffusion distillation achieves high-fidelity generation with very few sampling steps. However, applying these techniques directly to video diffusion often results in unsatisfactory frame quality due to the limited visual quality in public video datasets. This affects the performance of both teacher and student video diffusion models. Our study aims to improve video diffusion distillation while improving frame appearance using abundant high-quality image data. We propose motion consistency model (MCM), a single-stage video diffusion distillation method that disentangles motion and appearance learning. Specifically, MCM includes a video consistency model that distills motion from the video teacher model, and an image discriminator that enhances frame appearance to match high-quality image data. This combination presents two challenges: (1) conflicting frame learning objectives, as video distillation learns from low-quality video frames while the image discriminator targets high-quality images; and (2) training-inference discrepancies due to the differing quality of video samples used during training and inference. To address these challenges, we introduce disentangled motion distillation and mixed trajectory distillation. The former applies the distillation objective solely to the motion representation, while the latter mitigates training-inference discrepancies by mixing distillation trajectories from both the low- and high-quality video domains. Extensive experiments show that our MCM achieves the state-of-the-art video diffusion distillation performance. Additionally, our method can enhance frame quality in video diffusion models, producing frames with high aesthetic scores or specific styles without corresponding video data.","sentences":["Image diffusion distillation achieves high-fidelity generation with very few sampling steps.","However, applying these techniques directly to video diffusion often results in unsatisfactory frame quality due to the limited visual quality in public video datasets.","This affects the performance of both teacher and student video diffusion models.","Our study aims to improve video diffusion distillation while improving frame appearance using abundant high-quality image data.","We propose motion consistency model (MCM), a single-stage video diffusion distillation method that disentangles motion and appearance learning.","Specifically, MCM includes a video consistency model that distills motion from the video teacher model, and an image discriminator that enhances frame appearance to match high-quality image data.","This combination presents two challenges: (1) conflicting frame learning objectives, as video distillation learns from low-quality video frames while the image discriminator targets high-quality images; and (2) training-inference discrepancies due to the differing quality of video samples used during training and inference.","To address these challenges, we introduce disentangled motion distillation and mixed trajectory distillation.","The former applies the distillation objective solely to the motion representation, while the latter mitigates training-inference discrepancies by mixing distillation trajectories from both the low- and high-quality video domains.","Extensive experiments show that our MCM achieves the state-of-the-art video diffusion distillation performance.","Additionally, our method can enhance frame quality in video diffusion models, producing frames with high aesthetic scores or specific styles without corresponding video data."],"url":"http://arxiv.org/abs/2406.06890v1","category":"cs.CV"}
{"created":"2024-06-11 02:09:32","title":"Universal spatial inflation of human mobility","abstract":"Understanding the interplay between egocentric preference and urban structure in shaping human mobility has profound implications for improving epidemic intervention, social equity, and urban resilience. However, numerous existing studies either solely identify the egocentric preferences -- the anchoring effects from home -- or the impact of hierarchical urban structures. Here, we propose a network-based approach to present human mobility in both spatial and topological aspects within the urban system, using cell phone trajectory data from millions of users across three countries. By segmenting mobility trajectories into modules and examining their overlap with urban scales, we have observed the inflation law that the geospatial extent of these modules increases sub-linearly with their distance from home. Moreover, the egocentric preference for higher urban levels leads to this increase. This universal finding indicates that home-based preferences distort the hierarchical scales of human mobility in the urban environment, regardless of demographics or geography.","sentences":["Understanding the interplay between egocentric preference and urban structure in shaping human mobility has profound implications for improving epidemic intervention, social equity, and urban resilience.","However, numerous existing studies either solely identify the egocentric preferences -- the anchoring effects from home -- or the impact of hierarchical urban structures.","Here, we propose a network-based approach to present human mobility in both spatial and topological aspects within the urban system, using cell phone trajectory data from millions of users across three countries.","By segmenting mobility trajectories into modules and examining their overlap with urban scales, we have observed the inflation law that the geospatial extent of these modules increases sub-linearly with their distance from home.","Moreover, the egocentric preference for higher urban levels leads to this increase.","This universal finding indicates that home-based preferences distort the hierarchical scales of human mobility in the urban environment, regardless of demographics or geography."],"url":"http://arxiv.org/abs/2406.06889v1","category":"physics.soc-ph"}
{"created":"2024-06-11 01:37:07","title":"Boundedness for maximal operators over hypersurfaces in $\\mathbb{R}^3$","abstract":"In this article, we study maximal functions related to hypersurfaces with vanishing Gaussian curvature in $\\mathbb{R}^3$. Firstly, we characterize the $L^p\\rightarrow L^q$ boundedness of local maximal operators along homogeneous hypersurfaces. Moreover, weighted $L^p$-estimates are obtained for the corresponding global operators. Secondly, for a class of hypersurfaces that lack a homogeneous structure and pass through the origin, we attempt to look for other geometric properties instead of height of hypersurfaces to characterize the optimal $L^p$-boundedness of the corresponding global maximal operators.","sentences":["In this article, we study maximal functions related to hypersurfaces with vanishing Gaussian curvature in $\\mathbb{R}^3$. Firstly, we characterize the $L^p\\rightarrow L^q$ boundedness of local maximal operators along homogeneous hypersurfaces.","Moreover, weighted $L^p$-estimates are obtained for the corresponding global operators.","Secondly, for a class of hypersurfaces that lack a homogeneous structure and pass through the origin, we attempt to look for other geometric properties instead of height of hypersurfaces to characterize the optimal $L^p$-boundedness of the corresponding global maximal operators."],"url":"http://arxiv.org/abs/2406.06876v1","category":"math.CA"}
{"created":"2024-06-11 01:20:47","title":"Pushing the limits of the Cosmic Origin Spectrograph (COS) with an optimized background correction","abstract":"Observations utilizing the ultraviolet capabilities of the Cosmic Origin Spectrograph (COS) onboard the Hubble Space Telescope are of unique value to the astronomy community. Spectroscopy down to 900 A with COS has enabled new science areas. However, contrary to the situation at longer wavelengths, these observations are limited by detector background noise. The background correction currently applied by the standard calibration pipeline (CalCOS) is not optimized for faint targets, limiting the scientific value of low signal-to-noise observations. In this work we investigate a possible dependence of the variations of the dark rate in both segments of the COS far-ultraviolet (FUV) detector on time, detector high voltage (HV), and solar activity. Through our analysis we identified a number of detector states (on a configuration basis, e.g., HV and segment) characterizing the spatial distribution of dark counts, and created superdarks to be used in an optimized 2-dimensional (2D) background correction. We have developed and tested Another COS Dark Correction (ACDC), a dedicated pipeline to perform a 2D background correction based on statistical methods, producing background-corrected and flux-calibrated spectra. While our testing of ACDC showed an average improvement in S/N values of ~10%, in a few cases the improvements in S/N reached 60% across the whole wavelength range of individual segments.","sentences":["Observations utilizing the ultraviolet capabilities of the Cosmic Origin Spectrograph (COS) onboard the Hubble Space Telescope are of unique value to the astronomy community.","Spectroscopy down to 900 A with COS has enabled new science areas.","However, contrary to the situation at longer wavelengths, these observations are limited by detector background noise.","The background correction currently applied by the standard calibration pipeline (CalCOS) is not optimized for faint targets, limiting the scientific value of low signal-to-noise observations.","In this work we investigate a possible dependence of the variations of the dark rate in both segments of the COS far-ultraviolet (FUV) detector on time, detector high voltage (HV), and solar activity.","Through our analysis we identified a number of detector states (on a configuration basis, e.g., HV and segment) characterizing the spatial distribution of dark counts, and created superdarks to be used in an optimized 2-dimensional (2D) background correction.","We have developed and tested Another COS Dark Correction (ACDC), a dedicated pipeline to perform a 2D background correction based on statistical methods, producing background-corrected and flux-calibrated spectra.","While our testing of ACDC showed an average improvement in S/N values of ~10%, in a few cases the improvements in S/N reached 60% across the whole wavelength range of individual segments."],"url":"http://arxiv.org/abs/2406.06873v1","category":"astro-ph.IM"}
{"created":"2024-06-10 23:26:52","title":"A game-theoretic, market-based approach to extract flexibility from distributed energy resources","abstract":"In this paper, we propose a market design based on game theory to optimally utilize the flexibility of distributed energy resources (DERs) such as solar PV, batteries, electric vehicles, and flexible loads. Market agents perform multiperiod optimization to determine their feasible flexibility limits for power injections while satisfying all constraints of their DERs. This is followed by a Stackelberg game between the market operator and the agents. The market operator as the leader aims to regulate the aggregate power injection around a desired value by leveraging the flexibility of their agents, and computes optimal prices for both electricity and flexibility services. The agents follow by optimally bidding their desired flexible power injections in response to these prices. We show the existence of an equilibrium among the market agents between all agents and the operator, along with simulation results for a small example system.","sentences":["In this paper, we propose a market design based on game theory to optimally utilize the flexibility of distributed energy resources (DERs) such as solar PV, batteries, electric vehicles, and flexible loads.","Market agents perform multiperiod optimization to determine their feasible flexibility limits for power injections while satisfying all constraints of their DERs.","This is followed by a Stackelberg game between the market operator and the agents.","The market operator as the leader aims to regulate the aggregate power injection around a desired value by leveraging the flexibility of their agents, and computes optimal prices for both electricity and flexibility services.","The agents follow by optimally bidding their desired flexible power injections in response to these prices.","We show the existence of an equilibrium among the market agents between all agents and the operator, along with simulation results for a small example system."],"url":"http://arxiv.org/abs/2406.06844v1","category":"eess.SY"}
{"created":"2024-06-10 22:41:39","title":"Hidden correlations in stochastic photoinduced dynamics of a solid-state electrolyte","abstract":"Photoexcitation by ultrashort laser pulses plays a crucial role in controlling reaction pathways, creating nonequilibrium material properties, and offering a microscopic view of complex dynamics at the molecular level. The photo response following a laser pulse is, in general, non-identical between multiple exposures due to spatiotemporal fluctuations in a material or the stochastic nature of dynamical pathways. However, most ultrafast experiments using a stroboscopic pump-probe scheme struggle to distinguish intrinsic sample fluctuations from extrinsic apparatus noise, often missing seemingly random deviations from the averaged shot-to-shot response. Leveraging the stability and high photon-flux of time-resolved X-ray micro-diffraction at a synchrotron, we developed a method to quantitatively characterize the shot-to-shot variation of the photoinduced dynamics in a solid-state electrolyte. By analyzing temporal evolutions of the lattice parameter of a single grain in a powder ensemble, we found that the sample responses after different shots contain random fluctuations that are, however, not independent. Instead, there is a correlation between the nonequilibrium lattice trajectories following adjacent laser shots with a characteristic \"correlation length\" of approximately 1,500 shots, which represents an energy barrier of 0.38~eV for switching the photoinduced pathway, a value interestingly commensurate with the activation energy of lithium ion diffusion. Not only does our nonequilibrium noise correlation spectroscopy provide a new strategy for studying fluctuations that are central to phase transitions in both condensed matter and molecular systems, it also paves the way for discovering hidden correlations and novel metastable states buried in oft-presumed random, uncorrelated fluctuating dynamics.","sentences":["Photoexcitation by ultrashort laser pulses plays a crucial role in controlling reaction pathways, creating nonequilibrium material properties, and offering a microscopic view of complex dynamics at the molecular level.","The photo response following a laser pulse is, in general, non-identical between multiple exposures due to spatiotemporal fluctuations in a material or the stochastic nature of dynamical pathways.","However, most ultrafast experiments using a stroboscopic pump-probe scheme struggle to distinguish intrinsic sample fluctuations from extrinsic apparatus noise, often missing seemingly random deviations from the averaged shot-to-shot response.","Leveraging the stability and high photon-flux of time-resolved X-ray micro-diffraction at a synchrotron, we developed a method to quantitatively characterize the shot-to-shot variation of the photoinduced dynamics in a solid-state electrolyte.","By analyzing temporal evolutions of the lattice parameter of a single grain in a powder ensemble, we found that the sample responses after different shots contain random fluctuations that are, however, not independent.","Instead, there is a correlation between the nonequilibrium lattice trajectories following adjacent laser shots with a characteristic \"correlation length\" of approximately 1,500 shots, which represents an energy barrier of 0.38~eV for switching the photoinduced pathway, a value interestingly commensurate with the activation energy of lithium ion diffusion.","Not only does our nonequilibrium noise correlation spectroscopy provide a new strategy for studying fluctuations that are central to phase transitions in both condensed matter and molecular systems, it also paves the way for discovering hidden correlations and novel metastable states buried in oft-presumed random, uncorrelated fluctuating dynamics."],"url":"http://arxiv.org/abs/2406.06832v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-10 21:23:19","title":"Fast White-Box Adversarial Streaming Without a Random Oracle","abstract":"Recently, the question of adversarially robust streaming, where the stream is allowed to depend on the randomness of the streaming algorithm, has gained a lot of attention. In this work, we consider a strong white-box adversarial model (Ajtai et al. PODS 2022), in which the adversary has access to all past random coins and the parameters used by the streaming algorithm. We focus on the sparse recovery problem and extend our result to other tasks such as distinct element estimation and low-rank approximation of matrices and tensors. The main drawback of previous work is that it requires a random oracle, which is especially problematic in the streaming model since the amount of randomness is counted in the space complexity of a streaming algorithm. Also, the previous work suffers from large update time. We construct a near-optimal solution for the sparse recovery problem in white-box adversarial streams, based on the subexponentially secure Learning with Errors assumption. Importantly, our solution does not require a random oracle and has a polylogarithmic per item processing time. We also give results in a related white-box adversarially robust distributed model. Our constructions are based on homomorphic encryption schemes satisfying very mild structural properties that are currently satisfied by most known schemes.","sentences":["Recently, the question of adversarially robust streaming, where the stream is allowed to depend on the randomness of the streaming algorithm, has gained a lot of attention.","In this work, we consider a strong white-box adversarial model (Ajtai et al. PODS 2022), in which the adversary has access to all past random coins and the parameters used by the streaming algorithm.","We focus on the sparse recovery problem and extend our result to other tasks such as distinct element estimation and low-rank approximation of matrices and tensors.","The main drawback of previous work is that it requires a random oracle, which is especially problematic in the streaming model since the amount of randomness is counted in the space complexity of a streaming algorithm.","Also, the previous work suffers from large update time.","We construct a near-optimal solution for the sparse recovery problem in white-box adversarial streams, based on the subexponentially secure Learning with Errors assumption.","Importantly, our solution does not require a random oracle and has a polylogarithmic per item processing time.","We also give results in a related white-box adversarially robust distributed model.","Our constructions are based on homomorphic encryption schemes satisfying very mild structural properties that are currently satisfied by most known schemes."],"url":"http://arxiv.org/abs/2406.06808v1","category":"cs.DS"}
{"created":"2024-06-10 21:17:07","title":"Leveraging modular values in quantum algorithms: the Deutsch-Jozsa","abstract":"We present a novel approach to quantum algorithms, by taking advantage of modular values, i.e., complex and unbounded quantities resulting from specific post-selected measurement scenarios. Our focus is on the problem of ascertaining whether a given function acting on a set of binary values is constant (uniformly yielding outputs of either all 0 or all 1), or balanced (a situation wherein half of the outputs are 0 and the other half are 1). Such problem can be solved by relying on the Deutsch-Jozsa algorithm. The proposed method, relying on the use of modular values, provides a high number of degrees of freedom for optimizing the new algorithm inspired from the Deutsch-Jozsa one. In particular, we explore meticulously the choices of the pre- and post-selected states. We eventually test the novel theoretical algorithm on a quantum computing platform. While the outcomes are currently not on par with the conventional approach, they nevertheless shed light on potential for future improvements, especially with less-optimized algorithms. We are thus confidend that the proposed proof of concept could prove its validity in bridging quantum algorithms and modular values research fields.","sentences":["We present a novel approach to quantum algorithms, by taking advantage of modular values, i.e., complex and unbounded quantities resulting from specific post-selected measurement scenarios.","Our focus is on the problem of ascertaining whether a given function acting on a set of binary values is constant (uniformly yielding outputs of either all 0 or all 1), or balanced (a situation wherein half of the outputs are 0 and the other half are 1).","Such problem can be solved by relying on the Deutsch-Jozsa algorithm.","The proposed method, relying on the use of modular values, provides a high number of degrees of freedom for optimizing the new algorithm inspired from the Deutsch-Jozsa one.","In particular, we explore meticulously the choices of the pre- and post-selected states.","We eventually test the novel theoretical algorithm on a quantum computing platform.","While the outcomes are currently not on par with the conventional approach, they nevertheless shed light on potential for future improvements, especially with less-optimized algorithms.","We are thus confidend that the proposed proof of concept could prove its validity in bridging quantum algorithms and modular values research fields."],"url":"http://arxiv.org/abs/2406.06803v1","category":"quant-ph"}
{"created":"2024-06-10 20:52:14","title":"Stochastic Frank-Wolfe: Unified Analysis and Zoo of Special Cases","abstract":"The Conditional Gradient (or Frank-Wolfe) method is one of the most well-known methods for solving constrained optimization problems appearing in various machine learning tasks. The simplicity of iteration and applicability to many practical problems helped the method to gain popularity in the community. In recent years, the Frank-Wolfe algorithm received many different extensions, including stochastic modifications with variance reduction and coordinate sampling for training of huge models or distributed variants for big data problems. In this paper, we present a unified convergence analysis of the Stochastic Frank-Wolfe method that covers a large number of particular practical cases that may have completely different nature of stochasticity, intuitions and application areas. Our analysis is based on a key parametric assumption on the variance of the stochastic gradients. But unlike most works on unified analysis of other methods, such as SGD, we do not assume an unbiasedness of the real gradient estimation. We conduct analysis for convex and non-convex problems due to the popularity of both cases in machine learning. With this general theoretical framework, we not only cover rates of many known methods, but also develop numerous new methods. This shows the flexibility of our approach in developing new algorithms based on the Conditional Gradient approach. We also demonstrate the properties of the new methods through numerical experiments.","sentences":["The Conditional Gradient (or Frank-Wolfe) method is one of the most well-known methods for solving constrained optimization problems appearing in various machine learning tasks.","The simplicity of iteration and applicability to many practical problems helped the method to gain popularity in the community.","In recent years, the Frank-Wolfe algorithm received many different extensions, including stochastic modifications with variance reduction and coordinate sampling for training of huge models or distributed variants for big data problems.","In this paper, we present a unified convergence analysis of the Stochastic Frank-Wolfe method that covers a large number of particular practical cases that may have completely different nature of stochasticity, intuitions and application areas.","Our analysis is based on a key parametric assumption on the variance of the stochastic gradients.","But unlike most works on unified analysis of other methods, such as SGD, we do not assume an unbiasedness of the real gradient estimation.","We conduct analysis for convex and non-convex problems due to the popularity of both cases in machine learning.","With this general theoretical framework, we not only cover rates of many known methods, but also develop numerous new methods.","This shows the flexibility of our approach in developing new algorithms based on the Conditional Gradient approach.","We also demonstrate the properties of the new methods through numerical experiments."],"url":"http://arxiv.org/abs/2406.06788v1","category":"math.OC"}
{"created":"2024-06-10 19:50:16","title":"$Classi|Q\\rangle$ Towards a Translation Framework To Bridge The Classical-Quantum Programming Gap","abstract":"Quantum computing, albeit readily available as hardware or emulated on the cloud, is still far from being available in general regarding complex programming paradigms and learning curves. This vision paper introduces $Classi|Q\\rangle$, a translation framework idea to bridge Classical and Quantum Computing by translating high-level programming languages, e.g., Python or C++, into a low-level language, e.g., Quantum Assembly. Our idea paper serves as a blueprint for ongoing efforts in quantum software engineering, offering a roadmap for further $Classi|Q\\rangle$ development to meet the diverse needs of researchers and practitioners. $Classi|Q\\rangle$ is designed to empower researchers and practitioners with no prior quantum experience to harness the potential of hybrid quantum computation. We also discuss future enhancements to $Classi|Q\\rangle$, including support for additional quantum languages, improved optimization strategies, and integration with emerging quantum computing platforms.","sentences":["Quantum computing, albeit readily available as hardware or emulated on the cloud, is still far from being available in general regarding complex programming paradigms and learning curves.","This vision paper introduces $Classi|Q\\rangle$, a translation framework idea to bridge Classical and Quantum Computing by translating high-level programming languages, e.g., Python or C++, into a low-level language, e.g., Quantum Assembly.","Our idea paper serves as a blueprint for ongoing efforts in quantum software engineering, offering a roadmap for further $Classi|Q\\rangle$ development to meet the diverse needs of researchers and practitioners.","$Classi|Q\\rangle$ is designed to empower researchers and practitioners with no prior quantum experience to harness the potential of hybrid quantum computation.","We also discuss future enhancements to $Classi|Q\\rangle$, including support for additional quantum languages, improved optimization strategies, and integration with emerging quantum computing platforms."],"url":"http://arxiv.org/abs/2406.06764v1","category":"cs.SE"}
{"created":"2024-06-10 19:33:13","title":"Incremental Sliding Window Connectivity over Streaming Graphs","abstract":"We study index-based processing for connectivity queries within sliding windows on streaming graphs. These queries, which determine whether two vertices belong to the same connected component, are fundamental operations in real-time graph data processing and demand high throughput and low latency. While indexing methods that leverage data structures for fully dynamic connectivity can facilitate efficient query processing, they encounter significant challenges with deleting expired edges from the window during window updates. We introduce a novel indexing approach that eliminates the need for physically performing edge deletions. This is achieved through a unique bidirectional incremental computation framework, referred to as the BIC model. The BIC model implements two distinct incremental computations to compute connected components within the window, operating along and against the timeline, respectively. These computations are then merged to efficiently compute queries in the window. We propose techniques for optimized index storage, incremental index updates, and efficient query processing to improve BIC effectiveness. Empirically, BIC achieves a 14$\\times$ increase in throughput and a reduction in P95 latency by up to 3900$\\times$ when compared to state-of-the-art indexes.","sentences":["We study index-based processing for connectivity queries within sliding windows on streaming graphs.","These queries, which determine whether two vertices belong to the same connected component, are fundamental operations in real-time graph data processing and demand high throughput and low latency.","While indexing methods that leverage data structures for fully dynamic connectivity can facilitate efficient query processing, they encounter significant challenges with deleting expired edges from the window during window updates.","We introduce a novel indexing approach that eliminates the need for physically performing edge deletions.","This is achieved through a unique bidirectional incremental computation framework, referred to as the BIC model.","The BIC model implements two distinct incremental computations to compute connected components within the window, operating along and against the timeline, respectively.","These computations are then merged to efficiently compute queries in the window.","We propose techniques for optimized index storage, incremental index updates, and efficient query processing to improve BIC effectiveness.","Empirically, BIC achieves a 14$\\times$ increase in throughput and a reduction in P95 latency by up to 3900$\\times$ when compared to state-of-the-art indexes."],"url":"http://arxiv.org/abs/2406.06754v1","category":"cs.DB"}
{"created":"2024-06-10 19:23:40","title":"Starling Formation-Flying Optical Experiment: Initial Operations and Flight Results","abstract":"This paper presents initial flight results for distributed optical angles-only navigation of a swarm of small spacecraft, conducted during the Starling Formation-Flying Optical Experiment (StarFOX). StarFOX is a core payload of the NASA Starling mission, which consists of four CubeSats launched in 2023. Prior angles-only flight demonstrations have only featured one observer and target and have relied upon a-priori target orbit knowledge for initialization, translational maneuvers to resolve target range, and external absolute orbit updates to maintain convergence. StarFOX overcomes these limitations by applying the angles-only Absolute and Relative Trajectory Measurement System (ARTMS), which integrates three novel algorithms. Image Processing detects and tracks multiple targets in images from each satellite's on-board camera. Batch Orbit Determination computes initial swarm orbit estimates from bearing angle batches. Sequential Orbit Determination leverages an unscented Kalman filter to refine swarm state estimates over time. Multi-observer measurements shared over an intersatellite link are seamlessly fused to enable absolute and relative orbit determination. StarFOX flight data presents the first demonstrations of autonomous angles-only navigation for a satellite swarm, including multi-target and multi-observer relative navigation; autonomous initialization of navigation for unknown targets; and simultaneous absolute and relative orbit determination. Relative positioning uncertainties of 1.3% of target range (1$\\sigma$) are achieved for a single observer under challenging measurement conditions, reduced to 0.6% (1$\\sigma$) with multiple observers. Results demonstrate promising performance with regards to ongoing StarFOX campaigns and the application of angles-only navigation to future distributed missions.","sentences":["This paper presents initial flight results for distributed optical angles-only navigation of a swarm of small spacecraft, conducted during the Starling Formation-Flying Optical Experiment (StarFOX).","StarFOX is a core payload of the NASA Starling mission, which consists of four CubeSats launched in 2023.","Prior angles-only flight demonstrations have only featured one observer and target and have relied upon a-priori target orbit knowledge for initialization, translational maneuvers to resolve target range, and external absolute orbit updates to maintain convergence.","StarFOX overcomes these limitations by applying the angles-only Absolute and Relative Trajectory Measurement System (ARTMS), which integrates three novel algorithms.","Image Processing detects and tracks multiple targets in images from each satellite's on-board camera.","Batch Orbit Determination computes initial swarm orbit estimates from bearing angle batches.","Sequential Orbit Determination leverages an unscented Kalman filter to refine swarm state estimates over time.","Multi-observer measurements shared over an intersatellite link are seamlessly fused to enable absolute and relative orbit determination.","StarFOX flight data presents the first demonstrations of autonomous angles-only navigation for a satellite swarm, including multi-target and multi-observer relative navigation; autonomous initialization of navigation for unknown targets; and simultaneous absolute and relative orbit determination.","Relative positioning uncertainties of 1.3% of target range (1$\\sigma$) are achieved for a single observer under challenging measurement conditions, reduced to 0.6% (1$\\sigma$) with multiple observers.","Results demonstrate promising performance with regards to ongoing StarFOX campaigns and the application of angles-only navigation to future distributed missions."],"url":"http://arxiv.org/abs/2406.06748v1","category":"cs.RO"}
{"created":"2024-06-10 19:05:21","title":"A Multi-module Robust Method for Transient Stability Assessment against False Label Injection Cyberattacks","abstract":"The success of deep learning in transient stability assessment (TSA) heavily relies on high-quality training data. However, the label information in TSA datasets is vulnerable to contamination through false label injection (FLI) cyberattacks, resulting in degraded performance of deep TSA models. To address this challenge, a Multi-Module Robust TSA method (MMR) is proposed to rectify the supervised training process misguided by FLI in an unsupervised manner. In MMR, a supervised classification module and an unsupervised clustering module are alternatively trained to improve the clustering friendliness of representation leaning, thereby achieving accurate clustering assignments. Leveraging the clustering assignments, we construct a training label corrector to rectify the injected false labels and progressively enhance robustness and resilience against FLI. However, there is still a gap on accuracy and convergence speed between MMR and FLI-free deep TSA models. To narrow this gap, we further propose a human-in-the-loop training strategy, named MMR-HIL. In MMR-HIL, potential false samples can be detected by modeling the training loss with a Gaussian distribution. From these samples, the most likely false samples and most ambiguous samples are re-labeled by a TSA experts guided bi-directional annotator and then subjected to penalized optimization, aimed at improving accuracy and convergence speed. Extensive experiments indicate that MMR and MMR-HIL both exhibit powerful robustness against FLI in TSA performance. Moreover, the contaminated labels can also be effectively corrected, demonstrating superior resilience of the proposed methods.","sentences":["The success of deep learning in transient stability assessment (TSA) heavily relies on high-quality training data.","However, the label information in TSA datasets is vulnerable to contamination through false label injection (FLI) cyberattacks, resulting in degraded performance of deep TSA models.","To address this challenge, a Multi-Module Robust TSA method (MMR) is proposed to rectify the supervised training process misguided by FLI in an unsupervised manner.","In MMR, a supervised classification module and an unsupervised clustering module are alternatively trained to improve the clustering friendliness of representation leaning, thereby achieving accurate clustering assignments.","Leveraging the clustering assignments, we construct a training label corrector to rectify the injected false labels and progressively enhance robustness and resilience against FLI.","However, there is still a gap on accuracy and convergence speed between MMR and FLI-free deep TSA models.","To narrow this gap, we further propose a human-in-the-loop training strategy, named MMR-HIL.","In MMR-HIL, potential false samples can be detected by modeling the training loss with a Gaussian distribution.","From these samples, the most likely false samples and most ambiguous samples are re-labeled by a TSA experts guided bi-directional annotator and then subjected to penalized optimization, aimed at improving accuracy and convergence speed.","Extensive experiments indicate that MMR and MMR-HIL both exhibit powerful robustness against FLI in TSA performance.","Moreover, the contaminated labels can also be effectively corrected, demonstrating superior resilience of the proposed methods."],"url":"http://arxiv.org/abs/2406.06744v1","category":"cs.LG"}
{"created":"2024-06-10 19:05:13","title":"Gravitational Radiation Power Spectrum of Garfinkle-Vachaspati Cosmic String Loops","abstract":"We examine the power spectrum $P_n$ of the $n$th harmonic for Garfinkle-Vachaspati cosmic strings, which correspond to planar rectangular loops. While these loop tractories are self-intersecting, a slightly perturbed non-self-intersecting form of these trajectories has been suggested as a generic end state of cosmic string loop fragmentation. We find that $P_n$ scales as $n^{-2} \\ln n$, rather than the expected $n^{-2}$ for kink-kink collisions. This result is demonstrated analytically for even $n$ in square loops and numerically for all other cases. At lowest order, the effect of loop decays is to further enhance $P_n$ at large $n$ relative to its value at small $n$. The consequences for relic stochastic background radiation along with the caveats pertaining to our results are discussed.","sentences":["We examine the power spectrum $P_n$ of the $n$th harmonic for Garfinkle-Vachaspati cosmic strings, which correspond to planar rectangular loops.","While these loop tractories are self-intersecting, a slightly perturbed non-self-intersecting form of these trajectories has been suggested as a generic end state of cosmic string loop fragmentation.","We find that $P_n$ scales as $n^{-2} \\ln n$, rather than the expected $n^{-2}$ for kink-kink collisions.","This result is demonstrated analytically for even $n$ in square loops and numerically for all other cases.","At lowest order, the effect of loop decays is to further enhance $P_n$ at large $n$ relative to its value at small $n$. The consequences for relic stochastic background radiation along with the caveats pertaining to our results are discussed."],"url":"http://arxiv.org/abs/2406.06743v1","category":"gr-qc"}
{"created":"2024-06-10 19:01:15","title":"Scaling the Vocabulary of Non-autoregressive Models for Efficient Generative Retrieval","abstract":"Generative Retrieval introduces a new approach to Information Retrieval by reframing it as a constrained generation task, leveraging recent advancements in Autoregressive (AR) language models. However, AR-based Generative Retrieval methods suffer from high inference latency and cost compared to traditional dense retrieval techniques, limiting their practical applicability. This paper investigates fully Non-autoregressive (NAR) language models as a more efficient alternative for generative retrieval. While standard NAR models alleviate latency and cost concerns, they exhibit a significant drop in retrieval performance (compared to AR models) due to their inability to capture dependencies between target tokens. To address this, we question the conventional choice of limiting the target token space to solely words or sub-words. We propose PIXAR, a novel approach that expands the target vocabulary of NAR models to include multi-word entities and common phrases (up to 5 million tokens), thereby reducing token dependencies. PIXAR employs inference optimization strategies to maintain low inference latency despite the significantly larger vocabulary. Our results demonstrate that PIXAR achieves a relative improvement of 31.0% in MRR@10 on MS MARCO and 23.2% in Hits@5 on Natural Questions compared to standard NAR models with similar latency and cost. Furthermore, online A/B experiments on a large commercial search engine show that PIXAR increases ad clicks by 5.08% and revenue by 4.02%.","sentences":["Generative Retrieval introduces a new approach to Information Retrieval by reframing it as a constrained generation task, leveraging recent advancements in Autoregressive (AR) language models.","However, AR-based Generative Retrieval methods suffer from high inference latency and cost compared to traditional dense retrieval techniques, limiting their practical applicability.","This paper investigates fully Non-autoregressive (NAR) language models as a more efficient alternative for generative retrieval.","While standard NAR models alleviate latency and cost concerns, they exhibit a significant drop in retrieval performance (compared to AR models) due to their inability to capture dependencies between target tokens.","To address this, we question the conventional choice of limiting the target token space to solely words or sub-words.","We propose PIXAR, a novel approach that expands the target vocabulary of NAR models to include multi-word entities and common phrases (up to 5 million tokens), thereby reducing token dependencies.","PIXAR employs inference optimization strategies to maintain low inference latency despite the significantly larger vocabulary.","Our results demonstrate that PIXAR achieves a relative improvement of 31.0% in MRR@10 on MS MARCO and 23.2% in Hits@5 on Natural Questions compared to standard NAR models with similar latency and cost.","Furthermore, online A/B experiments on a large commercial search engine show that PIXAR increases ad clicks by 5.08% and revenue by 4.02%."],"url":"http://arxiv.org/abs/2406.06739v1","category":"cs.CL"}
{"created":"2024-06-10 18:15:01","title":"Application of Black-Litterman Bayesian in Statistical Arbitrage","abstract":"\\begin{abstract} In this paper, we integrated the statistical arbitrage strategy, pairs trading, into the Black-Litterman model and constructed efficient mean-variance portfolios. Typically, pairs trading underperforms under volatile or distressed market condition because the selected asset pairs fail to revert to equilibrium within the investment horizon. By enhancing this strategy with the Black-Litterman portfolio optimization, we achieved superior performance compared to the S\\&P 500 market index under both normal and extreme market conditions. Furthermore, this research presents an innovative idea of incorporating traditional pairs trading strategies into the portfolio optimization framework in a scalable and systematic manner.","sentences":["\\begin{abstract} In this paper, we integrated the statistical arbitrage strategy, pairs trading, into the Black-Litterman model and constructed efficient mean-variance portfolios.","Typically, pairs trading underperforms under volatile or distressed market condition because the selected asset pairs fail to revert to equilibrium within the investment horizon.","By enhancing this strategy with the Black-Litterman portfolio optimization, we achieved superior performance compared to the S\\&P 500 market index under both normal and extreme market conditions.","Furthermore, this research presents an innovative idea of incorporating traditional pairs trading strategies into the portfolio optimization framework in a scalable and systematic manner."],"url":"http://arxiv.org/abs/2406.06706v1","category":"q-fin.CP"}
{"created":"2024-06-10 18:05:02","title":"Video-based Exercise Classification and Activated Muscle Group Prediction with Hybrid X3D-SlowFast Network","abstract":"This paper introduces a simple yet effective strategy for exercise classification and muscle group activation prediction (MGAP). These tasks have significant implications for personal fitness, facilitating more affordable, accessible, safer, and simpler exercise routines. This is particularly relevant for novices and individuals with disabilities. Previous research in the field is mostly dominated by the reliance on mounted sensors and a limited scope of exercises, reducing practicality for everyday use. Furthermore, existing MGAP methodologies suffer from a similar dependency on sensors and a restricted range of muscle groups, often excluding strength training exercises, which are pivotal for a comprehensive fitness regimen. Addressing these limitations, our research employs a video-based deep learning framework that encompasses a broad spectrum of exercises and muscle groups, including those vital for strength training. Utilizing the \"Workout/Exercises Video\" dataset, our approach integrates the X3D and SlowFast video activity recognition models in an effective way to enhance exercise classification and MGAP performance. Our findings demonstrate that this hybrid method obtained via weighted ensemble outperforms existing baseline models in accuracy. Pretrained models play a crucial role in enhancing overall performance, with optimal channel reduction values for the SlowFast model identified near 10. Through an ablation study that explores fine-tuning, we further elucidate the interrelation between the two tasks. Our composite model, a weighted-average ensemble of X3D and SlowFast, sets a new benchmark in both exercise classification and MGAP across all evaluated categories, offering a robust solution to the limitations of previous approaches.","sentences":["This paper introduces a simple yet effective strategy for exercise classification and muscle group activation prediction (MGAP).","These tasks have significant implications for personal fitness, facilitating more affordable, accessible, safer, and simpler exercise routines.","This is particularly relevant for novices and individuals with disabilities.","Previous research in the field is mostly dominated by the reliance on mounted sensors and a limited scope of exercises, reducing practicality for everyday use.","Furthermore, existing MGAP methodologies suffer from a similar dependency on sensors and a restricted range of muscle groups, often excluding strength training exercises, which are pivotal for a comprehensive fitness regimen.","Addressing these limitations, our research employs a video-based deep learning framework that encompasses a broad spectrum of exercises and muscle groups, including those vital for strength training.","Utilizing the \"Workout/Exercises Video\" dataset, our approach integrates the X3D and SlowFast video activity recognition models in an effective way to enhance exercise classification and MGAP performance.","Our findings demonstrate that this hybrid method obtained via weighted ensemble outperforms existing baseline models in accuracy.","Pretrained models play a crucial role in enhancing overall performance, with optimal channel reduction values for the SlowFast model identified near 10.","Through an ablation study that explores fine-tuning, we further elucidate the interrelation between the two tasks.","Our composite model, a weighted-average ensemble of X3D and SlowFast, sets a new benchmark in both exercise classification and MGAP across all evaluated categories, offering a robust solution to the limitations of previous approaches."],"url":"http://arxiv.org/abs/2406.06703v1","category":"cs.CV"}
{"created":"2024-06-10 18:03:22","title":"The XMM-SERVS X-ray eXtended Galaxy Cluster (XVXGC) catalog","abstract":"To explain the well-known tension between cosmological parameter constraints obtained from the primary CMB and those drawn from galaxy cluster samples, we propose a possible explanation for the incompleteness of detected clusters are higher than estimated. We aim to search for galaxy groups and clusters with particularly extended surface brightness distributions by creating a new X-ray-selected catalog of extended galaxy clusters from the XMM-SERVS data, based on a dedicated source detection and characterization algorithm that is optimized for extended sources. Our state-of-the-art algorithm is composed of wavelet filtering, source detection, and characterization. We make a visual inspection of the optical image, and spatial distribution of galaxies within the same redshift layer to confirm the existence of clusters and estimate the cluster redshift with the spectroscopic and photometric redshifts of galaxies. The growth curve analysis is used to characterize the detections. We report a catalog of extended X-ray galaxy clusters detected from the XMM-SERVS data, named the XMM- SERVS X-ray eXtended Galaxy Cluster (XVXGC) catalog. It includes 141 cluster candidates. Specifically, there are 52 clusters previously identified as clusters with the intra-cluster medium (ICM) emission (class 3), 37 ones previously known as optical or infrared clusters but detected as X-ray clusters for the first time (class 2), and 52 identified as clusters for the first time (class 1). Compared with the class3 sample, the 'class1+2' sample is systematically fainter, and exhibits a flatter surface brightness profile. The median flux in [0.1-2.4]keV band for 'class1+2' and class3 sample is 2.336e-14 and 3.163e-14erg/s/cm2, respectively. The median slope of surface brightness profile are 0.502 and 0.577 for the 'class1+2' and class 3 samples, respectively.","sentences":["To explain the well-known tension between cosmological parameter constraints obtained from the primary CMB and those drawn from galaxy cluster samples, we propose a possible explanation for the incompleteness of detected clusters are higher than estimated.","We aim to search for galaxy groups and clusters with particularly extended surface brightness distributions by creating a new X-ray-selected catalog of extended galaxy clusters from the XMM-SERVS data, based on a dedicated source detection and characterization algorithm that is optimized for extended sources.","Our state-of-the-art algorithm is composed of wavelet filtering, source detection, and characterization.","We make a visual inspection of the optical image, and spatial distribution of galaxies within the same redshift layer to confirm the existence of clusters and estimate the cluster redshift with the spectroscopic and photometric redshifts of galaxies.","The growth curve analysis is used to characterize the detections.","We report a catalog of extended X-ray galaxy clusters detected from the XMM-SERVS data, named the XMM- SERVS X-ray eXtended Galaxy Cluster (XVXGC) catalog.","It includes 141 cluster candidates.","Specifically, there are 52 clusters previously identified as clusters with the intra-cluster medium (ICM) emission (class 3), 37 ones previously known as optical or infrared clusters but detected as X-ray clusters for the first time (class 2), and 52 identified as clusters for the first time (class 1).","Compared with the class3 sample, the 'class1+2' sample is systematically fainter, and exhibits a flatter surface brightness profile.","The median flux in [0.1-2.4]keV band for 'class1+2' and class3 sample is 2.336e-14 and 3.163e-14erg/s/cm2, respectively.","The median slope of surface brightness profile are 0.502 and 0.577 for the 'class1+2' and class 3 samples, respectively."],"url":"http://arxiv.org/abs/2406.06701v1","category":"astro-ph.CO"}
{"created":"2024-06-11 16:50:36","title":"QCD in the chiral SU(3) limit from baryon masses on Lattice QCD ensembles","abstract":"The baryon masses on CLS ensembles are used to determine the LEC that characterize QCD in the flavor-SU(3) limit with vanishing up, down, and strange quark masses. Here we reevaluate some of the baryon masses on flavor-symmetric ensembles with much-improved statistical precision, in particular for the decuplet states. These additional results then lead to a more significant chiral extrapolation of the Lattice data set to its chiral SU(3) limit. Our results are based on the chiral Lagrangian with baryon octet and decuplet fields considered at the one-loop level. Finite-box and discretization effects of the Lattice data are considered systematically. While in our global fit of the data we insist on large-Nc sum rules for the LEC that enter at N3LO, all other LEC are unconstrained. In particular, we obtain values for the chiral limit of the pion decay constant and the isospin-limit of the quark-mass ratio compatible with the FLAG report.","sentences":["The baryon masses on CLS ensembles are used to determine the LEC that characterize QCD in the flavor-SU(3) limit with vanishing up, down, and strange quark masses.","Here we reevaluate some of the baryon masses on flavor-symmetric ensembles with much-improved statistical precision, in particular for the decuplet states.","These additional results then lead to a more significant chiral extrapolation of the Lattice data set to its chiral SU(3) limit.","Our results are based on the chiral Lagrangian with baryon octet and decuplet fields considered at the one-loop level.","Finite-box and discretization effects of the Lattice data are considered systematically.","While in our global fit of the data we insist on large-Nc sum rules for the LEC that enter at N3LO, all other LEC are unconstrained.","In particular, we obtain values for the chiral limit of the pion decay constant and the isospin-limit of the quark-mass ratio compatible with the FLAG report."],"url":"http://arxiv.org/abs/2406.07442v1","category":"hep-lat"}
{"created":"2024-06-11 16:28:49","title":"Support for fragile porous dust in a gravitationally self-regulated disk around IM Lup","abstract":"Protoplanetary disks, the birthplace of planets, are expected to be gravitationally unstable in their early phase of evolution. IM Lup, a well-known T-Tauri star, is surrounded by a protoplanetary disk with spiral arms likely caused by gravitational instability. The IM Lup disk has been observed using various methods, but developing a unified explanatory model is challenging. Here we present a physical model of the IM Lup disk that offers a comprehensive explanation for diverse observations spanning from near-infrared to millimeter wavelengths. Our findings underscore the importance of dust fragility in retaining the observed millimeter emission and reveal the preference for moderately porous dust to explain observed millimeter polarization. We also find that the inner disk region is likely heated by gas accretion, providing a natural explanation for bright millimeter emission within 20 au. The actively heated inner region in the model casts a 100-au-scale shadow, aligning seamlessly with the near-infrared scattered light observation. The presence of accretion heating also supports the fragile dust scenario in which accretion efficiently heat the disk midplane. Due to the fragility of dust, it is unlikely that a potential embedded planet at 100 au formed via pebble accretion in a smooth disk, pointing to local dust enhancement boosting pebble accretion or alternative pathways such as outward migration or gravitational fragmentation.","sentences":["Protoplanetary disks, the birthplace of planets, are expected to be gravitationally unstable in their early phase of evolution.","IM Lup, a well-known T-Tauri star, is surrounded by a protoplanetary disk with spiral arms likely caused by gravitational instability.","The IM Lup disk has been observed using various methods, but developing a unified explanatory model is challenging.","Here we present a physical model of the IM Lup disk that offers a comprehensive explanation for diverse observations spanning from near-infrared to millimeter wavelengths.","Our findings underscore the importance of dust fragility in retaining the observed millimeter emission and reveal the preference for moderately porous dust to explain observed millimeter polarization.","We also find that the inner disk region is likely heated by gas accretion, providing a natural explanation for bright millimeter emission within 20 au.","The actively heated inner region in the model casts a 100-au-scale shadow, aligning seamlessly with the near-infrared scattered light observation.","The presence of accretion heating also supports the fragile dust scenario in which accretion efficiently heat the disk midplane.","Due to the fragility of dust, it is unlikely that a potential embedded planet at 100 au formed via pebble accretion in a smooth disk, pointing to local dust enhancement boosting pebble accretion or alternative pathways such as outward migration or gravitational fragmentation."],"url":"http://arxiv.org/abs/2406.07427v1","category":"astro-ph.EP"}
{"created":"2024-06-11 16:17:09","title":"Helium-4 gravitational form factors: exchange currents","abstract":"We evaluate the leading exchange corrections to the Helium-4 gravitational form factors (GFFs) upto momenta of the order of the nucleon mass. We use both the K-harmonic method with simple pair nucleon potential, and a Jastrow trial function using the Argonne $v_{14}$ potential, to evaluate the Helium-4 GFFs. The exchange current contributions include the pair interaction, plus the seagull and the pion exchange interactions, modulo the recoil corrections. To estimate the off-shellness of the pion nucleon coupling in this momenta range, we discuss the results using either the pseudo-scalar (PS) or pseudo-vector (PV) pion-nucleon couplings. When the PV coupling is used, the pair diagram contribution is higher order in the non relativistic expansion. The results for the Helium-4 A-GFF are comparable to those given by the impulse approximation, especially for the PS coupling using both the K-Harmonic method and variational method. The exchange current contributions with the PS coupling for the charge form factor of Helium-4, yield better agreement with the existing data over a broad range of momenta, especially when the Argonne $v_{14}$ potential including the D-wave admixture is used.","sentences":["We evaluate the leading exchange corrections to the Helium-4 gravitational form factors (GFFs) upto momenta of the order of the nucleon mass.","We use both the K-harmonic method with simple pair nucleon potential, and a Jastrow trial function using the Argonne $v_{14}$ potential, to evaluate the Helium-4 GFFs.","The exchange current contributions include the pair interaction, plus the seagull and the pion exchange interactions, modulo the recoil corrections.","To estimate the off-shellness of the pion nucleon coupling in this momenta range, we discuss the results using either the pseudo-scalar (PS) or pseudo-vector (PV) pion-nucleon couplings.","When the PV coupling is used, the pair diagram contribution is higher order in the non relativistic expansion.","The results for the Helium-4 A-GFF are comparable to those given by the impulse approximation, especially for the PS coupling using both the K-Harmonic method and variational method.","The exchange current contributions with the PS coupling for the charge form factor of Helium-4, yield better agreement with the existing data over a broad range of momenta, especially when the Argonne $v_{14}$ potential including the D-wave admixture is used."],"url":"http://arxiv.org/abs/2406.07412v1","category":"nucl-th"}
{"created":"2024-06-11 15:58:59","title":"Limited Out-of-Context Knowledge Reasoning in Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated strong capabilities as knowledge bases and significant in-context reasoning capabilities. However, previous work challenges their out-of-context reasoning ability, i.e., the ability to infer information from their training data, instead of from the context or prompt. This paper focuses on a significant facet of out-of-context reasoning: Out-of-Context Knowledge Reasoning (OCKR), which is to combine multiple knowledge to infer new knowledge. We designed a synthetic dataset with seven representative OCKR tasks to systematically assess the OCKR capabilities of LLMs. Using this dataset, we evaluated the LLaMA2-13B-chat model and discovered that its proficiency in this aspect is limited, regardless of whether the knowledge is trained in a separate or adjacent training settings. Moreover, training the model to reason with complete reasoning data did not result in significant improvement. Training the model to perform explicit knowledge retrieval helps in only one of the tasks, indicating that the model's limited OCKR capabilities are due to difficulties in retrieving relevant knowledge. Furthermore, we treat cross-lingual knowledge transfer as a distinct form of OCKR, and evaluate this ability. Our results show that the evaluated model also exhibits limited ability in transferring knowledge across languages. The dataset used in this study is available at https://github.com/NJUNLP/ID-OCKR.","sentences":["Large Language Models (LLMs) have demonstrated strong capabilities as knowledge bases and significant in-context reasoning capabilities.","However, previous work challenges their out-of-context reasoning ability, i.e., the ability to infer information from their training data, instead of from the context or prompt.","This paper focuses on a significant facet of out-of-context reasoning: Out-of-Context Knowledge Reasoning (OCKR), which is to combine multiple knowledge to infer new knowledge.","We designed a synthetic dataset with seven representative OCKR tasks to systematically assess the OCKR capabilities of LLMs.","Using this dataset, we evaluated the LLaMA2-13B-chat model and discovered that its proficiency in this aspect is limited, regardless of whether the knowledge is trained in a separate or adjacent training settings.","Moreover, training the model to reason with complete reasoning data did not result in significant improvement.","Training the model to perform explicit knowledge retrieval helps in only one of the tasks, indicating that the model's limited OCKR capabilities are due to difficulties in retrieving relevant knowledge.","Furthermore, we treat cross-lingual knowledge transfer as a distinct form of OCKR, and evaluate this ability.","Our results show that the evaluated model also exhibits limited ability in transferring knowledge across languages.","The dataset used in this study is available at https://github.com/NJUNLP/ID-OCKR."],"url":"http://arxiv.org/abs/2406.07393v1","category":"cs.CL"}
{"created":"2024-06-11 15:58:23","title":"Net Zero Averted Temperature Increase","abstract":"Using feedback-free estimates of the warming by increased atmospheric carbon dioxide (CO2) and observed rates of increase, we estimate that if the United States (U.S.) eliminated net CO2 emissions by the year 2050, this would avert a warming of 0.0084 C (0.015 F), which is below our ability to accurately measure. If the entire world forced net zero CO2 emissions by the year 2050, a warming of only 0.070 C (0.13 F) would be averted. If one assumes that the warming is a factor of 4 larger because of positive feedbacks, as asserted by the Intergovernmental Panel on Climate Change (IPCC), the warming averted by a net zero U.S. policy would still be very small, 0.034 C (0.061 F). For worldwide net zero emissions by 2050 and the 4-times larger IPCC climate sensitivity, the averted warming would be 0.28 C (0.50 F).","sentences":["Using feedback-free estimates of the warming by increased atmospheric carbon dioxide (CO2) and observed rates of increase, we estimate that if the United States (U.S.) eliminated net CO2 emissions by the year 2050, this would avert a warming of 0.0084 C (0.015 F), which is below our ability to accurately measure.","If the entire world forced net zero CO2 emissions by the year 2050, a warming of only 0.070 C (0.13 F) would be averted.","If one assumes that the warming is a factor of 4 larger because of positive feedbacks, as asserted by the Intergovernmental Panel on Climate Change (IPCC), the warming averted by a net zero U.S. policy would still be very small, 0.034 C (0.061 F).","For worldwide net zero emissions by 2050 and the 4-times larger IPCC climate sensitivity, the averted warming would be 0.28 C (0.50 F)."],"url":"http://arxiv.org/abs/2406.07392v1","category":"physics.ao-ph"}
{"created":"2024-06-11 15:41:05","title":"Searching for rapid pulsations in solar flare X-ray data","abstract":"Most studies of quasi-periodic pulsations in solar flares have identified characteristic periods in the 5 - 300s range. Due to observational limitations there have been few attempts to probe the < 5s period regime and understand the prevalence of such short-period quasi-periodic pulsations. However, the Fermi Gamma-ray Burst Monitor (GBM) has observed approximately 1500 solar flares to date in high cadence 16 Hz burst mode, providing us with an opportunity to study short-period quasi-periodic pulsations at X-ray energies. We systematically analyse every solar flare observed by Fermi/GBM in burst mode, estimating the prevalence of quasi-periodic pulsations in multiple X-ray energy bands. To better understand these results, we complement this with analysis of synthetic solar flare lightcurves, both with and without oscillatory signals present. Using these synthetic lightcurves, we can understand the likely false alarm and true positive rates in the real solar GBM data. We do not find strong evidence for widespread short-period quasi-periodic pulsations, indicating either a low base occurrence rate of such signatures or that their typical signal-to-noise ratios must be low - less than 1 - in Fermi/GBM data. Finally, we present a selection of the most interesting potential quasi-periodic pulsation events that were identified in the GBM solar X-ray data.","sentences":["Most studies of quasi-periodic pulsations in solar flares have identified characteristic periods in the 5 - 300s range.","Due to observational limitations there have been few attempts to probe the < 5s period regime and understand the prevalence of such short-period quasi-periodic pulsations.","However, the Fermi Gamma-ray Burst Monitor (GBM) has observed approximately 1500 solar flares to date in high cadence 16","Hz burst mode, providing us with an opportunity to study short-period quasi-periodic pulsations at X-ray energies.","We systematically analyse every solar flare observed by Fermi/GBM in burst mode, estimating the prevalence of quasi-periodic pulsations in multiple X-ray energy bands.","To better understand these results, we complement this with analysis of synthetic solar flare lightcurves, both with and without oscillatory signals present.","Using these synthetic lightcurves, we can understand the likely false alarm and true positive rates in the real solar GBM data.","We do not find strong evidence for widespread short-period quasi-periodic pulsations, indicating either a low base occurrence rate of such signatures or that their typical signal-to-noise ratios must be low - less than 1 - in Fermi/GBM data.","Finally, we present a selection of the most interesting potential quasi-periodic pulsation events that were identified in the GBM solar X-ray data."],"url":"http://arxiv.org/abs/2406.07372v1","category":"astro-ph.SR"}
{"created":"2024-06-11 15:29:01","title":"Comments on: \"Almost All Carbon/Oxygen White Dwarfs Can Support Double Detonations\"","abstract":"I critically review some claims in the paper ``Almost All Carbon/Oxygen White Dwarfs Can Support Double Detonations'' (arXiv:2405.19417). The claim of that paper that the community converges on a leading scenario of type Ia supernovae (SNe Ia), the double detonation scenario, is wrong, as hundreds of papers in recent years study five and more different SN Ia scenarios and their channels. Moreover, the finding by that paper that the double detonation scenario with the explosion of the secondary white dwarf (WD; the mass-donor WD) is common, i.e., the triple-detonation channel and the quadruple-detonation sub-channel, implies highly no-spherical explosions. The highly non-spherical explosions contradict the morphologies of many SN Ia remnants. I find that the results of that paper strengthen the claim that the double detonation scenario (with its channels) might account for a non-negligible fraction of peculiar SNe Ia but only for a very small fraction (or non at all) of normal SNe Ia.","sentences":["I critically review some claims in the paper ``Almost All Carbon/Oxygen White Dwarfs Can Support Double Detonations'' (arXiv:2405.19417).","The claim of that paper that the community converges on a leading scenario of type Ia supernovae (SNe Ia), the double detonation scenario, is wrong, as hundreds of papers in recent years study five and more different SN Ia scenarios and their channels.","Moreover, the finding by that paper that the double detonation scenario with the explosion of the secondary white dwarf (WD; the mass-donor WD) is common, i.e., the triple-detonation channel and the quadruple-detonation sub-channel, implies highly no-spherical explosions.","The highly non-spherical explosions contradict the morphologies of many SN Ia remnants.","I find that the results of that paper strengthen the claim that the double detonation scenario (with its channels) might account for a non-negligible fraction of peculiar SNe Ia but only for a very small fraction (or non at all) of normal SNe Ia."],"url":"http://arxiv.org/abs/2406.07363v1","category":"astro-ph.HE"}
{"created":"2024-06-11 15:02:51","title":"Fractonic solids","abstract":"Fractons are exotic quasiparticles whose mobility in space is restricted by symmetries. In potential real-world realisations, fractons are likely lodged to a physical material rather than absolute space. Motivated by this, we propose and explore a new symmetry principle that restricts the motion of fractons relative to a physical solid. Unlike models with restricted mobility in absolute space, these fractonic solids admit gauge-invariant momentum density, are compatible with boost symmetry, and can consistently be coupled to gravity. We also propose a holographic model for fractonic solids.","sentences":["Fractons are exotic quasiparticles whose mobility in space is restricted by symmetries.","In potential real-world realisations, fractons are likely lodged to a physical material rather than absolute space.","Motivated by this, we propose and explore a new symmetry principle that restricts the motion of fractons relative to a physical solid.","Unlike models with restricted mobility in absolute space, these fractonic solids admit gauge-invariant momentum density, are compatible with boost symmetry, and can consistently be coupled to gravity.","We also propose a holographic model for fractonic solids."],"url":"http://arxiv.org/abs/2406.07334v1","category":"hep-th"}
{"created":"2024-06-11 15:01:03","title":"Text Information Retrieval in Tetun: A Preliminary Study","abstract":"Tetun is one of Timor-Leste's official languages alongside Portuguese. It is a low-resource language with over 932,400 speakers that started developing when Timor-Leste restored its independence in 2002. The media mainly uses Tetun, and more than ten national online newspapers actively broadcast news in Tetun every day. However, since information retrieval-based solutions for Tetun do not exist, finding Tetun information on the internet is challenging. This work aims to investigate and develop solutions that can enable the application of information retrieval techniques to develop search solutions for Tetun. We present a preliminary result of an experiment conducted on the task of ad-hoc retrieval in Tetun.","sentences":["Tetun is one of Timor-Leste's official languages alongside Portuguese.","It is a low-resource language with over 932,400 speakers that started developing when Timor-Leste restored its independence in 2002.","The media mainly uses Tetun, and more than ten national online newspapers actively broadcast news in Tetun every day.","However, since information retrieval-based solutions for Tetun do not exist, finding Tetun information on the internet is challenging.","This work aims to investigate and develop solutions that can enable the application of information retrieval techniques to develop search solutions for Tetun.","We present a preliminary result of an experiment conducted on the task of ad-hoc retrieval in Tetun."],"url":"http://arxiv.org/abs/2406.07331v1","category":"cs.IR"}
{"created":"2024-06-11 14:50:56","title":"The magic of entangled top quarks","abstract":"Recent years have seen an increasing body of work examining how quantum entanglement can be measured at high energy particle physics experiments, thereby complementing traditional table-top experiments. This begs the question of whether more concepts from quantum computation can be examined at colliders, and we here consider the property of magic, which distinguishes those quantum states which have a genuine computational advantage over classical states. We examine top anti-top pair production at the LHC, showing that nature chooses to produce magic tops, where the amount of magic varies with the kinematics of the final state. We compare results for individual partonic channels and at proton-level, showing that averaging over final states typically increases magic. This is in contrast to entanglement measures, such as the concurrence, which typically decrease. Our results create new links between the quantum information and particle physics literatures, providing practical insights for further study.","sentences":["Recent years have seen an increasing body of work examining how quantum entanglement can be measured at high energy particle physics experiments, thereby complementing traditional table-top experiments.","This begs the question of whether more concepts from quantum computation can be examined at colliders, and we here consider the property of magic, which distinguishes those quantum states which have a genuine computational advantage over classical states.","We examine top anti-top pair production at the LHC, showing that nature chooses to produce magic tops, where the amount of magic varies with the kinematics of the final state.","We compare results for individual partonic channels and at proton-level, showing that averaging over final states typically increases magic.","This is in contrast to entanglement measures, such as the concurrence, which typically decrease.","Our results create new links between the quantum information and particle physics literatures, providing practical insights for further study."],"url":"http://arxiv.org/abs/2406.07321v1","category":"hep-ph"}
{"created":"2024-06-11 14:46:44","title":"Multimessenger constraints for electrophilic feebly interacting particles from supernovae","abstract":"Several extensions of the Standard Model predict the existence of sub-GeV particles that can be copiously produced in the cores of supernovae. A broad family of these particles are dubbed feebly interacting particles (FIPs), which can have masses of up to a few hundreds of MeV. Here, we review the most recent and leading constraints on electrophilic FIPs, describing multimessenger techniques that allow us to probe the full phenomenology of the electron/positron emission produced by these FIPs; from their associated X-ray emission to the production of the $511$~keV line. Furthermore, the approach described here is independent of the specific particle model and can be translated to the coupling and other properties of a variety of different particles, such as axion-like particles, sterile neutrinos or dark photons","sentences":["Several extensions of the Standard Model predict the existence of sub-GeV particles that can be copiously produced in the cores of supernovae.","A broad family of these particles are dubbed feebly interacting particles (FIPs), which can have masses of up to a few hundreds of MeV. Here, we review the most recent and leading constraints on electrophilic FIPs, describing multimessenger techniques that allow us to probe the full phenomenology of the electron/positron emission produced by these FIPs; from their associated X-ray emission to the production of the $511$~keV line.","Furthermore, the approach described here is independent of the specific particle model and can be translated to the coupling and other properties of a variety of different particles, such as axion-like particles, sterile neutrinos or dark photons"],"url":"http://arxiv.org/abs/2406.07316v1","category":"hep-ph"}
{"created":"2024-06-11 14:08:15","title":"Orbital paramagnetism without density of states enhancement in nodal-line semimetal ZrSiS","abstract":"Unconventional orbital paramagnetism without enhancement of the density of states was recently discovered in the nodal-line semimetal ZrSiS. Here, we propose a novel interband mechanism of orbital paramagnetism associated with the negative curvature of energy dispersions, which successfully explains the observed anomalous orbital paramagnetism. This negative curvature arises from energy fluctuations along the nodal line, inherent in realistic nodal-line materials. Our new mechanism indicates that such orbital paramagnetism serves as strong evidence for the existence of nodal lines not only in ZrSiS but potentially in various other nodal-line materials as well.","sentences":["Unconventional orbital paramagnetism without enhancement of the density of states was recently discovered in the nodal-line semimetal ZrSiS. Here, we propose a novel interband mechanism of orbital paramagnetism associated with the negative curvature of energy dispersions, which successfully explains the observed anomalous orbital paramagnetism.","This negative curvature arises from energy fluctuations along the nodal line, inherent in realistic nodal-line materials.","Our new mechanism indicates that such orbital paramagnetism serves as strong evidence for the existence of nodal lines not only in ZrSiS but potentially in various other nodal-line materials as well."],"url":"http://arxiv.org/abs/2406.07281v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-11 13:40:32","title":"Probing Mirror Neutrons and Dark Matter through Cold Neutron Interferometry","abstract":"We propose a novel neutron interferometry setup to explore the potential existence of mirror neutrons, a candidate for dark matter. Our work demonstrates that if mirror neutrons exist, neutrons will acquire an observable geometric phase due to mixing with these mirror counterparts. This geometric phase, detectable through our interferometric setup, could serve as a direct probe for the presence of mirror matter particles. Additionally, this investigation could shed light on unresolved issues in particle physics, such as the neutron lifetime puzzle. We discuss the setup's versatility and limitations, showing its capability to explore a wide range of parameters in neutron interferometry and potentially uncover new physics.","sentences":["We propose a novel neutron interferometry setup to explore the potential existence of mirror neutrons, a candidate for dark matter.","Our work demonstrates that if mirror neutrons exist, neutrons will acquire an observable geometric phase due to mixing with these mirror counterparts.","This geometric phase, detectable through our interferometric setup, could serve as a direct probe for the presence of mirror matter particles.","Additionally, this investigation could shed light on unresolved issues in particle physics, such as the neutron lifetime puzzle.","We discuss the setup's versatility and limitations, showing its capability to explore a wide range of parameters in neutron interferometry and potentially uncover new physics."],"url":"http://arxiv.org/abs/2406.07262v1","category":"hep-ph"}
{"created":"2024-06-11 13:39:50","title":"Coupled-channel $J^{--}$ meson resonances from lattice QCD","abstract":"We extend an earlier calculation within lattice QCD of excited light meson resonances with $J^{PC}=1^{--}, 2^{--}, 3^{--}$ at the SU(3) flavor point in the singlet representation, by considering the octet representation. In this case the resonances appear in coupled-channel amplitudes, which we determine, establishing the relative strength of pseudoscalar-pseudoscalar to pseudoscalar-vector decays. Combining the new octet results with the prior results for the singlet, we perform a plausible extrapolation to the physical quark mass, and compare to experimental $\\rho^\\star_J, K^\\star_J, \\omega^\\star_J$ and $\\phi^\\star_J$ resonances.","sentences":["We extend an earlier calculation within lattice QCD of excited light meson resonances with $J^{PC}=1^{--}, 2^{--}, 3^{--}$ at the SU(3) flavor point in the singlet representation, by considering the octet representation.","In this case the resonances appear in coupled-channel amplitudes, which we determine, establishing the relative strength of pseudoscalar-pseudoscalar to pseudoscalar-vector decays.","Combining the new octet results with the prior results for the singlet, we perform a plausible extrapolation to the physical quark mass, and compare to experimental $\\rho^\\star_J, K^\\star_J, \\omega^\\star_J$ and $\\phi^\\star_J$ resonances."],"url":"http://arxiv.org/abs/2406.07261v1","category":"hep-lat"}
{"created":"2024-06-11 13:28:11","title":"Optical and Electrical Properties of Diamond-like-Carbon Coatings Prepared by Electron Cyclotron Resonance Ion Beam Deposition Process","abstract":"Diamond-like carbon thin films have emerged as durable, chemically stable optical coatings for many optical and optoelectronics applications due to their hardness, chemical inertness, and optical transparency. This paper presents a novel high-energy electron cyclotron resonance ion beam sputter deposition technique to fabricate pure diamond-like carbon coatings at room temperature. The chemical composition of the deposited coatings including ratios of sp2/sp3 bonding in the thin films were determined by X-ray photoelectron spectroscopy. Results indicate that the sp3 percentage ranges from 45% - 85%. The transmission and reflectance spectra of the coatings were measured from UV to IR ({\\lambda}= 185 to 2500 nm) by utilizing a spectrophotometer. The measured spectra were analysed by the Tauc method to determine the optical band gap and Urbach energy and an optical fitting software, which utilizes the model modified by OJL, to extract the refractive index and extinction coefficient. By varying the ion energy, the optical properties were found to be n = 2.30 - 2.51, band gap energy = 0.4 - 0.68 eV, and the Urbach energy = 0.33 - 0.49 eV. This study provides a flexible method for tuning the structural, optical, and electronic properties of diamond-like carbon coatings by controlling the ion energy during deposition.","sentences":["Diamond-like carbon thin films have emerged as durable, chemically stable optical coatings for many optical and optoelectronics applications due to their hardness, chemical inertness, and optical transparency.","This paper presents a novel high-energy electron cyclotron resonance ion beam sputter deposition technique to fabricate pure diamond-like carbon coatings at room temperature.","The chemical composition of the deposited coatings including ratios of sp2/sp3 bonding in the thin films were determined by X-ray photoelectron spectroscopy.","Results indicate that the sp3 percentage ranges from 45% - 85%.","The transmission and reflectance spectra of the coatings were measured from UV to IR ({\\lambda}= 185 to 2500 nm) by utilizing a spectrophotometer.","The measured spectra were analysed by the Tauc method to determine the optical band gap and Urbach energy and an optical fitting software, which utilizes the model modified by OJL, to extract the refractive index and extinction coefficient.","By varying the ion energy, the optical properties were found to be n = 2.30 - 2.51, band gap energy = 0.4 - 0.68 eV, and the Urbach energy = 0.33 - 0.49 eV. This study provides a flexible method for tuning the structural, optical, and electronic properties of diamond-like carbon coatings by controlling the ion energy during deposition."],"url":"http://arxiv.org/abs/2406.07245v1","category":"physics.app-ph"}
{"created":"2024-06-11 12:36:39","title":"Contributions of QED diagrams with vacuum polarization insertions to the lepton anomaly within the Mellin--Barnes representation","abstract":"We investigate the radiative QED corrections to the lepton ($L=e,~\\mu$ and $\\tau$) anomalous magnetic moment arising from vacuum polarization diagrams by four closed lepton loops. The method is based on the consecutive application of dispersion relations for the polarization operator and the Mellin--Barnes transform for the propagators of massive particles. This allows one to obtain, for the first time, exact analytical expressions for the radiative corrections to the anomalous magnetic moments of leptons from diagrams with insertions of four identical lepton loops all of the same type $\\ell$ different from the external one, $L$. The result is expressed in terms of the mass ratio $r=m_\\ell/m_L$. We investigate the behaviour of the exact analytical expressions at $r\\to 0$ and $r\\to \\infty$ and compare with the corresponding asymptotic expansions known in the literature.","sentences":["We investigate the radiative QED corrections to the lepton ($L=e,~\\mu$ and $\\tau$) anomalous magnetic moment arising from vacuum polarization diagrams by four closed lepton loops.","The method is based on the consecutive application of dispersion relations for the polarization operator and the Mellin--Barnes transform for the propagators of massive particles.","This allows one to obtain, for the first time, exact analytical expressions for the radiative corrections to the anomalous magnetic moments of leptons from diagrams with insertions of four identical lepton loops all of the same type $\\ell$ different from the external one, $L$. The result is expressed in terms of the mass ratio $r=m_\\ell/m_L$. We investigate the behaviour of the exact analytical expressions at $r\\to 0$ and $r\\to \\infty$ and compare with the corresponding asymptotic expansions known in the literature."],"url":"http://arxiv.org/abs/2406.07211v1","category":"hep-ph"}
{"created":"2024-06-11 12:22:49","title":"Can Foundation Models Reliably Identify Spatial Hazards? A Case Study on Curb Segmentation","abstract":"Curbs serve as vital borders that delineate safe pedestrian zones from potential vehicular traffic hazards. Curbs also represent a primary spatial hazard during dynamic navigation with significant stumbling potential. Such vulnerabilities are particularly exacerbated for persons with blindness and low vision (PBLV). Accurate visual-based discrimination of curbs is paramount for assistive technologies that aid PBLV with safe navigation in urban environments. Herein, we investigate the efficacy of curb segmentation for foundation models. We introduce the largest curb segmentation dataset to-date to benchmark leading foundation models. Our results show that state-of-the-art foundation models face significant challenges in curb segmentation. This is due to their high false-positive rates (up to 95%) with poor performance distinguishing curbs from curb-like objects or non-curb areas, such as sidewalks. In addition, the best-performing model averaged a 3.70-second inference time, underscoring problems in providing real-time assistance. In response, we propose solutions including filtered bounding box selections to achieve more accurate curb segmentation. Overall, despite the immediate flexibility of foundation models, their application for practical assistive technology applications still requires refinement. This research highlights the critical need for specialized datasets and tailored model training to address navigation challenges for PBLV and underscores implicit weaknesses in foundation models.","sentences":["Curbs serve as vital borders that delineate safe pedestrian zones from potential vehicular traffic hazards.","Curbs also represent a primary spatial hazard during dynamic navigation with significant stumbling potential.","Such vulnerabilities are particularly exacerbated for persons with blindness and low vision (PBLV).","Accurate visual-based discrimination of curbs is paramount for assistive technologies that aid PBLV with safe navigation in urban environments.","Herein, we investigate the efficacy of curb segmentation for foundation models.","We introduce the largest curb segmentation dataset to-date to benchmark leading foundation models.","Our results show that state-of-the-art foundation models face significant challenges in curb segmentation.","This is due to their high false-positive rates (up to 95%) with poor performance distinguishing curbs from curb-like objects or non-curb areas, such as sidewalks.","In addition, the best-performing model averaged a 3.70-second inference time, underscoring problems in providing real-time assistance.","In response, we propose solutions including filtered bounding box selections to achieve more accurate curb segmentation.","Overall, despite the immediate flexibility of foundation models, their application for practical assistive technology applications still requires refinement.","This research highlights the critical need for specialized datasets and tailored model training to address navigation challenges for PBLV and underscores implicit weaknesses in foundation models."],"url":"http://arxiv.org/abs/2406.07202v1","category":"cs.CV"}
{"created":"2024-06-11 12:13:35","title":"A search for Galactic post-AGB stars in Gaia DR3","abstract":"Context. When low and intermediate-mass stars leave the Asymptotic Giant Branch (AGB) phase, and before they reach the Planetary Nebulae stage, they enter a very brief and rather puzzling stellar evolutionary stage named post-AGB.   Aims. To provide a reliable catalogue of galactic post-AGB stars together with their physical and evolutionary properties obtained through Gaia DR3 astrometry and photometry.   Methods. We started by identifying post-AGB stars or possible candidates from the bibliography with their Gaia DR3 counterpart sources. Using the available photometry, interstellar extinction, literature spectroscopically derived temperatures or spectral types and parallax-derived distances from Gaia DR3, we fitted their Spectral Energy Distributions and we estimated their luminosities and circumstellar extinctions. When compared to models, luminosity values allowed us to disclose objects that are likely post-AGB stars from other target types. Their position on the HR diagram allows direct comparison with updated post-AGB evolutionary tracks and an estimation of their masses and evolutionary ages.   Results. We obtained a sample of 69 reliable post-AGB candidates that meet our classification criteria, providing their coordinates, distances, effective temperature, total extinction, luminosity, mass, and evolutionary age. In addition, similar data for other stellar objects in our initial compilation, such as supergiant stars or young stellar objects, is provided.   Conclusions. We have filtered out the data that have the best precision in parallaxes and distances to obtain more accurate luminosities, which allows us to classify with confidence the objects of the sample among different stellar phases. This allows us to provide a small but reliable sample of post-AGB objects. Derived mean evolutionary time and average mass values are in agreement with theoretical expectations.","sentences":["Context.","When low and intermediate-mass stars leave the Asymptotic Giant Branch (AGB) phase, and before they reach the Planetary Nebulae stage, they enter a very brief and rather puzzling stellar evolutionary stage named post-AGB.   ","Aims.","To provide a reliable catalogue of galactic post-AGB stars together with their physical and evolutionary properties obtained through Gaia DR3 astrometry and photometry.   Methods.","We started by identifying post-AGB stars or possible candidates from the bibliography with their Gaia DR3 counterpart sources.","Using the available photometry, interstellar extinction, literature spectroscopically derived temperatures or spectral types and parallax-derived distances from Gaia DR3, we fitted their Spectral Energy Distributions and we estimated their luminosities and circumstellar extinctions.","When compared to models, luminosity values allowed us to disclose objects that are likely post-AGB stars from other target types.","Their position on the HR diagram allows direct comparison with updated post-AGB evolutionary tracks and an estimation of their masses and evolutionary ages.   ","Results.","We obtained a sample of 69 reliable post-AGB candidates that meet our classification criteria, providing their coordinates, distances, effective temperature, total extinction, luminosity, mass, and evolutionary age.","In addition, similar data for other stellar objects in our initial compilation, such as supergiant stars or young stellar objects, is provided.   ","Conclusions.","We have filtered out the data that have the best precision in parallaxes and distances to obtain more accurate luminosities, which allows us to classify with confidence the objects of the sample among different stellar phases.","This allows us to provide a small but reliable sample of post-AGB objects.","Derived mean evolutionary time and average mass values are in agreement with theoretical expectations."],"url":"http://arxiv.org/abs/2406.07196v1","category":"astro-ph.SR"}
{"created":"2024-06-11 10:39:44","title":"Constraints on Lorentz invariance violation from the extraordinary Mrk 421 flare of 2014 using a novel analysis method","abstract":"The Lorentz Invariance Violation (LIV), a proposed consequence of certain quantum gravity (QG) scenarios, could instigate an energy-dependent group velocity for ultra-relativistic particles. This energy dependence, although suppressed by the massive QG energy scale $E_\\mathrm{QG}$, expected to be on the level of the Planck energy $1.22 \\times 10^{19}$ GeV, is potentially detectable in astrophysical observations. In this scenario, the cosmological distances traversed by photons act as an amplifier for this effect. By leveraging the observation of a remarkable flare from the blazar Mrk\\,421, recorded at energies above 100 GeV by the MAGIC telescopes on the night of April 25 to 26, 2014, we look for time delays scaling linearly and quadratically with the photon energies. Using for the first time in LIV studies a binned-likelihood approach we set constraints on the QG energy scale. For the linear scenario, we set $95\\%$ lower limits $E_\\mathrm{QG}>2.7\\times10^{17}$ GeV for the subluminal case and $E_\\mathrm{QG}> 3.6 \\times10^{17}$ GeV for the superluminal case. For the quadratic scenario, the $95\\%$ lower limits for the subluminal and superluminal cases are $E_\\mathrm{QG}>2.6 \\times10^{10}$ GeV and $E_\\mathrm{QG}>2.5\\times10^{10}$ GeV, respectively.","sentences":["The Lorentz Invariance Violation (LIV), a proposed consequence of certain quantum gravity (QG) scenarios, could instigate an energy-dependent group velocity for ultra-relativistic particles.","This energy dependence, although suppressed by the massive QG energy scale $E_\\mathrm{QG}$, expected to be on the level of the Planck energy $1.22 \\times 10^{19}$ GeV, is potentially detectable in astrophysical observations.","In this scenario, the cosmological distances traversed by photons act as an amplifier for this effect.","By leveraging the observation of a remarkable flare from the blazar Mrk\\,421, recorded at energies above 100 GeV by the MAGIC telescopes on the night of April 25 to 26, 2014, we look for time delays scaling linearly and quadratically with the photon energies.","Using for the first time in LIV studies a binned-likelihood approach we set constraints on the QG energy scale.","For the linear scenario, we set $95\\%$ lower limits $E_\\mathrm{QG}>2.7\\times10^{17}$ GeV for the subluminal case and $E_\\mathrm{QG}> 3.6 \\times10^{17}$ GeV for the superluminal case.","For the quadratic scenario, the $95\\%$ lower limits for the subluminal and superluminal cases are $E_\\mathrm{QG}>2.6 \\times10^{10}$ GeV and $E_\\mathrm{QG}>2.5\\times10^{10}$ GeV, respectively."],"url":"http://arxiv.org/abs/2406.07140v1","category":"astro-ph.HE"}
{"created":"2024-06-11 09:42:41","title":"An efficient active-stress electromechanical isogeometric shell model for muscular thin film simulations","abstract":"We propose an isogeometric approach to model the deformation of active thin films using layered, nonlinear, Kirchhoff Love shells. Isogeometric Collocation and Galerkin formulations are employed to discretize the electrophysiological and mechanical sub-problems, respectively, with the possibility to adopt different element and time-step sizes. Numerical tests illustrate the capabilities of the active stress based approach to effectively simulate the contraction of thin films in both quasi-static and dynamic conditions.","sentences":["We propose an isogeometric approach to model the deformation of active thin films using layered, nonlinear, Kirchhoff Love shells.","Isogeometric Collocation and Galerkin formulations are employed to discretize the electrophysiological and mechanical sub-problems, respectively, with the possibility to adopt different element and time-step sizes.","Numerical tests illustrate the capabilities of the active stress based approach to effectively simulate the contraction of thin films in both quasi-static and dynamic conditions."],"url":"http://arxiv.org/abs/2406.07102v1","category":"math.NA"}
{"created":"2024-06-11 09:42:14","title":"Quantum Corner Symmetry: Representations and Gluing","abstract":"The corner symmetry algebra organises the physical charges induced by gravity on codimension-$2$ corners of a manifold. In this letter, we initiate a study of the quantum properties of this group. We first describe the central extensions and how the quantum corner symmetry group arises. We then classify the Casimirs and the induced unitary irreducible representation. We finally discuss the gluing of corners, achieved identifying the maximal commuting sub-algebra. This is a concrete implementation of the gravitational constraints at the quantum level, via the entangling product.","sentences":["The corner symmetry algebra organises the physical charges induced by gravity on codimension-$2$ corners of a manifold.","In this letter, we initiate a study of the quantum properties of this group.","We first describe the central extensions and how the quantum corner symmetry group arises.","We then classify the Casimirs and the induced unitary irreducible representation.","We finally discuss the gluing of corners, achieved identifying the maximal commuting sub-algebra.","This is a concrete implementation of the gravitational constraints at the quantum level, via the entangling product."],"url":"http://arxiv.org/abs/2406.07101v1","category":"hep-th"}
{"created":"2024-06-11 09:00:15","title":"A Neck Orthosis with Multi-Directional Variable Stiffness for Persons with Dropped Head Syndrome","abstract":"Dropped Head Syndrome (DHS) causes a passively correctable neck deformation. Currently, there is no wearable orthopedic neck brace to fulfill the needs of persons suffering from DHS. Related works have made progress in this area by creating mobile neck braces that provide head support to mitigate deformation while permitting neck mobility, which enhances user-perceived comfort and quality of life. Specifically, passive designs show great potential for fully functional devices in the short term due to their inherent simplicity and compactness, although achieving suitable support presents some challenges. This work introduces a novel compliant mechanism that provides non-restrictive adjustable support for the neck's anterior and posterior flexion movements while enabling its unconstrained free rotation. The results from the experiments on non-affected persons suggest that the device provides the proposed adjustable support that unloads the muscle groups involved in supporting the head without overloading the antagonist muscle groups. Simultaneously, it was verified that the free rotation is achieved regardless of the stiffness configuration of the device.","sentences":["Dropped Head Syndrome (DHS) causes a passively correctable neck deformation.","Currently, there is no wearable orthopedic neck brace to fulfill the needs of persons suffering from DHS.","Related works have made progress in this area by creating mobile neck braces that provide head support to mitigate deformation while permitting neck mobility, which enhances user-perceived comfort and quality of life.","Specifically, passive designs show great potential for fully functional devices in the short term due to their inherent simplicity and compactness, although achieving suitable support presents some challenges.","This work introduces a novel compliant mechanism that provides non-restrictive adjustable support for the neck's anterior and posterior flexion movements while enabling its unconstrained free rotation.","The results from the experiments on non-affected persons suggest that the device provides the proposed adjustable support that unloads the muscle groups involved in supporting the head without overloading the antagonist muscle groups.","Simultaneously, it was verified that the free rotation is achieved regardless of the stiffness configuration of the device."],"url":"http://arxiv.org/abs/2406.07074v1","category":"cs.RO"}
{"created":"2024-06-11 08:41:06","title":"Constraining $\u03bd$SMEFT coefficients: the case of the extra $\\text{U}(1)^\\prime$","abstract":"We study the constraints on low-energy coefficients of the $\\nu$SMEFT generalization of the Standard Model effective theory in the simple case of a $\\text{U}(1)^\\prime$ enlargement of the Standard Model gauge group. In particular, we analyse the constraints imposed by the requirement that the extended theory remains free of gauge anomalies. We present the cases of explicit realisations, showing the obtained correlations among the coefficients of $d=6$ operators.","sentences":["We study the constraints on low-energy coefficients of the $\\nu$SMEFT generalization of the Standard Model effective theory in the simple case of a $\\text{U}(1)^\\prime$ enlargement of the Standard Model gauge group.","In particular, we analyse the constraints imposed by the requirement that the extended theory remains free of gauge anomalies.","We present the cases of explicit realisations, showing the obtained correlations among the coefficients of $d=6$ operators."],"url":"http://arxiv.org/abs/2406.07059v1","category":"hep-ph"}
{"created":"2024-06-11 07:23:45","title":"Study of $\u03c0$, K, and p production in high multiplicity pp collisions at $\\sqrt{s}$ = 13 TeV with ALICE at the LHC","abstract":"High multiplicity proton-proton and proton-lead collisions at LHC energies exhibit similar signatures to those observed in Pb-Pb collisions (i.e. the strangeness enhancement, the ridge behaviours etc.), that were commonly attributed to the formation of the Quark-Gluon Plasma. In this contribution, the measurements of $\\pi$, K, and p transverse momentum spectra in the rapidity region |y| < 0.5 for various multiplicity classes in pp, p-Pb and A-A collisions with the ALICE detector at the LHC will be presented. Various results, including the integrated particle yields and particle ratios as a function of charged particle multiplicity for different systems and energies, will be discussed.","sentences":["High multiplicity proton-proton and proton-lead collisions at LHC energies exhibit similar signatures to those observed in Pb-Pb collisions (i.e. the strangeness enhancement, the ridge behaviours etc.), that were commonly attributed to the formation of the Quark-Gluon Plasma.","In this contribution, the measurements of $\\pi$, K, and p transverse momentum spectra in the rapidity region |y| < 0.5 for various multiplicity classes in pp, p-Pb and A-A collisions with the ALICE detector at the LHC will be presented.","Various results, including the integrated particle yields and particle ratios as a function of charged particle multiplicity for different systems and energies, will be discussed."],"url":"http://arxiv.org/abs/2406.07018v1","category":"hep-ex"}
{"created":"2024-06-11 07:13:24","title":"Thermodynamic Relations between Free Energy and Mobility","abstract":"Stochastic and dynamical processes lie at the heart of all physical, chemical, and biological systems. However, kinetic and thermodynamic properties which characterize these processes have largely been treated separately as they can be obtained independently for many systems at thermodynamic equilibrium. In this work we demonstrate the existence of a class of relations between kinetic and thermodynamic factors which holds even in the hydrodynamic limit, and which must be satisfied for all systems that satisfy detailed balance and Boltzmann distribution at equilibrium. We achieve this by proving that for systems with inhomogeneous equilibrium states governed by dynamics such as the Cahn-Hilliard (CH) dynamics, the chemical potential and self-diffusivity must mutually constrain each other. We discuss common issues in the literature which result in inconsistent formulations, construct the consistency requirement mathematically, develop a class of self-diffusivities that guarantee consistency, and discuss how the requirement originates from detailed balance and Boltzmann distribution, and is therefore applicable to both conserved and non-conserved dynamics.","sentences":["Stochastic and dynamical processes lie at the heart of all physical, chemical, and biological systems.","However, kinetic and thermodynamic properties which characterize these processes have largely been treated separately as they can be obtained independently for many systems at thermodynamic equilibrium.","In this work we demonstrate the existence of a class of relations between kinetic and thermodynamic factors which holds even in the hydrodynamic limit, and which must be satisfied for all systems that satisfy detailed balance and Boltzmann distribution at equilibrium.","We achieve this by proving that for systems with inhomogeneous equilibrium states governed by dynamics such as the Cahn-Hilliard (CH) dynamics, the chemical potential and self-diffusivity must mutually constrain each other.","We discuss common issues in the literature which result in inconsistent formulations, construct the consistency requirement mathematically, develop a class of self-diffusivities that guarantee consistency, and discuss how the requirement originates from detailed balance and Boltzmann distribution, and is therefore applicable to both conserved and non-conserved dynamics."],"url":"http://arxiv.org/abs/2406.07013v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-11 07:10:23","title":"Entropy and thermodynamical stability of white dwarfs","abstract":"A structure of spherical white dwarfs is calculated for a non-zero temperature. It is shown that the thermodynamical stability of the white dwarf stars can be described naturally within the concept of the Helmholtz free energy of the Coulomb fully ionized electron-ion plasma.","sentences":["A structure of spherical white dwarfs is calculated for a non-zero temperature.","It is shown that the thermodynamical stability of the white dwarf stars can be described naturally within the concept of the Helmholtz free energy of the Coulomb fully ionized electron-ion plasma."],"url":"http://arxiv.org/abs/2406.07010v1","category":"astro-ph.SR"}
{"created":"2024-06-11 06:52:58","title":"Strong decays of the isovector-scalar $D^\\ast\\bar{D}^\\ast$ hadronic molecule","abstract":"We adopt the effective Lagrangian approach to study the strong decays of the $1^-(0^{++})$ $D^\\ast\\bar{D}^\\ast$ molecular state [denoted as $T_{\\psi0}^a(4010)$ according to the LHCb naming convention] through triangle diagrams. The decay channels include the open-charm $D\\bar{D}$, and the hidden-charm $\\eta_c\\pi$, $J/\\psi\\rho$, and $\\chi_{c1}\\pi$. The coupling between the $T_{\\psi0}^a(4010)$ and its constituents $D^\\ast\\bar{D}^\\ast$ is obtained by solving for the residue of the scattering T-matrix at the pole. Our calculation yields a total width of $(12.0$$-$$35.4)$ MeV for the $T_{\\psi0}^a(4010)$ state, with its main decay channels being $\\eta_c\\pi$ and $\\chi_{c1}\\pi$. The $X(4100)$ and $X(4050)$ have similar masses and widths, with both masses being close to the $D^\\ast\\bar{D}^\\ast$ threshold. Additionally, their decay final states are consistent with those of the $T_{\\psi0}^a(4010)$. Therefore, it is likely that they represent the same state and both potentially correspond to the $T_{\\psi0}^a(4010)$. We suggest that future experiments focus on searching for the $T_{\\psi0}^a(4010)$ signal in the final states $\\eta_c\\pi^-$, $\\chi_{c1}\\pi^-$ and $D^0D^-$ of the $B^0\\to\\eta_c\\pi^-K^+$, $\\chi_{c1}\\pi^- K^+$ and $D^0D^-K^+$ processes, respectively, as well as further investigating its resonance parameters with Flatt\\'e-like formula.","sentences":["We adopt the effective Lagrangian approach to study the strong decays of the $1^-(0^{++})$ $D^\\ast\\bar{D}^\\ast$ molecular state [denoted as $T_{\\psi0}^a(4010)$ according to the LHCb naming convention] through triangle diagrams.","The decay channels include the open-charm $D\\bar{D}$, and the hidden-charm $\\eta_c\\pi$, $J/\\psi\\rho$, and $\\chi_{c1}\\pi$. The coupling between the $T_{\\psi0}^a(4010)$ and its constituents $D^\\ast\\bar{D}^\\ast$ is obtained by solving for the residue of the scattering T-matrix at the pole.","Our calculation yields a total width of $(12.0$$-$$35.4)$ MeV for the $T_{\\psi0}^a(4010)$ state, with its main decay channels being $\\eta_c\\pi$ and $\\chi_{c1}\\pi$. The $X(4100)$ and $X(4050)$ have similar masses and widths, with both masses being close to the $D^\\ast\\bar{D}^\\ast$ threshold.","Additionally, their decay final states are consistent with those of the $T_{\\psi0}^a(4010)$. Therefore, it is likely that they represent the same state and both potentially correspond to the $T_{\\psi0}^a(4010)$. We suggest that future experiments focus on searching for the $T_{\\psi0}^a(4010)$ signal in the final states $\\eta_c\\pi^-$, $\\chi_{c1}\\pi^-$ and $D^0D^-$ of the $B^0\\to\\eta_c\\pi^-K^+$, $\\chi_{c1}\\pi^- K^+$ and $D^0D^-K^+$ processes, respectively, as well as further investigating its resonance parameters with Flatt\\'e-like formula."],"url":"http://arxiv.org/abs/2406.07000v1","category":"hep-ph"}
{"created":"2024-06-11 06:44:55","title":"Spectrum of the molecular hexaquarks","abstract":"We investigate the mass spectra of molecular-type hexaquark states in the dibaryon systems. These systems are composed of the charmed baryons $[\\Sigma_c^{(\\ast)}$, $\\Xi_c^{(\\prime,\\ast)}]$, doubly charmed baryons $[\\Xi_{cc}^{(\\ast)}]$, and hyperons $[\\Sigma^{(\\ast)}$, $\\Xi^{(\\ast)}]$. We consider all possible combinations of particle-particle and particle-antiparticle pairs, including the S-wave spin multiplets in each combination. We establish the underlying connections among the molecular tetraquarks, pentaquarks, and hexaquarks with the effective quark-level interactions. We find that the existence of molecular states in $DD^\\ast$, $D\\bar{D}^\\ast$, and $\\Sigma_c\\bar{D}^{(\\ast)}$ systems leads to the emergence of a large number of deuteron-like hexaquarks in the heavy flavor sectors. Currently, there have been several experimental candidates for molecular tetraquarks and pentaquarks. The experimental search for near-threshold hexaquarks will further advance the establishment of the underlying dynamical picture of hadronic molecules and deepen our understanding of the role of spin-flavor symmetry in near-threshold residual strong interactions.","sentences":["We investigate the mass spectra of molecular-type hexaquark states in the dibaryon systems.","These systems are composed of the charmed baryons $[\\Sigma_c^{(\\ast)}$, $\\Xi_c^{(\\prime,\\ast)}]$, doubly charmed baryons $[\\Xi_{cc}^{(\\ast)}]$, and hyperons $[\\Sigma^{(\\ast)}$, $\\Xi^{(\\ast)}]$. We consider all possible combinations of particle-particle and particle-antiparticle pairs, including the S-wave spin multiplets in each combination.","We establish the underlying connections among the molecular tetraquarks, pentaquarks, and hexaquarks with the effective quark-level interactions.","We find that the existence of molecular states in $DD^\\ast$, $D\\bar{D}^\\ast$, and $\\Sigma_c\\bar{D}^{(\\ast)}$ systems leads to the emergence of a large number of deuteron-like hexaquarks in the heavy flavor sectors.","Currently, there have been several experimental candidates for molecular tetraquarks and pentaquarks.","The experimental search for near-threshold hexaquarks will further advance the establishment of the underlying dynamical picture of hadronic molecules and deepen our understanding of the role of spin-flavor symmetry in near-threshold residual strong interactions."],"url":"http://arxiv.org/abs/2406.06993v1","category":"hep-ph"}
{"created":"2024-06-11 05:44:48","title":"Non-Hermitian spacetime and generalized thermofield double formalism","abstract":"In this paper, we explore the non-Hermitian transition matrix and its gravity dual. States in quantum field theories or gravity theories are typically prepared using Euclidean path integrals. We demonstrate that it is both natural and necessary to introduce non-Hermitian transitions to describe the state when employing different inner products in Euclidean quantum field theories. Transition matrices that are $\\eta$-pseudo-Hermitian, with $\\eta$ being positive-definite, play the same role as density matrices, where the operator $\\eta$ is closely related to the definition of the inner product. Moreover, there exists a one-to-one correspondence between these transition matrices and density matrices. In the context of AdS/CFT correspondence, the Euclidean path integral in the boundary field theory can be translated to the bulk gravitational path integral. We provide an overview of the construction and interpretation of non-Hermitian spacetime. Specifically, we demonstrate the crucial role of the non-Hermitian transition matrix in realizing the thermofield concept in general cases and in understanding the gravity states dual to the eternal black hole. In this context, the pseudoentropy of the transition matrix can also be interpreted as black hole entropy. Finally, we highlight the strong subadditivity property of pseudoentropy, and the connection between non-Hermitian transition matrices and complex metrics.","sentences":["In this paper, we explore the non-Hermitian transition matrix and its gravity dual.","States in quantum field theories or gravity theories are typically prepared using Euclidean path integrals.","We demonstrate that it is both natural and necessary to introduce non-Hermitian transitions to describe the state when employing different inner products in Euclidean quantum field theories.","Transition matrices that are $\\eta$-pseudo-Hermitian, with $\\eta$ being positive-definite, play the same role as density matrices, where the operator $\\eta$ is closely related to the definition of the inner product.","Moreover, there exists a one-to-one correspondence between these transition matrices and density matrices.","In the context of AdS/CFT correspondence, the Euclidean path integral in the boundary field theory can be translated to the bulk gravitational path integral.","We provide an overview of the construction and interpretation of non-Hermitian spacetime.","Specifically, we demonstrate the crucial role of the non-Hermitian transition matrix in realizing the thermofield concept in general cases and in understanding the gravity states dual to the eternal black hole.","In this context, the pseudoentropy of the transition matrix can also be interpreted as black hole entropy.","Finally, we highlight the strong subadditivity property of pseudoentropy, and the connection between non-Hermitian transition matrices and complex metrics."],"url":"http://arxiv.org/abs/2406.06961v1","category":"hep-th"}
{"created":"2024-06-11 04:48:40","title":"High-velocity blue-shifted Fe XXV He$\u03b1$ line during a superflare of the RS CVn-type star IM Peg","abstract":"Monitor of All-sky X-ray Image (MAXI) detected a superflare, releasing $5\\times 10^{37}$ erg in 2$-$10 keV, of the RS CVn-type star IM Peg at 10:41 UT on 2023 July 23 with its Gas Slit Camera (GSC; 2$-$30 keV). We conducted X-ray follow-up observations of the superflare with Neutron Star Interior Composition ExploreR (NICER; 0.2$-$12 keV) starting at 16:52 UT on July 23 until 06:00 UT on August 2. NICER X-ray spectra clearly showed emission lines of the Fe XXV He$\\alpha$ and Fe XXVI Ly$\\alpha$ for $\\sim 1.5$ days since the MAXI detection. The Fe XXV He$\\alpha$ line was blue-shifted with its maximum Doppler velocity reaching $-2200 \\pm 600$ $\\mathrm{km \\: s^{-1}}$, suggesting an upward-moving plasma during the flare, such as a coronal mass ejection (CME) and/or chromospheric evaporation. This is the first case that the Fe XXV He$\\alpha$ line is blue-shifted during a stellar flare and its velocity overwhelmingly exceeds the escape velocity of the star ($-230$ $\\mathrm{km \\: s^{-1}}$). One hour before the most pronounced blueshift detection, a signature of reheating the flare plasma was observed. We discuss the origin of the blueshift, a CME or high-velocity chromospheric evaporation.","sentences":["Monitor of All-sky X-ray Image (MAXI) detected a superflare, releasing $5\\times 10^{37}$ erg in 2$-$10 keV, of the RS CVn-type star IM Peg at 10:41 UT on 2023 July 23 with its Gas Slit Camera (GSC; 2$-$30 keV).","We conducted X-ray follow-up observations of the superflare with Neutron Star Interior Composition ExploreR (NICER; 0.2$-$12 keV) starting at 16:52 UT on July 23 until 06:00 UT on August 2.","NICER X-ray spectra clearly showed emission lines of the Fe XXV He$\\alpha$ and Fe XXVI Ly$\\alpha$ for $\\sim 1.5$ days since the MAXI detection.","The Fe XXV He$\\alpha$ line was blue-shifted with its maximum Doppler velocity reaching $-2200 \\pm 600$ $\\mathrm{km \\: s^{-1}}$, suggesting an upward-moving plasma during the flare, such as a coronal mass ejection (CME) and/or chromospheric evaporation.","This is the first case that the Fe XXV He$\\alpha$ line is blue-shifted during a stellar flare and its velocity overwhelmingly exceeds the escape velocity of the star ($-230$ $\\mathrm{km \\: s^{-1}}$).","One hour before the most pronounced blueshift detection, a signature of reheating the flare plasma was observed.","We discuss the origin of the blueshift, a CME or high-velocity chromospheric evaporation."],"url":"http://arxiv.org/abs/2406.06940v1","category":"astro-ph.SR"}
{"created":"2024-06-11 04:44:28","title":"Dynamical properties of mildly relativistic ejecta produced by the mass-loading of gamma-ray burst jets in dense ambient media","abstract":"We present the results of a series of 3D special relativistic hydrodynamic simulations of a gamma-ray burst (GRB) jet in a massive circumstellar medium (CSM) surrounding the progenitor star. Our simulations reproduce the jet morphology transitioning from a well-collimated state to a thermal pressure-driven state for a range of CSM masses and outer radii. The jet-CSM interaction redistributes the jet energy to materials expanding into a wide solid angle and results in a quasi-spherical ejecta with 4-velocities from $\\Gamma\\beta\\simeq 0.1$ to $\\simeq 10$. The mass and kinetic energy of the ejecta with velocities faster than $0.1c$ are typically of the order of $0.1\\,M_\\odot$ and $10^{51}\\,\\mathrm{erg}$ with only a weak dependence on the CSM mass and radius for the explored CSM parameter ranges. We find that the numerically obtained density structure of the mildly relativistic ejecta is remarkably universal. The radial density profile is well approximated as a power-law function of the radial velocity with an index of $-5$, $\\rho\\propto v^{-5}$, in agreement with our previous simulations and other studies, as well as those suggested from recent studies on early-phase spectra of supernovae associated with GRBs. Such fast ejecta rapidly becomes transparent following its expansion. Gradually releasing the trapped thermal photons, the ejecta gives rise to bright UV--optical emission within $\\sim 1$ day. We discuss the potential link of the relativistic ejecta resulting from jet-CSM interaction to GRB-associated supernovae as well as fast and blue optical transients.","sentences":["We present the results of a series of 3D special relativistic hydrodynamic simulations of a gamma-ray burst (GRB) jet in a massive circumstellar medium (CSM) surrounding the progenitor star.","Our simulations reproduce the jet morphology transitioning from a well-collimated state to a thermal pressure-driven state for a range of CSM masses and outer radii.","The jet-CSM interaction redistributes the jet energy to materials expanding into a wide solid angle and results in a quasi-spherical ejecta with 4-velocities from $\\Gamma\\beta\\simeq 0.1$ to $\\simeq 10$. The mass and kinetic energy of the ejecta with velocities faster than $0.1c$ are typically of the order of $0.1\\,M_\\odot$ and $10^{51}\\,\\mathrm{erg}$ with only a weak dependence on the CSM mass and radius for the explored CSM parameter ranges.","We find that the numerically obtained density structure of the mildly relativistic ejecta is remarkably universal.","The radial density profile is well approximated as a power-law function of the radial velocity with an index of $-5$, $\\rho\\propto v^{-5}$, in agreement with our previous simulations and other studies, as well as those suggested from recent studies on early-phase spectra of supernovae associated with GRBs.","Such fast ejecta rapidly becomes transparent following its expansion.","Gradually releasing the trapped thermal photons, the ejecta gives rise to bright UV--optical emission within $\\sim 1$ day.","We discuss the potential link of the relativistic ejecta resulting from jet-CSM interaction to GRB-associated supernovae as well as fast and blue optical transients."],"url":"http://arxiv.org/abs/2406.06939v1","category":"astro-ph.HE"}
{"created":"2024-06-11 04:15:13","title":"Equivariant vector bundles on toric schemes over semirings","abstract":"We introduce a notion of equivariant vector bundles on schemes over semirings. We do this by considering the functor of points of a locally free sheaf. We prove that every toric vector bundle on a toric scheme $X$ over an idempotent semifield equivariantly splits as a sum of toric line bundles. We then study the equivariant Picard group $\\text{Pic}_G(X)$. Finally, we prove a version of Klyachko's classification theorem for toric vector bundles over an idempotent semifield.","sentences":["We introduce a notion of equivariant vector bundles on schemes over semirings.","We do this by considering the functor of points of a locally free sheaf.","We prove that every toric vector bundle on a toric scheme $X$ over an idempotent semifield equivariantly splits as a sum of toric line bundles.","We then study the equivariant Picard group $\\text{Pic}_G(X)$. Finally, we prove a version of Klyachko's classification theorem for toric vector bundles over an idempotent semifield."],"url":"http://arxiv.org/abs/2406.06933v1","category":"math.AG"}
{"created":"2024-06-11 03:41:16","title":"Starobinsky inflation and Swampland conjectures","abstract":"This paper is devoted to memory of late Professor V. G. Bagrov, who was my first teacher in theoretical physics. About 45 years later, theoretical physics has changed and me too. The subject of this paper relates some old ideas in cosmology with some recent ideas, as is reflected in the title. The current status of Starobinsky inflation is reviewed and compared to three main conjectures in the Swampland program. It is argued that the Starobinsky inflation model is not in conflict with those Swampland conjectures in their basic versions.","sentences":["This paper is devoted to memory of late Professor V. G. Bagrov, who was my first teacher in theoretical physics.","About 45 years later, theoretical physics has changed and me too.","The subject of this paper relates some old ideas in cosmology with some recent ideas, as is reflected in the title.","The current status of Starobinsky inflation is reviewed and compared to three main conjectures in the Swampland program.","It is argued that the Starobinsky inflation model is not in conflict with those Swampland conjectures in their basic versions."],"url":"http://arxiv.org/abs/2406.06923v1","category":"hep-th"}
{"created":"2024-06-11 03:15:06","title":"Frustrated phonon with charge density wave in vanadium Kagome metal","abstract":"Crystals with unique ionic arrangements and strong electronic correlations serve as a fertile ground for the emergence of exotic phases, as evidenced by the coexistence of charge density wave (CDW) and superconductivity in vanadium Kagome metals, specifically AV3Sb5 (where A represents K, Rb, or Cs). The formation of a star of David CDW superstructure, resulting from the coordinated displacements of vanadium ions on a corner sharing triangular lattice, has garnered significant attention in efforts to comprehend the influence of electron phonon interaction within this geometrically intricate lattice. However, understanding of the underlying mechanism behind CDW formation, coupled with symmetry protected lattice vibrations, remains elusive. In this study, we employed time resolved X ray scattering experiments utilising an X ray free electron laser. Our findings reveal that the phonon mode associated with the out of plane motion of Cs ions becomes frustrated in the CDW phase. Furthermore, we observed the photoinduced emergence of a metastable CDW phase, facilitated by the alleviation of frustration through nonadiabatic changes in free energy. By elucidating the longstanding puzzle surrounding the intervention of phonons in CDW ordering, this research offers fresh insights into the competition between phonons and periodic lattice distortions, a phenomenon widespread in other correlated quantum materials including layered high Tc superconductors.","sentences":["Crystals with unique ionic arrangements and strong electronic correlations serve as a fertile ground for the emergence of exotic phases, as evidenced by the coexistence of charge density wave (CDW) and superconductivity in vanadium Kagome metals, specifically AV3Sb5 (where A represents K, Rb, or Cs).","The formation of a star of David CDW superstructure, resulting from the coordinated displacements of vanadium ions on a corner sharing triangular lattice, has garnered significant attention in efforts to comprehend the influence of electron phonon interaction within this geometrically intricate lattice.","However, understanding of the underlying mechanism behind CDW formation, coupled with symmetry protected lattice vibrations, remains elusive.","In this study, we employed time resolved X ray scattering experiments utilising an X ray free electron laser.","Our findings reveal that the phonon mode associated with the out of plane motion of Cs ions becomes frustrated in the CDW phase.","Furthermore, we observed the photoinduced emergence of a metastable CDW phase, facilitated by the alleviation of frustration through nonadiabatic changes in free energy.","By elucidating the longstanding puzzle surrounding the intervention of phonons in CDW ordering, this research offers fresh insights into the competition between phonons and periodic lattice distortions, a phenomenon widespread in other correlated quantum materials including layered high Tc superconductors."],"url":"http://arxiv.org/abs/2406.06913v1","category":"cond-mat.str-el"}
